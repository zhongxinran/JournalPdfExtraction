{"1": "1. Alomari, M., Duckworth, P., Hawasly, M., Hogg, D.C., Cohn, A.G.: Natural lan- guage grounding and grammar induction for robotic manipulation commands. In: Proceedings of the First Workshop on Language Grounding for Robotics. pp. 35-43 (2017)  2. Alomari, M., Duckworth, P., Hogg, D.C., Cohn, A.G.: Learning of object proper- ties, spatial relations, and actions for embodied agents from language and vision. In: The AAAI 2017 Spring Symposium on Interactive Multisensory Object Percep- tion for Embodied Agents Technical Report SS-17-05. pp. 444-448. AAAI Press (2017)  3. Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S\u00a8underhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). vol. 2 (2018)  4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh, D.: Vqa: Visual question answering. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2425-2433 (2015)  5. Beattie, C., Leibo, J.Z., Teplyashin, D., Ward, T., Wainwright, M., K\u00a8uttler, H., Lefrancq, A., Green, S., Vald\u00b4es, V., Sadik, A., et al.: Deepmind lab. arXiv preprint arXiv:1612.03801 (2016)  6. Borenstein, J., Koren, Y.: Real-time obstacle avoidance for fast mobile robots. IEEE Transactions on systems, Man, and Cybernetics 19(5), 1179-1187 (1989) 7. Borenstein, J., Koren, Y.: The vector field histogram-fast obstacle avoidance for mobile robots. IEEE transactions on robotics and automation 7(3), 278-288 (1991) 8. Chang, A., Dai, A., Funkhouser, T., Halber, M., Nie\u00dfner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor environ- ments. arXiv preprint arXiv:1709.06158 (2017)  9. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  10. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied Ques- tion Answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)  11. Finn, C., Levine, S.: Deep visual foresight for planning robot motion. In: Robotics and Automation (ICRA), 2017 IEEE International Conference on. pp. 2786-2793. IEEE (2017)  12. Gu, S., Lillicrap, T., Sutskever, I., Levine, S.: Continuous deep q-learning with model-based acceleration. In: International Conference on Machine Learning. pp. 2829-2838 (2016)  13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)  14. Huang, T.H.K., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He, X., Kohli, P., Batra, D., et al.: Visual storytelling. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1233-1239 (2016) 15. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   Look Before You LeapReferences  1. Alomari, M., Duckworth, P., Hawasly, M., Hogg, D.C., Cohn, A.G.: Natural lan- guage grounding and grammar induction for robotic manipulation commands. In: Proceedings of the First Workshop on Language Grounding for Robotics. pp. 35-43 (2017)  2. Alomari, M., Duckworth, P., Hogg, D.C., Cohn, A.G.: Learning of object proper- ties, spatial relations, and actions for embodied agents from language and vision. In: The AAAI 2017 Spring Symposium on Interactive Multisensory Object Percep- tion for Embodied Agents Technical Report SS-17-05. pp. 444-448. AAAI Press (2017)  3. Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S\u00a8underhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). vol. 2 (2018)  4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh, D.: Vqa: Visual question answering. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2425-2433 (2015)  5. Beattie, C., Leibo, J.Z., Teplyashin, D., Ward, T., Wainwright, M., K\u00a8uttler, H., Lefrancq, A., Green, S., Vald\u00b4es, V., Sadik, A., et al.: Deepmind lab. arXiv preprint arXiv:1612.03801 (2016)  6. Borenstein, J., Koren, Y.: Real-time obstacle avoidance for fast mobile robots. IEEE Transactions on systems, Man, and Cybernetics 19(5), 1179-1187 (1989) 7. Borenstein, J., Koren, Y.: The vector field histogram-fast obstacle avoidance for mobile robots. IEEE transactions on robotics and automation 7(3), 278-288 (1991) 8. Chang, A., Dai, A., Funkhouser, T., Halber, M., Nie\u00dfner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor environ- ments. arXiv preprint arXiv:1709.06158 (2017)  9. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  10. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied Ques- tion Answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)  11. Finn, C., Levine, S.: Deep visual foresight for planning robot motion. In: Robotics and Automation (ICRA), 2017 IEEE International Conference on. pp. 2786-2793. IEEE (2017)  12. Gu, S., Lillicrap, T., Sutskever, I., Levine, S.: Continuous deep q-learning with model-based acceleration. In: International Conference on Machine Learning. pp. 2829-2838 (2016)  13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)  14. Huang, T.H.K., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He, X., Kohli, P., Batra, D., et al.: Visual storytelling. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1233-1239 (2016) 15. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   16  X. Wang et al.  16. Kempka, M., Wydmuch, M., Runc, G., Toczek, J., Ja\u00b4skowski, W.: Vizdoom: A doom-based ai research platform for visual reinforcement learning. In: Computa- tional Intelligence and Games (CIG), 2016 IEEE Conference on. pp. 1-8. IEEE (2016)  17. Kim, D., Nevatia, R.: Symbolic navigation with a generic map. Autonomous Robots  6(1), 69-88 (1999)  18. Lenz, I., Knepper, R.A., Saxena, A.: Deepmpc: Learning deep latent features for  model predictive control. In: Robotics: Science and Systems (2015)  19. Luong, M.T., Pham, H., Manning, C.D.: E\ufb00ective approaches to attention-based  neural machine translation. arXiv preprint arXiv:1508.04025 (2015)  20. Mei, H., Bansal, M., Walter, M.R.: Listen, attend, and walk: Neural mapping of  navigational instructions to action sequences. In: AAAI. vol. 1, p. 2 (2016)  21. Misra, D.K., Langford, J., Artzi, Y.: Mapping instructions and visual observations to actions with reinforcement learning. arXiv preprint arXiv:1704.08795 (2017) 22. Oh, J., Singh, S., Lee, H.: Value prediction network. In: Advances in Neural Infor-  mation Processing Systems. pp. 6120-6130 (2017)  23. Oriolo, G., Vendittelli, M., Ulivi, G.: On-line map building and navigation for autonomous mobile robots. In: Robotics and Automation, 1995. Proceedings., 1995 IEEE International Conference on. vol. 3, pp. 2900-2906. IEEE (1995)  24. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised prediction. In: International Conference on Machine Learning (ICML). vol. 2017 (2017)  25. Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with  recurrent neural networks. arXiv preprint arXiv:1511.06732 (2015)  26. Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac- Arnold, G., Reichert, D., Rabinowitz, N., Barreto, A., et al.: The predictron: End- to-end learning and planning. arXiv preprint arXiv:1612.08810 (2016)  27. Sutton, R.S.: Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In: Machine Learning Proceedings 1990, pp. 216-224. Elsevier (1990)  28. Talvitie, E.: Agnostic system identification for monte carlo planning. In: AAAI.  pp. 2986-2992 (2015)  29. Tamar, A., Wu, Y., Thomas, G., Levine, S., Abbeel, P.: Value iteration networks. In: Advances in Neural Information Processing Systems. pp. 2154-2162 (2016) 30. Thomason, J., Sinapov, J., Mooney, R.: Guiding interaction behaviors for multi- modal grounded language learning. In: Proceedings of the First Workshop on Lan- guage Grounding for Robotics. pp. 20-24 (2017)  31. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption generator. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. pp. 3156-3164. IEEE (2015)  32. Wang, X., Chen, W., Wang, Y.F., Wang, W.Y.: No metrics are perfect: Adversarial reward learning for visual storytelling. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 899-909. Association for Computational Linguistics (2018)  33. Wang, X., Chen, W., Wu, J., Wang, Y.F., Wang, W.Y.: Video captioning via hierarchical reinforcement learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4213-4222 (2018)  34. Wang, X., Wang, Y.F., Wang, W.Y.: Watch, listen, and describe: Globally and locally aligned cross-modal attentions for video captioning. In: Proceedings of the   Look Before You Leap2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 2 (Short Papers). pp. 795-801. Association for Computational Linguistics (2018)  35. Watter, M., Springenberg, J., Boedecker, J., Riedmiller, M.: Embed to control: A locally linear latent dynamics model for control from raw images. In: Advances in neural information processing systems. pp. 2746-2754 (2015)  36. Weber, T., Racani`ere, S., Reichert, D.P., Buesing, L., Guez, A., Rezende, D.J., Badia, A.P., Vinyals, O., Heess, N., Li, Y., et al.: Imagination-augmented agents for deep reinforcement learning. arXiv preprint arXiv:1707.06203 (2017)  37. Xiong, W., Guo, X., Yu, M., Chang, S., Zhou, B., Wang, W.Y.: Scheduled policy optimization for natural language communication with intelligent agents. arXiv preprint arXiv:1806.06187 (2018)  38. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: International Conference on Machine Learning. pp. 2048-2057 (2015) 39. Yao, H., Bhatnagar, S., Diao, D., Sutton, R.S., Szepesv\u00b4ari, C.: Multi-step dyna planning for policy evaluation and control. In: Advances in Neural Information Processing Systems. pp. 2187-2195 (2009)  40. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using hierarchical recurrent neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4584-4593 (2016)  41. Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A.: Target-driven visual navigation in indoor scenes using deep reinforcement learning. In: Robotics and Automation (ICRA), 2017 IEEE International Conference on. pp. 3357-3364. IEEE (2017)"}