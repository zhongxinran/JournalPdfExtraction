{"1": "1 (1995). ,  1. Alvarez, J.M., Salzmann, M.: Compression-aware Training of Deep Networks. In:  Neural Information Processing Systems (2017),  2. Boggs, P.T., Tolle, J.W.: Sequential Quadratic Programming. Acta Numerica 4,  3. Dai, Y.H.: Convergence Properties of the BFGS Algoritm. SIAM Journal on Op-  timization 13(3), 693-701 (2002). ,  4. Dai, Y.H., Schittkowski, K.: A Sequential Quadratic Programming Algorithm with Non-Monotone Line Search. Pacific Journal of Optimization 4, 335-351 (2008) 5. Gavish, M., Donoho, D.L.: The Optimal Hard Threshold for Singular Values is 4/\u221a3. IEEE Transactions on Information Theory 60(8), 5040-5053 (2014). , 6. Ge, R., Huang, F., Jin, C., Yuan, Y.: Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition. Journal of Machine Learning Re- search 40 (2015)  7. Gower, R.M., Goldfarb, D., Richtarik, P.: Stochastic Block BFGS: Squeezing More Curvature out of Data. In: International Conference on Machine Learning (2016). 8. Han, S., Mao, H., Dally, W.J.: Deep Compression - Compressing Deep Neural Net- works with Pruning, Trained Quantization and Hu\ufb00man Coding. In: International Conference on Learning Representations (2016),  9. Ioannou, Y., Robertson, D., Shotton, J., Cipolla, R., Criminisi, A.: Training CNNs with Low-Rank Filters for E\ufb03cient Image Classification. In: International Confer- ence on Learning Representations (2016),  10. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and Training of Neural Networks for E\ufb03cient Integer-Arithmetic-Only Inference. ArXiv (2017),  11. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up Convolutional Neural Net- works with Low Rank Expansions. In: British Machine Vision Conference (BMVC) (2014). ,  12. Keutzer, F.N.I., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Kurt: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size. In: International Conference on Learning Representations (2017). ,  13. Kim, Y.D., Park, E., Yoo, S., Choi, T., Yang, L., Shin, D.: Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications. In: International Conference on Learning Representations (2016),  14. Krizhevsky, A.: Learning Multiple Layers of Features from Tiny Images. Tech. rep.  15. Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., Lempitsky, V.: Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition. In: Interna- tional Conference on Learning Representations (2015),  16. Lebedev, V., Lempitsky, V.: Fast ConvNets Using Group-wise Brain Damage. In:  Conference on Computer Vision and Pattern Recognition (2016). ,  17. Lin, M., Chen, Q., Yan, S.: Network In Network. In: International Conference on  Learning Representations (2013). ,  18. Mokhtari, A.: E\ufb03cient Methods for Large-Scale Empirical Risk Minimization.  Ph.D. thesis, University of Pennsylvania (2017)  19. MOSEK: The MOSEK optimization toolbox for MATLAB manual. Tech. rep.  (2009)  (2017)  20. Nakajima, S., Tomioka, R., Sugiyama, M., Babacan, S.D.: Condition for Perfect Di- mensionality Recovery by Variational Bayesian PCA. Journal of Machine Learning Reasearch 16, 3757-3811 (2016)   Constrained Optimization Based Low-Rank ApproximationReferences  1 (1995). ,  1. Alvarez, J.M., Salzmann, M.: Compression-aware Training of Deep Networks. In:  Neural Information Processing Systems (2017),  2. Boggs, P.T., Tolle, J.W.: Sequential Quadratic Programming. Acta Numerica 4,  3. Dai, Y.H.: Convergence Properties of the BFGS Algoritm. SIAM Journal on Op-  timization 13(3), 693-701 (2002). ,  4. Dai, Y.H., Schittkowski, K.: A Sequential Quadratic Programming Algorithm with Non-Monotone Line Search. Pacific Journal of Optimization 4, 335-351 (2008) 5. Gavish, M., Donoho, D.L.: The Optimal Hard Threshold for Singular Values is 4/\u221a3. IEEE Transactions on Information Theory 60(8), 5040-5053 (2014). , 6. Ge, R., Huang, F., Jin, C., Yuan, Y.: Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition. Journal of Machine Learning Re- search 40 (2015)  7. Gower, R.M., Goldfarb, D., Richtarik, P.: Stochastic Block BFGS: Squeezing More Curvature out of Data. In: International Conference on Machine Learning (2016). 8. Han, S., Mao, H., Dally, W.J.: Deep Compression - Compressing Deep Neural Net- works with Pruning, Trained Quantization and Hu\ufb00man Coding. In: International Conference on Learning Representations (2016),  9. Ioannou, Y., Robertson, D., Shotton, J., Cipolla, R., Criminisi, A.: Training CNNs with Low-Rank Filters for E\ufb03cient Image Classification. In: International Confer- ence on Learning Representations (2016),  10. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and Training of Neural Networks for E\ufb03cient Integer-Arithmetic-Only Inference. ArXiv (2017),  11. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up Convolutional Neural Net- works with Low Rank Expansions. In: British Machine Vision Conference (BMVC) (2014). ,  12. Keutzer, F.N.I., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Kurt: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size. In: International Conference on Learning Representations (2017). ,  13. Kim, Y.D., Park, E., Yoo, S., Choi, T., Yang, L., Shin, D.: Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications. In: International Conference on Learning Representations (2016),  14. Krizhevsky, A.: Learning Multiple Layers of Features from Tiny Images. Tech. rep.  15. Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., Lempitsky, V.: Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition. In: Interna- tional Conference on Learning Representations (2015),  16. Lebedev, V., Lempitsky, V.: Fast ConvNets Using Group-wise Brain Damage. In:  Conference on Computer Vision and Pattern Recognition (2016). ,  17. Lin, M., Chen, Q., Yan, S.: Network In Network. In: International Conference on  Learning Representations (2013). ,  18. Mokhtari, A.: E\ufb03cient Methods for Large-Scale Empirical Risk Minimization.  Ph.D. thesis, University of Pennsylvania (2017)  19. MOSEK: The MOSEK optimization toolbox for MATLAB manual. Tech. rep.  (2009)  (2017)  20. Nakajima, S., Tomioka, R., Sugiyama, M., Babacan, S.D.: Condition for Perfect Di- mensionality Recovery by Variational Bayesian PCA. Journal of Machine Learning Reasearch 16, 3757-3811 (2016)   16  Chong Li and C.J. Richard Shi  21. Novikov, A., Vetrov, D., Podoprikhin, D., Osokin, A.: Tensorizing Neural Networks.  In: Neural Information Processing Systems (2015),  22. Nowak, I.: Relaxation and Decomposition Methods for Mixed Integer Nonlinear  Programming. Birkh\u00a8auser Basel (2005).  23. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, A.C.B., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (2015)  24. Park, E., Ahn, J., Yoo, S.: Weighted-Entropy-Based Quantization for Deep Neural Networks. In: Conference on Computer Vision and Pattern Recognition (2017). 25. Raghavan, P., Tompson, C.: Randomized rounding: A technique for provably good  algorithms and algorithmic proofs. Combinatorica 7(4), 365-374 (1987).  26. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image Recognition. In: International Conference on Learning Representations (2015)  27. Tai, C., Xiao, T., Zhang, Y., Wang, X., E, W.: Convolutional neural networks with low-rank regularization. In: International Conference on Learning Representations (2016),  28. Yu, X., Liu, T., Wang, X., Tao, D.: On compressing deep models by low rank and sparse decomposition. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2017).  29. Zhang, J., Mitliagkas, I., R\u00b4e, C.: YellowFin and the Art of Momentum Tuning.  arXiv preprint (2017),  30. Zhang, X., Zou, J., Ming, X., He, K., Sun, J.: E\ufb03cient and Accurate Approxima- tions of Nonlinear Convolutional Networks. In: Conference on Computer Vision and Pattern Recognition (2015),  31. Zhou, G.: Rank-Constrained Optimization : A Riemannian Manifold Approach.  Ph.D. thesis, Florida State University (2015)"}