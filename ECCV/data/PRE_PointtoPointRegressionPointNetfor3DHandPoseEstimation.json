{"1": "1. Ballan, L., Taneja, A., Gall, J., Gool, L.V., Pollefeys, M.: Motion capture of hands  in action using discriminative salient points. In: ECCV (2012)   14  Liuhao Ge, Zhou Ren, and Junsong Yuan  exhibit significant errors, which may make the evaluation on this dataset less meaningful and may limit the learning ability of our deep neural network.  In addition, we present some qualitative results for NYU [43], ICVL [36] and  MSRA [35] datasets in the supplementary material.  4.3 Runtime and Model Size  The runtime of our method is 23.9ms per frame in average, including 8.2ms for point sampling and surface normal calculation, 15.1ms for the two-stacked hierarchical PointNet forward propagation, 0.6ms for hand pose inference and post-processing. Thus, our method runs in real-time at about 41.8fps.  In addition, the model size our network is 17.2MB, including 11.1MB for the point-to-point regression network which is a two-stacked hierarchical PointNet and 6.1MB for the additional direct regression module which consists of three fully-connected layers. Compared with the model size of the 3D CNNs proposed in [11] which is about 420MB, our model size is smaller.  5 Conclusion  In this paper, we propose a novel approach that directly takes the 3D point cloud of hand as network input and outputs heat-maps as well as unit vector fields on the point cloud, re\ufb02ecting the per-point closeness and directions to hand joints. We infer 3D hand joint locations from the estimated heat-maps and unit vector fields using weighted fusion. Similar to the stacked hourglass network [18], we apply the stacked network architecture for the hierarchical PointNet [27], which allows repeated bottom-up and top-down inference on point cloud and is able to further boost the performance. Our proposed point-to-point regression method can also be easily combined with direct regression method to achieve more robust performance. Experimental results on three challenging hand pose datasets show that our method achieves superior accuracy performance in real-time.  Acknowledgment  This research is supported by the BeingTogether Centre, a collaboration between NTU Singapore and UNC at Chapel Hill. The BeingTogether Centre is supported by the National Research Foundation, Prime Minister\u2019s O\ufb03ce, Singapore under its International Research Centres in Singapore Funding Initiative. This work is also supported in part by start-up grants from University at Bu\ufb00alo and a gift grant from Snap Inc.  References  1. Ballan, L., Taneja, A., Gall, J., Gool, L.V., Pollefeys, M.: Motion capture of hands  in action using discriminative salient points. In: ECCV (2012)   Point-to-Point Regression PointNet for 3D Hand Pose Estimation2. Cai, Y., Ge, L., Cai, J., Yuan, J.: Weakly-supervised 3d hand pose estimation from  monocular rgb images. In: ECCV (2018)  3. Cao, Z., Huang, Q., Ramani, K.: 3d object classification via spherical projections.  In: 3DV (2017)  4. Chen, X., Wang, G., Guo, H., Zhang, C.: Pose guided structured region ensemble network for cascaded hand pose estimation. arXiv preprint arXiv:1708.03416 (2017) 5. Choi, C., Kim, S., Ramani, K.: Learning hand articulations by hallucinating heat  distribution. In: ICCV (2017)  6. Choi, C., Sinha, A., Hee Choi, J., Jang, S., Ramani, K.: A collaborative filtering  approach to real-time hand pose estimation. In: ICCV (2015)  7. Ge, L., Liang, H., Yuan, J., Thalmann, D.: Real-time 3d hand pose estimation with 3d convolutional neural networks. IEEE Trans. Pattern Anal. Mach. Intell. pp. 1-15 (2018)  8. Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3d hand pose estimation from single depth images using multi-view cnns. IEEE Trans. Image Processing 27(9), 4422-4436 (2018)  9. Ge, L., Cai, Y., Weng, J., Yuan, J.: Hand pointnet: 3D hand pose estimation using point sets. In: Proc. IEEE Conf. Comput. Vis. Pattern Recog. pp. 8417-8426 (2018) 10. Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3D hand pose estimation in single depth images: from single-view CNN to multi-view CNNs. In: CVPR (2016) 11. Ge, L., Liang, H., Yuan, J., Thalmann, D.: 3D convolutional neural networks for e\ufb03cient and robust hand pose estimation from single depth images. In: CVPR (2017)  12. Guo, H., Wang, G., Chen, X., Zhang, C., Qiao, F., Yang, H.: Region ensemble net- work: Improving convolutional network for hand pose estimation. In: ICIP (2017) 13. Keskin, C., Kra, F., Kara, Y., Akarun, L.: Hand pose estimation and hand shape classification using multi-layered randomized decision forests. In: ECCV (2012) 14. Khamis, S., Taylor, J., Shotton, J., Keskin, C., Izadi, S., Fitzgibbon, A.: Learning  an e\ufb03cient model of hand shape variation from depth images. In: CVPR (2015)  15. Klokov, R., Lempitsky, V.: Escape from cells: Deep kd-networks for the recognition  of 3d point cloud models. In: ICCV (2017)  16. Maturana, D., Scherer, S.: Voxnet: A 3D convolutional neural network for real-time  object recognition. In: IROS (2015)  17. Moon, G., Chang, J.Y., Lee, K.M.: V2V-PoseNet: Voxel-to-voxel prediction net- work for accurate 3d hand and human pose estimation from a single depth map. In: CVPR (2018)  18. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-  mation. In: ECCV (2016)  19. Oberweger, M., Lepetit, V.: Deepprior++: Improving fast and accurate 3d hand  pose estimation. In: ICCV Workshop (2017)  20. Oberweger, M., Riegler, G., Wohlhart, P., Lepetit, V.: E\ufb03ciently creating 3d train-  ing data for fine hand pose estimation. In: CVPR (2016)  21. Oberweger, M., Wohlhart, P., Lepetit, V.: Hands deep in deep learning for hand  22. Oberweger, M., Wohlhart, P., Lepetit, V.: Training a feedback loop for hand pose  pose estimation. In: CVWW (2015)  estimation. In: ICCV (2015)  23. Oikonomidis, I., Kyriazis, N., Argyros, A.: E\ufb03cient model-based 3D tracking of  hand articulations using Kinect. In: BMVC (2011)  24. Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K.: Coarse-to-fine volumetric  prediction for single-image 3d human pose. In: CVPR (2017)   16  Liuhao Ge, Zhou Ren, and Junsong Yuan  25. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: PointNet: Deep learning on point sets for  3D classification and segmentation. In: CVPR (2017)  26. Qi, C.R., Su, H., Nie\u00dfner, M., Dai, A., Yan, M., Guibas, L.J.: Volumetric and  multi-view cnns for object classification on 3d data. In: CVPR (2016)  27. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: PointNet++: Deep hierarchical feature learn-  ing on point sets in a metric space. In: NIPS (2017)  28. Remelli, E., Tkach, A., Tagliasacchi, A., Pauly, M.: Low-dimensionality calibration through local anisotropic scaling for robust hand model personalization. In: ICCV (2017)  29. Riegler, G., Ulusoy, A.O., Geiger, A.: Octnet: Learning deep 3d representations at  high resolutions. In: CVPR (2017)  30. Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net: Localization-classification-  regression for human pose. In: CVPR (2017)  31. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: modeling and capturing hands and bodies together. ACM Transactions on Graphics (TOG) 36(6), 245 (2017)  32. Sharp, T., Keskin, C., Robertson, D., Taylor, J., Shotton, J., Kim, D., Rheman- n, C., Leichter, I., Vinnikov, A., Wei, Y., Freedman, D., Kohli, P., Krupka, E., Fitzgibbon, A., Izadi, S.: Accurate, robust, and \ufb02exible real-time hand tracking. In: CHI (2015)  33. Song, S., Xiao, J.: Deep Sliding Shapes for amodal 3D object detection in RGB-D  images. In: CVPR (2016)  34. Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view convolutional  neural networks for 3D shape recognition. In: ICCV (2015)  35. Sun, X., Wei, Y., Liang, S., Tang, X., Sun, J.: Cascaded hand pose regression. In:  CVPR (2015)  36. Tang, D., Chang, H.J., Tejani, A., Kim, T.K.: Latent regression forest: Structured  estimation of 3D articulated hand posture. In: CVPR (2014)  37. Tang, D., Taylor, J., Kohli, P., Keskin, C., Kim, T.K., Shotton, J.: Opening the black box: Hierarchical sampling optimization for estimating human hand pose. In: ICCV (2015)  38. Taylor, J., Bordeaux, L., Cashman, T., Corish, B., Keskin, C., Sharp, T., Soto, E., Sweeney, D., Valentin, J., Lu\ufb00, B., Topalian, A., Wood, E., Khamis, S., Kohli, P., Izadi, S., Banks, R., Fitzgibbon, A., Shotton, J.: E\ufb03cient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. ACM Transactions on Graphics 35(4), 143 (2016)  39. Taylor, J., Shotton, J., Sharp, T., Fitzgibbon, A.: The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation. In: CVPR (2012) 40. Tkach, A., Tagliasacchi, A., Remelli, E., Pauly, M., Fitzgibbon, A.: Online gen- erative model personalization for hand tracking. ACM Transactions on Graphics (TOG) 36(6), 243 (2017)  41. Tome, D., Russell, C., Agapito, L.: Lifting from the deep: Convolutional 3d pose  estimation from a single image. In: CVPR (2017)  42. Tompson, J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional network and a graphical model for human pose estimation. In: NIPS (2014) 43. Tompson, J., Stein, M., Lecun, Y., Perlin, K.: Real-time continuous pose recovery of human hands using convolutional networks. ACM Transactions on Graphics 33(5), 169 (2014)  44. Tzionas, D., Ballan, L., Srikantha, A., Aponte, P., Pollefeys, M., Gall, J.: Cap- turing hands in action using discriminative salient points and physics simulation. International Journal of Computer Vision 118(2), 172-193 (2016)   Point-to-Point Regression PointNet for 3D Hand Pose Estimation45. Wan, C., Probst, T., Van Gool, L., Yao, A.: Crossing nets: Dual generative models  with a shared latent space for hand pose estimation. In: CVPR (2017)  46. Wan, C., Probst, T., Van Gool, L., Yao, A.: Dense 3d regression for hand pose  47. Wan, C., Yao, A., Van Gool, L.: Direction matters: hand pose estimation from  estimation pp. 5147-5156 (2018)  local surface normals. In: ECCV (2016)  48. Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X.: O-cnn: Octree-based con- volutional neural networks for 3d shape analysis. ACM Transactions on Graphics (TOG) 36(4), 72 (2017)  49. Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines.  50. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3D shapenets:  A deep representation for volumetric shapes. In: CVPR (2015)  51. Xu, C., Cheng, L.: E\ufb03cient hand pose estimation from a single depth image. In:  In: CVPR (2016)  ICCV (2013)  52. Xu, C., Govindarajan, L.N., Zhang, Y., Cheng, L.: Lie-X: Depth image based articulated object pose estimation, tracking, and action recognition on lie groups. International Journal of Computer Vision 123(3), 454-478 (2017)  53. Ye, Q., Yuan, S., Kim, T.K.: Spatial attention deep net with partial pso for hier-  archical hybrid hand pose estimation. In: ECCV (2016)  54. Yuan, S., Garcia-Hernando, G., Stenger, B., Moon, G., Chang, J.Y., Lee, K.M., Molchanov, P., Kautz, J., Honari, S., Ge, L., Yuan, J., Chen, X., Wang, G., Yang, F., Akiyama, K., Wu, Y., Wan, Q., Madadi, M., Escalera, S., Li, S., Lee, D., Oikonomidis, I., Argyros, A., Kim, T.K.: Depth-based 3D hand pose estimation: From current achievements to future goals. In: CVPR (2018)  55. Yuan, S., Ye, Q., Stenger, B., Jain, S., Kim, T.K.: Bighand2.2m benchmark: Hand  pose dataset and state of the art analysis. In: CVPR (2017)  56. Zhou, X., Wan, Q., Zhang, W., Xue, X., Wei, Y.: Model-based deep hand pose  estimation. In: IJCAI (2016)"}