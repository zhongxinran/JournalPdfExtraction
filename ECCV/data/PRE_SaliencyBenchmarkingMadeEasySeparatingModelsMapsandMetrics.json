{"1": "1. Adeli, H., Vitu, F., Zelinsky, G.J.: A model of the superior colliculus predicts fix- ation locations during scene viewing and visual search. J. Neurosci. 37(6), 1453- 1467 (Dec 2016). https://doi.org/10.1523/jneurosci.0825-16.2016, https://doi. org/10.1523/jneurosci.0825-16.2016 2  2. Barthelme, S., Trukenbrod, H., Engbert, R., Wichmann, F.: Modeling fixation locations using spatial point processes. Journal of Vision 13(12), 1-1 (Oct 2013). https://doi.org/10.1167/13.12.1, https://doi.org/10.1167/13.12.1 2, 3  3. Borji, A., Sihite, D.N., Itti, L.: Objects do not predict fixations better than early saliency: A re-analysis of einhauser et al.\u2019s data. Journal of Vision 13(10), 18-18 (Aug 2013). https://doi.org/10.1167/13.10.18, https://doi.org/10.1167/13.10. 18 2  4. Borji, A.,  Itti,  L.:  State-of-the-art  IEEE Trans. Pattern Anal. Mach. https://doi.org/10.1109/tpami.2012.89, 2012.89 2, 10, 13  in  visual  attention modeling. Intell. 35(1), 185-207 (Jan 2013). https://doi.org/10.1109/tpami.  5. Borji, A., Sihite, D.N., Itti, L.: Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study. IEEE Trans. on Image Process. 22(1), 55-69 (Jan 2013). https://doi.org/10.1109/tip.2012.2210727, https://doi. org/10.1109/tip.2012.2210727 10, 13  6. Bruce, N.D.B., Tsotsos, J.K.: Saliency, attention, and visual search: An in- 5-5 (Mar 2009).  formation theoretic approach. Journal of Vision 9(3), https://doi.org/10.1167/9.3.5, https://doi.org/10.1167/9.3.5 2, 9   14  M. K\u00a8ummerer, T.S.A. Wallis and M. Bethge  attentional guidance or an importance map for (e.g.) choosing the next place to fixate in a scene [34,48,26]. Our nomenclature is rather independent and intended for saliency model benchmarking.  The code for evaluating saliency models as demonstrated in this work has been released as part of the pysaliency python library (available at https: //github.com/matthias-k/pysaliency).  Conclusion Our work solves the problem that one saliency model cannot reach state-of-the-art performance in all relevant saliency metrics. Our key theoretical contribution is to decouple the notions of saliency models and saliency maps. For benchmarking practice, this means that saliency models can be meaningfully compared on all metrics in their original scale. Therefore, our method allows comparing to traditional models that do not use this method; it works even if only metric scores of other models are known (as for example in cases where metric scores are published in a paper). Practically, this means that there is no need to revise an existing benchmark: researchers who submit model densities can have their performance fairly evaluated, but existing models can remain in the table. The MIT saliency benchmark will implement this option.  Acknowledgements This study is part of Matthias K\u00a8ummerer\u2019s thesis work at the International Max Planck Research School for Intelligent Systems (IMPRS- IS). The research has been funded by the German Science Foundation (DFG; Collaborative Research Centre 1233) and the German Excellency Initiative (EXC307).  References  1. Adeli, H., Vitu, F., Zelinsky, G.J.: A model of the superior colliculus predicts fix- ation locations during scene viewing and visual search. J. Neurosci. 37(6), 1453- 1467 (Dec 2016). https://doi.org/10.1523/jneurosci.0825-16.2016, https://doi. org/10.1523/jneurosci.0825-16.2016 2  2. Barthelme, S., Trukenbrod, H., Engbert, R., Wichmann, F.: Modeling fixation locations using spatial point processes. Journal of Vision 13(12), 1-1 (Oct 2013). https://doi.org/10.1167/13.12.1, https://doi.org/10.1167/13.12.1 2, 3  3. Borji, A., Sihite, D.N., Itti, L.: Objects do not predict fixations better than early saliency: A re-analysis of einhauser et al.\u2019s data. Journal of Vision 13(10), 18-18 (Aug 2013). https://doi.org/10.1167/13.10.18, https://doi.org/10.1167/13.10. 18 2  4. Borji, A.,  Itti,  L.:  State-of-the-art  IEEE Trans. Pattern Anal. Mach. https://doi.org/10.1109/tpami.2012.89, 2012.89 2, 10, 13  in  visual  attention modeling. Intell. 35(1), 185-207 (Jan 2013). https://doi.org/10.1109/tpami.  5. Borji, A., Sihite, D.N., Itti, L.: Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study. IEEE Trans. on Image Process. 22(1), 55-69 (Jan 2013). https://doi.org/10.1109/tip.2012.2210727, https://doi. org/10.1109/tip.2012.2210727 10, 13  6. Bruce, N.D.B., Tsotsos, J.K.: Saliency, attention, and visual search: An in- 5-5 (Mar 2009).  formation theoretic approach. Journal of Vision 9(3), https://doi.org/10.1167/9.3.5, https://doi.org/10.1167/9.3.5 2, 9   Saliency Benchmarking Made Easy7. Bruce, N.D.B., Catton, C., Janjic, S.: A deeper look at saliency: Feature contrast, semantics, and beyond. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (Jun 2016). https://doi.org/10.1109/cvpr.2016.62, https://doi.org/10.1109/cvpr.2016.62 2  8. Bruce, N.D., Wloka, C., Frosst, N., Rahman, S., Tsotsos, J.K.: On computa- tional modeling of visual saliency: Examining what\u2019s right, and what\u2019s left. Vision Research 116, 95-112 (Nov 2015). https://doi.org/10.1016/j.visres.2015.01.010, https://doi.org/10.1016/j.visres.2015.01.010 2  9. Bylinskii, Z., Judd, T., Durand, F., Oliva, A., Torralba, A.: MIT saliency bench-  mark. http://saliency.mit.edu/ 2  10. Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., Durand, F.: What do di\ufb00erent evaluation metrics tell us about saliency models? arXiv:1604.03605 [cs] (2016), http://arxiv.org/abs/1604.03605 2  11. Bylinskii, Z., Recasens, A., Borji, A., Oliva, A., Torralba, A., Durand, F.: Where should saliency models look next? In: Computer Vision - ECCV 2016. pp. 809-824. Lecture Notes in Computer Science, Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46454-1 49, https://link.springer. com/chapter/10.1007/978-3-319-46454-1_49 2  12. Cerf, M., Harel, J., Huth, A., Einh\u00a8auser, W., Koch, C.: Decoding what from scanpaths. people see from where they look: Predicting visual stimuli In: Attention in Cognitive Systems, pp. 15-26. Springer Berlin Heidelberg (2009). https://doi.org/10.1007/978-3-642-00582-4 2, https://doi.org/10.1007/ 978-3-642-00582-4_2 2  13. Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Predicting human eye fixations via an LSTM-based saliency attentive model. arXiv:1611.09571 [cs] (2016), http: //arxiv.org/abs/1611.09571 10, 13  14. Einhauser, W., Spain, M., Perona, P.: Objects predict fixations bet- than early saliency. Journal of Vision 8(14), 18-18 (Nov 2008).  ter https://doi.org/10.1167/8.14.18, https://doi.org/10.1167/8.14.18 2  15. Harel, J., Koch, C., Perona, P.: Graph-based visual saliency. In: Advances in neural  information processing systems. pp. 545-552 (2006) 2  16. Huang, X., Shen, C., Boix, X., Zhao, Q.: SALICON: Reducing the se- mantic gap in saliency prediction by adapting deep neural networks. In: IEEE 2015 IEEE International Conference on Computer Vision (ICCV). (Dec 2015). https://doi.org/10.1109/iccv.2015.38, https://doi.org/10.1109/ iccv.2015.38 2  17. Itti, L., Koch, C., Niebur, E.: A model of saliency-based visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Machine Intell. 20(11), 1254- 1259 (1998). https://doi.org/10.1109/34.730558, https://doi.org/10.1109/34. 730558 2  18. Itti, L.: Quantifying the contribution of  low-level saliency to human eye movements in dynamic scenes. Visual Cognition 12(6), 1093-1123 (Aug 2005). https://doi.org/10.1080/13506280444000661, https://doi.org/10.1080/ 13506280444000661 2  19. Itti, L., Borji, A.: Computational models: Bottom-up and top-down aspects. In:  The Oxford Handbook of Attention. Oxford University Press (2014) 2  20. Jetley, S., Murray, N., Vig, E.: End-to-end saliency mapping via probability dis- tribution prediction. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (Jun 2016). https://doi.org/10.1109/cvpr.2016.620, https://doi.org/10.1109/cvpr.2016.620 2   16  M. K\u00a8ummerer, T.S.A. Wallis and M. Bethge  21. Jiang, M., Huang, S., Duan, J., Zhao, Q.: SALICON: Saliency in context. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (Jun 2015). https://doi.org/10.1109/cvpr.2015.7298710, https://doi.org/ 10.1109/cvpr.2015.7298710 2  22. Jost, T., Ouerhani, N., Wartburg, R.v., M\u00a8uri, R., H\u00a8ugli, H.: Assessing the con- tribution of color in visual attention. Computer Vision and Image Understanding 100(1-2), 107-123 (Oct 2005). https://doi.org/10.1016/j.cviu.2004.10.009, https: //doi.org/10.1016/j.cviu.2004.10.009 2, 7  23. Judd, T., Durand, F.d., Torralba, A.: A Benchmark of Computational Mod- els of Saliency to Predict Human Fixations. CSAIL Technical Reports (2012). https://doi.org/1721.1/68590 2, 8  24. Judd, T., Ehinger, K., Durand, F., Torralba, A.: Learning to predict where hu- mans look. In: 2009 IEEE 12th International Conference on Computer Vision. IEEE (Sep 2009). https://doi.org/10.1109/iccv.2009.5459462, https://doi.org/ 10.1109/iccv.2009.5459462 2, 9  25. Kienzle, W., Franz, M.O., Scholkopf, B., Wichmann, F.A.: Center-surround pat- terns emerge as optimal predictors for human saccade targets. Journal of Vision 9(5), 7-7 (May 2009). https://doi.org/10.1167/9.5.7, https://doi.org/10.1167/ 9.5.7 2  26. Koch, C., Ullman, S.: Shifts in selective visual attention: Towards the underlying neural circuitry. Human Neurobiology 4, 219-227 (1985), https://cseweb.ucsd. edu/classes/fa09/cse258a/papers/koch-ullman-1985.pdf 1, 14  27. Koehler, K., Guo, F., Zhang, S., Eckstein, M.P.: What do saliency models pre- dict? Journal of Vision 14(3), 14-14 (Mar 2014). https://doi.org/10.1167/14.3.14, https://doi.org/10.1167/14.3.14 2  28. Kruthiventi, S.S.S., Ayush, K., Babu, R.V.: DeepFix: A fully convolutional neu- ral network for predicting human eye fixations. IEEE Trans. on Image Process. 26(9), 4446-4456 (Sep 2017). https://doi.org/10.1109/tip.2017.2710620, https: //doi.org/10.1109/tip.2017.2710620 2  29. K\u00a8ummerer, M.: pysaliency. https://github.com/matthias-k/pysaliency 8 30. K\u00a8ummerer, M., Theis, L., Bethge, M.: Deep gaze i: Boosting saliency prediction with feature maps trained on ImageNet. In: 2015 International Conference on Learning Representations - Workshop Track (ICLR) (2015), https://arxiv.org/ abs/1411.1045 2  31. K\u00a8ummerer, M., Wallis, T.S.A., Gatys, L.A., Bethge, M.: Understanding low- and high-level contributions to fixation prediction. In: The IEEE International Confer- ence on Computer Vision (ICCV). IEEE (Oct 2017) 2, 9  32. K\u00a8ummerer, M., Wallis, T.S.A., Bethge, M.: Information-theoretic model com- parison unifies saliency metrics. Proc Natl Acad Sci USA 112(52), 16054- 16059 (Dec 2015). https://doi.org/10.1073/pnas.1510393112, https://doi.org/ 10.1073/pnas.1510393112 2, 3, 5, 6, 7, 9, 11, 12  33. Le Meur, O., Baccino, T.: Methods for comparing scanpaths and saliency maps: Strengths and weaknesses. Behav Res 45(1), 251-266 (Jul 2012). https://doi.org/10.3758/ https://doi.org/10.3758/s13428-012-0226-9, s13428-012-0226-9 2  34. Li, Z.: A saliency map in primary visual cortex. Trends in Cognitive Sciences 6(1), 9-16 (Jan 2002). https://doi.org/10.1016/s1364-6613(00)01817-9, https:// doi.org/10.1016/s1364-6613(00)01817-9 14  35. Nuthmann, A., Einh\u00a8auser, W., Sch\u00a8utz, I.: How well can saliency models pre- dict fixation selection in scenes beyond central bias? a new approach to   Saliency Benchmarking Made Easymodel evaluation using generalized linear mixed models. Front. Hum. Neurosci. 11 (Oct 2017). https://doi.org/10.3389/fnhum.2017.00491, https://doi.org/10. 3389/fnhum.2017.00491 2  36. Pan, J., Ferrer, C.C., McGuinness, K., O\u2019Connor, N.E., Torres, J., Sayrol, E., Giro-i Nieto, X.: SalGAN: Visual saliency prediction with generative adversarial networks. arXiv:1701.01081 [cs] (2017), http://arxiv.org/abs/1701.01081 2, 9 37. Peters, R.J., Iyer, A., Itti, L., Koch, C.: Components of bottom-up gaze allocation in natural images. Vision Research 45(18), 2397-2416 (Aug 2005). https://doi.org/10.1016/j.visres.2005.03.019, https://doi.org/10.1016/ j.visres.2005.03.019 2, 6  38. Riche, N.: Metrics for saliency model validation. In: From Human Atten- tion to Computational Attention, pp. 209-225. Springer New York (2016). https://doi.org/10.1007/ https://doi.org/10.1007/978-1-4939-3435-5 12, 978-1-4939-3435-5_12 2  39. Riche, N.: Saliency model evaluation. In: From Human Attention to Computational Attention, pp. 245-267. Springer New York (2016). https://doi.org/10.1007/978- 1-4939-3435-5 14, https://doi.org/10.1007/978-1-4939-3435-5_14 2  40. Riche, N., Duvinage, M., Mancas, M., Gosselin, B., Dutoit, T.: Saliency comparison metrics. and human fixations: State-of-the-art and study of IEEE (Dec In: 2013 IEEE International Conference on Computer Vision. 2013). https://doi.org/10.1109/iccv.2013.147, https://doi.org/10.1109/iccv. 2013.147 2  41. Rothkopf, C.A., Ballard, D.H., Hayhoe, M.M.: Task and context determine where you look. Journal of Vision 7(14), 16 (Jul 2016). https://doi.org/10.1167/7.14.16, https://doi.org/10.1167/7.14.16 2  42. Sch\u00a8utt, H.H., Rothkegel, L.O.M., Trukenbrod, H.A., Reich, S., Wichmann, F.A., Engbert, R.: Likelihood-based parameter estimation and comparison of dynamical cognitive models. Psychological Review 124(4), 505-524 (2017). https://doi.org/10.1037/rev0000068, https://doi.org/10.1037/rev0000068 2 43. Tatler, B.W., Hayhoe, M.M., Land, M.F., Ballard, D.H.: Eye guidance in nat- ural vision: Reinterpreting salience. Journal of Vision 11(5), 5-5 (May 2011). https://doi.org/10.1167/11.5.5, https://doi.org/10.1167/11.5.5 2  44. Tatler, B.W.: The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions. Journal of Vision 7(14), 4 (Nov 2007). https://doi.org/10.1167/7.14.4, https: //doi.org/10.1167/7.14.4 2  45. Tatler, B.W., Baddeley, R.J., Gilchrist, I.D.: Visual correlates of fixation se- lection: E\ufb00ects of scale and time. Vision Research 45(5), 643-659 (Mar 2005). https://doi.org/10.1016/j.visres.2004.09.017, https://doi.org/10.1016/ j.visres.2004.09.017 2, 5  46. Tatler, B.W., Vincent, B.T.: Systematic tendencies in scene viewing. Journal of Eye Movement Research 2(2), 1-18 (2008), http://csi.ufs.ac.za/resres/files/ tatler_2008_jemr.pdf 2  47. Thomas, C.: OpenSalicon: An open source implementation of the salicon saliency model. CoRR abs/1606.00110 (2016), http://arxiv.org/abs/1606.00110 9 48. Treisman, A.M., Gelade, G.: A feature-integration theory of attention. Cog- nitive Psychology 12(1), 97-136 (Jan 1980). https://doi.org/10.1016/0010- 0285(80)90005-5, https://doi.org/10.1016/0010-0285(80)90005-5 1, 14 49. Vig, E., Dorr, M., Cox, D.: Large-scale optimization of hierarchical  for  tures ence on Computer Vision and Pattern Recognition.  saliency prediction in natural  images.  fea- In: 2014 IEEE Confer- IEEE (Jun 2014).   18  M. K\u00a8ummerer, T.S.A. Wallis and M. Bethge  https://doi.org/10.1109/cvpr.2014.358, 358 2, 9  https://doi.org/10.1109/cvpr.2014.  50. Vincent, B.T., Baddeley, R., Correani, A., Troscianko, T., Leonards, U.: Do we look at lights? using mixture modelling to distinguish between low- and high- level factors in natural image viewing. Visual Cognition 17(6-7), 856-879 (Aug 2009). https://doi.org/10.1080/13506280902916691, https://doi.org/10.1080/ 13506280902916691 2  51. Wilming, N., Betz, T., Kietzmann, T.C., K\u00a8onig, P.: Measures and lim- its of models of fixation selection. PLoS ONE 6(9), e24038 (Sep 2011). https://doi.org/10.1371/ https://doi.org/10.1371/journal.pone.0024038, journal.pone.0024038 2  52. Xiao, J., Xu, P., Zhang, Y., Ehinger, K., Finkelstein, A., Kulkarni, S.: What can we learn from eye tracking data on 20,000 images? Journal of Vision 15(12), 790 (Sep 2015). https://doi.org/10.1167/15.12.790, https://doi.org/10.1167/15.12.790 2  53. Yu, F., Kontschieder, P., Song, S., Jiang, M., Zhang, Y., Zhao, C.Q., challenge.  understanding  scene  Funkhouser, T., Xiao, J.: Large-scale http://http://lsun.cs.princeton.edu/2017/ 2  54. Yu, F., Kontschieder, P., Song, S., Jiang, M., Zhang, Y., Zhao, C.Q., Funkhouser, T., Xiao, J.: SALICON saliency prediction challenge. http://salicon.net/challenge- 2017/ 2  55. Zhang, J., Sclaro\ufb00, S.: Saliency detection: A Boolean map approach.  In: 2013 IEEE International Conference on Computer Vision. IEEE (Dec 2013). https://doi.org/10.1109/iccv.2013.26, https://doi.org/10.1109/iccv.2013.26 2, 9  56. Zhang, L., Tong, M.H., Marks, T.K., Shan, H., Cottrell, G.W.: SUN: A Bayesian framework for saliency using natural statistics. Journal of Vision 8(7), 32 (Dec 2008). https://doi.org/10.1167/8.7.32, https://doi.org/10.1167/8.7.32 2"}