{"1": "1. Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R.: Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In: CVPR (2016)  2. Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.: SST: Single-stream  temporal action proposals. In: CVPR (2017)  3. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the  4. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object  Kinetics dataset. In: CVPR (2017)  interactions. In: WACV (2018)  5. Dai, X., Singh, B., Zhang, G., Davis, L.S., Chen, Y.Q.: Temporal context network  for activity localization in videos. In: ICCV (2017)  6. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning  by context prediction. In: ICCV (2015)  7. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)  8. Girdhar, R., Ramanan, D.: Attentional pooling for action recognition. In: NIPS  (2017)  9. Girshick, R.: Fast R-CNN. In: ICCV (2015)  10. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-  rate object detection and semantic segmentation. In: CVPR (2014)  11. Gkioxari, G., Malik, J.: Finding action tubes. In: CVPR (2015) 12. Gkioxari, G., Girshick, R., Doll\u00b4ar, P., He, K.: Detecting and recognizing human-  object intaractions. In: CVPR (2018)  13. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: AVA: A video dataset of spatio-temporally localized atomic visual actions. In: CVPR (2018)  14. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv:1505.04474 (2015) 15. Gupta, S., Hariharan, B., Malik, J.: Exploring person context and local scene  context for object detection. arXiv:1511.08177 (2015)  16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  (2008)  17. Heitz, G., Koller, D.: Learning spatial context: Using stu\ufb00 to find things. In: ECCV  18. Hou, R., Chen, C., Shah, M.: Tube convolutional neural network (T-CNN) for  action detection in videos. In: ICCV (2017)  19. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-o\ufb00s for modern convolutional object detectors. In: CVPR (2017)  20. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: FlowNet 2.0:  Evolution of optical \ufb02ow estimation with deep networks. In: CVPR (2017)  21. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer  22. Jhuang, H., Gall, J., Zu\ufb03, S., Schmid, C., Black, M.: Towards understanding action  networks. In: NIPS (2015)  recognition. In: ICCV (2013)  23. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.B.: CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In: CVPR (2017)   Actor-Centric Relation NetworkReferences  1. Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R.: Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In: CVPR (2016)  2. Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.: SST: Single-stream  temporal action proposals. In: CVPR (2017)  3. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the  4. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object  Kinetics dataset. In: CVPR (2017)  interactions. In: WACV (2018)  5. Dai, X., Singh, B., Zhang, G., Davis, L.S., Chen, Y.Q.: Temporal context network  for activity localization in videos. In: ICCV (2017)  6. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning  by context prediction. In: ICCV (2015)  7. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)  8. Girdhar, R., Ramanan, D.: Attentional pooling for action recognition. In: NIPS  (2017)  9. Girshick, R.: Fast R-CNN. In: ICCV (2015)  10. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-  rate object detection and semantic segmentation. In: CVPR (2014)  11. Gkioxari, G., Malik, J.: Finding action tubes. In: CVPR (2015) 12. Gkioxari, G., Girshick, R., Doll\u00b4ar, P., He, K.: Detecting and recognizing human-  object intaractions. In: CVPR (2018)  13. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: AVA: A video dataset of spatio-temporally localized atomic visual actions. In: CVPR (2018)  14. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv:1505.04474 (2015) 15. Gupta, S., Hariharan, B., Malik, J.: Exploring person context and local scene  context for object detection. arXiv:1511.08177 (2015)  16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  (2008)  17. Heitz, G., Koller, D.: Learning spatial context: Using stu\ufb00 to find things. In: ECCV  18. Hou, R., Chen, C., Shah, M.: Tube convolutional neural network (T-CNN) for  action detection in videos. In: ICCV (2017)  19. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-o\ufb00s for modern convolutional object detectors. In: CVPR (2017)  20. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: FlowNet 2.0:  Evolution of optical \ufb02ow estimation with deep networks. In: CVPR (2017)  21. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer  22. Jhuang, H., Gall, J., Zu\ufb03, S., Schmid, C., Black, M.: Towards understanding action  networks. In: NIPS (2015)  recognition. In: ICCV (2013)  23. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.B.: CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In: CVPR (2017)   16  C. Sun et al.  24. Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid, C.: Action tubelet detector  for spatio-temporal action localization. In: ICCV (2017)  25. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: CVPR (2014) 26. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The Kinetics human action video dataset. arXiv:1705.06950 (2017)  27. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-  volutional neural networks. In: NIPS (2012)  28. Liu, W., Anguelov, D., Erhan, D., Reed, S., Fu, C.Y., Berg, A.C.: SSD: Single shot  MultiBox detector. In: ECCV (2016)  29. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better.  arXiv:1506.04579 (2015)  ICLR (2017)  30. Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with restarts. In:  31. Lu, C., Krishna, R., Bernstein, M., Fei-Fei, L.: Visual relationship detection with  language priors. In: ECCV (2016)  32. Marszalek, M., Laptev, I., Schmid, C.: Actions in context. In: CVPR (2009) 33. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A.: The role of context for object detection and semantic segmentation in the wild. In: CVPR (2014)  34. Ng, J.Y., Hausknecht, M.J., Vijayanarasimhan, S., Vinyals, O., Monga, R., Toderici, G.: Beyond short snippets: Deep networks for video classification. In: CVPR (2015)  35. Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic representation  of the spatial envelope. IJCV (2001)  36. Pathak, D., Kr\u00a8ahenb\u00a8uhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-  coders: Feature learning by inpainting. In: CVPR (2016)  37. Peng, X., Schmid, C.: Multi-region two-stream R-CNN for action detection. In:  38. Peyre, J., Laptev, I., Schmid, C., Sivic, J.: Weakly-supervised learning of visual  39. Rabinovich, A., Vedaldi, A., Galleguillos, C., Wiewiora, E., Belongie, S.: Objects  ECCV (2016)  relations. In: ICCV (2017)  in context. In: ICCV (2007)  40. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object  detection with region proposal networks. In: NIPS (2015)  41. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. IJCV (2015)  42. Saha, S., G.Sing, Cuzzolin, F.: AMTnet: Action-micro-tube regression by end-to-  end trainable deep architecture. In: ICCV (2017)  43. Saha, S., Singh, G., Sapienza, M., Torr, P., Cuzzolin, F.: Deep learning for detecting  multiple space-time action tubes in videos. In: BMVC (2016)  44. Santoro, A., Raposo, D., Barrett, D.G.T., Malinowski, M., Pascanu, R., Battaglia, P., Lillicrap, T.P.: A simple neural network module for relational reasoning. In: NIPS (2017)  45. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention.  46. Shrivastava, A., Gupta, A.: Contextual priming and feedback for faster R-CNN.  arXiv:1511.04119 (2015)  In: ECCV (2016)   Actor-Centric Relation Network47. Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A.: Beyond skip connections:  Top-down modulation for object detection. arXiv:1612.06851 (2016)  48. Sigurdsson, G., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Hollywood in homes: Crowdsourcing data collection for activity understanding. In: ECCV (2016)  49. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Hol- lywood in homes: Crowdsourcing data collection for activity understanding. In: ECCV (2016)  50. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-  nition in videos. In: NIPS (2014)  51. Singh, G., Saha, S., Sapienza, M., Torr, P., Cuzzolin, F.: Online real-time multiple  spatiotemporal action localisation and prediction. In: ICCV (2017)  52. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR (2015) 53. Torralba, A., Murphy, K.P., Freeman, W.T., Rubin, M.A.: Context-based vision  system for place and object recognition. In: ICCV (2003)  54. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-  poral features with 3D convolutional networks. In: ICCV (2015)  55. Varol, G., Laptev, I., Schmid, C.: Long-term temporal convolutions for action  56. Wang, X., Girshick, R.B., Gupta, A., He, K.: Non-local neural networks. In: CVPR  recognition. IEEE PAMI (2017)  (2018)  action localization. In: ICCV (2015)  ization. arXiv:1605.05197 (2016)  57. Weinzaepfel, P., Harchaoui, Z., Schmid, C.: Learning to track for spatio-temporal  58. Weinzaepfel, P., Martin, X., Schmid, C.: Towards weakly-supervised action local-  59. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature  learning for video understanding. In: ECCV (2018)  60. Xu, H., Das, A., Saenko, K.: R-C3D: Region convolutional 3D network for temporal  activity detection. In: ICCV (2017)  61. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-  object interaction activities. In: CVPR (2010)  62. Zhao, H., Yan, Z., Wang, H., Torresani, L., Torralba, A.: SLAC: A sparsely labeled  dataset for action classification and localization. arXiv:1712.09374 (2017)  63. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features  for discriminative localization. In: CVPR (2016)  64. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features  for scene recognition using places database. In: NIPS (2014)  65. Zolfaghari, M., Oliveira, G., Sedaghat, N., Brox, T.: Chained multi-stream net- works exploiting pose, motion, and appearance for action classification and detec- tion. In: ICCV (2017)"}