{"1": "1. Marr, D., Hildreth, E.: Theory of edge detection. Proceedings of the Royal Society  of London B: Biological Sciences 207(1167) (1980) 187-217   14  Deng et al.  4.3 NYUDv2 dataset  The NYU depth dataset [14] is a large depth benchmark for indoor scenes, which is collected by a Microsoft Kinect sensor. It has a densely labeled dataset (every pixel has a depth annotation) which has 1449 pairs of aligned RGB and depth images. Gupta et al. [50] processed the data to generate edge annotation and split the dataset into 381 training images, 414 validation images, and 654 testing images. We follow their data-split setting and change several hyper-parameters of our method for training: mini-batch size (26), image resolution (480 \u00d7 480). The maximum tolerance allowed for correct matches of edge prediction in evaluation is increased from .0075 to .011, as used in [31, 13, 26]. We compare against the state-of-the-art methods which include OEF [49], gPb-UCM [5], gPb+NG [50], SE [26], SE+NG+ [51], HED [12] and RCF [13].  Motivated by the previous works [12, 13], we leverage the depth information to improve performance. We employ the HHA feature [51] in which the depth information is encoded into three channels: horizontal disparity, height above ground, and angle with gravity. The way of employing the HHA feature is s- traightforward. We simply train two versions of the proposed network, one on the RGB data, another on HHA feature images. The final prediction is generated by directly averaging the output of the RGB model and HHA model.  We show the quantitative results in Table 4 and the precision-recall curve in Figure 8. Our method achieves the best performance of ODS F-score .762. The qualitative results in Figure 7 show consistent performance with those of the experiments on BSDS 500. Our prediction produces sharper boundaries against the leading competitor RCF, which demonstrates the e\ufb00ectiveness of our method.  5 Conclusions  In this work, we have presented a simple yet e\ufb00ective method for edge detec- tion which achieves state-of-the-art results. We have shown that it is possible to achieve excellent boundary detection results using a carefully designed loss function and a simple convolutional encoder-decoder network.  In future work, we plan to extend the use of the edge detector to the tasks like object detection and optical \ufb02ow which have the requirement of boundary sharpness and a fast processing speed.  Acknowledgement  This work is funded by the China Scholarship Council (Grant No.201506370087), the National Natural Science Foundation of China (Grant No.61572527, Grant No.61628211, Grant No.61602524).  References  1. Marr, D., Hildreth, E.: Theory of edge detection. Proceedings of the Royal Society  of London B: Biological Sciences 207(1167) (1980) 187-217   Learning to predict crisp boundaries2. Gonzalez, R.C., Wood, R.E.: Digital image processing, 2nd edtn 3. Torre, V., Poggio, T.A.: On edge detection. IEEE Transactions on Pattern Analysis  and Machine Intelligence (2) (1986) 147-163  4. Senthilkumaran, N., Rajesh, R.:  image segmentation-a survey of soft computing approaches. International journal of re- cent trends in engineering 1(2) (2009) 250-254  Edge detection techniques  for  5. Arbelaez, P., Maire, M., Fowlkes, C., Malik, J.: Contour detection and hierar- chical image segmentation. IEEE transactions on pattern analysis and machine intelligence 33(5) (2011) 898-916  6. Chen, L.C., Barron, J.T., Papandreou, G., Murphy, K., Yuille, A.L.: Semantic image segmentation with task-specific edge detection using cnns and a discrimi- natively trained domain transform. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016) 4545-4554  7. Bertasius, G., Shi, J., Torresani, L.: Semantic segmentation with boundary neural fields. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016) 3602-3610  8. Ren, X.: Local grouping for optical \ufb02ow. In: Computer Vision and Pattern Recog-  nition, 2008. CVPR 2008. IEEE Conference on, IEEE (2008) 1-8  9. Revaud, J., Weinzaepfel, P., Harchaoui, Z., Schmid, C.: Epic\ufb02ow: Edge-preserving In: Proceedings of the IEEE  interpolation of correspondences for optical \ufb02ow. Conference on Computer Vision and Pattern Recognition. (2015) 1164-1172 10. Bertasius, G., Shi, J., Torresani, L.: Deepedge: A multi-scale bifurcated deep network for top-down contour detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 4380-4389  11. Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z.: Deepcontour: A deep convolu- tional feature learned by positive-sharing loss for contour detection. In: Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 3982-3991  12. Xie, S., Tu, Z.: Holistically-nested edge detection. In: Proceedings of the IEEE  international conference on computer vision. (2015) 1395-1403  13. Liu, Y., Cheng, M.M., Hu, X., Wang, K., Bai, X.: Richer convolutional features  for edge detection. arXiv preprint arXiv:1612.02103 (2016)  14. Nathan Silberman, Derek Hoiem, P.K., Fergus, R.: Indoor segmentation and sup-  port inference from rgbd images. In: ECCV. (2012)  15. Sobel, I.: Camera models and machine perception. Technical report, Stanford Univ  Calif Dept of Computer Science (1970)  16. Yu, Z., Feng, C., Liu, M.Y., Ramalingam, S.: Casenet: Deep category-aware se-  mantic edge detection. ArXiv e-prints (2017)  17. Yang, J., Price, B., Cohen, S., Lee, H., Yang, M.H.: Object contour detection with a fully convolutional encoder-decoder network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016) 193-202  18. Kittler, J.: On the accuracy of the sobel edge detector. Image and Vision Com-  puting 1(1) (1983) 37-42  19. Canny, J.: A computational approach to edge detection. IEEE Transactions on  pattern analysis and machine intelligence (6) (1986) 679-698  20. Fram, J.R., Deutsch, E.S.: On the quantitative evaluation of edge detection schemes and their comparison with human performance. Computers IEEE Trans- actions on C-24(6) (1975) 616-628  21. Perona, P., Malik, J.: Scale-space and edge detection using anisotropic di\ufb00usion. IEEE Transactions on pattern analysis and machine intelligence 12(7) (1990) 629- 639   16  Deng et al.  22. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-  tional Journal of Computer Vision 60(2) (2004) 91-110  23. Siddiqui, M., Medioni, G.: Human pose estimation from a single view point, real-time range sensor. In: Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, IEEE (2010) 1-8  24. Martin, D.R., Fowlkes, C.C., Malik, J.: Learning to detect natural image bound- aries using local brightness, color, and texture cues. IEEE transactions on pattern analysis and machine intelligence 26(5) (2004) 530-549  25. Dollar, P., Tu, Z., Belongie, S.: Supervised learning of edges and object bound- aries. In: Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. Volume 2., IEEE (2006) 1964-1971  26. Doll\u00b4ar, P., Zitnick, C.L.: Fast edge detection using structured forests. IEEE trans- actions on pattern analysis and machine intelligence 37(8) (2015) 1558-1570 27. Kokkinos, I.: Pushing the boundaries of boundary detection using deep learning.  arXiv preprint arXiv:1511.07386 (2015)  28. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in neural information processing systems. (2012) 1097-1105  29. LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E., Jackel, L.D.: Handwritten digit recognition with a back-propagation net- work. In: Advances in neural information processing systems. (1990) 396-404 30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint arXiv:1409.1556 (2014)  31. Xie, S., Tu, Z.: Holistically-nested edge detection. International Journal of Com-  puter Vision 125(1-3) (2017) 3-18  32. Gu, J., Zhou, Y., Zuo, X.: Making class bias useful: A strategy of learning from imbalanced data. Intelligent Data Engineering and Automated Learning-IDEAL 2007 (2007) 287-295  33. Tang, L., Liu, H.: Bias analysis in text classification for highly skewed data. In:  Data Mining, Fifth IEEE International Conference on, IEEE (2005) 4-pp  34. Lusa, L., et al.: Class prediction for high-dimensional class-imbalanced data. BMC  bioinformatics 11(1) (2010) 523  35. Haider, A.H., Schneider, E.B., Sriram, N., Dossick, D.S., Scott, V.K., Swoboda, S.M., Losonczy, L., Haut, E.R., Efron, D.T., Pronovost, P.J., et al.: Unconscious race and class bias: its association with decision making by trauma and acute care surgeons. Journal of Trauma and Acute Care Surgery 77(3) (2014) 409-416 36. Phillips, S.J., Dud\u00b4\u0131k, M.: Generative and discriminative learning with unknown labeling bias. In: Advances in Neural information Processing Systems. (2009) 401- 408  37. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks In: 3D Vision (3DV), 2016 Fourth  for volumetric medical image segmentation. International Conference on, IEEE (2016) 565-571  38. Dice, L.R.: Measures of the amount of ecologic association between species. Ecology  26(3) (1945) 297-302  39. Pinheiro, P.O., Lin, T.Y., Collobert, R., Doll\u00b4ar, P.: Learning to refine object  segments. In: European Conference on Computer Vision, Springer (2016) 75-91  40. Xie, S., Girshick, R., Dollr, P., Tu, Z., He, K.: Aggregated residual transformations  for deep neural networks. (2016)  41. Lin, G., Milan, A., Shen, C., Reid, I.: Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (2017)   Learning to predict crisp boundaries42. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 770-778  43. Adam Paszke, Sam Gross, S.C., Chanan, G.: Pytorch (2017) 44. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR ab-  s/1412.6980 (2014)  45. Wang, Y., Zhao, X., Huang, K.: Deep crisp boundaries. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2017) 3892-3900 46. Isola, P., Zoran, D., Krishnan, D., Adelson, E.H.: Crisp boundary detection us- ing pointwise mutual information. In: European Conference on Computer Vision, Springer (2014) 799-814  47. Bertasius, G., Shi, J., Torresani, L.: High-for-low and low-for-high: E\ufb03cient bound- ary detection from deep object features and its applications to high-level vision. In: Proceedings of the IEEE International Conference on Computer Vision. (2015) 504-512  48. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A.: The role of context for object detection and semantic segmentation in the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2014) 891-898  49. Hallman, S., Fowlkes, C.C.: Oriented edge forests for boundary detection.  In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 1732-1740  50. Gupta, S., Arbelaez, P., Malik, J.: Perceptual organization and recognition of indoor scenes from rgb-d images. In: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, IEEE (2013) 564-571  51. Gupta, S., Girshick, R., Arbel\u00b4aez, P., Malik, J.: Learning rich features from rgb- In: European Conference on  d images for object detection and segmentation. Computer Vision, Springer (2014) 345-360"}