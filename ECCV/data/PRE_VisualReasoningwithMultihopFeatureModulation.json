{"1": "1. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: Proc. of ICCV (2015)  2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. Deep Learning Sympo-  sium (NIPS) (2016)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: Proc. of ICLR (2015)  4. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-  rent neural networks on sequence modeling. In: Proc. of ICML (2015)  5. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D.,  Batra, D.: Visual dialog. In: Proc. of CVPR (2017)  6. De Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., Courville, A.: Guesswhat?! visual object discovery through multi-modal dialogue. In: Proc. of CVPR (2017)  7. Delbrouck, J.B., Dupont, S.: Modulating and attending the source image dur- ing encoding improves multimodal translation. Visually-Grounded Interaction and Language Workshop (NIPS) (2017)  8. Dumoulin, V., Shlens, J., Kudlur, M.: A Learned Representation For Artistic Style.  In: Proc. of ICLR (2017)  9. Dumoulin, V., Perez, E.,  Schucher, N.,  Courville, A., Bengio, Y.: Feature-wise https://doi.org/10.23915/distill.00011, transformations  Strub,  F., Vries, H.d., (2018). https://distill.pub/2018/feature-wise-  transformations. Distill  10. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision 88(2), 303-338 (2010)  11. Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multi- modal compact bilinear pooling for visual question answering and visual grounding. In: Proc. of EMNLP (2016)  12. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu- rate object detection and semantic segmentation. In: Proc. of of CVPR (2014) 13. Graves, A., Wayne, G., Danihelka, I.: Neural turing machines. arXiv preprint  arXiv:1410.5401 (2014)  14. Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska- Barwi\u00b4nska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J., et al.: Hybrid computing using a neural network with dynamic external memory. Nature 538(7626), 471 (2016)  15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  16. Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expres-  In: Proc. of CVPR (2016)  sions. In: Proc. of ECCV (2016)  17. Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural language  object retrieval. In: Proc. of CVPR (2016)  18. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine rea-  soning. In: Proc. of ICL (2018)  19. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In: Proc. of ICML (2015)  20. Jabri, A., Joulin, A., van der Maaten, L.: Revisiting visual question answering  baselines. In: Proc. of ECCV (2016)   Visual Reasoning with Multi-hop Feature ModulationReferences  1. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: Proc. of ICCV (2015)  2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. Deep Learning Sympo-  sium (NIPS) (2016)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: Proc. of ICLR (2015)  4. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-  rent neural networks on sequence modeling. In: Proc. of ICML (2015)  5. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D.,  Batra, D.: Visual dialog. In: Proc. of CVPR (2017)  6. De Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., Courville, A.: Guesswhat?! visual object discovery through multi-modal dialogue. In: Proc. of CVPR (2017)  7. Delbrouck, J.B., Dupont, S.: Modulating and attending the source image dur- ing encoding improves multimodal translation. Visually-Grounded Interaction and Language Workshop (NIPS) (2017)  8. Dumoulin, V., Shlens, J., Kudlur, M.: A Learned Representation For Artistic Style.  In: Proc. of ICLR (2017)  9. Dumoulin, V., Perez, E.,  Schucher, N.,  Courville, A., Bengio, Y.: Feature-wise https://doi.org/10.23915/distill.00011, transformations  Strub,  F., Vries, H.d., (2018). https://distill.pub/2018/feature-wise-  transformations. Distill  10. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal of computer vision 88(2), 303-338 (2010)  11. Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multi- modal compact bilinear pooling for visual question answering and visual grounding. In: Proc. of EMNLP (2016)  12. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu- rate object detection and semantic segmentation. In: Proc. of of CVPR (2014) 13. Graves, A., Wayne, G., Danihelka, I.: Neural turing machines. arXiv preprint  arXiv:1410.5401 (2014)  14. Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska- Barwi\u00b4nska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J., et al.: Hybrid computing using a neural network with dynamic external memory. Nature 538(7626), 471 (2016)  15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  16. Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expres-  In: Proc. of CVPR (2016)  sions. In: Proc. of ECCV (2016)  17. Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural language  object retrieval. In: Proc. of CVPR (2016)  18. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine rea-  soning. In: Proc. of ICL (2018)  19. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In: Proc. of ICML (2015)  20. Jabri, A., Joulin, A., van der Maaten, L.: Revisiting visual question answering  baselines. In: Proc. of ECCV (2016)   16  F. Strub and M. Seurin and E. Perez and H. Vries et al.  21. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: Proc. of CVPR (2017)  22. Ka\ufb02e, K., Kanan, C.: Visual question answering: Datasets, algorithms, and future  challenges. Computer Vision and Image Understanding 163, 3-20 (2017)  23. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to  objects in photographs of natural scenes. In: Proc. of EMNLP (2014)  24. Kim, J.H., On, K.W., Lim, W., Kim, J., Ha, J.W., Zhang, B.T.: Hadamard Product  for Low-rank Bilinear Pooling. In: Proc. of ICLR (2017)  25. Kim, J.H., Lee, S.W., Kwak, D., Heo, M.O., Kim, J., Ha, J.W., Zhang, B.T.:  Multimodal residual learning for visual qa. In: Proc. of NIPS (2016)  26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Proc. of  ICLR (2014)  27. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-  volutional neural networks. In: Proc. of of NIPS (2012)  28. Lee, S.W., Heo, Y.J., Zhang, B.T.: Answerer in questioner\u2019s mind for goal-oriented visual dialogue. Visually-Grounded Interaction and Language Workshop (NIPS) (2018)  29. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proc. of ECCV (2014)  30. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic  segmentation. In: Proc. of CVPR (2015)  31. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention  for visual question answering. In: Proc. of NIPS (2016)  32. Luo, R., Shakhnarovich, G.: Comprehension-guided referring expressions. In: Proc.  of CVPR (2017)  33. Luong, M.T., Pham, H., Manning, C.D.: E\ufb00ective approaches to attention-based  neural machine translation. In: Proc. of EMNLP (2015)  34. Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A neural-based ap-  proach to answering questions about images. In: Proc. of ICCV (2015)  35. Mller, H., Clough, P., Deselaers, T., Caputo, B.: ImageCLEF: Experimental Eval-  uation in Visual Information Retrieval. Springer (2012)  36. Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for  referring expression understanding. In: Proc. of ECCV (2016)  37. Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann ma-  chines. In: Proc. of ICML (2010)  38. Perez, E., Strub, F., De Vries, H., Dumoulin, V., Courville, A.: Film: Visual rea-  soning with a general conditioning layer. In: Proc. of AAAI (2018)  39. Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of textual  phrases in images by reconstruction. In: Proc. of ECCV (2016)  40. Rupprecht, C., Laina, I., Navab, N., Hager, G.D., Tombari, F.: Guide me: Inter-  acting with deep networks. In: Proc. of CVPR (2018)  41. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. International Journal of Computer Vision 115(3), 211-252 (2015) 42. Strub, F., De Vries, H., Mary, J., Piot, B., Courville, A., Pietquin, O.: End-to-end optimization of goal-driven and visually grounded dialogue systems harm de vries. In: Proc. of IJCAI (2017)  43. Sukhbaatar, S., Weston, J., Fergus, R., et al.: End-to-end memory networks. In:  Proc. of NIPS (2015)   Visual Reasoning with Multi-hop Feature Modulation44. de Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., Courville, A.C.:  Modulating early visual processing by language. In: Proc. of NIPS (2017)  45. Weston, J., Chopra, S., Bordes, A.: Memory networks. arXiv preprint  arXiv:1410.3916 (2014)  46. Xiong, C., Merity, S., Socher, R.: Dynamic memory networks for visual and textual  question answering. In: Proc. of ICML (2016)  47. Xu, H., Saenko, K.: Ask, attend and answer: Exploring question-guided spatial  attention for visual question answering. In: Proc. of ECCV (2016)  48. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: Proc. of ICML (2015)  49. Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: E\ufb03cient video object  segmentation via network modulation. In: Proc. of CVPR (2018)  50. Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.: Mattnet: Mod- ular attention network for referring expression comprehension. In: Proc. of CVPR (2018)  51. Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring  expressions. In: Proc. of ECCV (2016)  52. Yu, L., Tan, H., Bansal, M., Berg, T.L.: A joint speakerlistener-reinforcer model  for referring expressions. In: Proc. of CVPR (2016)  53. Zhu, Y., Zhang, S., Metaxas, D.: Reasoning about fine-grained attribute phrases using reference games. In: Visually-Grounded Interaction and Language Workshop (NIPS) (2017)  54. Zhuang, B., Wu, Q., Shen, C., Reid, I.D., van den Hengel, A.: Parallel attention: A unified framework for visual object discovery through dialogs and queries. Proc. of CVPR (2018)"}