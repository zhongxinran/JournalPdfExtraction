{"1": "1. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,  Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)  2. Radford, A., Metz, L., Chintala, S.:  ing with deep convolutional generative adversarial networks. arXiv:1511.06434 (2015)  Unsupervised representation learn- arXiv preprint  3. Berthelot, D., Schumm, T., Metz, L.: BEGAN: Boundary equilibrium generative  adversarial networks. arXiv preprint arXiv:1703.10717 (2017)  4. Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Smolley, S.P.: Least squares gener-  ative adversarial networks. arXiv preprint ArXiv:1611.04076 (2016)  5. Zhao, J., Mathieu, M., LeCun, Y.: Energy-based generative adversarial network.  arXiv preprint arXiv:1609.03126 (2016)  6. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.  In: ICML. (2017)  7. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved  training of wasserstein gans. In: NIPS. (2017) 5767-5777  8. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for  generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)  9. Wei, X., Gong, B., Liu, Z., Lu, W., Wang, L.: Improving the improved training of  wasserstein gans: A consistency term and its dual e\ufb00ect. In: ICLR. (2018)  10. Roth, K., Lucchi, A., Nowozin, S., Hofmann, T.: Stabilizing training of generative  adversarial networks through regularization. In: NIPS. (2017) 2015-2025  11. Nowozin, S., Cseke, B., Tomioka, R.:  f-gan: Training generative neural samplers  using variational divergence minimization. In: NIPS. (2016) 271-279  12. Zhang, H., Xu, T., Li, H., Zhang, S., Huang, X., Wang, X., Metaxas, D.: Stack- gan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV. (2017)  13. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative  adversarial networks. arXiv preprint arXiv:1612.04357 (2016)  14. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im- proved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017) 15. Arjovsky, M., Bottou, L.: Towards principled methods for training generative  adversarial networks. In: NIPS Workshop. Volume 2016. (2017)  16. Villani, C.: Optimal transport: old and new. Volume 338. Springer Science &  Business Media (2008) 17. Karakida, R., Amari, S.i.:  In: International Conference on Geometric Science of Information, Springer (2017) 119-126  Information geometry of wasserstein divergence.  18. Rothe, R., Timofte, R., Gool, L.V.: Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision (IJCV) (July 2016)  19. Evans, L.C.: Partial di\ufb00erential equations and monge-kantorovich mass transfer.  Current developments in mathematics 1997(1) (1997) 65-126  20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  21. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint  22. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:  In: CVPR. (2016) 770-778  arXiv:1412.6980 (2014)  ICCV. (2015)   Wasserstein Divergence for GANsReferences  1. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,  Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)  2. Radford, A., Metz, L., Chintala, S.:  ing with deep convolutional generative adversarial networks. arXiv:1511.06434 (2015)  Unsupervised representation learn- arXiv preprint  3. Berthelot, D., Schumm, T., Metz, L.: BEGAN: Boundary equilibrium generative  adversarial networks. arXiv preprint arXiv:1703.10717 (2017)  4. Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Smolley, S.P.: Least squares gener-  ative adversarial networks. arXiv preprint ArXiv:1611.04076 (2016)  5. Zhao, J., Mathieu, M., LeCun, Y.: Energy-based generative adversarial network.  arXiv preprint arXiv:1609.03126 (2016)  6. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.  In: ICML. (2017)  7. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved  training of wasserstein gans. In: NIPS. (2017) 5767-5777  8. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for  generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)  9. Wei, X., Gong, B., Liu, Z., Lu, W., Wang, L.: Improving the improved training of  wasserstein gans: A consistency term and its dual e\ufb00ect. In: ICLR. (2018)  10. Roth, K., Lucchi, A., Nowozin, S., Hofmann, T.: Stabilizing training of generative  adversarial networks through regularization. In: NIPS. (2017) 2015-2025  11. Nowozin, S., Cseke, B., Tomioka, R.:  f-gan: Training generative neural samplers  using variational divergence minimization. In: NIPS. (2016) 271-279  12. Zhang, H., Xu, T., Li, H., Zhang, S., Huang, X., Wang, X., Metaxas, D.: Stack- gan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV. (2017)  13. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative  adversarial networks. arXiv preprint arXiv:1612.04357 (2016)  14. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im- proved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017) 15. Arjovsky, M., Bottou, L.: Towards principled methods for training generative  adversarial networks. In: NIPS Workshop. Volume 2016. (2017)  16. Villani, C.: Optimal transport: old and new. Volume 338. Springer Science &  Business Media (2008) 17. Karakida, R., Amari, S.i.:  In: International Conference on Geometric Science of Information, Springer (2017) 119-126  Information geometry of wasserstein divergence.  18. Rothe, R., Timofte, R., Gool, L.V.: Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision (IJCV) (July 2016)  19. Evans, L.C.: Partial di\ufb00erential equations and monge-kantorovich mass transfer.  Current developments in mathematics 1997(1) (1997) 65-126  20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  21. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint  22. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:  In: CVPR. (2016) 770-778  arXiv:1412.6980 (2014)  ICCV. (2015)   16  Wu et al.  23. Yu, F., Se\ufb00, A., Zhang, Y., Song, S., Funkhouser, T., Xiao, J.: Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 (2015)  24. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local nash equilibrium. In: NIPS. (2017) 6629-6640  25. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:  Improved techniques for training gans. In: NIPS. (2016) 2234-2242  26. Lucic, M., Kurach, K., Michalski, M., Gelly, S., Bousquet, O.: Are GANs created  equal? a large-scale study. arXiv preprint arXiv:1711.10337 (2017)  27. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by  reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)"}