{"1": "1. Alishahi, A., Barking, M., Chrupala, G.: Encoding of phonology in a recurrent neural model  of grounded speech. In: CoNLL (2017)  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence, Z., Parikh, D.: VQA: Visual question answering. In: Proc. IEEE International Conference on Computer Vision (ICCV) (2015)  3. Arandjelovic, R., Zisserman, A.: Look, listen, and learn. In: ICCV (2017) 4. Aytar, Y., Vondrick, C., Torralba, A.: Soundnet: Learning sound representations from unla- beled video. In: Advances in Neural Information Processing Systems 29, pp. 892-900 (2016) 5. Bergamo, A., Bazzani, L., Anguelov, D., Torresani, L.: Self-taught object localization with  deep networks. CoRR abs/1409.3964 (2014), http://arxiv.org/abs/1409.3964  6. Bromley, J., Guyon, I., LeCun, Y., S\u00a8ackinger, E., Shah, R.: Signature verification using a \u201dsiamese\u201d time delay neural network. In: Cowan, J.D., Tesauro, G., Alspector, J. (eds.) Ad- vances in Neural Information Processing Systems 6, pp. 737-744. Morgan-Kaufmann (1994) 7. Cho, M., Kwak, S., Schmid, C., Ponce, J.: Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)  8. Chrupala, G., Gelderloos, L., Alishahi, A.: Representations of language in a model of visu-  ally grounded speech signal. In: ACL (2017)  9. Cinbis, R., Verbeek, J., Schmid, C.: Weakly supervised object localization with multi-fold multiple instance learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 39(1), 189-203 (2016)  10. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by context  prediction. CoRR abs/1505.05192 (2015), http://arxiv.org/abs/1505.05192  11. Drexler, J., Glass, J.: Analysis of audio-visual features for unsupervised speech recognition.  In: Grounded Language Understanding Workshop (2017)  12. Dupoux, E.: Cognitive science in the era of artificial intelligence: A roadmap for reverse-  engineering the infant language-learner. In: Cognition (2018)  13. Fang, H., Gupta, S., Iandola, F., Rupesh, S., Deng, L., Dollar, P., Gao, J., He, X., Mitchell, M., C., P.J., Zitnick, C.L., Zweig, G.: From captions to visual concepts and back. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)  14. Gao, H., Mao, J., Zhou, J., Huang, Z., Yuille, A.: Are you talking to a machine? dataset and  methods for multilingual image question answering. In: NIPS (2015)  15. Gelderloos, L., Chrupaa, G.: From phonemes to images: levels of representation in a recur- rent neural model of visually-grounded language learning. In: arXiv:1610.03342 (2016) 16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2013)  17. Gu\u00b4erin, J., Gibaru, O., Thiery, S., Nyiri, E.: CNN features are also great at unsupervised  classification. CoRR abs/1707.01700 (2017), http://arxiv.org/abs/1707.01700  18. Harwath, D., Glass, J.: Learning wor d-like units from joint audio-visual analysis. In: Proc.  Annual Meeting of the Association for Computational Linguistics (ACL) (2017)  19. Harwath, D., Torralba, A., Glass, J.R.: Unsupervised learning of spoken language with visual  context. In: Proc. Neural Information Processing Systems (NIPS) (2016)  20. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing  internal covariate shift. In: Journal of Machine Learning Research (JMLR) (2015)  21. Jansen, A., Church, K., Hermansky, H.: Toward spoken term discovery at scale with zero resources. In: Proc. Annual Conference of International Speech Communication Association (INTERSPEECH) (2010)   Jointly Discovering Visual Objects and Spoken Words from Raw Sensory InputReferences  1. Alishahi, A., Barking, M., Chrupala, G.: Encoding of phonology in a recurrent neural model  of grounded speech. In: CoNLL (2017)  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence, Z., Parikh, D.: VQA: Visual question answering. In: Proc. IEEE International Conference on Computer Vision (ICCV) (2015)  3. Arandjelovic, R., Zisserman, A.: Look, listen, and learn. In: ICCV (2017) 4. Aytar, Y., Vondrick, C., Torralba, A.: Soundnet: Learning sound representations from unla- beled video. In: Advances in Neural Information Processing Systems 29, pp. 892-900 (2016) 5. Bergamo, A., Bazzani, L., Anguelov, D., Torresani, L.: Self-taught object localization with  deep networks. CoRR abs/1409.3964 (2014), http://arxiv.org/abs/1409.3964  6. Bromley, J., Guyon, I., LeCun, Y., S\u00a8ackinger, E., Shah, R.: Signature verification using a \u201dsiamese\u201d time delay neural network. In: Cowan, J.D., Tesauro, G., Alspector, J. (eds.) Ad- vances in Neural Information Processing Systems 6, pp. 737-744. Morgan-Kaufmann (1994) 7. Cho, M., Kwak, S., Schmid, C., Ponce, J.: Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)  8. Chrupala, G., Gelderloos, L., Alishahi, A.: Representations of language in a model of visu-  ally grounded speech signal. In: ACL (2017)  9. Cinbis, R., Verbeek, J., Schmid, C.: Weakly supervised object localization with multi-fold multiple instance learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 39(1), 189-203 (2016)  10. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by context  prediction. CoRR abs/1505.05192 (2015), http://arxiv.org/abs/1505.05192  11. Drexler, J., Glass, J.: Analysis of audio-visual features for unsupervised speech recognition.  In: Grounded Language Understanding Workshop (2017)  12. Dupoux, E.: Cognitive science in the era of artificial intelligence: A roadmap for reverse-  engineering the infant language-learner. In: Cognition (2018)  13. Fang, H., Gupta, S., Iandola, F., Rupesh, S., Deng, L., Dollar, P., Gao, J., He, X., Mitchell, M., C., P.J., Zitnick, C.L., Zweig, G.: From captions to visual concepts and back. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)  14. Gao, H., Mao, J., Zhou, J., Huang, Z., Yuille, A.: Are you talking to a machine? dataset and  methods for multilingual image question answering. In: NIPS (2015)  15. Gelderloos, L., Chrupaa, G.: From phonemes to images: levels of representation in a recur- rent neural model of visually-grounded language learning. In: arXiv:1610.03342 (2016) 16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2013)  17. Gu\u00b4erin, J., Gibaru, O., Thiery, S., Nyiri, E.: CNN features are also great at unsupervised  classification. CoRR abs/1707.01700 (2017), http://arxiv.org/abs/1707.01700  18. Harwath, D., Glass, J.: Learning wor d-like units from joint audio-visual analysis. In: Proc.  Annual Meeting of the Association for Computational Linguistics (ACL) (2017)  19. Harwath, D., Torralba, A., Glass, J.R.: Unsupervised learning of spoken language with visual  context. In: Proc. Neural Information Processing Systems (NIPS) (2016)  20. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing  internal covariate shift. In: Journal of Machine Learning Research (JMLR) (2015)  21. Jansen, A., Church, K., Hermansky, H.: Toward spoken term discovery at scale with zero resources. In: Proc. Annual Conference of International Speech Communication Association (INTERSPEECH) (2010)   16  D. Harwath et al  22. Jansen, A., Van Durme, B.: Efficient spoken term discovery using randomized algorithms. In: Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding (ASRU) (2011)  23. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization networks for dense captioning. In: Proc. IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) (2016)  24. Kamper, H., Elsner, M., Jansen, A., Goldwater, S.: Unsupervised neural network based feature extraction using weak top-down constraints. In: Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2015)  25. Kamper, H., Jansen, A., Goldwater, S.: Unsupervised word segmentation and lexicon discov- ery using acoustic word embeddings. IEEE Transactions on Audio, Speech and Language Processing 24(4), 669-679 (Apr 2016)  26. Kamper, H., Settle, S., Shakhnarovich, G., Livescu, K.: Visually grounded learning of key-  word prediction from untranscribed speech. In: INTERSPEECH (2017)  27. Karpathy, A., Joulin, A., Fei-Fei, L.: Deep fragment embeddings for bidirectional image  sentence mapping. In: Proc. Neural Information Processing Systems (NIPS) (2014)  28. Karpathy, A., Li, F.F.: Deep visual-semantic alignments for generating image descriptions. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015) 29. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document  recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)  30. Lee, C., Glass, J.: A nonparametric Bayesian approach to acoustic model discovery. In: Proc.  Annual Meeting of the Association for Computational Linguistics (ACL) (2012)  31. Lewis, M.P., Simon, G.F., Fennig, C.D.: Ethnologue: Languages of the World, Nineteenth  edition. SIL International. Online version: http://www.ethnologue.com (2016)  32. Lin, T., Marie, M., Belongie, S., Bourdev, L., Girshick, R., Perona, P., Ramanan, D., Zitnick, C.L., Dollar, P.: Microsoft COCO: Common objects in context. In: arXiv:1405.0312 (2015) 33. Malinowski, M., Fritz, M.: A multi-world approach to question answering about real-world  scenes based on uncertain input. In: NIPS (2014)  34. Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A neural-based approach to  answering questions about images. In: ICCV (2015)  35. Ondel, L., Burget, L., Cernocky, J.: Variational inference for acoustic unit discovery. In: 5th  Workshop on Spoken Language Technology for Under-resourced Language (2016)  36. Owens, A., Isola, P., McDermott, J.H., Torralba, A., Adelson, E.H., Freeman, W.T.: Visually indicated sounds. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 2405-2413 (2016)  37. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient Sound Provides  Supervision for Visual Learning, pp. 801-816 (2016)  38. Park, A., Glass, J.: Unsupervised pattern discovery in speech. IEEE Transactions on Audio,  Speech and Language Processing 16(1), 186-197 (2008)  39. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)  40. Reed, S.E., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative adversarial text to image synthesis. CoRR abs/1605.05396 (2016), http://arxiv.org/abs/1605.05396 41. Ren, M., Kiros, R., Zemel, R.: Exploring models and data for image question answering. In:  NIPS (2015)  42. Renshaw, D., Kamper, H., Jansen, A., Goldwater, S.: A comparison of neural network meth- ods for unsupervised representation learning on the zero resource speech challenge. In: Proc. Annual Conference of International Speech Communication Association (INTERSPEECH) (2015)   Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input43. Roy, D.: Grounded spoken language acquisition: Experiments in word learning. IEEE Trans-  actions on Multimedia 5(2), 197-209 (2003)  44. Roy, D., Pentland, A.: Learning words from sights and sounds: a computational model. Cog-  nitive Science 26, 113-146 (2002)  45. Russell, B., Efros, A., Sivic, J., Freeman, W., Zisserman, A.: Using multiple segmentations to discover objects and their extent in image collections. In: Proc. IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) (2006)  46. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-  nition. CoRR abs/1409.1556 (2014)  47. Spelke, E.S.: Principles- http://www.sciencedirect.com/science/article/pii/036402139090025R  (1990).  object  of 29 perception. Cognitive Science https://doi.org/https://doi.org/10.1016/0364-0213(90)90025-R,  14(1),  48. Thiolliere, R., Dunbar, E., Synnaeve, G., Versteegh, M., Dupoux, E.: A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling. In: Proc. Annual Conference of International Speech Communication Association (INTERSPEECH) (2015)  49. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption gen- erator. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)  50. de Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., Courville, A.C.: Guess- what?! visual object discovery through multi-modal dialogue. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)  51. Weber, M., Welling, M., Perona, P.: Towards automatic discovery of object categories. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2010) 52. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML (2015)  53. Zhang, T., Ramakrishnan, R., Livny, M.: Birch: an efficient data clustering method for very large databases. In: ACM SIGMOD international conference on Management of data. pp. 103-114 (1996)  54. Zhang, Y., Salakhutdinov, R., Chang, H.A., Glass, J.: Resource configurable spoken query detection using deep boltzmann machines. In: Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2012)  55. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors emerge in deep  scene CNNs. arXiv preprint arXiv:1412.6856 (2014)  56. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors emerge in deep scene CNNs. In: Proc. International Conference on Learning Representations (ICLR) (2015) 57. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for dis- criminative localization. In: Proc. IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR) (2016)  58. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Proc. Neural Information Processing Systems (NIPS) (2014)  59. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ADE20K dataset. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)"}