{"1": "1. LeCun, Y., Bottou, L., Bengio, Y., Ha\ufb00ner, P.: Gradient-based learning applied to  document recognition. Proceedings of the IEEE 86(11) (1998) 2278-2324  2. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in neural information processing systems. (2012) 1097-1105   14  Seung Hyun Lee, Dae Ha Kim, Byung Cheol Song  Table 4: Performance comparison according to the number of DFVs.  VGG  Model  The number of DFVs 8 14  -VGG  S-DNN w/ pool 61.37 65.54 66.33 66.17 65.38 65.15 S-DNN w/ stride 54.17 61.28 61.54 61.63 61.82 62.00  becomes too large, the accuracy rises and drops again. This is because the distil- lation of too much amount of knowledge may cause transfer of even unnecessary information as mentioned in Section 3. However, S-DNN with stride shows a slight increase in performance. This is because the performance of the S-DNN is relatively low compared to that of the T-DNN, so receiving additional knowl- edge will significantly improve performance. Therefore, a reasonable number of DFVs should be used depending on the available cost, and the number of DFVs required can be determined according to the structure of the network.  5 Conclusion and Future Work  We propose a novel knowledge distillation method in this paper. The existing knowledge transfer technique 1) was limited to a limited network structure, 2) the quality of knowledge was low, and 3) as the learning progresses, the knowl- edge of the T-DNN vanished rapidly. We have proposed a method to transfer very rich information by defining novel knowledge using SVD and RBF, which are frequently used in traditional machine learning, without any structural lim- itations of the network. In addition, self-supervised learning associated with multi-task learning have been applied so that it was able to continue to re- ceive T-DNN\u2019s knowledge during the learning process, which could also lead to additional performance enhancement. Experimental results showed that the proposed method has a significant improvement of about 4.96% compared to the 3.17% improvement in terms of accuracy performance based on VGG net- work [10]. In the future, we will develop a semi-supervised learning scheme by extending self-supervised learning concept through proposed knowledge transfer.  Acknowledgements: This research was supported by National Research Foun- dation of Korea Grant funded by the Korean Government (2016R1A2B4007353).  References  1. LeCun, Y., Bottou, L., Bengio, Y., Ha\ufb00ner, P.: Gradient-based learning applied to  document recognition. Proceedings of the IEEE 86(11) (1998) 2278-2324  2. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in neural information processing systems. (2012) 1097-1105   Self-supervised Knowledge Distillation Using Singular Value Decomposition3. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 770-778  4. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. Volume 1. (2017) 3  5. Xie, S., Girshick, R., Doll\u00b4ar, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, IEEE (2017) 5987-5995  6. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shu\ufb04enet: An extremely e\ufb03cient convolu- tional neural network for mobile devices. arXiv preprint arXiv:1707.01083 (2017) 7. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An- dreetto, M., Adam, H.: Mobilenets: E\ufb03cient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)  8. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.  arXiv preprint arXiv:1503.02531 (2015)  9. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:  Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014)  10. Yim, J., Joo, D., Bae, J., Kim, J.: A gift from knowledge distillation: Fast opti- mization, network minimization and transfer learning. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (2017)  11. Alter, O., Brown, P.O., Botstein, D.: Singular value decomposition for genome-wide expression data processing and modeling. Proceedings of the National Academy of Sciences 97(18) (2000) 10101-10106  12. Zhang, Z., Ely, G., Aeron, S., Hao, N., Kilmer, M.: Novel methods for multilinear data completion and de-noising based on tensor-svd. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2014) 3842-3849 13. Ionescu, C., Vantzos, O., Sminchisescu, C.: Matrix backpropagation for deep net- works with structured layers. In: Proceedings of the IEEE International Conference on Computer Vision. (2015) 2965-2973  14. Kim, N., Byun, H.G., Kwon, K.H.: Learning behaviors of stochastic gradient radial basis function network algorithms for odor sensing systems. ETRI journal 28(1) (2006) 59-66  15. Wang, X.X., Chen, S., Harris, C.J.: Using the correlation criterion to position and shape rbf units for incremental modelling. International Journal of Automation and Computing 3(4) (2006) 392-403  16. Larsson, G., Maire, M., Shakhnarovich, G.: Learning representations for automatic colorization. In: European Conference on Computer Vision (ECCV). (2016) 17. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw puzzles. In: European Conference on Computer Vision, Springer (2016) 69- 84  18. Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. IEEE International Conference on Computer Vision (ICCV). (2017)  In: The  19. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint arXiv:1409.1556 (2014)  20. Zhou, X., Belkin, M.: Semi-supervised learning. In: Academic Press Library in  Signal Processing. Volume 1. Elsevier (2014) 1239-1269  21. Su, H., Zhu, J., Yin, Z., Dong, Y., Zhang, B.: E\ufb03cient and robust semi-supervised learning over a sparse-regularized graph. In: European Conference on Computer Vision, Springer (2016) 583-598   16  Seung Hyun Lee, Dae Ha Kim, Byung Cheol Song  22. Pascanu, R., Mikolov, T., Bengio, Y.: On the di\ufb03culty of training recurrent neural networks. In: International Conference on Machine Learning. (2013) 1310-1318 23. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man\u00b4e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi\u00b4egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015) Software available from tensor\ufb02ow.org.  24. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images.  (2009)  25. Kiefer, J., Wolfowitz, J.: Stochastic estimation of the maximum of a regression  function. The Annals of Mathematical Statistics (1952) 462-466  26. Nesterov, Y.: A method for unconstrained convex minimization problem with the rate of convergence o (1/k\u02c6 2). In: Doklady AN USSR. Volume 269. (1983) 543-547"}