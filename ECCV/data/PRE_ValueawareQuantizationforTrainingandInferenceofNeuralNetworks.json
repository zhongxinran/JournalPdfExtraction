{"1": "1. Bakunas-Milanowski, D., et al.: Efficient algorithms for stream compaction on gpus. Inter-  national Journal of Networking and Computing (IJNC) (2017)  2. Chen, T., et al.: Training deep nets with sublinear memory cost. arXiv:1604.06174 (2016) 3. De Sa, C., et al.: High-accuracy low-precision training. arXiv:1803.03383 (2018) 4. Ginsburg, B., et al.: NVIDIA Mixed Precision Training on Volta GPUs. GPU Technology  Conference (2017)  5. Gomez, A.N., et al.: The reversible residual network: Backpropagation without storing acti-  vations. Advances in Neural Information Processing Systems (NIPS) (2017)  6. Hazelwood, K., et al.: Applied machine learning at facebook: A datacenter infrastructure perspective. International Symposium on High-Performance Computer Architecture (HPCA) (2018)  7. He, X., et al.: Practical lessons from predicting clicks on ads at facebook. International Work-  shop on Data Mining for Online Advertising (ADKDD) (2014)  8. Hong, S., Kim, H.: An integrated GPU power and performance model. International Sympo-  sium on Computer Architecture (ISCA) (2010)  9. Hubara, I., et al.: Quantized neural networks: Training neural networks with low precision  weights and activations. arXiv:1609.07061 (2016)  10. Inan, H., Khosravi, K., Socher, R.: Tying word vectors and word classifiers: A loss frame-  work for language modeling. arXiv:1611.01462 (2016)  11. Jain, A., et al.: Gist: Efficient data encoding for deep neural network training. International  Symposium on Computer Architecture (ISCA) (2018)  12. Jia, Y., Peter, V.: Delivering real-time ai  in the palm of your hand. https:  //code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-t he-palm-of-your-hand/, accessed: 2018-7-25  13. Jouppi, N.P., et al.: In-datacenter performance analysis of a tensor processing unit. Interna-  tional Symposium on Computer Architecture (ISCA) (2017)  14. Kone\u02c7cn`y, J., et al.: Federated learning: Strategies for improving communication efficiency.  15. Mahajan, D., et al.: Exploring the limits of weakly supervised pretraining. arXiv:1805.00932  16. Marcel, S., Rodriguez, Y.: Torchvision the machine-vision package of torch. ACM Multime-  arXiv:1610.05492 (2016)  (2018)  dia (2010)  17. Marcus, M.P., Marcinkiewicz, M.A., Santorini, B.: Building a large annotated corpus of En-  glish: The Penn Treebank. Computational Linguistics (1993)  18. Migacz, S.: NVIDIA 8-bit inference width TensorRT. GPU Technology Conference (2017) 19. Mishra, A., Marr, D.: Apprentice: Using knowledge distillation techniques to improve low-  precision network accuracy. arXiv:1711.05852 (2017)  20. Mishra, A., et al.: Wrpn: Wide reduced-precision networks. arXiv:1709.01134 (2017) 21. Miyashita, D., Lee, E.H., Murmann, B.: Convolutional neural networks using logarithmic  data representation. arXiv:1603.01025 (2016)  22. Park, E., Ahn, J., Yoo, S.: Weighted-entropy-based quantization for deep neural networks.  Computer Vision and Pattern Recognition (CVPR) (2017)  23. Park, E., Kim, D., Yoo, S.: Energy-efficient neural network accelerator based on outlier- aware low-precision computation. International Symposium on Computer Architecture (ISCA) (2018)  24. Paszke, A., et al.: Pytorch (2017) 25. Polino, A., Pascanu, R., Alistarh, D.: Model compression via distillation and quantization.  International Conference on Learning Representation (ICLR) (2018)   Value-aware Quantization for Training and Inference of Neural NetworksReferences  1. Bakunas-Milanowski, D., et al.: Efficient algorithms for stream compaction on gpus. Inter-  national Journal of Networking and Computing (IJNC) (2017)  2. Chen, T., et al.: Training deep nets with sublinear memory cost. arXiv:1604.06174 (2016) 3. De Sa, C., et al.: High-accuracy low-precision training. arXiv:1803.03383 (2018) 4. Ginsburg, B., et al.: NVIDIA Mixed Precision Training on Volta GPUs. GPU Technology  Conference (2017)  5. Gomez, A.N., et al.: The reversible residual network: Backpropagation without storing acti-  vations. Advances in Neural Information Processing Systems (NIPS) (2017)  6. Hazelwood, K., et al.: Applied machine learning at facebook: A datacenter infrastructure perspective. International Symposium on High-Performance Computer Architecture (HPCA) (2018)  7. He, X., et al.: Practical lessons from predicting clicks on ads at facebook. International Work-  shop on Data Mining for Online Advertising (ADKDD) (2014)  8. Hong, S., Kim, H.: An integrated GPU power and performance model. International Sympo-  sium on Computer Architecture (ISCA) (2010)  9. Hubara, I., et al.: Quantized neural networks: Training neural networks with low precision  weights and activations. arXiv:1609.07061 (2016)  10. Inan, H., Khosravi, K., Socher, R.: Tying word vectors and word classifiers: A loss frame-  work for language modeling. arXiv:1611.01462 (2016)  11. Jain, A., et al.: Gist: Efficient data encoding for deep neural network training. International  Symposium on Computer Architecture (ISCA) (2018)  12. Jia, Y., Peter, V.: Delivering real-time ai  in the palm of your hand. https:  //code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-t he-palm-of-your-hand/, accessed: 2018-7-25  13. Jouppi, N.P., et al.: In-datacenter performance analysis of a tensor processing unit. Interna-  tional Symposium on Computer Architecture (ISCA) (2017)  14. Kone\u02c7cn`y, J., et al.: Federated learning: Strategies for improving communication efficiency.  15. Mahajan, D., et al.: Exploring the limits of weakly supervised pretraining. arXiv:1805.00932  16. Marcel, S., Rodriguez, Y.: Torchvision the machine-vision package of torch. ACM Multime-  arXiv:1610.05492 (2016)  (2018)  dia (2010)  17. Marcus, M.P., Marcinkiewicz, M.A., Santorini, B.: Building a large annotated corpus of En-  glish: The Penn Treebank. Computational Linguistics (1993)  18. Migacz, S.: NVIDIA 8-bit inference width TensorRT. GPU Technology Conference (2017) 19. Mishra, A., Marr, D.: Apprentice: Using knowledge distillation techniques to improve low-  precision network accuracy. arXiv:1711.05852 (2017)  20. Mishra, A., et al.: Wrpn: Wide reduced-precision networks. arXiv:1709.01134 (2017) 21. Miyashita, D., Lee, E.H., Murmann, B.: Convolutional neural networks using logarithmic  data representation. arXiv:1603.01025 (2016)  22. Park, E., Ahn, J., Yoo, S.: Weighted-entropy-based quantization for deep neural networks.  Computer Vision and Pattern Recognition (CVPR) (2017)  23. Park, E., Kim, D., Yoo, S.: Energy-efficient neural network accelerator based on outlier- aware low-precision computation. International Symposium on Computer Architecture (ISCA) (2018)  24. Paszke, A., et al.: Pytorch (2017) 25. Polino, A., Pascanu, R., Alistarh, D.: Model compression via distillation and quantization.  International Conference on Learning Representation (ICLR) (2018)   16  Eunhyeok Park, Sungjoo Yoo, and Peter Vajda  26. Press, O., Wolf, L.: Using the output embedding to improve language models. European  Chapter of the Association for Computational Linguistics (EACL) (2017)  27. Rastegari, M., et al.: Xnor-net: Imagenet classification using binary convolutional neural  networks. European Conference on Computer Vision (ECCV) (2016)  28. Umuroglu, Y., et al.: Finn: A framework for fast, scalable binarized neural network inference.  Architecture of Field-Programmable Gate Arrays (FPGA) (2017)  I., Vinyals, O.: Recurrent neural network regularization.  29. Zaremba, W., Sutskever, arXiv:1409.2329 (2014)  30. Zhou, S., et al.: Dorefa-net: Training low bitwidth convolutional neural networks with low  bitwidth gradients. arXiv:1606.06160 (2016)  31. Zhou, S., et al.: Balanced quantization: An effective and efficient approach to quantized  neural networks. Journal of Computer Science and Technology (2017) 32. Zhu, C., et al.: Trained ternary quantization. arXiv:1612.01064 (2016)"}