{"1": "1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. CVPR (2018)  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: ICCV (2015)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. ICLR (2014)  4. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. CoRR (2015) 5. Cho, K., Van Merri\u00a8enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)  6. Das, A., Agrawal, H., Zitnick, L., Parikh, D., Batra, D.: Human attention in vi- sual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding 163, 90-100 (2017)  7. Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multi- modal compact bilinear pooling for visual question answering and visual grounding. EMNLP (2016)  8. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. CVPR (2017)  9. Gu, J., Cai, J., Wang, G., Chen, T.: Stack-captioning: Coarse-to-fine learning for  10. Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image  image captioning. AAAI (2018)  captioning. In: ICCV (2017)  11. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. CVPR (2018)  12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  13. Heilman, M., Smith, N.A.: Good question! statistical ranking for question gen- eration. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pp. 609-617. HLT \u201910, Association for Computational Linguistics, Stroudsburg, PA, USA (2010), http://dl.acm.org/citation.cfm?id=1857999.1858085  14. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.:  Generating visual explanations. In: ECCV. pp. 3-19. Springer (2016)  15. Ilievski, I., Yan, S., Feng, J.: A focused dynamic attention model for visual question  answering. ECCV (2016)  16. Kazemi, V., Elqursh, A.: Show, ask, attend, and answer: A strong baseline for  visual question answering. arXiv preprint arXiv:1704.03162 (2017)  17. Li, Q., Fu, J., Yu, D., Mei, T., Luo, J.: Tell-and-answer: Towards explainable visual question answering using attributes and captions. arXiv preprint arXiv:1801.09041 (2018)  18. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention  for visual question answering. In: NIPS. pp. 289-297 (2016)  19. Nam, H., Ha, J.W., Kim, J.: Dual attention networks for multimodal reasoning  and matching. CVPR (2017)   VQA-EReferences  1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. CVPR (2018)  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: ICCV (2015)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. ICLR (2014)  4. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. CoRR (2015) 5. Cho, K., Van Merri\u00a8enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)  6. Das, A., Agrawal, H., Zitnick, L., Parikh, D., Batra, D.: Human attention in vi- sual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding 163, 90-100 (2017)  7. Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T., Rohrbach, M.: Multi- modal compact bilinear pooling for visual question answering and visual grounding. EMNLP (2016)  8. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. CVPR (2017)  9. Gu, J., Cai, J., Wang, G., Chen, T.: Stack-captioning: Coarse-to-fine learning for  10. Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image  image captioning. AAAI (2018)  captioning. In: ICCV (2017)  11. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. CVPR (2018)  12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  13. Heilman, M., Smith, N.A.: Good question! statistical ranking for question gen- eration. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pp. 609-617. HLT \u201910, Association for Computational Linguistics, Stroudsburg, PA, USA (2010), http://dl.acm.org/citation.cfm?id=1857999.1858085  14. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.:  Generating visual explanations. In: ECCV. pp. 3-19. Springer (2016)  15. Ilievski, I., Yan, S., Feng, J.: A focused dynamic attention model for visual question  answering. ECCV (2016)  16. Kazemi, V., Elqursh, A.: Show, ask, attend, and answer: A strong baseline for  visual question answering. arXiv preprint arXiv:1704.03162 (2017)  17. Li, Q., Fu, J., Yu, D., Mei, T., Luo, J.: Tell-and-answer: Towards explainable visual question answering using attributes and captions. arXiv preprint arXiv:1801.09041 (2018)  18. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention  for visual question answering. In: NIPS. pp. 289-297 (2016)  19. Nam, H., Ha, J.W., Kim, J.: Dual attention networks for multimodal reasoning  and matching. CVPR (2017)   16  Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo  20. Park, D.H., Hendricks, L.A., Akata, Z., Rohrbach, A., Schiele, B., Darrell, T., Rohrbach, M.: Multimodal explanations: Justifying decisions and pointing to the evidence. In: CVPR (2018)  21. Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word represen-  tation. In: EMNLP. pp. 1532-1543 (2014)  22. Ren, M., Kiros, R., Zemel, R.: Image question answering: A visual semantic em-  bedding model and a new dataset. NIPS 1(2), 5 (2015)  23. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence  training for image captioning. CVPR (2017)  24. Salimans, T., Kingma, D.P.: Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In: NIPS. pp. 901-909 (2016) 25. Shih, K.J., Singh, S., Hoiem, D.: Where to look: Focus regions for visual question  answering. In: ICCV. pp. 4613-4621 (2016)  26. Teney, D., Anderson, P., He, X., Hengel, A.v.d.: Tips and tricks for visual question  answering: Learnings from the 2017 challenge. CVPR (2018)  27. Wu, Q., Shen, C., Liu, L., Dick, A., van den Hengel, A.: What value do explicit  high level concepts have in vision to language problems? In: CVPR (2016)  28. Xu, H., Saenko, K.: Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In: ECCV. pp. 451-466. Springer (2016) 29. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML. vol. 14, pp. 77-81 (2015)  30. Yang, X., Zhang, H., Cai, J.: Shu\ufb04e-then-assemble: Learning object-agnostic visual  relationship features. In: ECCV (2018)  31. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks for  image question answering. In: CVPR. pp. 21-29 (2016)  32. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic  33. Yu, D., Fu, J., Mei, T., Rui, Y.: Multi-level attention networks for visual question  attention. In: CVPR (2016)  answering. In: CVPR (2017)  34. Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question an-  swering in images. In: CVPR. pp. 4995-5004 (2016)"}