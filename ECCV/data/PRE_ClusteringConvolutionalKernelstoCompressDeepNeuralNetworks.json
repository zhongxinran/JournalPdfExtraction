{"1": "1. Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural networks with the hashing trick. In: 32nd International Conference on Machine Learning. pp. 2285-2294 (2015)  2. Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., Shelhamer, E.: cudnn: E\ufb03cient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014)  3. Cohen, T., Welling, M.: Group equivariant convolutional networks. In: 33rd Inter-  national Conference on Machine Learning. pp. 2990-2999 (2016)  4. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural networks with binary weights during propagations. In: Advances in Neural Infor- mation Processing Systems. pp. 3123-3131 (2015)  5. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830 (2016)  6. Denil, M., Shakibi, B., Dinh, L., De Freitas, N., et al.: Predicting parameters in deep learning. In: Advances in Neural Information Processing Systems. pp. 2148- 2156 (2013)  7. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear structure within convolutional networks for e\ufb03cient evaluation. In: Advances in Neural Information Processing Systems. pp. 1269-1277 (2014)  8. Dieleman, S., De Fauw, J., Kavukcuoglu, K.: Exploiting cyclic symmetry in convo- lutional neural networks. In: 33rd International Conference on Machine Learning. pp. 1889-1898 (2016)  9. Gong, Y., Liu, L., Yang, M., Bourdev, L.: Compressing deep convolutional networks  using vector quantization. arXiv preprint arXiv:1412.6115 (2014)  10. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for e\ufb03cient dnns. In: Ad-  vances in Neural Information Processing Systems. pp. 1379-1387 (2016)  11. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with limited numerical precision. In: 32nd International Conference on Machine Learn- ing. pp. 1737-1746 (2015)  12. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net- works with pruning, trained quantization and hu\ufb00man coding. In: International Conference on Learning Representations (2016)  13. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for e\ufb03cient neural network. In: Advances in Neural Information Processing Systems. pp. 1135-1143 (2015)  14. Hassibi, B., Stork, D.G.: Second order derivatives for network pruning: Optimal brain surgeon. In: Advances in Neural Information Processing Systems. pp. 164- 171 (1993)  15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770-778 (2016)  16. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2017)  17. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Quantized neu- ral networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research 18(187), 1-30 (2018)   Clustering Convolutional Kernels to Compress Deep Neural NetworksReferences  1. Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural networks with the hashing trick. In: 32nd International Conference on Machine Learning. pp. 2285-2294 (2015)  2. Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., Shelhamer, E.: cudnn: E\ufb03cient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014)  3. Cohen, T., Welling, M.: Group equivariant convolutional networks. In: 33rd Inter-  national Conference on Machine Learning. pp. 2990-2999 (2016)  4. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural networks with binary weights during propagations. In: Advances in Neural Infor- mation Processing Systems. pp. 3123-3131 (2015)  5. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830 (2016)  6. Denil, M., Shakibi, B., Dinh, L., De Freitas, N., et al.: Predicting parameters in deep learning. In: Advances in Neural Information Processing Systems. pp. 2148- 2156 (2013)  7. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear structure within convolutional networks for e\ufb03cient evaluation. In: Advances in Neural Information Processing Systems. pp. 1269-1277 (2014)  8. Dieleman, S., De Fauw, J., Kavukcuoglu, K.: Exploiting cyclic symmetry in convo- lutional neural networks. In: 33rd International Conference on Machine Learning. pp. 1889-1898 (2016)  9. Gong, Y., Liu, L., Yang, M., Bourdev, L.: Compressing deep convolutional networks  using vector quantization. arXiv preprint arXiv:1412.6115 (2014)  10. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for e\ufb03cient dnns. In: Ad-  vances in Neural Information Processing Systems. pp. 1379-1387 (2016)  11. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with limited numerical precision. In: 32nd International Conference on Machine Learn- ing. pp. 1737-1746 (2015)  12. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net- works with pruning, trained quantization and hu\ufb00man coding. In: International Conference on Learning Representations (2016)  13. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for e\ufb03cient neural network. In: Advances in Neural Information Processing Systems. pp. 1135-1143 (2015)  14. Hassibi, B., Stork, D.G.: Second order derivatives for network pruning: Optimal brain surgeon. In: Advances in Neural Information Processing Systems. pp. 164- 171 (1993)  15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770-778 (2016)  16. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2017)  17. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Quantized neu- ral networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research 18(187), 1-30 (2018)   16  S. Son, S. Nah and K. M. Lee  18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.: Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5 mb model size. arXiv preprint arXiv:1602.07360 (2016)  19. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: 32nd International Conference on Machine Learning. pp. 448-456 (2015)  20. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural net-  works with low rank expansions. arXiv preprint arXiv:1405.3866 (2014)  21. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images  (2009)  22. Lavin, A., Gray, S.: Fast algorithms for convolutional neural networks. In: Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4013-4021 (2016)  23. Lebedev, V., Lempitsky, V.: Fast convnets using group-wise brain damage. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2554-2564 (2016)  24. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Advances in Neural  Information Processing Systems. pp. 598-605 (1990)  25. Li, F., Zhang, B., Liu, B.: Ternary weight networks. arXiv preprint  arXiv:1605.04711 (2016)  26. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for e\ufb03cient  convnets. In: International Conference on Learning Representations (2017)  27. Luo, J.H., Wu, J., Lin, W.: Thinet: A filter level pruning method for deep neural  network compression (2017)  28. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic di\ufb00erentiation in pytorch (2017) 29. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: Xnor-net: Imagenet classi- fication using binary convolutional neural networks. In: European Conference on Computer Vision. pp. 525-542 (2016)  30. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 115(3), 211-252 (2015)  31. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint arXiv:1409.1556 (2014)  32. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization and momentum in deep learning. In: 30th International Conference on Machine Learning. pp. 1139-1147 (2013)  33. Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in deep neural networks. In: Advances in Neural Information Processing Systems. pp. 2074-2082 (2016)  34. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural networks for mobile devices. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4820-4828 (2016)  35. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.  In: European conference on computer vision. pp. 818-833. Springer (2014)  36. Zhai, S., Cheng, Y., Zhang, Z.M., Lu, W.: Doubly convolutional neural networks. In: Advances in Neural Information Processing Systems. pp. 1082-1090 (2016) 37. Zhou, A., Yao, A., Guo, Y., Xu, L., Chen, Y.: Incremental network quantization: Towards lossless cnns with low-precision weights. In: International Conference on Learning Representations (2017)   Clustering Convolutional Kernels to Compress Deep Neural Networks38. Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., Zou, Y.: Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016)  39. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization. In: Interna-  tional Conference on Learning Representations (2017)"}