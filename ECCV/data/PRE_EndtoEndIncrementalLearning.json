{"1": "1. Supplementary material. Also available in the arXiv technical report. https://github.  com/fmcp/EndToEndIncrementalLearning  2. Ans, B., Rousset, S., French, R.M., Musca, S.: Self-refreshing memory in artificial neural networks: Learning temporal sequences without catastrophic forgetting. Connection Science 16(2), 71-99 (2004)  3. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspec-  4. Cauwenberghs, G., Poggio, T.: Incremental and decremental support vector machine learn-  tives. PAMI 35(8), 1798-1828 (2013)  ing. In: NIPS (2000)   14  Castro et al.  100 100 100 100  y y c c y a a c y r a r c u u r a u c r c u c c c c c c A A A A       5 5 5 5 - - - p - p p p o o T o o T T T  90 90  80 80 80 80  70 70  60 60 60 60  50 50  40 40 40 40 0 0  0 0  Our-CNN (90.4) iCaRL (85.0) Our-CNN (90.4) iCaRL-hybrid1 (83.5) iCaRL (85.0) Our-CNN (90.4) iCaRL (85.0) LwF.MC (79.1) iCaRL-hybrid1 (83.5) iCaRL-hybrid1 (83.5) Upper-Bound (95.1) Upper-Bound (95.1) Upper-Bound (95.1)  Our-CNN (90.4) iCaRL (85.0) iCaRL-hybrid1 (83.5)100  100 10080 80  60 6040 4020 2080 y y c c y a a c r a r u u 60 r u c c c c c c A A A     40 5 5 5 - - - p p p o o o T T T 20     y c a r u c c A   5 - p o T  Our-CNN (69.4) iCaRL (62.5) Our-CNN (69.4) iCaRL-hybrid1 (46.1) iCaRL (62.5) Our-CNN (69.4) iCaRL (62.5) LwF.MC (43.8) iCaRL-hybrid1 (46.1) iCaRL-hybrid1 (46.1) Upper-Bound (89.2) Upper-Bound (89.2) Upper-Bound (89.2)  Our-CNN (71.3) iCaRL (62.5) iCaRL-hybrid1 (46.1) 200 0 200 02000 0 0  20 20 2040 40 40  60 60 60 60 Number of classes Number of classes Number of classes Number of classes80 80 80100 100 100 1000  400 400600 400 600 Number of classes Number of classes Number of classes Number of classes  600 600  800 800800  1000 10001000  a)  b)  Fig. 5: Accuracy on ImageNet. One execution with (a) 10 and (b) 100 classes per incremental step. Average of the incremental steps is shown in parentheses for each method. (Best viewed in pdf.)  old classes is more balanced than in CIFAR-100, our approach achieves good accuracy even with large incremental steps of 100 classes.  8 Summary  This paper presents a novel approach for training CNNs in an incremental fashion us- ing a combination of cross-entropy and distillation loss functions. Experimental results on CIFAR-100 and ImageNet presented in the paper lead to the following conclusions. (i) Our end-to-end approach is more robust than other recent methods, such as iCaRL, relying on a sub-optimal, independently-learned external classifier. (ii) Representative memory, its size, and unbalanced training sets play an important role in the final accu- racy. As part of future work we plan to explore new sample selection strategies, using a dynamic number of samples per class.  Acknowledgements. This work was supported in part by the projects TIC-1692 (Junta de Andaluc\u00b4\u0131a), TIN2016-80920R (Spanish Ministry of Science and Tech.), ERC ad- vanced grant ALLEGRO, and EVEREST (no. 5302-1) funded by CEFIPRA. We grate- fully acknowledge the support of NVIDIA Corporation with the donation of a Titan X Pascal GPU used for this research.  References  1. Supplementary material. Also available in the arXiv technical report. https://github.  com/fmcp/EndToEndIncrementalLearning  2. Ans, B., Rousset, S., French, R.M., Musca, S.: Self-refreshing memory in artificial neural networks: Learning temporal sequences without catastrophic forgetting. Connection Science 16(2), 71-99 (2004)  3. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspec-  4. Cauwenberghs, G., Poggio, T.: Incremental and decremental support vector machine learn-  tives. PAMI 35(8), 1798-1828 (2013)  ing. In: NIPS (2000)   End-to-End Incremental Learning5. Chen, X., Shrivastava, A., Gupta, A.: NEIL: Extracting visual knowledge from web data. In:  ICCV (2013)  6. Cortes, C., Vapnik, V.: Support-vector networks. Machine Learning 20(3), 273-297 (1995) 7. Divvala, S., Farhadi, A., Guestrin, C.: Learning everything about anything: Webly-supervised  visual concept learning. In: CVPR (2014)  8. French, R.M.: Dynamically constraining connectionist networks to produce distributed, or- thogonal representations to reduce catastrophic interference. In: Cognitive Science Society Conf. (1994)  9. Furlanello, T., Zhao, J., Saxe, A.M., Itti, L., Tjan, B.S.: Active long term memory networks.  ArXiv e-prints, arXiv 1606.02355 (2016)  10. Goodfellow, I., Mirza, M., Xiao, D., Courville, A., Bengio, Y.: An empirical investigation of catastrophic forgetting in gradient-based neural networks. ArXiv e-prints, arXiv 1312.6211 (2013)  11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR  12. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NIPS  (2016)  workshop (2014)  13. Jung, H., Ju, J., Jung, M., Kim, J.: Less-forgetting learning in deep neural networks. ArXiv  e-prints, arXiv 1607.00122 (2016)  14. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., Hadsell, R.: Overcoming catastrophic forgetting in neural networks. Proc. National Academy of Sciences 114(13), 3521-3526 (2017)  15. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech. rep., University  16. Li, Z., Hoiem, D.: Learning without forgetting. PAMI (2018) 17. Lopez-Paz, D., Ranzato, M.A.: Gradient episodic memory for continual learning. In: NIPS  of Toronto (2009)  (2017)  18. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks: The se- quential learning problem. Psychology of Learning and Motivation 24, 109 - 165 (1989) 19. Mensink, T., Verbeek, J., Perronnin, F., Csurka, G.: Distance-based image classification:  Generalizing to new classes at near-zero cost. PAMI 35(11), 2624-2637 (2013)  20. Mitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Betteridge, J., Carlson, A., Mishra, B.D., Gardner, M., Kisiel, B., Krishnamurthy, J., Lao, N., Mazaitis, K., Mohamed, T., Nakashole, N., Platanios, E., Ritter, A., Samadi, M., Settles, B., Wang, R., Wijaya, D., Gupta, A., Chen, X., Saparov, A., Greaves, M., Welling, J.: Never-ending learning. In: AAAI (2015)  21. Neelakantan, A., Vilnis, L., Le, Q.V., Sutskever, I., Kaiser, L., Kurach, K., Martens, J.: Adding gradient noise improves learning for very deep networks. ArXiv e-prints, arXiv 1511.06807 (2017)  22. Ratcliff, R.: Connectionist models of recognition memory: constraints imposed by learning  and forgetting functions. Psychological review 97(2), 285 (1990)  23. Rebuffi, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: iCaRL: Incremental classifier and  24. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with  representation learning. In: CVPR (2017)  region proposal networks. In: NIPS (2015)  25. Ristin, M., Guillaumin, M., Gall, J., Gool, L.V.: Incremental learning of ncm forests for  large-scale image classification. In: CVPR (2014)  26. Ruping, S.: Incremental learning with support vector machines. In: ICDM (2001) 27. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. IJCV 115(3), 211-252 (2015)   16  Castro et al.  28. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. ArXiv e-prints, arXiv 1606.04671 (2016)  29. Ruvolo, P., Eaton, E.: ELLA: An efficient lifelong learning algorithm. In: ICML (2013) 30. Shmelkov, K., Schmid, C., Alahari, K.: Incremental learning of object detectors without  31. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in  catastrophic forgetting. In: ICCV (2017)  videos. In: NIPS (2014)  32. Terekhov, A.V., Montone, G., O\u2019Regan, J.K.: Knowledge transfer in deep block-modular  neural networks. In: Biomimetic and Biohybrid Systems (2015)  33. Thrun, S.: Lifelong Learning Algorithms, pp. 181-209. Springer US (1998) 34. Triki, A.R., Aljundi, R., Blaschko, M.B., Tuytelaars, T.: Encoder based lifelong learning. In:  ICCV (2017)  ACM Multimedia (2015)  35. Vedaldi, A., Lenc, K.: MatConvNet - Convolutional Neural Networks for MATLAB. In:  36. Welling, M.: Herding dynamical weights to learn. In: ICML (2009) 37. Xiao, T., Zhang, J., Yang, K., Peng, Y., Zhang, Z.: Error-driven incremental learning in deep convolutional neural network for large-scale image classification. In: ACM Multimedia (2014)"}