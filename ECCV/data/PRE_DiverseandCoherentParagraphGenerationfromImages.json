{"1": "CVPR (2018)  1. Aneja, J., Deshpande, A., Schwing, A.G.: Convolutional Image Captioning. In: Proc.  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: Proc. ICCV (2015)  3. Arjovsky, M., etal.: Wasserstein gan. arXiv preprint 2017 4. Chatterjee, M., Leuski, A.: A novel statistical approach for image and video retrieval  and its adaption for active learning. In: Proc. ACM Multimedia (2015)  5. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for  image caption generation. In: Proc. CVPR (2015)   14  Moitreya Chatterjee and Alexander G. Schwing  inputs, just by sampling a di\ufb00erent z each time (the two rightmost columns in Figure 7), permitting our results to be diverse. Moreover, for the Amazon dataset (third and fourth rows in Figure 7) we see that our model learns to synthesize \u2018sentiment\u2019 words depending on the number of input stars. We present additional visualizations in the supplementary.  Ablation study: In one setting, we judge the importance of coherence vectors, by just using the global vector and setting the coherence vectors to 0, in the sentence generation net. The results for this setting (\u2018Ours (NC)\u2019) are shown in Tables 1,2, while qualitative results are shown in Figure 7. These numbers reveal that just by incorporating the global topic vector it is feasible to generate reasonably good paragraphs. However, incorporating coherence vectors makes the synthesized paragraphs more human-like. A look at the second column of Figure 7 shows that even without coherence vectors we are able to detect global topics like \u2018car-park\u2019 but the sentences seem to exhibit sharp topic transition, quite like Regions-Hierarchical approach. We rectify this by introducing coherence vectors. In another setting, we set the global topic vector to 0, at every time-step, while retaining the coherence vectors. The performance in this setting is indicated by \u2018Ours (NG)\u2019 in Tables 1,2. The results suggest that incorporating the coherence vectors is much more critical for improved performance.  5 Conclusions and Future Work  In this work, we developed \u2018coherence vectors\u2019 which explicitly ensure consistency of themes between generated sentences during paragraph generation. Additionally, the \u2018global topic vector\u2019 was designed to capture the underlying main plot of an image. We demonstrated the e\ufb03cacy of the proposed technique on two datasets, showing that our model when trained with e\ufb00ective autoencoding techniques can achieve state-of-the-art performance for both caption and review generation tasks. In the future we plan to extend our technique for the task of generation of even longer narratives, such as stories. Acknowledgments: This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221, Samsung, and 3M. We thank NVIDIA for providing the GPUs used for this research.  References  CVPR (2018)  1. Aneja, J., Deshpande, A., Schwing, A.G.: Convolutional Image Captioning. In: Proc.  2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: Proc. ICCV (2015)  3. Arjovsky, M., etal.: Wasserstein gan. arXiv preprint 2017 4. Chatterjee, M., Leuski, A.: A novel statistical approach for image and video retrieval  and its adaption for active learning. In: Proc. ACM Multimedia (2015)  5. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for  image caption generation. In: Proc. CVPR (2015)   Diverse and Coherent Paragraph Generation from Images6. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent  neural networks on sequence modeling. arXiv preprint 2014  7. Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A.C., Bengio, Y.: A recurrent  latent variable model for sequential data. In: Proc. NIPS (2015)  8. Dai, B., etal.: Towards diverse and natural image descriptions via a conditional gan.  arXiv preprint 2017  9. Denkowski, M., Lavie, A.: Meteor universal: Language specific translation evaluation for any target language. In: Proc. ninth workshop on statistical machine translation (2014)  10. Deshpande, A., Aneja, J., Wang, L., Schwing, A.G., Forsyth, D.A.: Di- verse and Controllable Image Captioning with Part-of-Speech Guidance. In: https://arxiv.org/abs/1805.12589 (2018)  11. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: Proc. CVPR (2015)  12. Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., Xu, W.: Are you talking to a machine? dataset and methods for multilingual image question. In: Proc. NIPS (2015)  13. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Proc. NIPS (2014) 14. Gregor, K., Danihelka, I., Graves, A., Rezende, D.J., Wierstra, D.: Draw: A recurrent  neural network for image generation. arXiv preprint 2015  15. Jain, U., Zhang, Z., Schwing, A.: Creativity: Generating diverse questions using  variational autoencoders. In: Proc. CVPR (2017)  16. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization  networks for dense captioning. In: Proc. CVPR (2016)  17. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image  descriptions. In: Proc. CVPR (2015)  18. Kingma, D.P., etal.: Auto-encoding variational bayes. arXiv preprint 2013 19. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint20. Klambauer, G., Unterthiner, T., Mayr, A., Hochreiter, S.: Self-normalizing neural  networks. In: Proc. NIPS (2017)  21. Krause, J., Johnson, J., Krishna, R., Fei-Fei, L.: A hierarchical approach for  generating descriptive image paragraphs. In: Proc. CVPR (2017)  22. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV (2017)  23. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep  convolutional neural networks. In: Proc. NIPS (2012)  24. Lavrenko, V., Manmatha, R., Jeon, J.: A model for learning the semantics of  pictures. In: Proc. NIPS (2004)  25. Liang, X., Hu, Z., Zhang, H., Gan, C., Xing, E.P.: Recurrent Topic-Transition GAN  for Visual Paragraph Generation. In: Proc. ICCV (2017)  26. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proc. ECCV (2014) 27. Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A neural-based ap-  proach to answering questions about images. In: Proc. ICCV (2015)  28. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with  multimodal recurrent neural networks (m-rnn). arXiv preprint 2014   16  Moitreya Chatterjee and Alexander G. Schwing  29. McAuley, J., Targett, C., Shi, Q., Van Den Hengel, A.: Image-based recommenda-  tions on styles and substitutes. In: Proc. ACM SIGIR (2015)  30. Pan, J.Y., Yang, H.J., Duygulu, P., Faloutsos, C.: Automatic image captioning. In:  Proc. ICME (2004)  31. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic  evaluation of machine translation. In: Proc. ACL (2002)  32. Ren, M., Kiros, R., Zemel, R.: Exploring models and data for image question  33. Shih, K.J., Singh, S., Hoiem, D.: Where to look: Focus regions for visual question  answering. In: Proc. NIPS (2015)  answering. In: Proc. CVPR (2016)  34. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint 2014  35. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image  description evaluation. In: Proc. CVPR (2015)  36. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image  caption generator. In: Proc. CVPR (2015)  37. Wang, L., Schwing, A.G., Lazebnik, S.: Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space. In: Proc. NIPS (2017)  38. Xiao, Y., Chua, T.S., Lee, C.H.: Fusion of region and image-based techniques for automatic image annotation. In: Proc. International Conference on Multimedia Modeling (2007)  39. Xie, P.: Diversity-Promoting and Large-Scale Machine Learning for Healthcare. http: //www.cs.cmu.edu/~pengtaox/thesis_proposal_pengtaoxie.pdf (2018), [Online; accessed 25-July-2018]  40. Xiong, C., Merity, S., Socher, R.: Dynamic memory networks for visual and textual  question answering. In: Proc. ICML (2016)  41. Xu, H., Saenko, K.: Ask, attend and answer: Exploring question-guided spatial  attention for visual question answering. In: Proc. ECCV (2016)  42. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: Proc. ICML (2015)  43. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks for  image question answering. In: Proc. CVPR (2016)  44. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic  attention. In: Proc. CVPR (2016)  45. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using  hierarchical recurrent neural networks. In: Proc. CVPR (2016)"}