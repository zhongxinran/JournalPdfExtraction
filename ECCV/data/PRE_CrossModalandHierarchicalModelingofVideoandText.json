{"1": "1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: ICCV. pp. 5804-5813 2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: ICCV. pp. 2425-2433 (2015)  3. Chao, W.L., Hu, H., Sha, F.: Being negative but constructively: Lessons learnt from creating better visual question answering datasets. NAACL-HLT pp. 431- 441 (2018)  4. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur- rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014) 5. Collell, G., Moens, M.F.: Is an image worth more than a thousand words? on the fine-grain semantic di\ufb00erences between visual and linguistic representations. In: COLING. pp. 2807-2817 (2016)  6. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fu-  sion for video action recognition. In: CVPR. pp. 1933-1941 (2016)  7. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.: Devise: A deep visual-semantic embedding model. In: NIPS. pp. 2121-2129 (2013) 8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR. pp. 770-778 (2016)  9. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale  video benchmark for human activity understanding. In: CVPR. pp. 961-970 10. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation  9(8), 1735-1780 (1997)  11. Hu, H., Chao, W.L., Sha, F.: Learning answer embeddings for visual question  answering. In: CVPR. pp. 5428-5436 (2018)  12. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image  descriptions. In: CVPR. pp. 3128-3137 (2015)  13. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: CVPR. pp. 1725- 1732 (2014)  14. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)  15. Kiela, D., Bottou, L.: Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics. In: EMNLP. pp. 36-45 (2014) 16. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014) 17. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events  in videos. In: ICCV. pp. 706-715 (2017)  18. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-  volutional neural networks. In: NIPS. pp. 1106-1114 (2012)  19. Li, J., Luong, M.T., Jurafsky, D.: A hierarchical neural autoencoder for paragraphs  and documents. ACL pp. 1106-1115 (2015)  20. Li, Y., Yao, T., Pan, Y., Chao, H., Mei, T.: Jointly localizing and describing events  for dense video captioning. In: CVPR. pp. 7492-7500 (2018)  21. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740-755 (2014)   Cross-Modal and Hierarchical Modeling of Video and TextReferences  1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: ICCV. pp. 5804-5813 2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,  D.: Vqa: Visual question answering. In: ICCV. pp. 2425-2433 (2015)  3. Chao, W.L., Hu, H., Sha, F.: Being negative but constructively: Lessons learnt from creating better visual question answering datasets. NAACL-HLT pp. 431- 441 (2018)  4. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur- rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014) 5. Collell, G., Moens, M.F.: Is an image worth more than a thousand words? on the fine-grain semantic di\ufb00erences between visual and linguistic representations. In: COLING. pp. 2807-2817 (2016)  6. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fu-  sion for video action recognition. In: CVPR. pp. 1933-1941 (2016)  7. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.: Devise: A deep visual-semantic embedding model. In: NIPS. pp. 2121-2129 (2013) 8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR. pp. 770-778 (2016)  9. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale  video benchmark for human activity understanding. In: CVPR. pp. 961-970 10. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation  9(8), 1735-1780 (1997)  11. Hu, H., Chao, W.L., Sha, F.: Learning answer embeddings for visual question  answering. In: CVPR. pp. 5428-5436 (2018)  12. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image  descriptions. In: CVPR. pp. 3128-3137 (2015)  13. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: CVPR. pp. 1725- 1732 (2014)  14. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)  15. Kiela, D., Bottou, L.: Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics. In: EMNLP. pp. 36-45 (2014) 16. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014) 17. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events  in videos. In: ICCV. pp. 706-715 (2017)  18. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-  volutional neural networks. In: NIPS. pp. 1106-1114 (2012)  19. Li, J., Luong, M.T., Jurafsky, D.: A hierarchical neural autoencoder for paragraphs  and documents. ACL pp. 1106-1115 (2015)  20. Li, Y., Yao, T., Pan, Y., Chao, H., Mei, T.: Jointly localizing and describing events  for dense video captioning. In: CVPR. pp. 7492-7500 (2018)  21. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740-755 (2014)   16  B. Zhang, H. Hu, and F. Sha  22. Luong, M.T., Pham, H., Manning, C.D.: E\ufb00ective approaches to attention-based  neural machine translation. EMNLP pp. 1412-1421  23. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. JMLR 9(Nov), 2579-2605  (2008)  24. Niu, Z., Zhou, M., Wang, L., Gao, X., Hua, G.: Hierarchical multimodal lstm for  dense visual-semantic embedding. In: ICCV. pp. 1899-1907 (2017)  25. Pan, P., Xu, Z., Yang, Y., Wu, F., Zhuang, Y.: Hierarchical recurrent neural encoder for video representation with application to captioning. In: CVPR. pp. 1029-1038 (2016)  26. Pascanu, R., Mikolov, T., Bengio, Y.: On the di\ufb03culty of training recurrent neural  networks. In: ICML. pp. 1310-1318 (2013)  27. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-  sentation. In: EMNLP. pp. 1532-1543 (2014)  28. Qiu, Z., Yao, T., Mei, T.: Deep quantization: Encoding convolutional activations  with deep generative model. In: CVPR. pp. 4085-4094 (2017)  29. Schro\ufb00, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face  recognition and clustering. In: CVPR. pp. 815-823 (2015)  30. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-  nition in videos. In: NIPS. pp. 568-576 (2014)  31. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes  from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)  32. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural  networks. In: NIPS. pp. 3104-3112 (2014)  33. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem- poral features with 3d convolutional networks. In: ICCV. pp. 4489-4497 (2015) 34. Tsai, Y.H.H., Huang, L.K., Salakhutdinov, R.: Learning robust visual-semantic  embeddings. In: ICCV. pp. 3591-3600 (2017)  35. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.:  Sequence to sequence-video to text. In: ICCV. pp. 4534-4542 (2015)  36. Venugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R., Saenko, K.: language using deep recurrent neural networks.  Translating videos to natural NAACL-HLT pp. 1494-1504 (2015)  37. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image  caption generator. In: CVPR. pp. 3156-3164 (2015)  38. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks: Towards good practices for deep action recognition. In: ECCV. pp. 20-36 (2016)  39. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks for action recognition in videos. arXiv preprint arXiv:1705.02953 (2017)  40. Wang, L., Li, Y., Lazebnik, S.: Learning deep structure-preserving image-text em-  beddings. In: CVPR. pp. 5005-5013 (2016)  41. Wu, C.Y., Zaheer, M., Hu, H., Manmatha, R., Smola, A.J., Kr\u00a8ahenb\u00a8uhl, P.: Com-  pressed video action recognition. CVPR (2018)  42. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML. pp. 2048-2057 (2015)  43. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using  hierarchical recurrent neural networks. In: CVPR. pp. 4584-4593 (2016)  44. Zhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H.: Real-time action recognition  with enhanced motion vector cnns. In: CVPR. pp. 2718-2726 (2016)   Cross-Modal and Hierarchical Modeling of Video and Text45. Zhang, K., Chao, W.L., Sha, F., Grauman, K.: Video summarization with long  short-term memory. In: ECCV. pp. 766-782 (2016)  46. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin, D.: Temporal action detection  with structured segment networks. ICCV pp. 2933-2942 (2017)  47. Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question an-  swering in images. In: CVPR. pp. 4995-5004 (2016)"}