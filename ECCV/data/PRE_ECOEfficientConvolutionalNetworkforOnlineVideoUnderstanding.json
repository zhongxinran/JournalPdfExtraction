{"1": "1. Ballas, N., Yao, L., Pal, C., Courville, A.C.: Delving deeper into convolutional  networks for learning video representations. ICLR (2016) 14  2. Bilen, H., Fernando, B., Gavves, E., Vedaldi, A.: Action recognition with dynamic image networks. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1-1 (2017). https://doi.org/10.1109/TPAMI.2017.2769085 3  3. Bilen, H., Fernando, B., Gavves, E., Vedaldi, A., Gould, S.: Dynamic im- age networks for action recognition. In: 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 3034-3042 (June 2016). https://doi.org/10.1109/CVPR.2016.331 3  4. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the kinetics dataset. CoRR abs/1705.07750 (2017), http://arxiv.org/abs/1705. 07750 3, 9, 10  5. Diba, A., Fayyaz, M., Sharma, V., Karami, A.H., Arzani, M.M., Yousefzadeh, R., Gool, L.V.: Temporal 3d convnets: New architecture and transfer learning for video classification. CoRR abs/1711.08200 (2017), http://arxiv.org/abs/ 1711.08200 3, 9  6. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015) 3  7. Feichtenhofer, C., Pinz, A., Wildes, R.P.: Spatiotemporal multiplier net- works In: 2017 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 7445-7454 (July 2017). https://doi.org/10.1109/CVPR.2017.787 3  for video action recognition.  8. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fu- sion for video action recognition. CoRR abs/1604.06573 (2016), http://arxiv. org/abs/1604.06573 2  9. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Semantic  compositional networks for visual captioning. In: CVPR (2017) 4, 13, 14  10. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fr\u00a8und, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., Memisevic, R.: The \u201dsomething something\u201d video database for learning and evaluating visual common sense. CoRR abs/1706.04261 (2017), http:// arxiv.org/abs/1706.04261 1, 9  11. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: Youtube2text: Recognizing and describing arbi- trary activities using semantic hierarchies and zero-shot recognition. In: 2013 IEEE International Conference on Computer Vision. pp. 2712-2719 (Dec 2013). https://doi.org/10.1109/ICCV.2013.337 9, 13  12. Hara, K., Kataoka, H., Satoh, Y.: Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? CoRR abs/1711.09577 (2017), http://arxiv.org/ abs/1711.09577 9  13. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: CVPR. pp. 961-970. IEEE Computer Society (2015), http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015. html#HeilbronEGN15 1  14. Hori, C., Hori, T., Lee, T.Y., Zhang, Z., Harsham, B., Hershey, J.R., Marks, T.K., Sumi, K.: Attention-based multimodal fusion for video description. In: 2017 IEEE International Conference on Computer Vision (ICCV). pp. 4203-4212 (Oct 2017). https://doi.org/10.1109/ICCV.2017.450 4, 14   ECO: E\ufb03cient Convolutional Network for Online Video UnderstandingReferences  1. Ballas, N., Yao, L., Pal, C., Courville, A.C.: Delving deeper into convolutional  networks for learning video representations. ICLR (2016) 14  2. Bilen, H., Fernando, B., Gavves, E., Vedaldi, A.: Action recognition with dynamic image networks. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1-1 (2017). https://doi.org/10.1109/TPAMI.2017.2769085 3  3. Bilen, H., Fernando, B., Gavves, E., Vedaldi, A., Gould, S.: Dynamic im- age networks for action recognition. In: 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 3034-3042 (June 2016). https://doi.org/10.1109/CVPR.2016.331 3  4. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the kinetics dataset. CoRR abs/1705.07750 (2017), http://arxiv.org/abs/1705. 07750 3, 9, 10  5. Diba, A., Fayyaz, M., Sharma, V., Karami, A.H., Arzani, M.M., Yousefzadeh, R., Gool, L.V.: Temporal 3d convnets: New architecture and transfer learning for video classification. CoRR abs/1711.08200 (2017), http://arxiv.org/abs/ 1711.08200 3, 9  6. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015) 3  7. Feichtenhofer, C., Pinz, A., Wildes, R.P.: Spatiotemporal multiplier net- works In: 2017 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). pp. 7445-7454 (July 2017). https://doi.org/10.1109/CVPR.2017.787 3  for video action recognition.  8. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fu- sion for video action recognition. CoRR abs/1604.06573 (2016), http://arxiv. org/abs/1604.06573 2  9. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Semantic  compositional networks for visual captioning. In: CVPR (2017) 4, 13, 14  10. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fr\u00a8und, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., Memisevic, R.: The \u201dsomething something\u201d video database for learning and evaluating visual common sense. CoRR abs/1706.04261 (2017), http:// arxiv.org/abs/1706.04261 1, 9  11. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: Youtube2text: Recognizing and describing arbi- trary activities using semantic hierarchies and zero-shot recognition. In: 2013 IEEE International Conference on Computer Vision. pp. 2712-2719 (Dec 2013). https://doi.org/10.1109/ICCV.2013.337 9, 13  12. Hara, K., Kataoka, H., Satoh, Y.: Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? CoRR abs/1711.09577 (2017), http://arxiv.org/ abs/1711.09577 9  13. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: CVPR. pp. 961-970. IEEE Computer Society (2015), http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015. html#HeilbronEGN15 1  14. Hori, C., Hori, T., Lee, T.Y., Zhang, Z., Harsham, B., Hershey, J.R., Marks, T.K., Sumi, K.: Attention-based multimodal fusion for video description. In: 2017 IEEE International Conference on Computer Vision (ICCV). pp. 4203-4212 (Oct 2017). https://doi.org/10.1109/ICCV.2017.450 4, 14   16  M. Zolfaghari, K. Singh and T. Brox  15. Hu, B., Yuan, J., Wu, Y.: Discriminative action states discovery for online ac- tion recognition. IEEE Signal Processing Letters 23(10), 1374-1378 (Oct 2016). https://doi.org/10.1109/LSP.2016.2598878 4  16. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: Flownet 2.0: Evolution of optical \ufb02ow estimation with deep networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Jul 2017), http://lmb. informatik.uni-freiburg.de//Publications/2017/IMKDB17 2  17. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network train- ing by reducing internal covariate shift. In: Proceedings of the 32Nd Interna- tional Conference on International Conference on Machine Learning - Volume 37. pp. 448-456. ICML\u201915, JMLR.org (2015), http://dl.acm.org/citation.cfm?id= 3045118.3045167 6, 7  18. Kantorov, V., Laptev, I.: E\ufb03cient feature extraction, encoding, and classification for action recognition. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition. pp. 2593-2600 (June 2014). https://doi.org/10.1109/CVPR.2014.332 4  19. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Proceed- ings of the 2014 IEEE Conference on Computer Vision and Pattern Recog- nition. pp. 1725-1732. CVPR \u201914, IEEE Computer Society, Washington, DC, USA (2014). https://doi.org/10.1109/CVPR.2014.223, http://dx.doi.org/10. 1109/CVPR.2014.223 3  20. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The kinetics human action video dataset. CoRR abs/1705.06950 (2017), http:// arxiv.org/abs/1705.06950 1, 9  21. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: a large video database for human motion recognition. In: Proceedings of the International Con- ference on Computer Vision (ICCV) (2011) 9  22. Kviatkovsky, I., Rivlin, E., Shimshoni, I.: Online action recognition using covari- ance of shape and motion. Comput. Vis. Image Underst. 129(C), 15-26 (Dec 2014). https://doi.org/10.1016/j.cviu.2014.08.001, http://dx.doi.org/10.1016/ j.cviu.2014.08.001 4  23. Lavie, A., Agarwal, A.: Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In: Proceedings of the Sec- ond Workshop on Statistical Machine Translation. pp. 228-231. StatMT \u201907, As- sociation for Computational Linguistics, Stroudsburg, PA, USA (2007), http: //dl.acm.org/citation.cfm?id=1626355.1626389 13  24. Lev, G., Sadeh, G., Klein, B., Wolf, L.: Rnn fisher vectors for action recognition and image annotation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision - ECCV 2016. pp. 833-850. Springer International Publishing, Cham (2016) 3  25. Li, Z., Gavrilyuk, K., Gavves, E., Jain, M., Snoek, C.G.: Videolstm convolves, attends and \ufb02ows for action recognition. Comput. Vis. Image Underst. 166(C), 41-50 (Jan 2018). https://doi.org/10.1016/j.cviu.2017.10.011, https://doi.org/ 10.1016/j.cviu.2017.10.011 3  26. Ng, J.Y.H., Hausknecht, M.J., Vijayanarasimhan, S., Vinyals, O., Monga, R., Toderici, G.: Beyond short snippets: Deep networks for video classification. In: CVPR. pp. 4694-4702. IEEE Computer Society (2015), http://dblp.uni-trier. de/db/conf/cvpr/cvpr2015.html#NgHVVMT15 3   ECO: E\ufb03cient Convolutional Network for Online Video Understanding27. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: A method for au- the 40th tomatic evaluation of machine translation. Annual Meeting on Association for Computational Linguistics. pp. 311- 318. ACL \u201902, Association for Computational Linguistics, Stroudsburg, PA, USA (2002). https://doi.org/10.3115/1073083.1073135, https://doi.org/10. 3115/1073083.1073135 13  In: Proceedings of  28. Qiu, Z., Yao, T., Mei, T.: Deep quantization: Encoding convolutional activations  with deep generative model. In: CVPR (2017) 3  29. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog- nition in videos. In: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1. pp. 568-576. NIPS\u201914, MIT Press, Cambridge, MA, USA (2014), http://dl.acm.org/citation.cfm?id=2968826. 2968890 3  30. Singh, G., Saha, S., Sapienza, M., Torr, P.H.S., Cuzzolin, F.: Online real-time multiple spatiotemporal action localisation and prediction. In: IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. pp. 3657-3666 (2017). https://doi.org/10.1109/ICCV.2017.393, https://doi.org/ 10.1109/ICCV.2017.393 1, 2, 4, 13  31. Soomro, K., Idrees, H., Shah, M.: Predicting the where and what of actors and actions through online action localization. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016) 4, 13  32. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR abs/1212.0402 (2012), http://arxiv. org/abs/1212.0402 9  33. Tran, D., Bourdev, L.D., Fergus, R., Torresani, L., Paluri, M.: C3D: generic features for video analysis. CoRR abs/1412.0767 (2014), http://arxiv.org/abs/1412. 0767 3  34. Tran, D., Ray, J., Shou, Z., Chang, S., Paluri, M.: Convnet architecture search for spatiotemporal feature learning. CoRR abs/1708.05038 (2017), http://arxiv. org/abs/1708.05038 3, 6, 9, 10  35. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look at spatiotemporal convolutions for action recognition. CoRR abs/1711.11248 (2017), http://arxiv.org/abs/1711.11248 3  36. Varol, G., Laptev, I., Schmid, C.: Long-term temporal convolutions for action recognition. CoRR abs/1604.04494 (2016), http://arxiv.org/abs/1604.04494 3  37. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image description evaluation. In: CVPR. pp. 4566-4575. IEEE Computer Society (2015), http:// dblp.uni-trier.de/db/conf/cvpr/cvpr2015.html#VedantamZP15 13  38. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence to sequence - video to text. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2015) 14  39. Wang, L., Li, W., Li, W., Gool, L.V.: Appearance-and-relation networks for video classification. CoRR abs/1711.09125 (2017), http://arxiv.org/abs/ 1711.09125 3, 6, 7, 9, 10  40. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Gool, L.V.: Temporal segment networks for action recognition in videos. CoRR abs/1705.02953 (2017), http://arxiv.org/abs/1705.02953 3  41. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Val Gool, L.: Temporal segment networks: Towards good practices for deep action recognition. In: ECCV (2016) 6, 7, 9, 10   18  M. Zolfaghari, K. Singh and T. Brox  42. Xu, Z., Yang, Y., Hauptmann, A.G.: A discriminative CNN video representation for event detection. CoRR abs/1411.4006 (2014), http://arxiv.org/abs/1411. 4006 3  43. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using hierarchical recurrent neural networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 4584-4593 (2016) 4, 14  44. Zhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H.: Real-time action recogni- tion with enhanced motion vector cnns. CoRR abs/1604.07669 (2016), http: //arxiv.org/abs/1604.07669 4, 10  45. Zhang, X., Gao, K., Zhang, Y., Zhang, D., Li, J., Tian, Q.: Task-driven dynamic fusion: Reducing ambiguity in video description. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6250-6258 (July 2017). https://doi.org/10.1109/CVPR.2017.662 4, 14  46. Zhou, B., Andonian, A., Torralba, A.: Temporal relational reasoning in videos.  CoRR abs/1711.08496 (2017), http://arxiv.org/abs/1711.08496 9  47. Zhu, J., Zou, W., Zhu, Z., Li, L.: End-to-end video-level representation learning for action recognition. CoRR abs/1711.04161 (2017), http://arxiv.org/abs/ 1711.04161 9  48. Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox, T.: Chained multi-stream net- works exploiting pose, motion, and appearance for action classification and de- tection. In: IEEE International Conference on Computer Vision (ICCV) (2017), http://lmb.informatik.uni-freiburg.de/Publications/2017/ZOSB17a 3"}