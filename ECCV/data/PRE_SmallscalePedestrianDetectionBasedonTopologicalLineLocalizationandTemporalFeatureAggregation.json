{"1": "1. Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: An evaluation  of the state of the art. In: PAMI (2012) 743-761   14  T. Song, L. Sun, D. Xie, H. Sun, S. Pu.80  .64  .50  .40  .30  .20  .10  .05  2.60% MS-CNN 2.26% RPN+BF 2.15% SDS-RCNN 2.08% UDN+ 0.99% TLL(MRF)+FGFA 0.72% TLL(MRF)+LSTM 0.67% TLL(MRF) 0.67% TLL 0.41% ADM 0.00% SA-FastRCNN.80  .64  .50  .40  .30  .20  .10  .05  53.75% UDN+ 51.83% SA-FastRCNN 50.88% SDS-RCNN 49.13% MS-CNN 33.15% F-DNN+SS 30.82% ADM 26.25% TLL 25.55% TLL(MRF) 24.39% TLL(MRF)+FGFA 22.92% TLL(MRF)+LSTM.80  .64  .50  .40  .30  .20  .10  .05  100.00% UDN+ 100.00% RPN+BF 100.00% SA-FastRCNN 97.23% MS-CNN 77.37% F-DNN+SS 74.53% ADM 68.03% TLL 67.69% TLL(MRF) 63.28% TLL(MRF)+FGFA 60.79% TLL(MRF)+LSTM  10-3  10-2  10-1101  10-3  10-2  10-1101  10-3  10-2  10-1101  false positives per image (a) Near  false positives per image  (b) Middle  false positives per image  (c) Far  Fig. 9. Comparison of our proposed TLL approach with some state-of-the-art methods on the Caltech dataset under Near, Middle, and Far evaluation protocols.  based feature aggregation from adjacent frames. It can be seen that for some instances with defocus, blurred boundary and extremely tiny scale, the output feature activations from single-shot network are feeble, or even disappeared. In contrast, Conv-LSTM e\ufb00ectively aggregates the adjacent frame information to the reference frame, resulted in more high-activated features, which benefits the detection of small-scale objects. Quantitative result of the RNN based TLL is listed in Table 1. Fig. 9 illustrates MR-FPPI curves for di\ufb00erent scales objects together with best performance benchmarks on the Caltech standard image test set. We also list the result of our TLL combined with FGFA [27]. Compared with FGFA, RNN based aggregation propagates temporal information in a hid- den strategy, which allows the network to transfer feature from nearby frames in a more self-driven way, and improves the comprehensive performance more significantly.  6 Conclusions  In this work, we design a FCN based network to locate the somatic topolog- ical line for detecting multi-scale pedestrian instances while introduce a post- processing scheme based on MRF to eliminate ambiguities in occlusion cases. A temporal feature aggregation scheme is integrated to propagate temporal cues across frames and further improves the detection performance. From this work we conclude that: 1) problem itself may reside in the very origin of learning pipeline and it is more appropriate to provide more discriminative and less am- biguous information other than to just feed more information for achieving a better classifier. 2)One should abstract annotations with a more representative methodology. We hope it can inspire more works that focus on intrinsically solv- ing with generic small-scale objects and heavily occlusion scenes.  References  1. Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: An evaluation  of the state of the art. In: PAMI (2012) 743-761   TLL-TFA Pedestrian Detection2. Dollar, P., Appel, R., Belongie, S., Perona, P.: Fast feature pyramids for object  detection. In: PAMI (2014)  3. Zhang, L., Lin, L., Liang, X., He, K.: detection? In: ECCV (2016) 443-457  Is faster r-cnn doing well for pedestrian  4. Li, J., Liang, X., Shen, S.M., Xu, T., Feng, J., Yan, S.: Scale-aware fast r-cnn for  pedestrian detection. In: Multimedia (2015)  5. Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N.: A unified multi-scale deep convolu-  tional neural network for fast object detection. In: ECCV (2016) 354-370  6. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object  detection with region proposal networks. In: NIPS (2015)  7. Dai, J., Li, Y., He, K., Sun, J.: R-fcn: Object detection via region-based fully  convolutional networks. In: NIPS (2016)  8. Balke, A., Pearl, J.: Probabilistic evaluation of counterfactual queries. In: AAAI  (1994) 230-237  9. Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep  convolutional neural networks. In: CVPR (2015) 833-841  10. Zhang, Y., Zhou, D., Chen, S., Gao, S., Ma, Y.: Single-image crowd counting via  multi-column convolutional neural network. In: CVPR (2016) 589-597  11. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation  using part a\ufb03nity fields. In: CVPR (2017) 7291-7299  12. Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C., Murphy, K.: Towards accurate multi-person pose estimation in the wild. In: CVPR (2017) 4903-4911  13. Li, C., Zhong, Q., Xie, D., Pu, S.: Skeleton-based action recognition with convo-  lutional neural networks. In: ICMEW (2017)  14. Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: A benchmark.  In: CVPR (2009) 304-311  15. Zhang, S., Benenson, R., Schiele, B.: Citypersons: A diverse dataset for pedestrian  detection. In: CVPR (2017) 3213-3221  16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu- rate object detection and semantic segmentation. In: CVPR (2014) 580-587 17. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,  real-time object detection. In: CVPR (2016) 779-788  18. Redmon, J., Farhadi, A.: Yolo9000: Better, faster, stronger.  In: CVPR (2017)  7263-7271  19. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:  Ssd: Single shot multibox detector. In: ECCV (2016) 21-37  20. Du, X., El-Khamy, M., Lee, J., Davis, L.: Fused dnn: A deep neural network fusion  approach to fast and robust pedestrian detection. In: WACV (2017)  21. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  22. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., et al.: Going deeper with convolutions.  In: CVPR (2016) 770-778  In: CVPR (2015) 1-9  23. Brazil, G., Yin, X., Liu, X.: Illuminating pedestrians via simultaneous detection  & segmentation. In: ICCV (2017) 4950-4959  24. Zhang, X., Cheng, L., Li, B., Hu, H.M.: Too far to see? not really!\u2014pedestrian arXiv preprint (2017) arXiv:  detection with scale-aware localization policy. 1709.00235  25. Zhang, S., Benenson, R., Omran, M., Hosang, J., Schiele, B.: How far are we from  solving pedestrian detection? In: CVPR (2016) 1259-1267   16  T. Song, L. Sun, D. Xie, H. Sun, S. Pu  26. Kang, K., Ouyang, W., Li, H., Wang, X.: Object detection from video tubelets  with convolutional neural networks. In: CVPR (2016) 817-825  27. Zhu, X., Wang, Y., Dai, J., Yuan, L., Wei, Y.: Flow-guided feature aggregation for  video object detection. In: ICCV (2017) 408-417  28. Dosovitskiy, A., Fischery, P., Ilg, E., et al.: Flownet: Learning optical \ufb02ow with  convolutional networks. In: ICCV (2015) 2758-2766  29. Liu, M., Zhu, M.: Mobile video object detection with temporally-aware feature  maps. arXiv preprint (2017) arXiv: 1711.06368  30. Hopcroft, J.E., Karp, R.M.: An n5/2 algorithm for maximum matching in bipartite  graphs. SIAM Journal on Computing (1973)  31. Kuhn, H.W.: The hungarian method for the assignment problem. 50 Years of  Integer Programming 1958-2008 29-47  32. Ng, J.Y.H., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O., Monga, R., Toderici, G.: Beyond short snippets: Deep networks for video classification. In: CVPR (2015) 4694-4702  33. Grushin, A., Monner, D.D., Reggia, J.A., Mishra, A.: Robust human action recog-  nition via long short-term memory. In: IJCNN (2013)  34. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wang, W., WOO, W.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. In: NIPS (2015) 802-810  35. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti  vision benchmark suite. In: CVPR (2012)  36. Cordts, M., Omran, M., Ramos, S., et al.: The cityscapes dataset for semantic  urban scene understanding. In: CVPR (2016)  37. Ouyang, W., Zhou, H., Li, H., et al.: Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection. In: PAMI (2017) 38. Wang, X., Xiao, T., Jiang, Y., Shao, S., Sun, J., Shen, C.: Repulsion loss: Detecting  pedestrians in a crowd. arXiv preprint (2017) arXiv: 1711.07752"}