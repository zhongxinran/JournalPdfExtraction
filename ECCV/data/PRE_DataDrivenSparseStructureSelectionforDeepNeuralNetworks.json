{"1": "(2016)  1. Alvarez, J.M., Salzmann, M.: Learning the number of neurons in deep networks. In: NIPS  2. Baker, B., Gupta, O., Naik, N., Raskar, R.: Designing neural network architectures using  reinforcement learning. In: ICLR (2017)  3. Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM journal on imaging sciences 2(1), 183-202 (2009)  4. Bengio, Y., Boulanger-Lewandowski, N., Pascanu, R.: Advances in optimizing recurrent net-  works. In: ICASSP (2013)  5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., Zhang, Z.: MXNet: A \ufb02exible and efficient machine learning library for heterogeneous distributed systems. In: NIPS Workshop (2015)  6. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. In: NIPS (2016)  7. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear structure  within convolutional networks for efficient evaluation. In: NIPS (2014)  8. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for efficient dnns. In: NIPS (2016) 9. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for efficient  10. Hassibi, B., Stork, D.G., et al.: Second order derivatives for network pruning: Optimal brain  neural network. In: NIPS (2015)  surgeon. In: NIPS (1993)  11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR  12. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: ECCV  13. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural networks. In:  14. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NIPS  (2016)  (2016)  ICCV (2017)  Workshop (2014)  15. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applica- tions. arXiv preprint arXiv:1704.04861 (2017)  16. Hu, H., Peng, R., Tai, Y.W., Tang, C.K.: Network trimming: A data-driven neuron pruning  approach towards efficient deep architectures. arXiv:1607.03250 (2016)  17. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional  networks. In: CVPR (2017)  18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.: Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. arXiv:1602.07360 (2016)  19. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing  20. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural networks with  21. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Tech  internal covariate shift. ICML (2015)  low rank expansions. BMVC (2014)  Report (2009)  neural networks. In: NIPS (2012)  22. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional   Data-Driven Sparse Structure Selection for Deep Neural NetworksReferences  (2016)  1. Alvarez, J.M., Salzmann, M.: Learning the number of neurons in deep networks. In: NIPS  2. Baker, B., Gupta, O., Naik, N., Raskar, R.: Designing neural network architectures using  reinforcement learning. In: ICLR (2017)  3. Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM journal on imaging sciences 2(1), 183-202 (2009)  4. Bengio, Y., Boulanger-Lewandowski, N., Pascanu, R.: Advances in optimizing recurrent net-  works. In: ICASSP (2013)  5. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., Zhang, Z.: MXNet: A \ufb02exible and efficient machine learning library for heterogeneous distributed systems. In: NIPS Workshop (2015)  6. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. In: NIPS (2016)  7. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear structure  within convolutional networks for efficient evaluation. In: NIPS (2014)  8. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for efficient dnns. In: NIPS (2016) 9. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for efficient  10. Hassibi, B., Stork, D.G., et al.: Second order derivatives for network pruning: Optimal brain  neural network. In: NIPS (2015)  surgeon. In: NIPS (1993)  11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR  12. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: ECCV  13. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural networks. In:  14. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NIPS  (2016)  (2016)  ICCV (2017)  Workshop (2014)  15. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applica- tions. arXiv preprint arXiv:1704.04861 (2017)  16. Hu, H., Peng, R., Tai, Y.W., Tang, C.K.: Network trimming: A data-driven neuron pruning  approach towards efficient deep architectures. arXiv:1607.03250 (2016)  17. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional  networks. In: CVPR (2017)  18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.: Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. arXiv:1602.07360 (2016)  19. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing  20. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural networks with  21. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Tech  internal covariate shift. ICML (2015)  low rank expansions. BMVC (2014)  Report (2009)  neural networks. In: NIPS (2012)  22. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional   16  Z. Huang, N. Wang  NIPS (1990)  ConvNets. In: ICLR (2017)  works. In: CVPR (2015)  23. LeCun, Y., Denker, J.S., Solla, S.A., Howard, R.E., Jackel, L.D.: Optimal brain damage. In:  24. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient  25. Liu, B., Wang, M., Foroosh, H., Tappen, M., Pensky, M.: Sparse convolutional neural net-  26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolutional  networks through network slimming. In: ICCV (2017)  27. Luo, J.H., Wu, J., Lin, W.: ThiNet: A filter level pruning method for deep neural network  compression. In: ICCV (2017)  28. Mariet, Z., Sra, S.: Diversity networks. In: ICLR (2016) 29. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J.: Pruning convolutional neural net-  works for resource efficient inference. In: ICLR (2017)  30. Parikh, N., Boyd, S., et al.: Proximal algorithms. Foundations and Trends R (cid:13)  in Optimization  1(3), 127-239 (2014)  31. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: XNOR-Net: ImageNet classification  using binary convolutional neural networks. In: ECCV (2016)  32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Le, Q., Kurakin, A.: Large-scale  evolution of image classifiers. In: ICML (2017)  33. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: FitNets: Hints for  thin deep nets. In: ICLR (2015)  34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: ImageNet large scale visual recognition challenge. Interna- tional Journal of Computer Vision 115(3), 211-252 (2015)  35. Sergey, Z., Nikos, K.: Paying more attention to attention: Improving the performance of  convolutional neural networks via attention transfer. In: ICLR (2017)  36. Shen, F., Gan, R., Zeng, G.: Weighted residuals for very deep networks. In: ICSAI (2016) 37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-  38. Srinivas, S., Babu, R.V.: Learning neural network architectures using backpropagation. In:  nition. In: ICLR (2015)  BMVC (2016)  39. Srivastava, R.K., Greff, K., Schmidhuber, J.: Highway networks. In: ICML (2015) 40. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization and mo-  mentum in deep learning. In: ICML (2013)  41. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of relatively  42. Wang, R.J., Li, X., Ao, S., Ling, C.X.: Pelee: A real-time object detection system on mobile  43. Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in deep neural  44. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural networks for  shallow networks. In: NIPS (2016)  devices. In: ICLR Workshop (2018)  networks. In: NIPS (2016)  mobile devices. In: CVPR (2016)  45. Xiangyu, Z., , Xinyu, Z., Mengxiao, L., Jian, S.: Shuf\ufb02enet: An extremely efficient convolu-  tional neural network for mobile devices. In: BMVC (2016)  46. Xie, L., Yuille, A.: Genetic CNN. In: ICCV (2017) 47. Xie, S., Girshick, R., Doll\u00b4ar, P., Tu, Z., He, K.: Aggregated residual transformations for deep  neural networks. In: CVPR (2017)  48. Ye, J., Lu, X., Lin, Z., Wang, J.Z.: Rethinking the smaller-norm-less-informative assumption  in channel pruning of convolution layers. In: ICLR (2018)  49. Zhang, X., Zou, J., Ming, X., He, K., Sun, J.: Efficient and accurate approximations of non-  linear convolutional networks. In: CVPR (2015)   Data-Driven Sparse Structure Selection for Deep Neural Networks50. Zhou, H., Alvarez, J.M., Porikli, F.: Less is more: Towards compact cnns. In: ECCV (2016) 51. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. In: ICLR (2017)"}