{"1": "(2017)  1. Andrew, G., Arora, R., Bilmes, J., Livescu, K.: Deep canonical correlation analysis.  In: Proc. ICML. pp. 1247-1255. PMLR (2013)  2. Arandjelovic, R., Zisserman, A.: Look, listen and learn. In: Proc. ICCV. IEEE  3. Arandjelovi\u00b4c, R., Zisserman, A.: Objects that sound. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer Inter- national Publishing (2018)  4. Arevalo, J., Solorio, T., Montes-y G\u00b4omez, M., Gonz\u00b4alez, F.A.: Gated multimodal  units for information fusion. In: Proc. ICLR workshop (2017)  5. Assael, Y.M., Shillingford, B., Whiteson, S., de Freitas, N.: Lipnet: Sentence-level  lipreading. CoRR abs/1611.01599 (2016)  6. Aytar, Y., Vondrick, C., Torralba, A.: Soundnet: Learning sound representations  from unlabeled video. In: Proc. NIPS. Curran Associates, Inc. (2016)  7. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: Proc. ICLR (2015)  8. Baltru\u02c7saitis, T., Ahuja, C., Morency, L.P.: Multimodal machine learning: A survey  and taxonomy. IEEE TPAMI (2018)  9. Bulkin, D.A., Groh, J.M.: Seeing sounds: visual and auditory interactions in the  brain. Current opinion in neurobiology 16(4), 415-419 (2006)  10. Cakir, E., Heittola, T., Huttunen, H., Virtanen, T.: Polyphonic sound event detec- tion using multi label deep neural networks. In: Proc. IJCNN. IEEE (2015) 11. Chen, L., Srivastava, S., Duan, Z., Xu, C.: Deep cross-modal audio-visual genera-  tion. In: Proc. ACMMM Workshop. ACM (2017)  12. Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Lip reading sentences in the  wild. In: Proc. CVPR. IEEE (2017)  13. Ephrat, A., Mosseri,  I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., Freeman, W.T., Rubinstein, M.: Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619 (2018)  14. Escorcia, V., Caba Heilbron, F., Niebles, J.C., Ghanem, B.: Daps: Deep action proposals for action understanding. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision - ECCV 2016. Springer International Publishing (2016)  15. Fisher III, J.W., Darrell, T., Freeman, W.T., Viola, P.A.: Learning joint statistical models for audio-visual fusion and segregation. In: Proc. NIPS. Curran Associates, Inc. (2001)  16. Gao, R., Feris, R., Grauman, K.: Learning to separate object sounds by watching unlabeled video. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer International Publishing (2018)  17. Gaver, W.W.: What in the world do we hear?: An ecological approach to auditory  event perception. Ecological psychology 5(1), 1-29 (1993)  18. Gemmeke, J.F., Ellis, D.P., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: Proc. ICASSP. IEEE (2017)  19. G\u00a8onen, M., Alpayd\u0131n, E.: Multiple kernel learning algorithms. JMLR 12(Jul),  2211-2268 (2011)  20. Gurban, M., Thiran, J.P., Drugman, T., Dutoit, T.: Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition. In: Proc. ICMI. ACM (2008)   Audio-Visual Event Localization in Unconstrained VideosReferences  (2017)  1. Andrew, G., Arora, R., Bilmes, J., Livescu, K.: Deep canonical correlation analysis.  In: Proc. ICML. pp. 1247-1255. PMLR (2013)  2. Arandjelovic, R., Zisserman, A.: Look, listen and learn. In: Proc. ICCV. IEEE  3. Arandjelovi\u00b4c, R., Zisserman, A.: Objects that sound. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer Inter- national Publishing (2018)  4. Arevalo, J., Solorio, T., Montes-y G\u00b4omez, M., Gonz\u00b4alez, F.A.: Gated multimodal  units for information fusion. In: Proc. ICLR workshop (2017)  5. Assael, Y.M., Shillingford, B., Whiteson, S., de Freitas, N.: Lipnet: Sentence-level  lipreading. CoRR abs/1611.01599 (2016)  6. Aytar, Y., Vondrick, C., Torralba, A.: Soundnet: Learning sound representations  from unlabeled video. In: Proc. NIPS. Curran Associates, Inc. (2016)  7. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: Proc. ICLR (2015)  8. Baltru\u02c7saitis, T., Ahuja, C., Morency, L.P.: Multimodal machine learning: A survey  and taxonomy. IEEE TPAMI (2018)  9. Bulkin, D.A., Groh, J.M.: Seeing sounds: visual and auditory interactions in the  brain. Current opinion in neurobiology 16(4), 415-419 (2006)  10. Cakir, E., Heittola, T., Huttunen, H., Virtanen, T.: Polyphonic sound event detec- tion using multi label deep neural networks. In: Proc. IJCNN. IEEE (2015) 11. Chen, L., Srivastava, S., Duan, Z., Xu, C.: Deep cross-modal audio-visual genera-  tion. In: Proc. ACMMM Workshop. ACM (2017)  12. Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Lip reading sentences in the  wild. In: Proc. CVPR. IEEE (2017)  13. Ephrat, A., Mosseri,  I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., Freeman, W.T., Rubinstein, M.: Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619 (2018)  14. Escorcia, V., Caba Heilbron, F., Niebles, J.C., Ghanem, B.: Daps: Deep action proposals for action understanding. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision - ECCV 2016. Springer International Publishing (2016)  15. Fisher III, J.W., Darrell, T., Freeman, W.T., Viola, P.A.: Learning joint statistical models for audio-visual fusion and segregation. In: Proc. NIPS. Curran Associates, Inc. (2001)  16. Gao, R., Feris, R., Grauman, K.: Learning to separate object sounds by watching unlabeled video. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer International Publishing (2018)  17. Gaver, W.W.: What in the world do we hear?: An ecological approach to auditory  event perception. Ecological psychology 5(1), 1-29 (1993)  18. Gemmeke, J.F., Ellis, D.P., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: Proc. ICASSP. IEEE (2017)  19. G\u00a8onen, M., Alpayd\u0131n, E.: Multiple kernel learning algorithms. JMLR 12(Jul),  2211-2268 (2011)  20. Gurban, M., Thiran, J.P., Drugman, T., Dutoit, T.: Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition. In: Proc. ICMI. ACM (2008)   16  Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu  21. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality reduction by learning an in-  variant mapping. In: Proc. CVPR. IEEE (2006)  22. Harwath, D., Torralba, A., Glass, J.: Unsupervised learning of spoken language  with visual context. In: Proc. NIPS. Curran Associates, Inc. (2016)  23. Heittola, T., Mesaros, A., Eronen, A., Virtanen, T.: Context-dependent sound event detection. EURASIP Journal on Audio, Speech, and Music Processing 2013(1), 1 (2013)  24. Hershey, J.R., Movellan, J.R.: Audio vision: Using audio-visual synchrony to locate  sounds. In: Proc. NIPS. Curran Associates, Inc. (2000)  25. Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for large-scale audio classification. In: Proc. ICASSP. IEEE (2017)  26. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation  9(8), 1735-1780 (1997)  27. Hu, D., Li, X., et al.: Temporal multimodal learning in audiovisual speech recog-  nition. In: Proc. CVPR. IEEE (2016)  28. Kiela, D., Grave, E., Joulin, A., Mikolov, T.: E\ufb03cient large-scale multi-modal  classification. In: Proc. AAAI. AAAI Press (2018)  29. Kim, J.H., Lee, S.W., Kwak, D., Heo, M.O., Kim, J., Ha, J.W., Zhang, B.T.: Multimodal residual learning for visual qa. In: Proc. NIPS. Curran Associates, Inc. (2016)  30. Lea et al., C.: Temporal convolutional networks for action segmentation and de-  tection. In: Proc. CVPR. IEEE (2017)  31. LeCun, Y., Bottou, L., Bengio, Y., Ha\ufb00ner, P.: Gradient-based learning applied to  document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)  32. Li, B., Xu, C., Duan, Z.: Audio-visual source association for string ensembles  through multi-modal vibrato analysis. In: Proc. SMC (2017)  33. Lin, M., Chen, Q., Yan, S.: Network in network. arXiv preprint arXiv:1312.4400  (2013)  34. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive attention  via a visual sentinel for image captioning. In: Proc. CVPR. IEEE (2017)  35. Maron, O., Lozano-P\u00b4erez, T.: A framework for multiple-instance learning. In: Proc.  NIPS. Curran Associates, Inc. (1998)  36. Mesaros, A., Heittola, T., Virtanen, T.: Tut database for acoustic scene classifica-  tion and sound event detection. In: Proc. EUSIPCO. IEEE (2016)  37. Mroueh, Y., Marcheret, E., Goel, V.: Deep multimodal learning for audio-visual  speech recognition. In: Proc. ICASSP. IEEE (2015)  38. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep  learning. In: Proc. ICML. PMLR (2011)  39. Oneata, D., Verbeek, J., Schmid, C.: Action and event recognition with fisher  vectors on a compact feature set. In: Proc. ICCV. IEEE (2013)  40. Owens, A., Efros, A.A.: Audio-visual scene analysis with self-supervised multi- sensory features. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer International Publishing (2018)  41. Owens, A., Isola, P., McDermott, J., Torralba, A., Adelson, E.H., Freeman, W.T.:  Visually indicated sounds. In: Proc. CVPR. IEEE (2016)  42. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient sound provides supervision for visual learning. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision - ECCV 2016. Springer International Publishing (2016) 43. Parascandolo, G., Huttunen, H., Virtanen, T.: Recurrent neural networks for poly- phonic sound event detection in real life recordings. In: Proc. ICASSP. IEEE (2016)   Audio-Visual Event Localization in Unconstrained Videos44. Poria, S., Cambria, E., Gelbukh, A.: Deep convolutional neural network textual fea- tures and multiple kernel learning for utterance-level multimodal sentiment anal- ysis. In: Proc. EMNLP. Association for Computational Linguistics (2015)  45. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. IJCV 115(3), 211-252 (2015)  46. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE TSP  45(11), 2673-2681 (1997)  47. Senocak, A., Oh, T.H., Kim, J., Yang, M.H., Kweon, I.S.: Learning to localize  sound source in visual scenes. In: Proc. CVPR. IEEE (2018)  48. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos  via multi-stage cnns. In: Proc. CVPR. IEEE (2016)  49. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. In: Proc. ICLR (2015)  50. Srivastava, N., Salakhutdinov, R.: Learning representations for multimodal data  with deep belief nets. In: Proc. ICML workshop. PMLR (2012)  51. Srivastava, N., Salakhutdinov, R.R.: Multimodal learning with deep boltzmann  machines. In: Proc. NIPS. Curran Associates, Inc. (2012)  52. Sur\u00b4\u0131s, D., Duarte, A., Salvador, A., Torres, J., Gir\u00b4o-i Nieto, X.: Cross-modal em- beddings for video and audio retrieval. arXiv preprint arXiv:1801.02200 (2018) 53. Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: Movieqa: Understanding stories in movies through question-answering. In: Proc. CVPR. IEEE (2016)  54. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem- poral features with 3d convolutional networks. In: Proc. ICCV. IEEE (2015) 55. Wu, J., Yu, Y., Huang, C., Yu, K.: Deep multiple instance learning for image  classification and auto-annotation. In: Proc. CVPR. IEEE (2015)  56. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for  bridging video and language. In: Proc. CVPR. IEEE (2016)  57. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: Proc. ICLR (2015)  58. Yang, X., Ramesh, P., Chitta, R., Madhvanath, S., Bernal, E.A., Luo, J.: Deep multimodal representation learning from temporal data. In: Proc. CVPR. IEEE (2017)  59. Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., Torralba, A.: The sound of pixels. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision - ECCV 2018. Springer International Publishing (2018)  60. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin, D.: Temporal action detection  with structured segment networks. In: Proc. ICCV. IEEE (2017)  61. Zhou, Y., Wang, Z., Fang, C., Bui, T., Berg, T.L.: Visual to sound: Generating  natural sound for videos in the wild. In: CVPR. IEEE (2018)"}