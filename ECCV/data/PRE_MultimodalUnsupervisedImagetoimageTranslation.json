{"1": "1. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for  image super-resolution. In: ECCV. (2014)  2. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. (2016) 3. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-  coders: Feature learning by inpainting. In: CVPR. (2016)  4. La\ufb00ont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient attributes for high-  level understanding and editing of outdoor scenes. TOG (2014)  5. Gatys, L.A., Ecker, A.S., Bethge, M.: neural networks. In: CVPR. (2016)  Image style transfer using convolutional  6. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-  ditional adversarial networks. In: CVPR. (2017)  7. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual learning for  image-to-image translation. In: ICCV. (2017)  8. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation  using cycle-consistent adversarial networks. In: ICCV. (2017)  9. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain  relations with generative adversarial networks. In: ICML. (2017)  10. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation.  In: ICLR. (2017)  11. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,  E.: Toward multimodal image-to-image translation. In: NIPS. (2017)  12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: NIPS. (2016) 13. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement  networks. In: ICCV. (2017)  14. Liang, X., Zhang, H., Xing, E.P.: Generative semantic manipulation with contrast-  ing gan. arXiv preprint arXiv:1708.00315 (2017)  15. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-  works. In: NIPS. (2017)  16. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: NIPS. (2017) 17. Royer, A., Bousmalis, K., Gouws, S., Bertsch, F., Moressi, I., Cole, F., Murphy, K.: Xgan: Unsupervised image-to-image translation for many-to-many mappings. arXiv preprint arXiv:1711.05139 (2017)  18. Gan, Z., Chen, L., Wang, W., Pu, Y., Zhang, Y., Liu, H., Li, C., Carin, L.: Triangle  generative adversarial networks. In: NIPS. (2017) 5253-5262  19. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Unified genera- tive adversarial networks for multi-domain image-to-image translation. In: CVPR. (2018)  20. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High- resolution image synthesis and semantic manipulation with conditional gans. In: CVPR. (2018)  21. Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning from simulated and unsupervised images through adversarial training. In: CVPR. (2017)  22. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised In: CVPR.  pixel-level domain adaptation with generative adversarial networks. (2017)  23. Wolf, L., Taigman, Y., Polyak, A.: Unsupervised creation of parameterized avatars.  In: ICCV. (2017)   Multimodal Unsupervised Image-to-Image TranslationReferences  1. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for  image super-resolution. In: ECCV. (2014)  2. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. (2016) 3. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-  coders: Feature learning by inpainting. In: CVPR. (2016)  4. La\ufb00ont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient attributes for high-  level understanding and editing of outdoor scenes. TOG (2014)  5. Gatys, L.A., Ecker, A.S., Bethge, M.: neural networks. In: CVPR. (2016)  Image style transfer using convolutional  6. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-  ditional adversarial networks. In: CVPR. (2017)  7. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual learning for  image-to-image translation. In: ICCV. (2017)  8. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation  using cycle-consistent adversarial networks. In: ICCV. (2017)  9. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain  relations with generative adversarial networks. In: ICML. (2017)  10. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation.  In: ICLR. (2017)  11. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,  E.: Toward multimodal image-to-image translation. In: NIPS. (2017)  12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: NIPS. (2016) 13. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement  networks. In: ICCV. (2017)  14. Liang, X., Zhang, H., Xing, E.P.: Generative semantic manipulation with contrast-  ing gan. arXiv preprint arXiv:1708.00315 (2017)  15. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-  works. In: NIPS. (2017)  16. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: NIPS. (2017) 17. Royer, A., Bousmalis, K., Gouws, S., Bertsch, F., Moressi, I., Cole, F., Murphy, K.: Xgan: Unsupervised image-to-image translation for many-to-many mappings. arXiv preprint arXiv:1711.05139 (2017)  18. Gan, Z., Chen, L., Wang, W., Pu, Y., Zhang, Y., Liu, H., Li, C., Carin, L.: Triangle  generative adversarial networks. In: NIPS. (2017) 5253-5262  19. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Unified genera- tive adversarial networks for multi-domain image-to-image translation. In: CVPR. (2018)  20. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High- resolution image synthesis and semantic manipulation with conditional gans. In: CVPR. (2018)  21. Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning from simulated and unsupervised images through adversarial training. In: CVPR. (2017)  22. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised In: CVPR.  pixel-level domain adaptation with generative adversarial networks. (2017)  23. Wolf, L., Taigman, Y., Polyak, A.: Unsupervised creation of parameterized avatars.  In: ICCV. (2017)   16  Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz  24. TAU, T.G., Wolf, L., TAU, S.B.: The role of minimal complexity functions in  unsupervised learning of semantic mappings. In: ICLR. (2018)  25. Hoshen, Y., Wolf, L.: Identifying analogies across domains. In: ICLR. (2018) 26. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond  mean square error. In: ICLR. (2016)  27. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,  Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)  28. Denton, E.L., Chintala, S., Fergus, R.: Deep generative image models using a  laplacian pyramid of adversarial networks. In: NIPS. (2015)  29. Wang, X., Gupta, A.: Generative image modeling using style and structure adver-  sarial networks. In: ECCV. (2016)  30. Yang, J., Kannan, A., Batra, D., Parikh, D.: Lr-gan: Layered recursive generative  adversarial networks for image generation. In: ICLR. (2017)  31. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative  adversarial networks. In: CVPR. (2017)  32. Zhang, H., Xu, T., Li, H., Zhang, S., Huang, X., Wang, X., Metaxas, D.: Stack- gan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV. (2017)  33. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im-  proved quality, stability, and variation. In: ICLR. (2018)  34. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:  Improved techniques for training gans. In: NIPS. (2016)  35. Zhao, J., Mathieu, M., LeCun, Y.: Energy-based generative adversarial network.  36. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.  In: ICLR. (2017)  In: ICML. (2017)  37. Berthelot, D., Schumm, T., Metz, L.: Began: Boundary equilibrium generative  adversarial networks. arXiv preprint arXiv:1703.10717 (2017)  38. Mao, X., Li, Q., Xie, H., Lau, Y.R., Wang, Z., Smolley, S.P.: Least squares gener-  ative adversarial networks. In: ICCV. (2017)  39. Tolstikhin, I., Bousquet, O., Gelly, S., Schoelkopf, B.: Wasserstein auto-encoders.  In: ICLR. (2018)  40. Larsen, A.B.L., S\u00f8nderby, S.K., Larochelle, H., Winther, O.: Autoencoding beyond  pixels using a learned similarity metric. In: ICML. (2016)  41. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics  based on deep networks. In: NIPS. (2016)  42. Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S.: Variational arXiv preprint  approaches for auto-encoding generative adversarial networks. arXiv:1706.04987 (2017)  43. Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., Carin, L.: Alice: Towards un- derstanding adversarial learning for joint distribution matching. In: NIPS. (2017) 44. Srivastava, A., Valkoz, L., Russell, C., Gutmann, M.U., Sutton, C.: Veegan: Re- ducing mode collapse in gans using implicit variational learning. In: NIPS. (2017) 45. Ghosh, A., Kulharia, V., Namboodiri, V., Torr, P.H., Dokania, P.K.: Multi-agent  diverse generative adversarial networks. arXiv preprint arXiv:1704.02906 (2017)  46. Bansal, A., Sheikh, Y., Ramanan, D.: Pixelnn: Example-based image synthesis.  In: ICLR. (2018)  47. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint arXiv:1802.10151 (2018)   Multimodal Unsupervised Image-to-Image Translation48. Lee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M.K., Yang, M.H.: Diverse image-to-  image translation via disentangled representation. In: ECCV. (2018)  49. Anoosheh, A., Agustsson, E., Timofte, R., Van Gool, L.: Combogan: Unrestrained scalability for image domain translation. arXiv preprint arXiv:1712.06909 (2017) 50. Hui, L., Li, X., Chen, J., He, H., Yang, J., et al.: Unsupervised multi- domain image translation with domain-specific encoders/decoders. arXiv preprint arXiv:1712.02050 (2017)  51. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-  gies. In: SIGGRAPH. (2001)  52. Li, C., Wand, M.: Combining markov random fields and convolutional neural  networks for image synthesis. In: CVPR. (2016)  53. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer  and super-resolution. In: ECCV. (2016)  54. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance  normalization. In: ICCV. (2017)  55. Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer  via feature transforms. In: NIPS. (2017) 385-395  56. Li, Y., Liu, M.Y., Li, X., Yang, M.H., Kautz, J.: A closed-form solution to photo-  realistic image stylization. In: ECCV. (2018)  57. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info- gan: Interpretable representation learning by information maximizing generative adversarial nets. In: NIPS. (2016)  58. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., Lerchner, A.: beta-vae: Learning basic visual concepts with a constrained vari- ational framework. In: ICLR. (2017)  59. Tenenbaum, J.B., Freeman, W.T.: Separating style and content. In: NIPS. (1997) 60. Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., Erhan, D.: Domain  separation networks. In: NIPS. (2016)  61. Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content  for natural video sequence prediction. In: ICLR. (2017)  62. Mathieu, M.F., Zhao, J.J., Zhao, J., Ramesh, A., Sprechmann, P., LeCun, Y.: Disentangling factors of variation in deep representation using adversarial training. In: NIPS. (2016)  63. Denton, E.L., et al.: Unsupervised learning of disentangled representations from  video. In: NIPS. (2017)  64. Tulyakov, S., Liu, M.Y., Yang, X., Kautz, J.: Mocogan: Decomposing motion and  content for video generation. In: CVPR. (2018)  65. Donahue, C., Balsubramani, A., McAuley, J., Lipton, Z.C.: Semantically decom- posing the latent spaces of generative adversarial networks. In: ICLR. (2018) 66. Shen, T., Lei, T., Barzilay, R., Jaakkola, T.: Style transfer from non-parallel text by cross-alignment. In: Advances in Neural Information Processing Systems. (2017) 6833-6844  67. Donahue, J., Kr\u00a8ahenb\u00a8uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR.  (2017)  68. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,  Courville, A.: Adversarially learned inference. In: ICLR. (2017)  69. Automatic di\ufb00erentiation in PyTorch. In: NIPS Autodi\ufb00 Workshop. (2017) 70. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR. (2016)   18  Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz  71. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In: CVPR. (2017)  72. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.  In: ICLR. (2017)  73. Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: Zm-net: Real-time zero-  shot image manipulation network. arXiv preprint arXiv:1703.07255 (2017)  74. Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure of a real-time, arbitrary neural artistic stylization network. In: BMVC. (2017) 75. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. In: ICLR. (2015)  76. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for  practical domain adaptation. arXiv preprint arXiv:1603.04779 (2016)  77. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable  e\ufb00ectiveness of deep features as a perceptual metric. In: CVPR. (2018)  78. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in neural information processing systems. (2012)  79. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the incep-  tion architecture for computer vision. In: CVPR. (2016)  80. Yu, A., Grauman, K.: Fine-grained visual comparisons with local learning.  In:  CVPR. (2014)  81. Zhu, J.Y., Kr\u00a8ahenb\u00a8uhl, P., Shechtman, E., Efros, A.A.: Generative visual manip-  ulation on the natural image manifold. In: ECCV. (2016)  82. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV. (2015) 83. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In: CVPR. (2016)  84. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR. (2016)"}