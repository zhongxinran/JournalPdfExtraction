{"1": "1. Kubricht, J., Jiang, C., Zhu, Y., Zhu, S.C., Terzopoulos, D., Lu, H.: Probabilistic simulation predicts human performance on viscous \ufb02uid-pouring problem. CogSci (2016)  2. Bates, C.J., Yildirim, I., Tenenbaum, J.B., Battaglia, P.W.: Humans predict liquid  dynamics using probabilistic simulation. CogSci (2015)  3. Edmonds, M., Gao, F., Xie, X., Liu, H., Qi, S., Zhu, Y., Rothrock, B., Zhu, S.C.: Feeling the force: Integrating force and pose for \ufb02uent discovery through imitation learning to open medicine bottles. IROS (2017)  4. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark. arXiv:1609.08675 (2016)  5. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale  video benchmark for human activity understanding. In: CVPR. (2015)  6. Alayrac, J.B., Sivic, J., Laptev, I., Lacoste-Julien, S.: Joint discovery of object  states and manipulating actions. In: ICCV. (2017)  7. Mottaghi, R., Schenck, C., Fox, D., Farhadi, A.: See the glass half full: Reasoning  about liquid containers, their volume and content. In: ICCV. (2017)  8. Nishida, N., Nakayama, H.: Multimodal gesture recognition using multi-stream  recurrent neural network. In: PSIVT. (2015)  9. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes  from videos in the wild. arXiv:1212.0402 (2012)  10. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: Hmdb: a large video  database for human motion recognition. In: ICCV. (2011)  11. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A video dataset of spatio-temporally localized atomic visual actions. In: CVPR. (2018) 12. Rohrbach, M., Amin, S., Andriluka, M., Schiele, B.: A database for fine grained  activity detection of cooking activities. In: CVPR. (2012)  13. Ch\u00b4eron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based cnn features for action  14. Jhuang, H., Gall, J., Zu\ufb03, S., Schmid, C., Black, M.J.: Towards understanding  recognition. In: ICCV. (2015)  action recognition. In: ICCV. (2013)  15. Vu, T.H., Olsson, C., Laptev, I., Oliva, A., Sivic, J.: Predicting actions from static  16. Zhang, Y., Qu, W., Wang, D.: Action-scene model for human action recognition  scenes. In: ECCV. (2014)  from videos. (2014)  17. Moore, D.J., Essa, I.A., Hayes, M.H.: Exploiting human actions and object context  for recognition tasks. In: ICCV. (1999)  18. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action  recognition in still images. In: NIPS. (2011)  19. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using  spatial and functional compatibility for recognition. TPAMI (2009)  20. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action  understanding and object perception. In: CVPR. (2007)  21. Fathi, A., Rehg, J.M.: Modeling actions through state changes. In: CVPR. (2013) 22. Bambach, S., Lee, S., Crandall, D.J., Yu, C.: Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In: ICCV. (2015)   Liquid Pouring Monitoring via Rich Sensory InputsReferences  1. Kubricht, J., Jiang, C., Zhu, Y., Zhu, S.C., Terzopoulos, D., Lu, H.: Probabilistic simulation predicts human performance on viscous \ufb02uid-pouring problem. CogSci (2016)  2. Bates, C.J., Yildirim, I., Tenenbaum, J.B., Battaglia, P.W.: Humans predict liquid  dynamics using probabilistic simulation. CogSci (2015)  3. Edmonds, M., Gao, F., Xie, X., Liu, H., Qi, S., Zhu, Y., Rothrock, B., Zhu, S.C.: Feeling the force: Integrating force and pose for \ufb02uent discovery through imitation learning to open medicine bottles. IROS (2017)  4. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark. arXiv:1609.08675 (2016)  5. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale  video benchmark for human activity understanding. In: CVPR. (2015)  6. Alayrac, J.B., Sivic, J., Laptev, I., Lacoste-Julien, S.: Joint discovery of object  states and manipulating actions. In: ICCV. (2017)  7. Mottaghi, R., Schenck, C., Fox, D., Farhadi, A.: See the glass half full: Reasoning  about liquid containers, their volume and content. In: ICCV. (2017)  8. Nishida, N., Nakayama, H.: Multimodal gesture recognition using multi-stream  recurrent neural network. In: PSIVT. (2015)  9. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes  from videos in the wild. arXiv:1212.0402 (2012)  10. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: Hmdb: a large video  database for human motion recognition. In: ICCV. (2011)  11. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A video dataset of spatio-temporally localized atomic visual actions. In: CVPR. (2018) 12. Rohrbach, M., Amin, S., Andriluka, M., Schiele, B.: A database for fine grained  activity detection of cooking activities. In: CVPR. (2012)  13. Ch\u00b4eron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based cnn features for action  14. Jhuang, H., Gall, J., Zu\ufb03, S., Schmid, C., Black, M.J.: Towards understanding  recognition. In: ICCV. (2015)  action recognition. In: ICCV. (2013)  15. Vu, T.H., Olsson, C., Laptev, I., Oliva, A., Sivic, J.: Predicting actions from static  16. Zhang, Y., Qu, W., Wang, D.: Action-scene model for human action recognition  scenes. In: ECCV. (2014)  from videos. (2014)  17. Moore, D.J., Essa, I.A., Hayes, M.H.: Exploiting human actions and object context  for recognition tasks. In: ICCV. (1999)  18. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action  recognition in still images. In: NIPS. (2011)  19. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using  spatial and functional compatibility for recognition. TPAMI (2009)  20. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action  understanding and object perception. In: CVPR. (2007)  21. Fathi, A., Rehg, J.M.: Modeling actions through state changes. In: CVPR. (2013) 22. Bambach, S., Lee, S., Crandall, D.J., Yu, C.: Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In: ICCV. (2015)   16  T.Y. Wu\u22c6, J.T. Lin\u22c6, T.H. Wang, C.W. Hu, J.C. Niebles, M. Sun  23. Ma, M., Fan, H., Kitani, K.M.: Going deeper into first-person activity recognition.  24. Hu, J.F., Zheng, W.S., Lai, J., Zhang, J.: Jointly learning heterogeneous features  for rgb-d activity recognition. In: CVPR. (2015)  25. Lei, J., Ren, X., Fox, D.: Fine-grained kitchen activity recognition using rgb-d. In:  In: CVPR. (2016)  UbiComp. (2012)  26. Song, S., Cheung, N.M., Chandrasekhar, V., Mandal, B., Liri, J.: Egocentric ac- tivity recognition with multimodal fisher vector. In: Acoustics, Speech and Signal Processing (ICASSP), IEEE (2016)  27. de la Torre, F., Hodgins, J.K., Montano, J., Valcarcel, S.: Detailed human data ac- quisition of kitchen activities: the cmu-multimodal activity database (cmu-mmac). In: CHI Workshop. (2009)  28. Roggen, D., Calatroni, A., Rossi, M., Holleczek, T., F\u00a8orster, K., Tr\u00a8oster, G., Lukow- icz, P., Bannach, D., Pirkl, G., Ferscha, A., et al.: Collecting complex activity datasets in highly rich networked sensor environments. In: INSS, IEEE (2010) 29. Zhou, Y., Ni, B., Hong, R., Wang, M., Tian, Q.: Interaction part mining: A mid-  level approach for fine-grained action recognition. In: CVPR. (2015)  30. Zhou, Y., Ni, B., Yan, S., Moulin, P., Tian, Q.: Pipelining localized semantic  features for fine-grained action recognition. In: ECCV. (2014)  31. Peng, X., Zou, C., Qiao, Y., Peng, Q.: Action recognition with stacked fisher  vectors. In: ECCV. (2014)  32. Sun, S., Kuang, Z., Sheng, L., Ouyang, W., Zhang, W.: Optical \ufb02ow guided feature: A fast and robust motion representation for video action recognition. In: CVPR. (2018)  33. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look  at spatiotemporal convolutions for action recognition. In: CVPR. (2018)  34. Schenck, C., Fox, D.: Detection and tracking of liquids with fully convolutional  networks. In: RSS workshop. (2016)  35. Sermanet, P., Lynch, C., Hsu, J., Levine, S.: Time-contrastive networks: Self-  supervised learning from multi-view observation. arXiv:1704.06888 (2017)  36. Yamaguchi, A., Atkeson, C.G.: Stereo vision of liquid and particle \ufb02ow for robot  pouring. Humanoids (2016)  37. Tamosiunaite, M., Nemec, B., Ude, A., Wrgtter, F.: Learning to pour with a robot arm combining goal and shape learning for dynamic movement primitives. IEEE-RAS (2011)  38. Rozo, L., Jimnez, P., Torras, C.: Force-based robot learning of pouring skills using parametric hidden markov models. In: 9th International Workshop on Robot Motion and Control. (2013)  39. Brandi, S., Kroemer, O., Peters, J.: Generalizing pouring actions between objects  using warped parameters. In: Humanoids. (2014)  40. Schenck, C., Fox, D.: Visual closed-loop control for pouring liquids. In: ICRA.  (2017)  41. Yamaguchi, A., Atkeson, C.G.: Di\ufb00erential dynamic programming with temporally  decomposed dynamics. In: IEEE-RAS. (2015)  42. Kunze, L., Beetz, M.: Envisioning the qualitative e\ufb00ects of robot manipulation  actions using simulation-based projections. Artificial Intelligence (2017)  43. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR. (2016)  44. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,  Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)   Liquid Pouring Monitoring via Rich Sensory Inputs45. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-  Scale Hierarchical Image Database. In: CVPR. (2009)"}