{"1": "1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vi- jayanarasimhan, S.: YouTube-8M: A Large-Scale Video Classification Benchmark. In: CoRR (2016)  2. Alletto, S., Serra, G., Calderara, S., Cucchiara, R.: Understanding social relation-  ships in egocentric vision. In: Pattern Recognition (2015)  3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:  VQA: Visual Question Answering. In: ICCV (2015)  4. Banerjee, S., Pedersen, T.: An adapted lesk algorithm for word sense disambigua-  tion using wordnet. In: CICLing (2002)  5. Carnegie Mellon University: CMU sphinx. https://cmusphinx.github.io/ 6. Damen, D., Leelasawassuk, T., Haines, O., Calway, A., Mayol-Cuevas, W.: You- do, I-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video. In: BMVC (2014)  7. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D.,  Batra, D.: Visual Dialog. In: CVPR (2017)  8. De La Torre, F., Hodgins, J., Bargteil, A., Martin, X., Macey, J., Collado, A., Beltran, P.: Guide to the Carnegie Mellon University Multimodal Activity (CMU- MMAC) database. In: Robotics Institute (2008)  9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale  hierarchical image database. In: CVPR (2009)  10. Doughty, H., Damen, D., Mayol-Cuevas, W.: Who\u2019s better? who\u2019s best? pairwise  deep ranking for skill determination. In: CVPR (2018)  11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The  PASCAL Visual Object Classes (VOC) Challenge. In: IJCV (2010)  12. Fathi, A., Hodgins, J., Rehg, J.: Social interactions: A first-person perspective. In:  CVPR (2012)  ECCV (2012)  13. Fathi, A., Li, Y., Rehg, J.: Learning to recognize daily actions using gaze. In:  14. Fouhey, D.F., Kuo, W.c., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday  interactions. arXiv preprint arXiv:1712.02310 (2017)  15. Furnari, A., Battiato, S., Grauman, K., Farinella, G.M.: Next-active-object pre-  diction from egocentric videos. In: JVCIR (2017)  16. Georgia Tech: Extended GTEA Gaze+. http://webshare.ipat.gatech.edu/coc-rim-  wall-lab/web/yli440/egtea gp (2018)  17. Google: Google cloud speech api. https://cloud.google.com/speech 18. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fr\u00a8und, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., Memisevic, R.: The \u201dsomething something\u201d video database for learning and evaluating visual common sense. In: ICCV (2017)  19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  20. Heidarivincheh, F., Mirmehdi, M., Damen, D.: Action completion: A temporal  J., A.,  model for moment detection. In: BMVC (2018)  21. Huang,  Rathod,  V.,  D.,  Sun,  Chow, Tensor\ufb02ow  C., Detection  Zhu, M., API.  Z.:  Lu,  Fathi, https://github.com/tensor\ufb02ow/models/tree/master/research/object detection 22. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., et al.: Speed/accuracy trade-o\ufb00s for modern convolutional object detectors. In: CVPR (2017)  Object   Scaling Egocentric Vision: The EPIC-KITCHENS DatasetReferences  1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vi- jayanarasimhan, S.: YouTube-8M: A Large-Scale Video Classification Benchmark. In: CoRR (2016)  2. Alletto, S., Serra, G., Calderara, S., Cucchiara, R.: Understanding social relation-  ships in egocentric vision. In: Pattern Recognition (2015)  3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:  VQA: Visual Question Answering. In: ICCV (2015)  4. Banerjee, S., Pedersen, T.: An adapted lesk algorithm for word sense disambigua-  tion using wordnet. In: CICLing (2002)  5. Carnegie Mellon University: CMU sphinx. https://cmusphinx.github.io/ 6. Damen, D., Leelasawassuk, T., Haines, O., Calway, A., Mayol-Cuevas, W.: You- do, I-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video. In: BMVC (2014)  7. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D.,  Batra, D.: Visual Dialog. In: CVPR (2017)  8. De La Torre, F., Hodgins, J., Bargteil, A., Martin, X., Macey, J., Collado, A., Beltran, P.: Guide to the Carnegie Mellon University Multimodal Activity (CMU- MMAC) database. In: Robotics Institute (2008)  9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale  hierarchical image database. In: CVPR (2009)  10. Doughty, H., Damen, D., Mayol-Cuevas, W.: Who\u2019s better? who\u2019s best? pairwise  deep ranking for skill determination. In: CVPR (2018)  11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The  PASCAL Visual Object Classes (VOC) Challenge. In: IJCV (2010)  12. Fathi, A., Hodgins, J., Rehg, J.: Social interactions: A first-person perspective. In:  CVPR (2012)  ECCV (2012)  13. Fathi, A., Li, Y., Rehg, J.: Learning to recognize daily actions using gaze. In:  14. Fouhey, D.F., Kuo, W.c., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday  interactions. arXiv preprint arXiv:1712.02310 (2017)  15. Furnari, A., Battiato, S., Grauman, K., Farinella, G.M.: Next-active-object pre-  diction from egocentric videos. In: JVCIR (2017)  16. Georgia Tech: Extended GTEA Gaze+. http://webshare.ipat.gatech.edu/coc-rim-  wall-lab/web/yli440/egtea gp (2018)  17. Google: Google cloud speech api. https://cloud.google.com/speech 18. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fr\u00a8und, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., Memisevic, R.: The \u201dsomething something\u201d video database for learning and evaluating visual common sense. In: ICCV (2017)  19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  20. Heidarivincheh, F., Mirmehdi, M., Damen, D.: Action completion: A temporal  J., A.,  model for moment detection. In: BMVC (2018)  21. Huang,  Rathod,  V.,  D.,  Sun,  Chow, Tensor\ufb02ow  C., Detection  Zhu, M., API.  Z.:  Lu,  Fathi, https://github.com/tensor\ufb02ow/models/tree/master/research/object detection 22. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., et al.: Speed/accuracy trade-o\ufb00s for modern convolutional object detectors. In: CVPR (2017)  Object   16  D. Damen et al  to-text  23. IBM: IBM watson speech to text. https://www.ibm.com/watson/services/speech-  24. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In: ICML (2015)  25. Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid, C.: Joint learning of object  26. Karpathy, A., Fei-Fei, L.: Deep Visual-Semantic Alignments for Generating Image  and action detectors. In: ICCV (2017)  Descriptions. In: CVPR (2015)  27. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-  volutional neural networks. In: NIPS (2012)  28. Kuehne, H., Arslan, A., Serre, T.: The Language of Actions: Recovering the Syntax  and Semantics of Goal-Directed Human Activities. In: CVPR (2014)  29. Lee, Y., Ghosh, J., Grauman, K.: Discovering important people and objects for  egocentric video summarization. In: CVPR (2012)  30. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 31. Mikolov, T., Chen, K., Corrado, G., Dean, J.: E\ufb03cient estimation of word repre-  sentations in vector space. arXiv preprint arXiv:1301.3781 (2013) 32. Miller, G.: Wordnet: a lexical database for english. In: CACM (1995) 33. Moltisanti, D., Wray, M., Mayol-Cuevas, W., Damen, D.: Trespassing the bound- aries: Labeling temporal bounds for object interactions in egocentric video. In: ICCV (2017)  34. Nair, A., Chen, D., Agrawal, P., Isola, P., Abbeel, P., Malik, J., Levine, S.: Com- bining self-supervised learning and imitation for vision-based rope manipulation. In: ICRA (2017)  35. Park, H.S., Hwang, J.J., Niu, Y., Shi, J.: Egocentric future localization. In: CVPR  (2016)  36. Pirsiavash, H., Ramanan, D.: Detecting activities of daily living in first-person  camera views. In: CVPR (2012)  37. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object  detection with region proposal networks. In: NIPS (2015)  38. Rohrbach, A., Rohrbach, M., Tandon, N., Schiele, B.: A Dataset for Movie De-  scription. In: CVPR (2015)  39. Rohrbach, M., Amin, S., Andriluka, M., Schiele, B.: A Database for Fine Grained  Activity Detection of Cooking Activities. In: CVPR (2012)  40. Ryoo, M.S., Matthies, L.: First-person activity recognition: What are they doing  to me? In: CVPR (2013)  41. Sigurdsson, G.A., Gupta, A., Schmid, C., Farhadi, A., Alahari, K.: Charades-ego: A large-scale dataset of paired third and first person videos. In: ArXiv (2018) 42. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Hol- lywood in homes: Crowdsourcing data collection for activity understanding. In: ECCV (2016)  43. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog- nition in videos. In: Advances in neural information processing systems. pp. 568- 576 (2014)  44. Stein, S., McKenna, S.: Combining Embedded Accelerometers with Computer Vi-  sion for Recognizing Food Preparation Activities. In: UbiComp (2013)  45. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR (2015)   Scaling Egocentric Vision: The EPIC-KITCHENS Dataset46. Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: MovieQA: Understanding stories in movies through question-answering. In: CVPR (2016)  47. Vondrick, C., Pirsiavash, H., Torralba, A.: Anticipating visual representations from  unlabeled video. In: CVPR (2016)  48. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Val Gool, L.: Temporal segment networks: Towards good practices for deep action recognition. In: ECCV (2016)  49. Yamaguchi, K.: Bbox-annotator. https://github.com/kyamagu/bbox-annotator 50. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every moment counts: Dense detailed labeling of actions in complex videos. IJCV (2018) Network.  51. Yuanjun,  Temporal  PyTorch  Segment  X.:  https://github.com/yjxiong/tsn-pytorch (2017)  52. Zach, C., Pock, T., Bischof, H.: A duality based approach for realtime TV-L1  optical \ufb02ow. In: Pattern Recognition (2007)  53. Zhang, T., McCarthy, Z., Jow, O., Lee, D., Goldberg, K., Abbeel, P.: Deep imi- tation learning for complex manipulation tasks from virtual reality teleoperation. In: ICRA (2018)  54. Zhao, H., Yan, Z., Wang, H., Torresani, L., Torralba, A.: SLAC: A Sparsely Labeled Dataset for Action Classification and Localization. arXiv preprint arXiv:1712.09374 (2017)  55. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing  through ade20k dataset. In: CVPR (2017)  56. Zhou, L., Xu, C., Corso, J.J.: Towards automatic learning of procedures from web  instructional videos. arXiv preprint arXiv:1703.09788 (2017)"}