{"1": "1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and VQA. In: CVPR (2018)  2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint  arXiv:1607.06450 (2016)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: ICLR (2015)  4. Buschman, T.J., Miller, E.K.: Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices. Science 315(5820), 1860-1862 (2007) 5. Chorowski, J.K., Bahdanau, D., Serdyuk, D., Cho, K., Bengio, Y.: Attention-based  models for speech recognition. In: NIPS (2015)  6. Corbetta, M., Shulman, G.L.: Control of goal-directed and stimulus-driven atten-  tion in the brain. Nature Reviews Neuroscience 3(3), 201 (2002)  7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale  hierarchical image database. In: CVPR (2009)  8. Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., Mitchell, M.: Language models for image captioning: The quirks and what works. In: ACL (2015)  9. Eisenschtat, A., Wolf, L.: Linking image and text with 2-way nets. In: CVPR (2017) 10. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: VSE++: Improved visual-semantic  embeddings. arXiv preprint arXiv:1707.05612 (2017)  11. Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Doll\u00b4ar, P., Gao, J., He, X., Mitchell, M., Platt, J., et al.: From captions to visual concepts and back. In: CVPR (2015)  12. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-  rate object detection and semantic segmentation. In: CVPR (2014)  13. Gu, J., Cai, J., Joty, S., Niu, L., Wang, G.: Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models. In: CVPR (2018) 14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  15. He, X., Deng, L., Chou, W.: Discriminative learning in sequential pattern recogni-  tion. IEEE Signal Processing Magazine 25(5) (2008)  16. Huang, Y., Wang, W., Wang, L.: Instance-aware image and sentence matching  with selective multimodal LSTM. In: CVPR (2017)  17. Huang, Y., Wu, Q., Wang, L.: Learning semantic concepts and order for image and  sentence matching. In: CVPR (2018)  18. Juang, B.H., Hou, W., Lee, C.H.: Minimum classification error rate methods for speech recognition. IEEE Transactions on Speech and Audio processing 5(3), 257- 265 (1997)  19. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image  descriptions. In: CVPR (2015)  20. Karpathy, A., Joulin, A., Fei-Fei, L.: Deep fragment embeddings for bidirectional  image sentence mapping. In: NIPS (2014)  21. Katsuki, F., Constantinidis, C.: Bottom-up and top-down attention: Di\ufb00erent pro- cesses and overlapping neural systems. The Neuroscientist 20(5), 509-521 (2014) 22. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014)   Stacked Cross Attention for Image-Text MatchingReferences  1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and VQA. In: CVPR (2018)  2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint  arXiv:1607.06450 (2016)  3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: ICLR (2015)  4. Buschman, T.J., Miller, E.K.: Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices. Science 315(5820), 1860-1862 (2007) 5. Chorowski, J.K., Bahdanau, D., Serdyuk, D., Cho, K., Bengio, Y.: Attention-based  models for speech recognition. In: NIPS (2015)  6. Corbetta, M., Shulman, G.L.: Control of goal-directed and stimulus-driven atten-  tion in the brain. Nature Reviews Neuroscience 3(3), 201 (2002)  7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale  hierarchical image database. In: CVPR (2009)  8. Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., Mitchell, M.: Language models for image captioning: The quirks and what works. In: ACL (2015)  9. Eisenschtat, A., Wolf, L.: Linking image and text with 2-way nets. In: CVPR (2017) 10. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: VSE++: Improved visual-semantic  embeddings. arXiv preprint arXiv:1707.05612 (2017)  11. Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Doll\u00b4ar, P., Gao, J., He, X., Mitchell, M., Platt, J., et al.: From captions to visual concepts and back. In: CVPR (2015)  12. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-  rate object detection and semantic segmentation. In: CVPR (2014)  13. Gu, J., Cai, J., Joty, S., Niu, L., Wang, G.: Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models. In: CVPR (2018) 14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  In: CVPR (2016)  15. He, X., Deng, L., Chou, W.: Discriminative learning in sequential pattern recogni-  tion. IEEE Signal Processing Magazine 25(5) (2008)  16. Huang, Y., Wang, W., Wang, L.: Instance-aware image and sentence matching  with selective multimodal LSTM. In: CVPR (2017)  17. Huang, Y., Wu, Q., Wang, L.: Learning semantic concepts and order for image and  sentence matching. In: CVPR (2018)  18. Juang, B.H., Hou, W., Lee, C.H.: Minimum classification error rate methods for speech recognition. IEEE Transactions on Speech and Audio processing 5(3), 257- 265 (1997)  19. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image  descriptions. In: CVPR (2015)  20. Karpathy, A., Joulin, A., Fei-Fei, L.: Deep fragment embeddings for bidirectional  image sentence mapping. In: NIPS (2014)  21. Katsuki, F., Constantinidis, C.: Bottom-up and top-down attention: Di\ufb00erent pro- cesses and overlapping neural systems. The Neuroscientist 20(5), 509-521 (2014) 22. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014)   16  Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He  23. Klein, B., Lev, G., Sadeh, G., Wolf, L.: Associating neural word embeddings with  deep image representations using fisher vectors. In: CVPR (2015)  24. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision 123(1), 32-73 (2017)  25. Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., Zhong, V., Paulus, R., Socher, R.: Ask me anything: Dynamic memory networks for natural language processing. In: ICML (2016)  26. Lee, K.H., He, X., Zhang, L., Yang, L.: CleanNet: Transfer learning for scalable  image classifier training with label noise. In: CVPR (2018)  27. Lev, G., Sadeh, G., Klein, B., Wolf, L.: RNN fisher vectors for action recognition  and image annotation. In: ECCV (2016)  28. Li, J., Luong, M.T., Jurafsky, D.: A hierarchical neural autoencoder for paragraphs  and documents. In: ACL (2015)  29. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 30. Luong, M.T., Pham, H., Manning, C.D.: E\ufb00ective approaches to attention-based  neural machine translation. In: EMNLP (2015)  31. Nam, H., Ha, J.W., Kim, J.: Dual attention networks for multimodal reasoning  and matching. In: CVPR (2017)  32. Niu, Z., Zhou, M., Wang, L., Gao, X., Hua, G.: Hierarchical multimodal LSTM for  dense visual-semantic embedding. In: ICCV (2017)  33. Peng, Y., Qi, J., Yuan, Y.: CM-GANs: Cross-modal generative adversarial networks for common representation learning. arXiv preprint arXiv:1710.05106 (2017) 34. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object  detection with region proposal networks. In: NIPS (2015)  35. Rush, A.M., Chopra, S., Weston, J.: A neural attention model for abstractive  sentence summarization. In: EMNLP (2015)  36. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Trans-  actions on Signal Processing 45(11), 2673-2681 (1997)  37. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded composi- tional semantics for finding and describing images with sentences. In: ACL (2014) 38. Vendrov, I., Kiros, R., Fidler, S., Urtasun, R.: Order-embeddings of images and  39. Wang, L., Li, Y., Lazebnik, S.: Learning deep structure-preserving image-text em-  language. In: ICLR (2016)  beddings. In: CVPR (2016)  40. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML (2015)  41. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: AttnGAN: Fine-grained text to image generation with attentional generative adversarial net- works. In: CVPR (2018)  42. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical attention  networks for document classification. In: NAACL-HLT (2016)  43. Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In: ACL (2014)  44. Zheng, Z., Zheng, L., Garrett, M., Yang, Y., Shen, Y.D.: Dual-path convolutional  image-text embedding. arXiv preprint arXiv:1711.05535 (2017)"}