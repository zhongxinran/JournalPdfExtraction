{"1": "1. Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H.,  Courville, A., Schiele, B.: Movie Description. arXiv:1605.03705 (2016)  2. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A Large Video Description Dataset for  Bridging Video and Language. In: CVPR. (2016)  3. Chen, D.L., Dolan, W.B.: Collecting Highly Parallel Data for Paraphrase Evalua-  tion. In: ACL. (2011)  4. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term Recurrent Convolutional Networks for Visual Recognition and Description. In: CVPR. (2015)  5. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-shot Recognition. In: ICCV. (2013) 6. Rohrbach, A., Rohrbach, M., Schiele, B.: The Long-Short Story of Movie Descrip-  tion. In: GCPR. (2015)  7. Venugopalan, S., Marcus, R., Je\ufb00rey, D., Raymond, M., Trevor, D., Kate, S.: Se-  quence to Sequence - Video to Text. In: ICCV. (2015)  8. Xu, R., Xiong, C., Chen, W., Corso, J.J.: Jointly Modeling Deep Video and Com- positional Text to Bridge Vision and Language in a Unified Framework. In: AAAI. (2015)  9. Yu, Y., Ko, H., Choi, J., Kim, G.: End-to-end Concept Word Detection for Video  Captioning, Retrieval, and Question Answering. In: CVPR. (2017)  10. Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: MovieQA: Understanding Stories in Movies through Question-Answering. In: CVPR. (2016)  11. Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: TGIF-QA: Toward Spatio-Temporal  Reasoning in Visual Question Answering. In: CVPR. (2017)  12. Torabi, A., Tandon, N., Sigal, L.: Learning Language-Visual Embedding for Movie  Understanding with Natural-Language. arXiv:1609.08124 (2016)  13. Otani, M., Nakashima, Y., Rahtu, E., Heikkil\u00a8a, J., Yokoya, N.: Learning Joint Representations of Videos and Sentences with Web Image Search. arXiv:1608.02367 (2016)  14. Laptev, I., Lindeberg, T.: Space-time Interest Points. In: ICCV. (2003) 15. Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning Realistic Human  16. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A Dataset of 101 Human Actions  Actions from Movies. In: CVPR. (2008)  Classes from Videos in the Wild (2012)  17. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale Video Classification with Convolutional Neural Networks. In: CVPR. (2014)  18. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A Large-scale Video Benchmark for Human Activity Understanding. In: CVPR. (2015)  19. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying Visual-Semantic Embeddings  with Multimodal Neural Language Models. TACL (2014)  20. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.:  Devise: A Deep Visual-semantic Embedding Model. In: NIPS. (2013)  21. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded Com- positional Semantics for Finding and Describing Images with Sentences. TACL (2014)   A Joint Sequence Fusion Model for Video VQA and RetrievalReferences  1. Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H.,  Courville, A., Schiele, B.: Movie Description. arXiv:1605.03705 (2016)  2. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A Large Video Description Dataset for  Bridging Video and Language. In: CVPR. (2016)  3. Chen, D.L., Dolan, W.B.: Collecting Highly Parallel Data for Paraphrase Evalua-  tion. In: ACL. (2011)  4. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term Recurrent Convolutional Networks for Visual Recognition and Description. In: CVPR. (2015)  5. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-shot Recognition. In: ICCV. (2013) 6. Rohrbach, A., Rohrbach, M., Schiele, B.: The Long-Short Story of Movie Descrip-  tion. In: GCPR. (2015)  7. Venugopalan, S., Marcus, R., Je\ufb00rey, D., Raymond, M., Trevor, D., Kate, S.: Se-  quence to Sequence - Video to Text. In: ICCV. (2015)  8. Xu, R., Xiong, C., Chen, W., Corso, J.J.: Jointly Modeling Deep Video and Com- positional Text to Bridge Vision and Language in a Unified Framework. In: AAAI. (2015)  9. Yu, Y., Ko, H., Choi, J., Kim, G.: End-to-end Concept Word Detection for Video  Captioning, Retrieval, and Question Answering. In: CVPR. (2017)  10. Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.: MovieQA: Understanding Stories in Movies through Question-Answering. In: CVPR. (2016)  11. Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: TGIF-QA: Toward Spatio-Temporal  Reasoning in Visual Question Answering. In: CVPR. (2017)  12. Torabi, A., Tandon, N., Sigal, L.: Learning Language-Visual Embedding for Movie  Understanding with Natural-Language. arXiv:1609.08124 (2016)  13. Otani, M., Nakashima, Y., Rahtu, E., Heikkil\u00a8a, J., Yokoya, N.: Learning Joint Representations of Videos and Sentences with Web Image Search. arXiv:1608.02367 (2016)  14. Laptev, I., Lindeberg, T.: Space-time Interest Points. In: ICCV. (2003) 15. Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning Realistic Human  16. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A Dataset of 101 Human Actions  Actions from Movies. In: CVPR. (2008)  Classes from Videos in the Wild (2012)  17. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale Video Classification with Convolutional Neural Networks. In: CVPR. (2014)  18. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A Large-scale Video Benchmark for Human Activity Understanding. In: CVPR. (2015)  19. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying Visual-Semantic Embeddings  with Multimodal Neural Language Models. TACL (2014)  20. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.:  Devise: A Deep Visual-semantic Embedding Model. In: NIPS. (2013)  21. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded Com- positional Semantics for Finding and Describing Images with Sentences. TACL (2014)   16  Y. Yu , J. Kim and G. Kim  22. Socher, R., Lin, C.C., Manning, C., Ng, A.Y.: Parsing Natural Scenes and Natural  Language with Recursive Neural Networks. In: ICML. (2011)  23. Socher, R., Bauer, J., Manning, C.D., et al.: Parsing with Compositional Vector  Grammars. In: ACL. (2013)  24. Hodosh, M., Young, P., Hockenmaier, J.: Framing Image Description as a Ranking  Task: Data, models and Evaluation Metrics. JAIR (2013)  25. Lin, D., Fidler, S., Kong, C., Urtasun, R.: Visual Semantic Search: Retrieving  Videos via Complex Textual Queries. In: CVPR. (2014)  26. Vendrov, I., Kiros, R., Fidler, S., Urtasun, R.: Order-embeddings of Images and  27. Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural Language  Language. arXiv:1511.06361 (2015)  Object Retrieval. In: CVPR. (2016)  28. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation and Comprehension of Unambiguous Object Descriptions. In: CVPR. (2016) 29. Yu, Y., Ko, H., Choi, J., Kim, G.: Video Captioning and Retrieval Models with  Semantic Attention. arXiv preprint arXiv:1610.02947 (2016)  30. Kaufman, D., Levi, G., Hassner, T., Wolf, L.: Temporal Tessellation for Video  Annotation and Summarization. ICCV (2017)  31. Malinowski, M., Fritz, M.: A Multi-World Approach to Question Answering about  Real-World Scenes based on Uncertain Input. In: NIPS. (2014)  32. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:  VQA: Visual Question Answering. In: ICCV. (2015)  33. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In: CVPR. (2017)  34. Maharaj, T., Ballas, N., Courville, A.C., Pal, C.J.: A Dataset and Exploration of Models for Understanding Video Data Through Fill-in-the-blank Question- answering. arXiv:1611.07810  35. Mazaheri, A., Zhang, D., Shah, M.: Video Fill in the Blank with Merging LSTMs.  arXiv:1610.04062 (2016)  36. Mazaheri, A., Zhang, D., Shah, M.: Video Fill In the Blank using LR/RL LSTMs  with Spatial-Temporal Attentions. In: ICCV. (2017)  37. Pennington, J., Socher, R., Manning, C.D.: Glove: Global Vectors for Word Rep-  38. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.  resentation. In: EMNLP. (2014)  In: CVPR. (2016)  39. Hershey, S., Chaudhuri, S., Ellis, D.P.W., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., Slaney, M., Weiss, R.J., Wilson, K.W.: CNN Architectures for Large-Scale Audio Classification. In: ICASSP. (2017) 40. Schuster, M., Paliwal, K.K.: Bidirectional Recurrent Neural Networks. In: IEEE  TSP. (1997)  (2015)  41. Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. In: IEEE. (1997) 42. Kingma, D., Ba, J.: Adam: A Method for Stochastic Optimization.  In: ICLR.  43. Io\ufb00e, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training  by Reducing Internal Covariate Shift. In: ICML. (2015)  44. Miech, A., Alayrac, J.B., Bojanowski, P., Laptev, I., Sivic, J.: Learning from Video  and Text via Large-scale Discriminative Clustering. In: ICCV. (2017)  45. Kim, J.H., On, K.W., Lim, W., Kim, J., Ha, J.W., Zhang, B.T.: Hadamard Product  for Low-rank Bilinear Pooling. In: ICLR. (2017)   A Joint Sequence Fusion Model for Video VQA and Retrieval46. Tzeng, E., Ho\ufb00man, J., Darrell, T., Saenko, K.: Simultaneous Deep Transfer Across  Domains and Tasks. In: ICCV. (2015)"}