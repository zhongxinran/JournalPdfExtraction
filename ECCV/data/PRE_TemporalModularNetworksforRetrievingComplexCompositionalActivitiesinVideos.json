{"1": "1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675 (2016)  2. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Learning to compose neural  networks for question answering. arXiv preprint arXiv:1601.01705 (2016)  3. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In:  CVPR (2016)  4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:  VQA: Visual Question Answering. In: ICCV (2015)  5. B. Thomee, D.A. Shamma, G.F.B.E.K.N.D.P.D.B.L.L.: Yfcc100m: The new data in multimedia research. Communications of the ACM 59(2), pp. 64-73 (2016) 6. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale video benchmark for human activity understanding. In: CVPR (2015) 7. Chen, D.L., Dolan, W.B.: Collecting highly parallel data for paraphrase evalu- ation. In: Proceedings of the 49th Annual Meeting of the Association for Com- putational Linguistics: Human Language Technologies - Volume 1. pp. 190-200. HLT \u201911, Association for Computational Linguistics, Stroudsburg, PA, USA (2011), http://dl.acm.org/citation.cfm?id=2002472.2002497  8. Diederik P. Kingma, J.L.B.: Adam: a method for stochastic optimization. In: ICLR  (2015)  9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)  10. Feng, X., Perona, P.: Human action recognition by sequence of movelet 3D (2002).  codewords. Data Processing Visualization and Transmission. pp. https://doi.org/10.1109/TDPVT.2002.1024148  Symposium on 717-721  In: Proceedings. First  International  11. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.:  Devise: A deep visual-semantic embedding model. In: NIPS (2013)  12. Gaidon, A., Harchaoui, Z., Schmid, C.: Temporal localization of actions with ac-  toms. IEEE TPAMI 35(11), 2782-2795 (2013)  13. Gu, C., Sun, C., Vijayanarasimhan, S., Pantofaru, C., Ross, D.A., Toderici, G., Li, Y., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: AVA: A video dataset of spatio-temporally localized atomic visual actions. CoRR abs/1705.08421 (2017), http://arxiv.org/abs/1705.08421  14. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In: ICCV (2013)  15. Han, W., Khorrami, P., Paine, T.L., Ramachandran, P., Babaeizadeh, M., Shi, H., Li, J., Yan, S., Huang, T.S.: Seq-nms for video object detection. arXiv preprint arXiv:1602.08465 (2016)  16. Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.C.:  Localizing moments in video with natural language. In: ICCV (2017)  17. Hu, R., Andreas, J., Rohrbach, M., Darrell, T., Saenko, K.: Learning to reason: End-to-end module networks for visual question answering. In: ICCV (2017) 18. Hu, R., Rohrbach, M., Andreas, J., Darrell, T., Saenko, K.: Modeling relationships in referential expressions with compositional modular networks. In: CVPR (2017)   Temporal Modular Networks for Compositional Activity RetrievalReferences  1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675 (2016)  2. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Learning to compose neural  networks for question answering. arXiv preprint arXiv:1601.01705 (2016)  3. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In:  CVPR (2016)  4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:  VQA: Visual Question Answering. In: ICCV (2015)  5. B. Thomee, D.A. Shamma, G.F.B.E.K.N.D.P.D.B.L.L.: Yfcc100m: The new data in multimedia research. Communications of the ACM 59(2), pp. 64-73 (2016) 6. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale video benchmark for human activity understanding. In: CVPR (2015) 7. Chen, D.L., Dolan, W.B.: Collecting highly parallel data for paraphrase evalu- ation. In: Proceedings of the 49th Annual Meeting of the Association for Com- putational Linguistics: Human Language Technologies - Volume 1. pp. 190-200. HLT \u201911, Association for Computational Linguistics, Stroudsburg, PA, USA (2011), http://dl.acm.org/citation.cfm?id=2002472.2002497  8. Diederik P. Kingma, J.L.B.: Adam: a method for stochastic optimization. In: ICLR  (2015)  9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)  10. Feng, X., Perona, P.: Human action recognition by sequence of movelet 3D (2002).  codewords. Data Processing Visualization and Transmission. pp. https://doi.org/10.1109/TDPVT.2002.1024148  Symposium on 717-721  In: Proceedings. First  International  11. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.:  Devise: A deep visual-semantic embedding model. In: NIPS (2013)  12. Gaidon, A., Harchaoui, Z., Schmid, C.: Temporal localization of actions with ac-  toms. IEEE TPAMI 35(11), 2782-2795 (2013)  13. Gu, C., Sun, C., Vijayanarasimhan, S., Pantofaru, C., Ross, D.A., Toderici, G., Li, Y., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: AVA: A video dataset of spatio-temporally localized atomic visual actions. CoRR abs/1705.08421 (2017), http://arxiv.org/abs/1705.08421  14. Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., Saenko, K.: Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In: ICCV (2013)  15. Han, W., Khorrami, P., Paine, T.L., Ramachandran, P., Babaeizadeh, M., Shi, H., Li, J., Yan, S., Huang, T.S.: Seq-nms for video object detection. arXiv preprint arXiv:1602.08465 (2016)  16. Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.C.:  Localizing moments in video with natural language. In: ICCV (2017)  17. Hu, R., Andreas, J., Rohrbach, M., Darrell, T., Saenko, K.: Learning to reason: End-to-end module networks for visual question answering. In: ICCV (2017) 18. Hu, R., Rohrbach, M., Andreas, J., Darrell, T., Saenko, K.: Modeling relationships in referential expressions with compositional modular networks. In: CVPR (2017)   16  B.Liu, S.Yeung, E.Chou, D.Huang, L.Fei-Fei, J.C.Niebles.  19.  \u02d9Ikizler, N., Forsyth, D.A.: Searching for complex human activities with no visual examples. IJCV (2008)  20. Jain, M., van Gemert, J.C., Mensink, T., Snoek, C.G.: Objects2action: Classifying  and localizing actions without any video example. In: ICCV (2015)  21. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human  action recognition. TPAMI 35(1), 221-231 (2013)  22. Jiang, Y.G., Liu, J., Roshan Zamir, A., Toderici, G., Laptev, I., Shah, M., Suk- thankar, R.: THUMOS challenge: Action recognition with a large number of classes. http://crcv.ucf.edu/THUMOS14/ (2014)  23. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: CVPR. pp. 1988-1997. IEEE (2017)  24. Johnson, J., Hariharan, B., van der Maaten, L., Ho\ufb00man, J., Fei-Fei, L., Zitnick, C.L., Girshick, R.B.: Inferring and executing programs for visual reasoning. In: ICCV (2017)  25. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large- scale video classification with convolutional neural networks. In: CVPR (2014) 26. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)  27. Klein, D., Manning, C.D.: Accurate unlexicalized parsing. In: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. pp. 423-430. Association for Computational Linguistics (2003)  28. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: a large video  database for human motion recognition. In: ICCV (2011)  29. Li, L.J., Su, H., Fei-Fei, L., Xing, E.P.: Object bank: A high-level image represen-  tation for scene classification & semantic feature sparsification. In: NIPS (2010)  30. Li, L.J., Su, H., Lim, Y., Fei-Fei, L.: Objects as attributes for scene classification.  In: ECCV. pp. 57-69. Springer (2010)  31. Lillo, I., Niebles, J.C., Soto, A.: Sparse composition of body poses and atomic ac- tions for human activity recognition in rgb-d videos. Image and Vision Computing (2017)  32. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention  for visual question answering. In: NIPS (2016)  33. Monfort, M., Zhou, B., Bargal, S.A., Andonian, A., Yan, T., Ramakrishnan, K., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., et al.: Moments in time dataset: one million videos for event understanding. arXiv preprint arXiv:1801.03150 (2018) 34. Niebles, J.C., Chen, C.W., Fei-Fei, L.: Modeling temporal structure of decompos-  able motion segments for activity classification. In: ECCV (2010)  35. Peng, X., Schmid, C.: Multi-region two-stream r-cnn for action detection. In: ECCV  36. Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word represen-  (2016)  tation. In: EMNLP (2014)  computing 28(6), 976-990 (2010)  37. Poppe, R.: A survey on vision-based human action recognition. Image and vision  38. Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., Pinkal, M.: Ground- ing action descriptions in videos. Transactions of the Association for Computa- tional Linguistics (2013)  39. Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H.,  Courville, A., Schiele, B.: Movie description. IJCV 123(1), 94-120 (2017)   Temporal Modular Networks for Compositional Activity Retrieval40. Rohrbach, M., Rohrbach, A., Regneri, M., Amin, S., Andriluka, M., Pinkal, M., Schiele, B.: Recognizing fine-grained and composite activities using hand-centric features and script data. IJCV (2016)  41. Sadanand, S., Corso, J.J.: Action bank: A high-level representation of activity in  video. In: CVPR (2012)  CVPR) (2012)  42. Schiele, B.: A database for fine grained activity detection of cooking activities. In:  43. Scovanner, P., Ali, S., Shah, M.: A 3-dimensional sift descriptor and its application to action recognition. In: Proceedings of the 15th ACM International Conference on Multimedia. MM \u201907 (2007)  44. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Hol- lywood in homes: Crowdsourcing data collection for activity understanding. In: ECCV (2016)  45. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-  46. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  47. Sivic, J., Zisserman, A.: Video google: a text retrieval approach to object matching  nition in videos. In: NIPS (2014)  image recognition. ICLR (2015)  in videos. In: ICCV (2003)  48. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compo- sitional semantics for finding and describing images with sentences. Transactions of the Association of Computational Linguistics (2014)  49. Socher, R., Lin, C.C., Manning, C., Ng, A.Y.: Parsing natural scenes and natural  language with recursive neural networks. In: ICML (2011)  50. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A., Potts, C.: Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of the 2013 conference on empirical methods in natural language processing (2013)  51. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes  from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)  52. Torabi, A., Tandon, N., Sigal, L.: Learning language-visual embedding for movie understanding with natural-language. arXiv preprint arXiv:1609.08124 (2016) 53. Tran, A., Cheong, L.F.: Two-stream \ufb02ow-guided convolutional attention networks  for action recognition. arXiv preprint arXiv:1708.09268 (2017)  54. Tran, D., Bourdev, L.D., Fergus, R., Torresani, L., Paluri, M.: C3D: generic features for video analysis. CoRR abs/1412.0767 (2014), http://arxiv.org/abs/1412.0767 55. Wang, H., Kl\u00a8aser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajecto-  ries. In: CVPR (2011)  56. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks: Towards good practices for deep action recognition. In: ECCV (2016)  57. Weinland, D., Ronfard, R., Boyer, E.: A survey of vision-based methods for action representation, segmentation and recognition. Computer vision and image under- standing (2011)  58. Xiao, F., Sigal, L., Lee, Y.J.: Weakly-supervised visual grounding of phrases with  linguistic structures. In: CVPR (2017)  59. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for  bridging video and language. In: CVPR (2016)  60. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks for  image question answering. In: CVPR (2016)   18  B.Liu, S.Yeung, E.Chou, D.Huang, L.Fei-Fei, J.C.Niebles.  61. Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., Courville, A.:  Describing videos by exploiting temporal structure. In: ICCV (2015)  62. Yeung, S., Fathi, A., Fei-Fei, L.: Videoset: Video summary evaluation through text.  arXiv preprint arXiv:1406.5824 (2014)  63. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using  hierarchical recurrent neural networks. In: CVPR (2016)  64. Zacks, J.M., Tversky, B.: Event structure in perception and conception. Psycho-  logical bulletin (2001)  65. Zellers, R., Choi, Y.: Zero-shot activity recognition with verb attribute induction.  arXiv preprint arXiv:1707.09468 (2017)  66. Zhao, H., Yan, Z., Wang, H., Torresani, L., Torralba, A.: Slac: A sparsely labeled dataset for action classification and localization. arXiv preprint arXiv:1712.09374 (2017)  67. Zhou, L., Xu, C., Corso, J.J.: Procnets: Learning to segment procedures in  untrimmed and unconstrained videos. CoRR abs/1703.09788 (2017)"}