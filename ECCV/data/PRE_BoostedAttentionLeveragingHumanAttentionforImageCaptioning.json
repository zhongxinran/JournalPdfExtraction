{"1": "1. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for se- quence prediction with recurrent neural networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems - Vol- ume 1. pp. 1171-1179. NIPS\u201915, MIT Press, Cambridge, MA, USA (2015), http://dl.acm.org/citation.cfm?id=2969239.2969370  2. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6298-6306 (2017). https://doi.org/10.1109/CVPR.2017.667  3. Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Visual saliency for im- age captioning in new multimedia services. In: 2017 IEEE International Conference on Multimedia Expo Workshops (ICMEW). pp. 309-314 (2017). https://doi.org/10.1109/ICMEW.2017.8026277  4. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hocken- maier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: Proceedings of the 11th European Conference on Computer Vi- sion: Part IV. pp. 15-29. ECCV\u201910, Springer-Verlag, Berlin, Heidelberg (2010), http://dl.acm.org/citation.cfm?id=1888089.1888092  5. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Se- mantic compositional networks for visual captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1141-1150 (2017). https://doi.org/10.1109/CVPR.2017.127  6. Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., Lazebnik, S.: Improving image- sentence embeddings using large weakly annotated photo collections. In: ECCV (2014)  7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  CoRR abs/1512.03385 (2015)  8. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Com- https://doi.org/10.1162/neco.1997.9.8.1735,  1735-1780  9(8),  put. http://dx.doi.org/10.1162/neco.1997.9.8.1735  (1997).  9. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Int. Res. 47(1), 853-899 (2013), http://dl.acm.org/citation.cfm?id=2566972.2566993  10. Hu,  J.,  Sun, G.: abs/1709.01507 (2017), http://arxiv.org/abs/1709.01507  Squeeze-and-excitation  Shen, L.,  networks. CoRR  11. Jiang, M., Huang, S., Duan, J., Zhao, Q.: Salicon: Saliency in context. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1072- 1080 (2015). https://doi.org/10.1109/CVPR.2015.7298710  12. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3128-3137 (2015). https://doi.org/10.1109/CVPR.2015.7298932 13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR  abs/1412.6980 (2014), http://arxiv.org/abs/1412.6980  14. Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Baby talk: Understanding and generating image descriptions. In: Proceedings of the 24th CVPR (2011)  15. Lavie, A., Agarwal, A.: Meteor: An automatic metric for mt evaluation with In: Proceedings of the  high levels of correlation with human judgments.   Boosted Attention CaptioningReferences  1. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for se- quence prediction with recurrent neural networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems - Vol- ume 1. pp. 1171-1179. NIPS\u201915, MIT Press, Cambridge, MA, USA (2015), http://dl.acm.org/citation.cfm?id=2969239.2969370  2. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6298-6306 (2017). https://doi.org/10.1109/CVPR.2017.667  3. Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: Visual saliency for im- age captioning in new multimedia services. In: 2017 IEEE International Conference on Multimedia Expo Workshops (ICMEW). pp. 309-314 (2017). https://doi.org/10.1109/ICMEW.2017.8026277  4. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hocken- maier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: Proceedings of the 11th European Conference on Computer Vi- sion: Part IV. pp. 15-29. ECCV\u201910, Springer-Verlag, Berlin, Heidelberg (2010), http://dl.acm.org/citation.cfm?id=1888089.1888092  5. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Se- mantic compositional networks for visual captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1141-1150 (2017). https://doi.org/10.1109/CVPR.2017.127  6. Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., Lazebnik, S.: Improving image- sentence embeddings using large weakly annotated photo collections. In: ECCV (2014)  7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.  CoRR abs/1512.03385 (2015)  8. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Com- https://doi.org/10.1162/neco.1997.9.8.1735,  1735-1780  9(8),  put. http://dx.doi.org/10.1162/neco.1997.9.8.1735  (1997).  9. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Int. Res. 47(1), 853-899 (2013), http://dl.acm.org/citation.cfm?id=2566972.2566993  10. Hu,  J.,  Sun, G.: abs/1709.01507 (2017), http://arxiv.org/abs/1709.01507  Squeeze-and-excitation  Shen, L.,  networks. CoRR  11. Jiang, M., Huang, S., Duan, J., Zhao, Q.: Salicon: Saliency in context. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1072- 1080 (2015). https://doi.org/10.1109/CVPR.2015.7298710  12. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3128-3137 (2015). https://doi.org/10.1109/CVPR.2015.7298932 13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR  abs/1412.6980 (2014), http://arxiv.org/abs/1412.6980  14. Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Baby talk: Understanding and generating image descriptions. In: Proceedings of the 24th CVPR (2011)  15. Lavie, A., Agarwal, A.: Meteor: An automatic metric for mt evaluation with In: Proceedings of the  high levels of correlation with human judgments.   16  S. Chen and Q. Zhao  Second Workshop on Statistical Machine Translation. pp. 228-231. StatMT \u201907, Association for Computational Linguistics, Stroudsburg, PA, USA (2007), http://dl.acm.org/citation.cfm?id=1626355.1626389  16. Li, J., Xia, C., Song, Y., Fang, S., Chen, X.: A data-driven metric for comprehensive evaluation of saliency models. In: 2015 IEEE International Conference on Com- puter Vision (ICCV). pp. 190-198 (2015). https://doi.org/10.1109/ICCV.2015.30 17. Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple im- age descriptions using web-scale n-grams. In: Proceedings of the Fifteenth Con- ference on Computational Natural Language Learning. pp. 220-228. CoNLL \u201911, Association for Computational Linguistics, Stroudsburg, PA, USA (2011), http://dl.acm.org/citation.cfm?id=2018936.2018962  18. Lin, C.Y.: Rouge: a package for automatic evaluation of summaries. In: Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004) (2004) 19. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European Conference on Computer Vision (ECCV). Z\u00a8urich (2014), oral  Improved image 20. Liu, S., Zhu, Z., Ye, N., Guadarrama, S., Murphy, K.: captioning via policy gradient optimization of spider. In: 2017 IEEE In- ternational Conference on Computer Vision (ICCV). pp. 873-881 (2017). https://doi.org/10.1109/ICCV.2017.100  21. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)  23. Papineni, K., Roukos, automatic 40th Annual Meeting  22. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: Neural Information Processing Systems (NIPS) (2011) S., Ward, T., Zhu, W.J.: Bleu: A method of for the on Association for Computational Linguis- tics. pp. 311-318. ACL \u201902, Association for Computational Linguistics, Stroudsburg, PA, USA (2002). https://doi.org/10.3115/1073083.1073135, https://doi.org/10.3115/1073083.1073135  In: Proceedings  of machine  translation.  evaluation  24. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazeb- nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: 2015 IEEE International Conference on Computer Vision (ICCV). pp. 2641-2649 (2015). https://doi.org/10.1109/ICCV.2015.303 25. Ren, Z., Wang, X., Zhang, N., Lv, X., Li, L.J.: Deep reinforcement learning- based image captioning with embedding reward. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1151-1159 (2017). https://doi.org/10.1109/CVPR.2017.128  26. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence training for image captioning. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)  27. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556 (2014), http://arxiv.org/abs/1409.1556 28. Sugano, Y., Bulling, A.: Seeing with humans: Gaze-assisted neural image caption-  ing. CoRR abs/1608.05203 (2016), http://arxiv.org/abs/1608.05203  29. Tavakoliy, H.R., Shetty, R., Borji, A., Laaksonen, J.: Paying attention to descriptions generated by image captioning models. In: 2017 IEEE Inter- national Conference on Computer Vision (ICCV). pp. 2506-2515 (2017). https://doi.org/10.1109/ICCV.2017.272   Boosted Attention Captioning30. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image de- scription evaluation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. pp. 4566-4575. IEEE Computer Society (2015). https://doi.org/10.1109/CVPR.2015.7299087, https://doi.org/10.1109/CVPR.2015.7299087  31. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption generator. In: CVPR. pp. 3156-3164. IEEE Computer Society (2015) 32. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Ben- gio, Y.: Show, attend and tell: Neural image caption generation with visual atten- tion. In: Blei, D., Bach, F. (eds.) Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pp. 2048-2057. JMLR Workshop and Conference Proceedings (2015), http://jmlr.org/proceedings/papers/v37/xuc15.pdf  33. Yang, Z., Yuan, Y., Wu, Y., Cohen, W.W., Salakhutdinov, R.R.: Review net- works for caption generation. In: Lee, D.D., Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neural Information Processing Systems 29, pp. 2361-2369. Curran Associates, Inc. (2016), http://papers.nips.cc/paper/6167- review-networks-for-caption-generation.pdf  34. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic attention. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4651-4659 (2016). https://doi.org/10.1109/CVPR.2016.503  35. Yun, K., Peng, Y., Samaras, D., Zelinsky, G.J., Berg, T.L.: Studying relationships between human gaze, description, and computer vision. In: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Computer Society Conference on. IEEE (2013)"}