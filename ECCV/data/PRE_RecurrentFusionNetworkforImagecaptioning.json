{"1": "1. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence to sequence-video to text. In: Proceedings of the IEEE international conference on computer vision. (2015) 4534-4542  2. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events  in videos. In: ICCV. (2017) 706-715  3. Chen, Y., Wang, S., Zhang, W., Huang, Q.: Less is more: Picking informative  frames for video captioning. arXiv preprint arXiv:1803.01457 (2018)  4. Wang, B., Ma, L., Zhang, W., Liu, W.: Reconstruction network for video caption- ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018) 7622-7631  5. Wang, J., Jiang, W., Ma, L., Liu, W., Xu, Y.: Bidirectional attentive fusion with context gating for dense video captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018) 7190-7198  6. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image In: Proceedings of the IEEE conference on computer vision and  descriptions. pattern recognition. (2015) 3128-3137  7. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. IEEE transactions on pattern analysis and machine intelligence 39(4) (2017) 652-663  8. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional  networks for visual recognition. ECCV (2014)  9. Krizhevsky, A., Sutskever, I., Hinton, G.E.:  convolutional neural networks. systems. (2012) 1097-1105  Imagenet classification with deep In: Advances in neural information processing  10. Cho, K., Van Merri\u00a8enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder- decoder for statistical machine translation. arXiv:1406.1078 (2014)  11. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: ICLR 2015. (2014)  12. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML. (2015)  13. Chen, X., Ma, L., Jiang, W., Yao, J., Liu, W.: Regularizing rnns for caption generation by reconstructing the past with the present. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (June 2018)  14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni- In: Proceedings of the IEEE conference on computer vision and pattern  tion. recognition. (2016) 770-778  15. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the in-  ception architecture for computer vision. In: CVPR. (2016)  16. Szegedy, C., Io\ufb00e, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: AAAI. (2017) 4278-4284 17. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: Advances in neural information processing systems. (2014) 3104- 3112  18. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image  caption generator. In: CVPR. (2015) 3156-3164   Recurrent Fusion Network for Image CaptioningReferences  1. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence to sequence-video to text. In: Proceedings of the IEEE international conference on computer vision. (2015) 4534-4542  2. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events  in videos. In: ICCV. (2017) 706-715  3. Chen, Y., Wang, S., Zhang, W., Huang, Q.: Less is more: Picking informative  frames for video captioning. arXiv preprint arXiv:1803.01457 (2018)  4. Wang, B., Ma, L., Zhang, W., Liu, W.: Reconstruction network for video caption- ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018) 7622-7631  5. Wang, J., Jiang, W., Ma, L., Liu, W., Xu, Y.: Bidirectional attentive fusion with context gating for dense video captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018) 7190-7198  6. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image In: Proceedings of the IEEE conference on computer vision and  descriptions. pattern recognition. (2015) 3128-3137  7. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. IEEE transactions on pattern analysis and machine intelligence 39(4) (2017) 652-663  8. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional  networks for visual recognition. ECCV (2014)  9. Krizhevsky, A., Sutskever, I., Hinton, G.E.:  convolutional neural networks. systems. (2012) 1097-1105  Imagenet classification with deep In: Advances in neural information processing  10. Cho, K., Van Merri\u00a8enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder- decoder for statistical machine translation. arXiv:1406.1078 (2014)  11. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. In: ICLR 2015. (2014)  12. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: ICML. (2015)  13. Chen, X., Ma, L., Jiang, W., Yao, J., Liu, W.: Regularizing rnns for caption generation by reconstructing the past with the present. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (June 2018)  14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni- In: Proceedings of the IEEE conference on computer vision and pattern  tion. recognition. (2016) 770-778  15. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the in-  ception architecture for computer vision. In: CVPR. (2016)  16. Szegedy, C., Io\ufb00e, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: AAAI. (2017) 4278-4284 17. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: Advances in neural information processing systems. (2014) 3104- 3112  18. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image  caption generator. In: CVPR. (2015) 3156-3164   16  Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, Tong Zhang  19. Yang, Z., Yuan, Y., Wu, Y., Cohen, W.W., Salakhutdinov, R.R.: Review networks  for caption generation. In: NIPS. (2016)  20. Jiang, W., Ma, L., Chen, X., Zhang, H., Liu, W.: Learning to guide decoding for image captioning. The Thirty-Second AAAI Conference on Artificial Intelligence (2018)  21. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive atten- tion via a visual sentinel for image captioning. arXiv preprint arXiv:1612.01887 (2016)  22. Wu, Q., Shen, C., Liu, L., Dick, A., van den Hengel, A.: What value do explicit high level concepts have in vision to language problems? In: CVPR. (2016) 23. Yao, T., Pan, Y., Li, Y., Qiu, Z., Mei, T.: Boosting image captioning with at- tributes. In: The IEEE International Conference on Computer Vision (ICCV). (Oct 2017)  24. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence training for image captioning. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (July 2017)  25. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image  description evaluation. In: CVPR. (2015)  26. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist  reinforcement learning. Machine learning 8(3-4) (1992) 229-256  27. Luong, T., Le, Q.V., Sutskever, I., Vinyals, O., Kaiser, L.: Multi-task sequence to sequence learning. In: International Conference on Learning Representations. (2016)  28. Firat, O., Cho, K., Bengio, Y.: Multi-way, multilingual neural machine translation with a shared attention mechanism. In: Proceedings of NAACL-HLT. (2016) 866- 875  29. Xu, C., Tao, D., Xu, C.: A survey on multi-view learning.  arXiv preprint  arXiv:1304.5634 (2013)  30. Zhou, Z.H.: Ensemble methods: foundations and algorithms. CRC press (2012) 31. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)  32. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation  33. Kalchbrenner, N., Danihelka, I., Graves, A.: Grid long short-term memory. arXiv  9(8) (1997) 1735-1780  preprint arXiv:1507.01526 (2015)  34. Xu, H., Saenko, K.: Ask, Attend and Answer: Exploring Question-Guided Spatial  Attention for Visual Question Answering. ECCV (2016)  35. Fang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Doll\u00b4ar, P., Gao, J., He, X., Mitchell, M., Platt, J.C., et al.: From captions to visual concepts and back. In: CVPR. (2015) 1473-1482  36. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. (2014) 37. Mun, J., Cho, M., Han, B.: Text-guided attention model for image captioning.  AAAI (2017)  38. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected con- volutional networks. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (July 2017)  39. Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with  recurrent neural networks. ICLR-15 (2015)   Recurrent Fusion Network for Image Captioning40. Ren, Z., Wang, X., Zhang, N., Lv, X., Li, L.J.: Deep reinforcement learning-based image captioning with embedding reward. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (July 2017)  41. Liu, S., Zhu, Z., Ye, N., Guadarrama, S., Murphy, K.: Improved image captioning via policy gradient optimization of spider. In: The IEEE International Conference on Computer Vision (ICCV). (Oct 2017)  42. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017)  43. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence prediction with recurrent neural networks. In: Advances in Neural Information Processing Systems. (2015) 1171-1179  44. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. The Interna-  tional Conference on Learning Representations (ICLR) (2015)  45. Liu, H., Yang, Y., Shen, F., Duan, L., Shen, H.T.: Recurrent image captioner: Describing images with spatial-invariant trnsformation and attention fieltering. arXiv:1612.04949 (2016)  46. Gu, J., Wang, G., Chen, T.: Recurrent highway networks with language cnn for  image captioning. arXiv:1612.07086 (2016)  47. Zhou, L., Xu, C., Koch, P., Corso, J.J.: Watch what you just said: Image cap-  tioning with text-conditional attention. arXiv:1606.04621 (2016)  48. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic  attention. In: CVPR. (2016) 4651-4659  49. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with  multimodal recurrent neural networks (m-rnn). ICLR (2015)  50. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2015) 2625-2634  51. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE (2009) 248-255  52. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. (2016)  53. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic  evaluation of machine translation. In: ACL. (2002)  54. Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with  improved correlation with human judgments. In: ACL workshop. (2005)  55. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: ACL-04  workshop. (2004)  56. Anderson, P., Fernando, B., Johnson, M., Gould, S.: Spice: Semantic propositional image caption evaluation. In: European Conference on Computer Vision, Springer (2016) 382-398"}