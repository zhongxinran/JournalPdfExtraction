{"1": "1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017)  2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. arXiv preprint arXiv:1409.0473 (2014)  3. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)  4. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  5. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2625-2634 (2015)  6. Elliott, D., Keller, F.: Image description using visual dependency representations. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1292-1302 (2013)  7. Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Doll\u00b4ar, P., Gao, J., He, X., Mitchell, M., Platt, J., et al.: From captions to visual concepts and back (2015) 8. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: European conference on computer vision. pp. 15-29. Springer (2010)  9. Gan, C., Gan, Z., He, X., Gao, J., Deng, L.: Stylenet: Generating attractive visual  captions with styles. In: CVPR (2017)  10. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on. pp. 2414-2423. IEEE (2016)  11. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. arXiv preprint arXiv:1802.08218 (2018)  12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)  13. Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom, P.: Teaching machines to read and comprehend. In: Advances in Neural Information Processing Systems. pp. 1693-1701 (2015)  14. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a rank- ing task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research 47, 853-899 (2013)  15. Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., Xing, E.P.: Toward controlled generation of text. In: International Conference on Machine Learning. pp. 1587- 1596 (2017)  16. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: European Conference on Computer Vision. pp. 694-711. Springer (2016)  17. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   Stylized Image Captioning with Adaptive Learning and AttentionReferences  1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017)  2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning  to align and translate. arXiv preprint arXiv:1409.0473 (2014)  3. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)  4. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  5. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2625-2634 (2015)  6. Elliott, D., Keller, F.: Image description using visual dependency representations. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1292-1302 (2013)  7. Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Doll\u00b4ar, P., Gao, J., He, X., Mitchell, M., Platt, J., et al.: From captions to visual concepts and back (2015) 8. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: European conference on computer vision. pp. 15-29. Springer (2010)  9. Gan, C., Gan, Z., He, X., Gao, J., Deng, L.: Stylenet: Generating attractive visual  captions with styles. In: CVPR (2017)  10. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on. pp. 2414-2423. IEEE (2016)  11. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. arXiv preprint arXiv:1802.08218 (2018)  12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)  13. Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom, P.: Teaching machines to read and comprehend. In: Advances in Neural Information Processing Systems. pp. 1693-1701 (2015)  14. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a rank- ing task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research 47, 853-899 (2013)  15. Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., Xing, E.P.: Toward controlled generation of text. In: International Conference on Machine Learning. pp. 1587- 1596 (2017)  16. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: European Conference on Computer Vision. pp. 694-711. Springer (2016)  17. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   16  T.Chen, Z.Zhang, Q.You, C.Fang, Z.Wang, H.Jin, J.Luo  18. Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Baby talk: Understanding and generating image descriptions. In: Proceedings of the 24th CVPR. Citeseer (2011)  19. Kuznetsova, P., Ordonez, V., Berg, A.C., Berg, T.L., Choi, Y.: Collective genera- tion of natural image descriptions. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. pp. 359- 368. Association for Computational Linguistics (2012)  20. Kuznetsova, P., Ordonez, V., Berg, T., Choi, Y.: Treetalk: Composition and com- pression of trees for image descriptions. Transactions of the Association of Com- putational Linguistics 2(1), 351-362 (2014)  21. Lebret, R., Pinheiro, P.O., Collobert, R.: Simple image description generator via  a linear phrase-based approach. arXiv preprint arXiv:1412.8419 (2014)  22. Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple image descriptions using web-scale n-grams. In: Proceedings of the Fifteenth Conference on Computational Natural Language Learning. pp. 220-228. Association for Com- putational Linguistics (2011)  23. Li, Y., Yao, T., Mei, T., Chao, H., Rui, Y.: Share-and-chat: Achieving human-level video commenting by search and multi-view embedding. In: Proceedings of the 2016 ACM on Multimedia Conference. pp. 928-937. ACM (2016)  24. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). vol. 6 (2017)  25. Luong, M.T., Le, Q.V., Sutskever, I., Vinyals, O., Kaiser, L.: Multi-task sequence  to sequence learning. arXiv preprint arXiv:1511.06114 (2015)  26. Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., Yuille, A.L.: Learning like a child: Fast novel visual concept learning from sentence descriptions of images. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2533- 2541 (2015)  27. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632 (2014)  28. Mathews, A.P., Xie, L., He, X.: Senticap: Generating image descriptions with sen-  timents. In: AAAI. pp. 3574-3580 (2016)  29. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention. In:  Advances in neural information processing systems. pp. 2204-2212 (2014)  30. Neumann, L., Neumann, A.: Color style transfer techniques using hue, lightness and saturation histogram matching. In: Computational Aesthetics. pp. 111-122. Citeseer (2005)  31. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: Advances in neural information processing systems. pp. 1143-1151 (2011)  32. Rockt\u00a8aschel, T., Grefenstette, E., Hermann, K.M., Ko\u02c7cisk`y, T., Blunsom, P.: Rea- soning about entailment with neural attention. arXiv preprint arXiv:1509.06664 (2015)  33. Rush, A.M., Chopra, S., Weston, J.: A neural attention model for abstractive  sentence summarization. arXiv preprint arXiv:1509.00685 (2015)  34. Schweikert, G., R\u00a8atsch, G., Widmer, C., Sch\u00a8olkopf, B.: An empirical analysis of do- main adaptation algorithms for genomic sequence analysis. In: Advances in Neural Information Processing Systems. pp. 1433-1440 (2009)   Stylized Image Captioning with Adaptive Learning and Attention35. Shen, T., Lei, T., Barzilay, R., Jaakkola, T.: Style transfer from non-parallel text by cross-alignment. In: Advances in Neural Information Processing Systems. pp. 6833-6844 (2017)  36. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint arXiv:1409.1556 (2014)  37. Spratling, M.W., Johnson, M.H.: A feedback model of visual attention. Journal of  cognitive neuroscience 16(2), 219-237 (2004)  38. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: Advances in neural information processing systems. pp. 3104-3112 (2014)  39. Tang, Y., Srivastava, N., Salakhutdinov, R.R.: Learning generative models with visual attention. In: Advances in Neural Information Processing Systems. pp. 1808- 1816 (2014)  40. Tran, K., He, X., Zhang, L., Sun, J.: Rich image captioning in the wild. In: Com- puter Vision and Pattern Recognition Workshops (CVPRW), 2016 IEEE Confer- ence on. pp. 434-441. IEEE (2016)  41. Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: Feed- forward synthesis of textures and stylized images. In: ICML. pp. 1349-1357 (2016) 42. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption generator. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. pp. 3156-3164. IEEE (2015)  43. Wu, Z.Y.Y.Y.Y., Cohen, R.S.W.W.: Encode, review, and decode: Reviewer module  for caption generation. arXiv preprint arXiv:1605.07912 (2016)  44. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: International Conference on Machine Learning. pp. 2048-2057 (2015) 45. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic attention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4651-4659 (2016)"}