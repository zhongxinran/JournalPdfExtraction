{"1": "1. Anderson, P., Fernando, B., Johnson, M., Gould, S.: Spice: Semantic propositional  image caption evaluation. In: ECCV (2016)  2. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017)  3. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence prediction with recurrent neural networks. In: Advances in Neural Information Processing Systems. pp. 1171-1179 (2015)  4. Chen, H., Ding, G., Zhao, S., Han, J.: Temporal-di\ufb00erence learning with sampling  baseline for image captioning (2017)  5. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn: Spa- tial and channel-wise attention in convolutional networks for image captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5659-5667 (2017)  6. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)  7. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  8. Dai, B., Fidler, S., Urtasun, R., Lin, D.: Towards diverse and natural image descrip- tions via a conditional gan. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2970-2979 (2017)  9. Dai, B., Lin, D.: Contrastive learning for image captioning. In: Advances in Neural  Information Processing Systems. pp. 898-907 (2017)  10. Denkowski, M., Lavie, A.: Meteor universal: Language specific translation evalua- tion for any target language. In: Proceedings of the ninth workshop on statistical machine translation. pp. 376-380 (2014)  11. Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., Mitchell, M.: Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809 (2015)  12. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: Vse++: Improved visual-semantic  embeddings. arXiv preprint arXiv:1707.05612 (2017)  13. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: European conference on computer vision. pp. 15-29. Springer (2010)  14. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Semantic compositional networks for visual captioning. In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. vol. 2 (2017)  15. Gu, J., Cai, J., Wang, G., Chen, T.: Stack-captioning: Coarse-to-fine learning for  image captioning. arXiv preprint arXiv:1709.03376 (2017)  16. Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image captioning. In: Proceedings of the International Conference on Computer Vision (ICCV) (2017)  17. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   Show, Tell and DiscriminateReferences  1. Anderson, P., Fernando, B., Johnson, M., Gould, S.: Spice: Semantic propositional  image caption evaluation. In: ECCV (2016)  2. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017)  3. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence prediction with recurrent neural networks. In: Advances in Neural Information Processing Systems. pp. 1171-1179 (2015)  4. Chen, H., Ding, G., Zhao, S., Han, J.: Temporal-di\ufb00erence learning with sampling  baseline for image captioning (2017)  5. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn: Spa- tial and channel-wise attention in convolutional networks for image captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5659-5667 (2017)  6. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)  7. Chen, X., Lawrence Zitnick, C.: Mind\u2019s eye: A recurrent visual representation for image caption generation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2422-2431 (2015)  8. Dai, B., Fidler, S., Urtasun, R., Lin, D.: Towards diverse and natural image descrip- tions via a conditional gan. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2970-2979 (2017)  9. Dai, B., Lin, D.: Contrastive learning for image captioning. In: Advances in Neural  Information Processing Systems. pp. 898-907 (2017)  10. Denkowski, M., Lavie, A.: Meteor universal: Language specific translation evalua- tion for any target language. In: Proceedings of the ninth workshop on statistical machine translation. pp. 376-380 (2014)  11. Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., Mitchell, M.: Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809 (2015)  12. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: Vse++: Improved visual-semantic  embeddings. arXiv preprint arXiv:1707.05612 (2017)  13. Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: European conference on computer vision. pp. 15-29. Springer (2010)  14. Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., Deng, L.: Semantic compositional networks for visual captioning. In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. vol. 2 (2017)  15. Gu, J., Cai, J., Wang, G., Chen, T.: Stack-captioning: Coarse-to-fine learning for  image captioning. arXiv preprint arXiv:1709.03376 (2017)  16. Gu, J., Wang, G., Cai, J., Chen, T.: An empirical study of language cnn for image captioning. In: Proceedings of the International Conference on Computer Vision (ICCV) (2017)  17. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image de- scriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)   16  X. Liu. H. Li, J. Shao, D. Chen, X. Wang  18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980 (2014)  19. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision 123(1), 32-73 (2017)  20. Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Babytalk: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence 35(12), 2891- 2903 (2013)  21. Li, Y., Ouyang, W., Zhou, B., Cui, Y., Shi, J., Wang, X.: Factorizable net: An  e\ufb03cient subgraph-based framework for scene graph generation (2018)  22. Li, Y., Ouyang, W., Zhou, B., Wang, K., Wang, X.: Scene graph generation from  objects, phrases and region captions. In: ICCV (2017)  23. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. Text Summa-  rization Branches Out (2004)  24. Liu, S., Zhu, Z., Ye, N., Guadarrama, S., Murphy, K.: Improved image captioning via policy gradient optimization of spider. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 873-881 (2017)  25. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.  arXiv preprint arXiv:1608.03983 (2016)  26. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). vol. 6 (2017)  27. Luo, R., Price, B., Cohen, S., Shakhnarovich, G.: Discriminability objective for training descriptive captions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6964-6974 (2018)  28. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632 (2014)  29. Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A., Yamaguchi, K., Berg, T., Stratos, K., Daum\u00b4e III, H.: Midge: Generating image descriptions from computer vision detections. In: Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. pp. 747-756. Association for Computational Linguistics (2012)  30. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting on association for computational linguistics. pp. 311-318. Association for Computa- tional Linguistics (2002)  31. Pedersoli, M., Lucas, T., Schmid, C., Verbeek, J.: Areas of attention for image  captioning. In: ICCV-International Conference on Computer Vision (2017)  32. Pu, Y., Gan, Z., Henao, R., Yuan, X., Li, C., Stevens, A., Carin, L.: Variational autoencoder for deep learning of images, labels and captions. In: Advances in neural information processing systems. pp. 2352-2360 (2016)  33. Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with  recurrent neural networks. arXiv preprint arXiv:1511.06732 (2015)  34. Ren, Z., Wang, X., Zhang, N., Lv, X., Li, L.J.: Deep reinforcement learning-based image captioning with embedding reward. In: Computer Vision and Pattern Recog- nition (CVPR), 2017 IEEE Conference on. pp. 1151-1159. IEEE (2017)   Show, Tell and Discriminate35. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence training for image captioning. In: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition. pp. 7008-7024 (2017)  36. Shetty, R., Rohrbach, M., Hendricks, L.A., Fritz, M., Schiele, B.: Speaking the same language: Matching machine to human captions by adversarial training. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2017)  37. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction, vol. 1. MIT  press Cambridge (1998)  38. Vedantam, R., Bengio, S., Murphy, K., Parikh, D., Chechik, G.: Context-aware cap- tions from context-agnostic supervision. In: Computer Vision and Pattern Recog- nition (CVPR). vol. 3 (2017)  39. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4566-4575 (2015)  40. Vijayakumar, A.K., Cogswell, M., Selvaraju, R.R., Sun, Q., Lee, S., Crandall, D., Batra, D.: Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424 (2016)  41. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption generator. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. pp. 3156-3164. IEEE (2015)  42. Wang, L., Schwing, A., Lazebnik, S.: Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space. In: Advances in Neural Information Processing Systems. pp. 5758-5768 (2017)  43. Wang, Y., Lin, Z., Shen, X., Cohen, S., Cottrell, G.W.: Skeleton key: Image caption- ing by skeleton-attribute decomposition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7272-7281 (2017)  44. Wang, Z., Wu, F., Lu, W., Xiao, J., Li, X., Zhang, Z., Zhuang, Y.: Diverse image  captioning via grouptalk. In: IJCAI. pp. 2957-2964 (2016)  45. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist  reinforcement learning. Machine Learning 8(3-4), 229-256 (1992)  46. Wu, Q., Shen, C., Liu, L., Dick, A., van den Hengel, A.: What value do explicit high level concepts have in vision to language problems? In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 203-212 (2016) 47. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: International Conference on Machine Learning. pp. 2048-2057 (2015) 48. Yang, Y., Teo, C.L., Daum\u00b4e III, H., Aloimonos, Y.: Corpus-guided sentence gener- ation of natural images. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 444-454. Association for Computational Lin- guistics (2011)  49. Yao, T., Pan, Y., Li, Y., Qiu, Z., Mei, T.: Boosting image captioning with at- tributes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4894-4902 (2017)  50. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image captioning with semantic attention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4651-4659 (2016)  51. Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics 2, 67-78 (2014)"}