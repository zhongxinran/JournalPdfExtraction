{"1": "1. Patel, V.M., Gopalan, R., Li, R., Chellappa, R.: Visual domain adaptation: a  survey of recent advances. IEEE SPM 32(3) (2015) 53-69   14  B.B. Damodaran et al.  Table 4. Performance of DeepJDOT on the VisDA 2017 classification challenge. The scores in the bracket indicate the accuracy di\ufb00erence between the source (unadapted) model and target (adapted) model. The respective values of CORAL and DAN are reported from the evaluation server6.  Method Source only  plane bcycl bus 36.0  4.0 19.9 94.7 14.8 0.42 38.7 37.4 DeepCORAL [6] 62.5 21.7 66.3 64.6 31.1 36.7 54.2 73.8 29.9 43.4 34.2 45.3 (19.0) 55.3 18.4 59.8 68.6 55.3 41.4 63.4 78.8 23.0 62.9 40.2 49.8 (19.5) 85.4 50.4 77.3 87.3 69.1 14.1 91.5 53.3 91.9 31.2 88.5 61.8 66.9 (38.9)  car horse knife mcycl person plant sktbd train truck Mean 28.0  DAN [47] DeepJDOT  3.8 24.9 30.4  71.9  8.1  6.7  and target model. Our method is ranked sixth when the mean accuracy is consid- ered, and third when the di\ufb00erence between the source model and target model is considered at the time of publication. It is noted that the performance of our method depends on the capacity of the source model: if a larger CNN is used, the performance of our method could be improved further.  6 Conclusions  In this paper, we proposed the DeepJDOT model for unsupervised deep domain adaptation based on optimal transport. The proposed method aims at learning a common latent space for the source and target distributions, that conveys dis- criminant information for both domains. This is achieved by minimizing the dis- crepancy of joint deep feature/labels domain distributions by means of optimal transport. We propose an e\ufb03cient stochastic algorithm that solves this problem, and despite being simple and easily integrable into modern deep learning frame- works, our method outperformed the state-of-the-art on cross domain digits and o\ufb03ce-home adaptation, and provided satisfactory results on the VisDA-2017 adaptation.  Future works will consider the evaluation of this method in multi-domains scenario, as well as more complicated cost functions taking into account simi- larities of the representations across the embedding layers and/or similarities of labels across di\ufb00erent classifiers.  Acknowledgement  This work benefited from the support of Region Bretagne grant and OATMIL ANR-17-CE23-0012 project of the French National Research Agency (ANR). The constructive comments and suggestions of anonymous reviewers are grate- fully acknowledged.  References  1. Patel, V.M., Gopalan, R., Li, R., Chellappa, R.: Visual domain adaptation: a  survey of recent advances. IEEE SPM 32(3) (2015) 53-69   DeepJDOT2. Saenko, K., Kulis, B., Fritz, M., Darrell, T.: Adapting visual category models to  new domains. In: ECCV. (2010) 213-226  3. Gopalan, R., Li, R., Chellappa, R.: Domain adaptation for object recognition: An  unsupervised approach. In: ICCV. (2011) 999-1006  4. Courty, N., Flamary, R., Tuia, D., Rakotomamonjy, A.: Optimal transport for  domain adaptation. IEEE TPAMI 39(9) (2017) 1853-1865  5. Courty, N., Flamary, R., Habrard, A., Rakotomamonjy, A.: Joint distribution  optimal transportation for domain adaptation. In: NIPS. (2017)  6. Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation.  In: ECCV workshops. (2016) 443-450  7. Luo, Z., Zou, Y., Ho\ufb00man, J., Fei-Fei, L.: Label e\ufb03cient learning of transferable  representations across domains and tasks. In: NIPS. (2017)  8. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. J. Mach. Learn. Res. 17(1) (January 2016) 2096-2030  9. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks.  In Lee, D.D., Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R., eds.: NIPS. (2016) 469-477 10. Ben-David, S., Blitzer, J., Crammer, K., Pereira, F.: Analysis of representations  for domain adaptation. In: NIPS. (2007) 137-144  11. Jhuo, I.H., Liu, D., Lee, D.T., Chang, S.F.: Robust visual domain adaptation with  low-rank reconstruction. In: CVPR. (2012) 2168-2175  12. Ho\ufb00man, J., Rodner, E., Donahue, J., Saenko, K., Darrell, T.: E\ufb03cient learning  of domain-invariant image representations. In: ICLR. (2013)  13. Aljundi, R., Tuytelaars, T.: Lightweight unsupervised domain adaptation by con-  volutional filter reconstruction. In: ECCV. (2016)  14. Long, M., Cao, Y., Wang, J., Jordan, M.I.: Learning transferable features with  deep adaptation networks. In: ICML. (2015) 97-105  15. Long, M., Wang, J., Jordan, M.I.: Unsupervised domain adaptation with residual  16. Haeusser, P., Frerix, T., Mordvintsev, A., Cremers, D.: Associative domain adap-  17. Tzeng, E., Ho\ufb00man, J., Darrell, T., Saenko, K.: Simultaneous deep transfer across  transfer networks. In: NIPS. (2016)  tation. In: ICCV. (2017)  domains and tasks. In: ICCV. (2015)  18. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net- In Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vish-  works. wanathan, S., Garnett, R., eds.: NIPS. (2017) 700-708  19. Sankaranarayanan, S., Balaji, Y., Castillo, C.D., Chellappa, R.: Generate to adapt: Aligning domains using generative adversarial networks. CoRR abs/1704.01705 (2017)  20. Murez, Z., Kolouri, S., Kriegman, D., Ramamoorthi, R., Kim, K.: Image to Image  Translation for Domain Adaptation. ArXiv e-prints (December 2017)  21. Tzeng, E., Ho\ufb00man, J., Darrell, T., Saenko, K.: Adversarial discriminative domain  22. Monge, G.: M\u00b4emoire sur la th\u00b4eorie des d\u00b4eblais et des remblais. De l\u2019Imprimerie  adaptation. In: CVPR. (2017)  Royale (1781)  (N.S.) 37 (1942) 199-201  Wissenschaften. Springer (2009)  transport. In: ECML. (2014)  23. Kantorovich, L.: On the translocation of masses. C.R. (Doklady) Acad. Sci. URSS  24. Villani, C.: Optimal transport: old and new. Grundlehren der mathematischen  25. Courty, N., Flamary, R., Tuia, D.: Domain adaptation with regularized optimal   16  B.B. Damodaran et al.  26. Perrot, M., Courty, N., Flamary, R., Habrard, A.: Mapping estimation for discrete  optimal transport. In: NIPS. (2016) 4197-4205  27. Redko, I., Habrard, A., Sebban, M.: Theoretical analysis of domain adaptation  with optimal transport. In: ECML/PKDD. (2017) 737-753  28. Shen., J., Qu, Y., Zhang, W., Yu, Y.: Wasserstein distance guided representation  learning for domain adaptation. In: AAAI. (2018)  29. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.  30. Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transportation.  In: ICML. (2017) 214-223  In: NIPS. (2013) 2292-2300  31. Genevay, A., Cuturi, M., Peyr\u00b4e, G., Bach, F.: Stochastic optimization for large-  scale optimal transport. In: NIPS. (2016) 3432-3440  32. Seguy, V., Damodaran, B, B., Flamary, R., Courty, N., Rolet, A., Blondel, M.:  Large-scale optimal transport and mapping estimation. In: ICLR. (2018)  33. Shmelkov, K., Schmid, C., Alahari, K.: Incremental learning of object detectors  without catastrophic forgetting. In: ICCV, Venice, Italy (2017)  34. Li, Z., Hoiem, D.: Learning without forgetting. IEEE TPAMI (in press) 35. Genevay, A., Peyr\u00b4e, G., Cuturi, M.: Sinkhorn-autodi\ufb00: Tractable wasserstein learn-  ing of generative models. arXiv preprint arXiv:1706.00292 (2017)  36. Lecun, Y., Bottou, L., Bengio, Y., Ha\ufb00ner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11) (Nov 1998) 2278-2324 37. Hull, J.J.: A database for handwritten text recognition research. IEEE TPAMI  16(5) (May 1994) 550-554  38. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits in natural images with unsupervised feature learning. In: NIPS worksophs. (2011) 39. Arbelaez, P., Maire, M., Fowlkes, C., Malik, J.: Contour detection and hierarchical  image segmentation. IEEE TPAMI 33(5) (May 2011) 898-916  40. Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W.: Deep reconstruction- In: ECCV. (2016)  classification networks for unsupervised domain adaptation. 597-613  41. Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., Erhan, D.: Domain  separation networks. In: NIPS. (2016) 343-351  42. French, G., Mackiewicz, M., Fisher, M.: Self-ensembling for visual domain adap-  tation. In: International Conference on Learning Representations. (2018)  43. Venkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing network for unsupervised domain adaptation. In: (IEEE) Conference on Computer Vision and Pattern Recognition (CVPR). (2017)  44. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale  image recognition. arXiv preprint arXiv:1409.1556 (2014)  45. Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI\u201916, AAAI Press (2016) 2058-2065  46. Long, M., Wang, J., Ding, G., Sun, J., Yu, P.S.: Transfer feature learning with joint distribution adaptation. In: 2013 IEEE International Conference on Computer Vision. (Dec 2013) 2200-2207  47. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep adaptation networks. In Bach, F., Blei, D., eds.: Proceedings of the 32nd Inter- national Conference on Machine Learning. Volume 37 of Proceedings of Machine Learning Research., Lille, France, PMLR (07-09 Jul 2015) 97-105  48. Peng, X., Usman, B., Kaushik, N., Ho\ufb00man, J., Wang, D., Saenko, K.: Visda: The  visual domain adaptation challenge (2017)   DeepJDOT49. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision, Springer (2014) 740-755  50. Real, E., Shlens, J., Mazzocchi, S., Pan, X., Vanhoucke, V.:  Youtube- boundingboxes: A large high-precision human-annotated data set for object de- In: Computer Vision and Pattern Recognition (CVPR), 2017 tection in video. IEEE Conference on, IEEE (2017) 7464-7473  51. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 770-778"}