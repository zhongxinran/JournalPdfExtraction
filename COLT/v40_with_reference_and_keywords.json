{"Conference on Learning Theory 2015: Preface": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Conference on Learning Theory 2015: Preface", "abstract": "Preface to COLT 2015", "pdf_url": "http://proceedings.mlr.press/v40/Grunwald15.pdf", "keywords": []}, "Open Problem: Restricted Eigenvalue Condition for Heavy Tailed Designs": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: Restricted Eigenvalue Condition for Heavy Tailed Designs", "abstract": "The restricted eigenvalue (RE) condition characterizes the sample complexity of accurate recovery in the context of high-dimensional estimators such as Lasso and Dantzig selector (Bickel et al., 2009). Recent work has shown that random design matrices drawn from any thin-tailed (sub-Gaussian) distributions satisfy the RE condition with high probability, when the number of samples scale as the square of the Gaussian width of the restricted set (Banerjee et al., 2014; Tropp, 2015). We pose the equivalent question for heavy-tailed distributions: Given a random design matrix drawn from a heavy-tailed distribution satisfying the smallball property (Mendelson, 2015), does the design matrix satisfy the RE condition with the same order of sample complexity as sub-Gaussian distributions? An answer to the question will guide the design of highdimensional estimators for heavy tailed problems.", "pdf_url": "http://proceedings.mlr.press/v40/Banerjee15.pdf", "keywords": [], "reference": "A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances in  Neural Information Processing Systems (NIPS), 2014.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals  of Statistics, 37(4):1705-1732, 2009.  of Statistics, 35(6):2313-2351, 2007.  E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse prob-  lems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm. In  Advances in Neural Information Processing Systems (NIPS), 2014.  V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix without concen-  tration. arXiv:1312.3580, 2013.  G. Lecu\u00b4e and S. Mendelson. Sparse recovery under weak moment assumptions. arXiv:1401.2188, 2014.  S. Mendelson. Learning without concentration. Journal of the ACM, 2015.  3   OPEN PROBLEM: RESTRICTED EIGENVALUE CONDITION FOR HEAVY TAILED DESIGNS  2 = (cid:80)n  In addition to the results for sub-Gaussian X and general A, the RE condition for some special A has been established when X is heavy tailed. Oliveira (2013) makes the observation that for a given i=1(cid:104)xi, u(cid:105)2 has sub-Gaussian lower tails under weak moment assumptions u \u2208 A, (cid:107)Xu(cid:107)2 on X. In order to turn this into a uniform bound for all u \u2208 A, tools like generic chaining or covering arguments require upper bounds on (cid:107)Xu(cid:107)2 2, thus fail for heavy tailed X. But in the special case when A is the unit sphere Sp\u22121, PAC Bayesian based arguments show a result similar to the following with high probability:  (3)  (4)  (cid:107)Xu(cid:107)2  2 \u2265 c1n \u2212 c3p .  inf u\u2208A  Since w2(Sp\u22121) = O(p), the RE condition holds for the special case of A = Sp\u22121. The result in Koltchinskii and Mendelson (2013) relies on the small-ball property of A and the VC dimension of the class of functions, T\u03be = {I|(cid:104)x,u(cid:105)|>\u03be : u \u2208 A}. Using empirical process theory, they prove that with high probability,  (cid:107)Xu(cid:107)2  2 \u2265 c1n \u2212 c3VC(T\u03be) ,  inf u\u2208A  where VC(\u00b7) denote VC dimension. When A is the unit sphere Sp\u22121 it can be shown that VC(T\u03be) = O(p) = O(w2(Sp\u22121)), and the result coincides with Oliveira (2013). Based on this VC dimension argument, Lecu\u00b4e and Mendelson (2014) show that the RE condition is also true when A is the set of all unit s-sparse vectors. While these results illustrate that RE condition for heavy tails is true for certain special cases of A, the result for general A \u2286 Sp\u22121 under the small-ball property remains open.  Acknowledgements: We thank Joel Tropp for helpful discussions. The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.  References  A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances in  Neural Information Processing Systems (NIPS), 2014.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals  of Statistics, 37(4):1705-1732, 2009.  of Statistics, 35(6):2313-2351, 2007.  E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse prob-  lems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm. In  Advances in Neural Information Processing Systems (NIPS), 2014.  V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix without concen-  tration. arXiv:1312.3580, 2013.  G. Lecu\u00b4e and S. Mendelson. Sparse recovery under weak moment assumptions. arXiv:1401.2188, 2014.  S. Mendelson. Learning without concentration. Journal of the ACM, 2015. BANERJEE CHEN SIVAKUMAR  S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for the analysis of regularized  M -estimators. Statistical Science, 27(4):538-557, 2012.  R. I. Oliveira. The lower tail of random quadratic forms, with applications to ordinary least squares and  restricted eigenvalue properties. arXiv:1312.2903, 2013.  M. Talagrand. The Generic Chaining. Springer, 2005.  R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society,  Series B, 58(1):267-288, 1996.  J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements.  In  Sampling Theory, a Renaissance. 2015. "}, "Open Problem: The landscape of the loss surfaces of multilayer networks": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: The landscape of the loss surfaces of multilayer networks", "abstract": "Deep learning has enjoyed a resurgence of interest in the last few years for such applications as image and speech recognition, or natural language processing. The vast majority of practical applications of deep learning focus on supervised learning, where the supervised loss function is minimized using stochastic gradient descent. The properties of this highly non-convex loss function, such as its landscape and the behavior of critical points (maxima, minima, and saddle points), as well as the reason why large- and small-size networks achieve radically different practical performance, are however very poorly understood. It was only recently shown that new results in spin-glass theory potentially may provide an explanation for these problems by establishing a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models. The connection between both models relies on a number of possibly unrealistic assumptions, yet the empirical evidence suggests that the connection may exist in real. The question we pose is whether it is possible to drop some of these assumptions to establish a stronger connection between both models.", "pdf_url": "http://proceedings.mlr.press/v40/Choromanska15.pdf", "keywords": ["multilayer networks", "deep learning", "spherical spin-glass model", "Hamiltonian", "nonconvex optimization"], "reference": "arXiv:1003.1129, 2010.  A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses.  A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of  multilayer networks. In AISTATS, 2015.  Y. Dauphin, R. Pascanu, C\u00b8 . G\u00a8ulc\u00b8ehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking  the saddle point problem in high-dimensional non-convex optimization. In NIPS. 2014.  M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Freitas. Predicting parameters in deep  Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the  V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,  learning. In NIPS. 2013.  trade. Springer, 1998.  2010.  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning  in deep linear neural networks. In ICLR. 2014.  1. Index of \u22072L at w is the number of negative eigenvalues of the Hessian \u22072L at w. Local minima have index 0.  3   OPEN PROBLEM: THE LANDSCAPE OF THE LOSS SURFACES OF MULTILAYER NETWORKS  EM,I1,I2,...,I\u03a8[L(w)] = \u03c1  (cid:48)  + \u03c1  Zi1,i2,...,iHwi1wi2 . . . wiH.  1 \u039b(H\u22121)/2  \u039b (cid:88)  i1,i2,...,iH=1  (cid:80)\u039b  i = 1.  i=1 w2  It is also assumed that Z\u2019s are independent (A6u) . Finally, the spherical assumption (A7p) imposes that 1 \u039b Note that the term in bold is a Hamiltonian of the spherical spin-glass model Auffinger et al. (2010). It was recently shown Auffinger et al. (2010) that the Hamiltonian of this model has inter- esting properties when the size of the model (\u039b) goes to \u221e. We next list these properties along with the possible interpretation for neural networks: (i) critical points form an ordered structure such that there exists an energy barrier (a certain value of the Hamiltonian) below which with overwhelming probability one can find only low-index1 critical points, most of which are concentrated close to the barrier (this would explain why in case of large networks recovered local minima are typically cor- responding to the same test performance which is not the case for small networks, (ii) Recovering the ground state, i.e. global minimum, takes exponentially long time, (iii) with overwhelming prob- ability one can find only high-index saddle points above energy E\u2212\u221e and there are exponentially many of those (this would explain the importance of saddle points in the optimization problem), (iv) low-index critical points are \u2019geometrically\u2019 lying closer to the ground state than high-index critical points (this would explain why recovering poor quality local minima, which are \u2019far\u2019 from the global minimum, is more likely for small-size networks than for large-size networks).  Open problem: Is it possible to establish a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models under milder assumptions? The central problem is to eliminate unrealistic assumptions of variable independence (A5-6u). Note that assumption A5u implies that the activation mechanism of any path (for the ith path it is denoted as Ii) is independent of the input data, which clearly cannot be true. Similarly, assumption A6u implies all paths have independent inputs, which cannot be true since many paths share the same input. Alternatively, it would also be desired to find network architectures for which the connection to spin-glass models can be established explicitly with only mild (plausible), if any, assumptions.  References  arXiv:1003.1129, 2010.  A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses.  A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of  multilayer networks. In AISTATS, 2015.  Y. Dauphin, R. Pascanu, C\u00b8 . G\u00a8ulc\u00b8ehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking  the saddle point problem in high-dimensional non-convex optimization. In NIPS. 2014.  M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Freitas. Predicting parameters in deep  Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the  V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,  learning. In NIPS. 2013.  trade. Springer, 1998.  2010.  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning  in deep linear neural networks. In ICLR. 2014.  1. Index of \u22072L at w is the number of negative eigenvalues of the Hessian \u22072L at w. Local minima have index 0. CHOROMANSKA LECUN BEN AROUS  "}, "Open Problem: The Oracle Complexity of Smooth Convex Optimization in Nonstandard Settings": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: The Oracle Complexity of Smooth Convex Optimization in Nonstandard Settings", "abstract": "First-order convex minimization algorithms are currently the methods of choice for large-scale sparse \u2013 and more generally parsimonious \u2013 regression models. We pose the question on the limits of performance of black-box oriented methods for convex minimization in \\em non-standard settings, where the regularity of the objective is measured in a norm not necessarily induced by the feasible domain. This question is studied for \\ell_p/\\ell_q-settings, and their matrix analogues (Schatten norms), where we find surprising gaps on lower bounds compared to state of the art methods. We propose a conjecture on the optimal convergence rates for these settings, for which a positive answer would lead to significant improvements on minimization algorithms for parsimonious regression models.", "pdf_url": "http://proceedings.mlr.press/v40/Guzman15.pdf", "keywords": [], "reference": "A. Agarwal, S. Negahban, and M. Wainwright. Fast global convergence of gradient methods for  high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452-2482, 10 2012.  C. Guzm\u00b4an. Information, Complexity and Structure in Convex Optimization. PhD thesis, Georgia  Institute of Technology, May 2015.  C. Guzm\u00b4an and A. Nemirovski. On lower complexity bounds for large-scale smooth convex opti-  mization. Journal of Complexity, 31(1):1 - 14, 2015.  A. Nemirovskii and Y. Nesterov. Optimal methods of smooth convex optimization (in Russian) .  Zh. Vychisl. Mat. i Mat. Fiz., 25(3):356-369, 1985.  Y. Nesterov. Smooth Minimization of Non-Smooth Functions. Mathematical Programming, 103  (1):127-152, 2005.  3. Notice that in the case the objective satisfies a restricted strong convexity property then projected gradient descent converges linearly up to the statistical error Agarwal et al. (2012). Our open problem considers models where we do not have such nice properties.  3   THE COMPLEXITY OF SMOOTH CONVEX OPTIMIZATION  Open Problem 1 What is the large-scale minimax risk of minimization within the class F(cid:107)\u00b7(cid:107)q (\u03ba, L) over the unit ball of (cid:96)n p in the black-box oracle model? Can the rate obtained by Nesterov\u2019s method be significantly improved?  We finish this discussion by showing the importance of closing these gaps for certain classes of regression problems. Our main motivation is the study of linear regression models, where we search for a linear predictor within a norm-bounded set X, e.g., X \u2286 Bn p ; and the performance of a predictor is measured by a loss function arising from random samples (a1, b1), . . . , (am, bm) \u2208 Bn jx \u2212 bj)2 : (cid:107)x(cid:107)p \u2264 1}, fits within the (cid:96)p/(cid:96)q-setting, with \u03ba = 2 and L = R = 1.  q\u2217 \u00d7 [\u22121, 1]. Thus the empirical risk minimization problem we obtain, min{ 1 m  j=1(a(cid:48)  (cid:80)m  Perhaps the most important application of the regression model above is the case of compressed sensing, where p = 1 and q = 2 (we can also consider the matrix analog of nuclear norm mini- mization for low-rank matrix recovery). In this case, Nesterov\u2019s method gives a rate of convergence O(1/T 2), whereas our lower bound is \u02dc\u2126(1/T 3). Our conjecture says that the optimal convergence rate here is better than O(1/T 2), although to the best of our knowledge, results on sublinear algo- rithms beyond this rate are nonexisting3. In this sense, we believe surpassing the O(1/T 2) rate is indeed an extremely challenging problem.  Finally, we believe any progress on designing faster algorithms in this setting, will not only have an impact on compressed sensing and low-rank matrix recovery, but more broadly in convex minimization methods for parsimonious regression models, and possibly to the stochastic and online settings.  I would like to thank Alexandre d\u2019Aspremont and Arkadi Nemirovski for valuable discussions lead- ing to this open problem.  Acknowledgments  References  A. Agarwal, S. Negahban, and M. Wainwright. Fast global convergence of gradient methods for  high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452-2482, 10 2012.  C. Guzm\u00b4an. Information, Complexity and Structure in Convex Optimization. PhD thesis, Georgia  Institute of Technology, May 2015.  C. Guzm\u00b4an and A. Nemirovski. On lower complexity bounds for large-scale smooth convex opti-  mization. Journal of Complexity, 31(1):1 - 14, 2015.  A. Nemirovskii and Y. Nesterov. Optimal methods of smooth convex optimization (in Russian) .  Zh. Vychisl. Mat. i Mat. Fiz., 25(3):356-369, 1985.  Y. Nesterov. Smooth Minimization of Non-Smooth Functions. Mathematical Programming, 103  (1):127-152, 2005.  3. Notice that in the case the objective satisfies a restricted strong convexity property then projected gradient descent converges linearly up to the statistical error Agarwal et al. (2012). Our open problem considers models where we do not have such nice properties. "}, "Open Problem: Online Sabotaged Shortest Path": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: Online Sabotaged Shortest Path", "abstract": "There has been much work on extending the prediction with expert advice methodology to the case when experts are composed of components and there are combinatorially many such experts. One of the core examples is the Online Shortest Path problem where the components are edges and the experts are paths. In this note we revisit this online routing problem in the case where in each trial some of the edges or components are sabotaged / blocked. In the vanilla expert setting a known method can solve this extension where experts are now awake or asleep in each trial. We ask whether this technology can be upgraded efficiently to the case when at each trial every component can be awake or asleep. It is easy get to get an initial regret bound by using combinatorially many experts. However it is open whether there are efficient algorithms achieving the same regret.", "pdf_url": "http://proceedings.mlr.press/v40/Koolen15b.pdf", "keywords": ["Online learning", "combinatorial experts", "specialist", "sleeping"], "reference": "Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial optimization.  Math. Oper. Res., 39(1):31-45, 2014.  Nicol`o Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and  feels no regret). In Advances in Neural Information Processing Systems 25, pages 980-988, 2012.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proc. 29th Annual ACM Symposium on Theory of Computing, pages 334-343, 1997.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. Transactions on Computation  Theory, 6(3):11:1-11:16, 2014.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts  and bandits. Machine Learning, 80(2-3):245-272, 2010.  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In Proceedings  of the 23rd Annual Conference on Learning Theory, pages 93-105, 2010.  Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets and adver-  sarial losses. In Advances in Neural Information Processing Systems 27, pages 2780-2788, 2014.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine  Learning Research, 4:773-818, 2003.  1. Fancier bounds with mixture comparators and relative entropy are also available, but outside the scope of this note.  3   OPEN PROBLEM: ONLINE SABOTAGED SHORTEST PATH  \u221a  t:i\u2208At  (cid:1) \u2264  (cid:0)(cid:96)t,it \u2212 (cid:96)t,i  2012)1 that the the regret compared to each reference specialist i \u2208 {1, . . . , N } is bounded by E (cid:80) T ln N where the sum is taken over the rounds where expert i was awake, and the expectation E scopes the algorithm\u2019s random choice of expert it. Note that this regret has the same shape as (1). The algorithm runs in time O(N ) per round and uses O(N ) space. Sabotage can be reduced to specialists quite naturally as follows. We enumerate all source-sink paths P1, . . . , PD (note that D can be exponential in the size of the graph). We then use these paths as specialists, where a path P is awake at time t iff all its edges e \u2208 P are awake e \u2208 At. Disregarding gross inefficiency, the specialists algorithm would deliver the following regret bound. For any comparator path P , the expected regret (1) is bounded by RegretT (P ) \u2264 K T ln D where K is the length of the longest path (i.e. the loss range).  \u221a  Discussion This regret bound has two issues. First, we know that its regret bound is sub-optimal. In the simple case where no edges are sabotaged, the Component Hedge algorithm (instance of \u221a K for many graphs. This is due to the so-called mirror descent) improves the regret by a factor range factor problem discussed by Koolen et al. (2010) that arises from the fact that the algorithm does not take advantage of the fact that paths may overlap, and that their losses cannot be controlled independently. Second, and perhaps more importantly, the running time is horrible as one parameter is maintained per path. We have efficient algorithms for combinatorial settings without sleeping. These algorithms are \u201ccollapsed\u201d: they maintain one parameter per edge. The question is hence whether and how this methodology can be extended to incorporate sabotaged / sleeping edges.  We believe that Online Sabotaged Shortest Path a core problem which will become a starting point for various orthogonal extension such as switching between paths, semi-bandit feedback, etc.  References  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial optimization.  Math. Oper. Res., 39(1):31-45, 2014.  Nicol`o Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and  feels no regret). In Advances in Neural Information Processing Systems 25, pages 980-988, 2012.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proc. 29th Annual ACM Symposium on Theory of Computing, pages 334-343, 1997.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. Transactions on Computation  Theory, 6(3):11:1-11:16, 2014.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts  and bandits. Machine Learning, 80(2-3):245-272, 2010.  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In Proceedings  of the 23rd Annual Conference on Learning Theory, pages 93-105, 2010.  Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets and adver-  sarial losses. In Advances in Neural Information Processing Systems 27, pages 2780-2788, 2014.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine  Learning Research, 4:773-818, 2003.  1. Fancier bounds with mixture comparators and relative entropy are also available, but outside the scope of this note. "}, "Open Problem: Learning Quantum Circuits with Queries": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: Learning Quantum Circuits with Queries", "abstract": "We pose an open problem on the complexity of learning the behavior of a quantum circuit with value injection queries. We define the learning model for quantum circuits and give preliminary results. Using the test-path lemma of Angluin et al. (2009a), we show that new ideas are likely needed to tackle value injection queries for the quantum setting.", "pdf_url": "http://proceedings.mlr.press/v40/Kun15.pdf", "keywords": [], "reference": "Dana Angluin, James Aspnes, Jiang Chen, and Lev Reyzin. Learning large-alphabet and analog  circuits with value injection queries. Mach. Learn., 72(1-2):113-138, 2008.  Dana Angluin, James Aspnes, Jiang Chen, David Eisenstat, and Lev Reyzin. Learning acyclic probabilistic circuits using test paths. J. Mach. Learn. Res., 10(Aug):1881-1911, August 2009a.  Dana Angluin, James Aspnes, Jiang Chen, and Yinghua Wu. Learning a circuit by injecting values.  J. Comput. System Sci., 75(1):60-77, January 2009b.  Dana Angluin, James Aspnes, and Lev Reyzin. Optimally learning social networks with activations  and suppressions. Theor. Comput. Sci., 411(29-30):2729-2740, 2010.  David Kempe, Jon Kleinberg, and \u00b4Eva Tardos. Maximizing the spread of in\ufb02uence through a social  network. In Proceedings of the Ninth ACM SIGKDD, pages 137-146, 2003.  3  qqqBXORII012B=\uf8eb\uf8ec\uf8ec\uf8ed11000011001\u221211\u2212100\uf8f6\uf8f7\uf8f7\uf8f8 Open Problem: Learning Quantum Circuits with Queries  Measurement adds poly(k/\u03b5) FIQs to the above algorithm to make it \u03b5-approximate. Consider the case with two gates Gj, Gj(cid:48) with Aj, Aj(cid:48) operating on the same qubits. De- compose Aj according to a basis of the vector space SU (8). Inject the identity everywhere but j, j(cid:48) and inject all pairwise sums of basis operations to Aj(cid:48), query all 8 basis vectors, and measure the outputs. Using the basis coe\ufb03cients of Aj as variables, this gives a system of polynomial equalities in which all parameters are O(1). It can be solved to accuracy \u03b5/k. It is easy to see that the errors across gates grow linearly, and the general case is similar.  2.2. Quantum circuits fail the test path lemma with measured VIQs  Angluin et al. (2009a) use the idea of \u201ctest paths\u201d (from Angluin et al. (2009b)), to bound the query complexity of learning boolean probabilistic circuits. Their analysis also shows test paths fail for probabilistic circuits with an alphabet size greater than two. We show quantum circuits have the same barrier by constructing a gadget analogous to that of An- gluin et al. (2009a) (Lemma 8). Define B as a matrix as in Figure 1. Further define the stan- eij(k\u2295i\u2295j). dard quantum XOR gate using an extra scratchwork qubit by the mapping eijk  (cid:55)\u2192  Lemma 2 There is a circuit on which every (measured) VIQ leaving a path free makes the last output qubit uniformly random, yet with no VIQ the last output qubit is deterministic.  (cid:55)\u2192  1\u221a 2  (e000  (e000 + e110), e010  Proof Define C on three qubits as in Figure 1. The circuit normally maps e000 1\u221a 2  (cid:55)\u2192 e101), (cid:55)\u2192 i.e., the last qubit is 1 i\ufb00 the input\u2019s first qubit is 1. Likewise, when the scratch-work qubit is 1, the output bit is \ufb02ipped. When there is a VIQ, say, of 1 at the identity gate acting on the 2nd qubit, the mapping becomes e000 1\u221a 2  1\u221a 2 (cid:55)\u2192 e1100). The extra qubit introduced by the VIQ is  (e0111 + e1100), and e110  (e111 + e101), and e110  (e0110 + e1101), e010  e1101), e100  e110), e100  (e0111  (e0110  (e111  1\u221a 2  1\u221a 2  1\u221a 2  1\u221a 2  (cid:55)\u2192  (cid:55)\u2192  (cid:55)\u2192  the last index, and the qubit of interest is the third qubit, which is uniformly random.  (cid:55)\u2192  \u2212  \u2212  \u2212  \u2212  Left:  the circuit for Figure 1: Lemma 2. Right: the columns of B form the Bell basis.  (a)  (b)  References  Dana Angluin, James Aspnes, Jiang Chen, and Lev Reyzin. Learning large-alphabet and analog  circuits with value injection queries. Mach. Learn., 72(1-2):113-138, 2008.  Dana Angluin, James Aspnes, Jiang Chen, David Eisenstat, and Lev Reyzin. Learning acyclic probabilistic circuits using test paths. J. Mach. Learn. Res., 10(Aug):1881-1911, August 2009a.  Dana Angluin, James Aspnes, Jiang Chen, and Yinghua Wu. Learning a circuit by injecting values.  J. Comput. System Sci., 75(1):60-77, January 2009b.  Dana Angluin, James Aspnes, and Lev Reyzin. Optimally learning social networks with activations  and suppressions. Theor. Comput. Sci., 411(29-30):2729-2740, 2010.  David Kempe, Jon Kleinberg, and \u00b4Eva Tardos. Maximizing the spread of in\ufb02uence through a social  network. In Proceedings of the Ninth ACM SIGKDD, pages 137-146, 2003.qqqBXORII012B=\uf8eb\uf8ec\uf8ec\uf8ed11000011001\u221211\u2212100\uf8f6\uf8f7\uf8f7\uf8f8 "}, "Open Problem: Recursive Teaching Dimension Versus VC Dimension": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Open Problem: Recursive Teaching Dimension Versus VC Dimension", "abstract": "The Recursive Teaching Dimension (RTD) of a concept class \\mathcalC is a complexity parameter referring to the worst-case number of labelled examples needed to learn any target concept in \\mathcalC from a teacher following the recursive teaching model. It is the first teaching complexity notion for which interesting relationships to the VC dimension (VCD) have been established. In particular, for finite maximum classes of a given VCD d, the RTD equals d. To date, there is no concept class known for which the ratio of RTD over VCD exceeds 3/2. However, the only known upper bound on RTD in terms of VCD is exponential in the VCD and depends on the size of the concept class. We pose the following question: is the RTD upper-bounded by a function that grows only linearly in the VCD? Answering this question would further our understanding of the relationships between the complexity of teaching and the complexity of learning from randomly chosen examples. In addition, the answer to this question, whether positive or negative, is known to have implications on the study of the long-standing open sample compression conjecture, which claims that every concept class of VCD d has a sample compression scheme in which samples for concepts in the class are compressed to subsets of size no larger than d.", "pdf_url": "http://proceedings.mlr.press/v40/Simon15b.pdf", "keywords": [], "reference": "Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. JACM, 36(4):929-965, 1989.  Malte Darnst\u00a8adt, Thorsten Doliwa, Hans Ulrich Simon, and Sandra Zilles. Order compression  schemes. In ALT, pages 173-187, 2013.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimen-  sion, VC-dimension and sample compression. J. Mach. Learn. Res., 15:3107-3131, 2014.  Sally A. Goldman and Michael J. Kearns. On the complexity of teaching. Journal of Computer and  System Sciences, 50(1):20-31, 1995.  Christian Kuhlmann. On teaching and learning intersection-closed concept classes. In EuroCOLT,  pages 168-182, 1999.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff. Teaching and compressing for  low VC-dimension. CoRR, abs/1502.06187, 2015.  Benjamin I. P. Rubinstein and J. Hyam Rubinstein. A geometric approach to sample compression.  J. Mach. Learn. Res., 13:1221-1261, 2012.  Ayumi Shinohara and Satoru Miyano. Teachability in computational learning. New Gen. Comput.,  8:337-348, 1991.  Manfred K. Warmuth. Compressing to VC dimension many points. In COLT, pages 743-744, 2003.  Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching  and learning. J. Mach. Learn. Res., 12:349-384, 2011.  3   OPEN PROBLEM: RECURSIVE TEACHING DIMENSION VERSUS VC DIMENSION  What if RTD is linearly bounded by the VCD? This would, first of all, imply a close rela- tionship between learning from randomly chosen examples and learning from teachers, in terms of information complexity. The structural properties that make a concept class hard or easy to learn from randomly chosen examples would have implications on the difficulty of teaching the class by yielding an upper bound on the number of examples needed in the worst case. Second, any relation- ship between RTD and VCD may bring new insights into the long-standing sample compression conjecture which claims that every concept class has a sample compression scheme of size (linear in) its VCD (Warmuth, 2003). In the proof that RTD = VCD for maximum classes, the recursive teaching examples form compression sets equal to the ones of size at most VCD resulting from Rubinstein and Rubinstein\u2019s (2012) corner-peeling. Further relationships between RTD and sample compression were established in the study of order compression schemes (Darnst\u00a8adt et al., 2013).  What if RTD is not linearly bounded by the VCD? One possible approach to proving the sample compression conjecture would be to show that every finite concept class can be embedded into a maximum class with only linear increase in the VCD. As Doliwa et al. (2014) observed, this approach will be fruitless if RTD is not linearly bounded in VCD.  Hence, we believe that answering the question formulated above will entail substantial progress in the study of teaching models, information complexity in general, as well as sample compression.  References  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. JACM, 36(4):929-965, 1989.  Malte Darnst\u00a8adt, Thorsten Doliwa, Hans Ulrich Simon, and Sandra Zilles. Order compression  schemes. In ALT, pages 173-187, 2013.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimen-  sion, VC-dimension and sample compression. J. Mach. Learn. Res., 15:3107-3131, 2014.  Sally A. Goldman and Michael J. Kearns. On the complexity of teaching. Journal of Computer and  System Sciences, 50(1):20-31, 1995.  Christian Kuhlmann. On teaching and learning intersection-closed concept classes. In EuroCOLT,  pages 168-182, 1999.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff. Teaching and compressing for  low VC-dimension. CoRR, abs/1502.06187, 2015.  Benjamin I. P. Rubinstein and J. Hyam Rubinstein. A geometric approach to sample compression.  J. Mach. Learn. Res., 13:1221-1261, 2012.  Ayumi Shinohara and Satoru Miyano. Teachability in computational learning. New Gen. Comput.,  8:337-348, 1991.  Manfred K. Warmuth. Compressing to VC dimension many points. In COLT, pages 743-744, 2003.  Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching  and learning. J. Mach. Learn. Res., 12:349-384, 2011. "}, "On Consistent Surrogate Risk Minimization and Property Elicitation": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On Consistent Surrogate Risk Minimization and Property Elicitation", "abstract": "Surrogate risk minimization is a popular framework for supervised learning; property elicitation is a widely studied area in probability forecasting, machine learning, statistics and economics. In this paper, we connect these two themes by showing that calibrated surrogate losses in supervised learning can essentially be viewed as eliciting or estimating certain properties of the underlying conditional label distribution that are sufficient to construct an optimal classifier under the target loss of interest. Our study helps to shed light on the design of convex calibrated surrogates. We also give a new framework for designing convex calibrated surrogates under low-noise conditions by eliciting properties that allow one to construct \u2018coarse\u2019 estimates of the underlying distribution.", "pdf_url": "http://proceedings.mlr.press/v40/Agarwal15.pdf", "keywords": ["Surrogate risk minimization", "convex calibrated surrogates", "multiclass classification", "property elicitation", "proper scoring rules", "proper losses"], "reference": "Jacob D. Abernethy and Rafael M. Frongillo. A characterization of scoring rules for linear proper-  ties. In Proceedings of the 25th Annual Conference on Learning Theory, 2012.  Peter L. Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classification and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  David Buffoni, Cl\u00b4ement Calauz`enes, Patrick Gallinari, and Nicolas Usunier. Learning scoring func- tions with order-preserving losses and standardized supervision. In Proceedings of the 28th In- ternational Conference on Machine Learning, 2011.  Cl\u00b4ement Calauz`enes, Nicolas Usunier, and Patrick Gallinari. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems, 2012.  David Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset ranking. IEEE Trans-  actions on Information Theory, 54(11):5140-5154, 2008.  John Duchi, Lester Mackey, and Michael Jordan. On the consistency of ranking algorithms. In  Proceedings of the 27th International Conference on Machine Learning, 2010.  Rafael Frongillo and Ian Kash. Vector-valued property elicitation.  In Proceedings of the 28th  Annual Conference on Learning Theory, 2015.  Tilmann Gneiting. Quantiles as optimal point forecasts. International Journal of Forecasting, 27  (2):197-207, 2011.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  Kyrill Grant and Tilmann Gneiting. Consistent scoring functions for quantiles. In From Probability to Statistics and Back: High-Dimensional Models and Processes-A Festschrift in Honor of Jon A. Wellner, pages 163-173. Institute of Mathematical Statistics, 2013.  Nicholas M. Kiefer.  Incentive-compatible  elicitation of quantiles,  2010.  URL  https://www.american.edu/cas/economics/info-metrics/pdf/upload/ Working-Paper-Kiefer.pdf.  Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions.  In  ACM Conference on Electronic Commerce, 2009.  Nicolas S. Lambert, David M. Pennock, and Yoav Shoham. Eliciting properties of probability  distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, 2008.  Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. Statistical consistency of ranking meth- In Advances in Neural Information Processing  ods in a rank-differentiable probability space. Systems, 2012.  Harish G. Ramaswamy and Shivani Agarwal. Classification calibration dimension for general mul-  ticlass losses. In Advances in Neural Information Processing Systems, 2012.  13   ON CONSISTENT SURROGATE RISK MINIMIZATION AND PROPERTY ELICITATION  References  Jacob D. Abernethy and Rafael M. Frongillo. A characterization of scoring rules for linear proper-  ties. In Proceedings of the 25th Annual Conference on Learning Theory, 2012.  Peter L. Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classification and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  David Buffoni, Cl\u00b4ement Calauz`enes, Patrick Gallinari, and Nicolas Usunier. Learning scoring func- tions with order-preserving losses and standardized supervision. In Proceedings of the 28th In- ternational Conference on Machine Learning, 2011.  Cl\u00b4ement Calauz`enes, Nicolas Usunier, and Patrick Gallinari. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems, 2012.  David Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset ranking. IEEE Trans-  actions on Information Theory, 54(11):5140-5154, 2008.  John Duchi, Lester Mackey, and Michael Jordan. On the consistency of ranking algorithms. In  Proceedings of the 27th International Conference on Machine Learning, 2010.  Rafael Frongillo and Ian Kash. Vector-valued property elicitation.  In Proceedings of the 28th  Annual Conference on Learning Theory, 2015.  Tilmann Gneiting. Quantiles as optimal point forecasts. International Journal of Forecasting, 27  (2):197-207, 2011.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  Kyrill Grant and Tilmann Gneiting. Consistent scoring functions for quantiles. In From Probability to Statistics and Back: High-Dimensional Models and Processes-A Festschrift in Honor of Jon A. Wellner, pages 163-173. Institute of Mathematical Statistics, 2013.  Nicholas M. Kiefer.  Incentive-compatible  elicitation of quantiles,  2010.  URL  https://www.american.edu/cas/economics/info-metrics/pdf/upload/ Working-Paper-Kiefer.pdf.  Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions.  In  ACM Conference on Electronic Commerce, 2009.  Nicolas S. Lambert, David M. Pennock, and Yoav Shoham. Eliciting properties of probability  distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, 2008.  Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. Statistical consistency of ranking meth- In Advances in Neural Information Processing  ods in a rank-differentiable probability space. Systems, 2012.  Harish G. Ramaswamy and Shivani Agarwal. Classification calibration dimension for general mul-  ticlass losses. In Advances in Neural Information Processing Systems, 2012. AGARWAL AGARWAL  Harish G. Ramaswamy and Shivani Agarwal. Convex calibration dimension for multiclass loss  matrices. Journal of Machine Learning Research. To appear, 2015.  Harish G. Ramaswamy, Shivani Agarwal, and Ambuj Tewari. Convex calibrated surrogates for low- rank loss matrices with applications to subset ranking losses. In Advances in Neural Information Processing Systems, 2013.  Pradeep Ravikumar, Ambuj Tewari, and Eunho Yang. On NDCG consistency of listwise ranking In Proceedings of the 14th International Conference on Artificial Intelligence and  methods. Statistics, 2011.  Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the American  Statistical Association, 66(336):783-801, 1971.  Mark J. Schervish. A general method for comparing probability assessors. The Annals of Statistics,  17(4):1856-1879, 1989.  Mark J. Schervish, Joseph B. Kadane, and Teddy Seidenfeld. Characterization of proper and strictly  proper scoring rules for quantiles. Preprint, Carnegie Mellon University, March 2012.  Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approxima-  tion, 26:225-287, 2007.  Ingo Steinwart, Chlo\u00b4e Pasin, Robert Williamson, and Siyu Zhang. Elicitation and identification of  properties. In Proceedings of the 27th Annual Conference on Learning Theory, 2014.  Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classification methods. Journal  of Machine Learning Research, 8:1007-1025, 2007.  Elodie Vernet, Robert C. Williamson, and Mark D. Reid. Composite multiclass losses. In Advances  in Neural Information Processing Systems, 2011.  Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank: Theory and algorithm. In Proceedings of the 25th International Conference on Machine Learning, 2008.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. Annals of Statistics, 32(1):56-134, 2004a.  Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Jour-  nal of Machine Learning Research, 5:1225-1251, 2004b. ON CONSISTENT SURROGATE RISK MINIMIZATION AND PROPERTY ELICITATION  "}, "Online Learning with Feedback Graphs: Beyond Bandits": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Online Learning with Feedback Graphs: Beyond Bandits", "abstract": "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced T-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: \\emphstrongly observable graphs, \\emphweakly observable graphs, and \\emphunobservable graphs. We prove that the first class induces learning problems with \\widetilde\u0398(\u03b1^1/2 T^1/2) minimax regret, where \u03b1is the independence number of the underlying graph; the second class induces problems with \\widetilde\u0398(\u03b4^1/3T^2/3) minimax regret, where \u03b4is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.", "pdf_url": "http://proceedings.mlr.press/v40/Alon15.pdf", "keywords": [], "reference": "N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley & Sons, 2008.  N. Alon, N. Cesa-Bianchi, C. Gentile, and Y. Mansour. From bandits to experts: A tale of dom- In Advances in Neural Information Processing Systems 26, pages  ination and independence. 1610-1618. Curran Associates, Inc., 2013.  N. Alon, N. Cesa-Bianchi, C. Gentile, S. Mannor, Y. Mansour, and O. Shamir. Nonstochastic multi-  armed bandits with graph-structured feedback. CoRR, abs/1409.8428, 2014.  N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs: Beyond  bandits. arXiv preprint arXiv:1502.07617, 2015.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM Journal on Computing, 32(1):48-77, 2002.  G. Bart\u00b4ok, D. P. Foster, D. P\u00b4al, A. Rakhlin, and C. Szepesv\u00b4ari. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967-997, 2014.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. Helmbold, R. Schapire, and M. Warmuth. How to use  expert advice. Journal of the ACM, 44(3):427-485, 1997.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with  expert advice. Machine Learning, 66(2-3):321-352, 2007.  12   ALON CESA-BIANCHI DEKEL KOREN  Proof of Theorem 7 (sketch). First, we use the lemma to find an independent set U of weakly ob- servable vertices of size (cid:101)\u2126(\u03b4), with the crucial property that each vertex in the entire graph dom- inates at most (cid:101)O(1) vertices of U . Then, we embed in the set U a hard instance of the stochastic multiarmed bandit problem, in which the optimal action has expected loss smaller by (cid:15) than the expected loss of the other actions in U . To all other vertices of the graph, we assign the maximal loss of 1. Hence, unless the player is able to detect the optimal action, his regret cannot be better than \u2126((cid:15)T ).  The main observation is that, due to the properties of the set U , in order to obtain accurate estimates of the losses of all actions in U the player has to use (cid:101)\u2126(\u03b4) different actions outside of U and pick each for \u2126(1/(cid:15)2) times. Since each such action entails a constant instantaneous regret, the player has to pay an \u2126(\u03b4/(cid:15)2) penalty in his cumulative regret for exploration. The overall regret is thus of order \u2126(cid:0)min{(cid:15)T, \u03b4/(cid:15)2}(cid:1), which is maximized at (cid:15) = (\u03b4/T )1/3 and gives the stated lower bound.  We thank S\u00b4ebastien Bubeck for helpful discussions during various stages of this work, and G\u00b4abor Bart\u00b4ok for clarifying the connections to observability in partial monitoring. Part of this work was done while NCB and TK were visiting OD at Microsoft Research, whose support is gratefully acknowledged.  Acknowledgments  References  N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley & Sons, 2008.  N. Alon, N. Cesa-Bianchi, C. Gentile, and Y. Mansour. From bandits to experts: A tale of dom- In Advances in Neural Information Processing Systems 26, pages  ination and independence. 1610-1618. Curran Associates, Inc., 2013.  N. Alon, N. Cesa-Bianchi, C. Gentile, S. Mannor, Y. Mansour, and O. Shamir. Nonstochastic multi-  armed bandits with graph-structured feedback. CoRR, abs/1409.8428, 2014.  N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs: Beyond  bandits. arXiv preprint arXiv:1502.07617, 2015.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM Journal on Computing, 32(1):48-77, 2002.  G. Bart\u00b4ok, D. P. Foster, D. P\u00b4al, A. Rakhlin, and C. Szepesv\u00b4ari. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967-997, 2014.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. Helmbold, R. Schapire, and M. Warmuth. How to use  expert advice. Journal of the ACM, 44(3):427-485, 1997.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with  expert advice. Machine Learning, 66(2-3):321-352, 2007. ONLINE LEARNING WITH FEEDBACK GRAPHS: BEYOND BANDITS  Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an applica-  tion to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  D. P. Helmbold, N. Littlestone, and P. M. Long. Apple tasting. Information and Computation, 161  (2):85-139, 2000.  T. Koc\u00b4ak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Advances in Neural Information Processing Systems, pages 613-621, 2014.  N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computa-  tion, 108:212-261, 1994.  S. Mannor and O. Shamir. From bandits to experts: On the value of side-observations. In J. Shawe- Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Infor- mation Processing Systems 24, pages 684-692. Curran Associates, Inc., 2011.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  V. V. Vazirani. Approximation algorithms. Springer Science & Business Media, 2001.  V. Vovk. Aggregating strategies. In Proceedings of the 3rd Annual Workshop on Computational  Learning Theory, pages 371-386, 1990. "}, "Learning Overcomplete Latent Variable Models through Tensor Methods": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Learning Overcomplete Latent Variable Models through Tensor Methods", "abstract": "We provide guarantees for learning latent variable models  emphasizing on the overcomplete regime, where the dimensionality of the latent space exceeds  the observed dimensionality.  In particular, we consider multiview mixtures, ICA, and sparse coding models. Our main tool is a new algorithm for tensor decomposition that works in the overcomplete regime. In the semi-supervised setting, we exploit label information to get a rough estimate of the model parameters, and then refine it using the tensor method on unlabeled samples. We establish learning guarantees   when the number of components scales as k=o(d^p/2), where d is the observed dimension, and p is the order of the observed moment employed in the tensor method (usually p=3,4).  In the unsupervised setting, a simple initialization algorithm based on SVD of the tensor slices is proposed, and the guarantees are provided under the stricter condition that k \u2264\u03b2d (where constant \u03b2can be larger than 1). For the learning applications, we provide tight sample complexity bounds through novel covering arguments.", "pdf_url": "http://proceedings.mlr.press/v40/Anandkumar15.pdf", "keywords": ["unsupervised and semi-supervised learning", "latent variable models", "overcomplete representations", "tensor decomposition"], "reference": "Rados\u0142aw Adamczak, Rafa\u0142 Lata\u0142a, Alexander E Litvak, Alain Pajor, and Nicole Tomczak- Jaegermann. Chevet type inequality and norms of submatrices. arXiv preprint arXiv:1107.4066, 2011.  A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning Sparsely Used Over- complete Dictionaries via Alternating Minimization. Available on arXiv:1310.7991, Oct. 2013.  A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y. K. Liu. Two SVDs Suffice: Spectral to appear Decompositions for Probabilistic Topic Modeling and Latent Dirichlet Allocation. in the special issue of Algorithmica on New Theoretical Challenges in Machine Learning, July 2013a.  A. Anandkumar, R. Ge, D. Hsu, and S. M. Kakade. A Tensor Spectral Approach to Learning Mixed Membership Community Models. In Conference on Learning Theory (COLT), June 2013b.  A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade. When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. In Neural Information Processing (NIPS), Dec. 2013c.  A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor Methods for Learning  Latent Variable Models. J. of Machine Learning Research, 15:2773-2832, 2014a.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed Non-Orthogonal Tensor Decom-  position via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014b.  J. Anderson, M. Belkin, N. Goyal, L. Rademacher, and J. Voss. The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures. arXiv preprint arXiv:1311.2891, Nov. 2013.  S. Arora, R. Ge, and A. Moitra. New Algorithms for Learning Incoherent and Overcomplete Dic-  tionaries. ArXiv e-prints, August 2013.  Boaz Barak, Jonathan Kelner, and David Steurer. Dictionary learning and tensor decomposition via  the sum-of-squares method. arXiv preprint arXiv:1407.1543, 2014.  Y. Bengio, A. Courville, and P. Vincent. Unsupervised feature learning and deep learning: A review  and new perspectives. arXiv preprint arXiv:1206.5538, 2012.  A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of tensor decom-  positions. arXiv preprint arXiv:1311.3651, 2013.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? Information Theory, IEEE Transactions on, 52(12):5406-5425, 2006.  J. F. Cardoso and Pierre Comon.  Independent component analysis, a survey of some algebraic  methods. In IEEE International Symposium on Circuits and Systems, pages 93-96, 1996.  13   LEARNING OVERCOMPLETE LVMS THROUGH TENSOR METHODS  References  Rados\u0142aw Adamczak, Rafa\u0142 Lata\u0142a, Alexander E Litvak, Alain Pajor, and Nicole Tomczak- Jaegermann. Chevet type inequality and norms of submatrices. arXiv preprint arXiv:1107.4066, 2011.  A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning Sparsely Used Over- complete Dictionaries via Alternating Minimization. Available on arXiv:1310.7991, Oct. 2013.  A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y. K. Liu. Two SVDs Suffice: Spectral to appear Decompositions for Probabilistic Topic Modeling and Latent Dirichlet Allocation. in the special issue of Algorithmica on New Theoretical Challenges in Machine Learning, July 2013a.  A. Anandkumar, R. Ge, D. Hsu, and S. M. Kakade. A Tensor Spectral Approach to Learning Mixed Membership Community Models. In Conference on Learning Theory (COLT), June 2013b.  A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade. When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. In Neural Information Processing (NIPS), Dec. 2013c.  A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor Methods for Learning  Latent Variable Models. J. of Machine Learning Research, 15:2773-2832, 2014a.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed Non-Orthogonal Tensor Decom-  position via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014b.  J. Anderson, M. Belkin, N. Goyal, L. Rademacher, and J. Voss. The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures. arXiv preprint arXiv:1311.2891, Nov. 2013.  S. Arora, R. Ge, and A. Moitra. New Algorithms for Learning Incoherent and Overcomplete Dic-  tionaries. ArXiv e-prints, August 2013.  Boaz Barak, Jonathan Kelner, and David Steurer. Dictionary learning and tensor decomposition via  the sum-of-squares method. arXiv preprint arXiv:1407.1543, 2014.  Y. Bengio, A. Courville, and P. Vincent. Unsupervised feature learning and deep learning: A review  and new perspectives. arXiv preprint arXiv:1206.5538, 2012.  A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of tensor decom-  positions. arXiv preprint arXiv:1311.3651, 2013.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? Information Theory, IEEE Transactions on, 52(12):5406-5425, 2006.  J. F. Cardoso and Pierre Comon.  Independent component analysis, a survey of some algebraic  methods. In IEEE International Symposium on Circuits and Systems, pages 93-96, 1996. ANANDKUMAR GE JANZAMIN  A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer networks in unsupervised feature  learning. Journal of Machine Learning Research - Proceedings Track, 15:215-223, 2011.  P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287-314,  P. Comon. Tensor decompositions. Mathematics in Signal Processing V, pages 1-24, 2002.  P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component Analysis  and Applications. Academic Press. Elsevier, 2010.  Sanjoy Dasgupta, Daniel Hsu, and Nakul Verma. A concentration theorem for projections.  In  Twenty-Second Conference on Uncertainty in Artificial Intelligence, 2006.  L. De Lathauwer, J. Castaing, and J.-F. Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. Signal Processing, IEEE Transactions on, 55(6):2965-2973, 2007.  D. Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289-1306,  1994.  2006.  N. Goyal, S. Vempala, and Y. Xiao. Fourier pca. arXiv preprint arXiv:1306.5825, 2013.  Olivier Gu\u00b4edon and Mark Rudelson. Lp-moments of random vectors via majorizing measures.  Advances in Mathematics, 208(2):798-823, 2007.  D. Hsu and S. M. Kakade. Learning Mixtures of Spherical Gaussians: Moment Methods and  Spectral Decompositions. arXiv preprint arXiv:1206.5766, 2012.  F. Huang, U. N. Niranjan, M. Hakeem, and A. Anandkumar. Fast Detection of Overlapping Com-  munities via Online Tensor Methods. ArXiv 1309.0787, Sept. 2013.  A. Hyvarinen and E. Oja. Independent component analysis: algorithms and applications. Neural  Networks, 13(4-5):411-430, 2000.  R. Latala. Estimates of moments and tails of Gaussian chaoses. Ann. Prob., 34(6):2315-2331, 2006.  Q. V. Le, A. Karpenko, J. Ngiam, and A. Y. Ng. ICA with Reconstruction Cost for Efficient Over-  complete Feature Learning. In NIPS, pages 1017-1025, 2011.  M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. Neural computation,  12(2):337-365, 2000.  N. H. Nguyen, P. Drineas, and T. D. Tran. Tensor sparsification via a bound on the spectral norm  of random tensors. arXiv preprint arXiv:1005.4732, May 2010.  M. Rudelson and R. Vershynin. The smallest singular value of a random rectangular matrix. Com-  munications on Pure and Applied Mathematics, 62(12):1707-1739, 2009.  L. Song, A. Anandkumar, B. Dai, and B. Xie. Nonparametric estimation of multi-view latent vari-  able models. Available on arXiv:1311.3287, Nov. 2013. LEARNING OVERCOMPLETE LVMS THROUGH TENSOR METHODS  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  M. A. O. Vasilescu and D. Terzopoulos. Multilinear subspace analysis of image ensembles. In Com- puter Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Confer- ence on, volume 2, pages II-93. IEEE, 2003.  Eugene P Wigner. Characteristic vectors of bordered matrices with infinite dimensions. The Annals  of Mathematics, 62(3):548-564, 1955.  T. Zhang and G. Golub. Rank-one approximation to high order tensors. SIAM Journal on Matrix  Analysis and Applications, 23:534-550, 2001.  J. Y. Zou, D. Hsu, D. C. Parkes, and R. P. Adams. Contrastive learning using spectral methods. In  Advances in Neural Information Processing Systems, pages 2238-2246, 2013.  "}, "Simple, Efficient, and Neural Algorithms for Sparse Coding": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "abstract": "Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization.  Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used", "pdf_url": "http://proceedings.mlr.press/v40/Arora15.pdf", "keywords": [], "reference": "A. Agarwal, A. Anandkumar, and P. Netrapalli. Exact recovery of sparsely used overcom-  plete dictionaries. In arXiv:1309.1952, 2013.  A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning sparsely used overcomplete dictionaries via alternating minimization. In COLT, pages 123-137, 2014.  M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. In IEEE Trans. on Signal Processing, pages 4311- 4322, 2006.  S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete  dictionaries. In COLT, pages 779-806, 2014.  12   Arora Ge Ma Moitra  Algorithm 3 Pairwise Initialization  Set L = \u2205 While |L| < m choose samples u and v  (cid:80)p2  i=1(cid:104)y(i), u(cid:105)(cid:104)y(i), v(cid:105)y(i)(y(i))T  Set (cid:99)Mu,v = 1 p2 Compute the top two singular values \u03c31, \u03c32 and top singular vector z of (cid:99)Mu,v If \u03c31 \u2265 \u2126(k/m) and \u03c32 < O\u2217(k/m log m)  If z is not within distance 1/ log m of any vector in L (even after sign \ufb02ip), add  z to L Set (cid:101)A such that its columns are z \u2208 L and output A = ProjB (cid:101)A where B is the convex set defined in Definition 28  Moreover the error terms E1 + E2 + E3 has spectral norm bounded by O\u2217(k/m log n), |\u03b2i| \u2265 i| \u2265 \u2126(1) for all i \u2208 supp(\u03b1(cid:48)). \u2126(1) for all i \u2208 supp(\u03b1) and |\u03b2(cid:48)  We will invoke this lemma several times in order to analyze Algorithm 3 to verify whether or not the supports of u and v share a common element, and again to show that if they do we can approximately recover the corresponding column of A\u2217 from the top singular vector of Mu,v.  Conclusions  \u221a  Going beyond n sparsity requires new ideas as alternating minimization appears to break down. Mysterious properties of alternating minimization are also left to explore, such as why a random initialization works. Are these heuristics information theoretically optimal in terms of their sample complexity? Finally, can we analyse energy minimization in other contexts as well?  We are grateful to Dmitri Chklovskii and Sebastian Seung for useful discussions about neural computation.  Acknowledgements  References  A. Agarwal, A. Anandkumar, and P. Netrapalli. Exact recovery of sparsely used overcom-  plete dictionaries. In arXiv:1309.1952, 2013.  A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning sparsely used overcomplete dictionaries via alternating minimization. In COLT, pages 123-137, 2014.  M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. In IEEE Trans. on Signal Processing, pages 4311- 4322, 2006.  S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete  dictionaries. In COLT, pages 779-806, 2014. Simple, Efficient, and Neural Algorithms for Sparse Coding  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. CoRR, abs/1408.2156, 2014. URL http://arxiv.org/abs/1408.2156.  Boaz Barak, John Kelner, and David Steurer. Dictionary learning using sum-of-square  hierarchy. 2014.  E. Candes and T. Tao. Decoding by linear programming. In IEEE Trans. on Information  Theory, pages 4203-4215, 2005.  E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. In Communications of Pure and Applied Math, pages 1207-1223, 2006.  D. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. In IEEE  Trans. on Information Theory, pages 2845-2862, 1999.  M. Elad. Sparse and redundant representations. In Springer, 2010.  K. Engan, S. Aase, and J. Hakon-Husoy. Method of optimal directions for frame design. In  ICASSP, pages 2443-2446, 1999.  Quan Geng and John Wright. On the local correctness of l1-minimization for dictionary learning. In 2014 IEEE International Symposium on Information Theory, Honolulu, HI, USA, June 29 - July 4, 2014, pages 3180-3184, 2014. doi: 10.1109/ISIT.2014.6875421. URL http://dx.doi.org/10.1109/ISIT.2014.6875421.  R. Gribonval and M. Nielsen. Sparse representations in unions of bases. In IEEE Transac-  tions on Information Theory, pages 3320-3325, 2003.  M. Hardt. On the provable convergence of alternating minimization for matrix completion.  In arxiv:1312.0925, 2013.  R. Horn and C. Johnson. Matrix analysis. In Cambridge University Press, 1990.  P. Jain, P. Netrapalli, and S. Sanghavi. Low rank matrix completion using alternating  minimization. In STOC, pages 665-674, 2013.  Rodolphe Jenatton, R\u00b4emi Gribonval, and Francis R. Bach. Local stability and robustness of sparse dictionary learning in the presence of noise. CoRR, abs/1210.0685, 2012. URL http://arxiv.org/abs/1210.0685.  M. Lewicki and T. Sejnowski. Learning overcomplete representations. In Neural Computa-  tion, pages 337-365, 2000.  S. Mallat. A wavelet tour of signal processing. In Academic-Press, 1998.  Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.  Phase retrieval using Information Processing Sys- alternating minimization. tems 26: Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, URL http://papers.nips.cc/paper/ United States., pages 2796-2804, 2013. 5041-phase-retrieval-using-alternating-minimization.  27th Annual Conference on Neural  In Advances in Neural Arora Ge Ma Moitra  Praneeth Netrapalli, Niranjan U. N, Sujay Sanghavi, Animashree Anandkumar, and Prateek Jain. Non-convex robust PCA. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 1107-1115, 2014. URL http://papers.nips. cc/paper/5430-non-convex-robust-pca.  Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: a  strategy employed by v1. Vision Research, 37:3311-3325, 1997a.  Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A  strategy employed by v1? Vision research, 37(23):3311-3325, 1997b.  Marc\u2019Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun.  Sparse feature learn- In Advances Information Process- ing for deep belief networks. the Twenty-First Annual Conference on Neu- ing Systems 20, Proceedings of ral Information Processing Systems, Vancouver, British Columbia, Canada, De- cember 3-6, 2007, pages 1185-1192, 2007. URL http://papers.nips.cc/paper/ 3363-sparse-feature-learning-for-deep-belief-networks.  in Neural  Karin Schnass. Local Identification of Overcomplete Dictionaries. ArXiv e-prints, January  2014a.  Karin Schnass. On the identifiability of overcomplete dictionaries via the minimisation principle underlying k-svd. Applied and Computational Harmonic Analysis, 37(3):464 - 491, 2014b. ISSN 1063-5203. doi: http://dx.doi.org/10.1016/j.acha.2014.01.005. URL http://www.sciencedirect.com/science/article/pii/S1063520314000207.  D. Spielman, H. Wang, and J. Wright. Exact recovery of sparsely-used dictionaries.  In  Journal of Machine Learning Research, 2012. Simple, Efficient, and Neural Algorithms for Sparse Coding  "}, "Label optimal regret bounds for online local learning": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Label optimal regret bounds for online local learning", "abstract": "We resolve an open question from Christiano (2014b) posed in COLT\u201914 regarding the optimal dependency of the regret achievable for online local learning on the size of the label set. In this framework, the algorithm is shown a pair of items at each step, chosen from a set of n items. The learner then predicts a label for each item, from a label set of size L and receives a real valued payoff. This is a natural framework which captures many interesting scenarios such as online gambling and online max cut. Christiano (2014a) designed an efficient online learning algorithm for this problem achieving a regret of O(\\sqrtnL^3 T), where T is the number of rounds. Information theoretically, one can achieve a regret of O(\\sqrtn \\log L T). One of the main open questions left in this framework concerns closing the above gap. In this work, we provide a complete answer to the question above via two main results. We show, via a tighter analysis, that the semi-definite programming based algorithm of Christiano (2014a) in fact achieves a regret of O(\\sqrtnLT). Second, we show a matching computational lower bound. Namely, we show that a polynomial time algorithm for online local learning with lower regret would imply a polynomial time algorithm for the planted clique problem which is widely believed to be hard. We prove a similar hardness result under a related conjecture concerning planted dense subgraphs that we put forth. Unlike planted clique, the planted dense subgraph problem does not have any known quasi-polynomial time algorithms. Computational lower bounds for online learning are relatively rare, and we hope that the ideas developed in this work will lead to lower bounds for other online learning scenarios as well.", "pdf_url": "http://proceedings.mlr.press/v40/Awasthi15a.pdf", "keywords": [], "reference": "Benny Applebaum, Boaz Barak, and Avi Wigderson. Public-key cryptography from different as- sumptions. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 171-180. ACM, 2010.  Sanjeev Arora, Boaz Barak, Markus Brunnermeier, and Rong Ge. Computational complexity and  information asymmetry in financial products. In ICS, pages 49-65, 2010.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an o (n 1/4) approximation for densest k-subgraph. In Proceedings of the forty-second ACM Symposium on Theory of Computing, pages 201-210. ACM, 2010.  Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan, Venkatesan Guruswami, and Yuan Zhou. Polynomial integrality gaps for strong sdp relaxations of densest k-subgraph. In Proceed- ings of the twenty-third annual ACM-SIAM Symposium on Discrete Algorithms, pages 388-405. SIAM, 2012.  Paul Christiano. Online local learning via semidefinite programming. In Proceedings of the 53rd  Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2014a.  Paul Christiano. Open problem: Online local learning. In Proceedings of The 27th Conference on  Learning Theory, pages 1290-1294, 2014b.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper learning complexity. In Proceedings of the 46th Annual ACM Symposium on Theory of Comput- ing, pages 441-448. ACM, 2014.  Alfredo DeSantis, George Markowsky, and Mark N Wegman. Learning probabilistic prediction functions. In Foundations of Computer Science, 1988., 29th Annual Symposium on, pages 110- 119. IEEE, 1988.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for detecting planted cliques. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 655-664. ACM, 2013.  Elad Hazan. The convex optimization approach to regret minimization. Technical report, 2009.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Proceedings of The 25th Conference on Learning Theory, 2012.  Sham M Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation  algorithms. SIAM Journal on Computing, 39(3):1088-1106, 2009.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. ACM Transactions on  Computation Theory (TOCT), 6(3):11, 2014.  13   LABEL OPTIMAL REGRET BOUNDS FOR ONLINE LOCAL  References  Benny Applebaum, Boaz Barak, and Avi Wigderson. Public-key cryptography from different as- sumptions. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 171-180. ACM, 2010.  Sanjeev Arora, Boaz Barak, Markus Brunnermeier, and Rong Ge. Computational complexity and  information asymmetry in financial products. In ICS, pages 49-65, 2010.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an o (n 1/4) approximation for densest k-subgraph. In Proceedings of the forty-second ACM Symposium on Theory of Computing, pages 201-210. ACM, 2010.  Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan, Venkatesan Guruswami, and Yuan Zhou. Polynomial integrality gaps for strong sdp relaxations of densest k-subgraph. In Proceed- ings of the twenty-third annual ACM-SIAM Symposium on Discrete Algorithms, pages 388-405. SIAM, 2012.  Paul Christiano. Online local learning via semidefinite programming. In Proceedings of the 53rd  Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2014a.  Paul Christiano. Open problem: Online local learning. In Proceedings of The 27th Conference on  Learning Theory, pages 1290-1294, 2014b.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper learning complexity. In Proceedings of the 46th Annual ACM Symposium on Theory of Comput- ing, pages 441-448. ACM, 2014.  Alfredo DeSantis, George Markowsky, and Mark N Wegman. Learning probabilistic prediction functions. In Foundations of Computer Science, 1988., 29th Annual Symposium on, pages 110- 119. IEEE, 1988.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for detecting planted cliques. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 655-664. ACM, 2013.  Elad Hazan. The convex optimization approach to regret minimization. Technical report, 2009.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Proceedings of The 25th Conference on Learning Theory, 2012.  Sham M Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation  algorithms. SIAM Journal on Computing, 39(3):1088-1106, 2009.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. ACM Transactions on  Computation Theory (TOCT), 6(3):11, 2014. AWASTHI CHARIKAR LAI RISTESKI  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm.  Information and  computation, 108(2):212-261, 1994.  Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics and  econometrics. 1995.  Raghu Meka, Aaron Potechin, and Avi Wigderson. Sum-of-squares lower bounds for the planted clique problem. In Proceedings of the forty-seventh ACM Symposium on Theory of Computing, 2015.  Prasad Raghavendra. Optimal algorithms and inapproximability results for every csp?  In Pro- ceedings of the fortieth annual ACM symposium on Theory of computing, pages 245-254. ACM, 2008.  V Vavock. Aggregating strategies. In Conference on Computational Learning Theory, 1990.  Martin J Wainwright and Michael I Jordan. Log-determinant relaxation for approximate inference in discrete markov random fields. Signal Processing, IEEE Transactions on, 54(6):2099-2109, 2006.  Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding belief propagation and its  generalizations. Exploring artificial intelligence in the new millennium, 8:236-239, 2003. LABEL OPTIMAL REGRET BOUNDS FOR ONLINE LOCAL  "}, "Efficient Learning of Linear Separators under Bounded Noise": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Efficient Learning of Linear Separators under Bounded Noise", "abstract": "We study the learnability of linear separators in \\Re^d in the presence of bounded  (a.k.a Massart) noise. This is a  realistic generalization of the random classification noise model, where the adversary can flip each example x with probability \u03b7(x) \u2264\u03b7. We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit sphere in \\Re^d, for some constant value of \u03b7. While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design  algorithms achieving arbitrarily small excess error in  polynomial time  under this realistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work, instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only logarithmic in the desired excess error \u03b5.", "pdf_url": "http://proceedings.mlr.press/v40/Awasthi15b.pdf", "keywords": []}, "Efficient Representations for Lifelong Learning and Autoencoding": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Efficient Representations for Lifelong Learning and Autoencoding", "abstract": "It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, \u201clearning to learn\u201d as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.  Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning  nonlinear Boolean combinations of features.  Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural \u201canchor-set\u201d assumption.", "pdf_url": "http://proceedings.mlr.press/v40/Balcan15.pdf", "keywords": ["Life-long learning", "multi-task learning", "shared representations", "auto-encoding"], "reference": "Journal, 2008.  A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning  Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models - going beyond SVD. In 53rd  Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 1-10, 2012.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom- plete dictionaries. In Proceedings of The 27th Conference on Learning Theory (COLT), pages 779-806, 2014.  Pranjal Awasthi, Maria-Florina Balcan, and Philip M. Long. The power of localization for efficiently learning linear separators with noise. In Symposium on Theory of Computing (STOC), pages 449- 458, 2014.  M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Annual Conference on Learning Theory, 2013.  T. Bansal, C. Bhattacharyya, and R. Kannan. A provable SVD-based algorithm for learning topics  in dominant admixture corpus. ArXiv e-prints, October 2014.  J. Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 2000.  Jonathan Baxter. A bayesian/information theoretic model of learning to learn via multiple task  sampling. Machine Learning, 28(1):7-39, 1997.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In COLT,  2003.  Y. Bengio. Deep learning of representations: Looking forward, 2013. arXiv report 1305.0445.  Y. Bengio and O. Delalleau. On the expressive power of deep architectures. In ALT, 2011.  G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear algorithms for online multitask classification.  Journal of Machine Learning Research, 2010.  Shimon Edelman. Representation, similarity, and the chorus of prototypes. Minds and Machines, 5  (1):45-68, 1995. URL http://dx.doi.org/10.1007/BF00974189.  Scott E. Fahlman and Christian Lebiere. The cascade-correlation learning architecture.  In Ad- vances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 524-532, 1989. URL http://papers.nips.cc/paper/ 207-the-cascade-correlation-learning-architecture.  Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory of  NP-Completeness. W. H. Freeman & Co., New York, NY, USA, 1979. ISBN 0716710447.  A. Gopnik, A. Meltzoff, and P. Kuhl. How babies think. Orion, 2001.  S. Hanneke. Personal communication. 2013.  13   LIFELONG LEARNING AND AUTOENCODING  References  Journal, 2008.  A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning  Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models - going beyond SVD. In 53rd  Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 1-10, 2012.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom- plete dictionaries. In Proceedings of The 27th Conference on Learning Theory (COLT), pages 779-806, 2014.  Pranjal Awasthi, Maria-Florina Balcan, and Philip M. Long. The power of localization for efficiently learning linear separators with noise. In Symposium on Theory of Computing (STOC), pages 449- 458, 2014.  M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Annual Conference on Learning Theory, 2013.  T. Bansal, C. Bhattacharyya, and R. Kannan. A provable SVD-based algorithm for learning topics  in dominant admixture corpus. ArXiv e-prints, October 2014.  J. Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 2000.  Jonathan Baxter. A bayesian/information theoretic model of learning to learn via multiple task  sampling. Machine Learning, 28(1):7-39, 1997.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In COLT,  2003.  Y. Bengio. Deep learning of representations: Looking forward, 2013. arXiv report 1305.0445.  Y. Bengio and O. Delalleau. On the expressive power of deep architectures. In ALT, 2011.  G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear algorithms for online multitask classification.  Journal of Machine Learning Research, 2010.  Shimon Edelman. Representation, similarity, and the chorus of prototypes. Minds and Machines, 5  (1):45-68, 1995. URL http://dx.doi.org/10.1007/BF00974189.  Scott E. Fahlman and Christian Lebiere. The cascade-correlation learning architecture.  In Ad- vances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 524-532, 1989. URL http://papers.nips.cc/paper/ 207-the-cascade-correlation-learning-architecture.  Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory of  NP-Completeness. W. H. Freeman & Co., New York, NY, USA, 1979. ISBN 0716710447.  A. Gopnik, A. Meltzoff, and P. Kuhl. How babies think. Orion, 2001.  S. Hanneke. Personal communication. 2013. BALCAN BLUM VEMPALA  A. R. Klivans, P. M. Long, and A. Tang. Baum\u2019s algorithm learns intersections of halfspaces with  respect to log-concave distributions. In RANDOM, 2009.  A. Kumar and H. Daume. Learning task grouping and overlap in multi-task learning.  In NIPS,  2012.  Nick Littlestone, Philip M. Long, and Manfred K. Warmuth. On-line learning of linear functions.  Computational Complexity, 5(1):1-23, 1995.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algo-  rithms. Random Structures & Algorithms, 30(3):307-358, 2007.  A. Maurer and M. Pontil. Excess risk bounds for multitask learning with trace norm regularization.  In Proceedings of the 26th Annual Conference on Learning Theory, 2013.  Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research,  6:967-994, 2005. URL http://www.jmlr.org/papers/v6/maurer05a.html.  Ronald L. Rivest and Robert H. Sloan. Learning complicated concepts reliably and usefully. In  COLT, pages 69-79, 1988.  Volker Roth and Julia E Vogt. A complete analysis of the l 1, p group-lasso. In Proceedings of the  29th International Conference on Machine Learning (ICML-12), pages 185-192, 2012.  R. E. Schapire and L. M. Sellie. Learning sparse multivariate polynomials over a field with queries and counterexamples. In Proceedings of the 6th Annual Conference on Computational Learning Theory, 1993.  D. Spielman. Lecture notes for 18.409: The behavior of algorithms in practice. Lecture 2: On the  condition number. 2002.  S. Thrun. Explanation-Based Neural Network Learning: A Lifelong Learning Approach. Kluwer  Academic Publishers, Boston, MA, 1996.  S. Thrun and L.Y. Pratt, editors. Learning To Learn. Kluwer Academic Publishers, Boston, MA,  Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems,  Sebastian Thrun and Tom M. Mitchell. Learning one more thing. In Proc. 14th International Joint  Conference on Artificial Intelligence (IJCAI), pages 1217-1225, 1995b.  L. G. Valiant. A neuroidal architecture for cognitive computation. Journal of the ACM, 2000.  S. Vempala. A random-sampling-based algorithm for learning intersections of halfspaces. JACM,  L. Yang. Mathematical Theories of Interaction with Oracles. PhD thesis, CMU Dept. Machine  1997.  15(1-2):25-46, 1995a.  57(6), 2010.  Learning, 2013. LIFELONG LEARNING AND AUTOENCODING  Acknowledgments  This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF-1422910, CCF- 1217793, EAGER-1415498, IIS-1065251, AFOSR grant FA9550-09-1-0538, ONR grant N00014- 09-1-0751, and a Microsoft Research Faculty Fellowship.  "}, "Optimally Combining Classifiers Using Unlabeled Data": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Optimally Combining Classifiers Using Unlabeled Data", "abstract": "We develop a worst-case analysis of aggregation of classifier ensembles for binary classification. The task of predicting to minimize error is formulated as a game played over a given set of unlabeled data (a transductive setting), where prior label information is encoded as constraints on the game. The minimax solution of this game identifies cases where a weighted combination of the classifiers can perform significantly better than any single classifier.", "pdf_url": "http://proceedings.mlr.press/v40/Balsubramani15.pdf", "keywords": ["Ensemble aggregation", "transductive", "minimax"], "reference": "Massih Amini, Nicolas Usunier, and Franc\u00b8ois Laviolette. A transductive bound for the voted classi- fier with an application to semi-supervised learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 65-72, 2009.  Alexandr Andoni and Rina Panigrahy. A differential equations approach to optimizing regret trade-  offs. arXiv preprint arXiv:1305.1359, 2013.  Avrim Blum and John Langford. Pac-mdl bounds. In Learning Theory and Kernel Machines, pages  344-357. Springer, 2003.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Nicolo Cesa-Bianchi and G`abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, New York, NY, USA, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David P Helmbold, David Haussler, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 382-391. ACM, 1993.  B. Efron and C. Morris. Stein\u2019s Paradox in Statistics. Scientific American, 236:119-127, 1977.  Meir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences.  Information Theory, IEEE Transactions on, 38(4):1258-1270, 1992.  Ludmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classifier ensembles and  their relationship with the ensemble accuracy. Machine learning, 51(2):181-207, 2003.  Gert Lanckriet, Laurent E Ghaoui, Chiranjib Bhattacharyya, and Michael I Jordan. Minimax prob- ability machine. In Advances in neural information processing systems, pages 801-807, 2001.  Anqi Liu and Brian Ziebart. Robust classification under sample selection bias.  In Advances in  Neural Information Processing Systems, pages 37-45, 2014.  Robert E Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of statistics, pages 1651-1686, 1998.  Robert J. Vanderbei. Linear programming: Foundations and extensions, 1996.  Vladimir Vovk, Akimichi Takemura, and Glenn Shafer. Defensive forecasting. AISTATS 2005,  pages 365-372, 2005.  Volodimir G Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371-383. Morgan Kaufmann, 1990.  11   MINIMAX CLASSIFIER AGGREGATION  References  Massih Amini, Nicolas Usunier, and Franc\u00b8ois Laviolette. A transductive bound for the voted classi- fier with an application to semi-supervised learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 65-72, 2009.  Alexandr Andoni and Rina Panigrahy. A differential equations approach to optimizing regret trade-  offs. arXiv preprint arXiv:1305.1359, 2013.  Avrim Blum and John Langford. Pac-mdl bounds. In Learning Theory and Kernel Machines, pages  344-357. Springer, 2003.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Nicolo Cesa-Bianchi and G`abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, New York, NY, USA, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David P Helmbold, David Haussler, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 382-391. ACM, 1993.  B. Efron and C. Morris. Stein\u2019s Paradox in Statistics. Scientific American, 236:119-127, 1977.  Meir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences.  Information Theory, IEEE Transactions on, 38(4):1258-1270, 1992.  Ludmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classifier ensembles and  their relationship with the ensemble accuracy. Machine learning, 51(2):181-207, 2003.  Gert Lanckriet, Laurent E Ghaoui, Chiranjib Bhattacharyya, and Michael I Jordan. Minimax prob- ability machine. In Advances in neural information processing systems, pages 801-807, 2001.  Anqi Liu and Brian Ziebart. Robust classification under sample selection bias.  In Advances in  Neural Information Processing Systems, pages 37-45, 2014.  Robert E Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of statistics, pages 1651-1686, 1998.  Robert J. Vanderbei. Linear programming: Foundations and extensions, 1996.  Vladimir Vovk, Akimichi Takemura, and Glenn Shafer. Defensive forecasting. AISTATS 2005,  pages 365-372, 2005.  Volodimir G Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371-383. Morgan Kaufmann, 1990. BALSUBRAMANI FREUND  "}, "Minimax Fixed-Design Linear Regression": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Minimax Fixed-Design Linear Regression", "abstract": "We consider a linear regression game in which the covariates are known in advance: at each round, the learner predicts a real-value, the adversary reveals a label, and the learner incurs a squared error loss. The aim is to minimize the regret with respect to linear predictions.  For a variety of constraints on the adversary\u2019s labels, we show that the minimax optimal strategy is linear, with a parameter choice that is reminiscent of ordinary least squares (and as easy to compute).  The predictions depend on all covariates, past and future, with a particular weighting assigned to future covariates corresponding to the role that they play in the minimax regret.  We study two families of label sequences: box constraints (under a covariate compatibility condition), and a weighted 2-norm constraint that emerges naturally from the analysis.  The strategy is adaptive in the sense that it requires no knowledge of the constraint set.  We obtain an explicit expression for the minimax regret for these games.  For the case of uniform box constraints, we show that, with worst case covariate sequences, the regret is O(d\\log T), with no dependence on the scaling of the covariates.", "pdf_url": "http://proceedings.mlr.press/v40/Bartlett15.pdf", "keywords": ["linear regression", "online learning", "minimax regret"], "reference": "Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211\u2013246, 2001.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. Neural Networks, IEEE Transactions on, 7(3):604\u2013619, 1996.  J\u00a8urgen Forster. On relative loss bounds in generalized linear regression. In Fundamentals of Com-  putation Theory, pages 269\u2013280. Springer, 1999.  Dean P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084\u20131090, 1991.  Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1\u201363, 1997.  Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Ef\ufb01cient minimax strategies for square loss  games. In Advances in Neural Information Processing Systems, pages 3230\u20133238, 2014.  Edward Moroshko and Koby Crammer. Weighted last-step min\u2013max algorithm with improved sub-  logarithmic regret. Theoretical Computer Science, 558:107\u2013124, 2014.  Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In  13th COLT, pages 100\u2013106, 2000.  Volodimir G. Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371\u2013383. Morgan Kaufmann, 1990.  Volodya Vovk. Competitive on-line linear regression. Advances in Neural Information Processing  Systems, pages 364\u2013370, 1998.  Appendix A. Alternative Pt recurrence  Lemma 11 For the Pt matrices de\ufb01ned in Theorem 2, we have  P \u22121  t =  xqx(cid:124)  q +  t (cid:88)  q=1  T (cid:88)  q=t+1  x 1 + x  (cid:124) q Pqxq (cid:124) q Pqxq  xqx(cid:124) q .  Proof The proof is by induction. We start with  P \u22121  T =  xtx  (cid:124) t .  T (cid:88)  t=1  13   MINIMAX FIXED-DESIGN LINEAR REGRESSION  References  Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, 2001.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. Neural Networks, IEEE Transactions on, 7(3):604-619, 1996.  J\u00a8urgen Forster. On relative loss bounds in generalized linear regression. In Fundamentals of Com-  putation Theory, pages 269-280. Springer, 1999.  Dean P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084-1090, 1991.  Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1-63, 1997.  Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Efficient minimax strategies for square loss  games. In Advances in Neural Information Processing Systems, pages 3230-3238, 2014.  Edward Moroshko and Koby Crammer. Weighted last-step min-max algorithm with improved sub-  logarithmic regret. Theoretical Computer Science, 558:107-124, 2014.  Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In  13th COLT, pages 100-106, 2000.  Volodimir G. Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371-383. Morgan Kaufmann, 1990.  Volodya Vovk. Competitive on-line linear regression. Advances in Neural Information Processing  Systems, pages 364-370, 1998.  "}, "Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions", "abstract": "We consider the problem of optimizing an approximately convex function over a bounded convex set in \\mathbbR^n  using only function evaluations. The problem is reduced to sampling from an \\emphapproximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \\mathcalO^* complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an \u03b5-minimizer after \\mathcalO^*(n^7.5\u03b5^-2) noisy function evaluations  by inducing a \\mathcalO(\u03b5/n)-approximately log concave distribution. We also consider in detail the case when the \u201camount of non-convexity\u201d decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.", "pdf_url": "http://proceedings.mlr.press/v40/Belloni15.pdf", "keywords": [], "reference": "Adamczak, R., Litvak, A. E., Pajor, A., Tomczak-Jaegermann, N., et al. (2010). Quantitative esti- mates of the convergence of the empirical covariance matrix in log-concave ensembles. J. Amer. Math. Soc, 23(2):535-561.  Agarwal, A., Foster, D. P., Hsu, D., Kakade, S. M., and Rakhlin, A. (2013). Stochastic convex  optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213-240.  Applegate, D. and Kannan, R. (1991). Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156-163. ACM.  Belloni, A. and Chernozhukov, V. (2009). On the computational complexity of mcmc-based esti-  mators in large samples. The Annals of Statistics, pages 2011-2055.  Bertsimas, D. and Vempala, S. (2004). Solving convex programs by random walks. Journal of the  ACM (JACM), 51(4):540-556.  Bourgain, J. (1996). Random points in isotropic convex sets. Convex geometric analysis, 34:53-58.  Cholewa, P. W. (1984). Remarks on the stability of functional equations. Aequationes Mathemati-  Conn, A. R., Scheinberg, K., and Vicente, L. N. (2009). Introduction to derivative-free optimization,  Dyer, M., Kannan, R., and Stougie, L. (2013). A simple randomised algorithm for convex optimi-  sation. Mathematical Programming, pages 1-23.  Green, J. W. et al. (1952). Approximately convex functions. Duke Mathematical Journal,  Gu\u00b4edon, O. and Rudelson, M. (2007). Lp-moments of random vectors via majorizing measures.  Advances in Mathematics, 208(2):798-823.  Kalai, A. and Vempala, S. (2006). Simulated annealing for convex optimization. Mathematics of  Operations Research, 31(2):253-266.  Kannan, R., Lov\u00b4asz, L., and Simonovits, M. (1997). Random walks and an o*(n5) volume algorithm  for convex bodies. Random structures and algorithms, 11(1):1-50.  Laczkovich, M. (1999). The local stability of convexity, affinity and of the jensen equation. aequa-  tiones mathematicae, 58(1-2):135-142.  Liang, T., Narayanan, H., and Rakhlin, A. (2014). On zeroth-order stochastic convex optimization  via random walks. arXiv preprint arXiv:1402.2667.  Lov\u00b4asz, L. and Simonovits, M. (1993). Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412.  cae, 27(1):76-86.  volume 8. Siam.  19(3):499-504.  13   OPTIMIZATION OF APPROXIMATELY CONVEX FUNCTIONS  References  Adamczak, R., Litvak, A. E., Pajor, A., Tomczak-Jaegermann, N., et al. (2010). Quantitative esti- mates of the convergence of the empirical covariance matrix in log-concave ensembles. J. Amer. Math. Soc, 23(2):535-561.  Agarwal, A., Foster, D. P., Hsu, D., Kakade, S. M., and Rakhlin, A. (2013). Stochastic convex  optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213-240.  Applegate, D. and Kannan, R. (1991). Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156-163. ACM.  Belloni, A. and Chernozhukov, V. (2009). On the computational complexity of mcmc-based esti-  mators in large samples. The Annals of Statistics, pages 2011-2055.  Bertsimas, D. and Vempala, S. (2004). Solving convex programs by random walks. Journal of the  ACM (JACM), 51(4):540-556.  Bourgain, J. (1996). Random points in isotropic convex sets. Convex geometric analysis, 34:53-58.  Cholewa, P. W. (1984). Remarks on the stability of functional equations. Aequationes Mathemati-  Conn, A. R., Scheinberg, K., and Vicente, L. N. (2009). Introduction to derivative-free optimization,  Dyer, M., Kannan, R., and Stougie, L. (2013). A simple randomised algorithm for convex optimi-  sation. Mathematical Programming, pages 1-23.  Green, J. W. et al. (1952). Approximately convex functions. Duke Mathematical Journal,  Gu\u00b4edon, O. and Rudelson, M. (2007). Lp-moments of random vectors via majorizing measures.  Advances in Mathematics, 208(2):798-823.  Kalai, A. and Vempala, S. (2006). Simulated annealing for convex optimization. Mathematics of  Operations Research, 31(2):253-266.  Kannan, R., Lov\u00b4asz, L., and Simonovits, M. (1997). Random walks and an o*(n5) volume algorithm  for convex bodies. Random structures and algorithms, 11(1):1-50.  Laczkovich, M. (1999). The local stability of convexity, affinity and of the jensen equation. aequa-  tiones mathematicae, 58(1-2):135-142.  Liang, T., Narayanan, H., and Rakhlin, A. (2014). On zeroth-order stochastic convex optimization  via random walks. arXiv preprint arXiv:1402.2667.  Lov\u00b4asz, L. and Simonovits, M. (1993). Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412.  cae, 27(1):76-86.  volume 8. Siam.  19(3):499-504. BELLONI LIANG NARAYANAN RAKHLIN  Lov\u00b4asz, L. and Vempala, S. (2006a). Fast algorithms for logconcave functions: Sampling, rounding, integration and optimization. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 57-68. IEEE.  Lov\u00b4asz, L. and Vempala, S. (2006b). Hit-and-run from a corner. SIAM Journal on Computing,  35(4):985-1005.  Lov\u00b4asz, L. and Vempala, S. (2007). The geometry of logconcave functions and sampling algorithms.  Random Structures & Algorithms, 30(3):307-358.  Nemirovskii, A. S. and Yudin, D. B. (1983). Problem Complexity and Method Efficiency in Opti-  mization. John Wiley.  Rakhlin, A., Shamir, O., and Sridharan, K. (2012). Relax and randomize: From value to algorithms.  In Advances in Neural Information Processing Systems, pages 2141-2149.  Robert, C. P. and Casella, G. (2004). Monte Carlo statistical methods, volume 319. Citeseer.  Rudelson, M. (1999). Random vectors in the isotropic position. Journal of Functional Analysis,  164(1):60-72.  CoRR, abs/1209.2388.  Shamir, O. (2012). On the complexity of bandit and derivative-free stochastic convex optimization.  Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of Com-  putational Mathematics, 12(4):389-434.  Vershynin, R. (2010).  Introduction to the non-asymptotic analysis of random matrices. arXiv  preprint arXiv:1011.3027. OPTIMIZATION OF APPROXIMATELY CONVEX FUNCTIONS  "}, "Bandit Convex Optimization: \\sqrtT Regret in One Dimension": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Bandit Convex Optimization: \\sqrtT Regret in One Dimension", "abstract": "We analyze the minimax regret of the adversarial bandit convex optimization problem.  Focusing on the one-dimensional case, we prove that the minimax regret is \\widetilde\u0398(\\sqrtT) and partially resolve a decade-old open problem. Our analysis is non-constructive, as we do not present a concrete algorithm that attains this regret rate. Instead, we use minimax duality to reduce the problem to a Bayesian setting, where the convex loss functions are drawn from a worst-case distribution, and then we solve the Bayesian version of the problem with a variant of Thompson Sampling. Our analysis features a novel use of convexity, formalized as a \u201clocal-to-global\u201d property of convex functions, that may be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v40/Bubeck15a.pdf", "keywords": [], "reference": "J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2008.  J. Abernethy, A. Agarwal, P. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.  A. Agarwal, D. Foster, D. Hsu, S. Kakade, and A. Rakhlin. Stochastic convex optimization with  bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza- tion with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization: \u221aT regret in one  dimension. arXiv preprint arXiv:1502.06398, 2015.  V. Dani, T. Hayes, and S. Kakade. The price of bandit information for online optimization.  In  Advances in Neural Information Processing Systems (NIPS), 2008.  A. Flaxman, A. Kalai, and B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.  N. Gravin, Y. Peres, and B. Sivan. Towards optimal algorithms for prediction with expert advice.  Arxiv preprint arXiv:1409.3040, 2014.  E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural  Information Processing Systems (NIPS), 2014.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Information and Computation, 132(1):1-63, 1997.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural  Information Processing Systems (NIPS), 2004.  A. Neyman. The maximal variation of martingales of probabilities and repeated games with incom-  plete information. Journal of Theoretical Probability, 26(2):557-567, 2013.  D. Russo and B. van Roy. An information-theoretic analysis of thompson sampling. arXiv preprint  arXiv:1403.5341, 2014.  12   BUBECK DEKEL KOREN PERES  References  J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2008.  J. Abernethy, A. Agarwal, P. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.  A. Agarwal, D. Foster, D. Hsu, S. Kakade, and A. Rakhlin. Stochastic convex optimization with  bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza- tion with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization: \u221aT regret in one  dimension. arXiv preprint arXiv:1502.06398, 2015.  V. Dani, T. Hayes, and S. Kakade. The price of bandit information for online optimization.  In  Advances in Neural Information Processing Systems (NIPS), 2008.  A. Flaxman, A. Kalai, and B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.  N. Gravin, Y. Peres, and B. Sivan. Towards optimal algorithms for prediction with expert advice.  Arxiv preprint arXiv:1409.3040, 2014.  E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural  Information Processing Systems (NIPS), 2014.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Information and Computation, 132(1):1-63, 1997.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural  Information Processing Systems (NIPS), 2004.  A. Neyman. The maximal variation of martingales of probabilities and repeated games with incom-  plete information. Journal of Theoretical Probability, 26(2):557-567, 2013.  D. Russo and B. van Roy. An information-theoretic analysis of thompson sampling. arXiv preprint  arXiv:1403.5341, 2014. BANDIT CONVEX OPTIMIZATION: \u221aT REGRET IN ONE DIMENSION  A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In International Conference on Artificial Intelligence and Statistics (AISTAT), pages 636-642, 2011.  W. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Bulletin of the American Mathematics Society, 25:285-294, 1933. "}, "The entropic barrier: a simple and optimal universal self-concordant barrier": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "The entropic barrier: a simple and optimal universal self-concordant barrier", "abstract": "We prove that the Fenchel dual of the log-Laplace transform of the uniform measure on a convex body in \\mathbbR^n is a (1+o(1)) n-self-concordant barrier, improving a seminal result of Nesterov and Nemirovski. This gives the first explicit construction of a universal barrier for convex bodies with optimal self-concordance parameter. The proof is based on basic geometry of log-concave distributions, and elementary duality in exponential families. The result also gives a new perspective on the minimax regret for the linear bandit problem.", "pdf_url": "http://proceedings.mlr.press/v40/Bubeck15b.pdf", "keywords": []}, "Optimum Statistical Estimation with Strategic Data Sources": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Optimum Statistical Estimation with Strategic Data Sources", "abstract": "We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high quality data is provided at low cost, in the sense that the weighted sum of payments and estimation error is minimized.  The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generalizes to several objectives, including minimizing estimation error subject to budget constraints. Besides our concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples.", "pdf_url": "http://proceedings.mlr.press/v40/Cai15.pdf", "keywords": [], "reference": "[1] A. Badanidiyuru, R. Kleinberg, and A. Slivkins \u201cBandits with Knapsacks: Dynamic procure-  ment for crowdsourcing,\u201d in (31).  [2] D. F. Bacon, Y. Chen, I. Kash, D. C. Parkes, M. Rao, and M. Sridharan \u201cPredicting your own effort,\u201d International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2012.  [3] S. Chawla, J. D. Hartline, and B. Sivan \u201cOptimal crowdsourcing contests,\u201d SODA 2012.  [4] X. Chen, Q. Lin and D. Zhou \u201cOptimistic Knowledge Gradient Policy for Optimal Budget  Allocation in Crowdsourcing,\u201d ICML 2013.  [5] X. Chen, P. N. Bennett, and K. Collins-Thompson \u201cPairwise ranking aggregation in a crowd-  sourced setting,\u201d WSDM, 2013.  [6] E. Clarke \u201cMultipart pricing of public goods,\u201d Public Choice, 11(1):1733, 1971.  [7] A. Dasgupta and A. Ghosh \u201cCrowdsourced judgement elicitation with endogenous profi-  ciency,\u201d International conference on World Wide Web (WWW), 2013.  [8] O. Dekel, F. Fischer, and A. D. Procaccia \u201cIncentive compatible regression learning,\u201d Sympo-  sium on Discrete algorithms (SODA), 2008.  [9] A. Doan, R. Ramakrishnan, and A. Y. Halevy \u201cCrowdsourcing systems on the world-wide  web,\u201d CACM 2011.  [10] F. Fang, M. Stinchcombe, and A. Whinston \u201cPutting Your Money Where Your Mouth Is - A  Betting Platform for Better Prediction,\u201d Review of Network Economics, 6(2), 2007.  [11] RPH Fishe, RP McAfee \u201dNonlinear contracts, zero profits and moral hazard,\u201d Economica,  1987.  in (31).  [12] G. Goel, A. Nikzad, and A. Singla \u201cMatching Workers Expertise with Tasks: Incentives in  Heterogeneous Crowdsourcing Markets,\u201d in (31).  [13] T. Groves \u201cIncentives in teams,\u201d Econometrica, 41(4):617631, 1973.  [14] C. Ho, A. Slivkins, and J. Wortman-Vaughan \u201cAdaptive Contract Design for Crowdsourcing,\u201d  [15] T. Hastie, R. Tibshirani, and J. Friedman \u201cThe Elements of Statistical Learning,\u201d Springer-  Verlag, New York, 2001  [16] J. Horton and L. Chilton \u201cThe labor economics of paid crowdsourcing,\u201d EC 2010.  [17] ICML 2013 Worskhop: Machine Learning Meets Crowdsourcing, http://www.ics.  uci.edu/\u02dcqliu1/MLcrowd_ICML_workshop/  [18] N. Immorlica, G. Stoddard, and V. Syrgkanis \u201cSocial Status and the Design of Optimal  Badges,\u201d in (31).  13   OPTIMUM STATISTICAL ESTIMATION WITH STRATEGIC DATA SOURCES  References  [1] A. Badanidiyuru, R. Kleinberg, and A. Slivkins \u201cBandits with Knapsacks: Dynamic procure-  ment for crowdsourcing,\u201d in (31).  [2] D. F. Bacon, Y. Chen, I. Kash, D. C. Parkes, M. Rao, and M. Sridharan \u201cPredicting your own effort,\u201d International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2012.  [3] S. Chawla, J. D. Hartline, and B. Sivan \u201cOptimal crowdsourcing contests,\u201d SODA 2012.  [4] X. Chen, Q. Lin and D. Zhou \u201cOptimistic Knowledge Gradient Policy for Optimal Budget  Allocation in Crowdsourcing,\u201d ICML 2013.  [5] X. Chen, P. N. Bennett, and K. Collins-Thompson \u201cPairwise ranking aggregation in a crowd-  sourced setting,\u201d WSDM, 2013.  [6] E. Clarke \u201cMultipart pricing of public goods,\u201d Public Choice, 11(1):1733, 1971.  [7] A. Dasgupta and A. Ghosh \u201cCrowdsourced judgement elicitation with endogenous profi-  ciency,\u201d International conference on World Wide Web (WWW), 2013.  [8] O. Dekel, F. Fischer, and A. D. Procaccia \u201cIncentive compatible regression learning,\u201d Sympo-  sium on Discrete algorithms (SODA), 2008.  [9] A. Doan, R. Ramakrishnan, and A. Y. Halevy \u201cCrowdsourcing systems on the world-wide  web,\u201d CACM 2011.  [10] F. Fang, M. Stinchcombe, and A. Whinston \u201cPutting Your Money Where Your Mouth Is - A  Betting Platform for Better Prediction,\u201d Review of Network Economics, 6(2), 2007.  [11] RPH Fishe, RP McAfee \u201dNonlinear contracts, zero profits and moral hazard,\u201d Economica,  1987.  in (31).  [12] G. Goel, A. Nikzad, and A. Singla \u201cMatching Workers Expertise with Tasks: Incentives in  Heterogeneous Crowdsourcing Markets,\u201d in (31).  [13] T. Groves \u201cIncentives in teams,\u201d Econometrica, 41(4):617631, 1973.  [14] C. Ho, A. Slivkins, and J. Wortman-Vaughan \u201cAdaptive Contract Design for Crowdsourcing,\u201d  [15] T. Hastie, R. Tibshirani, and J. Friedman \u201cThe Elements of Statistical Learning,\u201d Springer-  Verlag, New York, 2001  [16] J. Horton and L. Chilton \u201cThe labor economics of paid crowdsourcing,\u201d EC 2010.  [17] ICML 2013 Worskhop: Machine Learning Meets Crowdsourcing, http://www.ics.  uci.edu/\u02dcqliu1/MLcrowd_ICML_workshop/  [18] N. Immorlica, G. Stoddard, and V. Syrgkanis \u201cSocial Status and the Design of Optimal  Badges,\u201d in (31). CAI DASKALAKIS PAPADIMITRIOU  [19] S. Ioannidis and P. Loiseau \u201cLinear regression as a non-cooperative game,\u201d Web and Internet  Economics (WINE), 2013.  [20] J. Kieffer. \u201cOptimal Experiment Designs, (with discussion),\u201d J. Roy. Statist. Soc. B. 272-319,  [21] J. Kieffer. \u201cOptimum designs in regression problems,\u201d II. Ann. Math. Statist. 32 298-325,  1959.  1961.  Statist. 2 849-879, 1974.  30 271-294, 1959  363-366, 1960.  [22] J.Kieffer. \u201cGeneral equivalence theory for optimum designs (approximate theory),\u201d Ann.  [23] J. Kieffer and J. Wolfowitz. \u201cOptimum designs in regression problems,\u201d Ann. Math. Statist.  [24] J. Kieffer and J. Wolfowitz. \u201cThe equivalence of two extremum problems, \u201d Canad.J.Math. 14  [25] J. Kieffer and J. Wolfowitz. \u201cOn a theorem of Hoel and Levine on extrapolation,\u201d Ann.  Math.Statist3.61627-1655, 1965.  [26] A. Mas-Colell, M. D. Whinston, and J. R. Green. Microeconomic Theory. Oxford university  press, 1995.  [27] R. Meir, A. D. Procaccia, and J. S. Rosenschein \u201cStrategyproof Classification under Constant  Hypotheses: A Tale of Two Functions,\u201d AAAI, 2008.  [28] R. Meir, A. D. Procaccia, and J. S. Rosenschein \u201cStrategyproof Classification with Shared  Inputs,\u201d IJCAI, 2009.  [29] N. Miller, P. Resnick, and R. Zeckhauser. \u201dEliciting informative feedback: The peer-prediction  method.\u201d Management Science 51.9 (2005): 1359-1373.  [30] S. Nath, P. Dayama, D. Garg, and Y. Narahari \u201cMechanism design for time critical and cost  critical task execution via crowdsourcing,\u201d Internet and Network Economics (WINE), 2012.  [31] NIPS 2013 Woskshop on Croudsourcing: Theory, Algorithms, and Applications, http://  www.ics.uci.edu/\u02dcqliu1/nips13_workshop/.  [32] N. Nisan, T. Roughgarden, E. Tardos, and V. Vazirani (eds.) \u201cAlgorithmic Game Theory,\u201d  Cambridge University Press, 2007.  [33] R. Nix and M. Kantarciouglu \u201cIncentive compatible privacy-preserving distributed classifica- tion,\u201d IEEE Transactions on Dependable and Secure Computing, 9(4): 451-462, 2012.  [34] R. Myerson \u201cOptimal Auction Design,\u201d Mathematics of Operations Research, 6(1):5873,  1981.  [35] D. DiPalantino and M. Vojnovic \u201cCrowdsourcing and all-pay auctions,\u201d EC 2009.  [36] G. Patterson, G. Van Horn, S. Belongie, P. Perona, and J. Hays \u201cBootstrapping Fine-Grained  Classifiers: Active Learning with a Crowd in the Loop,\u201d in (31). OPTIMUM STATISTICAL ESTIMATION WITH STRATEGIC DATA SOURCES  [37] F. Pukelsheim. \u201cOptimal Design of Experiments,\u201d Wiley & Sons, 1993.  [38] V. Rajan, S. Bhattacharya, L. Celis, D. Chander, K. Dasgupta and S. Karanam \u201cCrowdControl: An online learning approach for optimal task scheduling in a dynamic crowd platform,\u201d in (17)  [39] P. Ruvolo, J. Whitehill and J. Movellan \u201cExploiting Commonality and Interaction Effects in  Crowdsourcing Tasks Using Latent Factor Models,\u201d in (31).  [40] S. Sabato and A. Kalai \u201cFeature Multi-Selection among Subjective Features,\u201d in (17).  [41] Y. Singer and M. Mittal \u201cPricing mechanisms for crowdsourcing markets,\u201d WWW 2013.  [42] A. Singla and A. Krause \u201cTruthful incentives in crowdsourcing tasks using regret minimization  mechanisms,\u201d WWW 20131.  [43] A. Singla and A. Krause \u201cTruthful Incentives for Privacy Tradeoff: Mechanisms for Data  Gathering in Community Sensing,\u201d in (17)  [44] T. Jaakkola \u201cLecture Notes of Machine Learning\u201d  http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/ 6-867-machine-learning-fall-2006/lecture-notes/lec5.pdf  [45] W. Vickrey \u201cCounterspeculation, auctions, and competitive sealed tenders,\u201d The Journal of  Finance, 16(1):837, 1961.  [46] M.Cerny, and M. Hladik. \u201cTwo complexity results on c-optimality in experimental design,\u201d  Computational Optimization and Applications 51, 13971408, 2012.  "}, "On the Complexity of Learning with Kernels": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On the Complexity of Learning with Kernels", "abstract": "A well-recognized limitation of kernel learning is the requirement to handle a kernel matrix, whose size is quadratic in the number of training examples. Many methods have been proposed to reduce this computational cost, mostly by using a subset of the kernel matrix entries, or some form of low-rank matrix approximation, or a random projection method. In this paper, we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix. We show that there are kernel learning problems where no such method will lead to non-trivial computational savings. Our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function, the regularization parameter, the norm of the desired predictor, and the kernel matrix rank. Our results also suggest cases where more efficient kernel learning might be possible.", "pdf_url": "http://proceedings.mlr.press/v40/Cesa-Bianchi15.pdf", "keywords": [], "reference": "abs/1411.0306, 2014.  A. El Alaoui and M. Mahoney. Fast randomized kernel methods with statistical guarantees. CoRR,  F. Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT, 2013.  F. Bach and M. Jordan. Predictive low-rank decomposition for kernel methods. In ICML, 2005.  Z. Bar-Yossef. Sampling lower bounds via information theory. In STOC, 2003.  Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. Tracking the best hyperplane with  a simple budget Perceptron. Machine Learning, 69(2-3):143-167, 2007.  C. Cortes, M. Mohri, and A. Talwalkar. On the impact of kernel approximation on learning accuracy.  A. Cotter, S. Shalev-Shwartz, and N. Srebro. The kernelized stochastic batch Perceptron. In ICML,  A. Cotter, S. Shalev-Shwartz, and N. Srebro. Learning optimally sparse Support Vector Machines.  In AISTATS, 2010.  2012.  In ICML, 2013.  B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. Balcan, and L. Song. Scalable kernel methods via  doubly stochastic gradients. arXiv preprint arXiv:1407.5599, 2014.  O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: A kernel-based Perceptron on a budget.  SIAM Journal on Computing, 37(5):1342-1372, 2008.  P. Drineas and M. Mahoney. On the Nystr\u00a8om method for approximating a Gram matrix for improved  kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, 2005.  S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representations. The  Journal of Machine Learning Research, 2:243-264, 2002.  A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for finding low-rank approxi-  mations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  C.-J. Hsieh, S. Si, and I. Dhillon. A divide-and-conquer solver for kernel Support Vector Machines.  In ICML, 2014.  S. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems, pages 793-800, 2009.  S. Kumar, M. Mohri, and A. Talwalkar. Sampling techniques for the Nystr\u00a8om method. In AISTATS,  2009.  M. Lin, S. Weng, and C. Zhang. On the sample complexity of random Fourier features for online learning: How many random Fourier features do we need? ACM Transactions on Knowledge Discovery from Data (TKDD), 8(3):13, 2014.  13   ON THE COMPLEXITY OF LEARNING WITH KERNELS  References  abs/1411.0306, 2014.  A. El Alaoui and M. Mahoney. Fast randomized kernel methods with statistical guarantees. CoRR,  F. Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT, 2013.  F. Bach and M. Jordan. Predictive low-rank decomposition for kernel methods. In ICML, 2005.  Z. Bar-Yossef. Sampling lower bounds via information theory. In STOC, 2003.  Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. Tracking the best hyperplane with  a simple budget Perceptron. Machine Learning, 69(2-3):143-167, 2007.  C. Cortes, M. Mohri, and A. Talwalkar. On the impact of kernel approximation on learning accuracy.  A. Cotter, S. Shalev-Shwartz, and N. Srebro. The kernelized stochastic batch Perceptron. In ICML,  A. Cotter, S. Shalev-Shwartz, and N. Srebro. Learning optimally sparse Support Vector Machines.  In AISTATS, 2010.  2012.  In ICML, 2013.  B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. Balcan, and L. Song. Scalable kernel methods via  doubly stochastic gradients. arXiv preprint arXiv:1407.5599, 2014.  O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: A kernel-based Perceptron on a budget.  SIAM Journal on Computing, 37(5):1342-1372, 2008.  P. Drineas and M. Mahoney. On the Nystr\u00a8om method for approximating a Gram matrix for improved  kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, 2005.  S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representations. The  Journal of Machine Learning Research, 2:243-264, 2002.  A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for finding low-rank approxi-  mations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  C.-J. Hsieh, S. Si, and I. Dhillon. A divide-and-conquer solver for kernel Support Vector Machines.  In ICML, 2014.  S. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems, pages 793-800, 2009.  S. Kumar, M. Mohri, and A. Talwalkar. Sampling techniques for the Nystr\u00a8om method. In AISTATS,  2009.  M. Lin, S. Weng, and C. Zhang. On the sample complexity of random Fourier features for online learning: How many random Fourier features do we need? ACM Transactions on Knowledge Discovery from Data (TKDD), 8(3):13, 2014. CESA-BIANCHI MANSOUR SHAMIR  M. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings  of the National Academy of Sciences, 106(3):697-702, 2009.  A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.  A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with  randomization in learning. In NIPS, 2008.  G. Raskutti, M. Wainwright, and B. Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. Journal of Machine Learning Research, 15(1):335-366, 2014.  B. Scholkopf and A. Smola. Learning with kernels: Support Vector Machines, regularization,  optimization, and beyond. MIT press, 2001.  J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge University  K. Sridharan, S. Shalev-Shwartz, and N. Srebro. Fast rates for regularized objectives. In NIPS,  T. Yang, Y.-F. Li, M. Mahdavi, R. Jin, and Z.-H. Zhou. Nystr\u00a8om method vs random Fourier features:  A theoretical and empirical comparison. In NIPS, 2012.  Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive  Approximation, 26(2):289-315, 2007.  Y. Zhang, J. Duchi, and M. Wainwright. Divide and conquer kernel Ridge Regression. In COLT,  Press, 2004.  2009.  2013. ON THE COMPLEXITY OF LEARNING WITH KERNELS  "}, "Learnability of Solutions to Conjunctive Queries: The Full Dichotomy": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Learnability of Solutions to Conjunctive Queries: The Full Dichotomy", "abstract": "The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classification program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In order to obtain our negative result, we make use of universal-algebraic concepts, and our result is phrased in terms of the varietal property of non-congruence modularity.", "pdf_url": "http://proceedings.mlr.press/v40/Chen15a.pdf", "keywords": ["conjunctive query", "prediction with membership queries", "universal algebra"], "reference": "E. Allender, M. Bauland, N. Immerman, H. Schnoor, and H. Vollmer. The Complexity of Satisfia- bility Problems: Refining Schaefer\u2019s Theorem. Journal of Computer and System Sciences, 75(4): 245-254, 2009.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1987.  Dana Angluin and Michael Kharitonov. When won\u2019t membership queries help? J. Comput. Syst.  Sci., 50(2):336-355, 1995.  Learning, 9:147-164, 1992.  Dana Angluin, Michael Frazier, and Leonard Pitt. Learning conjunctions of horn clauses. Machine  L. Barto. A proof of the Valeriote conjecture. 2014. Submitted.  Libor Barto and Marcin Kozik. Constraint satisfaction problems solvable by local consistency  methods. J. ACM, 61(1):3, 2014.  Arnab Bhattacharyya and Yuichi Yoshida. An algebraic characterization of testable boolean csps. In Automata, Languages, and Programming - 40th International Colloquium, ICALP 2013, Riga, Latvia, July 8-12, 2013, Proceedings, Part I, pages 123-134, 2013.  Simone Bova, Hubie Chen, and Matthew Valeriote. Generic expression hardness results for primi-  tive positive formula comparison. Inf. Comput., 222:108-120, 2013.  Nader H. Bshouty. Exact learning from membership queries: Some techniques, results and new di- rections. In Algorithmic Learning Theory - 24th International Conference, ALT 2013, Singapore, October 6-9, 2013. Proceedings, pages 33-52, 2013.  Nader H. Bshouty, Jeffrey C. Jackson, and Christino Tamon. Exploring learnability between exact  and PAC. J. Comput. Syst. Sci., 70(4):471-484, 2005.  A. Bulatov, P. Jeavons, and A. Krokhin. Classifying the Complexity of Constraints using Finite  Algebras. SIAM Journal on Computing, 34(3):720-742, 2005.  Andrei Bulatov, Hubie Chen, and Victor Dalmau. Learning intersection-closed classes with signa-  tures. Theoretical Computer Science, 382(3):209-220, 2007.  11   LEARNABILITY OF SOLUTIONS TO CONJUNCTIVE QUERIES  Essentially, Theorem 13 is proved in the following way. In order to translate a 2-sorted con- junctive query \u03c6 over pentagons to a conjunctive query \u03c6(cid:48) over A, the relations \u03b2 and \u03b3 are used to simulate the two sorts, and the relation \u03b1 is used to simulate the behavior of the relation R. Also, in the resulting conjunctive query \u03c6(cid:48), all of the variables are related by the relation DU (where U is the total number of variables), effectively localizing \u03c6(cid:48) to the pentagons found in the set P.  The first author was supported by the Spanish project TIN2013-46181-C2-2-R, by the Basque project GIU12/26, and by the Basque grant UFI11/45. The second author was supported by the Natural Sciences and Engineering Research Council of Canada.  Acknowledgments  References  E. Allender, M. Bauland, N. Immerman, H. Schnoor, and H. Vollmer. The Complexity of Satisfia- bility Problems: Refining Schaefer\u2019s Theorem. Journal of Computer and System Sciences, 75(4): 245-254, 2009.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1987.  Dana Angluin and Michael Kharitonov. When won\u2019t membership queries help? J. Comput. Syst.  Sci., 50(2):336-355, 1995.  Learning, 9:147-164, 1992.  Dana Angluin, Michael Frazier, and Leonard Pitt. Learning conjunctions of horn clauses. Machine  L. Barto. A proof of the Valeriote conjecture. 2014. Submitted.  Libor Barto and Marcin Kozik. Constraint satisfaction problems solvable by local consistency  methods. J. ACM, 61(1):3, 2014.  Arnab Bhattacharyya and Yuichi Yoshida. An algebraic characterization of testable boolean csps. In Automata, Languages, and Programming - 40th International Colloquium, ICALP 2013, Riga, Latvia, July 8-12, 2013, Proceedings, Part I, pages 123-134, 2013.  Simone Bova, Hubie Chen, and Matthew Valeriote. Generic expression hardness results for primi-  tive positive formula comparison. Inf. Comput., 222:108-120, 2013.  Nader H. Bshouty. Exact learning from membership queries: Some techniques, results and new di- rections. In Algorithmic Learning Theory - 24th International Conference, ALT 2013, Singapore, October 6-9, 2013. Proceedings, pages 33-52, 2013.  Nader H. Bshouty, Jeffrey C. Jackson, and Christino Tamon. Exploring learnability between exact  and PAC. J. Comput. Syst. Sci., 70(4):471-484, 2005.  A. Bulatov, P. Jeavons, and A. Krokhin. Classifying the Complexity of Constraints using Finite  Algebras. SIAM Journal on Computing, 34(3):720-742, 2005.  Andrei Bulatov, Hubie Chen, and Victor Dalmau. Learning intersection-closed classes with signa-  tures. Theoretical Computer Science, 382(3):209-220, 2007. CHEN VALERIOTE  Andrei A. Bulatov. The complexity of the counting constraint satisfaction problem. J. ACM, 60(5):  34, 2013.  Stanley Burris and H. P. Sankappanavar. A Course in Universal Algebra. Springer, 1981.  Hubie Chen. Meditations on quantified constraint satisfaction. In Robert Constable and Alexandra Silva, editors, Logic and Program Semantics, volume 7230 of Lecture Notes in Computer Science, pages 35-49. Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-29484-6.  N. Creignou, S. Khanna, and M. Sudan. Complexity Classification of Boolean Constraint Satis- faction Problems. SIAM Monographs on Discrete Mathematics and Applications. Society for Industrial and Applied Mathematics, 2001.  Victor Dalmau. A dichotomy theorem for learning quantified boolean formulas. Machine Learning,  35(3):207-224, 1999.  (1-3):485-511, 2003.  Victor Dalmau and Peter Jeavons. Learnability of quantified formulas. Theor. Comput. Sci., 306  Ralph Freese and Matthew A. Valeriote. On the complexity of some Maltsev conditions. Internat.  J. Algebra Comput., 19(1):41-77, 2009. ISSN 0218-1967.  P. Idziak, P. Markovic, R. McKenzie, M. Valeriote, and R. Willard. Tractability and learnability  arising from algebras with few subpowers. SIAM J. Comput., 39(7):3023-3037, 2010.  Jeffrey C. Jackson and Rocco A. Servedio. On learning random DNF formulas under the uniform  distribution. Theory of Computing, 2(1):147-172, 2006.  A. Kazda. Personal communication, 2014.  Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae  and finite automata. J. ACM, 41(1):67-95, 1994.  R. McKenzie, G. McNulty, and W. Taylor. Algebras, Lattices, Varieties, vol. 1. Wadsworth &  Brooks/Cole, 1987.  (3):430-467, 1990.  Leonard Pitt and Manfred K. Warmuth. Prediction-preserving reducibility. J. Comput. Syst. Sci., 41  Prasad Raghavendra. Optimal algorithms and inapproximability results for every csp? In Proceed- ings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008, pages 245-254, 2008.  T. J. Schaefer. The complexity of satisfiability problems. In Proceedings of STOC\u201978, pages 216-  226, 1978. "}, "Sequential Information Maximization: When is Greedy Near-optimal?": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Sequential Information Maximization: When is Greedy Near-optimal?", "abstract": "Optimal information gathering is a central challenge in machine learning and science in general.  A common objective that quantifies the usefulness of observations is Shannon\u2019s mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations.  In this paper, we analyze the widely used greedy policy for this task, and identify problem instances where it provides provably near-maximal utility, even in the challenging setting of persistent noise.  Our results depend on a natural separability condition associated with a channel injecting noise into the observations. We also identify examples where this separability parameter is necessary in the bound: if it is too small, then the greedy policy fails to select informative tests.", "pdf_url": "http://proceedings.mlr.press/v40/Chen15b.pdf", "keywords": ["Active learning", "Information theory", "Optimization"], "reference": "Maria-Florina Balcan and Ruth Urner. Active learning-modern learning theory. 2015.  Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Pro- ceedings of the 23rd international conference on Machine learning, pages 65-72. ACM, 2006.  Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In Learning  Theory, pages 35-50. Springer, 2007.  Scott M Berry, Bradley P Carlin, J Jack Lee, and Peter Muller. Bayesian adaptive methods for  clinical trials. CRC press, 2010.  Marat Valievich Burnashev. Data transmission over a discrete channel with feedback. random trans-  mission time. Problemy peredachi informatsii, 12(4):10-30, 1976.  Venkatesan T Chakaravarthy, Vinayaka Pandit, Sambuddha Roy, Pranjal Awasthi, and Mukesh Mo- hania. Decision trees for entity identification: Approximation algorithms and hardness results. In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 53-62. ACM, 2007.  Yuxin Chen, Shervin Javdani, Amin Karbasi, James Andrew Bagnell, Siddhartha Srinivasa, and An- dreas Krause. Submodular surrogates for value of information. In Proc. Conference on Artificial Intelligence (AAAI), 2015.  Herman Chernoff. Sequential design of experiments. The Annals of Mathematical Statistics, pages  755-770, 1959.  Imre Csiszar and J\u00b4anos K\u00a8orner.  Information theory: coding theorems for discrete memoryless  systems. Cambridge University Press, 2011.  Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Information  Processing Systems 17. 2005.  Sanjoy Dasgupta and J Langford. Active learning. Encyclopedia of Machine Learning, 2011.  Amol Deshpande, Lisa Hellerstein, and Devorah Kletenik. Approximation algorithms for stochastic boolean function evaluation and stochastic submodular set cover. In Proceedings of the Twenty- Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1453-1467. SIAM, 2014.  Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 1972.  Yoav Freund, H Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the  query by committee algorithm. Machine learning, 28(2-3):133-168, 1997.  13   SEQUENTIAL INFORMATION MAXIMIZATION  This work was supported in part by the DARPA MSEE FA8650-11-1-7156, ERC StG 307036, a Microsoft Research Faculty Fellowship, and a Google European Doctoral Fellowship.  Acknowledgments  References  Maria-Florina Balcan and Ruth Urner. Active learning-modern learning theory. 2015.  Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Pro- ceedings of the 23rd international conference on Machine learning, pages 65-72. ACM, 2006.  Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In Learning  Theory, pages 35-50. Springer, 2007.  Scott M Berry, Bradley P Carlin, J Jack Lee, and Peter Muller. Bayesian adaptive methods for  clinical trials. CRC press, 2010.  Marat Valievich Burnashev. Data transmission over a discrete channel with feedback. random trans-  mission time. Problemy peredachi informatsii, 12(4):10-30, 1976.  Venkatesan T Chakaravarthy, Vinayaka Pandit, Sambuddha Roy, Pranjal Awasthi, and Mukesh Mo- hania. Decision trees for entity identification: Approximation algorithms and hardness results. In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 53-62. ACM, 2007.  Yuxin Chen, Shervin Javdani, Amin Karbasi, James Andrew Bagnell, Siddhartha Srinivasa, and An- dreas Krause. Submodular surrogates for value of information. In Proc. Conference on Artificial Intelligence (AAAI), 2015.  Herman Chernoff. Sequential design of experiments. The Annals of Mathematical Statistics, pages  755-770, 1959.  Imre Csiszar and J\u00b4anos K\u00a8orner.  Information theory: coding theorems for discrete memoryless  systems. Cambridge University Press, 2011.  Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Information  Processing Systems 17. 2005.  Sanjoy Dasgupta and J Langford. Active learning. Encyclopedia of Machine Learning, 2011.  Amol Deshpande, Lisa Hellerstein, and Devorah Kletenik. Approximation algorithms for stochastic boolean function evaluation and stochastic submodular set cover. In Proceedings of the Twenty- Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1453-1467. SIAM, 2014.  Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 1972.  Yoav Freund, H Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the  query by committee algorithm. Machine learning, 28(2-3):133-168, 1997. CHEN HASSANI KARBASI KRAUSE  Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active  learning and stochastic optimization. JAIR, 2011.  Daniel Golovin, Andreas Krause, and Debajyoti Ray. Near-optimal bayesian active learning with noisy observations. In Proc. Neural Information Processing Systems (NIPS), December 2010.  Alon Gonen, Sivan Sabato, and Shai Shalev-Shwartz. Efficient active learning of halfspaces: an  aggressive approach. The Journal of Machine Learning Research, 14(1):2583-2615, 2013.  Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the  24th international conference on Machine learning, pages 353-360. ACM, 2007.  Steve Hanneke. Statistical Theory of Active Learning. Now Publishers Incorporated, 2014.  Steve Hanneke and Liu Yang. Minimax analysis of active learning. CoRR, abs/1410.0996, 2014.  Michael Horstein. Sequential transmission using noiseless feedback. Information Theory, IEEE  Transactions on, 9(3):136-143, 1963.  Matti K\u00a8a\u00a8ari\u00a8ainen. Active learning in the non-realizable case. In Algorithmic Learning Theory, pages  63-77. Springer, 2006.  Haim Kaplan, Eyal Kushilevitz, and Yishay Mansour. Learning with attribute costs. In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 356-365. ACM, 2005.  Chun-Wa Ko, Jon Lee, and Maurice Queyranne. An exact algorithm for maximum entropy sam-  pling. Operations Research, 43(4):684-691, 1995.  S Rao Kosaraju, Teresa M Przytycka, and Ryan Borgstrom. On an optimal split tree problem. In  Algorithms and Data Structures, pages 157-168. Springer, 1999.  Andreas Krause and Carlos Guestrin. Near-optimal nonmyopic value of information in graphical  models. In Conference on Uncertainty in Artificial Intelligence (UAI), July 2005.  Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of  Mathematical Statistics, pages 986-1005, 1956.  SP Luttrell. The use of transinformation in the design of data sampling schemes for inverse prob-  lems. Inverse Problems, 1(3):199, 1985.  David MacKay. Information-based objective functions for active data selection. Neural computa-  tion, 4(4):590-604, 1992.  CoRR, abs/1312.2315, 2013a.  Mohammad Naghshvar, Tara Javidi, and Kamalika Chaudhuri. Noisy bayesian active learning.  Mohammad Naghshvar, Tara Javidi, et al. Active sequential hypothesis testing. The Annals of  Statistics, 41(6):2703-2738, 2013b.  George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265-294, 1978. SEQUENTIAL INFORMATION MAXIMIZATION  Robert Nowak. Noisy generalized binary search. In Advances in neural information processing  systems, pages 1366-1374, 2009.  Burr Settles. Active learning. Synthesis Lectures on Artificial Intelligence and Machine Learning,  6(1):1-114, 2012.  379-423, 1948.  pages 135-166, 2004.  C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27(3):  Alexandre B Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statistics,  Jason L Williams, John Iii, and Alan S Willsky. Performance guarantees for information theoretic active inference. In International Conference on Artificial Intelligence and Statistics, pages 620- 627, 2007.  Alice X. Zheng, Irina Rish, and Alina Beygelzimer. Efficient test selection in active diagnosis via  entropy approximation. In In Proceedings of UAI-05, 2005.  "}, "Efficient Sampling for Gaussian Graphical Models  via Spectral Sparsification": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Efficient Sampling for Gaussian Graphical Models  via Spectral Sparsification", "abstract": "Motivated by a sampling problem basic to computational statistical inference, we develop a toolset based on spectral sparsification for a family of fundamental problems involving Gaussian sampling, matrix functionals, and reversible Markov chains. Drawing on the connection between Gaussian graphical models and the recent breakthroughs in spectral graph theory, we give the first nearly linear time algorithm for the following basic matrix problem: Given an n\\times n Laplacian matrix \\mathbfM and a constant -1 \u2264p \u22641, provide efficient access to a sparse n\\times n linear operator \\tilde\\mathbfC such that $\\mathbfM^p \u2248\\tilde\\mathbfC \\tilde\\mathbfC^\u22a4, where \u2248denotes spectral similarity. When p is set to -1, this gives the first parallel sampling algorithm that is essentially optimal both in total work and randomness for Gaussian random fields with symmetric diagonally dominant (SDD) precision matrices. It only requires \\em nearly linear work and 2n \\em i.i.d. random univariate Gaussian samples to generate an n-dimensional \\em i.i.d. Gaussian random sample in polylogarithmic depth. The key ingredient of our approach is an integration of spectral sparsification with multilevel method: Our algorithms are based on factoring \\mathbfM^p$ into a product of well-conditioned matrices, then introducing powers and replacing dense matrices with sparse approximations. We give two sparsification methods for this approach that may be of independent interest. The first invokes Maclaurin series on the factors, while the second builds on our new nearly linear time spectral sparsification algorithm for random-walk matrix polynomials. We expect these algorithmic advances will also help to strengthen the connection between machine learning and spectral graph theory, two of the most active fields in understanding large data and networks.", "pdf_url": "http://proceedings.mlr.press/v40/Cheng15.pdf", "keywords": ["Gaussian Sampling", "Spectral Sparsification", "Matrix Polynomials", "Random Walks"], "reference": "2014.  Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient MCMC. In ICML,  Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to  MCMC for machine learning. Machine learning, 50, 2003.  Rajendra Bhatia. Positive definite matrices. Princeton University Press, 2007.  Christopher M. Bishop. Pattern recognition and machine learning. springer New York, 2006.  Edmond Chow and Yousef Saad. Preconditioned krylov subspace methods for sampling multivariate  gaussian distributions. SIAM Journal on Scientific Computing, 2014.  Paul Christiano, Jonathan A Kelner, Aleksander Madry, Daniel A Spielman, and Shang-Hua Teng. Electrical \ufb02ows, laplacian systems, and faster approximation of maximum \ufb02ow in undirected graphs. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 273-282. ACM, 2011.  Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and structure defini- tion of data: Multiscale methods. Proceedings of the National Academy of Sciences of the United States of America, 102(21):7432-7437, 2005.  Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized \ufb02ow via interior point algorithms. In Proceedings of the 40th annual ACM symposium on Theory of computing, STOC \u201908. ACM, 2008.  Joseph Gonzalez, Yucheng Low, Arthur Gretton, and Carlos Guestrin. Parallel gibbs sampling:  From colored fields to thin junction trees. In Proc. of AISTATS\u201911, 2011.  Keith D Gremban. Combinatorial preconditioners for sparse, symmetric, diagonally dominant  linear systems. PhD thesis, Carnegie Mellon University, 1996.  Nicholas J Higham and Lijing Lin. On pth roots of stochastic matrices. Linear Algebra and its  Applications, 435(3):448-463, 2011.  Alexander Ihler, Er T. Ihler, Erik Sudderth, William Freeman, and Alan Willsky. Efficient multiscale  sampling from products of gaussian mixtures. In In NIPS. MIT Press, 2003.  Matthew Johnson, James Saunderson, and Alan Willsky. Analyzing hogwild parallel gaussian gibbs sampling. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2715-2723. Curran Associates, Inc., 2013.  M. I. Jordan. Learning in Graphical Models. The MIT press, 1998.  Jonathan A Kelner and Alex Levin. Spectral sparsification in the semi-streaming setting. Theory of  Computing Systems, 53(2):243-262, 2013.  13   EFFICIENT GAUSSIAN SAMPLING VIA SPECTRAL SPARSIFICATION  References  2014.  Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient MCMC. In ICML,  Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to  MCMC for machine learning. Machine learning, 50, 2003.  Rajendra Bhatia. Positive definite matrices. Princeton University Press, 2007.  Christopher M. Bishop. Pattern recognition and machine learning. springer New York, 2006.  Edmond Chow and Yousef Saad. Preconditioned krylov subspace methods for sampling multivariate  gaussian distributions. SIAM Journal on Scientific Computing, 2014.  Paul Christiano, Jonathan A Kelner, Aleksander Madry, Daniel A Spielman, and Shang-Hua Teng. Electrical \ufb02ows, laplacian systems, and faster approximation of maximum \ufb02ow in undirected graphs. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 273-282. ACM, 2011.  Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and structure defini- tion of data: Multiscale methods. Proceedings of the National Academy of Sciences of the United States of America, 102(21):7432-7437, 2005.  Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized \ufb02ow via interior point algorithms. In Proceedings of the 40th annual ACM symposium on Theory of computing, STOC \u201908. ACM, 2008.  Joseph Gonzalez, Yucheng Low, Arthur Gretton, and Carlos Guestrin. Parallel gibbs sampling:  From colored fields to thin junction trees. In Proc. of AISTATS\u201911, 2011.  Keith D Gremban. Combinatorial preconditioners for sparse, symmetric, diagonally dominant  linear systems. PhD thesis, Carnegie Mellon University, 1996.  Nicholas J Higham and Lijing Lin. On pth roots of stochastic matrices. Linear Algebra and its  Applications, 435(3):448-463, 2011.  Alexander Ihler, Er T. Ihler, Erik Sudderth, William Freeman, and Alan Willsky. Efficient multiscale  sampling from products of gaussian mixtures. In In NIPS. MIT Press, 2003.  Matthew Johnson, James Saunderson, and Alan Willsky. Analyzing hogwild parallel gaussian gibbs sampling. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2715-2723. Curran Associates, Inc., 2013.  M. I. Jordan. Learning in Graphical Models. The MIT press, 1998.  Jonathan A Kelner and Alex Levin. Spectral sparsification in the semi-streaming setting. Theory of  Computing Systems, 53(2):243-262, 2013. CHENG CHENG LIU PENG TENG  Jonathan A. Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combina- torial algorithm for solving sdd systems in nearly-linear time. In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC \u201913, 2013.  Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques ISBN 0262013193,  - Adaptive Computation and Machine Learning. The MIT Press, 2009. 9780262013192.  Ioannis Koutis. A simple parallel algorithm for spectral sparsification. CoRR, 2014.  Ioannis Koutis, Gary L. Miller, and Richard Peng. Approaching optimality for solving sdd linear systems. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 235-244, 2010.  S. Leang Shieh, Jason Tsai, and Sui Lian. Determining continuous-time state equations from discrete-time state equations via the principal q th root method. Automatic Control, IEEE Trans- actions on, 1986.  Ying Liu, Oliver Kosut, and Alan S. Willsky. Sampling from gaussian graphical models using sub- graph perturbations. In Proceedings of the 2013 IEEE International Symposium on Information Theory, Istanbul, Turkey, July 7-12, 2013, 2013.  Gary L. Miller, Richard Peng, and Shen Chen Xu. Improved parallel algorithms for spanners and  hopsets. CoRR, 2013.  Geng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wright. Hogwild: A lock-free approach  to parallelizing stochastic gradient descent. In NIPS, 2011.  Richard Peng. Algorithm Design Using Spectral Graph Theory. PhD thesis, Carnegie Mellon  University, Pittsburgh, August 2013. CMU CS Tech Report CMU-CS-13-121.  Richard Peng and Daniel A. Spielman. An efficient parallel solver for sdd linear systems.  In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC \u201914. ACM, 2014.  Daniel A. Spielman and Nikil Srivastava. Graph sparsification by effective resistances. SIAM J.  Comput., 40(6):1913-1926, 2011.  Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In Proceedings of the Thirty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201904, 2004.  Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM J. Comput.,  2011.  Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis and Applica- tions, 35(3):835-885, 2014. EFFICIENT GAUSSIAN SAMPLING VIA SPECTRAL SPARSIFICATION  "}, "Stochastic Block Model and Community Detection in Sparse Graphs: A spectral algorithm with optimal rate of recovery": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Stochastic Block Model and Community Detection in Sparse Graphs: A spectral algorithm with optimal rate of recovery", "abstract": "In this paper, we present  and analyze  a simple  and robust  spectral algorithm for the stochastic block model with k blocks, for any k fixed.  Our algorithm works with graphs having constant edge density, under an optimal condition on the gap between the density inside a block and the density between the blocks. As a co-product, we settle an open question posed by Abbe et. al. concerning  censor block models.", "pdf_url": "http://proceedings.mlr.press/v40/Chin15.pdf", "keywords": ["Stochastic Block Model", "Censor Block Model", "Spectral Algorithm"], "reference": "E. Abbe, A. S. Bandeira, and G. Hall. Exact Recovery in the Stochastic Block Model. ArXiv  e-prints, 1405.3267, May 2014.  Emmanuel Abbe, Afonso S. Bandeira, Annina Bracher, and Amit Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. CoRR, abs/1404.4749, 2014.  12   CHIN RAO VU  Figure 9: On the left is the density plot of the input (unclustered) matrix with parameters n = 3000, a = 22, b = 2 and on the right is the density plot of the permuted matrix after running the algorithm described above. This took less than 1sec in Matlab running on a 2009 MacPro.  Spectral Partition II.  1. Input the adjacency matrix Y, p.  2. Zero out all the rows and columns of Y corresponding to vertices whose degree is bigger than  20pn, to obtain the matrix Y0.  3. Find the eigenspace U corresponding to the top two eigenvalues of Y0.  6. Sort the vertices according to their values in v 2, and let V (cid:48)  1 \u2282 V be the top n vertices, and  4. Compute v 1, the projection of all-ones vector on to U  5. Let v 2 be the unit vector in W perpendicular to v 1.  V (cid:48) 2 \u2282 V be the remaining n vertices  7. Output (V (cid:48)  1, V (cid:48)  2).  Figure 10: Algorithm 3  References  E. Abbe, A. S. Bandeira, and G. Hall. Exact Recovery in the Stochastic Block Model. ArXiv  e-prints, 1405.3267, May 2014.  Emmanuel Abbe, Afonso S. Bandeira, Annina Bracher, and Amit Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. CoRR, abs/1404.4749, 2014. SPECTRAL ALGORITHM FOR SPARSE GRAPHS  N. Alon and J.H. Spencer. The Probabilistic Method. Wiley-Interscience series in discrete mathematics and optimization. Wiley, 2004. ISBN 9780471653981. URL http://books. google.com/books?id=q3lUjheWiMoC.  Noga Alon and Nabil Kahale. A spectral technique for coloring random 3-colorable graphs. In  SIAM Journal on Computing, pages 346-355, 1994.  Rajendra Bhatia. Matrix Analysis, volume 169. Springer, 1997. ISBN 0387948465.  B. Bollobas. Random Graphs. Cambridge University Press, 2001.  Ravi B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In Proceedings of the 28th Annual Symposium on Foundations of Computer Science, SFCS \u201987, pages 280-285, Washington, DC, USA, 1987. IEEE Computer Society. ISBN 0-8186-0807-2. doi: 10.1109/ SFCS.1987.22. URL http://dx.doi.org/10.1109/SFCS.1987.22.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities. A nonasymptotic theory of in- dependence, 2013. URL halv3-preprod.archives-ouvertes.fr/hal-00794821.  T.N. Bui, S. Chaudhuri, F.T. Leighton, and M. Sipser. Graph bisection algorithms with good av- ISSN 0209-9683. doi: 10.1007/  erage case behavior. Combinatorica, 7(2):171-191, 1987. BF02579448. URL http://dx.doi.org/10.1007/BF02579448.  Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability and Computing, 19:227-284, 3 2010. ISSN 1469-2163. doi: 10.1017/S0963548309990514. URL http://journals.cambridge.org/article_S0963548309990514.  Chandler Davis. The rotation of eigenvectors by a perturbation.  Journal of Mathematical Analysis and Applications, 6(2):159 - 173, 1963. ISSN 0022-247X. doi: http://dx.doi.org/ 10.1016/0022-247X(63)90001-5. URL http://www.sciencedirect.com/science/ article/pii/0022247X63900015.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106, Dec 2011. doi: 10.1103/PhysRevE.84.066106. URL http://link.aps.org/ doi/10.1103/PhysRevE.84.066106.  M. E. Dyer and A. M. Frieze. The solution of some random np-hard problems in polynomial expected time. J. Algorithms, 10(4):451-489, December 1989. ISSN 0196-6774. doi: 10. 1016/0196-6774(89)90001-1. URL http://dx.doi.org/10.1016/0196-6774(89) 90001-1.  Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs. Random Structures & Algorithms, 27(2):251-275, 2005. ISSN 1098-2418. doi: 10.1002/rsa.20089. URL http: //dx.doi.org/10.1002/rsa.20089.  J. Friedman, J. Kahn, and E. Szemer\u00b4edi. On the second eigenvalue of random regular graphs. In Proceedings of the Twenty-first Annual ACM Symposium on Theory of Computing, STOC \u201989, pages 587-598, New York, NY, USA, 1989. ACM. ISBN 0-89791-307-8. doi: 10.1145/73007. 73063. URL http://doi.acm.org/10.1145/73007.73063. CHIN RAO VU  O. Gu\u00b4edon and R. Vershynin. Community detection in sparse networks via Grothendieck\u2019s inequal-  ity. ArXiv e-prints, 1411.4686, November 2014.  Mark Jerrum and G.B. Sorkin. Simulated annealing for graph bisection. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on, pages 94-103, Nov 1993. doi: 10.1109/ SFCS.1993.366878.  M. Lelarge, L. Massoulie, and Jiaming Xu. Reconstruction in the labeled stochastic block model. In Information Theory Workshop (ITW), 2013 IEEE, pages 1-5, Sept 2013. doi: 10.1109/ITW. 2013.6691264.  Laurent Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. CoRR,  abs/1311.3085, 2013.  F. McSherry. Spectral partitioning of random graphs. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 529-537, Oct 2001. doi: 10.1109/SFCS.2001. 959929.  E. Mossel, J. Neeman, and A. Sly. Stochastic Block Models and Reconstruction. ArXiv e-prints,  1202.1499, February 2012.  E. Mossel, J. Neeman, and A. Sly. A Proof Of The Block Model Threshold Conjecture. ArXiv  e-prints, 1311.4115, November 2013a.  E. Mossel, J. Neeman, and A. Sly. Belief Propagation, Robust Reconstruction, and Optimal Recov-  ery of Block Models. ArXiv e-prints, 1309.1380, September 2013b.  E. Mossel, J. Neeman, and A. Sly. Consistency Thresholds for Binary Symmetric Block Models.  ArXiv e-prints, 1407.1591, July 2014.  JoelA. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa- tional Mathematics, 12(4):389-434, 2012. ISSN 1615-3375. doi: 10.1007/s10208-011-9099-z. URL http://dx.doi.org/10.1007/s10208-011-9099-z.  V. Vu. A simple SVD algorithm for finding hidden partitions. ArXiv e-prints, 1404.3918, April  2014.  Se-Young Yun and Alexandre Proutiere. Accurate community detection in the stochastic block model via spectral algorithms. CoRR, abs/1412.7335, 2014. URL http://arxiv.org/ abs/1412.7335.  Y. Zhang and H. Zhou. Minimax rates of community detection in stochastic block model.  "}, "On-Line Learning Algorithms for Path Experts with Non-Additive Losses": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On-Line Learning Algorithms for Path Experts with Non-Additive Losses", "abstract": "We consider two broad families of non-additive loss functions covering a large number of applications: rational losses and tropical losses. We give new algorithms extending the Follow-the-Perturbed-Leader (FPL) algorithm to both of these families of loss functions and similarly give new algorithms extending the Randomized Weighted Majority (RWM) algorithm to both of these families. We prove that the time complexity of our extensions to rational losses of both FPL and RWM is polynomial and present regret bounds for both.  We further show that these algorithms can play a critical role in improving performance in applications such as structured prediction.", "pdf_url": "http://proceedings.mlr.press/v40/Cortes15.pdf", "keywords": ["on-line learning", "experts", "non-additive losses", "structured prediction"], "reference": "2006.  of ICML, 2014.  C. Allauzen and M. Mohri. Efficient algorithms for testing the twins property. Journal of Automata,  Languages and Combinatorics, 8(2):117-144, 2003.  C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri. OpenFst: a general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11-23. Springer, 2007.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  C. Cortes, P. Haffner, and M. Mohri. Rational kernels: Theory and algorithms. JMLR, 5, 2004.  C. Cortes, V. Kuznetsov, and M. Mohri. Ensemble methods for structured prediction. In Proceedings  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and application  to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging structured concepts. In Proceedings of  N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computa-  M. Mohri. Finite-state transducers in language and speech processing. Computational Linguistics,  M. Mohri. Edit-distance of weighted automata: General definitions and algorithms. Int. J. Found.  Comput. Sci., 14(6):957-982, 2003.  M. Mohri. Weighted automata algorithms. In Handbook of Weighted Automata, pages 213-254.  COLT, pages 93-105, 2010.  tion, 108(2):212-261, 1994.  23(2):269-311, 1997.  Springer, 2009.  Springer, 2015.  M. Mohri and M. Riley. On the disambiguation of weighted automata. In Proceedings of CIAA.  M. Mohri, F. Pereira, and M. Riley. Weighted automata in text and speech processing. In Proceed-  ings of ECAI-96 Workshop on Extended finite state models of language, 1996.  M. Mohri, F. Pereira, and M. Riley. A rational design for a weighted finite-state transducer library.  In Proceedings of WIA, volume 1436 of LNCS, pages 144-158. Springer, 1998.  F. Pereira and M. Riley. Speech recognition by composition of weighted finite automata. In Finite-  State Language Processing, pages 431-453. MIT Press, 1997.  E. Takimoto and M. K. Warmuth. Path kernels and multiplicative updates. JMLR, 4:773-818, 2003.  T. van Erven, W. Kotlowski, and M. K. Warmuth. Follow the leader with dropout perturbations. In  COLT, 2014.  13   ON-LINE LEARNING ALGORITHMS FOR PATH EXPERTS WITH NON-ADDITIVE LOSSES  References  2006.  of ICML, 2014.  C. Allauzen and M. Mohri. Efficient algorithms for testing the twins property. Journal of Automata,  Languages and Combinatorics, 8(2):117-144, 2003.  C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri. OpenFst: a general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11-23. Springer, 2007.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  C. Cortes, P. Haffner, and M. Mohri. Rational kernels: Theory and algorithms. JMLR, 5, 2004.  C. Cortes, V. Kuznetsov, and M. Mohri. Ensemble methods for structured prediction. In Proceedings  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and application  to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging structured concepts. In Proceedings of  N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computa-  M. Mohri. Finite-state transducers in language and speech processing. Computational Linguistics,  M. Mohri. Edit-distance of weighted automata: General definitions and algorithms. Int. J. Found.  Comput. Sci., 14(6):957-982, 2003.  M. Mohri. Weighted automata algorithms. In Handbook of Weighted Automata, pages 213-254.  COLT, pages 93-105, 2010.  tion, 108(2):212-261, 1994.  23(2):269-311, 1997.  Springer, 2009.  Springer, 2015.  M. Mohri and M. Riley. On the disambiguation of weighted automata. In Proceedings of CIAA.  M. Mohri, F. Pereira, and M. Riley. Weighted automata in text and speech processing. In Proceed-  ings of ECAI-96 Workshop on Extended finite state models of language, 1996.  M. Mohri, F. Pereira, and M. Riley. A rational design for a weighted finite-state transducer library.  In Proceedings of WIA, volume 1436 of LNCS, pages 144-158. Springer, 1998.  F. Pereira and M. Riley. Speech recognition by composition of weighted finite automata. In Finite-  State Language Processing, pages 431-453. MIT Press, 1997.  E. Takimoto and M. K. Warmuth. Path kernels and multiplicative updates. JMLR, 4:773-818, 2003.  T. van Erven, W. Kotlowski, and M. K. Warmuth. Follow the leader with dropout perturbations. In  COLT, 2014. CORTES KUZNETSOV MOHRI WARMUTH  "}, "Truthful Linear Regression": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Truthful Linear Regression", "abstract": "We consider the problem of fitting a linear model to data held by individuals who are concerned about their privacy. Incentivizing most players to truthfully report their data to the analyst constrains our design to mechanisms that provide a privacy guarantee to the participants; we use differential privacy to model individuals\u2019 privacy losses. This immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation, and existing approaches to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators. We overcome this challenge through an appropriate design of the computation and payment scheme.", "pdf_url": "http://proceedings.mlr.press/v40/Cummings15.pdf", "keywords": ["privacy", "data privacy", "differential privacy", "linear regression", "mechanism design"], "reference": "Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization, revisited.  arXiv preprint 1405.7085, 2014.  J. Eric Bickel. Some comparisons among quadratic, spherical, and logarithmic scoring rules. Deci-  sion Analysis, 4(2):49-65, June 2007.  Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,  78(1), 1950.  Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation  with strategic data sources. arXiv preprint 1408.2539, 2014.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. J. Mach. Learn. Res., 12:1069-1109, July 2011.  Yiling Chen, Stephen Chong, Ian A. Kash, Tal Moran, and Salil Vadhan. Truthful mechanisms for agents that value privacy. In Proceedings of the 14th ACM Conference on Electronic Commerce, EC \u201913, pages 215-232, 2013.  Rachel Cummings, Katrina Ligett, Aaron Roth, Zhiwei Steven Wu, and Juba Ziani. Accuracy for sale: Aggregating data with a variance constraint. In Proceedings of the 6th Innovations in Theoretical Computer Science, ITCS \u201915, 2015.  Ofer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. Journal  of Computer and System Sciences, 76(8):759 - 777, 2010.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC \u201906, pages 265-284, 2006.  Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy. In Pro- ceedings of the IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 51-60, 2010.  Lisa K. Fleischer and Yu-Han Lyu. Approximately optimal auctions for selling privacy when costs are correlated with data. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 568-585, New York, NY, USA, 2012. ACM.  Arpita Ghosh and Aaron Roth. Selling privacy at auction. Games and Economic Behavior, 2013. Preliminary Version appeared un the Proceedings of the Twelfth ACM Conference on Electronic Commerce (EC 2011).  Arpita Ghosh, Katrina Ligett, Aaron Roth, and Grant Schoenebeck. Buying private data without verification. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 931-948, 2014.  Thibaut Horel, Stratis Ioannidis, and S. Muthukrishnan. Budget feasible mechanisms for experimen- tal design. In Alberto Pardo and Alfredo Viola, editors, LATIN 2014: Theoretical Informatics, Lecture Notes in Computer Science, pages 719-730. 2014.  13   TRUTHFUL LINEAR REGRESSION  References  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization, revisited.  arXiv preprint 1405.7085, 2014.  J. Eric Bickel. Some comparisons among quadratic, spherical, and logarithmic scoring rules. Deci-  sion Analysis, 4(2):49-65, June 2007.  Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,  78(1), 1950.  Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation  with strategic data sources. arXiv preprint 1408.2539, 2014.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. J. Mach. Learn. Res., 12:1069-1109, July 2011.  Yiling Chen, Stephen Chong, Ian A. Kash, Tal Moran, and Salil Vadhan. Truthful mechanisms for agents that value privacy. In Proceedings of the 14th ACM Conference on Electronic Commerce, EC \u201913, pages 215-232, 2013.  Rachel Cummings, Katrina Ligett, Aaron Roth, Zhiwei Steven Wu, and Juba Ziani. Accuracy for sale: Aggregating data with a variance constraint. In Proceedings of the 6th Innovations in Theoretical Computer Science, ITCS \u201915, 2015.  Ofer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. Journal  of Computer and System Sciences, 76(8):759 - 777, 2010.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC \u201906, pages 265-284, 2006.  Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy. In Pro- ceedings of the IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 51-60, 2010.  Lisa K. Fleischer and Yu-Han Lyu. Approximately optimal auctions for selling privacy when costs are correlated with data. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 568-585, New York, NY, USA, 2012. ACM.  Arpita Ghosh and Aaron Roth. Selling privacy at auction. Games and Economic Behavior, 2013. Preliminary Version appeared un the Proceedings of the Twelfth ACM Conference on Electronic Commerce (EC 2011).  Arpita Ghosh, Katrina Ligett, Aaron Roth, and Grant Schoenebeck. Buying private data without verification. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 931-948, 2014.  Thibaut Horel, Stratis Ioannidis, and S. Muthukrishnan. Budget feasible mechanisms for experimen- tal design. In Alberto Pardo and Alfredo Viola, editors, LATIN 2014: Theoretical Informatics, Lecture Notes in Computer Science, pages 719-730. 2014. CUMMINGS IOANNIDIS LIGETT  Justin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, and Zhiwei Steven Wu. Private matchings and allocations. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 21-30, 2014.  Stratis Ioannidis and Patrick Loiseau. Linear regression as a non-cooperative game. In Yiling Chen and Nicole Immorlica, editors, Web and Internet Economics, Lecture Notes in Computer Science, pages 277-290. 2013.  Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. Mechanism design in large games: Incentives and privacy. In Proceedings of the 5th Conference on Innovations in Theoretical Com- puter Science, ITCS \u201914, pages 403-410, 2014.  Donald Knuth. Seminumerical algorithms, volume 2, pages 130-131. Addison-Wesley Publishing  Company, 2 edition, 1981.  Katrina Ligett and Aaron Roth. Take it or leave it: Running a survey when privacy comes at a cost. In Proceedings of the 8th International Conference on Internet and Network Economics, WINE\u201912, pages 378-391, 2012.  Frank McSherry. Privacy integrated queries: an extensible platform for privacy-preserving data  analysis. In In Proceeding SIGMOD Conference, pages 19-30, 2009.  Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-  prediction method. Management Science, 51(9):1359-1373, Sept 2005.  Kobbi Nissim, Claudio Orlandi, and Rann Smorodinsky. Privacy-aware mechanism design.  In Proceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 774-789, 2012.  Kobbi Nissim, Salil Vadhan, and David Xiao. Is privacy compatible with truthfulness? In Proceed-  ings of the 4th Innovations in Theoretical Computer Science, ITCS \u201914, 2014. To appear.  Javier Perote and Juan Perote-Pena. Strategy-proof estimators for simple regression. In Mathemat-  ical Social Sciences 47, pages 153-176, 2004.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok, editors, Compressed Sensing, theory and applications, chapter 5, pages 210-268. Cambridge University Press, 2012.  "}, "A PTAS for Agnostically Learning Halfspaces": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "A PTAS for Agnostically Learning Halfspaces", "abstract": "We present a PTAS for agnostically learning halfspaces w.r.t. the uniform distribution on the d dimensional sphere. Namely, we show that for every \u03bc>0 there is an algorithm that runs in time \\mathrmpoly\\left(d,\\frac1\u03b5\\right), and is guaranteed to return a classifier with error at most (1+\u03bc)\\mathrmopt+\u03b5, where \\mathrmopt is the error of the best halfspace classifier. This improves on Awasthi, Balcan and Long (STOC 2014) who showed an algorithm with an (unspecified) constant approximation ratio. Our algorithm combines the classical technique of polynomial regression, together with the new localization technique of Awasthi et. al.", "pdf_url": "http://proceedings.mlr.press/v40/Daniely15.pdf", "keywords": ["Agnostic learning", "Uniform distribution", "Halfspaces", "Approximation algorithms", "Polynomial approximation", "Localization", "Polynomial regression"], "reference": "Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on, pages 724-733. IEEE, 1993.  Pranjal Awasthi, Maria-Florina Balcan, and Phil Long. The power of localization for efficiently  learning linear separators with noise. In STOC, 2014.  P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics,  33(4):1497-1537, 2005.  S. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error rate  using a surrogate convex loss. In ICML, 2012.  I. Ben-Eliezer, S. Lovett, and A. Yadin. Polynomial threshold functions: Structure, approximation  and pseudorandomness. Unpublished manuscript, 2009.  A. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-accuracy  tradeoffs. In NIPS, 2012.  Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf\u2019s.  In  Arxiv preprint arXiv:1404.3378 v1, 2014.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. In STOC, 2014a.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. The complexity of learning halfspaces using  generalized linear methods. In COLT, 2014b.  Philip J Davis. Interpolation and approximation. Courier Dover Publications, 1975.  Ilias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A Servedio, and Emanuele Viola. Bounded independence fools halfspaces. SIAM Journal on Computing, 39(8):3441-3462, 2010a.  Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Sym- posium on, pages 11-20. IEEE, 2010b.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, 2006.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proceedings of  the 47th Foundations of Computer Science (FOCS), 2006.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In Pro-  ceedings of the 46th Foundations of Computer Science (FOCS), 2005.  Michael Kearns and Ming Li. Learning in the presence of malicious errors. pages 267-280, May  1988. SIAM Journal on Computing.  12   DANIELY  References  Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on, pages 724-733. IEEE, 1993.  Pranjal Awasthi, Maria-Florina Balcan, and Phil Long. The power of localization for efficiently  learning linear separators with noise. In STOC, 2014.  P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics,  33(4):1497-1537, 2005.  S. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error rate  using a surrogate convex loss. In ICML, 2012.  I. Ben-Eliezer, S. Lovett, and A. Yadin. Polynomial threshold functions: Structure, approximation  and pseudorandomness. Unpublished manuscript, 2009.  A. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-accuracy  tradeoffs. In NIPS, 2012.  Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf\u2019s.  In  Arxiv preprint arXiv:1404.3378 v1, 2014.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. In STOC, 2014a.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. The complexity of learning halfspaces using  generalized linear methods. In COLT, 2014b.  Philip J Davis. Interpolation and approximation. Courier Dover Publications, 1975.  Ilias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A Servedio, and Emanuele Viola. Bounded independence fools halfspaces. SIAM Journal on Computing, 39(8):3441-3462, 2010a.  Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Sym- posium on, pages 11-20. IEEE, 2010b.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, 2006.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proceedings of  the 47th Foundations of Computer Science (FOCS), 2006.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In Pro-  ceedings of the 46th Foundations of Computer Science (FOCS), 2005.  Michael Kearns and Ming Li. Learning in the presence of malicious errors. pages 267-280, May  1988. SIAM Journal on Computing. A PTAS FOR AGNOSTICALLY LEARNING HALFSPACES  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning.  Machine Learning, 17:115-141, 1994.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In  RANDOM, 2014.  Adam R Klivans, Ryan O\u2019Donnell, and Rocco Servedio. Learning intersections and thresholds of In Foundations of Computer Science, 2002. Proceedings. The 43rd Annual IEEE  halfspaces. Symposium on, pages 177-186. IEEE, 2002.  A.R. Klivans, P.M. Long, and R.A. Servedio. Learning halfspaces with malicious noise. The Journal  of Machine Learning Research, 10:2715-2740, 2009.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier transform, and  learnability. In FOCS, pages 574-579, October 1989.  P.M. Long and R.A. Servedio. Learning large-margin halfspaces with more malicious noise. In  NIPS, 2011.  Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 52:55-66, 2010.  S. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the 0-1  loss. SIAM Journal on Computing, 40:1623-1646, 2011.  V. N. Vapnik. Statistical Learning Theory. Wiley, 1998. DANIELY  "}, "S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification", "abstract": "This paper investigates the problem of active learning for binary label prediction on a graph. We introduce a simple and label-efficient algorithm called S^2 for this task. At each step, S^2 selects the vertex to be labeled based on the structure of the graph and all previously gathered labels. Specifically, S^2 queries for the label of the vertex that bisects the \\em shortest shortest path between any pair of oppositely labeled vertices. We present a theoretical estimate of the number of queries S^2 needs in terms of a novel  parametrization of the complexity of binary functions on graphs. We also present experimental results demonstrating the performance of S^2 on both real and synthetic data. While other graph-based active learning algorithms have shown promise in practice, our algorithm is the first with both good performance and theoretical guarantees. Finally, we demonstrate the implications of the S^2 algorithm to the theory of nonparametric active learning. In particular, we show that S^2 achieves near minimax optimal excess risk for an important class of nonparametric classification problems.", "pdf_url": "http://proceedings.mlr.press/v40/Dasarathy15.pdf", "keywords": ["active learning on graphs", "query complexity of finding a cut", "nonparametric classification"], "reference": "Peyman Afshani, Ehsan Chiniforooshan, Reza Dorrigiv, Arash Farzan, Mehdi Mirzazadeh, Narges Simjour, and Hamid Zarrabi-Zadeh. On the complexity of finding an unknown cut via vertex queries. In Computing and Combinatorics, pages 459-469. Springer, 2007.  K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.  ics.uci.edu/ml.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Neural Information Processing Systems, 2011.  12   DASARATHY NOWAK ZHU  6. Experiments  We performed some preliminary experiments on the following data sets: (a) Digits: This dataset is from the Cedar Buffalo binary digits database originally Hull (1994). We preprocessed the dig- its by reducing the size of each image down to a 16x16 grid with down-sampling and Gaussian smoothing Le Cun et al. (1990). Each image is thus a 256-dimensional vector with elements being gray-scale pixel values in 0-255. We considered two separate binary classification tasks on this data set: 1 vs 2 and 4 vs 9. Intuitively one might expect the former task to be much simpler than the latter. For each task, we randomly chose 200 digits in the positive class and 200 in the negative. We computed the Euclidean distance between these 400 digits based on their feature vectors. We then constructed a symmetrized 10-nearest-neighbor graph, with an edge between images i, j iff i is among j\u2019s 10 nearest neighbors or vice versa. Each task is thus represented by a graph with exactly 400 nodes and about 3000 undirected unweighted edges. Nonetheless, due to the intrinsic confusability, the cut size and the boundary (i.e., edges connecting the two classes) varies drastically across the tasks: 1 vs 2 has a boundary of 92, while 4 vs 9 has a boundary of 290. (b) Congressional Voting Records (CVR): This is the congressional voting records data set from the UCI machine learning repository (Bache and Lichman, 2013). We created a graph out of this by thresholding (at 0.5) the Euclidean distance between the data points. This was then processed to retain the largest connected component which had 380 vertices and a boundary size of 234. (c) Grid: This is a syn- thetic example of a 15x15 grid of vertices with a positive core in the center. The core was generated from a square by randomly dithering its boundary. See Figure 3.  We compared the performance of four algorithms: (a) S2 (b) AFS - the active learning algo- rithm from Afshani et al. (2007); (c) ZLG - the algorithm from Zhu et al. (2003b); and (d) BND - the experiment design-like algorithm from Gu and Han (2012). We show the number of queries needed before all nodes in \u2202C have been queried. This number, which we call \u2202C-query com- plexity, is by definition no smaller than |\u2202C|. Notice that before completely querying \u2202C, it is impossible for any algorithm to guarantee zero error without prior assumptions. Thus we posit that \u2202C-query complexity is a sensible measure for the setting considered in this paper. In fact \u2202C-query complexity can be thought of as the experimental analogue of the theoretical query complexity of Section 4. These results are shown in Table 1. The bold figures show the best performance in each experiment. As can be seen, S2 clearly outperforms AFS and BOUND as suggested by our theory. It is quite surprising to see how well ZLG performs given that it was not designed with this objective in mind. We believe that trying to understanding this will be a fruitful avenue for future work.  References  Peyman Afshani, Ehsan Chiniforooshan, Reza Dorrigiv, Arash Farzan, Mehdi Mirzazadeh, Narges Simjour, and Hamid Zarrabi-Zadeh. On the complexity of finding an unknown cut via vertex queries. In Computing and Combinatorics, pages 459-469. Springer, 2007.  K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.  ics.uci.edu/ml.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Neural Information Processing Systems, 2011. S2 : ACTIVE LEARNING ON GRAPHS  Christopher M Bishop et al. Pattern recognition and machine learning, volume 4. springer New  York, 2006.  Avrim Blum and Shuchi Chawla. Learning from Labeled and Unlabeled Data using Graph Mincuts.  In International Conference on Machine Learning, pages 19-26, 2001.  R. Castro and R. Nowak. Minimax bounds for active learning. IEEE Transactions on Information  Theory, pages 2339-2353, 2008.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, pages 493-507, 1952.  Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active Clustering: Robust and Efficient Hierarchical Clustering using Adaptively Selected Similarities. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Quanquan Gu and Jiawei Han. Towards active learning on graphs: An error bound minimization  approach. In ICDM, pages 882-887, 2012.  S. Hanneke. Rates of convergence in active learning. The Annals of Statistics, pages 333-361, 2011.  Jonathan J. Hull. A database for handwritten text recognition research.  IEEE Transactions on  Pattern Analysis and Machine Intelligence, 16(5), 1994.  V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. Journal  of Machine Learning Research, pages 2457-2485, 2010.  Y. Le Cun, B. Boser, J. S Denker, D. Henderson, R. E.. Howard, W. Howard, and L. D. Jackel. Hand- written digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems, 2, 1990.  S. Minsker. Plug-in approach to active learning. Journal of Machine Learning Research, pages  67-90, 2012.  Clayton Scott and Robert D Nowak. Minimax-optimal classification with dyadic decision trees.  Information Theory, IEEE Transactions on, 52(4):1335-1353, 2006.  L. Wang. Smoothness, disagreement coefficient, and the label complexity of agnostic active learn-  ing. Journal of Machine Learning Research, pages 2269-2292, 2011.  Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. In International Conference on Machine Learning, pages 912- 919, 2003a.  Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. Combining active learning and semi- supervised learning using Gaussian fields and harmonic functions. In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, 2003b. DASARATHY NOWAK ZHU  Acknowledgments  RN is supported in part by the National Science Foundation grant CCF1218189 and the National Institutes of Health grant 1 U54 AI117924-01; XZ is supported in part by National Science Foun- dation grants IIS 0916038, IIS 0953219, IIS 1216758, and the National Institutes of Health grant 1 U54 AI117924-01.  "}, "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems", "abstract": "Given a large data matrix A\u2208\\mathbbR^n\\times n, we consider the problem of determining whether its entries are i.i.d. from some known marginal distribution A_ij\u223cP_0, or instead A contains a principal submatrix A_\\sf Q,\\sf Q whose entries have marginal distribution A_ij\u223cP_1\u2260P_0. As a special case, the hidden (or planted) clique problem is finding a planted clique in an otherwise uniformly random graph. Assuming unbounded computational resources, this hypothesis testing problem is statistically solvable  provided |\\sf Q|\\ge C \\log n for a suitable constant C. However, despite substantial effort, no polynomial time algorithm is known that succeeds with high probability when |\\sf Q| = o(\\sqrtn).  Recently, \\citemeka2013association proposed a method to establish lower bounds for the hidden clique problem within the Sum of Squares (SOS) semidefinite hierarchy. Here we consider the degree-4 SOS relaxation, and study the construction of \\citemeka2013association to prove that SOS fails unless k\\ge C\\,n^1/3/\\log n. An argument presented by \\citeBarakLectureNotes implies that this lower bound cannot be substantially  improved unless  the witness construction is changed in the proof. Our proof uses the moment method to bound the spectrum of a certain random association scheme, i.e. a symmetric random matrix whose rows and columns are indexed by the edges of an Erd\u00f6s-Renyi random graph.", "pdf_url": "http://proceedings.mlr.press/v40/Deshpande15.pdf", "keywords": [], "reference": "Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a random graph. In Proceedings of the ninth annual ACM-SIAM symposium on Discrete algorithms, pages 594-598. Society for Industrial and Applied Mathematics, 1998.  Brendan P.W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Boaz Barak. Sums of Squares upper bounds, lower bounds, and open questions (Lecture notes, Fall  2014). http://www.boazbarak.org/sos/, 2014.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  arXiv:1404.5236, 2014.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In Conference on Learning Theory, pages 1046-1066, 2013.  T Tony Cai, Tengyuan Liang, and Alexander Rakhlin. Computational and statistical boundaries for  submatrix localization in a large noisy matrix. arXiv:1502.01988, 2015.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. arXiv:1402.1267, 2014.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high  probability. In ANALCO, pages 67-75. SIAM, 2011.  12   DESHPANDE MONTANARI  Using these observations and Eq. (36) we obtain that (cid:107)P2QP2(cid:107) (cid:46) \u03ba4n, while for any other pair (a, b) \u2208 {0, 1, 2}2 we have that (cid:107)PaQPb(cid:107) (cid:46) \u03ba4n3/2. As noted before, since \u03bb0 \u2248 n2\u03ba4, \u03bb1 \u2248 n\u03ba3 and \u03bb1 \u2248 \u03ba2 whence the condition in Eq. (35) reduces to:  \uf8eb  \uf8ed  n2\u03ba4 0 0  0 n\u03ba3 0  0 0 \u03ba2  \uf8f6 \uf8f8 \u2212 \u03ba4  \uf8eb  \uf8ed  n3/2 n3/2 n3/2 n3/2 n3/2 n3/2 n3/2 n3/2 n  \uf8f6 \uf8f8 (cid:60) 0.  The 2, 2 entry of this matrix inequality yields that \u03ba2 \u2212 \u03ba4n (cid:29) 0 or \u03ba (cid:28) n\u22121/2. Considering the (1, 1) entry yields a similar condition. The key condition is that corresponding to the minor indexed by rows (and columns) 1, 2:  (41)  (42)  (cid:18) n\u03ba3  \u2212n3/2\u03ba4  (cid:19)  \u2212n3/2\u03ba4 \u03ba2  (cid:60) 0.  This requires that n\u03ba5 (cid:29) n3\u03ba8 or, equivalently \u03ba (cid:28) n\u22122/3. Translating this to clique size k = n\u03ba, we obtain the condition k (cid:28) n1/3. This calculation thus demonstrates the origin of the threshold of n1/3 beyond which the Meka-Wigderson witness fails to be positive semidefinite. The coun- terexample of Barak (2014) shows that our estimates are fairly tight (indeed, up to a logarithmic factor).  References  Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a random graph. In Proceedings of the ninth annual ACM-SIAM symposium on Discrete algorithms, pages 594-598. Society for Industrial and Applied Mathematics, 1998.  Brendan P.W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Boaz Barak. Sums of Squares upper bounds, lower bounds, and open questions (Lecture notes, Fall  2014). http://www.boazbarak.org/sos/, 2014.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  arXiv:1404.5236, 2014.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In Conference on Learning Theory, pages 1046-1066, 2013.  T Tony Cai, Tengyuan Liang, and Alexander Rakhlin. Computational and statistical boundaries for  submatrix localization in a large noisy matrix. arXiv:1502.01988, 2015.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. arXiv:1402.1267, 2014.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high  probability. In ANALCO, pages 67-75. SIAM, 2011. SUM-OF-SQUARES FOR HIDDEN CLIQUE AND HIDDEN SUBMATRIX  Yash Deshpande and Andrea Montanari. Finding hidden cliques of size (cid:112)  N/e in nearly linear time. Foundations of Computational Mathematics, pages 1-60, 2014. ISSN 1615-3375. doi: 10.1007/s10208-014-9215-y.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Structures and Algorithms, 16(2):195-208, 2000.  Uriel Feige and Dorit Ron. Finding hidden cliques in linear time. DMTCS Proceedings, (01):  189-204, 2010.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo-  rithms and a lower bound for planted clique. arXiv:1201.1214, 2012.  Zolt\u00b4an F\u00a8uredi and J\u00b4anos Koml\u00b4os. The eigenvalues of random symmetric matrices. Combinatorica,  1(3):233-241, 1981.  Geoffrey R Grimmett and Colin JH McDiarmid. On colouring random graphs. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 77, pages 313-324. Cambridge Univ Press, 1975.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Computational lower bounds for community detection  on random graphs. arXiv preprint arXiv:1406.6625, 2014.  Johan Hastad. Clique is hard to approximate within n1\u2212(cid:15). In Foundations of Computer Science,  1996. Proceedings., 37th Annual Symposium on, pages 627-636. IEEE, 1996.  Mark Jerrum. Large cliques elude the Metropolis process. Random Structures & Algorithms, 3(4):  347-359, 1992.  Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis  in high dimensions. Journal of the American Statistical Association, 104(486), 2009.  Richard M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W. Thatcher,  editors, Complexity of Computer Computations. Plenum, 1972.  Subhash Khot. Improved inapproximability results for maxclique, chromatic number and approxi- mate graph coloring. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Sym- posium on, pages 600-609. IEEE, 2001.  Jean B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal  on Optimization, 11(3):796-817, 2001.  Zongming Ma and Yihong Wu.  Computational barriers in minimax submatrix detection.  arXiv:1309.5914, 2013.  Raghu Meka and Avi Wigderson. Association schemes, non-commutative polynomial concentra- tion, and sum-of-squares lower bounds for planted clique. In Electronic Colloquium on Compu- tational Complexity (ECCC), volume 20, page 105, 2013.  Raghu Meka, Aaron Potechin, and Avi Wigderson. Sum-of-squares lower bounds for the planted  clique problem. In ACM Symposium on Theory of Computing, 2015. DESHPANDE MONTANARI  Samet Oymak, Amin Jalali, Maryam Fazel, Yonina C Eldar, and Babak Hassibi. Simultaneously structured models with application to sparse and low-rank matrices. arXiv:1212.3753, 2012.  Pablo A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical  programming, 96(2):293-320, 2003.  Jean-Pierre Serre. Linear representations of finite groups. Graduate Texts in Mathematics, 42, 1977.  Andrey A Shabalin, Victor J Weigman, Charles M Perou, and Andrew B Nobel. Finding large average submatrices in high dimensional data. The Annals of Applied Statistics, pages 985-1012, 2009.  NZ Shor. Class of global minimum bounds of polynomial functions. Cybernetics and Systems  Analysis, 23(6):731-734, 1987.  Madhur Tulsiani. CSP gaps and reductions in the Lasserre hierarchy. In Proceedings of the forty-  first annual ACM symposium on Theory of computing, pages 303-312. ACM, 2009.  "}, "Contextual Dueling Bandits": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Contextual Dueling Bandits", "abstract": "We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (COLT\u201909), which we extend to incorporate context. Roughly, the learner\u2019s goal is to find the best policy, or way of behaving, in some space of policies, although \u201cbest\u201d is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a \\emphvon Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three \\emphefficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space.", "pdf_url": "http://proceedings.mlr.press/v40/Dudik15.pdf", "keywords": ["contextual dueling bandits", "online learning", "bandit algorithms", "game theory"], "reference": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- In Proceedings of the  ing the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning (ICML), 2014.  Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In Proceedings of the International Conference on Machine Learning (ICML), pages 856-864, 2014.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Computing, 32(1):48-77, 2002.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit  algorithms with supervised learning guarantees. J. Mach. Learn. Res., 15:19-26, 2011.  R\u00b4obert Busa-Fekete and Eyke H\u00a8ullermeier. A survey of preference-based online learning with  bandit algorithms. In Algorithmic Learning Theory (ALT), pages 18-39, 2014.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. Large-scale validation and analysis of interleaved search evaluation. ACM Transactions on Information Systems (TOIS), 30 (1):6:1-6:41, 2012.  Miroslav Dud\u00b4\u0131k, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Uncertainty in Artificial Intelligence (UAI), pages 169-178, 2011.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Martin Gardner. Mathematical games: The paradox of the nontransitive dice and the elusive princi-  ple of indifference. Scientific American, 223:110-114, 1970.  Fan Guo, Lei Li, and Christos Faloutsos. Tailoring click models to user goals. In Workshop on Web  Search Click Data (WSCD), pages 88-92, 2009a.  Fan Guo, Chao Liu, and Yi-Min Wang. Efficient multiple-click models in web search. In Proceed- ings of the International Conference on Web Search and Data Mining (WSDM), pages 124-131, 2009b.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. A probabilistic method for inferring In Proceedings of the International Conference on Information and  preferences from clicks. Knowledge Management (CIKM), pages 249-258, 2011.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. ACM Transactions on Information Systems (TOIS), 31(4), 2013.  13   CONTEXTUAL DUELING BANDITS  References  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- In Proceedings of the  ing the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning (ICML), 2014.  Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In Proceedings of the International Conference on Machine Learning (ICML), pages 856-864, 2014.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Computing, 32(1):48-77, 2002.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit  algorithms with supervised learning guarantees. J. Mach. Learn. Res., 15:19-26, 2011.  R\u00b4obert Busa-Fekete and Eyke H\u00a8ullermeier. A survey of preference-based online learning with  bandit algorithms. In Algorithmic Learning Theory (ALT), pages 18-39, 2014.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. Large-scale validation and analysis of interleaved search evaluation. ACM Transactions on Information Systems (TOIS), 30 (1):6:1-6:41, 2012.  Miroslav Dud\u00b4\u0131k, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Uncertainty in Artificial Intelligence (UAI), pages 169-178, 2011.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Martin Gardner. Mathematical games: The paradox of the nontransitive dice and the elusive princi-  ple of indifference. Scientific American, 223:110-114, 1970.  Fan Guo, Lei Li, and Christos Faloutsos. Tailoring click models to user goals. In Workshop on Web  Search Click Data (WSCD), pages 88-92, 2009a.  Fan Guo, Chao Liu, and Yi-Min Wang. Efficient multiple-click models in web search. In Proceed- ings of the International Conference on Web Search and Data Mining (WSDM), pages 124-131, 2009b.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. A probabilistic method for inferring In Proceedings of the International Conference on Information and  preferences from clicks. Knowledge Management (CIKM), pages 249-258, 2011.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. ACM Transactions on Information Systems (TOIS), 31(4), 2013. DUD\u00b4IK HOFMANN SCHAPIRE SLIVKINS ZOGHI  Adam Kalai and Santosh Vempala. Efficient algorithms for the online decision problem. In Confer-  ence on Learning Theory (COLT), pages 26-40, 2003.  John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Annual Conference on Neural Information Processing Systems (NIPS), pages 817-824, 2007.  Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons.  In Annual Conference on Neural Information Processing Systems (NIPS), 2012.  Guillermo Owen. Game Theory. Emerald Group Publishing Limited, 3rd edition, 1995.  Filip Radlinski, Madhu Kurup, and Thorsten Joachims. How does clickthrough data re\ufb02ect re- trieval quality? In Proceedings of the International Conference on Information and Knowledge Management (CIKM), pages 43-52, 2008.  Markus Schulze. A new monotonic, clone-independent, reversal symmetric, and Condorcet- consistent single-winner election method. Social Choice and Welfare, 36(2):267-303, 2011.  Tanguy Urvoy, Fabrice Clerot, Raphael F\u00b4eraud, and Sami Naamane. Generic exploration and k- In Proceedings of the International Conference on Machine Learning  armed voting bandits. (ICML), pages 91-99, 2013.  Yisong Yue and Thorsten Joachims.  Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the International Conference on Machine Learning (ICML), pages 1201-1208, 2009.  Yisong Yue and Thorsten Joachims. Beat the mean bandit.  In Proceedings of the International  Conference on Machine Learning (ICML), pages 241-248, 2011.  Yisong Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. In  Conference on Learning Theory (COLT), 2009.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the International Conference on Machine Learning (ICML), pages 928-936, 2003.  Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R\u00b4emi Munos. Relative confidence sam- pling for efficient on-line ranker evaluation. In Proceedings of the International Conference on Web Search and Data Mining (WSDM), pages 73-82, 2014a.  Masrour Zoghi, Shimon Whiteson, R\u00b4emi Munos, and Maarten de Rijke. Relative upper confidence bound for the k-armed dueling bandits problem. In Proceedings of the International Conference on Machine Learning (ICML), pages 10-18, 2014b.  Masrour Zoghi, Zohar Karnin, Shimon Whiteson, and Maarten de Rijke. Copeland dueling bandits.  2015a. arxiv:1506.00312.  Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale In Proceedings of the International Conference on Web Search and  online ranker evaluation. Data Mining (WSDM), pages 17-26, 2015b. CONTEXTUAL DUELING BANDITS  "}, "Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering", "abstract": "Hierarchical clustering is a popular method for analyzing data which associates a tree to a dataset. Hartigan consistency has been used extensively as a framework to analyze such clustering algorithms from a statistical point of view. Still, as we show in the paper, a tree which is Hartigan consistent with a given density can look very different than the correct limit tree. Specifically, Hartigan consistency permits two types of undesirable configurations which we term \\emphover-segmentation and \\emphimproper nesting.  Moreover, Hartigan consistency is a limit property and does not directly quantify difference between trees. In this paper we identify two limit properties, \\emphseparation and \\emphminimality, which address both over-segmentation and improper nesting and together imply (but are not implied by) Hartigan consistency. We proceed to introduce a \\emphmerge distortion metric between hierarchical clusterings and show that convergence in our distance implies both separation and minimality. We also prove that uniform separation and minimality imply convergence in the merge distortion metric.  Furthermore, we show that our merge distortion metric is stable under perturbations of the density. Finally, we demonstrate applicability of these concepts by proving convergence results for two clustering algorithms. First, we show convergence (and hence separation and minimality) of the recent robust single linkage algorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence results on manifolds for  topological  split tree clustering.", "pdf_url": "http://proceedings.mlr.press/v40/Eldridge15.pdf", "keywords": ["hierarchical clustering", "Hartigan consistency", "metric distortion"], "reference": "Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry Wasser- man. Cluster Trees on Manifolds. In Advances in Neural Information Processing Systems, pages 2679-2687, 2013.  Micka\u00a8el Buchet, Fr\u00b4ed\u00b4eric Chazal, Tamal K. Dey, Fengtao Fan, Steve Y. Oudot, and Yusu Wang.  Topological analysis of scalar fields with outliers. CoRR, arXiv:1412.1680, 2014.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society, 2001.  Gunnar Carlsson and Facundo M\u00b4emoli. Characterization, stability and convergence of hierarchical  clustering methods. The Journal of Machine Learning Research, 11:1425-1470, 2010.  Hamish Carr, Jack Snoeyink, and Ulrike Axen. Computing contour trees in all dimensions. Comput.  Geom, 24(2):75-94, 2003.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In Advances  in Neural Information Processing Systems, pages 343-351, 2010.  Kamalika Chaudhuri, Sanjoy Dasgupta, Samory Kpotufe, and Ulrike von Luxburg. Consistent procedures for cluster tree estimation and pruning. IEEE Transactions on Information Theory, 60(12):7900-7912, 2014.  Fr\u00b4ed\u00b4eric Chazal, Leonidas J. Guibas, Steve Y. Oudot, and Primoz Skraba. Persistence-based clus-  tering in riemannian manifolds. J. ACM, 60(6):41, 2013.  T. K. Dey, J. Sun, and Y. Wang. Approximating cycles in a shortest basis of the first homology  group from point data. Inverse Problems, 27(12):124004, 2011.  J. A. Hartigan. Consistency of Single Linkage for High-Density Clusters. Journal of the American Statistical Association, 76(374):388-394, June 1981. ISSN 0162-1459. doi: 10.1080/01621459. 1981.10477658.  John A. Hartigan. Clustering Algorithms. John Wiley & Sons, Inc., New York, NY, USA, 99th  edition, 1975. ISBN 047135645X.  Anil K Jain and Richard C Dubes. Algorithms for clustering data, volume 6. Prentice hall Engle-  wood Cliffs, 1988.  systems, pages 463-470, 2003.  Jon Kleinberg. An impossibility theorem for clustering. Advances in neural information processing  Samory Kpotufe and Ulrike V. Luxburg. Pruning nearest neighbor cluster trees. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 225-232, New York, NY, USA, 2011. ACM.  Werner Stuetzle and Rebecca Nugent. A Generalized Single Linkage Method for Estimating the Cluster Tree of a Density. Journal of Computational and Graphical Statistics, 19(2):397-418, January 2010. ISSN 1061-8600. doi: 10.1198/jcgs.2009.07049.  13   CLUSTER TREE DISTANCE  References  Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry Wasser- man. Cluster Trees on Manifolds. In Advances in Neural Information Processing Systems, pages 2679-2687, 2013.  Micka\u00a8el Buchet, Fr\u00b4ed\u00b4eric Chazal, Tamal K. Dey, Fengtao Fan, Steve Y. Oudot, and Yusu Wang.  Topological analysis of scalar fields with outliers. CoRR, arXiv:1412.1680, 2014.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society, 2001.  Gunnar Carlsson and Facundo M\u00b4emoli. Characterization, stability and convergence of hierarchical  clustering methods. The Journal of Machine Learning Research, 11:1425-1470, 2010.  Hamish Carr, Jack Snoeyink, and Ulrike Axen. Computing contour trees in all dimensions. Comput.  Geom, 24(2):75-94, 2003.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In Advances  in Neural Information Processing Systems, pages 343-351, 2010.  Kamalika Chaudhuri, Sanjoy Dasgupta, Samory Kpotufe, and Ulrike von Luxburg. Consistent procedures for cluster tree estimation and pruning. IEEE Transactions on Information Theory, 60(12):7900-7912, 2014.  Fr\u00b4ed\u00b4eric Chazal, Leonidas J. Guibas, Steve Y. Oudot, and Primoz Skraba. Persistence-based clus-  tering in riemannian manifolds. J. ACM, 60(6):41, 2013.  T. K. Dey, J. Sun, and Y. Wang. Approximating cycles in a shortest basis of the first homology  group from point data. Inverse Problems, 27(12):124004, 2011.  J. A. Hartigan. Consistency of Single Linkage for High-Density Clusters. Journal of the American Statistical Association, 76(374):388-394, June 1981. ISSN 0162-1459. doi: 10.1080/01621459. 1981.10477658.  John A. Hartigan. Clustering Algorithms. John Wiley & Sons, Inc., New York, NY, USA, 99th  edition, 1975. ISBN 047135645X.  Anil K Jain and Richard C Dubes. Algorithms for clustering data, volume 6. Prentice hall Engle-  wood Cliffs, 1988.  systems, pages 463-470, 2003.  Jon Kleinberg. An impossibility theorem for clustering. Advances in neural information processing  Samory Kpotufe and Ulrike V. Luxburg. Pruning nearest neighbor cluster trees. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 225-232, New York, NY, USA, 2011. ACM.  Werner Stuetzle and Rebecca Nugent. A Generalized Single Linkage Method for Estimating the Cluster Tree of a Density. Journal of Computational and Graphical Statistics, 19(2):397-418, January 2010. ISSN 1061-8600. doi: 10.1198/jcgs.2009.07049. ELDRIDGE BELKIN WANG  Suyi Wang, Yusu Wang, and Rephael Wenger. The js-graphs of join and split trees. In Proc. 29th  Annu. ACM Sympos. Comput. Geom., page 539, 2014.  David Wishart. Mode analysis: A generalization of nearest neighbor which reduces chaining effects.  Numerical taxonomy, 76(282-311):17, 1969.  "}, "Faster Algorithms for Testing under Conditional Sampling": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Faster Algorithms for Testing under Conditional Sampling", "abstract": "There has been considerable recent interest in distribution-tests whose run-time and sample requirements are sublinear in the domain-size k. We study two of the most important tests under the conditional-sampling model where each query specifies a subset S of the domain, and the response is a sample drawn from S according to the underlying distribution. For identity testing, which asks whether the underlying distribution equals a specific given distribution or \u03b5-differs from it, we reduce the known time and sample complexities from \\widetilde\\mathcalO(\u03b5^-4) to \\widetilde\\mathcalO(\u03b5^-2), thereby matching the information theoretic lower bound. For closeness testing, which asks whether two distributions underlying observed data sets are equal or different, we reduce existing complexity from \\widetilde\\mathcalO(\u03b5^-4 \\log^5 k) to an even sub-logarithmic \\widetilde\\mathcalO(\u03b5^-5 \\log \\log k) thus providing a better bound to an open problem in Bertinoro Workshop on Sublinear Algorithms (Fisher, 2014).", "pdf_url": "http://proceedings.mlr.press/v40/Falahatgar15.pdf", "keywords": ["Property testing", "conditional sampling", "sublinear algorithms"], "reference": "Jayadev Acharya and Constantinos Daskalakis. Testing poisson binomial distributions. In Proceed- ings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1829-1840, 2015.  Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan, and In Proceedings of  Ananda Theertha Suresh. Competitive classification and closeness testing. the 25th Annual Conference on Learning Theory (COLT), pages 22.1-22.18, 2012.  Jayadev Acharya, Cl\u00b4ement L. Canonne, and Gautam Kamath. A chasm between identity and equiv-  alence testing with conditional queries. CoRR, abs/1411.7346, 2014a.  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitksy, and Ananda Theertha Suresh. Sublinear algo- rithms for outlier detection and generalized closeness testing. In Proceedings of the 2014 IEEE International Symposium on Information Theory (ISIT), 2014b.  Maria-Florina Balcan, Eric Blais, Avrim Blum, and Liu Yang. Active property testing.  In 53rd Annual IEEE Symposium on Foundations of Computer Science, FOCS 2012, New Brunswick, NJ, USA, October 20-23, 2012, pages 21-30, 2012.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing that In Annual Symposium on Foundations of Computer Science (FOCS),  distributions are close. pages 259-269, 2000.  Tugkan Batu, Lance Fortnow, Eldar Fischer, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In 42nd Annual Symposium on Foun- dations of Computer Science, FOCS 2001, 14-17 October 2001, Las Vegas, Nevada, USA, pages 442-451, 2001.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the 36th Annual ACM Symposium on Theory of Com- puting, Chicago, IL, USA, June 13-16, 2004, pages 381-390, 2004.  Cl\u00b4ement L. Canonne, Dana Ron, and Rocco A. Servedio. Testing equivalence between distributions using conditional samples. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 1174-1192, 2014.  Sourav Chakraborty, Eldar Fischer, Yonatan Goldhirsh, and Arie Matsliah. On the power of condi- tional samples in distribution testing. In Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013, pages 561-580, 2013.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 604-613, 2014a.  Siu On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing  closeness of discrete distributions. In Symposium on Discrete Algorithms (SODA), 2014b.  13   FASTER ALGORITHMS FOR TESTING UNDER CONDITIONAL SAMPLING  References  Jayadev Acharya and Constantinos Daskalakis. Testing poisson binomial distributions. In Proceed- ings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1829-1840, 2015.  Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan, and In Proceedings of  Ananda Theertha Suresh. Competitive classification and closeness testing. the 25th Annual Conference on Learning Theory (COLT), pages 22.1-22.18, 2012.  Jayadev Acharya, Cl\u00b4ement L. Canonne, and Gautam Kamath. A chasm between identity and equiv-  alence testing with conditional queries. CoRR, abs/1411.7346, 2014a.  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitksy, and Ananda Theertha Suresh. Sublinear algo- rithms for outlier detection and generalized closeness testing. In Proceedings of the 2014 IEEE International Symposium on Information Theory (ISIT), 2014b.  Maria-Florina Balcan, Eric Blais, Avrim Blum, and Liu Yang. Active property testing.  In 53rd Annual IEEE Symposium on Foundations of Computer Science, FOCS 2012, New Brunswick, NJ, USA, October 20-23, 2012, pages 21-30, 2012.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing that In Annual Symposium on Foundations of Computer Science (FOCS),  distributions are close. pages 259-269, 2000.  Tugkan Batu, Lance Fortnow, Eldar Fischer, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In 42nd Annual Symposium on Foun- dations of Computer Science, FOCS 2001, 14-17 October 2001, Las Vegas, Nevada, USA, pages 442-451, 2001.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the 36th Annual ACM Symposium on Theory of Com- puting, Chicago, IL, USA, June 13-16, 2004, pages 381-390, 2004.  Cl\u00b4ement L. Canonne, Dana Ron, and Rocco A. Servedio. Testing equivalence between distributions using conditional samples. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 1174-1192, 2014.  Sourav Chakraborty, Eldar Fischer, Yonatan Goldhirsh, and Arie Matsliah. On the power of condi- tional samples in distribution testing. In Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013, pages 561-580, 2013.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 604-613, 2014a.  Siu On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing  closeness of discrete distributions. In Symposium on Discrete Algorithms (SODA), 2014b. FALAHATGAR JAFARPOUR ORLITSKY PICHAPATI SURESH  Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, and Paul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of the Twenty- Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013, pages 1833-1852, 2013.  Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Testing identity of structured distribu- tions. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1841-1854, 2015.  Eldar Fisher. Distinguishing distributions with conditional samples. Bertinoro 2014, 2014. URL  http://sublinear.info/66.  Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. Electronic Collo-  quium on Computational Complexity (ECCC), 7(20), 2000.  Sudipto Guha, Andrew McGregor, and Suresh Venkatasubramanian. Sublinear estimation of en-  tropy and information distances. ACM Transactions on Algorithms, 5(4), 2009.  Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distributions.  Theory of Computing, 9:295-347, 2013.  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data.  IEEE Transactions on Information Theory, 54(10):4750-4755, 2008.  Ronitt Rubinfeld and Rocco A. Servedio. Testing monotone high-dimensional distributions. Ran-  dom Struct. Algorithms, 34(1):24-44, 2009.  Jayakrishnan Unnikrishnan. On optimal two sample homogeneity tests for finite alphabets.  In Proceedings of the 2012 IEEE International Symposium on Information Theory (ISIT), pages 2027-2031, 2012.  Gregory Valiant and Paul Valiant. Instance-by-instance optimal identity testing. Electronic Collo-  quium on Computational Complexity (ECCC), 20:111, 2013.  Paul Valiant. Testing symmetric properties of distributions. SIAM Journal on Computing, 40(6):  1927-1968, December 2011. ISSN 0097-5397.  Bo Waggoner. Lp testing and learning of discrete distributions. In Proceedings of the 2015 Con- ference on Innovations in Theoretical Computer Science, ITCS 2015, Rehovot, Israel, January 11-13, 2015, pages 347-356, 2015.  Jacob Ziv. On classification with empirically observed statistics and universal data compression.  IEEE Transactions on Information Theory, 34(2):278-286, 1988.  "}, "Learning and inference in the presence of corrupted inputs": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Learning and inference in the presence of corrupted inputs", "abstract": "We consider a model where given an uncorrupted input an adversary can corrupt it to one out of m corrupted inputs. We model the classification and inference problems as a zero-sum game between a learner, minimizing the expected error, and an adversary, maximizing the expected error. The value of this game is the optimal error rate achievable. For learning using a limited hypothesis class \\mathcalH over corrupted inputs, we give an efficient algorithm that given an uncorrupted sample returns a hypothesis h\u2208\\mathcalH whose error on adversarially corrupted inputs is near optimal. Our algorithm uses as a blackbox an oracle that solves the ERM problem for the hypothesis class \\mathcalH. We provide a generalization bound for our setting, showing that for a sufficiently large sample, the performance on the sample and future unseen corrupted inputs will be similar. This gives an efficient learning algorithm for our adversarial setting, based on an ERM oracle. We also consider an inference related setting of the problem, where given a corrupted input, the learner queries the  target function on various uncorrupted inputs and generates a prediction regarding the given corrupted input. There is no limitation on the prediction function the learner may generate, so implicitly the hypothesis class includes all possible hypotheses. In this setting we characterize the optimal learner policy as a minimum vertex cover in a given bipartite graph, and the optimal adversary policy as a maximum matching in the same bipartite graph. We design efficient local algorithms for approximating minimum vertex cover in bipartite graphs, which implies an efficient near optimal algorithm for the learner.", "pdf_url": "http://proceedings.mlr.press/v40/Feige15.pdf", "keywords": [], "reference": "1988.  Noga Alon, Ronitt Rubinfeld, Shai Vardi, and Ning Xie. Space-efficient local computation algo-  rithms. In SODA, pages 1132-1139, 2012.  Dana Angluin and Philip Laird. Learning from noisy examples. Mach. Learn., 2(4):343-370, April  A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization. Princeton Series in Applied  Mathematics. Princeton University Press, October 2009.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2/3):321-352, 2007. Preliminary version in COLT 2005.  Guy Even, Moti Medina, and Dana Ron. Best of two local models: Local centralized and local dis- tributed algorithms. CoRR, abs/1402.3796, 2014. URL http://arxiv.org/abs/1402. 3796.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999. Preliminary version in COLT 1996.  Oded Goldreich and Dana Ron. Property testing in bounded degree graphs. Algorithmica, 32(2):  302-343, 2002. Preliminary version in 29th STOC, 1997.  John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings in bipartite  graphs. SIAM J. Comput., 2(4):225-231, 1973.  Michael J. Kearns and Ming Li. Learning in the presence of malicious errors. SIAM J. Comput., 22  (4):807-837, 1993.  Yishay Mansour and Shai Vardi. A local computation approximation scheme to maximum matching.  In APPROX-RANDOM, pages 260-273, 2013.  Yishay Mansour, Aviad Rubinstein, Shai Vardi, and Ning Xie. Converting online algorithms to local  computation algorithms. In ICALP (1), pages 653-664, 2012.  Yishay Mansour, Aviad Rubinstein, and Moshe Tennenholtz. Robust probabilistic inference. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 449-460, 2015.  Ronitt Rubinfeld, Gil Tamir, Shai Vardi, and Ning Xie. Fast local computation algorithms. In ICS,  pages 223-238, 2011.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.  ISBN 0262017180, 9780262017183.  L. G. Valiant. Learning disjunction of conjunctions. In Proceedings of the 9th International Joint  Conference on Artificial Intelligence - Volume 1, IJCAI\u201985, pages 560-566, 1985.  13   LEARNING AND INFERENCE IN THE PRESENCE OF CORRUPTED INPUTS  References  1988.  Noga Alon, Ronitt Rubinfeld, Shai Vardi, and Ning Xie. Space-efficient local computation algo-  rithms. In SODA, pages 1132-1139, 2012.  Dana Angluin and Philip Laird. Learning from noisy examples. Mach. Learn., 2(4):343-370, April  A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization. Princeton Series in Applied  Mathematics. Princeton University Press, October 2009.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2/3):321-352, 2007. Preliminary version in COLT 2005.  Guy Even, Moti Medina, and Dana Ron. Best of two local models: Local centralized and local dis- tributed algorithms. CoRR, abs/1402.3796, 2014. URL http://arxiv.org/abs/1402. 3796.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999. Preliminary version in COLT 1996.  Oded Goldreich and Dana Ron. Property testing in bounded degree graphs. Algorithmica, 32(2):  302-343, 2002. Preliminary version in 29th STOC, 1997.  John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings in bipartite  graphs. SIAM J. Comput., 2(4):225-231, 1973.  Michael J. Kearns and Ming Li. Learning in the presence of malicious errors. SIAM J. Comput., 22  (4):807-837, 1993.  Yishay Mansour and Shai Vardi. A local computation approximation scheme to maximum matching.  In APPROX-RANDOM, pages 260-273, 2013.  Yishay Mansour, Aviad Rubinstein, Shai Vardi, and Ning Xie. Converting online algorithms to local  computation algorithms. In ICALP (1), pages 653-664, 2012.  Yishay Mansour, Aviad Rubinstein, and Moshe Tennenholtz. Robust probabilistic inference. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 449-460, 2015.  Ronitt Rubinfeld, Gil Tamir, Shai Vardi, and Ning Xie. Fast local computation algorithms. In ICS,  pages 223-238, 2011.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.  ISBN 0262017180, 9780262017183.  L. G. Valiant. Learning disjunction of conjunctions. In Proceedings of the 9th International Joint  Conference on Artificial Intelligence - Volume 1, IJCAI\u201985, pages 560-566, 1985. FEIGE MANSOUR SCHAPIRE  Acknowledgements  UF: Work partly done in Microsoft Research, Hertzelia. Work supported in part by the Israel Sci- ence Foundation (grant No. 621/12) and by the I-CORE Program of the Planning and Budgeting Committee and The Israel Science Foundation (grant No. 4/11). YM: This research was supported in part by The Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation, by a grant from United States-Israel Binational Science Foundation (BSF).  "}, "From Averaging to Acceleration, There is Only a Step-size": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "From Averaging to Acceleration, There is Only a Step-size", "abstract": "We show that accelerated gradient descent, averaged gradient descent and the heavy-ball method for quadratic non-strongly-convex problems may be reformulated as   constant parameter second-order difference equation algorithms, where stability of the system is equivalent to convergence at rate O(1/n^2), where n is the number of iterations. We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system, showing various oscillatory and non-oscillatory behaviors, together with a sharp stability result with explicit constants. We also consider the situation where noisy gradients are available, where we extend our general convergence result, which suggests an alternative algorithm (i.e., with different step sizes) that exhibits the good aspects of both averaging and acceleration.", "pdf_url": "http://proceedings.mlr.press/v40/Flammarion15.pdf", "keywords": ["Convex optimization", "acceleration", "averaging", "stochastic gradient"], "reference": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information  12  012345\u22124\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA FLAMMARION BACH  Figure 3: Quadratic optimization with regression noise. Left \u03c3 = 1, r = 1. Right \u03c3 = 0.1, r = 10.  Figure 4: Least-Square Regression. Left \u03c3 = 1, r = 1. Right \u03c3 = 0.1, r = 10.  6. Conclusion  We have provided a joint analysis of averaging and acceleration for non-strongly-convex quadratic functions in a single framework, both with noiseless and noisy gradients. This allows us to define a class of algorithms that can benefit simultaneously from the known improvements of averaging and acceleration: faster forgetting of initial conditions (for acceleration), and better robustness to noise when the noise covariance is proportional to the Hessian (for averaging).  Our current analysis of our class of algorithms in Eq. (5), that considers two different affine combinations of previous iterates (instead of one for traditional acceleration), is limited to quadratic functions; an extension of its analysis to all smooth or self-concordant-like functions would widen its applicability. Similarly, an extension to least-squares regression with natural heteroscedastic stochastic gradient, as suggested by our simulations, would be an interesting development.  This work was partially supported by the MSR-Inria Joint Centre and a grant by the European Research Council (SIERRA project 239993). The authors would like to thank Aymeric Dieuleveut for helpful discussions.  Acknowledgments  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information012345\u22124\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA FROM AVERAGING TO ACCELERATION  Theory, 58(5):3235-3249, 2012.  1998.  L. Arnold. Random dynamical systems. Springer Monographs in Mathematics. Springer-Verlag,  F. Bach and E. Moulines. Non-Asymptotic Analysis of Stochastic Approximation Algorithms for  Machine Learning. In Advances in Neural Information Processing Systems, 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems, December 2013.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  A. d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM J. Optim., 19(3):1171-  1183, 2008.  A. D\u00b4efossez and F. Bach. Constant step size least-mean-square: Bias-variance trade-offs and opti-  mal sampling distributions. Technical Report 1412.0156, arXiv, 2014.  O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with  inexact oracle. Math. Program., 146(1-2, Ser. A):37-75, 2014.  A. Dieuleveut and F. Bach. Non-parametric Stochastic Approximation with Large Step sizes. Tech-  nical Report 1408.0361, arXiv, August 2014.  C. Hu, W. Pan, and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online  learning. In Advances in Neural Information Processing Systems, 2009.  G. Lan. An optimal method for stochastic composite optimization. Math. Program., 133(1-2, Ser.  A):365-397, 2012.  Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).  Soviet Mathematics Doklady, 27(2):372-376, 1983.  Y. Nesterov. Introductory Lectures on Convex Optimization, volume 87 of Applied Optimization.  Kluwer Academic Publishers, Boston, MA, 2004. A basic course.  Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser.  B):125-161, 2013.  B. O\u2019Donoghue and E. Cand`es. Adaptive restart for accelerated gradient schemes. Foundations of  Computational Mathematics, pages 1-18, 2013.  J. M. Ortega and W. C. Rheinboldt. Iterative solution of nonlinear equations in several variables, volume 30 of Classics in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2000.  B. T. Polyak. Some methods of speeding up the convergence of iteration methods. {USSR} Com-  putational Mathematics and Mathematical Physics, 4(5):1-17, 1964. FLAMMARION BACH  B. T. Polyak. Introduction to Optimization. Translations Series in Mathematics and Engineering.  Optimization Software, Inc., Publications Division, New York, 1987.  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J.  Control Optim., 30(4):838-855, 1992.  M. Schmidt, N. Le Roux, and F. Bach. Convergence Rates of Inexact Proximal-Gradient Methods In Advances in Neural Information Processing Systems, December  for Convex Optimization. 2011.  W. Su, S. Boyd, and E. Cand`es. A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient Method: Theory and Insights. In Advances in Neural Information Processing Systems, 2014.  A. B. Tsybakov. Optimal rates of aggregation. In Proceedings of the Annual Conference on Com-  putational Learning Theory, 2003.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J.  Mach. Learn. Res., 11:2543-2596, 2010.  "}, "Variable Selection is Hard": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Variable Selection is Hard", "abstract": "Variable selection for sparse linear regression is the problem of finding, given an m\\times p  matrix B and a target vector \\bfy,  a sparse vector \\bfx such that B\\bfx approximately equals \\bfy. Assuming a standard complexity hypothesis, we show that no polynomial-time algorithm can find a k\u2019-sparse \\bfx with \\|B\\bfx-\\bfy\\|^2\\le h(m,p), where k\u2019=k\u22c52^\\log ^1-\u03b4 p and h(m,p)= p^C_1 m^1-C_2, where \u03b4>0,C_1>0,C_2>0 are arbitrary. This is true even under the promise that there is an unknown k-sparse vector \\bfx^* satisfying B\\bfx^*=\\bfy. We prove a similar result for a statistical version of the problem in which the data are corrupted by noise. To the authors\u2019 knowledge, these are the first hardness results for sparse regression that apply when the algorithm simultaneously has k\u2019>k and h(m,p)>0.", "pdf_url": "http://proceedings.mlr.press/v40/Foster15.pdf", "keywords": [], "reference": "Edoardo Amaldi and Viggo Kann. On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. Theoretical Computer Science, 209(12):237 - 260, 1998. ISSN 0304-3975. doi: http://dx.doi.org/10.1016/S0304-3975(97)00115-1. URL http://www.sciencedirect. com/science/article/pii/S0304397597001151.  Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z. Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. J. Comput. Syst. Sci., 54(2):317-331, 1997. doi: 10. 1006/jcss.1997.1472. URL http://dx.doi.org/10.1006/jcss.1997.1472.  Mihir Bellare, Shafi Goldwasser, Carsten Lund, and Alexander Russell. Efficient probabilistic checkable proofs and applications to approximation. In Frank Thomson Leighton and Michael T. Goodrich, editors, Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr\u00b4eal, Qu\u00b4ebec, Canada, page 820. ACM, 1994. ISBN 0-89791-663-8. doi: 10.1145/195058.195467. URL http://doi.acm.org/10.1145/195058.195467.  Thomas Blumensath and Mike E. Davies. On the difference between orthogonal matching pursuit and orthogonal least squares. http://eprints.soton.ac.uk/142469/1/BDOMPvsOLS07.pdf, 2007.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis pursuit. SIAM Rev., 43(1):129-159, January 2001. ISSN 0036-1445. doi: 10.1137/S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  Ali C\u00b8 ivril. A note on the hardness of sparse approximation. Inf. Process. Lett., 113(14-16):543-545, 2013. doi: 10.1016/j.ipl.2013.04.014. URL http://dx.doi.org/10.1016/j.ipl.2013.04.014.  G. Davis, S. Mallat, and M. Avellaneda. Adaptive greedy approximations. Constructive Approximation, 13(1):57-98, 1997. ISSN 0176-4276. doi: 10.1007/BF02678430. URL http://dx.doi.org/10. 1007/BF02678430.  Irit Dinur and David Steurer. Analytical approach to parallel repetition.  In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 624-633. ACM, 2014. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591884. URL http://doi. acm.org/10.1145/2591796.2591884.  Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634-652, 1998. doi: 10.1145/  285055.285059. URL http://doi.acm.org/10.1145/285055.285059.  Dean P. Foster and Edward I. George. The risk in\ufb02ation criterion for multiple regression. The Annals of Statistics, 22(4):1947-1975, 1994. doi: 10.1214/aos/1176325766. URL http://dx.doi.org/10. 1214/aos/1176325766.  Dean P. Foster, Howard J. Karloff, and Justin Thaler. Variable selection is hard. CoRR, abs/1412.4832,  2014. URL http://arxiv.org/abs/1412.4832.  Carsten Lund and Mihalis Yannakakis. On the hardness of approximating minimization problems. J. ACM, 41(5):960-981, 1994. doi: 10.1145/185675.306789. URL http://doi.acm.org/10.1145/ 185675.306789.  13   VARIABLE SELECTION IS HARD  References  Edoardo Amaldi and Viggo Kann. On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. Theoretical Computer Science, 209(12):237 - 260, 1998. ISSN 0304-3975. doi: http://dx.doi.org/10.1016/S0304-3975(97)00115-1. URL http://www.sciencedirect. com/science/article/pii/S0304397597001151.  Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z. Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. J. Comput. Syst. Sci., 54(2):317-331, 1997. doi: 10. 1006/jcss.1997.1472. URL http://dx.doi.org/10.1006/jcss.1997.1472.  Mihir Bellare, Shafi Goldwasser, Carsten Lund, and Alexander Russell. Efficient probabilistic checkable proofs and applications to approximation. In Frank Thomson Leighton and Michael T. Goodrich, editors, Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr\u00b4eal, Qu\u00b4ebec, Canada, page 820. ACM, 1994. ISBN 0-89791-663-8. doi: 10.1145/195058.195467. URL http://doi.acm.org/10.1145/195058.195467.  Thomas Blumensath and Mike E. Davies. On the difference between orthogonal matching pursuit and orthogonal least squares. http://eprints.soton.ac.uk/142469/1/BDOMPvsOLS07.pdf, 2007.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis pursuit. SIAM Rev., 43(1):129-159, January 2001. ISSN 0036-1445. doi: 10.1137/S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  Ali C\u00b8 ivril. A note on the hardness of sparse approximation. Inf. Process. Lett., 113(14-16):543-545, 2013. doi: 10.1016/j.ipl.2013.04.014. URL http://dx.doi.org/10.1016/j.ipl.2013.04.014.  G. Davis, S. Mallat, and M. Avellaneda. Adaptive greedy approximations. Constructive Approximation, 13(1):57-98, 1997. ISSN 0176-4276. doi: 10.1007/BF02678430. URL http://dx.doi.org/10. 1007/BF02678430.  Irit Dinur and David Steurer. Analytical approach to parallel repetition.  In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 624-633. ACM, 2014. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591884. URL http://doi. acm.org/10.1145/2591796.2591884.  Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634-652, 1998. doi: 10.1145/  285055.285059. URL http://doi.acm.org/10.1145/285055.285059.  Dean P. Foster and Edward I. George. The risk in\ufb02ation criterion for multiple regression. The Annals of Statistics, 22(4):1947-1975, 1994. doi: 10.1214/aos/1176325766. URL http://dx.doi.org/10. 1214/aos/1176325766.  Dean P. Foster, Howard J. Karloff, and Justin Thaler. Variable selection is hard. CoRR, abs/1412.4832,  2014. URL http://arxiv.org/abs/1412.4832.  Carsten Lund and Mihalis Yannakakis. On the hardness of approximating minimization problems. J. ACM, 41(5):960-981, 1994. doi: 10.1145/185675.306789. URL http://doi.acm.org/10.1145/ 185675.306789. FOSTER KARLOFF THALER  S. J. Montgomery-Smith. The distribution of rademacher sums. Proceedings of the American Math- ematical Society, 109(2):517-522, 06 1990. URL http://www.ams.org/journals/proc/ 1990-109-02/S0002-9939-1990-1013975-0/S0002-9939-1990-1013975-0.pdf.  Dana Moshkovitz. The projection games conjecture and the np-hardness of ln n-approximating set- In Anupam Gupta, Klaus Jansen, Jos\u00b4e D. P. Rolim, and Rocco A. Servedio, editors, Approx- cover. imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th Interna- tional Workshop, APPROX 2012, and 16th International Workshop, RANDOM 2012, Cambridge, MA, USA, August 15-17, 2012. Proceedings, volume 7408 of Lecture Notes in Computer Science, pages 276-287. Springer, 2012. doi: 10.1007/978-3-642-32512-0 24. URL http://dx.doi.org/10.1007/978-3-642-32512-0_24.  ISBN 978-3-642-32511-3.  B. K. Natarajan. 227-234, 1995. S0097539792240406.  SIAM J. Comput., 24(2): Sparse approximate solutions to linear systems. doi: 10.1137/S0097539792240406. URL http://dx.doi.org/10.1137/  Sara A. van de Geer and Peter Bhlmann. On the conditions used to prove oracle results for the lasso. Electron. J. Statist., 3:1360-1392, 2009. doi: 10.1214/09-EJS506. URL http://dx.doi.org/10. 1214/09-EJS506.  Yuchen Zhang, Martin J. Wainwright, and Michael I. Jordan. Lower bounds on the performance of polynomial-time algorithms for sparse linear regression. In Maria-Florina Balcan and Csaba Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13- 15, 2014. Also see arXiv:1402.1918, last revised May 21, 2014., volume 35 of JMLR Proceedings, pages 921-948. JMLR.org, 2014. URL http://jmlr.org/proceedings/papers/v35/zhang14. html. "}, "Vector-Valued Property Elicitation": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Vector-Valued Property Elicitation", "abstract": "The elicitation of a statistic, or property of a distribution, is the task of devising proper scoring rules, equivalently proper losses, which incentivize an agent or algorithm to truthfully estimate the desired property of the underlying probability distribution or data set.  Leveraging connections between elicitation and convex analysis, we address the vector-valued property case, which has received little attention in the literature despite its applications to both machine learning and statistics. We first provide a very general characterization of linear and ratio-of-linear properties, the first of which resolves an open problem by unifying and strengthening several previous characterizations in machine learning and statistics.  We then ask which vectors of properties admit nonseparable scores, which cannot be expressed as a sum of scores for each coordinate separately, a natural desideratum for machine learning.  We show that linear and ratio-of-linear do admit nonseparable scores, and provide evidence for a conjecture that these are the only such properties (up to link functions). Finally, we give a general method for producing identification functions and address an open problem by showing that convex maximal level sets are insufficient for elicitability in general.", "pdf_url": "http://proceedings.mlr.press/v40/Frongillo15.pdf", "keywords": ["Elicitation", "property", "proper loss", "scoring rule", "identification function", "expectile"], "reference": "J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the  25th Conference on Learning Theory, 2012.  A. Archer and R. Kleinberg. Truthful germs are contagious: a local to global characterization of truthfulness.  In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 21-30, 2008.  A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664-2669, July 2005. ISSN 0018-9448. doi: 10.1109/ TIT.2005.850145.  R. De Maesschalck, D. Jouan-Rimbaud, and D. L. Massart. The Mahalanobis distance. Chemometrics and In- telligent Laboratory Systems, 50(1):1-18, January 2000. ISSN 0169-7439. doi: 10.1016/S0169-7439(99) 00047-7.  Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osband\u2019s principle. arXiv:1503.08123  [math, q-fin, stat], March 2015. arXiv: 1503.08123.  Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis.  In Web and  Internet Economics, pages 354-370. Springer, 2014.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106  (494):746-762, 2011.  T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the Ameri-  can Statistical Association, 102(477):359-378, 2007.  Tilmann Gneiting and Matthias Katzfuss. Probabilistic forecasting. Annual Review of Statistics and Its  Application, 1:125-151, 2014.  Kyrill Grant and Tilmann Gneiting. Consistent scoring functions for quantiles.  In From Probability to Statistics and Back: High-Dimensional Models and ProcessesA Festschrift in Honor of Jon A. Wellner, pages 163-173. Institute of Mathematical Statistics, 2013.  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings  of the 9th ACM Conference on Electronic Commerce, pages 129-138, 2008.  Christian Lonard. A Set of Lecture Notes on Convex Optimization with some Applications to Probability  Theory. Technical Report, 2006.  P. C. Mahalanobis. On the generalised distance in statistics. Proc. Nat. Inst. Sci. India, A, 2, 49, 55, 1936.  Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica:  Journal of the Econometric Society, pages 819-847, 1987.  Kent Osband. Optimal Forecasting Incentives. Journal of Political Economy, 97(5):1091-1112, October  1989. ISSN 0022-3808. doi: 10.2307/1831887.  Kent Osband and Stefan Reichelstein. Information-eliciting compensation schemes. Journal of Public Eco-  nomics, 27(1):107-115, June 1985. ISSN 0047-2727. doi: 10.1016/0047-2727(85)90031-3.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley,  1985.  13   VECTOR-VALUED PROPERTY ELICITATION  References  J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the  25th Conference on Learning Theory, 2012.  A. Archer and R. Kleinberg. Truthful germs are contagious: a local to global characterization of truthfulness.  In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 21-30, 2008.  A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664-2669, July 2005. ISSN 0018-9448. doi: 10.1109/ TIT.2005.850145.  R. De Maesschalck, D. Jouan-Rimbaud, and D. L. Massart. The Mahalanobis distance. Chemometrics and In- telligent Laboratory Systems, 50(1):1-18, January 2000. ISSN 0169-7439. doi: 10.1016/S0169-7439(99) 00047-7.  Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osband\u2019s principle. arXiv:1503.08123  [math, q-fin, stat], March 2015. arXiv: 1503.08123.  Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis.  In Web and  Internet Economics, pages 354-370. Springer, 2014.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106  (494):746-762, 2011.  T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the Ameri-  can Statistical Association, 102(477):359-378, 2007.  Tilmann Gneiting and Matthias Katzfuss. Probabilistic forecasting. Annual Review of Statistics and Its  Application, 1:125-151, 2014.  Kyrill Grant and Tilmann Gneiting. Consistent scoring functions for quantiles.  In From Probability to Statistics and Back: High-Dimensional Models and ProcessesA Festschrift in Honor of Jon A. Wellner, pages 163-173. Institute of Mathematical Statistics, 2013.  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings  of the 9th ACM Conference on Electronic Commerce, pages 129-138, 2008.  Christian Lonard. A Set of Lecture Notes on Convex Optimization with some Applications to Probability  Theory. Technical Report, 2006.  P. C. Mahalanobis. On the generalised distance in statistics. Proc. Nat. Inst. Sci. India, A, 2, 49, 55, 1936.  Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica:  Journal of the Econometric Society, pages 819-847, 1987.  Kent Osband. Optimal Forecasting Incentives. Journal of Political Economy, 97(5):1091-1112, October  1989. ISSN 0022-3808. doi: 10.2307/1831887.  Kent Osband and Stefan Reichelstein. Information-eliciting compensation schemes. Journal of Public Eco-  nomics, 27(1):107-115, June 1985. ISSN 0047-2727. doi: 10.1016/0047-2727(85)90031-3.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley,  1985. FRONGILLO KASH  Andrew J. Patton. Volatility forecast comparison using imperfect volatility proxies. Journal of Econometrics,  R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational Analysis. Springer, October 2011.  ISBN  R.T. Rockafellar. Convex analysis, volume 28 of Princeton Mathematics Series. Princeton University Press,  160(1):246-256, 2011.  9783540627722.  1997.  & Sons, 2005.  Peter J. Rousseeuw and Annick M. Leroy. Robust regression and outlier detection, volume 589. John Wiley  Marco Saerens. Building cost functions minimizing to some summary statistics. Neural Networks, IEEE  Transactions on, 11(6):1263-1271, 2000.  L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical Asso-  ciation, pages 783-801, 1971.  1856-1879, 1989.  M. J. Schervish. A general method for comparing probability assessors. The Annals of Statistics, 17(4):  Ingo Steinwart, Robert Williamson, Siyu Zhang, and others. Elicitation and Identification of Properties. In  Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014.  C. Zlinescu. Convex Analysis in General Vector Spaces. World Scientific Publishing Company, Incorporated,  January 2002. ISBN 9789812380678. VECTOR-VALUED PROPERTY ELICITATION  "}, "Competing with the Empirical Risk Minimizer in a Single Pass": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Competing with the Empirical Risk Minimizer in a Single Pass", "abstract": "In many estimation problems, e.g. linear and logistic regression, we wish to minimize an unknown objective given only unbiased samples of the objective function. Furthermore, we aim to achieve this using as few samples as possible.  In the absence of computational constraints, the minimizer of a sample average of observed data\u00a0\u2013 commonly referred to as either the empirical risk minimizer (ERM) or the M-estimator\u00a0\u2013 is widely regarded as the estimation strategy of choice due to its desirable statistical convergence properties. Our goal in this work is to perform as well as the ERM, on \\emphevery problem, while minimizing the use of computational resources such as running time and space usage. We provide a simple streaming algorithm which, under standard regularity assumptions on the underlying problem, enjoys the following properties: \\beginenumerate \\item The algorithm can be implemented in linear time with a single pass of the observed data, using space linear in the size of a single sample. \\item The algorithm achieves the same statistical rate of convergence as the empirical risk minimizer on every problem, even considering constant factors. \\item The algorithm\u2019s performance depends on the initial error at a rate that decreases super-polynomially. \\item The algorithm is easily parallelizable. \\endenumerate Moreover, we quantify the (finite-sample) rate at which the algorithm becomes competitive with the ERM.", "pdf_url": "http://proceedings.mlr.press/v40/Frostig15.pdf", "keywords": [], "reference": "A. Agarwal and L. Bottou. A lower bound for the optimization of finite sums. Technical report,  arXiv, 2014. URL http://arxiv.org/abs/1410.0723.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, May 2012.  D. Anbar. On Optimal Estimation Methods Using Stochastic Approximation Procedures. University of California, 1971. URL http://books.google.com/books?id=MmpHJwAACAAJ.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-  414, 2010.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for  machine learning. In Neural Information Processing Systems (NIPS), 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Neural Information Processing Systems (NIPS), 2013.  L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Neural Information Processing  Systems (NIPS), volume 20, pages 161-168, 2008.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Founda-  tions of Computational Mathematics, 7(3):331-368, 2007.  A. Defossez and F. Bach. Constant step size least-mean-square: Bias-variance trade-offs and opti- mal sampling distributions. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.  A. Dieuleveut and F. Bach. Non-parametric stochastic approximation with large step sizes. Techni-  cal report, arXiv, 2014. URL http://arxiv.org/abs/1408.0361.  V. Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics, 1  (3), 1973.  E. Hazan and S. Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014. URL http://jmlr.org/papers/v15/hazan14a.html.  13   COMPETING WITH THE EMPIRICAL RISK MINIMIZER IN A SINGLE PASS  Acknowledgments  We thank the COLT reviewers for their detailed reviews and helpful comments, and we thank Jonathan Kelner, Yin Tat Lee, and Boaz Barak for helpful discussion. Part of this work was done while AS was visiting the Simons Institute for the Theory of Computing, UC Berkeley. This work was partially supported by NSF awards 0843915 and 1111109, NSF Graduate Research Fellowship (grant no. 1122374).  References  A. Agarwal and L. Bottou. A lower bound for the optimization of finite sums. Technical report,  arXiv, 2014. URL http://arxiv.org/abs/1410.0723.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, May 2012.  D. Anbar. On Optimal Estimation Methods Using Stochastic Approximation Procedures. University of California, 1971. URL http://books.google.com/books?id=MmpHJwAACAAJ.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-  414, 2010.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for  machine learning. In Neural Information Processing Systems (NIPS), 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Neural Information Processing Systems (NIPS), 2013.  L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Neural Information Processing  Systems (NIPS), volume 20, pages 161-168, 2008.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Founda-  tions of Computational Mathematics, 7(3):331-368, 2007.  A. Defossez and F. Bach. Constant step size least-mean-square: Bias-variance trade-offs and opti- mal sampling distributions. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.  A. Dieuleveut and F. Bach. Non-parametric stochastic approximation with large step sizes. Techni-  cal report, arXiv, 2014. URL http://arxiv.org/abs/1408.0361.  V. Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics, 1  (3), 1973.  E. Hazan and S. Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014. URL http://jmlr.org/papers/v15/hazan14a.html. FROSTIG GE KAKADE SIDFORD  D. Hsu, S. M. Kakade, and T. Zhang. A tail inequality for quadratic forms of subgaussian random  vectors. Technical report, arXiv, 2011. URL http://arxiv.org/abs/1110.2842.  D. Hsu, S. M. Kakade, and T. Zhang. Tail inequalities for sums of random matrices that depend on  the intrinsic dimension. Electronic Communications in Probability, 17(14):1-13, 2012.  D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. Foundations of  Computational Mathematics, 14(3):569-600, June 2014.  R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-  tion. In Neural Information Processing Systems (NIPS), 2013.  A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex  functions, 2010. URL https://hal.archives-ouvertes.fr/hal-00508933.  H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Uncon-  strained Systems. Springer-Verlag, 1978.  H. J. Kushner and G. Yin. Stochastic Approximation and Recursive Algorithms and Applications. Applications of mathematics. Springer, 2003. ISBN 9780387008943. URL http://books. google.com/books?id=_0bIieuUJGkC.  N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential conver-  gence rate for finite training sets. In Neural Information Processing Systems (NIPS), 2012.  E. L. Lehmann and G. Casella. Theory of Point Estimation. Springer Texts in Statistics. Springer,  1998. ISBN 9780387985022.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  A. S. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization.  Wiley (Chichester and New York), 1983.  Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Applied Optimization.  Springer, 2004. ISBN 9781402075537.  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM  Journal on Control and Optimization, 30(4):838-855, July 1992. ISSN 0363-0129.  A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex  stochastic optimization. In International Conference on Machine Learning, (ICML), 2012.  H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-  tics, 22(3):400-407, Sept 1951.  D. Ruppert. Efficient Estimations from a Slowly Convergent Robbins-Monro Process. Technical report, ORIE, Cornell University, 1988. URL http://hdl.handle.net/1813/8664.  S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss.  Journal of Machine Learning Research (JMLR), 14(1):567-599, February 2013. COMPETING WITH THE EMPIRICAL RISK MINIMIZER IN A SINGLE PASS  S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform  convergence. Journal of Machine Learning Research, 11:2635-2670, December 2010.  O. Shamir. The sample complexity of learning linear predictors with the squared loss. Technical  report, arXiv, 2014. URL http://arxiv.org/abs/1406.5143.  T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential convergence.  Journal of Fourier Analysis and Applications, 15:262-278, 2009.  Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716- 5735, 2014.  A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Math-  ematics. Cambridge University Press, 2000. ISBN 9780521784504.  Yuan Yao. On complexity issues of online learning algorithms. IEEE Transactions on Information  Theory, 56(12), 2010.  Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations  of Computational Mathematics, 8(5):561-596, 2008.  "}, "A Chaining Algorithm for Online Nonparametric Regression": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "A Chaining Algorithm for Online Nonparametric Regression", "abstract": "We consider the problem of online nonparametric regression with arbitrary deterministic sequences. Using ideas from the chaining technique, we design an algorithm that achieves a Dudley-type regret bound similar to the one obtained in a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound is expressed in terms of the metric entropy in the sup norm, which yields optimal guarantees when the metric and sequential entropies are of the same order of magnitude. In particular our algorithm is the first one that achieves optimal rates for online regression over H\u00f6lder balls. In addition we show for this example how to adapt our chaining algorithm to get a reasonable computational efficiency with similar regret guarantees (up to a log factor).", "pdf_url": "http://proceedings.mlr.press/v40/Gaillard15.pdf", "keywords": ["online learning", "nonparametric regression", "chaining", "individual sequences"], "reference": "1999.  2006.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of  independence. Oxford University Press, 2013.  N. Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. J. Comput.  System Sci., 59(3):392\u2013411, 1999.  N. Cesa-Bianchi and G. Lugosi. On prediction of individual sequences. Ann. Statist., 27:1865\u20131895,  N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors. Mach.  Learn., 43:247\u2013264, 2001.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  R.M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes.  Journal of Functional Analysis, 1(3):290 \u2013 330, 1967.  F. Gao, C.-K. Ing, and Y. Yang. Metric entropy and sparse linear approximation of (cid:96)q-hulls for  0 < q (cid:54) 1. J. Approx. Theory, 166:42\u201355, 2013.  S. Gerchinovitz. Prediction of individual sequences and prediction in the statistical framework: some links around sparse regression and aggregation techniques. PhD thesis, Universit\u00b4e Paris- Sud 11, Orsay, 2011.  13   A CHAINING ALGORITHM FOR ONLINE NONPARAMETRIC REGRESSION  with the parameters \u03b3 = BT \u22121/3 and M = (cid:6)log2(\u03b3T /B)(cid:7) satisfies, for some absolute constant c > 0, RegT (F) (cid:54) c max{B, B2}T 1/3 log T .  The proof is postponed to the "}, "Escaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decomposition": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Escaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decomposition", "abstract": "We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in \\em saddle points. In this paper we identify \\em strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that from an \\em arbitrary starting point,  stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives \\em global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of  latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee.", "pdf_url": "http://proceedings.mlr.press/v40/Ge15.pdf", "keywords": ["stochastic gradient", "non-convex optimization", "saddle points", "tensor decomposition"]}, "Learning the dependence structure of rare events: a non-asymptotic study": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Learning the dependence structure of rare events: a non-asymptotic study", "abstract": "Assessing the probability of occurrence of extreme events  is a crucial issue in various fields like finance, insurance, telecommunication or environmental sciences. In a multivariate framework, the tail dependence is characterized by the so-called \\emphstable tail dependence function (\\textscstdf). Learning this structure is the keystone of multivariate extremes. Although extensive studies have proved consistency and asymptotic normality for the empirical version of the \\textscstdf, non-asymptotic bounds are still missing. The main purpose of this paper is to fill this gap. Taking advantage of adapted VC-type concentration inequalities, upper bounds are derived with expected rate of convergence in O(k^-1/2). The concentration tools involved in this analysis rely on a more general study of maximal deviations in low probability regions, and thus directly apply to the classification of extreme data.", "pdf_url": "http://proceedings.mlr.press/v40/Goix15.pdf", "keywords": [], "reference": "Martin Anthony and John Shawe-Taylor. A result of Vapnik with applications. Discrete Applied  Mathematics, 47(3):207 - 217, 1993.  Jan Beirlant, Petra Vynckier, and Jozef L. Teugels. Tail index estimation, pareto quantile plots regression diagnostics. Journal of the American Statistical Association, 91(436):1659-1667, 1996.  11   LEARNING RATES FOR THE DEPENDENCE STRUCTURE OF RARE EVENTS  Recall that l is 1-Lipschitz on [0, T ]d regarding to the (cid:107).(cid:107)1-norm, so that  \u03a51(n) \u2264 sup 0\u2264x\u2264T  d (cid:88)  l=1  (cid:12) (cid:12) (cid:12) (cid:12)  (cid:98)kxj(cid:99) k  \u2212  U j  n k  ((cid:98)kxj (cid:99))  (cid:12) (cid:12) (cid:12) (cid:12)  so that by Lemma 9, with probability greater than 1 \u2212 (d + 1)\u03b4:  \u03a51(n) \u2264 Cd  (cid:114)  2T k  log  .  1 \u03b4  On the other hand, \u03a52(n) \u2264 sup0\u2264x\u2264T with probability at least 1 \u2212 (d + 3)\u03b4:  (cid:80)d  l=1  (cid:98)kxj (cid:99)  k \u2212 xj  (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) \u2264 d (cid:12)  k . Finally we get, for every n > 0,  sup 0\u2264x\u2264T  |ln(x) \u2212 l(x)| \u2264 \u039b(n) + \u03a51(n) + \u03a52(n) + \u039e(n)  \u2264 Cd  log  + Cd  log  +  + sup  (cid:114)  (cid:114)  2T k  2T k  1 \u03b4  1 \u03b4  (cid:114)  2T k  1 \u03b4  d k  (cid:12) (cid:12) (cid:12) (cid:12)  n k  \u02dcF (  k n  0\u2264x\u22642T  \u2264 C(cid:48)d  log  + sup  x) \u2212 l(x)  0\u2264x\u22642T (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) (cid:12) (cid:12)  \u02dcF (x) \u2212  n k  l(  (cid:12) (cid:12) x) (cid:12) (cid:12)  k n  5. Discussion  We provide a non-asymptotic bound of VC type controlling the error of the empirical version of the STDF. Our bound achieves the expected rate in O(k\u22121/2) + bias(k), where k is the number of (extreme) observations retained in the learning process. In practice the smaller k/n, the smaller the bias. Since no assumption is made on the underlying distribution, other than the existence of the STDF, it is not possible in our framework to control the bias explicitly. One option would be to make an additional hypothesis of \u2018second order regular variation\u2019 (see e.g. de Haan and Resnick, 1996). We made the choice of making as few assumptions as possible, however, since the bias term is separated from the \u2018variance\u2019 term, it is probably feasible to refine our result with more assumptions.  For the purpose of controlling the empirical STDF, we have adopted the more general framework of maximal deviations in low probability regions. The VC-type bounds adapted to low probability regions derived in Section 3 may directly be applied to a particular prediction context, namely where the objective is to learn a classifier (or a regressor) that has good properties on low proba- bility regions. This may open the road to the study of classification of extremal observations, with immediate applications to the field of anomaly detection.  References  Martin Anthony and John Shawe-Taylor. A result of Vapnik with applications. Discrete Applied  Mathematics, 47(3):207 - 217, 1993.  Jan Beirlant, Petra Vynckier, and Jozef L. Teugels. Tail index estimation, pareto quantile plots regression diagnostics. Journal of the American Statistical Association, 91(436):1659-1667, 1996. GOIX SABOURIN CL \u00b4EMENC\u00b8 ON  Olivier Bousquet, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Introduction to statistical learning the- ory. In Olivier Bousquet, Ulrike von Luxburg, and Gunnar R\u00a8atsch, editors, Advanced Lectures on Machine Learning, volume 3176 of Lecture Notes in Computer Science, pages 169-207. Springer Berlin Heidelberg, 2004.  L. de Haan and A. Ferreira. Extreme value theory. Springer Series in Operations Research and  Financial Engineering. Springer, New York, 2006. An introduction.  Laurens de Haan and Sidney Resnick. Second-order regular variation and rates of convergence in  extreme-value theory. The Annals of Probability, pages 97-124, 1996.  A. L. M. Dekkers, J. H. J. Einmahl, and L. De Haan. A moment estimator for the index of an  extreme-value distribution. Ann. Statist., 17(4):1833-1855, 12 1989.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Applications of mathematics : stochastic modelling and applied probability. U.S. Government Printing Office, 1996.  Holger Drees and Xin Huang. Best attainable rates of convergence for estimators of the stable tail  dependence function. J. Multivar. Anal., 64(1):25-47, January 1998.  John H. J. Einmahl, Laurens de Haan, and Deyuan Li. Weighted approximations of tail copula processes with application to testing the bivariate extreme value condition. Ann. Statist., 34(4): 1987-2014, 08 2006.  John H. J. Einmahl, Jun Li, and Regina Y. Liu. Thresholding events of extreme in simultaneous monitoring of multiple risks. Journal of the American Statistical Association, 104(487):982- 992, 2009.  John H. J. Einmahl, Andrea Krajina, and Johan Segers. An m-estimator for tail dependence in  arbitrary dimensions. Ann. Statist., 40(3):1764-1793, 06 2012.  Paul Embrechts, Laurens de Haan, and Xin Huang. Modelling multivariate extremes. Extremes and  Integrated Risk Management (Ed. P. Embrechts), RISK Books(59-67), 2000.  M. Falk, J. Huesler, and R. D. Reiss. Laws of Small Numbers: Extremes and Rare Events.  Birkhauser, Boston, 1994.  3(5):1163-1174, 09 1975.  Bruce M. Hill. A simple general approach to inference about the tail of a distribution. Ann. Statist.,  Xin Huang. Statistics of bivariate extreme values, 1992.  V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization (with  discussion). The Annals of Statistics, 34:2593-2706, 2006.  Colin McDiarmid. Concentration. In Michel Habib, Colin McDiarmid, Jorge Ramirez-Alfonsin, and Bruce Reed, editors, Probabilistic Methods for Algorithmic Discrete Mathematics, volume 16 of Algorithms and Combinatorics, pages 195-248. Springer Berlin Heidelberg, 1998. LEARNING RATES FOR THE DEPENDENCE STRUCTURE OF RARE EVENTS  Yongcheng Qi. Almost sure convergence of the stable tail empirical dependence function in multi-  variate extreme statistics. Acta Mathematicae Applicatae Sinica, 13(2):167-175, 1997.  Sidney Resnick. Extreme Values, Regular Variation, and Point Processes. Springer Series in Oper-  ations Research and Financial Engineering, 1987.  Sidney Resnick. Heavy-tail phenomena: probabilistic and statistical modeling. Springer Science  & Business Media, 2007.  1987.  Richard L. Smith. Estimating tails of probability distributions. Ann. Statist., 15(3):1174-1207, 09  V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, Moscow, 1974. (German Translation: W. Wapnik & A. Tscherwonenkis, Theorie der Zeichenerkennung, Akademie-Verlag, Berlin, 1979).  JonA. Wellner. Limit theorems for the ratio of the empirical distribution function to the true distri- bution function. Zeitschrift f\u00a8ur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 45(1):73-88, 1978.  "}, "Thompson Sampling for Learning Parameterized Markov Decision Processes": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Thompson Sampling for Learning Parameterized Markov Decision Processes", "abstract": "We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space.", "pdf_url": "http://proceedings.mlr.press/v40/Gopalan15.pdf", "keywords": ["Thompson sampling", "Markov Decision Process", "Reinforcement learning"], "reference": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm. CoRR, abs/1406.3926, 2014. URL http: //arxiv.org/abs/1406.3926.  R. Agrawal, D. Teneketzis, and V. Anantharam. Asymptotically efficient adaptive allocation schemes for controlled Markov chains: finite parameter space. IEEE Trans. Aut. Cont., 34(12): 1249-1259, Dec 1989.  Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit  problem. In COLT, volume 23 of Proc. JMLR, pages 39.1-39.26, 2012.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In Proc. ICML, 2013.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. J. Mach. Learn. Res., 11:2785-2836, December 2010.  Andrew R. Barron. Information-theoretic characterization of Bayes Performance and the Choice of  Priors in Parametric and Nonparametric Problems. Bayesian Statistics, 6:27-52, 1998.  P.L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In Proc. UAI, pages 35-42, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures in Machine Learning, pages 208-240. Springer, 2004.  Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-  optimal reinforcement learning. JMLR, 3:213-231, 2003.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision  processes. Math. Oper. Res., 22(1):pp. 222-255, 1997.  Taeryon Choi and R. V. Ramamoorthi. Remarks on consistency of posterior distributions, volume 3  of Collections. Institute of Mathematical Statistics, 2008.  Victor H. de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Pseudo-maximization and self-  normalized processes. Probab. Surveys, 4:172-192, 2007.  Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In Proc.  UAI, 1999.  S. Ghosal, J. K. Ghosh, and R. V. Ramamoorthi. Posterior consistency of Dirichlet mixtures in  density estimation. Ann. Statist., 27(1):143-158, 03 1999.  Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior  distributions. Ann. Statist., 28(2):500-531, 04 2000.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson Sampling for Complex Online  Problems. In Proc. ICML, 2014.  13   THOMPSON SAMPLING FOR LEARNING PARAMETERIZED MDPS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm. CoRR, abs/1406.3926, 2014. URL http: //arxiv.org/abs/1406.3926.  R. Agrawal, D. Teneketzis, and V. Anantharam. Asymptotically efficient adaptive allocation schemes for controlled Markov chains: finite parameter space. IEEE Trans. Aut. Cont., 34(12): 1249-1259, Dec 1989.  Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit  problem. In COLT, volume 23 of Proc. JMLR, pages 39.1-39.26, 2012.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In Proc. ICML, 2013.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. J. Mach. Learn. Res., 11:2785-2836, December 2010.  Andrew R. Barron. Information-theoretic characterization of Bayes Performance and the Choice of  Priors in Parametric and Nonparametric Problems. Bayesian Statistics, 6:27-52, 1998.  P.L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In Proc. UAI, pages 35-42, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures in Machine Learning, pages 208-240. Springer, 2004.  Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-  optimal reinforcement learning. JMLR, 3:213-231, 2003.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision  processes. Math. Oper. Res., 22(1):pp. 222-255, 1997.  Taeryon Choi and R. V. Ramamoorthi. Remarks on consistency of posterior distributions, volume 3  of Collections. Institute of Mathematical Statistics, 2008.  Victor H. de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Pseudo-maximization and self-  normalized processes. Probab. Surveys, 4:172-192, 2007.  Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In Proc.  UAI, 1999.  S. Ghosal, J. K. Ghosh, and R. V. Ramamoorthi. Posterior consistency of Dirichlet mixtures in  density estimation. Ann. Statist., 27(1):143-158, 03 1999.  Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior  distributions. Ann. Statist., 28(2):500-531, 04 2000.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson Sampling for Complex Online  Problems. In Proc. ICML, 2014. GOPALAN MANNOR  Geoffrey Grimmett and David Stirzaker. Probability and Random Processes. Oxford University  Press, 1992.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement  Learning. JMLR, 11:1563-1600, 2010.  Emilie Kaufmann, Nathaniel Korda, and R\u00b4emi Munos. Thompson Sampling: An Asymptotically  Optimal Finite-time Analysis. In Proc. ALT, 2012.  Ger Koole. A simple proof of the optimality of a threshold policy in a two-server queueing system.  Syst. Control Lett., 26(5):301-303, December 1995.  Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson Sampling for 1-Dimensional  Exponential Family Bandits. In Proc. NIPS, 2013.  Christina E Lee, Asuman Ozdaglar, and Devavrat Shah. Computing the Stationary Distribution  Locally. In Proc. NIPS, pages 1376-1384. Curran Associates, Inc., 2013.  David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. Amer.  Math. Soc., 2006.  Woei Lin and P.R. Kumar. Optimal control of a queueing system with two heterogeneous servers.  Automatic Control, IEEE Transactions on, 29(8):696-703, Aug 1984.  Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with  function approximation. In Proc. ICML, pages 664-671, 2008.  P A Ortega and D A Braun. A Minimum Relative Entropy Principle for Learning and Acting. JAIR,  38:475-511, 2010.  Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder Dimen- sion. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1466-1474. Curran Associates, Inc., 2014.  Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in Factored MDPs. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 604-612. Curran Associates, Inc., 2014.  Ian Osband, Dan Russo, and Benjamin Van Roy.  (More) Efficient Reinforcement Learning via  Posterior Sampling. In Proc. NIPS, pages 3003-3011. Curran Associates, Inc., 2013.  Herbert Robbins and David Siegmund. Boundary crossing probabilities for the Wiener process and  sample sums. Ann. Math. Statist., 41(5):1410-1429, 1970.  Dan Russo and Benjamin Van Roy. Eluder Dimension and the Sample Complexity of Optimistic  Exploration. In Proc. NIPS, pages 2256-2264. Curran Associates, Inc., 2013.  Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior distributions. Ann. Stat.,  29(3):687-714, 06 2001. THOMPSON SAMPLING FOR LEARNING PARAMETERIZED MDPS  Ambuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for  irreducible MDPs. In Proc. NIPS, 2008.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 24(3-4):285-294, 1933.  Appendices for the paper Thompson Sampling for Learning Parameterized Markov Decision Processes  "}, "Computational Lower Bounds for Community Detection on Random Graphs": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Computational Lower Bounds for Community Detection on Random Graphs", "abstract": "This paper studies the problem of detecting the presence of a small dense community planted in a large Erd\u0151s-R\u00e9nyi random graph \\calG(N,q), where the edge probability within the community exceeds q by a constant factor. Assuming the hardness of the planted clique detection problem, we show that the  computational  complexity of detecting the community exhibits the following phase transition phenomenon: As the graph size N grows and the graph becomes sparser according to q=N^-\u03b1, there exists a critical value of \u03b1= \\frac23, below which there exists a computationally intensive procedure that can detect far smaller communities than any computationally efficient procedure, and above which a linear-time procedure is statistically optimal. The results also lead to the average-case hardness results for recovering the dense community and approximating the densest K-subgraph.", "pdf_url": "http://proceedings.mlr.press/v40/Hajek15.pdf", "keywords": [], "reference": "arXiv:1405.3267, 2014.  E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Arxiv preprint  N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13(3-4), 1998.  N. Alon, A. Andoni, T. Kaufman, K. Matulef, R. Rubinfeld, and N. Xie. Testing k-wise and almost k-wise independence. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 496-505. ACM, 2007.  N. Alon, S. Arora, R. Manokaran, D. Moshkovitz, and O. Weinstein. Inapproximabilty of densest \u03ba- subgraph from average case hardness. Manuscript, available at https://www.nada.kth. se/\u02dcrajsekar/papers/dks.pdf, 2011.  B. PW Ames. Robust convex relaxation for the planted clique and densest k-subgraph problems.  arXiv:1305.4891, 2013.  B. PW Ames and S. A Vavasis. Nuclear norm minimization for the planted clique and biclique  problems. Mathematical programming, 129(1):69-89, 2011.  B. Applebaum, B. Barak, and A. Wigderson. Public-key cryptography from different assumptions. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, STOC \u201910, pages 171-180, 2010. http://www.cs.princeton.edu/\u02dcboaz/Papers/ncpkcFull1. pdf.  E. Arias-Castro and N. Verzelen. Community detection in dense random networks. The Annals of  Statistics, 42(3):940-969, 06 2014.  S. Arora, B. Barak, M. Brunnermeier, and R. Ge. Computational complexity and information asym- metry in financial products. In Innovations in Computer Science (ICS 2010), pages 49-65, 2010. http://www.cs.princeton.edu/\u02dcrongge/derivativelatest.pdf.  S. Balakrishnan, M. Kolar, A. Rinaldo, A. Singh, and L. Wasserman. Statistical and computational In NIPS 2011 Workshop on Computational Trade-offs in Statistical  tradeoffs in biclustering. Learning, 2011.  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component  detection. J. Mach. Learn. Res., 30:1046-1066 (electronic), 2013.  A. Bhaskara, M. Charikar, E. Chlamtac, U. Feige, and A. Vijayaraghavan. Detecting high log- densities: An o(n1/4) approximation for densest k-subgraph. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, STOC \u201910, pages 201-210, 2010.  C. Butucea and Y. I. Ingster. Detection of a sparse submatrix of a high-dimensional noisy matrix.  Bernoulli, 19(5B):2652-2688, 11 2013.  V. Chandrasekaran and M. I Jordan. Computational and statistical tradeoffs via convex relaxation.  PNAS, 110(13):E1181-E1190, 2013.  13   COMPUTATIONAL LOWER BOUNDS FOR COMMUNITY DETECTION  References  arXiv:1405.3267, 2014.  E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Arxiv preprint  N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13(3-4), 1998.  N. Alon, A. Andoni, T. Kaufman, K. Matulef, R. Rubinfeld, and N. Xie. Testing k-wise and almost k-wise independence. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 496-505. ACM, 2007.  N. Alon, S. Arora, R. Manokaran, D. Moshkovitz, and O. Weinstein. Inapproximabilty of densest \u03ba- subgraph from average case hardness. Manuscript, available at https://www.nada.kth. se/\u02dcrajsekar/papers/dks.pdf, 2011.  B. PW Ames. Robust convex relaxation for the planted clique and densest k-subgraph problems.  arXiv:1305.4891, 2013.  B. PW Ames and S. A Vavasis. Nuclear norm minimization for the planted clique and biclique  problems. Mathematical programming, 129(1):69-89, 2011.  B. Applebaum, B. Barak, and A. Wigderson. Public-key cryptography from different assumptions. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, STOC \u201910, pages 171-180, 2010. http://www.cs.princeton.edu/\u02dcboaz/Papers/ncpkcFull1. pdf.  E. Arias-Castro and N. Verzelen. Community detection in dense random networks. The Annals of  Statistics, 42(3):940-969, 06 2014.  S. Arora, B. Barak, M. Brunnermeier, and R. Ge. Computational complexity and information asym- metry in financial products. In Innovations in Computer Science (ICS 2010), pages 49-65, 2010. http://www.cs.princeton.edu/\u02dcrongge/derivativelatest.pdf.  S. Balakrishnan, M. Kolar, A. Rinaldo, A. Singh, and L. Wasserman. Statistical and computational In NIPS 2011 Workshop on Computational Trade-offs in Statistical  tradeoffs in biclustering. Learning, 2011.  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component  detection. J. Mach. Learn. Res., 30:1046-1066 (electronic), 2013.  A. Bhaskara, M. Charikar, E. Chlamtac, U. Feige, and A. Vijayaraghavan. Detecting high log- densities: An o(n1/4) approximation for densest k-subgraph. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, STOC \u201910, pages 201-210, 2010.  C. Butucea and Y. I. Ingster. Detection of a sparse submatrix of a high-dimensional noisy matrix.  Bernoulli, 19(5B):2652-2688, 11 2013.  V. Chandrasekaran and M. I Jordan. Computational and statistical tradeoffs via convex relaxation.  PNAS, 110(13):E1181-E1190, 2013. HAJEK WU XU  Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix localiza-  tion with a growing number of clusters and submatrices. arXiv:1402.1267, 2014.  Y. Dekel, O. Gurel-Gurevich, and Y. Peres. Finding hidden cliques in linear time with high proba-  bility. arxiv:1010.2997, 2010.  arxiv:1304.7047, 2012.  Y. Deshpande and A. Montanari. Finding hidden cliques of size (cid:112)N/e in nearly linear time.  D. Dubhashi and D. Ranjan. Balls and bins: A study in negative dependence. Random Structures  and Algorithms, 13(2):99-124, 1998.  U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph.  Random Structures & Algorithms, 16(2):195-208, 2000.  U. Feige and D. Ron. Finding hidden cliques in linear time.  In 21st International Meeting on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms (AofA10), Discrete Math. Theor. Comput. Sci. Proc., AM, pages 189-203, 2010.  V. Feldman, E. Grigorescu, L. Reyzin, S. Vempala, and Y. Xiao. Statistical algorithms and a lower In Proceedings of the 45th annual ACM symposium on  bound for detecting planted cliques. Symposium on theory of computing, pages 655-664, 2013.  S. Fortunato. Community detection in graphs. Physics Reports, 486(3):75-174, 2010.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite program-  ming. preprint, arxiv:1412.6156, Nov 2014.  E. Hazan and R. Krauthgamer. How hard is it to approximate the best Nash equilibrium? SIAM  Journal on Computing, 40(1):79-91, 2011.  M. Jerrum. Large cliques elude the metropolis process. Random Structures & Algorithms, 3(4):  A. Juels and M. Peinado. Hiding cliques for cryptographic security. Designs, Codes & Crypto.,  347-359, 1992.  2000.  M. Kolar, S. Balakrishnan, A. Rinaldo, and A. Singh. Minimax localization of structural information  in large noisy matrices. In NIPS, 2011.  L. Ku\u02c7cera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics, 57  (2):193-212, 1995.  Z. Ma and Y. Wu. Computational barriers in minimax submatrix detection. to appear in The Annals  of Statistics, arXiv:1309.5914, 2015.  L. Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property. arxiv:1109.3318,  2013.  F. McSherry. Spectral partitioning of random graphs. In FOCS, pages 529 - 537, 2001. COMPUTATIONAL LOWER BOUNDS FOR COMMUNITY DETECTION  M. Mitzenmacher and E. Upfal. Probability and Computing: Randomized Algorithms and Proba-  bilistic Analysis. Cambridge University Press, New York, NY, USA, 2005.  E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. available at:  http://arxiv.org/abs/1202.1499, 2012.  E. Mossel, J. Neeman, and A. Sly.  A proof of the block model  threshold conjecture.  arxiv:1311.4115, 2013.  preprint arXiv:1407.1591, 2014.  E. Mossel, J. Neeman, and A. Sly. Consistency thresholds for binary symmetric block models. Arxiv  R. Vershynin. A simple decoupling inequality in probability theory. Manuscript, available at http: //www-personal.umich.edu/\u02dcromanv/papers/decoupling-simple.pdf, 2011.  N. Verzelen and E. Arias-Castro.  Community detection in sparse random networks.  arXiv:1308.2955, 2013.  J. Xu, R. Wu, K. Zhu, B. Hajek, R. Srikant, and L. Ying. Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs. SIGMETRICS Perform. Eval. Rev., 42(1):29-41, June 2014.  "}, "Adaptive Recovery of Signals by Convex Optimization": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Adaptive Recovery of Signals by Convex Optimization", "abstract": "We present a theoretical framework for adaptive estimation and prediction of signals of unknown structure in the presence of noise. The framework allows to address two intertwined challenges: (i) designing optimal statistical estimators; (ii) designing efficient numerical algorithms. In particular, we establish oracle inequalities for the performance of adaptive procedures, which rely upon convex optimization and thus can be efficiently implemented. As an application of the proposed approach, we consider denoising of harmonic oscillations.", "pdf_url": "http://proceedings.mlr.press/v40/Harchaoui15.pdf", "keywords": ["Nonparametric statistics", "adaptive estimation", "statistical applications of convex optimization", "line spectral denoising"], "reference": "E. D. Andersen and K. D. Andersen. The MOSEK optimization toolbox for MATLAB manual.  Version 7.0, 2013. http://docs.mosek.com/7.0/toolbox/.  A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: analysis, algorithms, and  engineering applications, volume 2. SIAM, 2001.  B.N. Bhaskar, G. Tang, and B. Recht. Atomic norm denoising with applications to line spectral  estimation. Signal Processing, IEEE Transactions on, 61(23):5987-5999, 2013.  L. Birg\u00b4e. An alternative point of view on Lepski\u2019s method, volume 36 of Lecture Notes-Monograph  Series, pages 113-133. JSTOR, 2001.  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012. ISSN 1615-3375. doi: 10.1007/s10208-012-9135-7. URL http://dx.doi.org/10.1007/ s10208-012-9135-7.  D. L. Donoho. Statistical estimation and optimal recovery. Ann. Statist., 22(1):238-270, 03 1994.  D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. Ann. Statist., 26(3): 879-921, 06 1998. doi: 10.1214/aos/1024691081. URL http://dx.doi.org/10.1214/ aos/1024691081.  D. L. Donoho and M. G. Low. Renormalization exponents and optimal pointwise rates of conver-  gence. Ann. Statist., 20(2):944-970, 06 1992.  D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: asymptopia?  Journal of the Royal Statistical Society. Series B (Methodological), pages 301-369, 1995.  G. K. Golubev. Adaptive asymptotically minimax estimators of smooth signals. Problemy Peredachi  Informatsii, 23(1):57-67, 1987.  S. Haykin. Adaptive Filter Theory (3rd Ed.). Prentice-Hall, Inc., 1996.  I. Ibragimov and R. Khasminskii. On nonparametric estimation of regression. Soviet Math. Dokl.,  21:810-814, 1980.  I. Ibragimov and R. Khasminskii. Nonparametric estimation of the value of a linear functional in  gaussian white noise. Theor. Probab. & Appl., 29:1-32, 1984.  I. A. Ibragimov and R. Z. Hasminskii. Statistical estimation. Asymptotic Theory, volume 16 of  Applications of Mathematics. Springer, 1981.  13   ADAPTIVE RECOVERY BY CONVEX OPTIMIZATION  This work was supported by the LabEx Persyval-Lab (ANR-11-LABX-0025), the project Titan (CNRS-Mastodons), the project Macaron (ANR-14-CE23-0003-01), the MSR-Inria joint centre, and the Moore-Sloan Data Science Environment at NYU.  Acknowledgments  References  E. D. Andersen and K. D. Andersen. The MOSEK optimization toolbox for MATLAB manual.  Version 7.0, 2013. http://docs.mosek.com/7.0/toolbox/.  A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: analysis, algorithms, and  engineering applications, volume 2. SIAM, 2001.  B.N. Bhaskar, G. Tang, and B. Recht. Atomic norm denoising with applications to line spectral  estimation. Signal Processing, IEEE Transactions on, 61(23):5987-5999, 2013.  L. Birg\u00b4e. An alternative point of view on Lepski\u2019s method, volume 36 of Lecture Notes-Monograph  Series, pages 113-133. JSTOR, 2001.  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012. ISSN 1615-3375. doi: 10.1007/s10208-012-9135-7. URL http://dx.doi.org/10.1007/ s10208-012-9135-7.  D. L. Donoho. Statistical estimation and optimal recovery. Ann. Statist., 22(1):238-270, 03 1994.  D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. Ann. Statist., 26(3): 879-921, 06 1998. doi: 10.1214/aos/1024691081. URL http://dx.doi.org/10.1214/ aos/1024691081.  D. L. Donoho and M. G. Low. Renormalization exponents and optimal pointwise rates of conver-  gence. Ann. Statist., 20(2):944-970, 06 1992.  D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: asymptopia?  Journal of the Royal Statistical Society. Series B (Methodological), pages 301-369, 1995.  G. K. Golubev. Adaptive asymptotically minimax estimators of smooth signals. Problemy Peredachi  Informatsii, 23(1):57-67, 1987.  S. Haykin. Adaptive Filter Theory (3rd Ed.). Prentice-Hall, Inc., 1996.  I. Ibragimov and R. Khasminskii. On nonparametric estimation of regression. Soviet Math. Dokl.,  21:810-814, 1980.  I. Ibragimov and R. Khasminskii. Nonparametric estimation of the value of a linear functional in  gaussian white noise. Theor. Probab. & Appl., 29:1-32, 1984.  I. A. Ibragimov and R. Z. Hasminskii. Statistical estimation. Asymptotic Theory, volume 16 of  Applications of Mathematics. Springer, 1981. HARCHAOUI JUDITSKY NEMIROVSKI OSTROVSKY  A. Juditsky and A. Nemirovski. Nonparametric denoising of signals with unknown local structure, i: Oracle inequalities. Applied and Computational Harmonic Analysis, 27(2):157-179, 2009a.  A. Juditsky and A. Nemirovski. Nonparametric estimation by convex programming. The Annals of  Statistics, pages 2278-2300, 2009b.  A. Juditsky and A. Nemirovski. Nonparametric denoising signals of unknown local structure, ii: Nonparametric function recovery. Applied and Computational Harmonic Analysis, 29(3):354- 367, 2010.  A. Juditsky and A. Nemirovski. On detecting harmonic oscillations.  Bernoulli Journal, 2013. URL http://www.bernoulli-society.org/index.php/publications/ bernoulli-journal/bernoulli-journal-papers.  S. M. Kay. Fundamentals of statistical signal processing: estimation theory. Prentice-Hall, Inc.,  1993.  O. V. Lepskii. On a problem of adaptive estimation in gaussian white noise. Theory of Probability  & Its Applications, 35(3):454-466, 1991.  A. Nemirovski. Topics in non-parametric statistics. Lectures on Probability Theory and Statistics:  Ecole d\u2019Et\u00b4e de Probabilit\u00b4es de Saint-Flour XXVIII-1998, 28:85, 2000.  M. S. Pinsker. Optimal filtering of square-integrable signals in gaussian noise. Problemy Peredachi  Informatsii, 16(2):52-68, 1980.  pages 1348-1360, 1980.  C. J. Stone. Optimal rates of convergence for nonparametric estimators. The annals of Statistics,  G. Tang, B.N. Bhaskar, and B. Recht. Near minimax line spectral estimation.  In Information  Sciences and Systems (CISS), 2013 47th Annual Conference on, pages 1-6. IEEE, 2013.  R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), pages 267-288, 1996.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incor-  porated, 1st edition, 2008.  L. Wasserman. All of Nonparametric Statistics. Springer Texts in Statistics. Springer, 2006.  "}, "Tensor principal component analysis via sum-of-square proofs": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Tensor principal component analysis via sum-of-square proofs", "abstract": "We study a statistical model for the \\emphtensor principal component analysis problem introduced by Montanari and Richard: Given a order-3 tensor \\mathbf T of the form \\mathbf T = \u03c4\u22c5v_0^\u22973 + \\mathbf A, where \u03c4\u22650 is a signal-to-noise ratio, v_0 is a unit vector, and \\mathbf A is a random noise tensor, the goal is to recover the planted vector v_0. For the case that \\mathbf A has iid standard Gaussian entries, we give an efficient algorithm to recover v_0 whenever \u03c4\u2265\u03c9(n^3/4 \\log(n)^1/4), and certify that the recovered vector is close to a maximum likelihood estimator, all with high probability over the random choice of \\mathbf A. The previous best algorithms with provable guarantees required \u03c4\u2265\u03a9(n). In the regime \u03c4\u2264o(n), natural tensor-unfolding-based spectral relaxations for the underlying optimization problem break down. To go beyond this barrier, we use convex relaxations based on the sum-of-squares method. Our recovery algorithm proceeds by rounding a degree-4 sum-of-squares relaxations of the maximum-likelihood-estimation problem for the statistical model. To complement our algorithmic results, we show that degree-4 sum-of-squares relaxations break down for \u03c4\u2264O(n^3/4/\\log(n)^1/4), which demonstrates that improving our current guarantees (by more than logarithmic factors) would require new techniques or might even be intractable. Finally, we show how to exploit additional problem structure in order to solve our sum-of-squares relaxations, up to some approximation, very efficiently. Our fastest algorithm runs in nearly-linear time using shifted (matrix) power iteration and has similar guarantees as above. The analysis of this algorithm also confirms a variant of a conjecture of Montanari and Richard about singular vectors of tensor unfoldings.", "pdf_url": "http://proceedings.mlr.press/v40/Hopkins15.pdf", "keywords": ["Tensors", "principal component analysis", "parameter estimation", "sum-of-squares method", "semidefinite programming", "spectral algorithms", "shifted power iteration"], "reference": "Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral approach to  learning mixed membership community models. CoRR, abs/1302.2684, 2013.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynamics: Applications to learning overcomplete latent variable models. CoRR, abs/1411.1488, 2014a. URL http://arxiv.org/abs/1411.1488.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1): 2773-2832, 2014b. URL http://dl.acm.org/citation.cfm?id=2697055.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor  decomposition via alternating rank-1 updates. CoRR, abs/1402.5180, 2014c.  Boaz Barak and Ankur Moitra. Tensor prediction, rademacher complexity and random 3-xor. CoRR,  abs/1501.06521, 2015. URL http://arxiv.org/abs/1501.06521.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  CoRR, abs/1404.5236, 2014.  Boaz Barak, Fernando G. S. L. Brand\u00e3o, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Howard J. Karloff and Toniann Pitassi, editors, STOC, pages 307-326. ACM, 2012. ISBN 978-1-4503-1245-5. URL http://dl.acm.org/citation.cfm?id=2213977.  Boaz Barak, Guy Kindler, and David Steurer. On the optimality of semidefinite relaxations for  average-case and generalized constraint satisfaction. In ITCS, pages 197-214, 2013.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations. In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 31-40. ACM, 2014a. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591886. URL http://doi.acm.org/10.1145/2591796.2591886.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. CoRR, abs/1407.1543, 2014b. URL http://arxiv.org/ abs/1407.1543.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 1046-1066, 2013. URL http://jmlr. org/proceedings/papers/v30/Berthet13.html.  13   TENSOR PCA  acknowledges the support of an NSF Graduate Research Fellowship under award no. 1144153. D. S. acknowledges support from the Simons Foundation, the National Science Foundation, a Alread P. Sloan Fellowship, and a Microsoft Research Faculty Fellowship, A large portion of this work was completed while the authors were long-term visitors to the Simons Institute for the Theory of Computing (Berkeley) for the program on Algorithmic Spectral Graph Theory.  References  Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral approach to  learning mixed membership community models. CoRR, abs/1302.2684, 2013.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynamics: Applications to learning overcomplete latent variable models. CoRR, abs/1411.1488, 2014a. URL http://arxiv.org/abs/1411.1488.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1): 2773-2832, 2014b. URL http://dl.acm.org/citation.cfm?id=2697055.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor  decomposition via alternating rank-1 updates. CoRR, abs/1402.5180, 2014c.  Boaz Barak and Ankur Moitra. Tensor prediction, rademacher complexity and random 3-xor. CoRR,  abs/1501.06521, 2015. URL http://arxiv.org/abs/1501.06521.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  CoRR, abs/1404.5236, 2014.  Boaz Barak, Fernando G. S. L. Brand\u00e3o, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Howard J. Karloff and Toniann Pitassi, editors, STOC, pages 307-326. ACM, 2012. ISBN 978-1-4503-1245-5. URL http://dl.acm.org/citation.cfm?id=2213977.  Boaz Barak, Guy Kindler, and David Steurer. On the optimality of semidefinite relaxations for  average-case and generalized constraint satisfaction. In ITCS, pages 197-214, 2013.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations. In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 31-40. ACM, 2014a. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591886. URL http://doi.acm.org/10.1145/2591796.2591886.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. CoRR, abs/1407.1543, 2014b. URL http://arxiv.org/ abs/1407.1543.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 1046-1066, 2013. URL http://jmlr. org/proceedings/papers/v30/Berthet13.html. HOPKINS SHI STEURER  Andrew C Doherty and Stephanie Wehner. Convergence of sdp hierarchies for polynomial optimiza-  tion on the hypersphere. arXiv preprint arXiv:1210.5048, 2012.  Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. Theory of  Computing, 3(1):25-43, 2007.  Uriel Feige, Jeong Han Kim, and Eran Ofek. Witnesses for non-satisfiability of dense random 3cnf  formulas. In FOCS, pages 497-508, 2006.  Joel Friedman, Andreas Goerdt, and Michael Krivelevich. Recognizing more unsatisfiable ran- doi: 10.1137/  dom k-sat instances efficiently. S009753970444096X. URL http://dx.doi.org/10.1137/S009753970444096X.  SIAM J. Comput., 35(2):408-430, 2005.  Andreas Goerdt and Michael Krivelevich. Efficient recognition of random unsatisfiable k-sat instances by spectral methods. In STACS 2001, 18th Annual Symposium on Theoretical Aspects of Computer Science, Dresden, Germany, February 15-17, 2001, Proceedings, pages 294-304, 2001. doi: 10. 1007/3-540-44693-1_26. URL http://dx.doi.org/10.1007/3-540-44693-1_26.  Andrea Montanari and Emile Richard. A statistical model for tensor pca. In Advances in Neural  Information Processing Systems, pages 2897-2905, 2014.  Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral methods: From the gaussian hidden clique problem to rank one perturbations of gaussian tensors. arXiv preprint arXiv:1411.6149, 2014.  Ryan O\u2019Donnell and Yuan Zhou. Approximability and proof complexity. In Sanjeev Khanna, editor, SODA, pages 1537-1556. SIAM, 2013. ISBN 978-1-61197-251-1. doi: 10.1137/1.9781611973105. URL http://dx.doi.org/10.1137/1.9781611973105.  Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.  Ryota Tomioka and Taiji Suzuki. Spectral norm of random tensors. arXiv preprint arXiv:1407.1870,  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389-434, 2012. doi: 10.1007/s10208-011-9099-z. URL http://dx.doi. org/10.1007/s10208-011-9099-z.  Roman Vershynin. Introduction to the non-asymtotic analysis of random matrices. pages 210-268,  2014.  2011.  "}, "Fast Exact Matrix Completion with Finite Samples": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Fast Exact Matrix Completion with Finite Samples", "abstract": "Matrix completion is the problem of recovering a low rank matrix by observing a small fraction of its entries. A series of recent works (Keshavan 2012),(Jain et al. 2013) and (Hardt, 2013) have proposed fast non-convex optimization based iterative algorithms to solve this problem. However, the sample complexity in all these results is sub-optimal in its dependence on the rank, condition number and the desired accuracy. In this paper, we present a fast iterative algorithm that solves the matrix completion problem by observing O\\left(nr^5 \\log^3 n\\right) entries, which is independent of the condition number and the desired accuracy. The run time of our algorithm is O\\left( nr^7\\log^3 n\\log 1/\u03b5\\right) which is near linear in the dimension of the matrix. To the best of our knowledge, this is the first near linear time algorithm for exact matrix completion with finite sample complexity (i.e. independent of \u03b5). Our algorithm is based on a well known projected gradient descent method, where the projection is onto the (non-convex) set of low rank matrices. There are two key ideas in our result: 1) our argument is based on a \\ell_\u221enorm potential function (as opposed to the spectral norm) and provides a novel way to obtain perturbation bounds for it. 2) we prove and use a natural extension of the Davis-Kahan theorem to obtain perturbation bounds on the best low rank approximation of matrices with good eigen gap. Both of these ideas may be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v40/Jain15.pdf", "keywords": ["Matrix completion", "Matrix perturbation theory", "Non-convex Optimization"], "reference": "Robert Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neighborhood  interpolation weights. In ICDM, pages 43-52, 2007. doi: 10.1109/ICDM.2007.90.  Rajendra Bhatia. Matrix Analysis. Springer, 1997.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In ICML, 2014.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational Mathematics, 9(6):717-772, December 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. IEEE Trans. Inform. Theory, 56(5):2053-2080, 2009.  Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple- tion. In Proceedings of The 31st International Conference on Machine Learning, pages 674-682, 2014.  L\u00b4aszl\u00b4o Erdos, Antti Knowles, Horng-Tzer Yau, and Jun Yin. Spectral statistics of Erdos-R\u00b4enyi  graphs I: Local semicircle law. The Annals of Probability, 41(3B):2279-2375, 2013.  David F. Gleich and Lek-Heng Lim. Rank aggregation via nuclear norm minimization. In KDD,  pages 60-68, 2011.  2014.  Moritz Hardt. Understanding alternating minimization for matrix completion. In FOCS, 2014.  Moritz Hardt and May Wootters. Fast matrix completion without the condition number. In COLT,  Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for  matrix completion. In COLT, pages 703-725, 2014.  Cho-Jui Hsieh, Kai-Yang Chiang, and Inderjit S. Dhillon. Low rank modeling of signed networks.  In KDD, pages 507-515, 2012.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value  projection. In NIPS, pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proceedings of the 45th annual ACM Symposium on theory of computing, pages 665-674. ACM, 2013.  Raghunandan H. Keshavan. Efficient algorithms for collaborative filtering. Phd Thesis, Stanford  University, 2012.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.  Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Matrix completion from power-law distributed  samples. In NIPS, 2009.  13   FAST EXACT MATRIX COMPLETION WITH FINITE SAMPLES  References  Robert Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neighborhood  interpolation weights. In ICDM, pages 43-52, 2007. doi: 10.1109/ICDM.2007.90.  Rajendra Bhatia. Matrix Analysis. Springer, 1997.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In ICML, 2014.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational Mathematics, 9(6):717-772, December 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. IEEE Trans. Inform. Theory, 56(5):2053-2080, 2009.  Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple- tion. In Proceedings of The 31st International Conference on Machine Learning, pages 674-682, 2014.  L\u00b4aszl\u00b4o Erdos, Antti Knowles, Horng-Tzer Yau, and Jun Yin. Spectral statistics of Erdos-R\u00b4enyi  graphs I: Local semicircle law. The Annals of Probability, 41(3B):2279-2375, 2013.  David F. Gleich and Lek-Heng Lim. Rank aggregation via nuclear norm minimization. In KDD,  pages 60-68, 2011.  2014.  Moritz Hardt. Understanding alternating minimization for matrix completion. In FOCS, 2014.  Moritz Hardt and May Wootters. Fast matrix completion without the condition number. In COLT,  Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for  matrix completion. In COLT, pages 703-725, 2014.  Cho-Jui Hsieh, Kai-Yang Chiang, and Inderjit S. Dhillon. Low rank modeling of signed networks.  In KDD, pages 507-515, 2012.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value  projection. In NIPS, pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proceedings of the 45th annual ACM Symposium on theory of computing, pages 665-674. ACM, 2013.  Raghunandan H. Keshavan. Efficient algorithms for collaborative filtering. Phd Thesis, Stanford  University, 2012.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.  Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Matrix completion from power-law distributed  samples. In NIPS, 2009. JAIN NETRAPALLI  Benjamin Recht. A simple approach to matrix completion. JMLR, 2009.  Benjamin Recht and Christopher R\u00b4e. Parallel stochastic gradient algorithms for large-scale matrix  completion. Mathematical Programming Computation, 5(2):201-226, 2013.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012. FAST EXACT MATRIX COMPLETION WITH FINITE SAMPLES  "}, "Exp-Concavity of Proper Composite Losses": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Exp-Concavity of Proper Composite Losses", "abstract": "The goal of online prediction with expert advice is to find a decision strategy which will perform almost as well as the best expert in a given pool of experts, on any sequence of outcomes. This problem has been widely studied and O(\\sqrtT) and O(\\logT) regret bounds can be achieved for convex losses and strictly convex losses with bounded first and second derivatives respectively. In special cases like the Aggregating Algorithm with mixable losses and the Weighted Average Algorithm with exp-concave losses, it is possible to achieve O(1) regret bounds. But mixability and exp-concavity are roughly equivalent under certain conditions. Thus by understanding the underlying relationship between these two notions we can gain the best of both algorithms (strong theoretical performance guarantees of the Aggregating Algorithm and the computational efficiency of the Weighted Average Algorithm). In this paper we provide a complete characterization of the exp-concavity of any proper composite loss. Using this characterization and the mixability condition of proper losses, we show that it is possible to transform (re-parameterize) any \u03b2-mixable binary proper loss into a \u03b2-exp-concave composite loss with the same \u03b2. In the multi-class case, we propose an approximation approach for this transformation.", "pdf_url": "http://proceedings.mlr.press/v40/Kamalaruban15.pdf", "keywords": ["proper scoring rules", "link functions", "composite losses", "sequential prediction", "regret bound", "aggregating algorithm", "weighted average algorithm", "mixability", "exp-concavity", "substitution functions"], "reference": "Arindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as a  Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664-2669, 2005.  Andreas Buja, Werner Stuetzle, and Yi Shen.  Loss functions for binary class probabil- ity estimation and classification: Structure and applications, manuscript, available at www- stat.wharton.upenn.edu/  buja, 2005.  \u223c  Sever Silvestru Dragomir. Some Gronwall type inequalities and applications. RGMIA Monographs,  Victoria University, Australia, 19, 2000.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  David J. Hand. Deconstructing statistical questions. Journal of the Royal Statistical Society, Series  A (Statistics in Society), 157(3):317-356, 1994.  David J. Hand and Veronica Vinciotti. Local versus global models for classification problems:  Fitting models where it matters. The American Statistician, 57(2):124-131, 2003.  David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. Sequential prediction of individual se- quences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906- 1925, 1998.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixability. In  Proc. Annual Conf. Computational Learning Theory, pages 188-203. Springer, 2005.  Jyrki Kivinen and Manfred K. Warmuth. Averaging expert predictions.  In Proc. Annual Conf.  Computational Learning Theory, pages 153-167. Springer, 1999.  Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning  Research, 11:2387-2422, 2010.  Mark D. Reid and Robert C. Williamson. Information, divergence and risk for binary experiments.  Journal of Machine Learning Research, 12:731-817, 2011.  R. Tyrrell Rockafellar. Convex analysis. Princeton university press, 1970.  Tim  van  Erven.  From  exp-concavity  to  mixability,  2012.  http://www.timvanerven.nl/blog/2012/12/from-exp-concavity-to-mixability/.  Tim Van Erven, Mark D. Reid, and Robert C. Williamson. Mixability is Bayes risk curvature relative  to log loss. Journal of Machine Learning Research, 13(1):1639-1663, 2012.  Elodie Vernet, Mark D. Reid,  and Robert C. Williamson.  losses.  class http://users.cecs.anu.edu.au/  Journal of Machine Learning Research, williams/papers/P189.pdf.  Composite multi- review.  Under  2014.  \u223c  13   EXP-CONCAVITY OF PROPER COMPOSITE LOSSES  References  Arindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as a  Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664-2669, 2005.  Andreas Buja, Werner Stuetzle, and Yi Shen.  Loss functions for binary class probabil- ity estimation and classification: Structure and applications, manuscript, available at www- stat.wharton.upenn.edu/  buja, 2005.  \u223c  Sever Silvestru Dragomir. Some Gronwall type inequalities and applications. RGMIA Monographs,  Victoria University, Australia, 19, 2000.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  David J. Hand. Deconstructing statistical questions. Journal of the Royal Statistical Society, Series  A (Statistics in Society), 157(3):317-356, 1994.  David J. Hand and Veronica Vinciotti. Local versus global models for classification problems:  Fitting models where it matters. The American Statistician, 57(2):124-131, 2003.  David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. Sequential prediction of individual se- quences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906- 1925, 1998.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixability. In  Proc. Annual Conf. Computational Learning Theory, pages 188-203. Springer, 2005.  Jyrki Kivinen and Manfred K. Warmuth. Averaging expert predictions.  In Proc. Annual Conf.  Computational Learning Theory, pages 153-167. Springer, 1999.  Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning  Research, 11:2387-2422, 2010.  Mark D. Reid and Robert C. Williamson. Information, divergence and risk for binary experiments.  Journal of Machine Learning Research, 12:731-817, 2011.  R. Tyrrell Rockafellar. Convex analysis. Princeton university press, 1970.  Tim  van  Erven.  From  exp-concavity  to  mixability,  2012.  http://www.timvanerven.nl/blog/2012/12/from-exp-concavity-to-mixability/.  Tim Van Erven, Mark D. Reid, and Robert C. Williamson. Mixability is Bayes risk curvature relative  to log loss. Journal of Machine Learning Research, 13(1):1639-1663, 2012.  Elodie Vernet, Mark D. Reid,  and Robert C. Williamson.  losses.  class http://users.cecs.anu.edu.au/  Journal of Machine Learning Research, williams/papers/P189.pdf.  Composite multi- review.  Under  2014.  \u223c KAMALARUBAN WILLIAMSON ZHANG  Figure 6: Super-prediction set (S(cid:96)) of a binary game (  diction is represented by (g(  1), g(1)).  1, 1  , [0, 1], (cid:96)), where a generalized pre- }  {\u2212  \u2212  Volodya Vovk. A game of prediction with expert advice. In Proc. Annual Conf. Computational  Learning Theory, pages 51-60. ACM, 1995.  Volodya Vovk. Competitive on-line statistics.  International Statistical Review, 69(2):213-248,  2001.  Volodya Vovk and Fedor Zhdanov. Prediction with expert advice for the Brier game. Journal of  Machine Learning Research, 10:2445-2471, 2009.  Robert C. Williamson. The geometry of losses. In Proc. Annual Conf. Computational Learning  Theory, pages 1078-1108, 2014.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  Proceedings of the International Conference on Machine Learning, pages 928-936, 2003.  "}, "On Learning Distributions from their Samples": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On Learning Distributions from their Samples", "abstract": "One of the most natural and important questions in statistical learning is: how well can a distribution be approximated from its samples. Surprisingly, this question has so far been resolved for only one loss, the KL-divergence and even in this case, the estimator used is ad hoc and not well understood. We study distribution approximations for general loss measures.  For \\ell_2^2 we determine the best approximation possible, for \\ell_1 and \u03c7^2 we derive tight bounds on the best approximation, and when the probabilities are bounded away from zero, we resolve the question for all sufficiently smooth loss measures, thereby providing a coherent understanding of the rate at which distributions can be approximated from their samples.", "pdf_url": "http://proceedings.mlr.press/v40/Kamath15.pdf", "keywords": ["Probability estimation", "online learning", "min-max loss", "f -divergence"], "reference": "Theory, 128:187-206, 2004.  D. Braess and T. Sauer. Bernstein polynomials and learning theory. Journal of Approximation  D. Braess, J. Forster, T. Sauer, and H.U. Simon. How to achieve minimax expected Kullback-Leibler distance from an unknown finite distribution. In Algorithmic Learning Theory: Proceedings of the 13th International Conference ALT , Springer, Heidelberg, pages 380-394, 2002.  Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Learning mixtures of struc- In Proceedings of the Twenty-Fourth Annual ACM- tured distributions over discrete domains. SIAM Symposium on Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013, pages 1380-1394, 2013.  T.M. Cover. Admissibility properties of Gilbert\u2019s encoding for unknown source probabilities. Trans-  actions on Information Theory, 18(1):216-217, January 1972.  I. Csisz\u00b4ar. Information type measures of differences of probability distribution and indirect obser-  vations. Studia Math. Hungarica, 2:299-318, 1967.  P. Diaconis and S. Zabell. Closed form summation for classical distributions: Variations on a theme  of De Moivre. Statistical Science, 6(3):284-302, 1991.  M. Drmota and W. Szpankowski. Precise minimax redundancy and regret. Transactions on Infor-  mation Theory, 50(11):2686-2707, November 2004.  A.L. Gibbs and F.E. Su. On choosing and bounding probability metrics. International Statistical  Review, 70(3):419-435, October 2002.  R. Groeneveld and G. Meeden. The mode, median, and mean inequality. The American Statistician,  31(3):120-121, August 1977.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of discrete distributions under  l1 loss. arXiv preprint arXiv:1411.1467, 2014.  D. Kershaw. Some extensions of W. Gautschi\u2019s inequalities for the gamma function. Mathematics  of Computation, 41(164):607-611, October 1983.  R.E. Krichevsky. The performance of universal encoding. Transactions on Information Theory, 44  (1):296-303, January 1998.  R.E. Krichevsky and V.K. Trofimov. The performance of universal encoding. Transactions on  Information Theory, 27(2):199-207, June 1981.  L. Paninski. Variational minimax estimation of discrete distributions under KL loss. In Advances  in Neural Information Processing Systems (NIPS), 2004.  Q. Xie and A.R. Barron. Minimax redundancy for the class of memoryless sources. Transactions  on Information Theory, 43(2):646-657, March 1997.  13   ON LEARNING DISTRIBUTIONS FROM THEIR SAMPLES  References  Theory, 128:187-206, 2004.  D. Braess and T. Sauer. Bernstein polynomials and learning theory. Journal of Approximation  D. Braess, J. Forster, T. Sauer, and H.U. Simon. How to achieve minimax expected Kullback-Leibler distance from an unknown finite distribution. In Algorithmic Learning Theory: Proceedings of the 13th International Conference ALT , Springer, Heidelberg, pages 380-394, 2002.  Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Learning mixtures of struc- In Proceedings of the Twenty-Fourth Annual ACM- tured distributions over discrete domains. SIAM Symposium on Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013, pages 1380-1394, 2013.  T.M. Cover. Admissibility properties of Gilbert\u2019s encoding for unknown source probabilities. Trans-  actions on Information Theory, 18(1):216-217, January 1972.  I. Csisz\u00b4ar. Information type measures of differences of probability distribution and indirect obser-  vations. Studia Math. Hungarica, 2:299-318, 1967.  P. Diaconis and S. Zabell. Closed form summation for classical distributions: Variations on a theme  of De Moivre. Statistical Science, 6(3):284-302, 1991.  M. Drmota and W. Szpankowski. Precise minimax redundancy and regret. Transactions on Infor-  mation Theory, 50(11):2686-2707, November 2004.  A.L. Gibbs and F.E. Su. On choosing and bounding probability metrics. International Statistical  Review, 70(3):419-435, October 2002.  R. Groeneveld and G. Meeden. The mode, median, and mean inequality. The American Statistician,  31(3):120-121, August 1977.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of discrete distributions under  l1 loss. arXiv preprint arXiv:1411.1467, 2014.  D. Kershaw. Some extensions of W. Gautschi\u2019s inequalities for the gamma function. Mathematics  of Computation, 41(164):607-611, October 1983.  R.E. Krichevsky. The performance of universal encoding. Transactions on Information Theory, 44  (1):296-303, January 1998.  R.E. Krichevsky and V.K. Trofimov. The performance of universal encoding. Transactions on  Information Theory, 27(2):199-207, June 1981.  L. Paninski. Variational minimax estimation of discrete distributions under KL loss. In Advances  in Neural Information Processing Systems (NIPS), 2004.  Q. Xie and A.R. Barron. Minimax redundancy for the class of memoryless sources. Transactions  on Information Theory, 43(2):646-657, March 1997. KAMATH ORLITSKY PICHAPATI SURESH  "}, "MCMC Learning": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "MCMC Learning", "abstract": "The theory of learning under the uniform distribution is rich and deep, with connections to cryptography, computational complexity, and the analysis of boolean functions to name a few areas. This theory however is very limited due to the fact that the uniform distribution and the corresponding Fourier basis are rarely encountered as a statistical model. A family of distributions that vastly generalizes the uniform distribution on the Boolean cube is that of distributions represented by Markov Random Fields (MRF). Markov Random Fields are one of the main tools for modeling high dimensional data in many areas of statistics and machine learning. In this paper we initiate the investigation of extending central ideas, methods and algorithms from the theory of learning under the uniform distribution to the setup of learning concepts given examples from MRF distributions. In particular, our results establish a novel connection between properties of MCMC sampling of MRFs and learning under the MRF distribution.", "pdf_url": "http://proceedings.mlr.press/v40/Kanade15.pdf", "keywords": ["computational learning theory", "MCMC", "markov random fields"], "reference": "put., 117(2):181\u2013186, 1995.  Part of the work was carried out while the authors were at the Simons Institute for the Theory of Computing at the University of California, Berkeley.  David Aldous and Umesh Vazirani. A markovian extension of valiant\u2019s learning model. Inf. Com-  Peter L. Bartlett, Paul Fischer, and Klaus-Uwe H\u00a8offgen. Exploiting random walks for learning. In Proceedings of the seventh annual conference on Computational learning theory, COLT \u201994, pages 318\u2013327, 1994.  Avrim Blum. Learning boolean functions in an in\ufb01nite attribute space. Mach. Learn., 9(4):373\u2013386,  1992.  13   MCMC LEARNING  The proof of the above proposition follows from Lemma 11 in "}, "Online PCA with Spectral Bounds": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Online PCA with Spectral Bounds", "abstract": "This paper revisits the online PCA problem. Given a stream of n vectors x_t \u2208\\mathbbR^d (columns of X) the algorithm must output y_t \u2208\\mathbbR^\\ell  (columns of Y) before receiving x_t+1. The goal of online PCA is to simultaneously minimize the target dimension \\ell and the error \\|X - (XY^\\scriptstyle  \\textrm +)Y\\|^2. We describe two simple and deterministic algorithms. The first, receives a parameter \u2206and guarantees that \\|X - (XY^\\scriptstyle  \\textrm +)Y\\|^2 is not significantly larger than \u2206. It requires a target dimension of \\ell = O(k/\u03b5) for any k,\u03b5such that \u2206\\ge \u03b5\\sigma_1^2 + \\sigma_k+1^2, with \\sigma_i being the i\u2019th singular value of X. The second receives k and \u03b5and guarantees that \\|X - (XY^\\scriptstyle  \\textrm +)Y\\|^2 \\le \u03b5\\sigma_1^2 + \\sigma_k+1^2. It requires a target dimension of O( k\\log n/\u03b5^2). Different models and algorithms for Online PCA were considered in the past. This is the first that achieves a bound on the spectral norm of the residual matrix.", "pdf_url": "http://proceedings.mlr.press/v40/Karnin15.pdf", "keywords": ["Online", "PCA", "SVD", "Principal Component Analysis", "Dimension Reduction"], "reference": "Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1565-1573, 2013.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of pca with capped msg. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1815-1823. 2013.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,  pca. Advances in Neural Information Processing Systems 26, pages 3174-3182. 2013.  11   ONLINE PCA  It follows that (cid:107)(I \u2212 Utm\u22121U T  (cid:107)(I \u2212 Utm\u22121U T tm\u22121)Btm(cid:107)2 > \u2206 and a direction entered U at time tm \u2264 t(cid:48).  F /d \u2265 \u2206 + \u03c1  tm\u22121)Xt:tm(cid:107)2  Theorem 11 Combining the above we get the following: assume Algorithm 3 received as input pa- rameters k, \u03b5 and a sketching algorithm with guarantee \u03c1, update time Tsketch(X, \u03c1) and a memory requirement of Ssketch(X, \u03c1). We have  1. The target dimension of the sketch is O(k log(n)/\u03b52). 2. The running time is bounded by Tsketch(X, \u03c1) + O(nnz(X)k log(n)/\u03b52) + O (cid:0)kd log(n)/\u03b53(cid:1) where nnz(X) is the number of non-zero entries in X. For sufficiently large n (as common in streaming scenarios) this quantity is in fact Tsketch(X, \u03c1) + O(nnz(X)k log(n)/\u03b52).  3. The space requirement of the algorithm is Ssketch(X, \u03c1) + O(kd log(n)/\u03b52).  4. The error of the output is bounded by  (cid:107)X\u2212(XY +)Y (cid:107)2 \u2264 \u03c32  k+1+5\u03b5\u03c32  1+O  (cid:16)(cid:112)  k log(n)/\u03b52 (cid:16)  \u03c1 + max  t  (cid:107)xt(cid:107)2(cid:17)(cid:17)  = \u03c32  k+1+O(\u03b5\u03c32  1) .  k+1 + \u03b5\u03c32  In some cases we have a crude approximation for \u2206\u2217 = \u03c32  Possible improvements: Having prior knowledge about the matrix, two potential improvements 1. By this can be made. we mean having knowledge of a scalar \u22060 such that \u22060 \u2264 \u2206\u2217 but \u22060 \u2265 \u2206\u2217/c, for some large constant c. If this happens to be the case we can initialize \u2206 to be \u22060 and the log(n) terms in the above theorems become log(c). The second improvement can be made when we have some lower bound 1 < \u03ba \u2264 \u03c32 k+1. First, notice that typically it makes sense to have an input k for which k+1 (cid:28) \u03c32 \u03c32 1, hence \u03ba can be potentially large. When having knowledge of such a parameter we can set the multiplicative update of \u2206 to grow by 1 + \u03b5\u03ba rather than 1 + \u03b5. The results stated above regarding the error guarantee remain the same as long as \u03ba \u2264 \u03c32 k+1; however, the running time, memory complexity and target dimension are decrease by a factor of max{1/\u03b5, \u03ba}. To conclude, in an optimistic, yet not unlikely scenario where we have knowledge of \u22060 = \u2126(\u2206\u2217) and \u03ba = \u2126(\u03b5\u22121) we get a target dimension of O(k/\u03b5).  1/\u03c32  1/\u03c32  References  Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1565-1573, 2013.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of pca with capped msg. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1815-1823. 2013.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,  pca. Advances in Neural Information Processing Systems 26, pages 3174-3182. 2013. KARNIN LIBERTY  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal components anal- ysis. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 887-901, 2015.  Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.  Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix approximation.  In APPROX-RANDOM, pages 292-303, 2006.  Petros Drineas and Ravi Kannan. Pass efficient algorithms for approximating large matrices. In Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms, pages 223- 232, 2003.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank In Proceedings of the 39th Annual Symposium on Foundations of Computer  approximations. Science, pages 370-, 1998.  Mina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima- tions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 707-717, 2014a.  Mina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima-  tions. In SODA, 2014b.  Mina Ghashami, Edo Liberty, Jeff M. Phillips, and David P. Woodruff. Frequent directions : Simple  and deterministic matrix sketching. CoRR, abs/1501.01711, 2015.  Edo Liberty. Simple and deterministic matrix sketching. In KDD, pages 581-588, 2013.  Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert. Ran- domized algorithms for the low-rank approximation of matrices. Proceedings of the National Academy of Sciences,, 104(51):20167-20172, December 2007.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming PCA. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2886-2894. 2013.  Jiazhong Nie, Wojciech Kot\u0142owski, and Manfred K. Warmuth. Online PCA with optimal regrets. In  ALT, pages 98-112, 2013.  Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geo-  metric functional analysis. J. ACM, 54(4), July 2007. ISSN 0004-5411.  Tam\u00b4as Sarl\u00b4os. Improved approximation algorithms for large matrices via random projections. In  FOCS, pages 143-152, 2006.  Manfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret bounds  that are logarithmic in the dimension, 2007. "}, "Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem", "abstract": "We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight asymptotic regret lower bound that is based on the information divergence. An algorithm that is inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the first one with a regret upper bound that matches the lower bound. Experimental comparisons of dueling bandit algorithms show that the proposed algorithm significantly outperforms existing ones.", "pdf_url": "http://proceedings.mlr.press/v40/Komiyama15.pdf", "keywords": ["multi-armed bandit problem", "dueling bandit problem", "online learning"], "reference": "R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, 27:1054-1078, 1995.  Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-  dits. In ICML, pages 856-864, 2014.  Peter Auer, Nicol\u00b4o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, 2002.  Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach In Proceedings of the 2010 Eurographics/ACM SIGGRAPH  to procedural animation design. Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103-112, 2010.  S\u00b4ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit\u00b4e des Sciences et  Technologie de Lille - Lille I, June 2010.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and  beyond. In COLT, pages 359-376, 2011.  Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni In In Preference Learning (PL-09)  Semeraro. Preference learning in recommender systems. ECML/PKDD-09 Workshop, 2009.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. Transactions on Information Systems, 31(4):17:1-43, 2013.  Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded  Support Models. In COLT, pages 67-79, 2010.  Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order responses.  In KDD, pages 583-588, 2003.  1091-1114, 09 1987.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):  T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985.  Microsoft Research. Microsoft Learning to Rank Datasets, 2010. URL http://research.  microsoft.com/en-us/projects/mslr/.  Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on  learning to rank for information retrieval. Inf. Retr., 13(4):346-374, 2010.  Tanguy Urvoy, Fabrice Cl\u00b4erot, Rapha\u00a8el Feraud, and Sami Naamane. Generic exploration and k-  armed voting bandits. In ICML, pages 91-99, 2013.  Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241-248, 2011.  13   REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM  References  R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, 27:1054-1078, 1995.  Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-  dits. In ICML, pages 856-864, 2014.  Peter Auer, Nicol\u00b4o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, 2002.  Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach In Proceedings of the 2010 Eurographics/ACM SIGGRAPH  to procedural animation design. Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103-112, 2010.  S\u00b4ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit\u00b4e des Sciences et  Technologie de Lille - Lille I, June 2010.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and  beyond. In COLT, pages 359-376, 2011.  Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni In In Preference Learning (PL-09)  Semeraro. Preference learning in recommender systems. ECML/PKDD-09 Workshop, 2009.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. Transactions on Information Systems, 31(4):17:1-43, 2013.  Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded  Support Models. In COLT, pages 67-79, 2010.  Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order responses.  In KDD, pages 583-588, 2003.  1091-1114, 09 1987.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):  T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985.  Microsoft Research. Microsoft Learning to Rank Datasets, 2010. URL http://research.  microsoft.com/en-us/projects/mslr/.  Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on  learning to rank for information retrieval. Inf. Retr., 13(4):346-374, 2010.  Tanguy Urvoy, Fabrice Cl\u00b4erot, Rapha\u00a8el Feraud, and Sami Naamane. Generic exploration and k-  armed voting bandits. In ICML, pages 91-99, 2013.  Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241-248, 2011. KOMIYAMA HONDA KASHIMA NAKAGAWA  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits  problem. In COLT, 2009.  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits  problem. J. Comput. Syst. Sci., 78(5):1538-1556, 2012.  Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non- In The 49th Annual Meeting of the Association for Computational Linguistics professionals. (ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port- land, Oregon, USA, pages 1220-1229, 2011.  Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R\u00b4emi Munos. Relative confidence sam-  pling for efficient on-line ranker evaluation. In WSDM, pages 73-82, 2014a.  Masrour Zoghi, Shimon Whiteson, R\u00b4emi Munos, and Maarten de Rijke. Relative upper confidence  bound for the k-armed dueling bandit problem. In ICML, pages 10-18, 2014b.  Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale  online ranker evaluation. In WSDM, 2015. "}, "Second-order Quantile Methods for Experts and Combinatorial Games": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Second-order Quantile Methods for Experts and Combinatorial Games", "abstract": "We aim to design strategies for sequential decision making that adjust to the difficulty of the learning problem. We study this question both in the setting of prediction with expert advice, and for more general combinatorial decision tasks. We are not satisfied with just guaranteeing minimax regret rates, but we want our algorithms to perform significantly better on easy data. Two popular ways to formalize such adaptivity are second-order regret bounds and quantile bounds. The underlying notions of \u2018easy data\u2019, which may be paraphrased as \u201cthe learning problem has small variance\u201d and \u201cmultiple decisions are useful\u201d, are synergetic. But even though there are sophisticated algorithms that exploit one of the two, no existing algorithm is able to adapt to both. The difficulty in combining the two notions lies in tuning a parameter called the learning rate, whose optimal value behaves non-monotonically.  We introduce a potential function for which (very surprisingly!) it is sufficient to simply put a prior on learning rates; an approach that does not work for any previous method. By choosing the right prior we construct efficient algorithms and show that they reap both benefits by proving the first bounds that are both second-order and incorporate quantiles.", "pdf_url": "http://proceedings.mlr.press/v40/Koolen15a.pdf", "keywords": ["Online learning", "prediction with expert advice", "combinatorial prediction", "easy data"], "reference": "Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari. Online linear optimization via smoothing. In Proceedings of The 27th Conference on Learning Theory, COLT, volume 35 of JMLR Proceedings, pages 807-823, 2014.  Dmitry Adamskiy, Wouter M. Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at adaptive regret. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT), LNAI 7568, pages 290-304. Springer, 2012.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Math. Oper. Res., 39(1):31-45, 2014.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64:48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. J. Comput. Syst. Sci., 78(5):1404-  Press, 2006.  1422, 2012.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2/3):321-352, 2007.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. In Ad-  vances in Neural Information Processing Systems 22 (NIPS 2009), pages 297-305, 2009.  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In  Uncertainty in Artificial Intelligence, pages 117-125, 2010.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory, number 23 in JMLR W&CP, pages 6.1-6.20, 2012.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In JMLR Workshop and Conference Proceedings, volume 35: Proceedings of the 27th Conference on Learning Theory (COLT), pages 176-196, 2014.  Eyal Gofer and Yishay Mansour. Lower bounds on individual sequence regret.  In Algorithmic  Learning Theory (ALT), volume 7568, pages 275-289. Springer Berlin Heidelberg, 2012.  Peter D. Gr\u00a8unwald. Safe learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity. In JMLR W&CP 19: Proceedings of the 24th Annual Conference on Learning Theory, COLT, pages 397-420, 2011.  Peter D. Gr\u00a8unwald. The safe Bayesian. In Algorithmic Learning Theory (ALT), volume 7568, pages  169-183. Springer Berlin Heidelberg, 2012.  13   SECOND-ORDER QUANTILE METHODS  References  Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari. Online linear optimization via smoothing. In Proceedings of The 27th Conference on Learning Theory, COLT, volume 35 of JMLR Proceedings, pages 807-823, 2014.  Dmitry Adamskiy, Wouter M. Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at adaptive regret. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT), LNAI 7568, pages 290-304. Springer, 2012.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Math. Oper. Res., 39(1):31-45, 2014.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64:48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. J. Comput. Syst. Sci., 78(5):1404-  Press, 2006.  1422, 2012.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2/3):321-352, 2007.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. In Ad-  vances in Neural Information Processing Systems 22 (NIPS 2009), pages 297-305, 2009.  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In  Uncertainty in Artificial Intelligence, pages 117-125, 2010.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory, number 23 in JMLR W&CP, pages 6.1-6.20, 2012.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In JMLR Workshop and Conference Proceedings, volume 35: Proceedings of the 27th Conference on Learning Theory (COLT), pages 176-196, 2014.  Eyal Gofer and Yishay Mansour. Lower bounds on individual sequence regret.  In Algorithmic  Learning Theory (ALT), volume 7568, pages 275-289. Springer Berlin Heidelberg, 2012.  Peter D. Gr\u00a8unwald. Safe learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity. In JMLR W&CP 19: Proceedings of the 24th Annual Conference on Learning Theory, COLT, pages 397-420, 2011.  Peter D. Gr\u00a8unwald. The safe Bayesian. In Algorithmic Learning Theory (ALT), volume 7568, pages  169-183. Springer Berlin Heidelberg, 2012. KOOLEN AND VAN ERVEN  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation  in costs. Machine learning, 80(2-3):165-188, 2010.  David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights.  Journal of Machine Learning Research, 10:1705-1736, 2009.  Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32:151-178,  1998.  Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader. Jour-  nal of Machine Learning Research, 6:639-660, 2005.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  Wouter M. Koolen. The Pareto regret frontier. Systems (NIPS) 26, pages 863-871, 2013.  In Advances in Neural Information Processing  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts.  In  Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 93-105, 2010.  Wouter M. Koolen, Tim van Erven, and Peter D. Gr\u00a8unwald. Learning the learning rate for prediction In Advances in Neural Information Processing Systems (NIPS) 27, pages  with expert advice. 2294-2302, 2014.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.  Information and  Computation, 108(2):212-261, 1994.  Haipeng Luo and Robert E Schapire. A drifting-games analysis for online learning and applications to boosting. In Advances in Neural Information Processing Systems 27, pages 1368-1376. Curran Associates, Inc., 2014.  Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive NormalHedge.  CoRR, abs/1502.05934, 2015. To appear at COLT.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Localization and adaptation in online learning. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, AISTATS, volume 31 of JMLR Proceedings, pages 516-526. JMLR.org, 2013.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, Hedge if you must. Journal of Machine Learning Research, 15:1281-1316, 2014.  Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In Advances in Neural Information Processing Systems 27, pages 810-818. Curran Associates, Inc., 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient  algorithm. In International Conference on Machine Learning (ICML), 2014. SECOND-ORDER QUANTILE METHODS  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Ma-  chine Learning Research, 4:773-818, 2003.  Koji Tsuda, Gunnar R\u00a8atsch, and Manfred K. Warmuth. Matrix Exponentiated Gradient updates for on-line learning and Bregman projections. Journal of Machine Learning Research, 6:995-1018, 2005.  Vladimir Vovk. Aggregating strategies. In COLT Proceedings, pages 371-383, 1990.  Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, 1998.  Manfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension. Journal of Machine Learning Research, 9:2287-2320, 2008.  Manfred K. Warmuth and Dima Kuzmin. Bayesian generalized probability calculus for density  matrices. Machine Learning, 78(1-2):63-101, 2010.  Manfred K. Warmuth, Wouter M. Koolen, and David P. Helmbold. Combining initial segments of lists. Theoretical Computer Science, 519:29-45, 2014. The special issue on Algorithmic Learning Theory for ALT 2011.  Olivier Wintenberger. Optimal learning with Bernstein online aggregation. Preprint, available from  http://arxiv.org/abs/1404.1356v2, 2014. KOOLEN AND VAN ERVEN  This section collects the proofs omitted from Sections 3 and 4.  "}, "Hierarchical Label Queries with Data-Dependent Partitions": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Hierarchical Label Queries with Data-Dependent Partitions", "abstract": "Given a joint distribution P_X, Y over a space \\Xcal and a label set \\Ycal=\\braces0, 1, we consider the problem of recovering the labels of an unlabeled sample with as few label queries as possible. The recovered \t\t     labels can be passed to a passive learner, thus turning the procedure into an active learning approach. We analyze a family of labeling procedures based on a hierarchical clustering of the data. While such labeling procedures have been studied in the past, we provide a new parametrization of P_X, Y that captures their behavior in general low-noise settings, and which accounts for data-dependent clustering, thus providing new theoretical underpinning to practically used tools.", "pdf_url": "http://proceedings.mlr.press/v40/Kpotufe15.pdf", "keywords": [], "reference": "2008.  M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. COLT,  N. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. ICML, 2006.  Shai Ben-David and Ruth Urner. The sample complexity of agnostic learning under deterministic  labels. In Proceedings of The 27th Conference on Learning Theory, pages 527-542, 2014.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures on Machine Learning, pages 208-240. Springer, 2004.  S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. STOC, 2008.  S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. NIPS, 2007.  Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767-1781,  2011.  Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In Proceedings of the  25th international conference on Machine learning, pages 208-215. ACM, 2008.  S. Hanneke. Adaptive rates of convergence in active learning. COLT, 2009.  Samory Kpotufe and Sanjoy Dasgupta. A tree-based regressor that adapts to intrinsic dimension.  Journal of Computer and System Sciences, 78(5):1496-1515, 2012.  Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Annals of Statistics,  27(6):1808-1829, 1999.  2366, 2006.  P. Massart and E. Nedelec. Risk bounds for statististical learning. Annals of Statistics, 34(5):2326-  Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster  assumption. In Journal of Machine Learning Research, pages 1369-1392, 2007.  Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statis-  tics, 32(1):135-166, 2004.  Ruth Urner, Sharon Wullf, and Shai Ben-David. Plal: Cluster-based active learning. In Proceedings  of the Conference on Learning Theory (COLT), 2013.  V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their expectation. Theory of probability and its applications, 16:264-280, 1971.  Santosh Vempala, Deepak D\u2019Souza, Telikepalli Kavitha, and Jaikumar Radhakrishnan. Randomly-  oriented kd trees adapt to intrinsic dimension. In FSTTCS, pages 48-57, 2012.  13   HIERARCHICAL LABEL QUERIES  References  2008.  M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. COLT,  N. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. ICML, 2006.  Shai Ben-David and Ruth Urner. The sample complexity of agnostic learning under deterministic  labels. In Proceedings of The 27th Conference on Learning Theory, pages 527-542, 2014.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures on Machine Learning, pages 208-240. Springer, 2004.  S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. STOC, 2008.  S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. NIPS, 2007.  Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767-1781,  2011.  Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In Proceedings of the  25th international conference on Machine learning, pages 208-215. ACM, 2008.  S. Hanneke. Adaptive rates of convergence in active learning. COLT, 2009.  Samory Kpotufe and Sanjoy Dasgupta. A tree-based regressor that adapts to intrinsic dimension.  Journal of Computer and System Sciences, 78(5):1496-1515, 2012.  Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Annals of Statistics,  27(6):1808-1829, 1999.  2366, 2006.  P. Massart and E. Nedelec. Risk bounds for statististical learning. Annals of Statistics, 34(5):2326-  Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster  assumption. In Journal of Machine Learning Research, pages 1369-1392, 2007.  Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statis-  tics, 32(1):135-166, 2004.  Ruth Urner, Sharon Wullf, and Shai Ben-David. Plal: Cluster-based active learning. In Proceedings  of the Conference on Learning Theory (COLT), 2013.  V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their expectation. Theory of probability and its applications, 16:264-280, 1971.  Santosh Vempala, Deepak D\u2019Souza, Telikepalli Kavitha, and Jaikumar Radhakrishnan. Randomly-  oriented kd trees adapt to intrinsic dimension. In FSTTCS, pages 48-57, 2012. KPOTUFE URNER BEN-DAVID  "}, "Algorithms for Lipschitz Learning on Graphs": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Algorithms for Lipschitz Learning on Graphs", "abstract": "We develop fast algorithms for solving regression problems on graphs where one is given the value of a function at some vertices, and must find its smoothest possible extension to all vertices. The extension we compute is the absolutely minimal Lipschitz extension, and is the limit for large p of p-Laplacian regularization. We present an algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes an absolutely minimal Lipschitz extension in expected time \\widetildeO (m n). The latter algorithm has variants that seem to run much faster in practice. These extensions are particularly amenable to regularization: we can perform l_0-regularization on the given values in polynomial time and l_1-regularization on the initial function values and on graph edge weights in time \\widetildeO (m^3/2). Our definitions and algorithms naturally extend to directed graphs.", "pdf_url": "http://proceedings.mlr.press/v40/Kyng15.pdf", "keywords": []}, "Low Rank Matrix Completion with Exponential Family Noise": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Low Rank Matrix Completion with Exponential Family Noise", "abstract": "The matrix completion problem consists in reconstructing a matrix from a sample of entries, possibly observed with noise. A popular class of estimator, known as nuclear norm penalized estimators, are based on minimizing the sum of a data fitting term and a nuclear norm penalization. Here, we investigate the case where the noise distribution belongs to the exponential family and is sub-exponential. Our framework allows for a general sampling scheme. We first consider an estimator defined as the minimizer of the sum of a log-likelihood term and a nuclear norm penalization and prove an upper bound on the Frobenius prediction risk. The rate obtained improves on previous works on matrix completion for exponential family. When the sampling distribution is known, we propose another estimator and prove an oracle inequality \\em w.r.t. the Kullback-Leibler prediction risk, which translates immediately into an upper bound on the Frobenius prediction risk. Finally, we show that all the rates obtained are minimax optimal up to a logarithmic factor.", "pdf_url": "http://proceedings.mlr.press/v40/Lafond15.pdf", "keywords": ["Low rank matrix estimation", "matrix completion", "exponential family model", "nuclear norm"], "reference": "York, 1997.  R. Bhatia. Matrix analysis, volume 169 of Graduate Texts in Mathematics. Springer-Verlag, New  T. T. Cai and W-X. Zhou. Matrix completion via max-norm constrained optimization. CoRR,  abs/1303.0341, 2013a.  T. T. Cai and W-X. Zhou. A max-norm constrained minimization approach to 1-bit matrix comple-  tion. J. Mach. Learn. Res., 14:3619-3647, 2013b.  E. J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925-936,  M. A. Davenport, Y. Plan, E. van den Berg, and M. Wootters. 1-bit matrix completion. CoRR,  D. Gross. Recovering low-rank matrices from few coefficients in any basis. Information Theory,  IEEE Transactions on, 57(3):1548-1566, 2011.  S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural  R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. J. Mach. Learn.  O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2(1):  2010.  abs/1209.3672, 2012.  constraints. ICML, 2014.  Res., 11:2057-2078, 2010.  282-303, 02 2014.  2014.  O. Klopp, J. Lafond, E. Moulines, and J. Salmon. Adaptive Multinomial Matrix Completion. August  V. Koltchinskii. A remark on low rank matrix recovery and noncommutative Bernstein type in- equalities, volume Volume 9 of Collections, pages 213-226. Institute of Mathematical Statistics, 2013.  V. Koltchinskii, A. B. Tsybakov, and K. Lounici. Nuclear-norm penalization and optimal rates for  noisy low-rank matrix completion. Ann. Statist., 39(5):2302-2329, 2011.  J. Lafond, O. Klopp, E. Moulines, and J. Salmon. Probabilistic low-rank matrix completion on finite  M. Ledoux and M. Talagrand. Probability in Banach spaces, volume 23. Springer-Verlag, Berlin,  P. Massart. About the constants in Talagrand\u2019s concentration inequalities for empirical processes.  S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion:  optimal bounds with noise. J. Mach. Learn. Res., 13, 2012.  alphabets. In NIPS. 2014.  1991.  Ann. Probab., 28, 2000.  13   EXPONENTIAL FAMILY MATRIX COMPLETION  References  York, 1997.  R. Bhatia. Matrix analysis, volume 169 of Graduate Texts in Mathematics. Springer-Verlag, New  T. T. Cai and W-X. Zhou. Matrix completion via max-norm constrained optimization. CoRR,  abs/1303.0341, 2013a.  T. T. Cai and W-X. Zhou. A max-norm constrained minimization approach to 1-bit matrix comple-  tion. J. Mach. Learn. Res., 14:3619-3647, 2013b.  E. J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925-936,  M. A. Davenport, Y. Plan, E. van den Berg, and M. Wootters. 1-bit matrix completion. CoRR,  D. Gross. Recovering low-rank matrices from few coefficients in any basis. Information Theory,  IEEE Transactions on, 57(3):1548-1566, 2011.  S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural  R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. J. Mach. Learn.  O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2(1):  2010.  abs/1209.3672, 2012.  constraints. ICML, 2014.  Res., 11:2057-2078, 2010.  282-303, 02 2014.  2014.  O. Klopp, J. Lafond, E. Moulines, and J. Salmon. Adaptive Multinomial Matrix Completion. August  V. Koltchinskii. A remark on low rank matrix recovery and noncommutative Bernstein type in- equalities, volume Volume 9 of Collections, pages 213-226. Institute of Mathematical Statistics, 2013.  V. Koltchinskii, A. B. Tsybakov, and K. Lounici. Nuclear-norm penalization and optimal rates for  noisy low-rank matrix completion. Ann. Statist., 39(5):2302-2329, 2011.  J. Lafond, O. Klopp, E. Moulines, and J. Salmon. Probabilistic low-rank matrix completion on finite  M. Ledoux and M. Talagrand. Probability in Banach spaces, volume 23. Springer-Verlag, Berlin,  P. Massart. About the constants in Talagrand\u2019s concentration inequalities for empirical processes.  S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion:  optimal bounds with noise. J. Mach. Learn. Res., 13, 2012.  alphabets. In NIPS. 2014.  1991.  Ann. Probab., 28, 2000. LAFOND  N. Srebro and R. R. Salakhutdinov. Collaborative filtering in a non-uniform world: Learning with  the weighted trace norm. 2010.  J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math., 12(4):  A. B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. Springer,  389-434, 2012.  New York, 2009.  M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational infer-  ence. Foundations and Trends R(cid:13) in Machine Learning, 1, 2008.  G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and  its Applications, 170:33-45, 1992.  C. H. Zhang and T. Zhang. A General Framework of Dual Certificate Analysis for Structured Sparse  Recovery Problems. arXiv.org, January 2012.  "}, "Bad Universal Priors and Notions of Optimality": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Bad Universal Priors and Notions of Optimality", "abstract": "A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a \\emphrelative theory, dependent on the choice of the UTM.", "pdf_url": "http://proceedings.mlr.press/v40/Leike15.pdf", "keywords": ["AIXI", "general reinforcement learning", "universal Turing machine", "Legg-Hutter intelligence", "balanced Pareto optimality", "asymptotic optimality"], "reference": "Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. Tech-  nical Report cs.AI/0004001, 2000. http://arxiv.org/abs/cs.AI/0004001.  Marcus Hutter. New error bounds for Solomonoff prediction. Journal of Computer and System  Sciences, 62(4):653-667, 2001.  12   LEIKE HUTTER  Name \u00b5-optimal policy Pareto optimality Balanced Pareto optimality Self-optimizing Strong asymptotic optimality Weak asymptotic optimality  Issue/Comment requires to know the true environment \u00b5 in advance trivial (Theorem 18) dependent on UTM (Corollary 13 and Corollary 14) does not apply to MCCS LSC impossible (Lattimore and Hutter, 2011, Thm. 8) BayesExp (Lattimore, 2013, Ch. 5), but not AIXI (Orseau, 2010)  Table 1: Proposed notions of optimality (Hutter, 2002; Orseau, 2010; Lattimore and Hutter, 2011) and their issues. Weak asymptotic optimality stands out to be the only possible nontrivial optimality notion.  timality) is highly subjective, because it depends on the choice of the UTM: AIXI is not balanced Pareto optimal with respect to all universal mixtures. Moreover, according to Corollary 15, any com- putable policy is nearly balanced Pareto optimal, save some \u03b5 > 0. For finite lifetime discounting, there are UTMs such that every policy has maximal intelligence (Theorem 6). The self-optimizing theorem (Hutter, 2002, Thm. 4 & Thm. 7) is not applicable to the class of all computable envi- ronments MCCS LSC that we consider here, since this class does not allow for self-optimizing policies. Therefore no nontrivial and non-subjective optimality results for AIXI remain (see Table 1). We have to regard AIXI as a relative theory of intelligence, dependent on the choice of the UTM (Sune- hag and Hutter, 2014).  (cid:80)t  (cid:0)V \u2217  The underlying problem is that a discounting Bayesian agent such as AIXI does not have enough time to explore sufficiently; exploitation has to start as soon as possible. In the beginning the agent does not know enough about its environment and therefore relies heavily on its prior. Lack of ex- ploration then retains the prior\u2019s biases. This fundamental problem can be alleviated by adding an extra exploration component. Lattimore (2013) defines BayesExp, a weakly asymptotically op- timal policy \u03c0 that converges (independent of the UTM) to the optimal value in Ces`aro mean: 1 t  \u03bd (\u00e6 <k)(cid:1) \u2192 0 as t \u2192 \u221e \u03bd-almost surely for all \u03bd \u2208 MCCS LSC .  k=1 But it is not clear that weak asymptotic optimality is a good optimality criterion. For example, weak asymptotic optimality can be achieved by navigating into traps (parts of the environment with a simple optimal policy but possibly very low rewards that cannot be escaped). Furthermore, to be weakly asymptotically optimal requires an excessive amount of exploration: BayesExp needs to take exploratory actions that it itself knows to very likely be extremely costly or dangerous. This leaves us with the following open question: What are good optimality criteria for generally intelligent agents (Hutter, 2009, Sec. 5)?  \u03bd (\u00e6 <k) \u2212 V \u03c0  References  Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. Tech-  nical Report cs.AI/0004001, 2000. http://arxiv.org/abs/cs.AI/0004001.  Marcus Hutter. New error bounds for Solomonoff prediction. Journal of Computer and System  Sciences, 62(4):653-667, 2001. BAD UNIVERSAL PRIORS AND NOTIONS OF OPTIMALITY  Marcus Hutter. Self-optimizing and Pareto-optimal policies in general environments based on  Bayes-mixtures. In Computational Learning Theory, pages 364-379. Springer, 2002.  Marcus Hutter. Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Prob-  Marcus Hutter. Open problems in universal induction & intelligence. Algorithms, 3(2):879-906,  ability. Springer, 2005.  2009.  versity, 2013.  pages 368-382. Springer, 2011.  Science, 519:140-154, 2014.  Tor Lattimore. Theory of General Reinforcement Learning. PhD thesis, Australian National Uni-  Tor Lattimore and Marcus Hutter. Asymptotically optimal agents. In Algorithmic Learning Theory,  Tor Lattimore and Marcus Hutter. General time consistent discounting. Theoretical Computer  Shane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. Minds  & Machines, 17(4):391-444, 2007.  Shane Legg and Joel Veness. An approximation of the universal intelligence measure. In Algo- rithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, pages 236-249. Springer, 2013.  Jan Leike and Marcus Hutter. On the computability of AIXI. In Uncertainty in Artificial Intelli-  gence, 2015.  2010.  Ming Li and Paul M. B. Vit\u00b4anyi. An Introduction to Kolmogorov Complexity and Its Applications.  Texts in Computer Science. Springer, 3rd edition, 2008.  Markus M\u00a8uller. Stationary algorithmic probability. Theoretical Computer Science, 411(1):113-130,  Laurent Orseau. Optimality issues of universal greedy agents with static priors.  In Algorithmic  Learning Theory, pages 345-359. Springer, 2010.  Laurent Orseau. Asymptotic non-learnability of universal agents with computable horizon func-  tions. Theoretical Computer Science, 473:149-156, 2013.  Laurent Orseau. Universal knowledge-seeking agents. Theoretical Computer Science, 519:127-  139, 2014.  Laurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for  stochastic environments. In Algorithmic Learning Theory, pages 158-172. Springer, 2013.  Ray Solomonoff. A formal theory of inductive inference. Parts 1 and 2. Information and Control, 7  (1):1-22 and 224-254, 1964.  Ray Solomonoff. Complexity-based induction systems: Comparisons and convergence theorems.  IEEE Transactions on Information Theory, 24(4):422-432, 1978. LEIKE HUTTER  Peter Sunehag and Marcus Hutter. Optimistic agents are asymptotically optimal. In Australasian  Joint Conference on Artificial Intelligence, pages 15-26. Springer, 2012.  Peter Sunehag and Marcus Hutter. Intelligence as inference or forcing Occam on the world. In  Artificial General Intelligence, pages 186-195. Springer, 2014.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,  Cambridge, MA, 1998.  Ian Wood, Peter Sunehag, and Marcus Hutter. (Non-)equivalence of universal priors. In Solomonoff  85th Memorial Conference, pages 417-425. Springer, 2011. BAD UNIVERSAL PRIORS AND NOTIONS OF OPTIMALITY  "}, "Learning with Square Loss: Localization through Offset Rademacher Complexity": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Learning with Square Loss: Localization through Offset Rademacher Complexity", "abstract": "We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of \\citepRakSriTsy15 for the bounded case while also providing guarantees without the boundedness assumption.", "pdf_url": "http://proceedings.mlr.press/v40/Liang15.pdf", "keywords": [], "reference": "Audibert, J. (2007). Progressive mixture rules are deviation suboptimal. Advances in Neural Infor-  mation Processing Systems, 20(2).  of Statistics, 33(4):1497-1537.  Bartlett, P., Bousquet, O., and Mendelson, S. (2005). Local rademacher complexities. The Annals  Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic  theory of independence. Oxford University Press.  Bousquet, O. (2002). Concentration inequalities and empirical processes theory applied to the  analysis of learning algorithms. PhD thesis, Ecole Polytechnique.  Bousquet, O., Koltchinskii, V., and Panchenko, D. (2002). Some local measures of complexity of convex hulls and generalization bounds. In Springer, editor, Computational Learning Theory, pages 164-171, Sydney, Australia.  Dai, D., Rigollet, P., and Zhang, T. (2012). Deviation optimal learning using greedy Q-aggregation.  Gin\u00b4e, E. and Zinn, J. (1984). Some limit theorems for empirical processes. Annals of Probability,  Annals of Statistics.  12(4):929-989.  Koltchinskii, V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: \u00b4Ecole d\u2019 \u00b4Et\u00b4e de Probabilit\u00b4es de Saint-Flour XXXVIII-2008, volume 2033. Springer.  Koltchinskii, V. and Panchenko, D. (2000). Rademacher processes and bounding the risk of function  learning. High Dimensional Probability, II:443-459.  Lecu\u00b4e, G. and Mendelson, S. (2009). Aggregation via empirical risk minimization. Probability  Theory and Related Fields, 145(3):591-613.  Lecu\u00b4e, G. and Mendelson, S. (2013). Learning subgaussian classes: Upper and minimax bounds.  arXiv preprint arXiv:1305.4825.  Mendelson, S. (2002). Improving the sample complexity using global data. Information Theory,  IEEE Transactions on, 48(7):1977-1991.  Mendelson, S. (2003). A few notes on statistical learning theory. In Mendelson, S. and Smola, A. J., editors, Advanced Lectures in Machine Learning, LNCS 2600, Machine Learning Summer School 2002, Canberra, Australia, February 11-22, pages 1-40. Springer.  Mendelson, S. (2014a). Learning without Concentration. In Conference on Learning Theory.  Mendelson, S. (2014b). Learning without Concentration for General Loss Functions. ArXiv e-prints.  13   OFFSET RADEMACHER COMPLEXITY  We gratefully acknowledge the support of NSF under grants CAREER DMS-0954737 and CCF- 1116928.  Acknowledgments  References  Audibert, J. (2007). Progressive mixture rules are deviation suboptimal. Advances in Neural Infor-  mation Processing Systems, 20(2).  of Statistics, 33(4):1497-1537.  Bartlett, P., Bousquet, O., and Mendelson, S. (2005). Local rademacher complexities. The Annals  Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic  theory of independence. Oxford University Press.  Bousquet, O. (2002). Concentration inequalities and empirical processes theory applied to the  analysis of learning algorithms. PhD thesis, Ecole Polytechnique.  Bousquet, O., Koltchinskii, V., and Panchenko, D. (2002). Some local measures of complexity of convex hulls and generalization bounds. In Springer, editor, Computational Learning Theory, pages 164-171, Sydney, Australia.  Dai, D., Rigollet, P., and Zhang, T. (2012). Deviation optimal learning using greedy Q-aggregation.  Gin\u00b4e, E. and Zinn, J. (1984). Some limit theorems for empirical processes. Annals of Probability,  Annals of Statistics.  12(4):929-989.  Koltchinskii, V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: \u00b4Ecole d\u2019 \u00b4Et\u00b4e de Probabilit\u00b4es de Saint-Flour XXXVIII-2008, volume 2033. Springer.  Koltchinskii, V. and Panchenko, D. (2000). Rademacher processes and bounding the risk of function  learning. High Dimensional Probability, II:443-459.  Lecu\u00b4e, G. and Mendelson, S. (2009). Aggregation via empirical risk minimization. Probability  Theory and Related Fields, 145(3):591-613.  Lecu\u00b4e, G. and Mendelson, S. (2013). Learning subgaussian classes: Upper and minimax bounds.  arXiv preprint arXiv:1305.4825.  Mendelson, S. (2002). Improving the sample complexity using global data. Information Theory,  IEEE Transactions on, 48(7):1977-1991.  Mendelson, S. (2003). A few notes on statistical learning theory. In Mendelson, S. and Smola, A. J., editors, Advanced Lectures in Machine Learning, LNCS 2600, Machine Learning Summer School 2002, Canberra, Australia, February 11-22, pages 1-40. Springer.  Mendelson, S. (2014a). Learning without Concentration. In Conference on Learning Theory.  Mendelson, S. (2014b). Learning without Concentration for General Loss Functions. ArXiv e-prints. LIANG RAKHLIN SRIDHARAN  Mendelson, S. (2015). On aggregation for heavy-tailed classes. Preprint.  Rakhlin, A. and Sridharan, K. (2014). Online non-parametric regression. In Conference on Learning  Theory.  Rakhlin, A., Sridharan, K., and Tsybakov, A. (2015). Empirical entropy, minimax regret and mini-  max risk. Bernoulli. Forthcoming.  Yang, Y. and Barron, A. (1999). Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, 27(5):1564-1599. OFFSET RADEMACHER COMPLEXITY  "}, "Achieving All with No Parameters: AdaNormalHedge": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Achieving All with No Parameters: AdaNormalHedge", "abstract": "We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm (Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. (2009) and Chernov and Vovk (2010). Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen (2014) on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "pdf_url": "http://proceedings.mlr.press/v40/Luo15.pdf", "keywords": ["expert algorithm", "NormalHedge", "adaptivity", "unknown competitors", "time-varying competitors", "first order bounds", "sleeping expert", "adaptive regret", "shifting regret"], "reference": "Dmitry Adamskiy, Wouter M Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at adap-  tive regret. In Algorithmic Learning Theory, pages 290-304, 2012.  Avrim Blum. Empirical support for Winnow and Weighted-Majority algorithms: Results on a cal-  endar scheduling domain. Machine Learning, 26(1):5-23, 1997.  Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning  Research, 8:1307-1324, 2007.  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3:363-396, 2003.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and feels no regret). In Advances in Neural Information Processing Systems 25, 2012.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. In Ad-  vances in Neural Information Processing Systems 22, 2009.  Alexey Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In  Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, 2010.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research, 15:1281-1316, 2014.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, August 1997.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, pages 334-343, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In  Proceedings of the 27th Annual Conference on Learning Theory, 2014.  Elad Hazan and C. Seshadhri. Adaptive algorithms for online decision problems.  In Electronic  Colloquium on Computational Complexity (ECCC), volume 14, 2007.  David P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a  decision tree. Machine Learning, 27(1):51-68, April 1997.  13   ADAPTIVE NORMALHEDGE  References  Dmitry Adamskiy, Wouter M Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at adap-  tive regret. In Algorithmic Learning Theory, pages 290-304, 2012.  Avrim Blum. Empirical support for Winnow and Weighted-Majority algorithms: Results on a cal-  endar scheduling domain. Machine Learning, 26(1):5-23, 1997.  Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning  Research, 8:1307-1324, 2007.  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3:363-396, 2003.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and feels no regret). In Advances in Neural Information Processing Systems 25, 2012.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. In Ad-  vances in Neural Information Processing Systems 22, 2009.  Alexey Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In  Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, 2010.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research, 15:1281-1316, 2014.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, August 1997.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, pages 334-343, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In  Proceedings of the 27th Annual Conference on Learning Theory, 2014.  Elad Hazan and C. Seshadhri. Adaptive algorithms for online decision problems.  In Electronic  Colloquium on Computational Complexity (ECCC), volume 14, 2007.  David P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a  decision tree. Machine Learning, 27(1):51-68, April 1997. LUO SCHAPIRE  Mark Herbster and Manfred Warmuth. Tracking the best expert.  In Proceedings of the Twelfth  International Conference on Machine Learning, pages 286-294, 1995.  Mark Herbster and Manfred Warmuth. Tracking the best expert. Machine Learning, 32(2):151-178,  1998.  Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. The Journal of Machine  Learning Research, 1:281-309, 2001.  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza- tion: Competing with dynamic comparators. In The 18th International Conference on Artificial Intelligence and Statistics, 2015.  Wouter M Koolen, Dmitry Adamskiy, and Manfred K Warmuth. Putting bayes to sleep. In Advances  in Neural Information Processing Systems 25, pages 135-143, 2012.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.  Information and  Computation, 108:212-261, 1994.  Haipeng Luo and Robert E. Schapire. A Drifting-Games Analysis for Online Learning and Appli-  cations to Boosting. In Advances in Neural Information Processing Systems 27, 2014.  H Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert In Proceedings of the 27th Annual  spaces: Minimax algorithms and normal approximations. Conference on Learning Theory, 2014.  Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information  Processing Systems 26, pages 1806-1814, 2013.  Francesco Orabona.  Simultaneous model selection and optimization through parameter-free  stochastic learning. In Advances in Neural Information Processing Systems 27, 2014.  Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In  Advances in Neural Information Processing Systems 27, 2014.  Robert E. Schapire. Drifting games. Machine Learning, 43(3):265-291, June 2001.  Matthew Streeter and Brendan Mcmahan. No-regret algorithms for unconstrained online convex In Advances in Neural Information Processing Systems 25, pages 2402-2410,  optimization. 2012.  Tim Van Erven, Wojciech Kotlowski, and Manfred K Warmuth. Follow the leader with dropout  perturbations. In Proceedings of the 27th Annual Conference on Learning Theory, 2014.  V. G. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, April 1998.  Manfred K. Warmuth and Wouter M. Koolen. Open problem: Shifting experts on easy data. In  Proceedings of the 27th Annual Conference on Learning Theory, 2014. ADAPTIVE NORMALHEDGE  Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. Context tree weighting: A sequen- tial universal source coding procedure for FSMX sources. In Proceedings 1993 IEEE Interna- tional Symposium on Information Theory, page 59, 1993.  Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. The context tree weighting method:  Basic properties. IEEE Transactions on Information Theory, 41(3):653-664, 1995. LUO SCHAPIRE  "}, "Lower and Upper Bounds on the Generalization of  Stochastic   Exponentially Concave Optimization": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Lower and Upper Bounds on the Generalization of  Stochastic   Exponentially Concave Optimization", "abstract": "In this paper we derive \\textithigh probability lower and upper  bounds on the excess risk of stochastic optimization of exponentially concave loss functions. Exponentially  concave loss functions encompass several fundamental problems in machine learning such as squared loss in linear regression, logistic loss in classification, and negative logarithm loss in portfolio management.  We demonstrate an O(d \\log T/T) upper bound on the excess risk of stochastic online Newton step algorithm, and an O(d/T) lower bound on the excess risk of any stochastic  optimization method for \\textitsquared loss, indicating that the obtained upper bound  is  optimal up to a logarithmic factor. The analysis of upper bound is based on recent advances in  concentration inequalities for bounding self-normalized martingales, which is interesting by its own right, and the proof technique used to achieve the lower bound is a probabilistic method and relies on an information-theoretic minimax analysis.", "pdf_url": "http://proceedings.mlr.press/v40/Mahdavi15.pdf", "keywords": ["stochastic optimization", "exponentially concave losses", "excess risk"], "reference": "Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320, 2011.  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural  Information Processing Systems, pages 41-48, 2008.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Victor H de la Pe\u02dcna and Guodong Pang. Exponential inequalities for self-normalized processes with  applications. Electronic Communications in Probability, 14:372-381, 2009.  Victor H de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: Exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 32(3):1902- 1933, 2004.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118,  1975.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learn- ing Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic In Proceedings of The 27th Conference on Learning Theory, COLT and online optimization. 2014, Barcelona, Spain, June 13-15, 2014, pages 197-209, 2014. URL http://jmlr.org/ proceedings/papers/v35/hazan14a.html.  Anatoli Juditsky and Yuri Nesterov. Primal-dual subgradient methods for minimizing uniformly  convex functions. Technical report, 2010.  13   GENERALIZATION OF STOCHASTIC EXPONENTIALLY CONCAVE OPTIMIZATION  References  Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320, 2011.  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural  Information Processing Systems, pages 41-48, 2008.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Victor H de la Pe\u02dcna and Guodong Pang. Exponential inequalities for self-normalized processes with  applications. Electronic Communications in Probability, 14:372-381, 2009.  Victor H de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: Exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 32(3):1902- 1933, 2004.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118,  1975.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learn- ing Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic In Proceedings of The 27th Conference on Learning Theory, COLT and online optimization. 2014, Barcelona, Spain, June 13-15, 2014, pages 197-209, 2014. URL http://jmlr.org/ proceedings/papers/v35/hazan14a.html.  Anatoli Juditsky and Yuri Nesterov. Primal-dual subgradient methods for minimizing uniformly  convex functions. Technical report, 2010. MAHDAVI ZHANG JIN  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex pro- gramming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Tomer Koren. Open problem: Fast stochastic exp-concave optimization. In Proceedings of the 26th  Annual Conference on Learning Theory, pages 1073-1075, 2013.  Mehrdad Mahdavi and Rong Jin. Excess risk bounds for exponentially concave losses. ArXiv  e-prints, arXiv:1401.4566, 2014.  Wiley & Sons Ltd, 1983.  A. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization. John  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course, volume 87 of Applied  optimization. Kluwer Academic Publishers, 2004.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Ohad Shamir. The sample complexity of learning linear predictors with the squared loss. ArXiv  e-prints, arXiv:1406.5143, 2014.  "}, "Correlation Clustering with Noisy Partial Information": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Correlation Clustering with Noisy Partial Information", "abstract": "In this paper, we propose and study a semi-random model for the Correlation Clustering problem on arbitrary graphs G. We give two approximation algorithms for Correlation Clustering instances from this model. The first algorithm finds a solution of value (1+ \u03b4)\\mathrmopt-cost + O_\u03b4(n\\log^3 n) with high probability, where \\mathrmopt-cost is the value of the optimal solution (for every \u03b4> 0). The second algorithm finds the ground truth clustering with an arbitrarily small classification error \u03b7(under some additional assumptions on the instance).", "pdf_url": "http://proceedings.mlr.press/v40/Makarychev15.pdf", "keywords": ["Correlation clustering", "semi-random model", "polynomial-time approximation scheme"], "reference": "Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking  and clustering. J. ACM, 55(5):23:1-23:27, November 2008.  G D Bader and C W Hogue. An automated method for finding molecular complexes in large protein  interaction networks. BMC Bioinformatics, 4(1):2, 2003.  Maria-Florina Balcan and Mark Braverman. Finding low error clusterings. In Conference on Learn-  ing Theory (COLT), 2009.  (1-3):89-113, 2004.  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning, 56  Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Journal of  Computational Biology, 6(3/4):281-297, 1999.  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. J. Algo-  rithms, 19:204-234, September 1995.  Steven Kay Butler. Eigenvalues and structures of graphs. PhD thesis, UC San Diego, 2008.  Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative informa-  tion. J. Comput. Syst. Sci., 71(3):360-383, October 2005.  Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near opti- mal LP rounding algorithm for correlation clustering on complete and complete k-partite graphs. CoRR, abs/1412.0681, 2014.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2204-2212. Curran Associates, Inc., 2012.  Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via  convex optimization. Journal of Machine Learning Research, 15:2213-2238, 2014.  Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical Computer Science, 361:172-187, 2006. Approximation and Online Algorithms.  Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering be- yond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP \u201909, pages 19-27, 2009.  Uriel Feige and Joe Kilian. Heuristics for finding large independent sets, with applications to col- oring semi-random graphs. In Proceedings of Symposium on Foundations of Computer Science, pages 674-683, 1998.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Struct. Algorithms, 16:195-208, March 2000.  13   CORRELATION CLUSTERING  References  Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking  and clustering. J. ACM, 55(5):23:1-23:27, November 2008.  G D Bader and C W Hogue. An automated method for finding molecular complexes in large protein  interaction networks. BMC Bioinformatics, 4(1):2, 2003.  Maria-Florina Balcan and Mark Braverman. Finding low error clusterings. In Conference on Learn-  ing Theory (COLT), 2009.  (1-3):89-113, 2004.  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning, 56  Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Journal of  Computational Biology, 6(3/4):281-297, 1999.  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. J. Algo-  rithms, 19:204-234, September 1995.  Steven Kay Butler. Eigenvalues and structures of graphs. PhD thesis, UC San Diego, 2008.  Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative informa-  tion. J. Comput. Syst. Sci., 71(3):360-383, October 2005.  Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near opti- mal LP rounding algorithm for correlation clustering on complete and complete k-partite graphs. CoRR, abs/1412.0681, 2014.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2204-2212. Curran Associates, Inc., 2012.  Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via  convex optimization. Journal of Machine Learning Research, 15:2213-2238, 2014.  Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical Computer Science, 361:172-187, 2006. Approximation and Online Algorithms.  Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering be- yond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP \u201909, pages 19-27, 2009.  Uriel Feige and Joe Kilian. Heuristics for finding large independent sets, with applications to col- oring semi-random graphs. In Proceedings of Symposium on Foundations of Computer Science, pages 674-683, 1998.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Struct. Algorithms, 16:195-208, March 2000. MAKARYCHEV MAKARYCHEV VIJAYARAGHAVAN  Santo Fortunato. Community detection in graphs. Physics Reports, 486:75-174, 2010.  David A Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100-118,  1975.  Naveen Garg, Vijay V. Vazirani, and Mihalis Yannakakis. Approximate max-\ufb02ow min-(multi)cut theorems and their applications. In Proceedings of Symposium on Theory of Computing, pages 698-707, 1993.  M. Girvan and M. E. J. Newman. Community structure in social and biological networks. Proceed-  ings of the National Academy of Sciences, 99(12):7821-7826, 2002.  Amir Globerson, Tim Roughgarden, David Sontag, and Cafer Yildirim. Tight error bounds for  structured prediction. CoRR, abs/1409.5834, 2014.  Brian Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks.  Phys. Rev. E, 83:016107, Jan 2011.  Alexandra Kolla, Konstantin Makarychev, and Yury Makarychev. How to play unique games against In Proceeding of  a semi-random adversary: Study of semi-random models of unique games. Symposium on Foundations of Computer Science, pages 443-452, 2011.  Ravi Kumar, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins. Trawling the web  for emerging cyber-communities. In Computer Networks, pages 1481-1493, 1999.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation al- In Proceedings of Symposium on Theory of  gorithms for semi-random partitioning problems. Computing, pages 367-384, 2012.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Sorting noisy data with partial information. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, pages 515-528, 2013.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Constant factor ap- proximation for balanced cut in the random PIE model. In Proceedings of Symposium on Theory of Computing, 2014.  Claire Mathieu and Warren Schudy. Correlation clustering with noisy input.  In Proceedings of  Symposium on Discrete Algorithms, pages 712-728, 2010.  Mark Newman, Albert-Laszlo Barabasi, and Duncan J. Watts. The Structure and Dynamics of Networks: (Princeton Studies in Complexity). Princeton University Press, Princeton, NJ, USA, 2006.  Sebastian Nowozin and Christoph H. Lampert. Structured learning and prediction in computer  vision. Foundations and Trends in Computer Graphics and Vision, 6(34):185-365, 2010.  Noah A. Smith. Linguistic Structure Prediction. Synthesis Lectures on Human Language Tech-  nologies. Morgan and Claypool, May 2011. CORRELATION CLUSTERING  Chaitanya Swamy. Correlation clustering: Maximizing agreements via semidefinite programming.  In Proceedings of Symposium on Discrete Algorithms, pages 526-527, 2004.  Ben Taskar, Ming-fai Wong, Pieter Abbeel, and Daphne Koller. Link prediction in relational data. In S. Thrun, L. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems (NIPS) 16. Cambridge, MA: MIT Press, 2004.  Julian Yarkony, Alexander T. Ihler, and Charless C. Fowlkes. Fast planar correlation clustering for image segmentation. In 12th European Conference on Computer Vision (ECCV), pages 568-581, 2012.  Xinyuan Zhao, Defeng Sun, and Kim-Chuan Toh. A newton-cg augmented lagrangian method for  semidefinite programming. SIAM J. Optimization, 20:1737-1765, 2010.  "}, "Online Density Estimation of Bradley-Terry Models": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Online Density Estimation of Bradley-Terry Models", "abstract": "We consider an online density estimation problem for the Bradley-Terry model, where each model parameter defines the probability of a match result between any pair in a set of n teams. The problem is hard because the loss function (i.e., the negative log-likelihood function in our problem setting) is not convex. To avoid the non-convexity, we can change parameters so that the loss function becomes convex with respect to the new parameter. But then the radius K of the reparameterized domain may be infinite, where K depends on the outcome sequence. So we put a mild assumption that guarantees that K is finite. We can thus employ standard online convex optimization algorithms, namely OGD and ONS, over the reparameterized domain, and get regret bounds O(n^\\frac12(\\ln K)\\sqrtT) and O(n^\\frac32K\\ln T), respectively, where T is the horizon of the game. The bounds roughly means that OGD is better when K is large while ONS is better when K is small. But how large can K be? We show that K can be as large as \u0398(T^n-1), which implies that the worst case regret bounds of OGD and ONS are O(n^\\frac32\\sqrtT\\ln T) and \\tildeO(n^\\frac32(T)^n-1), respectively. We then propose a version of Follow the Regularized Leader, whose regret bound is close to the minimum of those of OGD and ONS. In other words, our algorithm is competitive with both for a wide range of values of K. In particular, our algorithm achieves the worst case regret bound O(n^\\frac52T^\\frac13 \\ln T), which is slightly better than OGD with respect to T. In addition, our algorithm works without the knowledge K, which is a practical advantage.", "pdf_url": "http://proceedings.mlr.press/v40/Matsumoto15.pdf", "keywords": ["online density estimation", "Bradley-Terry model", "ranking"], "reference": "384-414, 2010.  Katy S. Azoury and Manfred K. Warmuth. Relative Loss Bounds for On-Line Density Estimation  with the Exponential Family of Distributions. Machine Learning, 43:211-246, 2001.  Francis Bach. Self-concordant analysis for logistic regression. Electric Journal of Statisitics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15:595627, 2014.  Stephan Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Lester R. Ford, Jr. Solution of a ranking problem from binary comparisons. American Mathematical  Monthly, 64(8):28\u2014-33, 1957.  Yoav Freund. Predicting a binary sequence almost as well as the optimal biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT \u201996), pages 89-98, 1996.  12   MATSUMOTO HATANO TAKIMOTO  Bayesian Approach For online density estimation problems, Bayesian approaches have been quite effective for obtaining O(ln T ) regret bounds (e.g., Freund (1996); Azoury and Warmuth (2001); Kotlowski et al. (2010)). The typical Bayesian approach assumes a prior distribution over parameters and predicts the average of parameters w.r.t. the posterior distribution. Unfortunately, unlike the exponential family, it is not straightforward to obtain a regret bound for Bradly-Terry and logistic models using this approach. Note that FTRL has a natural Bayesian interpretation that, FTRL predicts the maximum a posterior estimate of parameters w.r.t. the posterior distribution, where the prior distribution is defined as the likelihood of the virtual 2\u03bb matches between each two players. It is an interesting open question whether a Bayesian approach achieves O(ln T ) regret bound for these models.  6. Conclusion  We considered the online density estimation problem for Bradley-Terry models. We derived match- ing upper and lower bounds of the radius, analyzed FTRL with virtual match regularization, and showed better regret bounds than standard algorithms.  There are some interesting open questions to explore. An obvious open problem is to obtain better upper/lower regret bounds for Bradley-Terry models. It might be possible to design Bayesian- based algorithms for Bradley-Terry models as well. In addition, minimax analyses for Bradley-Terry models would be an interesting approach.  Acknowledgments  We thank anonymous reviewers and Hiroe Inoue for many helpful comments. Hatano is grate- ful to the supports from JSPS KAKENHI Grant Number 25330261 and CORE Project Grant of Microsoft Research Asia. Takimoto is grateful to the supports from JSPS KAKENHI Grant Num- ber 15H02667. In addition, both authors acknowledge the support from MEXT KAKENHI Grant Number 24106010 (the ELC project).  References  384-414, 2010.  Katy S. Azoury and Manfred K. Warmuth. Relative Loss Bounds for On-Line Density Estimation  with the Exponential Family of Distributions. Machine Learning, 43:211-246, 2001.  Francis Bach. Self-concordant analysis for logistic regression. Electric Journal of Statisitics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15:595627, 2014.  Stephan Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Lester R. Ford, Jr. Solution of a ranking problem from binary comparisons. American Mathematical  Monthly, 64(8):28\u2014-33, 1957.  Yoav Freund. Predicting a binary sequence almost as well as the optimal biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT \u201996), pages 89-98, 1996. ONLINE DENSITY ESTIMATION OF BRADLEY-TERRY MODELS  Elad Hazan. The convex optimization approach to regret minimization. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning, chapter 10, pages 287-304. MIT Press, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Levy. Logistic Regression: Tight Bounds for Stochastic and Online Optimization. In Proceedings of the 27th Conference on Learning Theory (COLT\u201914), pages 197-209, 2014.  David R. Hunter. MM algorithms for generalized Bradley-Terry models. Annals of Statistics, 32  (1):384-406, 2004.  Wojciech Kotlowski, Peter Gr\u00a8unwald, and Steven de Rooij. Following the Flattened Leader. In Proceedings of 23th Annual Conference on Learning Theory (COLT 2010), pages 106-118, 2010.  John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall, 1995.  H. Brendan McMahan and Matthew J. Streeter. Open Problem: Better Bounds for Online Logistic Regression. In Proceeding of the 25th Annual Conference on Learning Theory (COLT 2012), pages 44.1-44.3, 2012.  Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends  in Machine Learning, 4(2):107-194, 2011.  Yu. M. Shtar\u2019kov. Universal Sequential Coding of Single Messages. Problems of Information  Transmission, 23(3):175-186, 1987.  Eiji Takimoto and Manfred K. Warmuth. The Minimax Strategy for Gaussian Density Estimation. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory (COLT 2000), pages 100-106, 2000.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2003), pages 928-936, 2003.  "}, "First-order regret bounds for combinatorial semi-bandits": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "First-order regret bounds for combinatorial semi-bandits", "abstract": "We consider the problem of online combinatorial optimization under semi-bandit feedback, where a learner has to repeatedly pick actions from a combinatorial decision set in order to minimize the total losses associated with its decisions. After making each decision, the learner observes the losses associated with its action, but not other losses. For this problem, there are several learning algorithms that guarantee that the learner\u2019s expected regret grows as \\widetildeO(\\sqrtT) with the number of rounds T. In this paper, we propose an algorithm that improves this scaling to \\widetildeO(\\sqrtL_T^*), where L_T^* is the total loss of the best action. Our algorithm is among the first to achieve such guarantees in a partial-feedback scheme, and the first one to do so in a combinatorial setting.", "pdf_url": "http://proceedings.mlr.press/v40/Neu15.pdf", "keywords": ["online learning", "online combinatorial optimization", "semi-bandit feedback", "follow the perturbed leader", "improvements for small losses", "first-order bounds"], "reference": "J. Abernethy, E. Hazan, and A. Rakhlin.  Interior-point methods for full-information and bandit  online learning. Information Theory, IEEE Transactions on, 58(7):4164-4175, July 2012.  J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M.-F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 807-823. JMLR.org, 2014.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and Gy. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In J. L. Balc\u00b4azar, P. M. Long, and F. Stephan, editors, Proceedings of the 17th International Conference on Algorithmic Learning Theory (ALT 2006), volume 4264 of Lecture Notes in Computer Science, pages 229-243, Berlin, Heidelberg, October 7-10 2006. Springer. ISBN 978-3-540-46649-9.  J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceed-  ings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Jour-  nal of Machine Learning Research, 11:2635-2686, 2010.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Regret in online combinatorial optimization. Mathematics  of Operations Research, 39:31-45, 2014.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM J. Comput., 32(1):48-77, 2002a. ISSN 0097-5397.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64:48-75, 2002b. doi: doi:10.1006/jcss.2001.1795.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  New York, NY, USA, 2006.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. In S. Dasgupta and A. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, pages 237-246. Omnipress, June 18-21 2009.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,  78:1404-1422, 2012.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. In Proceedings of the 18th Annual Conference on Learning Theory (COLT-2005), pages 217-232. Springer, 2005.  W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and ap- In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International plications. Conference on Machine Learning (ICML 2013), volume 28 of JMLR Workshop and Conference Proceedings, pages 151-159, 2013.  13   FIRST-ORDER REGRET BOUNDS FOR COMBINATORIAL SEMI-BANDITS  References  J. Abernethy, E. Hazan, and A. Rakhlin.  Interior-point methods for full-information and bandit  online learning. Information Theory, IEEE Transactions on, 58(7):4164-4175, July 2012.  J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M.-F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 807-823. JMLR.org, 2014.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and Gy. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In J. L. Balc\u00b4azar, P. M. Long, and F. Stephan, editors, Proceedings of the 17th International Conference on Algorithmic Learning Theory (ALT 2006), volume 4264 of Lecture Notes in Computer Science, pages 229-243, Berlin, Heidelberg, October 7-10 2006. Springer. ISBN 978-3-540-46649-9.  J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceed-  ings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Jour-  nal of Machine Learning Research, 11:2635-2686, 2010.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Regret in online combinatorial optimization. Mathematics  of Operations Research, 39:31-45, 2014.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM J. Comput., 32(1):48-77, 2002a. ISSN 0097-5397.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64:48-75, 2002b. doi: doi:10.1006/jcss.2001.1795.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  New York, NY, USA, 2006.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. In S. Dasgupta and A. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, pages 237-246. Omnipress, June 18-21 2009.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,  78:1404-1422, 2012.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. In Proceedings of the 18th Annual Conference on Learning Theory (COLT-2005), pages 217-232. Springer, 2005.  W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and ap- In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International plications. Conference on Machine Learning (ICML 2013), volume 28 of JMLR Workshop and Conference Proceedings, pages 151-159, 2013. NEU  L. Devroye, G. Lugosi, and G. Neu. Prediction by random-walk perturbation.  In S. I. Shalev- Shwartz, S., editor, Proceedings of the 25th Annual Conference on Learning Theory, pages 460- 473, 2013.  Y. Gai, B. Krishnamachari, and R. Jain. Combinatorial network optimization with unknown vari- ables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Trans- actions on Networking, 20(5):1466-1478, Oct 2012.  A. Gy\u00a8orgy, T. Linder, G. Lugosi, and Gy.. Ottucs\u00b4ak. The on-line shortest path problem under partial monitoring. Journal of Machine Learning Research, 8:2369-2403, 2007. ISSN 1532-4435.  J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the theory of games, 3:  97-139, 1957.  E. Hazan and S. Kale. Extracting certainty from uncertainty: regret bounded byvariation incosts.  Machine Learning, 80(2-3):165-188, 2010.  E. Hazan and S. Kale. Better algorithms for benign bandits. The Journal of Machine Learning  Research, 12:1287-1311, 2011.  M. Hutter and J. Poland. Prediction with expert advice by following the perturbed leader for general weights. In S. Ben-David, J. Case, and A. Maruoka, editors, Proceedings of the 15th International Conference on Algorithmic Learning Theory (ALT), volume 3244 of Lecture Notes in Computer Science, pages 279-293. Springer, 2004.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71:291-307, 2005.  T. Koc\u00b4ak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 613-621, 2014.  W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging structured concepts. In Proceedings of the  23rd Annual Conference on Learning Theory (COLT), pages 93-105, 2010.  B. Kveton, Z. Wen, A. Ashkan, and Cs. Szepesv\u00b4ari. Tight regret bounds for stochastic combinatorial  semi-bandits. In AISTATS, 2015.  G. Neu and G. Bart\u00b4ok. An efficient algorithm for learning with semi-bandit feedback. In S. Jain, R. Munos, F. Stephan, and T. Zeugmann, editors, Proceedings of the 24th International Confer- ence on Algorithmic Learning Theory, volume 8139 of Lecture Notes in Computer Science, pages 234-248. Springer, 2013.  J. Poland. FPL analysis for adaptive bandits.  In In 3rd Symposium on Stochastic Algorithms,  Foundations and Applications (SAGA\u201905), pages 58-69, 2005.  A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In S. I. Shalev-Shwartz, S., editor, Proceedings of the 25th Annual Conference on Learning Theory, pages 993-1019, 2013. FIRST-ORDER REGRET BOUNDS FOR COMBINATORIAL SEMI-BANDITS  S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize : From value to algorithms. In  Advances in Neural Information Processing Systems 25, pages 2150-2158. 2012.  A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in online optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 810-818, 2014.  G. Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD  thesis, Universit\u00b4e Paris-Sud, 2005.  T. Van Erven, M. Warmuth, and W. Kot\u0142owski. Follow the leader with dropout perturbations. In M.- F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 949-974. JMLR.org, 2014.  "}, "Norm-Based Capacity Control in Neural Networks": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Norm-Based Capacity Control in Neural Networks", "abstract": "We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.", "pdf_url": "http://proceedings.mlr.press/v40/Neyshabur15.pdf", "keywords": ["Feed-forward neural networks", "deep learning", "scale-sensitive capacity control"], "reference": "bridge University Press, 2009.  HAL-01098505, 2014.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Technical report,  Maria-Florina Balcan and Christopher Berlind. A new perspective on learning linear separators with large lqlp margins. Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 68-76, 2014.  Peter L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on information theory, 44(2):525-536, 1998.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, pages 463-482, 2003.  Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. Advances in neural information processing systems, pages 123-130, 2005.  Xavier Glorot Antoine Bordes and Yoshua Bengio. Deep sparse rectifier networks. AISTATS, 2011.  Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. STOC, 2014.  Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231-  283, 1981.  Sham M Kakade, Karthik Sridharan, and AmbujTewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. Advances in neural information processing systems, pages 793-800, 2009.  Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of  halfspaces. FOCS, pages 553-562, 2006.  Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the  generalization error of combined classifiers. Annals of Statistics, pages 1-50, 2002.  13   NORM-BASED CAPACITY CONTROL IN NEURAL NETWORKS  Acknowledgments  This research was partially supported by NSF grant IIS-1302662 and an Intel ICRI-CI award. We thank the COLT anonymous reviewers for pointing out an error in the statement of Lemma 15 and suggesting other corrections.  References  bridge University Press, 2009.  HAL-01098505, 2014.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Technical report,  Maria-Florina Balcan and Christopher Berlind. A new perspective on learning linear separators with large lqlp margins. Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 68-76, 2014.  Peter L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on information theory, 44(2):525-536, 1998.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, pages 463-482, 2003.  Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. Advances in neural information processing systems, pages 123-130, 2005.  Xavier Glorot Antoine Bordes and Yoshua Bengio. Deep sparse rectifier networks. AISTATS, 2011.  Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. STOC, 2014.  Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231-  283, 1981.  Sham M Kakade, Karthik Sridharan, and AmbujTewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. Advances in neural information processing systems, pages 793-800, 2009.  Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of  halfspaces. FOCS, pages 553-562, 2006.  Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the  generalization error of combined classifiers. Annals of Statistics, pages 1-50, 2002. NEYSHABUR TOMIOKA SREBRO  Wee Sun Lee, Peter L Bartlett, and Robert C Williamson. Efficient agnostic learning of neural Information Theory, IEEE Transactions on, 42(6):2118-2132,  networks with bounded fan-in. 1996.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. Advances in Neural Information Processing Systems, pages 855-863, 2014.  Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.  ICML, 2010.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q.V. Le, P. Nguyen, A. Senior, V. Van- ICASSP,  houcke, J. Dean, and G.E. Hinton. On rectified linear units for speech processing. 2013.  "}, "Cortical Learning via Prediction": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Cortical Learning via Prediction", "abstract": "What is the mechanism of learning in the brain?  Despite breathtaking advances in neuroscience,  and in machine learning, we do not seem close to an answer.  Using Valiant\u2019s neuronal model as a foundation, we introduce PJOIN (for \u201cpredictive join\"), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiant\u2019s conservative, formal model of cortical computation. Using PJOIN \u2014 and almost nothing else \u2014 we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiant\u2019s work). This algorithm relies crucially on prediction, and entails significant downward traffic (\u201cfeedback\") while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the first theoretical prediction of their essential role in learning.", "pdf_url": "http://proceedings.mlr.press/v40/Papadimitriou15.pdf", "keywords": ["Cortical algorithms", "neuroidal computation", "unsupervised learning", "prediction", "predictive join (PJOIN)"], "reference": "Patricia S. Churchland, V. S. Ramachandran, and Terrence J. Sejnowski. A critique of pure vision. In Cristof Koch and Joel L. Davis, editors, Large Scale Neuronal Theories of the Brain. MIT Press, Cambridge, MA, 1994.  Paul Erd\u02ddos and Alfred Renyi. On the evolution of random graphs. Publ. Math. Inst. Hungary. Acad.  Sci., 5:17-61, 1960.  Vitaly Feldman and Leslie G. Valiant. Experience-induced neural circuits that achieve high capacity.  Neural Computation, 21(10):2715-2754, 2009. doi: 10.1162/neco.2009.08-08-851.  Jeff Hawkins and Sandra Blakeslee. On Intelligence. Times Books, 2004. ISBN 0805074562.  G E Hinton and R R Salakhutdinov. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504-507, July 2006.  J. J. Hopfield. Neural networks and physical systems with emergent collective computational abil- ities. Proceedings of the National Academy of Sciences of the United States of America, 79(8): 2554-2558, 1982.  P. Kanerva.  Sparse Distributed Memory.  A Bradford book. MIT Press, 1988.  ISBN  9780262111324. URL http://books.google.com/books?id=I9tCr21-s-AC.  Victor A. F. Lamme and Pieter R. Roelfaema. Trends in Neurosciences, page 571.  Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to  document recognition. In Proceedings of the IEEE, volume 86, pages 2278-2324, 1998.  R.P.N. Rao and D.H. Ballard. Predictive coding in the visual cortex: a functional interpretation of  some extra-classical receptive-field effects. Nature Neuroscience, 2:79-87, 1999.  S Song, PJ Sj\u00a8ostr\u00a8om, M Reigl, S Nelson, and DB Chklovskii. Highly nonrandom features of  synaptic connectivity in local cortical circuits. PLoS Biology, 3(3), 2005.  Leslie G. Valiant. Circuits of the mind. Oxford University Press, 1994. ISBN 978-0-19-508926-4.  Leslie G. Valiant. A neuroidal architecture for cognitive computation. J. ACM, 47(5):854-882,  2000. doi: 10.1145/355483.355486.  Leslie G. Valiant. Memorization and association on a realistic neural model. Neural Computation,  17(3):527-555, 2005. doi: 10.1162/0899766053019890.  Leslie G. Valiant. A quantitative theory of neural computation. Biological Cybernetics, 95(3):  205-211, 2006. doi: 10.1007/s00422-006-0079-3.  Leslie G. Valiant. The hippocampus as a stable memory allocator for cortex. Neural Computation,  24(11):2873-2899, 2012. doi: 10.1162/NECO a 00357.  13   CORTICAL LEARNING VIA PREDICTION  References  Patricia S. Churchland, V. S. Ramachandran, and Terrence J. Sejnowski. A critique of pure vision. In Cristof Koch and Joel L. Davis, editors, Large Scale Neuronal Theories of the Brain. MIT Press, Cambridge, MA, 1994.  Paul Erd\u02ddos and Alfred Renyi. On the evolution of random graphs. Publ. Math. Inst. Hungary. Acad.  Sci., 5:17-61, 1960.  Vitaly Feldman and Leslie G. Valiant. Experience-induced neural circuits that achieve high capacity.  Neural Computation, 21(10):2715-2754, 2009. doi: 10.1162/neco.2009.08-08-851.  Jeff Hawkins and Sandra Blakeslee. On Intelligence. Times Books, 2004. ISBN 0805074562.  G E Hinton and R R Salakhutdinov. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504-507, July 2006.  J. J. Hopfield. Neural networks and physical systems with emergent collective computational abil- ities. Proceedings of the National Academy of Sciences of the United States of America, 79(8): 2554-2558, 1982.  P. Kanerva.  Sparse Distributed Memory.  A Bradford book. MIT Press, 1988.  ISBN  9780262111324. URL http://books.google.com/books?id=I9tCr21-s-AC.  Victor A. F. Lamme and Pieter R. Roelfaema. Trends in Neurosciences, page 571.  Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to  document recognition. In Proceedings of the IEEE, volume 86, pages 2278-2324, 1998.  R.P.N. Rao and D.H. Ballard. Predictive coding in the visual cortex: a functional interpretation of  some extra-classical receptive-field effects. Nature Neuroscience, 2:79-87, 1999.  S Song, PJ Sj\u00a8ostr\u00a8om, M Reigl, S Nelson, and DB Chklovskii. Highly nonrandom features of  synaptic connectivity in local cortical circuits. PLoS Biology, 3(3), 2005.  Leslie G. Valiant. Circuits of the mind. Oxford University Press, 1994. ISBN 978-0-19-508926-4.  Leslie G. Valiant. A neuroidal architecture for cognitive computation. J. ACM, 47(5):854-882,  2000. doi: 10.1145/355483.355486.  Leslie G. Valiant. Memorization and association on a realistic neural model. Neural Computation,  17(3):527-555, 2005. doi: 10.1162/0899766053019890.  Leslie G. Valiant. A quantitative theory of neural computation. Biological Cybernetics, 95(3):  205-211, 2006. doi: 10.1007/s00422-006-0079-3.  Leslie G. Valiant. The hippocampus as a stable memory allocator for cortex. Neural Computation,  24(11):2873-2899, 2012. doi: 10.1162/NECO a 00357. PAPADIMITRIOU VEMPALA  "}, "Partitioning Well-Clustered Graphs: Spectral Clustering Works!": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!", "abstract": "In this work we study the widely used \\emphspectral clustering algorithms, i.e. partition a graph into k clusters via (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix, and (2) partitioning embedded points via k-means algorithms.  We show that, for a wide class of \\emphwell-clustered graphs, spectral clustering algorithms can give a good approximation of the optimal clustering. To the best of our knowledge, it is the \\emphfirst theoretical analysis of spectral clustering algorithms for a wide family of graphs, even though such approach was proposed in the early 1990s and has comprehensive applications. We also give a nearly-linear time algorithm for partitioning well-clustered graphs, which is based on heat kernel embeddings and approximate nearest neighbor data structures.", "pdf_url": "http://proceedings.mlr.press/v40/Peng15.pdf", "keywords": [], "reference": "Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In 39th Annual ACM Symposium on Theory of Computing (STOC\u201907), pages 227-236, 2007.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games In 51st Annual IEEE Symposium on Foundations of Computer Science  and related problems. (FOCS\u201910), pages 563-572, 2010.  12   PENG SUN ZANETTI  Lemma 5.1 Let k = \u2126(log n) and \u03a5 = \u2126(k3). Then, there is t such that with high probability the embedding {xt(u)}u\u2208V [G] defined by (5.2) satisfies  1 2e  1 2e  \u00b7 (cid:107)F (u) \u2212 F (v)(cid:107)2 (cid:54)(cid:107)xt(u) \u2212 xt(v)(cid:107)2 (cid:54) (cid:107)F (u) \u2212 F (v)(cid:107)2 +  1 nc ,  \u00b7 (cid:107)F (u)(cid:107)2 (cid:54)(cid:107)xt(u)(cid:107)2 (cid:54) (cid:107)F (u)(cid:107)2 +  1 nc  for all u, v, where c is some constant. Moreover, these pairwise distances (cid:107)xt(u) \u2212 xt(v)(cid:107)2 for all {u, v} \u2208 E(G) can be approximated up to a constant factor in (cid:101)O (m) time.  Now we use the doubling argument to find a required t: We run the seeding and the grouping steps using the heat kernel embedding for all t of the form t = 2i, where i = 1, 2, . . . . , O(log n), and keep track of the best partition found so far. It is easy to see that the final partition after enumerating these t\u2019s gives a desired approximation. The overall description of our algorithm for large values of k is shown in Algorithm 2.  Algorithm 2 Clustering Algorithm  1: INPUT: (G, k) 2: for i = 1, . . . , k do 3: A(cid:48) i := \u2205 4: end for 5: COST(A1, . . . , Ak) := \u221e; 6: for t = 2, 4, 8, . . . , poly(n) do 7:  9: 10: end for 11: return (A(cid:48)  1, \u00b7 \u00b7 \u00b7 , A(cid:48) k)  Acknowledgments  8:  (c1, . . . , ck) \u2190 SEEDANDTRIM(k, xt) Compute a partition A1, . . . , Ak of V : for every v \u2208 V assign v to its nearest center ci using the \u03b5-NNS algorithm with \u03b5 = log k. If COST(A1, . . . , Ak) (cid:54) COST(A(cid:48)  i := Ai FOR 1 (cid:54) i (cid:54) k  1, . . . , A(cid:48)  k) SETA(cid:48)  Part of this work was done while He Sun and Luca Zanetti worked at Max Planck Institute for In- formatics, and He Sun was visiting the Simons Institute for the Theory of Computing, UC Berkeley. We are grateful to Luca Trevisan for insightful comments on an early version of our paper, and to Gary Miller for very helpful discussions about heat kernels on graphs.  References  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In 39th Annual ACM Symposium on Theory of Computing (STOC\u201907), pages 227-236, 2007.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games In 51st Annual IEEE Symposium on Foundations of Computer Science  and related problems. (FOCS\u201910), pages 563-572, 2010. PARTITIONING WELL-CLUSTERED GRAPHS: SPECTRAL CLUSTERING WORKS!  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding.  In 18th  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\u201907), pages 1027-1035, 2007.  Fan R. K. Chung. A local graph partitioning algorithm using heat kernel pagerank. Internet Math-  ematics, 6(3):315-330, 2009.  IEEE, 67(5):773-785, 1979.  Guy B. Coleman and Harry C. Andrews. Image segmentation by clustering. Proceedings of the  Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and Linden-  strauss. Random Structures & Algorithms, 22(1):60-65, 2003.  Chandler Davis and William M. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM  Journal on Numerical Analysis, 7(1):1-46, 1970.  Tamal K. Dey, Alfred Rossi, and Anastasios Sidiropoulos. Spectral concentration, robust k-center,  and simple clustering. arXiv:1404.3918, 2014.  Santo Fortunato. Community detection in graphs. Physics Reports, 486(3):75-174, 2010.  Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2012.  Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of In 30th Annual ACM Symposium on Theory of Computing (STOC\u201998), pages  dimensionality. 604-613, 1998.  Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time algo- rithm for approximate max \ufb02ow in undirected graphs, and its multicommodity generalizations. In 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\u201914), pages 217-226, 2014.  Ioannis Koutis, Alex Levin, and Richard Peng. Improved Spectral Sparsification and Numerical Algorithms for SDD Matrices. In 29th International Symposium on Theoretical Aspects of Com- puter Science (STACS\u201912), pages 266-277, 2012.  Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1+ \u03b5)-approximation algorithm for geometric k-means clustering in any dimensions. In 45th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201904), pages 454-462, 2004.  Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Shayan Oveis Gharan, and Luca Trevisan. Improved Cheeger\u2019s inequality: analysis of spectral partitioning algorithms through higher order spectral gap. In 45th Annual ACM Symposium on Theory of Computing (STOC\u201913), pages 11-20, 2013.  James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multi-way spectral partitioning and higher- order Cheeger inequalities. In 44th Annual ACM Symposium on Theory of Computing (STOC\u201912), pages 1117-1130, 2012.  Frank T. Leighton and Satish Rao. Multicommodity max-\ufb02ow min-cut theorems and their use in  designing approximation algorithms. Journal of the ACM, 46(6):787-832, 1999.  Anand Louis, Prasad Raghavendra, Prasad Tetali, and Santosh Vempala. Many sparse cuts via higher eigenvalues. In 44th Annual ACM Symposium on Theory of Computing (STOC\u201912), pages 1131-1140, 2012. PENG SUN ZANETTI  David W. Matula and Farhad Shahrokhi. Sparsest cuts and bottlenecks in graphs. Discrete Applied  Mathematics, 27(1-2):113-123, 1990.  Frank McSherry. Spectral partitioning of random graphs.  In 42nd Annual IEEE Symposium on  Foundations of Computer Science (FOCS\u201901), pages 529-537, 2001.  Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algo-  rithm. Advances in Neural Information Processing Systems, 2:849-856, 2002.  Lorenzo Orecchia, Leonard J. Schulman, Umesh V. Vazirani, and Nisheeth K. Vishnoi. On par- In 40th Annual ACM Symposium on Theory of  titioning graphs via single commodity \ufb02ows. Computing (STOC\u201908), pages 461-470, 2008.  Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K Vishnoi. Approximating the exponential, the Lanczos method and an (cid:101)O(m)-time spectral algorithm for balanced separator. In 44th Annual ACM Symposium on Theory of Computing (STOC\u201912), pages 1141-1160, 2012.  Rafail Ostrovsky, Yuval Rabani, Leonard J. Schulman, and Chaitanya Swamy. The effectiveness of  Lloyd-type methods for the k-means problem. Journal of the ACM, 59(6):28, 2012.  Shayan Oveis Gharan and Luca Trevisan. Partitioning into expanders. In 25th Annual ACM-SIAM  Symposium on Discrete Algorithms (SODA\u201914), pages 1256-1266, 2014.  Prasad Raghavendra, David Steurer, and Madhur Tulsiani. Reductions between expansion problems.  In 27th Conference on Computational Complexity (CCC\u201912), pages 64-73, 2012.  Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional stochastic  blockmodel. The Annals of Statistics, 39(4):1878-1915, 2011.  \u221a  Jonah Sherman. Breaking the multicommodity \ufb02ow barrier for O(  log n)-approximations to spars- est cut. In 50th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201909), pages 363-372, 2009.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM  Journal on Computing, 40(6):1913-1926, 2011.  Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM Journal on  Computing, 40(4):981-1025, 2011.  Luca Trevisan. Approximation algorithms for unique games. Theory of Computing, 4(1):111-128,  Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416,  2008.  2007.  Van Vu. A simple SVD algorithm for finding hidden partitions. arXiv:1404.3918, 2014.  Zeyuan Allen Zhu, Silvio Lattanzi, and Vahab S. Mirrokni. A local algorithm for finding well- connected clusters. In 30th International Conference on Machine Learning (ICML\u201913), pages 396-404, 2013. PARTITIONING WELL-CLUSTERED GRAPHS: SPECTRAL CLUSTERING WORKS!  "}, "Batched Bandit Problems": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Batched Bandit Problems", "abstract": "Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic multi-armed bandits under the constraint that the employed policy must split trials into a small number of batches. Our results show that a very small number of batches gives already close to minimax optimal regret bounds and we also evaluate the number of trials in each batch. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.", "pdf_url": "http://proceedings.mlr.press/v40/Perchet15.pdf", "keywords": []}, "Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints", "abstract": "We study online prediction where regret of the algorithm is measured against a benchmark defined via evolving constraints. This framework captures online prediction on graphs, as well as other prediction problems with combinatorial structure. A key aspect here is that finding the optimal benchmark predictor (even in hindsight, given all the data) might be computationally hard due to the combinatorial nature of the constraints. Despite this, we provide polynomial-time prediction algorithms that achieve low regret against combinatorial benchmark sets. We do so by building improper learning algorithms based on two ideas that work together. The first is to alleviate part of the computational burden through random playout, and the second is to employ Lasserre semidefinite hierarchies to approximate the resulting integer program. Interestingly, for our prediction algorithms, we only need to compute the values of the semidefinite programs and not the rounded solutions. However, the integrality gap for Lasserre hierarchy does enter the generic regret bound in terms of Rademacher complexity of the benchmark set. This establishes a trade-off between the computation time and the regret bound of the algorithm.", "pdf_url": "http://proceedings.mlr.press/v40/Rakhlin15.pdf", "keywords": [], "reference": "B. Barak and A. Moitra. arXiv:1501.06521, 2015.  J. Abernethy. Can we learn to gamble efficiently? In COLT, pages 318-319, 2010.  Tensor prediction, Rademacher complexity and random 3-XOR.  arXiv preprint  B. Barak, M. Hardt, T. Holenstein, and D. Steurer. Subsampling mathematical relaxations and average-case complexity. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pages 512-531. SIAM, 2011.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.  N. Cesa-Bianchi and O. Shamir. Efficient online learning via randomized rounding. In Advances in Neural Information  Processing Systems, pages 343-351, 2011.  V. Chandrasekaran and M.I. Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the  National Academy of Sciences, 110(13):E1181-E1190, 2013.  V. Chandrasekaran, B. Recht, P.A. Parrilo, and A.S. Willsky. The convex geometry of linear inverse problems. Founda-  tions of Computational mathematics, 12(6):805-849, 2012.  C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and approximation algorithms for the  metric labeling problem. SIAM Journal on Discrete Mathematics, 18(3):608-625, 2004.  P. Christiano. Online local learning via semidefinite programming. In Proceedings of the 46th Annual ACM Symposium  on Theory of Computing, pages 468-474. ACM, 2014.  V. Guruswami and A. K. Sinop. Rounding Lasserre SDPs using column selection and spectrum-based approximation  schemes for graph partitioning and Quadratic IPs. arXiv preprint arXiv:1312.3024, 2013.  E. Hazan, S. Kale, and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136,  2012. URL http://arxiv.org/abs/1204.0136.  S. Kakade, A. Kalai, and K. Ligett. Playing games with approximation algorithms. SIAM J. Comput., 39(3):1088-1106,  2009.  (3):796-817, 2001.  293-320, 2003.  J. Kleinberg and E. Tardos. Approximation algorithms for classification problems with pairwise relationships: Metric  labeling and markov random fields. Journal of the ACM (JACM), 49(5):616-639, 2002.  J. B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Optimization, 11  P. A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical programming, 96(2):  P. Raghavendra. Optimal algorithms and inapproximability results for every CSP? In Proceedings of the fortieth annual  ACM symposium on Theory of computing, pages 245-254. ACM, 2008.  P. Raghavendra and D. Steurer. How to round any CSP. In Foundations of Computer Science, 2009. FOCS\u201909. 50th  Annual IEEE Symposium on, pages 586-594. IEEE, 2009.  A. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms.  In Advances in Neural  Information Processing Systems 25, pages 2150-2158, 2012.  G. Schoenebeck. Linear level lasserre lower bounds for certain k-CSPs. In Foundations of Computer Science, 2008.  FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 593-602. IEEE, 2008.  M. Tulsiani. CSP gaps and reductions in the Lasserre hierarchy. In Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing, STOC \u201909, pages 303-312, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-506-2. doi: 10.1145/1536414.1536457. URL http://doi.acm.org/10.1145/1536414.1536457.  13   HIERARCHIES OF RELAXATIONS  References  B. Barak and A. Moitra. arXiv:1501.06521, 2015.  J. Abernethy. Can we learn to gamble efficiently? In COLT, pages 318-319, 2010.  Tensor prediction, Rademacher complexity and random 3-XOR.  arXiv preprint  B. Barak, M. Hardt, T. Holenstein, and D. Steurer. Subsampling mathematical relaxations and average-case complexity. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pages 512-531. SIAM, 2011.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.  N. Cesa-Bianchi and O. Shamir. Efficient online learning via randomized rounding. In Advances in Neural Information  Processing Systems, pages 343-351, 2011.  V. Chandrasekaran and M.I. Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the  National Academy of Sciences, 110(13):E1181-E1190, 2013.  V. Chandrasekaran, B. Recht, P.A. Parrilo, and A.S. Willsky. The convex geometry of linear inverse problems. Founda-  tions of Computational mathematics, 12(6):805-849, 2012.  C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and approximation algorithms for the  metric labeling problem. SIAM Journal on Discrete Mathematics, 18(3):608-625, 2004.  P. Christiano. Online local learning via semidefinite programming. In Proceedings of the 46th Annual ACM Symposium  on Theory of Computing, pages 468-474. ACM, 2014.  V. Guruswami and A. K. Sinop. Rounding Lasserre SDPs using column selection and spectrum-based approximation  schemes for graph partitioning and Quadratic IPs. arXiv preprint arXiv:1312.3024, 2013.  E. Hazan, S. Kale, and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136,  2012. URL http://arxiv.org/abs/1204.0136.  S. Kakade, A. Kalai, and K. Ligett. Playing games with approximation algorithms. SIAM J. Comput., 39(3):1088-1106,  2009.  (3):796-817, 2001.  293-320, 2003.  J. Kleinberg and E. Tardos. Approximation algorithms for classification problems with pairwise relationships: Metric  labeling and markov random fields. Journal of the ACM (JACM), 49(5):616-639, 2002.  J. B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Optimization, 11  P. A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical programming, 96(2):  P. Raghavendra. Optimal algorithms and inapproximability results for every CSP? In Proceedings of the fortieth annual  ACM symposium on Theory of computing, pages 245-254. ACM, 2008.  P. Raghavendra and D. Steurer. How to round any CSP. In Foundations of Computer Science, 2009. FOCS\u201909. 50th  Annual IEEE Symposium on, pages 586-594. IEEE, 2009.  A. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms.  In Advances in Neural  Information Processing Systems 25, pages 2150-2158, 2012.  G. Schoenebeck. Linear level lasserre lower bounds for certain k-CSPs. In Foundations of Computer Science, 2008.  FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 593-602. IEEE, 2008.  M. Tulsiani. CSP gaps and reductions in the Lasserre hierarchy. In Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing, STOC \u201909, pages 303-312, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-506-2. doi: 10.1145/1536414.1536457. URL http://doi.acm.org/10.1145/1536414.1536457. RAKHLIN SRIDHARAN  "}, "Fast Mixing for Discrete Point Processes": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Fast Mixing for Discrete Point Processes", "abstract": "We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions \u03bc(S)\u221d\\exp(\u03b2f(S)) over all subsets S\u22082^V of a finite set V through a bounded set function f:2^V\u2192\\mathbbR and a parameter \u03b2>0. A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage. We show that if the set function (not necessarily submodular) displays a natural notion of decay of correlation, then, for \u03b2small enough, it is possible to design fast mixing Markov chain Monte Carlo methods that yield error bounds on marginal approximations that do not depend on the size of the set V. The sufficient conditions that we derive involve a control on the (discrete) Hessian of set functions, a quantity that has not been previously considered in the literature. We specialize our results for submodular functions, and we discuss canonical examples where the Hessian can be easily controlled.", "pdf_url": "http://proceedings.mlr.press/v40/Rebeschini15.pdf", "keywords": ["Discrete point processes", "MCMC", "fast mixing", "submodular functions", "decay of correlation", "Hessian of set functions"], "reference": "Soren Asmussen and Peter W. Glynn. Stochastic Simulation: Algorithms and Analysis: Algorithms  and Analysis. Stochastic Modelling and Applied Probability. Springer, 2007.  Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes. 2007.  Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 329-338. IEEE Computer Society, 2010.  Josip Djolonga and Andreas Krause. From MAP to marginals: Variational inference in bayesian submodular models. In Advances in Neural Information Processing Systems 27, pages 244-252. Curran Associates, Inc., 2014.  R. L. Dobrushin and S. B. Shlosman. Constructive criterion for the uniqueness of Gibbs field. In Statistical physics and dynamical systems (K\u00a8oszeg, 1984), volume 10 of Progr. Phys., pages 347-370. Birkh\u00a8auser Boston, Boston, MA, 1985.  R. L. Dobru\u02c7sin. Definition of a system of random variables by means of conditional distributions.  Teor. Verojatnost. i Primenen., 15:469-497, 1970.  Martin Dyer, Leslie Ann Goldberg, and Mark Jerrum. Matrix norms and rapid mixing for spin  systems. Ann. Appl. Probab., 19(1):71-107, 02 2009.  Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the  blogosphere. In KDD, 2009.  50(3):273-285, 1979.  H. F\u00a8ollmer. Tail structure of Markov chains on infinite product spaces. Z. Wahrsch. Verw. Gebiete,  Hans-Otto Georgii. Gibbs measures and phase transitions, volume 9 of de Gruyter Studies in  Mathematics. Walter de Gruyter & Co., Berlin, second edition, 2011.  Leslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields.  Combinatorics, Probability and Computing, 2007.  J Ben Hough, Manjunath Krishnapur, Yuval Peres, B\u00b4alint Vir\u00b4ag, et al. Determinantal processes and  independence. 2006.  Rishabh Iyer and Jeff Bilmes. Submodular point processes. In 18th International Conference on  Artificial Intelligence and Statistics (AISTATS-2015), May 2015.  Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. Fast semidifferential-based submodular function  optimization. NIPS, 2013.  NIPS, 2011.  Stefanie Jegelka, Hui Lin, and Jeff A Bilmes. On fast approximate submodular minimization. In  Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the ising model.  SIAM Journal on Computing, 22(5):1087-1116, 1993.  13   FAST MIXING FOR DISCRETE POINT PROCESSES  References  Soren Asmussen and Peter W. Glynn. Stochastic Simulation: Algorithms and Analysis: Algorithms  and Analysis. Stochastic Modelling and Applied Probability. Springer, 2007.  Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes. 2007.  Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 329-338. IEEE Computer Society, 2010.  Josip Djolonga and Andreas Krause. From MAP to marginals: Variational inference in bayesian submodular models. In Advances in Neural Information Processing Systems 27, pages 244-252. Curran Associates, Inc., 2014.  R. L. Dobrushin and S. B. Shlosman. Constructive criterion for the uniqueness of Gibbs field. In Statistical physics and dynamical systems (K\u00a8oszeg, 1984), volume 10 of Progr. Phys., pages 347-370. Birkh\u00a8auser Boston, Boston, MA, 1985.  R. L. Dobru\u02c7sin. Definition of a system of random variables by means of conditional distributions.  Teor. Verojatnost. i Primenen., 15:469-497, 1970.  Martin Dyer, Leslie Ann Goldberg, and Mark Jerrum. Matrix norms and rapid mixing for spin  systems. Ann. Appl. Probab., 19(1):71-107, 02 2009.  Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the  blogosphere. In KDD, 2009.  50(3):273-285, 1979.  H. F\u00a8ollmer. Tail structure of Markov chains on infinite product spaces. Z. Wahrsch. Verw. Gebiete,  Hans-Otto Georgii. Gibbs measures and phase transitions, volume 9 of de Gruyter Studies in  Mathematics. Walter de Gruyter & Co., Berlin, second edition, 2011.  Leslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields.  Combinatorics, Probability and Computing, 2007.  J Ben Hough, Manjunath Krishnapur, Yuval Peres, B\u00b4alint Vir\u00b4ag, et al. Determinantal processes and  independence. 2006.  Rishabh Iyer and Jeff Bilmes. Submodular point processes. In 18th International Conference on  Artificial Intelligence and Statistics (AISTATS-2015), May 2015.  Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. Fast semidifferential-based submodular function  optimization. NIPS, 2013.  NIPS, 2011.  Stefanie Jegelka, Hui Lin, and Jeff A Bilmes. On fast approximate submodular minimization. In  Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the ising model.  SIAM Journal on Computing, 22(5):1087-1116, 1993. REBESCHINI KARBASI  Byungkon Kang. Fast determinantal point process sampling with application to clustering.  In Advances in Neural Information Processing Systems 26, pages 2319-2327. Curran Associates, Inc., 2013.  Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability: Practical  Approaches to Hard Problems (to appear). Cambridge University Press, February 2014.  Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. 2012.  D.A. Levin, Y. Peres, and E.L. Wilmer. Markov Chains and Mixing Times. American Mathematical  Soc., 2008.  Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular  maximization: Identifying representative elements in massive data. In NIPS, 2013.  Elchanan Mossel and Allan Sly. Exact thresholds for Ising-Gibbs samplers on general graphs. Ann.  Probab., 41(1):294-328, 01 2013.  Patrick Rebeschini and Ramon van Handel. Comparison theorems for Gibbs measures. Journal of  Statistical Physics, 157(2):234-281, 2014.  Amar Shah and Zoubin Ghahramani. Determinantal clustering processes - A nonparametric In Proceedings of the Twenty-  bayesian approach to kernel based semi-supervised clustering. Ninth Conference on Uncertainty in Artificial Intelligence, 2013.  Peter Stobbe and Andreas Krause. Efficient minimization of decomposable submodular functions. In Advances in Neural Information Processing Systems 23, pages 2208-2216. Curran Associates, Inc., 2010.  Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational  inference. Foundations and Trends R(cid:13) in Machine Learning, 2008.  Dror Weitz. Combinatorial criteria for uniqueness of Gibbs measures. Random Structures Algo-  rithms, 27(4):445-475, 2005.  "}, "Generalized Mixability via Entropic Duality": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Generalized Mixability via Entropic Duality", "abstract": "Mixability is a property of a loss which characterizes when constant regret is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the \\exp and \\log operations present in the usual theory are not as special as one might have thought.  In doing so we introduce a more general notion of \u03a6-mixability where \u03a6is a general entropy (\\emphi.e., any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical Aggregating Algorithm, is guaranteed a constant regret when used with \u03a6-mixable losses.  We characterize which \u03a6have non-trivial \u03a6-mixable losses and relate \u03a6-mixability and its associated Aggregating Algorithm to potential-based methods, a Blackwell-like condition, mirror descent, and risk measures from finance.  We also define a notion of \u201cdominance\u201d between different entropies in terms of bounds they guarantee and conjecture that classical mixability gives optimal bounds, for which we provide some supporting empirical evidence.", "pdf_url": "http://proceedings.mlr.press/v40/Reid15.pdf", "keywords": ["online learning", "prediction with expert advice", "convex analysis", "aggregating algorithm"], "reference": "Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Jacob D Abernethy and Rafael M Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the Conference on Learning Theory (COLT), volume 23 of JMLR WCP, pages 27-1, 2012.  Charalambos D. Aliprantis and Kim C. Border.  Infinite Dimensional Analysis: A Hitchhiker\u2019s  Guide. Springer, 2007.  Katy S Azoury and Manfred K Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, 2001.  Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for  convex optimization. Operations Research Letters, 31(3):167-175, 2003.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Potential-based algorithms in on-line prediction and game  theory. Machine Learning, 51(3):239-261, 2003.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  regret learning. 189-198, 2010.  functions. IEEE, 1988.  matics, 2004.  Yiling Chen and Jennifer Wortman Vaughan. A new understanding of prediction markets via no- In Proceedings of the 11th ACM conference on Electronic commerce, pages  A Philip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Mathe-  matics, 59(1):77-93, 2007.  Alfredo DeSantis, George Markowsky, and Mark N Wegman. Learning probabilistic prediction In 29th Annual Symposium on Foundations of Computer Science, pages 110-119.  Hans F\u00a8ollmer and Alexander Schied. Stochastic finance, volume 27 of de gruyter studies in mathe-  Rafael M. Frongillo. Eliciting Private Information from Selfish Agents. PhD thesis, University of  California, Berkeley, 2013.  Rafael M Frongillo, Nicol\u00b4as Della Penna, and Mark D Reid.  Interpreting prediction markets: a  stochastic approach. In Proceedings of Neural Information Processing Systems, 2012.  Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Towards optimal algorithms for prediction  with expert advice, 2014. URL http://arxiv.org/abs/1409.3040.  13   MIXABILITY VIA ENTROPIC DUALITY  References  Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Jacob D Abernethy and Rafael M Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the Conference on Learning Theory (COLT), volume 23 of JMLR WCP, pages 27-1, 2012.  Charalambos D. Aliprantis and Kim C. Border.  Infinite Dimensional Analysis: A Hitchhiker\u2019s  Guide. Springer, 2007.  Katy S Azoury and Manfred K Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, 2001.  Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for  convex optimization. Operations Research Letters, 31(3):167-175, 2003.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Potential-based algorithms in on-line prediction and game  theory. Machine Learning, 51(3):239-261, 2003.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  regret learning. 189-198, 2010.  functions. IEEE, 1988.  matics, 2004.  Yiling Chen and Jennifer Wortman Vaughan. A new understanding of prediction markets via no- In Proceedings of the 11th ACM conference on Electronic commerce, pages  A Philip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Mathe-  matics, 59(1):77-93, 2007.  Alfredo DeSantis, George Markowsky, and Mark N Wegman. Learning probabilistic prediction In 29th Annual Symposium on Foundations of Computer Science, pages 110-119.  Hans F\u00a8ollmer and Alexander Schied. Stochastic finance, volume 27 of de gruyter studies in mathe-  Rafael M. Frongillo. Eliciting Private Information from Selfish Agents. PhD thesis, University of  California, Berkeley, 2013.  Rafael M Frongillo, Nicol\u00b4as Della Penna, and Mark D Reid.  Interpreting prediction markets: a  stochastic approach. In Proceedings of Neural Information Processing Systems, 2012.  Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Towards optimal algorithms for prediction  with expert advice, 2014. URL http://arxiv.org/abs/1409.3040. REID FRONGILLO WILLIAMSON MEHTA  David Haussler, Jyrki Kivinen, and Manfred K Warmuth. Sequential prediction of individual se- quences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906- 1925, 1998.  Jean-Bapiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of convex analysis. Springer  Verlag, 2001.  Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixability.  Journal of Computer and System Sciences, 74:1228-1244, 2008.  Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1-63, 1997.  Jyrki Kivinen and Manfred K Warmuth. Averaging expert predictions.  In Proceedings of the 4th European Conference on Computational Learning Theory (EuroCOLT\u201999), pages 153-167. Springer, 1999.  Tomasz Maszczyk and W\u0142odzis\u0142aw Duch. Comparison of Shannon, R\u00b4enyi and Tsallis entropy used in decision trees. In Artificial Intelligence and Soft Computing-ICAISC 2008, pages 643-651. Springer, 2008.  Francesco Orabona, Koby Crammer, and Nicolo Cesa-Bianchi. A generalized online mirror descent with applications to classification and regression. Machine Learning, 99(3):411-435, June 2015.  R. Tyrell Rockafellar. Convex analysis. Princeton University Press, 1997.  Leonard J Savage. Elicitation of personal probabilities and expectations. Journal of the American  Statistical Association, 66(336):783-801, 1971.  Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Frederick A. Valentine. Convex Sets. McGraw-Hill, New York, 1964.  Tim van Erven and Peter Harremo\u00a8es. R\u00b4enyi divergence and Kullback-Leibler divergence. IEEE  Transactions on Information Theory, 60(7):3797-3820, 2014.  Tim van Erven, Mark D Reid, and Robert C Williamson. Mixability is Bayes risk curvature relative  to log loss. The Journal of Machine Learning Research, 13(1):1639-1663, 2012.  Elodie Vernet, Robert C Williamson, and Mark D Reid. Composite multiclass losses. In NIPS,  volume 24, pages 1224-1232, 2011.  Volodya Vovk. Aggregating strategies. In Proceedings of the Third Annual Workshop on Computa-  tional Learning Theory (COLT), pages 371-383, 1990.  Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  Volodya Vovk. Competitive on-line statistics.  International Statistical Review, 69(2):213-248,  56(2):153-173, 1998.  2001. MIXABILITY VIA ENTROPIC DUALITY  Peter M Williams. Bayesian conditionalisation and the principle of minimum information. British  Journal for the Philosophy of Science, 31(2):131-144, 1980.  "}, "On the Complexity of Bandit Linear Optimization": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On the Complexity of Bandit Linear Optimization", "abstract": "We study the attainable regret for online linear optimization problems with bandit feedback, where unlike the full-information setting, the player can only observe its own loss rather than the full loss vector. We show that the price of bandit information in this setting can be as large as d, disproving the well-known conjecture (Danie et al. (2007)) that the regret for bandit linear optimization is at most \\sqrtd times the full-information regret. Surprisingly, this is shown using \u201ctrivial\u201d modifications of standard domains, which have no effect in the full-information setting. This and other results we present highlight some interesting differences between full-information and bandit learning, which were not considered in previous literature.", "pdf_url": "http://proceedings.mlr.press/v40/Shamir15.pdf", "keywords": [], "reference": "COLT, 2011.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Minimax policies for combinatorial prediction games. In  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. The nonstochastic multiarmed bandit prob-  lem. SIAM Journal on Computing, 32(1):48-77, 2002.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza-  tion with bandit feedback. In COLT, 2012.  12   SHAMIR  6. Conclusion  In this paper, we studied the attainable regret for bandit linear optimization problems, focusing on lower bounds, and made several observations:  \u221a  \u2022 We formally proved that the price of bandit information for linear optimization can be as large as d (rather than d), settling the conjecture made in Dani et al. (2007). In a stochastic optimization setting, this implies that even though we get \u201c1/d\u201d as much information in the bandit setting, compared to the full-information setting, we might need d2 (rather than d) more queries to achieve a similar error.  \u2022 In a bandit setting, the dependence of the regret on the domain geometry can be quite different than in the full-information setting, sometimes in counter-intuitive ways. Essentially, this is because changing the domain also changes the information feedback structure. In particular, supposedly \u201ctrivial\u201d changes to the domain, such as shifting it away from the origin or in- tersecting it with a halfspace, can increase the regret from O( T ). Moreover, regret lower bounds for a given domain W don\u2019t necessarily extend to its convex hull.  dT ) to \u2126(d  \u221a  \u221a  \u221a  Our results build on the following observation: At least for stochastic adversaries, and domains with an Euclidean-norm geometry (such as an Euclidean sphere), the question of whether the regret d or d crucially depends on whether the player can control the variance in the observed scales as feedback. In particular, our results indicate that when the feedback is inherently noisy, e.g. of the form (cid:104)xt, wt(cid:105) + \u03bet, where \u03bet is independent sub-Gaussian noise, then even for a benign domain such as an origin-centered Euclidean sphere, the regret is \u0398(d T ) (see Thm. 2 and the following discussion). We conjecture that with such noisy feedback, \u0398(d T ) is the attainable regret for any domain which is \u201cnon-degenerate\u201d under the Euclidean norm. However, proving this conjecture, and understanding how the regret scales for arbitrary domains, is left to future work.  \u221a \u221a  This research was supported by an Israel Science Foundation grant 425/13 and an FP7 Marie Curie CIG grant. We thank S\u00b4ebastien Bubeck for several illuminating discussions which helped initiate the research presented here.  Acknowledgments  References  COLT, 2011.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Minimax policies for combinatorial prediction games. In  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. The nonstochastic multiarmed bandit prob-  lem. SIAM Journal on Computing, 32(1):48-77, 2002.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza-  tion with bandit feedback. In COLT, 2012. ON THE COMPLEXITY OF BANDIT LINEAR OPTIMIZATION  78(5), 2012.  2006.  2007.  2008.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,  V. Dani, S. Kakade, and T. Hayes. The price of bandit information for online optimization. In NIPS,  V. Dani, T. Hayes, and S. Kakade. Stochastic linear optimization under bandit feedback. In COLT,  O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: t2/3 regret. 2014.  Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an applica-  tion to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.  E. Hazan, Z. Karnin, and R. Mehka. Volumetric spanners: an efficient exploration basis for learning.  R. T. Rockafellar. Convex analysis. Princeton University Press, 1970.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.  In  In COLT, 2014.  COLT, 2013.  "}, "An Almost Optimal PAC Algorithm": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "An Almost Optimal PAC Algorithm", "abstract": "The best currently known general lower and upper bounds on the number of labeled examples needed for learning a concept class in the PAC framework (the realizable case) do not perfectly match: they leave a gap of order \\log(1/\u03b5) (resp.\u00a0a gap which is logarithmic in another one of the relevant parameters). It is an unresolved question whether there exists an \u201coptimal PAC algorithm\u201d which establishes a general upper bound with precisely the same order of magnitude as the general lower bound. According to a result of Auer and Ortner, there is no way for showing that arbitrary consistent algorithms are optimal because they can provably differ from optimality by factor \\log(1/\u03b5). In contrast to this result, we show that every consistent algorithm L (even a provably suboptimal one) induces a family (L_K)_K\\ge1 of PAC algorithms (with 2K-1 calls of L as a subroutine) which come very close to optimality: the number of labeled examples needed by L_K exceeds the general lower bound only by factor \\ell_K(1/\\epsillon) where \\ell_K denotes (a truncated version of) the K-times iterated logarithm. Moreover, L_K is applicable to any concept class C of finite VC-dimension and it can be implemented efficiently whenever the consistency problem for C is feasible. We show furthermore that, for every consistent algorithm L, L_2 is an optimal PAC algorithm for precisely the same concept classes which were used by Auer and Ortner for showing the existence of suboptimal consistent algorithms.  This can be seen as an indication that L_K may have an even better performance than it is suggested by our worstcase analysis.", "pdf_url": "http://proceedings.mlr.press/v40/Simon15a.pdf", "keywords": ["PAC-learning", "estimation error", "sample size", "optimal PAC algorithm", "majority vote"], "reference": "Peter Auer and Ronald Ortner. A new PAC bound for intersection-closed concept classes. Machine  Learning, 66(2-3):151-163, 2007.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occam\u2019s razor.  Information Processing Letters, 24:377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the Association on Computing Machinery, 36 (4):929-965, 1989.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. Ann. Math. Statist., 23:493-507, 1952.  Malte Darnst\u00a8adt. The optimal PAC bound for intersection-closed concept classes.  Information  Processing Letters, 115(4):458-461, 2014.  Malte Darnst\u00a8adt, Bal\u00b4azs Sz\u00a8or\u00b4enyi, and Hans U. Simon. Supervised learning and co-training. Theo-  retical Computer Science, 519:68-87, 2014.  Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A general lower bound on the number of examples needed for learning. Information and Computation, 82(3):247-261, 1989.  Steve Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Carnegie Mellon Univer-  sity, 2009.  David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0, 1} functions on ran-  domly drawn points. Information and Computation, 115(2):284-293, 1994.  Leonard Pitt and Leslie G. Valiant. Computational limitations on learning from examples. Journal  of the Association on Computing Machinery, 35(4):965-984, 1988.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Manfred Warmuth. The optimal PAC algorithm. In Proceedings of the 17th Annual Conference on  Learning Theory, pages 641-642, 2004.  SIMON  12   \u2022 Can we combine the hypothesis (hk)k=1,...,2K\u22121 in a more clever way than just returning the majority vote? Is the experience found in the boosting literature (Schapire and Freund, 2012) helpful for this purpose?  \u2022 How does LK compare experimentally to consistent algorithms?  References  Peter Auer and Ronald Ortner. A new PAC bound for intersection-closed concept classes. Machine  Learning, 66(2-3):151-163, 2007.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occam\u2019s razor.  Information Processing Letters, 24:377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the Association on Computing Machinery, 36 (4):929-965, 1989.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. Ann. Math. Statist., 23:493-507, 1952.  Malte Darnst\u00a8adt. The optimal PAC bound for intersection-closed concept classes.  Information  Processing Letters, 115(4):458-461, 2014.  Malte Darnst\u00a8adt, Bal\u00b4azs Sz\u00a8or\u00b4enyi, and Hans U. Simon. Supervised learning and co-training. Theo-  retical Computer Science, 519:68-87, 2014.  Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A general lower bound on the number of examples needed for learning. Information and Computation, 82(3):247-261, 1989.  Steve Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Carnegie Mellon Univer-  sity, 2009.  David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0, 1} functions on ran-  domly drawn points. Information and Computation, 115(2):284-293, 1994.  Leonard Pitt and Leslie G. Valiant. Computational limitations on learning from examples. Journal  of the Association on Computing Machinery, 35(4):965-984, 1988.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Manfred Warmuth. The optimal PAC algorithm. In Proceedings of the 17th Annual Conference on  Learning Theory, pages 641-642, 2004.  SIMON "}, "Minimax rates for memory-bounded sparse linear regression": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Minimax rates for memory-bounded sparse linear regression", "abstract": "We establish a minimax lower bound of \u03a9(\\frackdB\u03b5) on the sample size needed to estimate parameters in a k-sparse linear regression of dimension d under memory restrictions to B bits, where \u03b5is the \\ell_2 parameter error.  When the covariance of the regressors is the identity matrix, we also provide an algorithm that uses \\tildeO(B+k) bits and requires \\tildeO(\\frackdB\u03b5^2) observations to achieve error \u03b5. Our lower bound also holds in the more general communication-bounded setting, where instead of a memory bound, at most B bits of information are allowed to be (adaptively) communicated about each sample.", "pdf_url": "http://proceedings.mlr.press/v40/Steinhardt15.pdf", "keywords": [], "reference": "A. Agarwal, S. Negahban, and M. Wainwright. Stochastic optimization and sparse statistical recov- ery: An optimal algorithm for high dimensions. In Advances in Neural Information Processing Systems 25, 2012.  N. Alon, Y. Matias, and M. Szegedy. The space complexity of approximating the frequency mo-  ments. Journal of Computer and System Sciences, 58(1):137-147, 1999.  E. Arias-Castro, E. J. Candes, and M. A. Davenport. On the fundamental limits of adaptive sensing.  Information Theory, IEEE Transactions on, 59(1):472-481, 2013.  P. Assouad. Deux remarques sur l\u2019estimation. Comptes rendus des s\u00b4eances de l\u2019Acad\u00b4emie des  sciences. S\u00b4erie 1, Math\u00b4ematique, 296(23):1021-1024, 1983.  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, 2013.  M. Braverman. Interactive information complexity. In Proceedings of the Fourty-Fourth Annual  ACM Symposium on the Theory of Computing, 2012.  L. Breiman. Probability. Society for Industrial and Applied Mathematics, 1992.  M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in data streams. In Automata,  Languages and Programming, pages 693-703. Springer, 2002.  A. Daniely, N. Linial, and S. Shalev-Shwartz. More data speeds up training time in learning halfs- paces over sparse vectors. In Proceedings of the Twenty Sixth Annual Conference on Computa- tional Learning Theory, 2013.  A. Daniely, N. Linial, and S. Shalev-Shwartz. From average case complexity to improper learn- ing complexity. In Proceedings of the Fourty-Sixth Annual ACM Symposium on the Theory of Computing, 2014.  13   MEMORY-BOUNDED SPARSE REGRESSION  (cid:16)  (cid:17) (cid:112)log(2d/\u03b4) and that the count-sketch 1 + 3(cid:112)(cid:15)(d \u2212 k) To interpret this, remember that V = 2 structure stores O (log(n/\u03b4)/(cid:15)) bits; also, we need (cid:15) \u2264 1 k for Proposition 2 to hold. As long as B \u2265 k, we can take (cid:15) = O (1/B) (using \u02dcO(B) bits) and have V = \u02dcO((cid:112)d/B). The entire first term (cid:1). This essentially yields Theorem 3; above is thus \u02dcO  , while the second is \u02dcO (cid:0)R2\u03c12 dk B  (cid:113) dkn B  R\u03c1\u03c3  (cid:18)  (cid:19)  the full proof is in Section A.5.  In the above, we analyzed a procedure that stores \u02dcO(B) real numbers. From real numbers to bits. In fact, each number only requires \u02dcO(1) bits of precision for the algorithm to run correctly. A detailed argument for this is given in Section B.  Acknowledgments. The anonymous reviewers were instrumental in improving this paper. JS was supported by an NSF Graduate Research Fellowship and a Fannie & John Hertz Fellowship.  References  A. Agarwal, S. Negahban, and M. Wainwright. Stochastic optimization and sparse statistical recov- ery: An optimal algorithm for high dimensions. In Advances in Neural Information Processing Systems 25, 2012.  N. Alon, Y. Matias, and M. Szegedy. The space complexity of approximating the frequency mo-  ments. Journal of Computer and System Sciences, 58(1):137-147, 1999.  E. Arias-Castro, E. J. Candes, and M. A. Davenport. On the fundamental limits of adaptive sensing.  Information Theory, IEEE Transactions on, 59(1):472-481, 2013.  P. Assouad. Deux remarques sur l\u2019estimation. Comptes rendus des s\u00b4eances de l\u2019Acad\u00b4emie des  sciences. S\u00b4erie 1, Math\u00b4ematique, 296(23):1021-1024, 1983.  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, 2013.  M. Braverman. Interactive information complexity. In Proceedings of the Fourty-Fourth Annual  ACM Symposium on the Theory of Computing, 2012.  L. Breiman. Probability. Society for Industrial and Applied Mathematics, 1992.  M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in data streams. In Automata,  Languages and Programming, pages 693-703. Springer, 2002.  A. Daniely, N. Linial, and S. Shalev-Shwartz. More data speeds up training time in learning halfs- paces over sparse vectors. In Proceedings of the Twenty Sixth Annual Conference on Computa- tional Learning Theory, 2013.  A. Daniely, N. Linial, and S. Shalev-Shwartz. From average case complexity to improper learn- ing complexity. In Proceedings of the Fourty-Sixth Annual ACM Symposium on the Theory of Computing, 2014. STEINHARDT DUCHI  V. H. de la Pe\u02dcna and E. Gin\u00b4e. Decoupling: From Dependence to Independence. Springer, 1999.  J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 429- 438. IEEE, 2013.  J. C. Duchi, M. I. Jordan, M. J. Wainwright, and Y. Zhang. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. arXiv:1405.0782 [cs.IT], 2014.  A. Garg, T. Ma, and H. Nguyen. On communication cost of distributed statistical estimation and dimensionality. In Advances in Neural Information Processing Systems 27, pages 2726-2734, 2014.  A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE, 98(6):  937-947, 2010.  S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn  privately? SIAM Journal on Computing, 40(3):793-826, 2011.  B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24  (2):227-234, 1995.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  O. Shamir. Fundamental limits of online and distributed algorithms for statistical learning and  estimation. In Neural Information Processing Systems 28, 2014.  J. Steinhardt, S. Wager, and P. Liang. The statistics of streaming sparse regression. arXiv preprint  arXiv:1412.4182, 2014.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1- constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5): 2183-2202, 2009.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Jour-  nal of Machine Learning Research, 11:2543-2596, 2010.  Y. Zhang, J. Duchi, M. Jordan, and M. J. Wainwright. Information-theoretic lower bounds for dis- tributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, pages 2328-2336, 2013.  Y. Zhang, M. J. Wainwright, and M. I. Jordan. Lower bounds on the performance of polynomial- time algorithms for sparse linear regression. In Conference on Learning Theory (COLT), 2014. MEMORY-BOUNDED SPARSE REGRESSION  "}, "Interactive Fingerprinting Codes and the Hardness of Preventing False Discovery": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Interactive Fingerprinting Codes and the Hardness of Preventing False Discovery", "abstract": "We show an essentially tight bound on the number of adaptively chosen statistical queries that a computationally efficient algorithm can answer accurately given n samples from an unknown distribution.  A statistical query asks for the expectation of a predicate over the underlying distribution, and an answer to a statistical query is accurate if it is \u201cclose\u201d to the correct expectation over the distribution.  This question was recently studied by Dwork et al. (2015), who showed how to answer \\tilde\u03a9(n^2) queries efficiently, and also by Hardt and Ulman (2014), who showed that answering \\tildeO(n^3) queries is hard.  We close the gap between the two bounds and show that, under a standard hardness assumption, there is no computationally efficient algorithm that, given n samples from an unknown distribution, can give valid answers to O(n^2) adaptively chosen statistical queries.  An implication of our results is that computationally efficient algorithms for answering arbitrary, adaptively chosen statistical queries may as well be \\emphdifferentially private. We obtain our results using a new connection between the problem of answering adaptively chosen statistical queries and a combinatorial object called an \\emphinteractive fingerprinting code Fiat and Tassa (2001).  In order to optimize our hardness result, we give a new Fourier-analytic approach to analyzing fingerprinting codes that is simpler, more flexible, and yields better parameters than previous constructions.", "pdf_url": "http://proceedings.mlr.press/v40/Steinke15.pdf", "keywords": ["Statistical Query Model", "Differential Privacy", "Computational Complexity"], "reference": "Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289-300, 1995.  Dan Boneh and James Shaw. Collusion-secure fingerprinting for digital data. IEEE Transactions  on Information Theory, 44(5):1897-1905, 1998.  Carlo Emilio Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. Pubbl. d. R. Ist.  Super. di Sci. Econom. e Commerciali di Firenze., 8, 1936.  Mark Bun, Jonathan Ullman, and Salil P. Vadhan. Fingerprinting codes and the price of approximate  differential privacy. In STOC, pages 1-10. ACM, May 31 - June 3 2014.  Benny Chor, Amos Fiat, and Moni Naor. Tracing traitors. In CRYPTO, pages 257-270. Springer,  Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy.  In PODS, pages  August 21-25 1994.  202-210. ACM, June 9-12 2003.  ation, 56:52-64, 1961.  Olive Jean Dunn. Multiple comparisons among means. Journal of the American Statistical Associ-  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity  in private data analysis. In TCC, pages 265-284. Springer, March 4-7 2006.  Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the com- plexity of differentially private data release: efficient algorithms and hardness results. In STOC, pages 381-390. ACM, May 31 - June 2 2009.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth.  Preserving statistical validity in adaptive data analysis. In STOC, 2015.  N. Etemadi. On some classical results in probability theory. Sankhya: The Indian Journal of  Statistics, Series A (1961-2002), 47(2):pp. 215-221, 1985.  Amos Fiat and Tamir Tassa. Dynamic traitor tracing. J. Cryptology, 14(3):211-223, 2001.  Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis is hard.  In FOCS. IEEE, October 19-21 2014.  Michael J. Kearns. Efficient noise-tolerant learning from statistical queries. In STOC, pages 392-  401. ACM, May 16-18 1993.  T. Laarhoven, J. Doumen, P. Roelse, B. Skoric, and B. de Weger. Dynamic tardos traitor tracing schemes. Information Theory, IEEE Transactions on, 59(7):4230-4242, July 2013. ISSN 0018- 9448. doi: 10.1109/TIT.2013.2251756.  Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014.  12   STEINKE ULLMAN  References  Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289-300, 1995.  Dan Boneh and James Shaw. Collusion-secure fingerprinting for digital data. IEEE Transactions  on Information Theory, 44(5):1897-1905, 1998.  Carlo Emilio Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. Pubbl. d. R. Ist.  Super. di Sci. Econom. e Commerciali di Firenze., 8, 1936.  Mark Bun, Jonathan Ullman, and Salil P. Vadhan. Fingerprinting codes and the price of approximate  differential privacy. In STOC, pages 1-10. ACM, May 31 - June 3 2014.  Benny Chor, Amos Fiat, and Moni Naor. Tracing traitors. In CRYPTO, pages 257-270. Springer,  Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy.  In PODS, pages  August 21-25 1994.  202-210. ACM, June 9-12 2003.  ation, 56:52-64, 1961.  Olive Jean Dunn. Multiple comparisons among means. Journal of the American Statistical Associ-  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity  in private data analysis. In TCC, pages 265-284. Springer, March 4-7 2006.  Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P. Vadhan. On the com- plexity of differentially private data release: efficient algorithms and hardness results. In STOC, pages 381-390. ACM, May 31 - June 2 2009.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth.  Preserving statistical validity in adaptive data analysis. In STOC, 2015.  N. Etemadi. On some classical results in probability theory. Sankhya: The Indian Journal of  Statistics, Series A (1961-2002), 47(2):pp. 215-221, 1985.  Amos Fiat and Tamir Tassa. Dynamic traitor tracing. J. Cryptology, 14(3):211-223, 2001.  Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis is hard.  In FOCS. IEEE, October 19-21 2014.  Michael J. Kearns. Efficient noise-tolerant learning from statistical queries. In STOC, pages 392-  401. ACM, May 16-18 1993.  T. Laarhoven, J. Doumen, P. Roelse, B. Skoric, and B. de Weger. Dynamic tardos traitor tracing schemes. Information Theory, IEEE Transactions on, 59(7):4230-4242, July 2013. ISSN 0018- 9448. doi: 10.1109/TIT.2013.2251756.  Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014. INTERACTIVE FINGERPRINTING CODES AND THE HARDNESS OF PREVENTING FALSE DISCOVERY  Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. CoRR,  abs/1501.06095, 2015.  G\u00b4abor Tardos. Optimal probabilistic fingerprint codes. J. ACM, 55(2), 2008.  Tamir Tassa. Low bandwidth dynamic traitor tracing schemes. J. Cryptology, 18(2):167-183, 2005.  Jonathan Ullman. Answering n2+o(1) counting queries with differential privacy is hard. In STOC,  pages 361-370. ACM, June 1-4 2013.  Jonathan Ullman. Private multiplicative weights beyond linear queries. CoRR, abs/1407.1571,  2014.  "}, "Convex Risk Minimization and Conditional Probability Estimation": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Convex Risk Minimization and Conditional Probability Estimation", "abstract": "This paper proves, in very general settings, that convex risk minimization is a procedure to select a unique conditional probability model determined by the classification problem. Unlike most previous work, we give results that are general enough to include cases in which no minimum exists, as occurs typically, for instance, with standard boosting algorithms. Concretely, we first show that any sequence of predictors minimizing convex risk over the source distribution will converge to this unique model when the class of predictors is linear (but potentially of infinite dimension). Secondly, we show the same result holds for \\emphempirical risk minimization whenever this class of predictors is finite dimensional, where the essential technical contribution is a norm-free generalization bound.", "pdf_url": "http://proceedings.mlr.press/v40/Telgarsky15.pdf", "keywords": ["Convex duality", "classification", "conditional probability estimation", "maximum entropy", "consistency", "Orlicz spaces"], "reference": "convex duality. 2006.  Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. JMLR, 3:463-482, Nov 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  St\u00b4ephane Boucheron, Olivier Bousquet, and G\u00b4abor Lugosi. Theory of classification: a survey of  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Joseph T. Chang and David Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):  287-317, 1997.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Breg-  man distances. Machine Learning, 48(1-3):253-285, 2002.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer, 1996.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. Annals of Statistics, 28(2):337-407, 2000.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of  Statistics, 29:1189-1232, 2000.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. In  FOCS, 2006.  1994.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Michael Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT Press,  Christian L\u00b4eonard. Orlicz spaces. http://www.cmap.polytechnique.fr/\u02dcleonard/  papers/orlicz.pdf, 2007. Accessed 2015-04-28.  Christian L\u00b4eonard. Minimization of entropy functionals. J. Math. Anal. Appl., 346:183-204, 2008.  Kfir Levy, Elad Hazan, and Tomer Koren. Logistic regression: Tight bounds for stochastic and  online optimization. In COLT, 2014.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  R. Tyrrell Rockafellar. Integrals which are convex functionals I. Pacific J. Math., 24:525-539, 1968.  13   CONVEX RISK MINIMIZATION AND CONDITIONAL PROBABILITY ESTIMATION  References  convex duality. 2006.  Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. JMLR, 3:463-482, Nov 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  St\u00b4ephane Boucheron, Olivier Bousquet, and G\u00b4abor Lugosi. Theory of classification: a survey of  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Joseph T. Chang and David Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):  287-317, 1997.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Breg-  man distances. Machine Learning, 48(1-3):253-285, 2002.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer, 1996.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. Annals of Statistics, 28(2):337-407, 2000.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of  Statistics, 29:1189-1232, 2000.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. In  FOCS, 2006.  1994.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Michael Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT Press,  Christian L\u00b4eonard. Orlicz spaces. http://www.cmap.polytechnique.fr/\u02dcleonard/  papers/orlicz.pdf, 2007. Accessed 2015-04-28.  Christian L\u00b4eonard. Minimization of entropy functionals. J. Math. Anal. Appl., 346:183-204, 2008.  Kfir Levy, Elad Hazan, and Tomer Koren. Logistic regression: Tight bounds for stochastic and  online optimization. In COLT, 2014.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  R. Tyrrell Rockafellar. Integrals which are convex functionals I. Pacific J. Math., 24:525-539, 1968. TELGARSKY DUD\u00b4IK SCHAPIRE  R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.  R. Tyrrell Rockafellar. Conjugate Duality and Optimization. SIAM Publications, 1974.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new  explanation for the effectiveness of voting methods. In ICML, pages 322-330, 1997.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Nathan Srebro, and Karthik Sridharan. Fast rates for regularized objectives.  In NIPS, 2008.  Matus Telgarsky. A primal-dual convergence analysis of boosting. JMLR, 13:561-606, 2012.  Matus Telgarsky. Boosting with the logistic loss is consistent. In COLT, 2013.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. The Annals of Statistics, 32:56-85, 2004.  Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals  of Statistics, 33:1538-1579, 2005.  "}, "Regularized Linear Regression: A Precise Analysis of the Estimation Error": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Regularized Linear Regression: A Precise Analysis of the Estimation Error", "abstract": "Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The  machinery builds upon  Gordon\u2019s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization  (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.", "pdf_url": "http://proceedings.mlr.press/v40/Thrampoulidis15.pdf", "keywords": ["Linear Regression", "mean-square-error ", "structured signals", "sparsity", "LASSO", "Gaussian min-max Theorem", "convexity"], "reference": "Radoslaw Adamczak, Alexander E Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Re- stricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Constructive Approximation, 34(1):61-88, 2011.  Per Kragh Andersen and Richard D Gill. Cox\u2019s regression model for counting processes: a large  sample study. The annals of statistics, pages 1100-1120, 1982.  Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems, pages 1556-1564, 2014.  Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. Information Theory,  IEEE Transactions on, 58(4):1997-2017, 2012.  Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal recovery of  sparse signals via conic programming. Biometrika, 98(4):791-806, 2011.  Dimitri P Bertsekas, Angelia Nedi\u00b4c, and Asuman E Ozdaglar. Convex analysis and optimization.  Athena Scientific Belmont, 2003.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and  dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford University Press, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2009.  Emmanuel Cand`es and Terence Tao. The dantzig selector: Statistical estimation when p is much  larger than n. The Annals of Statistics, pages 2313-2351, 2007.  Emmanuel J Cand`es. Mathematics of sparsity (and few other things). 2014.  Emmanuel J Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact sig- Information Theory, IEEE  nal reconstruction from highly incomplete frequency information. Transactions on, 52(2):489-509, 2006.  Emmanuel J Cand`es, Yonina C Eldar, Deanna Needell, and Paige Randall. Compressed sensing with coherent and redundant dictionaries. Applied and Computational Harmonic Analysis, 31 (1):59-73, 2011.  Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex ge- ometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  David L Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289-  1306, 2006.  13   PRECISE ERROR ANALYSIS FOR REGULARIZED LINEAR REGRESSION  References  Radoslaw Adamczak, Alexander E Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Re- stricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Constructive Approximation, 34(1):61-88, 2011.  Per Kragh Andersen and Richard D Gill. Cox\u2019s regression model for counting processes: a large  sample study. The annals of statistics, pages 1100-1120, 1982.  Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems, pages 1556-1564, 2014.  Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. Information Theory,  IEEE Transactions on, 58(4):1997-2017, 2012.  Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal recovery of  sparse signals via conic programming. Biometrika, 98(4):791-806, 2011.  Dimitri P Bertsekas, Angelia Nedi\u00b4c, and Asuman E Ozdaglar. Convex analysis and optimization.  Athena Scientific Belmont, 2003.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and  dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford University Press, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2009.  Emmanuel Cand`es and Terence Tao. The dantzig selector: Statistical estimation when p is much  larger than n. The Annals of Statistics, pages 2313-2351, 2007.  Emmanuel J Cand`es. Mathematics of sparsity (and few other things). 2014.  Emmanuel J Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact sig- Information Theory, IEEE  nal reconstruction from highly incomplete frequency information. Transactions on, 52(2):489-509, 2006.  Emmanuel J Cand`es, Yonina C Eldar, Deanna Needell, and Paige Randall. Compressed sensing with coherent and redundant dictionaries. Applied and Computational Harmonic Analysis, 31 (1):59-73, 2011.  Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex ge- ometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  David L Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289-  1306, 2006. THRAMPOULIDIS OYMAK HASSIBI  David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity phase transition in compressed sensing. Information Theory, IEEE Transactions on, 57(10):6920-6941, 2011a.  David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity phase transition in compressed sensing. Information Theory, IEEE Transactions on, 57(10):6920-6941, 2011b.  Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio. Regularization networks and sup-  port vector machines. Advances in computational mathematics, 13(1):1-50, 2000.  Rina Foygel and Lester Mackey. Corrupted sensing: Novel guarantees for separating structured  signals. Information Theory, IEEE Transactions on, 60(2):1223-1247, 2014.  Yehoram Gordon. Some inequalities for gaussian processes and applications.  Israel Journal of  Mathematics, 50(4):265-289, 1985.  Yehoram Gordon. Elliptically contoured distributions. Probability theory and related fields, 76(4):  Yehoram Gordon. On Milman\u2019s inequality and random subspaces which escape through a mesh in  429-438, 1987.  Rn. Springer, 1988.  preprint arXiv:1401.2188, 2014.  volume 23. Springer, 1991.  Guillaume Lecu\u00b4e and Shahar Mendelson. Sparse recovery under weak moment assumptions. arXiv  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes,  Shahar Mendelson. Learning without concentration. In Proceedings of The 27th Conference on  Learning Theory, pages 25-39, 2014.  Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified frame- work for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, 2012.  Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing. Hand-  book of econometrics, 4:2111-2245, 1994.  Samet Oymak and Babak Hassibi. New null space results and recovery thresholds for matrix rank  minimization. arXiv preprint arXiv:1011.6326, 2010.  Samet Oymak and Babak Hassibi. Sharp mse bounds for proximal denoising. arXiv preprint  arXiv:1305.2714, 2013.  Samet Oymak, Christos Thrampoulidis, and Babak Hassibi. The squared-error of generalized lasso:  A precise analysis. arXiv preprint arXiv:1311.0830, 2013.  Mert Pilanci and Martin J Wainwright. Randomized sketches of convex programs with sharp guar- antees. In Information Theory (ISIT), 2014 IEEE International Symposium on, pages 921-925. IEEE, 2014.  Calyampudi Radhakrishna Rao and Helge Toutenburg. Linear models. Springer, 1995. PRECISE ERROR ANALYSIS FOR REGULARIZED LINEAR REGRESSION  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated  gaussian designs. The Journal of Machine Learning Research, 99:2241-2259, 2010.  R Tyrell Rockafellar. Convex analysis, volume 28. Princeton university press, 1997.  Mark Rudelson and Roman Vershynin. On sparse reconstruction from fourier and gaussian mea-  surements. Communications on Pure and Applied Mathematics, 61(8):1025-1045, 2008.  Maurice Sion et al. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171-176,  Mihailo Stojnic. Various thresholds for (cid:96)1-optimization in compressed sensing. arXiv preprint  Mihailo Stojnic. A framework to characterize performance of lasso algorithms. arXiv preprint  Mihailo Stojnic. Meshes that trap random subspaces. arXiv preprint arXiv:1304.0003, 2013b.  Mihailo Stojnic. Spherical perceptron as a storage memory with limited errors. arXiv preprint  1958.  arXiv:0907.3666, 2009.  arXiv:1303.7291, 2013a.  arXiv:1306.3809, 2013c.  arXiv:1303.7289, 2013d.  Mihailo Stojnic.  Upper-bounding (cid:96)1-optimization weak thresholds.  arXiv preprint  Christos Thrampoulidis and Babak Hassibi. Estimating structured signals in sparse noise: A precise In Communication, Control, and Computing (Allerton), 2014 52nd  noise sensitivity analysis. Annual Allerton Conference on, pages 866-873. IEEE, 2014.  Christos Thrampoulidis and Babak Hassibi. Isotropically random orthogonal matrices: Performance of lasso and minimum conic singular values. arXiv preprint arXiv:1503.07236, accepted to ISIT 2015, 2015.  Christos Thrampoulidis, Ashkan Panahi, Daniel Guo, and Babak Hassibi. Precise error analysis of the lasso. In in 40th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015, available on arXiv:1502.04977, 2015a.  Christos Thrampoulidis, Ashkan Panahi, and Babak Hassibi. Asymptotically exact error analysis  for the generalized (cid:96)2  2-lasso. arXiv preprint arXiv:1502.04977, accepted to ISIT 2015, 2015b.  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), pages 267-288, 1996.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  arXiv:1405.5103, 2014.  Roman Vershynin. Estimation in high dimensions: a geometric perspective.  arXiv preprint  Lie Wang. The l1 penalized lad estimator for high dimensional linear regression. Journal of Multi-  variate Analysis, 120:135-151, 2013. THRAMPOULIDIS OYMAK HASSIBI  John Wright and Yi Ma. Dense error correction via-minimization.  Information Theory, IEEE  Transactions on, 56(7):3540-3560, 2010.  Yihong Wu and Sergio Verd\u00b4u. Optimal phase transitions in compressed sensing. Information The-  ory, IEEE Transactions on, 58(10):6241-6263, 2012.  Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Jour-  nal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.  Tong Zhang et al. Some sharp performance bounds for least squares regression with l1 regulariza-  tion. The Annals of Statistics, 37(5A):2109-2144, 2009.  "}, "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity", "abstract": "We present a simple, general technique for reducing the sample complexity of matrix and tensor decomposition algorithms applied to distributions. We use the technique to give a polynomial-time algorithm for standard ICA with sample complexity nearly linear in the dimension, thereby improving substantially on previous bounds. The analysis is based on properties of random polynomials, namely the spacings of an ensemble of polynomials. Our technique also applies to other applications of tensor decompositions, including spherical Gaussian mixture models.", "pdf_url": "http://proceedings.mlr.press/v40/Vempala15.pdf", "keywords": ["Independent Component Analysis", "Tensor decomposition", "Fourier PCA", "Sample Complexity", "Eigenvalue spacing"], "reference": "Anima Anandkumar, Dean Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm for latent dirichlet allocation. In Advances in Neural Information Processing Systems 25, pages 926-934, 2012a.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012b.  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James Voss. The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures. arXiv:1311.2891, 2013.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown gaus- sian noise, with implications for gaussian mixtures and autoencoders. In NIPS, pages 2384-2392, 2012.  Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of  Gaussian noise. In Proc. of COLT, 2013.  C. Brubaker. Extensions of principal component analysis. Phd. Thesis, School of CS, Georgia Tech,  J.F. Cardoso. Source separation using higher order moments. In International Conference on Acous-  tics, Speech, and Signal Processing, 1989.  Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press,  2009.  2010.  Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation III.  SIAM Journal on Numerical Analysis, 7(1):1-46, 1970.  Paul Deheuvels. Strong limit theorems for maximal spacings from a general univariate distribution.  The Annals of Probability, pages 1181-1193, 1984.  Paul Deheuvels. The limiting behaviour of the maximal spacing generated by an iid sequence of  gaussian random variables. Journal of applied probability, pages 816-827, 1985.  Paul Deheuvels. On the in\ufb02uence of the extremes of an iid sequence on the maximal spacings. The  Annals of Probability, pages 194-208, 1986.  William Feller. An Introduction to Probability Theory and its Applications, vol 1. John Wiley and  Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In FOCS, pages  sons, 1968.  359-368, 1996.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA and robust tensor decomposition. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 584-593, 2014.  13   RECURSIVE EIGEN-DECOMPOSITION  References  Anima Anandkumar, Dean Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm for latent dirichlet allocation. In Advances in Neural Information Processing Systems 25, pages 926-934, 2012a.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012b.  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James Voss. The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures. arXiv:1311.2891, 2013.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown gaus- sian noise, with implications for gaussian mixtures and autoencoders. In NIPS, pages 2384-2392, 2012.  Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of  Gaussian noise. In Proc. of COLT, 2013.  C. Brubaker. Extensions of principal component analysis. Phd. Thesis, School of CS, Georgia Tech,  J.F. Cardoso. Source separation using higher order moments. In International Conference on Acous-  tics, Speech, and Signal Processing, 1989.  Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press,  2009.  2010.  Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation III.  SIAM Journal on Numerical Analysis, 7(1):1-46, 1970.  Paul Deheuvels. Strong limit theorems for maximal spacings from a general univariate distribution.  The Annals of Probability, pages 1181-1193, 1984.  Paul Deheuvels. The limiting behaviour of the maximal spacing generated by an iid sequence of  gaussian random variables. Journal of applied probability, pages 816-827, 1985.  Paul Deheuvels. On the in\ufb02uence of the extremes of an iid sequence on the maximal spacings. The  Annals of Probability, pages 194-208, 1986.  William Feller. An Introduction to Probability Theory and its Applications, vol 1. John Wiley and  Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In FOCS, pages  sons, 1968.  359-368, 1996.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA and robust tensor decomposition. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 584-593, 2014. VEMPALA XIAO  Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. The  elements of statistical learning, volume 2. Springer, 2009.  Christopher Hillar and Lek-Heng Lim. Most tensor problems are NP-hard. Journal of the ACM, 60,  2013.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods and  spectral decompositions. In ITCS, pages 11-20, 2013.  Aapo Hyv\u00a8arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. Wiley, 2001.  Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W Koh, Quoc V Le, and Andrew Y Ng. Tiled convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1279-1287, 2010.  Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU  signatures. J. Cryptology, 22(2):139-160, 2009.  Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory. Academic press, 1990.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of  Computer and System Sciences, 68(4):841-860, 2004.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok, editors, Compressed Sensing, Theory and Applications, pages 210-268. Cambridge University Press, Oxford, 2010.  Ying Xiao. Fourier pca package. GitHuB, 2014. http://github.com/yingusxiaous/libFPCA.  Arie Yeredor. Blind source separation via the second characteristic function. Signal Processing, 80  (5):897-902, 2000.  6. "}, "On Convergence of Emphatic Temporal-Difference Learning": {"volumn": "v40", "url": "http://proceedings.mlr.press/v40/", "header": "On Convergence of Emphatic Temporal-Difference Learning", "abstract": "We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD(\u03bb) and ELSTD(\u03bb). We prove, under general off-policy conditions, the convergence in L^1 for ELSTD(\u03bb) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD(\u03bb) also converges under off-policy training for \u03bbsufficiently large.", "pdf_url": "http://proceedings.mlr.press/v40/Yu15.pdf", "keywords": ["Markov decision processes", "approximate policy evaluation", "reinforcement learning", "temporal difference methods", "importance sampling", "stochastic approximation", "convergence"], "reference": "MA, 1996.  bridge, 2008.  L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc.  The 12th Int. Conf. Machine Learning, pages 30-37, 1995.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  D. P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large linear  systems. Journal of Computational and Applied Mathematics, 227(1):27-50, 2009.  V. S. Borkar. Stochastic Approximation: A Dynamic Viewpoint. Cambridge University Press, Cam-  V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation and  reinforcement learning. SIAM J. Control Optim., 38:447-469, 2000.  J. A. Boyan. Least-squares temporal difference learning.  In Proc. The 16th Int. Conf. Machine  Learning, pages 49-56, 1999.  C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: A survey and  comparison. Journal of Machine Learning Res., 15:809-883, 2014.  J. L. Doob. Stochastic Processes. John Wiley & Sons, New York, 1953.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.  M. Geist and B. Scherrer. Off-policy learning with eligibility traces: A survey. Journal of Machine  Learning Res., 15:289-333, 2014.  P. W. Glynn and D. L. Iglehart. Science, 35:1367-1392, 1989.  Importance sampling for stochastic simulations. Management  H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Uncon-  strained Systems. Springer-Verlag, New York, 1978.  H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer-Verlag, New York, 2nd edition, 2003.  H. R. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta,  2011.  A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy In Proc. Conf. Advances in Neural Information  learning with linear function approximation. Processing Systems (NIPS) 27, 2014.  A. R. Mahmood, H. Yu, M. White, and R. S. Sutton. Emphatic temporal-difference learning. In  European Workshops on Reinforcement Learning, Lille, France, 2015.  S. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press,  Cambridge, 2nd edition, 2009.  13   ON CONVERGENCE OF EMPHATIC TEMPORAL-DIFFERENCE LEARNING  References  MA, 1996.  bridge, 2008.  L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc.  The 12th Int. Conf. Machine Learning, pages 30-37, 1995.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  D. P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large linear  systems. Journal of Computational and Applied Mathematics, 227(1):27-50, 2009.  V. S. Borkar. Stochastic Approximation: A Dynamic Viewpoint. Cambridge University Press, Cam-  V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation and  reinforcement learning. SIAM J. Control Optim., 38:447-469, 2000.  J. A. Boyan. Least-squares temporal difference learning.  In Proc. The 16th Int. Conf. Machine  Learning, pages 49-56, 1999.  C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: A survey and  comparison. Journal of Machine Learning Res., 15:809-883, 2014.  J. L. Doob. Stochastic Processes. John Wiley & Sons, New York, 1953.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.  M. Geist and B. Scherrer. Off-policy learning with eligibility traces: A survey. Journal of Machine  Learning Res., 15:289-333, 2014.  P. W. Glynn and D. L. Iglehart. Science, 35:1367-1392, 1989.  Importance sampling for stochastic simulations. Management  H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Uncon-  strained Systems. Springer-Verlag, New York, 1978.  H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer-Verlag, New York, 2nd edition, 2003.  H. R. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta,  2011.  A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy In Proc. Conf. Advances in Neural Information  learning with linear function approximation. Processing Systems (NIPS) 27, 2014.  A. R. Mahmood, H. Yu, M. White, and R. S. Sutton. Emphatic temporal-difference learning. In  European Workshops on Reinforcement Learning, Lille, France, 2015.  S. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press,  Cambridge, 2nd edition, 2009. J. Neveu. Discrete-Parameter Martingales. North-Holland, Amsterdam, 1975.  D. Precup, R. S. Sutton, and S. Dasgupta. Off-policy temporal-difference learning with function  approximation. In Proc. The 18th Int. Conf. Machine Learning, pages 417-424, 2001.  M. L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. John  Wiley & Sons, 1994.  R. S. Randhawa and S. Juneja. Combining importance sampling and temporal difference control variates to simulate Markov chains. ACM Trans. Modeling and Computer Simulation, 14(1): 1-30, 2004.  B. Scherrer. Should one compute the temporal difference fix point or minimize the Bellman resid- ual? The unified oblique projection view. In Proc. The 27th Int. Conf. Machine Learning, pages 959-966, 2010.  R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:  9-44, 1988.  R. S. Sutton. TD models: Modeling the world at a mixture of time scales. In Proc. The 12th Int.  Conf. Machine Learning, pages 531-539, 1995.  R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press, Cambridge, MA, 1998.  R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy  temporal-difference learning, 2015. http://arxiv.org/abs/1503.04269.  J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxi-  mation. IEEE Trans. Automat. Contr., 42(5):674-690, 1997.  R. S. Varga. Matrix Iterative Analysis. Springer-Verlag, Berlin, 2nd edition, 2000.  H. Yu. Least squares temporal difference methods: An analysis under general conditions. SIAM J.  Control Optim., 50:3310-3343, 2012.  H. Yu. On convergence of emphatic temporal-difference larning. Technical report, University of  Alberta, 2015. http://arxiv.org/abs/1506.02582.  H. Yu and D. P. Bertsekas. Weighted Bellman equations and their applications in approximate  dynamic programming. LIDS Technical Report 2876, MIT, 2012.  "}}