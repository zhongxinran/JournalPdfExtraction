{"1": "Jacob Abernethy, Alekh Agarwal, Peter L. Bartlett, and Alexander Rakhlin. A stochastic In Proceedings of the 22nd Annual  view of optimal regret through minimax duality. Conference on Learning Theory, 2009.  Nicol`o Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge  University Press, 2006.  246   Van Erven, Reid and Williamson  A function h is concave if and only if  h is convex. Thus h is concave if and only  \u2212 h(x0) + Dh(x0)  h(x)  \u2264  (x  \u00b7  \u2212  x0),  x, x0.  \u2200  2. The concavity of h is equivalent to the following holding for all  Let h(x) = f (x) x, x0:  \u03b1  x (cid:107)  (cid:107)  \u2212  (cid:107)  x  \u03b1  \u2264  2 (cid:107) 2 \u03b1 x (cid:107) (cid:107) f (x0) (cid:107) \u2212 f (x0) + Df (x0)  f (x0) f (x0) x0 \u03b1 (cid:107)  \u2264  (cid:107)  \u03b1  \u2212  x0 x0 \u2212 (cid:107) 2 + Df (x0)  \u03b1  (cid:107)  2 + (Df (x0) 2 + Df (x0) (x \u00b7 x0) + \u03b1  (cid:107)  (x  \u00b7 \u2212 x0) + \u03b1 2 x0  2\u03b1x0) x0) 2 x (cid:107)  (cid:107)  \u2212 (x  \u2212 x (cid:107)  \u2212  (cid:107)  \u00b7  \u2212  x0)  (x  \u00b7  \u2212  \u2212  \u2212 2\u03b1x0 2\u03b1x0  \u00b7  \u00b7  x0)  (x  \u2212 x + 2\u03b1  x0  2 (cid:107)  (cid:107)  f (x)  f (x)  f (x)  f (x)  (22).  \u2212  \u2212  \u2264  \u2264  \u21d4  \u21d4  \u21d4  \u21d4  \u2212  \u2212  (cid:107) \u00b7 (cid:107)  \u21d0\u21d2  2\u03b1I (cid:52) 0  Thus f is \u03b1-\ufb02at if and only if H(f  \u03b1 Hf (cid:52) 2\u03b1I. Hence requiring  2) is negative semidefinite, which is equivalent L is \u03b1-\ufb02at is a constraint on the to Hf 2\u03b1I. However our main result curvature of L relative to a \ufb02at surface: L is \u03b1-\ufb02at i\ufb00 HL (cid:60) \u2212 shows that the mixability constant (which is the best possible constant one can have in a bound such as (1)) is governed by the curvature of \u02dcL normalised by the curvature of \u02dcLlog. The necessity of comparison with log loss is not that surprising in light of the observations 17.9). regarding mixability by Gr\u00a8unwald (2007, \u00a7  \u2212  5. Conclusion  We have characterised the mixability constant for strictly proper multiclass losses (and shown how the result also applies to improper losses). The result shows in a precise and intuitive way the e\ufb00ect of the choice of loss function on the performance of an aggregating forecaster and the special role played by Log-loss in such settings.  Acknowledgments  This work was supported by the Australian Research Council and NICTA through backing Australia\u2019s ability. Some of the work was done while all the authors were visiting Microsoft Research, Cambridge and some was done while Tim van Erven was visiting ANU and NICTA. It was also supported in part by the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886. This publication only re\ufb02ects the authors\u2019 views.  References  Jacob Abernethy, Alekh Agarwal, Peter L. Bartlett, and Alexander Rakhlin. A stochastic In Proceedings of the 22nd Annual  view of optimal regret through minimax duality. Conference on Learning Theory, 2009.  Nicol`o Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge  University Press, 2006. Mixability is Bayes Risk Curvature Relative to Log Loss  Alexey Chernov, Yuri Kalnishkan, Fedor Zhdanov, and Vladimir Vovk. Supermartingales in prediction with expert advice. Theoretical Computer Science, 411:2647-2669, 2010.  Paul K. Fackler. Notes on matrix calculus. North Carolina State University, 2005.  Wendell H. Fleming. Functions of Several Variables. Springer, 1977.  Peter D. Gr\u00a8unwald. The Minimum Description Length Principle. MIT Press, Cambridge,  MA, 2007.  Peter D. Gr\u00a8unwald and A. Phillip Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. The Annals of Statistics, 32(4):1367- 1433, 2004.  David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. Sequential prediction of individ- ual sequences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906-1925, 1998.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Convex Analysis and Minimization  Algorithms: Part I: Fundamentals. Springer, Berlin, 1993.  Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge University Press, 1985.  Yuri Kalnishkan and Michael V. Vyugin. On the absence of predictive complexity for some In Proceedings of the 13th International Conference on Algorithmic Learning games. Theory, volume 2533 of Lecture Notes in Artificial Intelligence, pages 164-172. Springer- Verlag, 2002a.  Yuri Kalnishkan and Michael V. Vyugin. Mixability and the existence of weak complexities. In The 15th Annual Conference on Computational Learning Theory (COLT 2002), volume 2375 of Lecture Notes in Artificial Intelligence, pages 105-120. Springer-Verlag, 2002b.  Yuri Kalnishkan and Michael V. Vyugin. The weak aggregating algorithm and weak mixa-  bility. Journal of Computer and System Sciences, 74:1228-1244, 2008.  Yuri Kalnishkan, Volodya Vovk, and Michael V. Vyugin. Loss functions, complexities, and  the Legendre transformation. Theoretical Computer Science, 313:195-207, 2004.  Jan R. Magnus and Heinz Neudecker. Matrix Di\ufb00erential Calculus with Applications in  Statistics and Econometrics (revised edition). John Wiley & Sons, Ltd., 1999.  Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine  Learning Research, 11:2387-2422, 2010.  Mark D. Reid and Robert C. Williamson.  Information, divergence and risk for binary  experiments. Journal of Machine Learning Research, 12:731-817, March 2011.  John A. Thorpe. Elementary Topics in Di\ufb00erential Geometry. Springer, 1979.  Volodya Vovk. Aggregating strategies. In Proceedings of the Third Annual Workshop on  Computational Learning Theory (COLT), pages 371-383, 1990. Van Erven, Reid and Williamson  Volodya Vovk. A game of prediction with expert advice.  In Proceedings of the Eighth  Annual Conference on Computational Learning Theory, pages 51-60. ACM, 1995.  Volodya Vovk and Fedor Zhdanov. Prediction with expert advice for the Brier game. Journal  of Machine Learning Research, 10:2445-2471, 2009."}