{"1": "M. Balcan and A. Blum. A discriminative model for semi-supervised learning. JACM, 57(3), 2010.  30.12   HELMBOLD LONG  points are in Ti, and (b) some training point is in Ti+1 (i.e. some training point is 3(cid:96)+i+1). Let EXACTcm/2 be the event that no training point is labeled \u201c\u2212\u201d. Therefore the EXACTi events are disjoint and MISSi = (cid:83)  j\u2265i EXACTj.  Note that if EXACTi occurs, then the smallest negative example is 3(cid:96)+i+1. Furthermore, all points in Ti are less then half this value and the maximum margin algorithm predicts incorrectly on exactly the i points in Ti, so Pr(error|EXACTi) = i/(cm). Thus, for m > 2c, we have  cm/2 (cid:88)  i=1  Pr(error) =  Pr(error|EXACTi) Pr(EXACTi) =  Pr(EXACTi)  =  1 cm  cm/2 (cid:88)  i=1  Pr(MISSi) =  1 cm  cm/2 (cid:88)  i=1  (cid:19)m  (cid:18) cm \u2212 i cm  \u2265  1 cm  c2 (cid:88)  (cid:18)  1 \u2212  i=1  (cid:19)m  .  i/c m  cm/2 (cid:88)  i=1  i cm  For i \u2264 c2, in the limit as m \u2192 \u221e, relative to the constant c) we can continue as follows.  1 \u2212 i/c m  (cid:16)  (cid:17)m  \u2192 exp(\u2212i/c), so for large enough m (large  Pr(error) \u2265  (1 \u2212 (cid:15)) exp(\u2212i/c) =  exp(\u22121/c)i  1 cm  c2 (cid:88)  i=1  1 \u2212 (cid:15) cm  c2 (cid:88)  i=1  =  =  exp(\u22121/c)  1 \u2212 (cid:15) cm (1 \u2212 (cid:15))(1 \u2212 (cid:15)2) cm  1 \u2212 exp(\u22121/c)c2 1 \u2212 exp(\u22121/c) exp(\u22121/c) 1 \u2212 exp(\u22121/c)  =  1 \u2212 (cid:15) cm  exp(\u22121/c)  1 \u2212 exp(\u2212c) 1 \u2212 exp(\u22121/c)  where (cid:15)2 = e\u2212c. Now, replacing 1/c by a we get: Pr(error) \u2265 (1\u2212(cid:15))(1\u2212(cid:15)2) a exp(\u2212a) 1\u2212exp(\u2212a) Using L\u2019Hopitals rule, we see that the limit of the second fraction as a \u2192 0 is 1. So for large enough c, the second fraction is at least 1 \u2212 (cid:15)3 and Pr(error) \u2265 (1\u2212(cid:15))(1\u2212(cid:15)2)(1\u2212(cid:15)3) . Thus, by making the constant c large enough, and choosing m large enough compared to c, the expected error of the maximum margin algorithm can be made arbitrarily close to 1/m.  m  m  6. Conclusion  Algorithms that know the underlying marginal distribution D over the instances can learn signifi- cantly more accurately than algorithms that do not. Since knowledge of D has been proposed as a proxy for a large number of unlabeled examples, our results indicate a benefit for semi-supervised learning. It is particularly intriguing that our analysis shows the benefit of semi-supervised learning when the distribution is nearly uniform, but slightly concentrated near the decision boundary. This is in sharp contrast to previous analyses showing the benefits of semi-supervised learning, which typically rely on a \u201ccluster assumption\u201d postulating that examples are sparse along the decision boundary.  References  M. Balcan and A. Blum. A discriminative model for semi-supervised learning. JACM, 57(3), 2010.  30.12   NEW BOUNDS FOR INTERVALS  S. Ben-David, T. Lu, and D. Pal. Does unlabeled data provably help? Worst-case analysis of the  sample complexity of semi-supervised learning. COLT, 2008.  B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers. Proceedings of the 1992 Workshop on Computational Learning Theory, pages 144-152, 1992.  O. Chapelle, B. Sch\u00a8olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, 2006.  Malte Darnst\u00a8adt and Hans-Ulrich Simon. Smart pac-learners. Theor. Comput. Sci., 412(19):1756-  1766, 2011.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. G. Valiant. A general lower bound on the number  of examples needed for learning. Information and Computation, 82(3):247-251, 1989.  D. Haussler, N. Littlestone, and M. K. Warmuth. Predicting {0, 1}-functions on randomly drawn  points. Information and Computation, 115(2):129-161, 1994.  R. Herbrich, T. Graepel, and C. Campbell. Bayes point machines. Journal of Machine Learning  Research, 1:245-279, 2001.  M. K\u00a8a\u00a8ari\u00a8ainen. Generalization error bounds using unlabeled data. COLT, 2005.  Y. Li, P. M. Long, and A. Srinivasan. The one-inclusion graph algorithm is near-optimal for the prediction model of learning. IEEE Transactions on Information Theory, 47(3):1257-1261, 2001.  P.A.P. Moran. The random division of an interval. Supplement to the Journal of the Royal Statistical  L. Pitt and M. K. Warmuth. Prediction preserving reducibility. Journal of Computer and System  R. Urner, S. Shalev-Shwartz, and S. Ben-David. Access to unlabeled data can speed up prediction  V. N. Vapnik and A. Lerner. Pattern recognition using generalized portrait method. Automation and  X. Zhu and A. B. Goldberg.  Introduction to Semi-Supervised Learning. Synthesis Lectures on  Artificial Intelligence and Machine Learning. Morgan-Claypool, 2009.  Society, 9(1):92-98, 1947.  Sciences, 41(3), 1990.  time. ICML, 2011.  remote control, 24, 1963."}