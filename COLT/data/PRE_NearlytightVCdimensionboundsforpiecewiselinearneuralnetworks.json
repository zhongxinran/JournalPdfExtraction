{"1": "University Press, 1999.  published manuscript.  CL is supported by an NSERC graduate scholarship. AM is supported by an NSERC Postdoctoral Fellowship and a Simons-Berkeley Research Fellowship. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing at UC Berkeley.  Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge  Peter Bartlett. The impact of the nonlinearity on the VC-dimension of a deep network, 2017. un-  Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise  polynomial networks. Neural Computation, 10(8):2159-2173, Nov 1998.  Eric B. Baum and David Haussler. What size net gives valid generalization? Neural Computation,  1(1):151-160, 1989.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. JACM, 36(4), 1989. (Conference version in STOC\u201986).  4   HARVEY LIAW MEHRABIAN  functions can easily be simulated using ReLU functions. We refer the reader to the excellent mono- graph by Anthony and Bartlett (1999) that covers these and many other theoretical results on neural networks.  Recently there have been several papers that study neural networks from an approximation the- ory point of view and aim to understand which functions can be expressed using a neural network of given a depth and size. There are technical similarities between our work and these. Last year, two striking papers considered the problem of approximating a deep neural network with a shallower network. Telgarsky (2016) shows that there is a ReLU network with L layers and U = \u0398(L) units such that any network approximating it with only O(L1/3) layers must have \u2126(2L1/3) units; this phenomenon holds even for real-valued functions. Eldan and Shamir (2016) show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.  Very recently, several authors have shown that deep neural networks are capable of approxi- mating broad classes of functions. Safran and Shamir (2017) show that a sufficiently non-linear C2 function on [0, 1]d can be approximated with (cid:15) error in L2 by a ReLU network with O(polylog(1/(cid:15))) layers and weights, but any such approximation with O(1) layers requires \u2126(1/(cid:15)) weights. Yarot- sky (2017) shows that any Cn-function on [0, 1]d can be approximated with (cid:15) error in L\u221e by a ReLU network with O(log(1/(cid:15))) layers and O(( 1 (cid:15) )d/n log(1/(cid:15))) weights. Liang and Srikant (2017) show that a sufficiently smooth univariate function can be approximated with (cid:15) error in L\u221e by a network with ReLU and threshold gates with \u0398(log(1/(cid:15))) layers and O(polylog(1/(cid:15))) weights, but that \u2126(poly(1/(cid:15))) weights would be required if there were only o(log(1/(cid:15))) layers; they also prove analogous results for multivariate functions. Lastly, Cohen et al. (2016) draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks.  Acknowledgments  References  University Press, 1999.  published manuscript.  CL is supported by an NSERC graduate scholarship. AM is supported by an NSERC Postdoctoral Fellowship and a Simons-Berkeley Research Fellowship. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing at UC Berkeley.  Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge  Peter Bartlett. The impact of the nonlinearity on the VC-dimension of a deep network, 2017. un-  Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise  polynomial networks. Neural Computation, 10(8):2159-2173, Nov 1998.  Eric B. Baum and David Haussler. What size net gives valid generalization? Neural Computation,  1(1):151-160, 1989.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. JACM, 36(4), 1989. (Conference version in STOC\u201986). NEARLY-TIGHT VC-DIMENSION BOUNDS FOR NEURAL NETWORKS  N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis.  In COLT, 2016.  2016.  Thomas M. Cover. Capacity problems for linear machines. In L. Kanal, editor, Pattern Recognition,  pages 283-289. Thompson Book Co., 1968.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In COLT,  Paul W. Goldberg and Mark R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning, 18(2):131-148, 1995. (Conference version in COLT\u201993).  Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:  //www.deeplearningbook.org.  Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks, 2017. URL https://arxiv.org/abs/1703.02930.  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,  2015.  arXiv:1610.04161.  Sept 1994.  Shyu Liang and R. Srikant. Why deep neural networks for function approximation?, 2017.  Wolfgang Maass. Neural nets with superlinear VC-dimension. Neural Computation, 6(5):877-884,  I. Safran and O. Shamir. Depth-width tradeoffs in approximating natural functions with neural  networks, 2017. arXiv:1610.09887.  Matus Telgarsky. Benefits of depth in neural networks. In COLT, 2016.  V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971. doi: 10.1137/1116025.  Hugh E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the  American Mathematical Society, 133(1):167-178, 1968.  Dmitry Yarotsky.  Error bounds for approximations with deep ReLU networks, 2017.  arXiv:1610.01145."}