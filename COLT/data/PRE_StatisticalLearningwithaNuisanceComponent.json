{"1": "Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1-C68, 2018.  Jerzy Neyman. Optimal asymptotic tests of composite hypotheses. Probability and statsitics, pages  Jerzy Neyman. C (\u03b1) tests and their use. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages  213-234, 1959.  1-21, 1979.  Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax  regret and minimax risk. Bernoulli, 23(2):789-824, 2017.  Vladimir N Vapnik. The nature of statistical learning theory. Springer, 1995.  Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, pages 1564-1599, 1999.  3   STATISTICAL LEARNING WITH A NUISANCE COMPONENT  nuisance estimation error is of the order R2 Gn suffice when the target is parametric.  so, once again, n\u22121/4 RMSE rates for the nuisance  We give conditions on the relative complexity of the target and nuisance classes\u2014quantified via metric entropy\u2014under which the sample splitting meta-algorithm achieves oracle rates (assuming the two black-box estimation algorithms are appropriately instantiated). This allows us to extend several prior works beyond the parametric regime to complex nonparametric target classes. Our technical results extend the work of Yang and Barron (1999); Rakhlin et al. (2017), which provide minimax optimal rates without nuisance components and utilize the technique of aggregation in designing optimal algorithms. We also provide bounds for plug-in empirical risk minimization that extend the local Rademacher complexity analysis of generalization error (Bartlett et al., 2005) to account for the impact of the nuisance error.  References  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1-C68, 2018.  Jerzy Neyman. Optimal asymptotic tests of composite hypotheses. Probability and statsitics, pages  Jerzy Neyman. C (\u03b1) tests and their use. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages  213-234, 1959.  1-21, 1979.  Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax  regret and minimax risk. Bernoulli, 23(2):789-824, 2017.  Vladimir N Vapnik. The nature of statistical learning theory. Springer, 1995.  Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, pages 1564-1599, 1999."}