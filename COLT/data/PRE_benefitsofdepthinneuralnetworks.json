{"1": "Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Yoshua Bengio and Olivier Delalleau. Shallow vs. deep sum-product networks. In NIPS, 2011.  Jacek Bochnak, Michal Coste, and Marie-Franc\u00b8oise Roy. Real Algebraic Geometry. Springer, 1998.  Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning  algorithms. pages 161-168, 2006.  John Duchi. Statistics 311/electrical engineering 377: Information theory and statistics. Stanford  University, 2016.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. 2015.  arXiv:1512.03965 [cs.LG].  Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 1980.  Johan H\u02daastad. Computational Limitations of Small Depth Circuits. PhD thesis, Massachusetts  Institute of Technology, 1986.  Daniel Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. 2015. arXiv:1511.07860v1 [cs.CC].  Andrei Kolmogorov. \u00a8Uber die beste ann\u00a8aherung von funktionen einer gegebenen funktionenklasse.  Annals of Mathematics, 37(1):107-110, 1936.  Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition. 114:953-956, 1957.  Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolu-  tional neural networks. In NIPS, 2012.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks.  2015. arXiv:1411.7717v3 [cs.LG].  Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear  regions of deep neural networks. In NIPS, 2014.  Hoifung Poon and Pedro M. Domingos. Sum-product networks: A new deep architecture. In UAI  2011, pages 337-346, 2011.  Benjamin Rossman, Rocco A. Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem  for boolean circuits. In FOCS, 2015.  13   BENEFITS OF DEPTH IN NEURAL NETWORKS  References  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Yoshua Bengio and Olivier Delalleau. Shallow vs. deep sum-product networks. In NIPS, 2011.  Jacek Bochnak, Michal Coste, and Marie-Franc\u00b8oise Roy. Real Algebraic Geometry. Springer, 1998.  Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning  algorithms. pages 161-168, 2006.  John Duchi. Statistics 311/electrical engineering 377: Information theory and statistics. Stanford  University, 2016.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. 2015.  arXiv:1512.03965 [cs.LG].  Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 1980.  Johan H\u02daastad. Computational Limitations of Small Depth Circuits. PhD thesis, Massachusetts  Institute of Technology, 1986.  Daniel Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. 2015. arXiv:1511.07860v1 [cs.CC].  Andrei Kolmogorov. \u00a8Uber die beste ann\u00a8aherung von funktionen einer gegebenen funktionenklasse.  Annals of Mathematics, 37(1):107-110, 1936.  Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition. 114:953-956, 1957.  Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolu-  tional neural networks. In NIPS, 2012.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks.  2015. arXiv:1411.7717v3 [cs.LG].  Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear  regions of deep neural networks. In NIPS, 2014.  Hoifung Poon and Pedro M. Domingos. Sum-product networks: A new deep architecture. In UAI  2011, pages 337-346, 2011.  Benjamin Rossman, Rocco A. Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem  for boolean circuits. In FOCS, 2015. TELGARSKY  Michael Schmitt. Lower bounds on the complexity of approximating continuous functions by sig-  moidal neural networks. In NIPS, 2000.  Lech Szymanski and Brendan McCane. Deep networks are effective encoders of periodicity. IEEE  Transactions on Neural Networks and Learning Systems, 25(10):1816-1827, 2014.  Matus Telgarsky.  Representation benefits of deep feedforward networks.  2015.  arXiv:1509.08101v2 [cs.LG].  Anatoli Vitushkin. On multidimensional variations. GITTL, 1955. In Russian.  Anatoli Vitushkin. Estimation of the complexity of the tabulation problem. Fizmatgiz., 1959. In  Russian.  Hugh E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the  American Mathematical Society, 133(1):167-178, 1968.  Karl Weierstrass.  \u00a8Uber die analytische darstellbarkeit sogenannter willk\u00a8urlicher functionen einer reellen ver\u00a8anderlichen. Sitzungsberichte der Akademie zu Berlin, pages 633-639, 789-805, 1885."}