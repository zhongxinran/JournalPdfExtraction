{"1": "Takeshi Amemiya. Regression analysis when the dependent variable is truncated normal. Econo-  metrica: Journal of the Econometric Society, pages 997-1016, 1973.  N Balakrishnan and Erhard Cramer. The art of progressive censoring. Springer, 2014.  4   TRUNCATED LINEAR REGRESSION  Nevertheless, we show that, given oracle access to set S, we can get an un-biased sample of the gradient. If (xt, yt) is a (randomly chosen) sample processed by PSGD at step t, and wt the current iterate, we perform rejection sampling to obtain a sample from the Gaussian N (wT t xt, 1) condi- tioned on the truncation set S, in order to compute an unbiased estimate of the gradient. Because we use rejection sampling, it is important to maintain that PSGD remains within a region where the rejection sampling will succeed with constant probability with respect to a random choice of xt, and this is guaranteed by our analysis.  1.2. Further Related Work  We have already surveyed work on truncated and censored linear regression since the 1950s. Early precursors of this literature can be found in the simpler, non-regression version of our problem, where the x(i)\u2019s are single-dimensional and equal, which corresponds to estimating a truncated Normal distribution. This problem goes back to at least Galton (1897), Pearson (1902), Pearson and Lee (1908), and Fisher (1931). Following these early works, there has been a large volume of research devoted to estimating truncated Gaussians or other truncated distributions in one or multiple dimensions; see e.g. Hotelling (1948); Tukey (1949), and Schneider (1986); Cohen (2016); Balakrishnan and Cramer (2014) for an overview of this work. There do exist consistent estimators for estimating the parameters of truncated distributions, but, as in the case of truncated and censored regression, the optimal estimation rates are mostly not well-understood. Only very recentwork of Daskalakis et al. (2018) provides computationally and statistically efficient estimators for the parameters of truncated high-dimensional Gaussians. Similar to the present work, Daskalakis et al. (2018) we use PSGD to optimize the negative log-likelihood of the truncated samples. Showing that the negative log-likelihood is convex in the truncated Gaussian setting follows immediately from the fact that a truncated Gaussian belongs to the exponential family. In our setting it is non- standard; see also discussion in Amemiya (1973). Moreover, identifying the set where the negative log-likelihood is strongly convex and establishing its strong convexity are also simpler tasks in the truncated Gaussian setting compared to the truncated regression setting, due to the shifting of the mean of the samples induced by the different covariate vectors x(i).  Last but not least, our work is related, albeit more loosely, to the literature on robust Statistics, which has recently been revived by a strand of fantastic works Xu et al. (2010); Cand`es et al. (2011); Diakonikolas et al. (2016); Lai et al. (2016); Diakonikolas et al. (2017, 2018); Bhatia et al. (2015); Diakonikolas et al. (2019). For the most part, these works assume that an adversary perturbs a small fraction of the samples arbitrarily. Compared to truncation and censoring, these perturbations are harder to handle. As such only small amounts of perturbation can be accommodated, and the parameters cannot be estimated to arbitrary precision. In contrast, in our setting the truncation set S may very well have an ex ante probability of obliterating most of the observations, say 99% of them, yet the parameters of the model can still be estimated to arbitrary precision.  References  Takeshi Amemiya. Regression analysis when the dependent variable is truncated normal. Econo-  metrica: Journal of the Econometric Society, pages 997-1016, 1973.  N Balakrishnan and Erhard Cramer. The art of progressive censoring. Springer, 2014. TRUNCATED LINEAR REGRESSION  Yoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.  Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Richard Breen et al. Regression models: Censored, sample selected, or truncated data, volume 111.  Sage, 1996.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  A Clifford Cohen. Truncated and censored samples: theory and applications. CRC press, 2016.  Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In the 59th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2018.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In Proceedings of the 34th International Being robust (in high dimensions) can be practical. Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 999-1008, 2017. URL http://proceedings.mlr.press/v70/diakonikolas17a. html.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a gaussian: Getting optimal error, efficiently. In Proceedings of the Twenty- Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2683-2702, 2018. doi: 10.1137/1.9781611975031.171. URL https://doi.org/10.1137/1.9781611975031.171.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for In the 30th Annual ACM-SIAM Symposium on Discrete Algorithms  robust linear regression. (SODA), 2019.  RA Fisher. Properties and applications of Hh functions. Mathematical tables, 1:815-852, 1931.  Francis Galton. An examination into the registered speeds of american trotting horses, with remarks on their value as hereditary data. Proceedings of the Royal Society of London, 62(379-387):310- 315, 1897.  Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.  In International Conference on Machine Learning, pages 3059-3068, 2016. TRUNCATED LINEAR REGRESSION  Jerry A Hausman and David A Wise. Social experimentation, truncated distributions, and efficient  estimation. Econometrica: Journal of the Econometric Society, pages 919-938, 1977.  Harold Hotelling. Fitting generalized truncated normal distributions. In Annals of Mathematical  Statistics, volume 19, pages 596-596, 1948.  Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 665-674, 2016. doi: 10.1109/ FOCS.2016.76. URL https://doi.org/10.1109/FOCS.2016.76.  Alice Lee. Table of the Gaussian \u201cTail\u201d Functions; When the \u201cTail\u201d is Larger than the Body.  Biometrika, 10(2/3):208-214, 1914.  Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807- 814, 2010.  Karl Pearson. On the systematic fitting of frequency curves. Biometrika, 2:2-7, 1902.  Karl Pearson and Alice Lee. On the generalised probable error in multiple normal correlation.  Biometrika, 6(1):59-68, 1908.  Helmut Schneider. Truncated and censored samples from normal populations. Marcel Dekker, Inc.,  1986.  309-311, 1949.  James Tobin. Estimation of relationships for limited dependent variables. Econometrica: journal  of the Econometric Society, pages 24-36, 1958.  John W Tukey. Sufficiency, truncation and selection. The Annals of Mathematical Statistics, pages  Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis with contami-  nated data: The high dimensional case. arXiv preprint arXiv:1002.4658, 2010."}