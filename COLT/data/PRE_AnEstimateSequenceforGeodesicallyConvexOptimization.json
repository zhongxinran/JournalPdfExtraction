{"1": "University Press, 2009.  The authors thank the anonymous reviewers for helpful feedback. This work was supported in part by NSF-IIS-1409802 and the DARPA Lagrange grant.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton  Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local  minima for nonconvex optimization in linear time. CoRR, abs/1611.01146, 2016.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror  descent. arXiv:1407.1537, 2014.  Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9, et al. Metric measure spaces with Riemannian Ricci curvature  bounded from below. Duke Mathematical Journal, 163(7):1405-1490, 2014.  Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds for smooth and strongly  convex optimization problems. arXiv:1503.06833, 2015.  11   AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  6. Discussion  In this work, we proposed a Riemannian generalization of the accelerated gradient algorithm and developed its convergence and complexity analysis. For the first time (to the best of our knowledge), we show gradient based algorithms on Riemannian manifolds can be accelerated, at least in a neighborhood of the minimizer. Central to our analysis are the two main technical contributions of our work: a new estimate sequence (Lemma 4), which relaxes the assumption of Nesterov\u2019s original construction and handles metric distortion on Riemannian manifolds; a tangent space distance comparison theorem (Theorem 10), which provides sufficient conditions for bounding the metric distortion and could be of interest for a broader range of problems on Riemannian manifolds.  Despite not matching the standard convex results, our result exposes the key difficulty of analyzing Nesterov-style algorithms on Riemannian manifolds, an aspect missing in previous work. Critically, the convergence analysis relies on bounding a new distortion term per each step. Furthermore, we observe that the side length sequence d(yk, vk+1) can grow much greater than d(yk, x\u2217), even if we reduce the \u201cstep size\u201d hk in Algorithm 1, defeating any attempt to control the distortion globally by modifying the algorithm parameters. This is a benign feature in vector space analysis, since (8) trivially holds nonetheless; however it poses a great difficulty for analysis in nonlinear space. Note the stark contrast to (stochastic) gradient descent, where the step length can be effectively controlled by reducing the step size, hence bounding the distortion terms globally (Zhang and Sra, 2016).  A topic of future interest is to study whether assumption (8) can be further relaxed, while maintaining that overall the algorithm still converges. By bounding the squared distance distortion in every step, our analysis provides guarantee for the worst-case scenario, which seems unlikely to happen in practice. It would be interesting to conduct experiments to see how often (8) is violated versus how often it is loose. It would also be interesting to construct some adversarial problem case (if any) and study the complexity lower bound of gradient based Riemannian optimization, to see if geodesically convex optimization is strictly more difficult than convex optimization. Generalizing the current analysis to non-strongly g-convex functions is another interesting direction.  Acknowledgments  References  University Press, 2009.  The authors thank the anonymous reviewers for helpful feedback. This work was supported in part by NSF-IIS-1409802 and the DARPA Lagrange grant.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton  Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local  minima for nonconvex optimization in linear time. CoRR, abs/1611.01146, 2016.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror  descent. arXiv:1407.1537, 2014.  Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9, et al. Metric measure spaces with Riemannian Ricci curvature  bounded from below. Duke Mathematical Journal, 163(7):1405-1490, 2014.  Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds for smooth and strongly  convex optimization problems. arXiv:1503.06833, 2015. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  Hedy Attouch, J\u00e9r\u00f4me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, 137(1-2):91-129, 2013.  Miroslav Bac\u00e1k. Convex analysis and optimization in Hadamard spaces, volume 22. Walter de Gruyter GmbH  & Co KG, 2014.  Nicolas Boumal, P-A Absil, and Coralia Cartis. Global rates of convergence for nonconvex optimization on  manifolds. arXiv:1605.08101, 2016a.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016b.  S\u00e9bastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov\u2019s accelerated gradient  descent. arXiv:1506.08187, 2015.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society Providence, 2001.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex  optimization. CoRR, abs/1611.00756, 2016.  Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. \" convex until proven guilty\": Dimension-free  acceleration of gradient descent on non-convex functions. arXiv preprint arXiv:1705.02766, 2017.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  OP Ferreira and PR Oliveira. Proximal point algorithm on Riemannian manifolds. Optimization, 51(2):  257-270, 2002.  Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In Conference  on Learning Theory, pages 658-695, 2015.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified  geometric analysis. arXiv:1704.00708, 2017.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic  programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In  Advances in Neural Information Processing Systems, pages 315-323, 2013.  J\u00fcrgen Jost. Riemannian Geometry and Geometric Analysis. Springer Science & Business Media, 2011.  Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the  sixteenth annual ACM symposium on Theory of computing, pages 302-311. ACM, 1984.  Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic variance reduced gradient on  Grassmann manifold. arXiv:1605.07367, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing  Systems, pages 586-594, 2016. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  Leonid G Khachiyan. Polynomial algorithms in linear programming. USSR Computational Mathematics and  Mathematical Physics, 20(1):53-72, 1980.  Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via  integral quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.  Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order methods In Advances in Neural Information  for geodesically convex optimization on Riemannian manifolds. Processing Systems, pages 4875-4884, 2017.  Wolfgang Meyer. Toponogov\u2019s theorem and applications. SMR, 404:9, 1989.  Bamdev Mishra and Rodolphe Sepulchre. Riemannian preconditioning. SIAM Journal on Optimization, 26(1):  635-660, 2016.  optimization. Wiley, 1983.  Arkadi\u02d8\u0131 Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method efficiency in  Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In  Soviet Mathematics Doklady, volume 27(2), pages 372-376, 1983.  Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media,  2004.  B.T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and  Mathematical Physics, 3(4):864-878, January 1963.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 314-323, 2016.  Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average  gradient. arXiv:1309.2388, 2013.  Ohad Shamir. A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate. In International  Conference on Machine Learning (ICML-15), pages 144-152, 2015.  Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510-2518, 2014.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the  geometric picture. IEEE Transactions on Information Theory, 63(2):853-884, 2017.  Constantin Udriste. Convex functions and optimization methods on Riemannian manifolds, volume 297.  Springer Science & Business Media, 1994.  Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in  optimization. Proceedings of the National Academy of Sciences, page 201614734, 2016.  Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In 29th Annual  Conference on Learning Theory (COLT), pages 1617-1638, 2016.  Hongyi Zhang, Sashank J. Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on  Riemannian manifolds. In Advances in Neural Information Processing Systems 29, 2016. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION"}