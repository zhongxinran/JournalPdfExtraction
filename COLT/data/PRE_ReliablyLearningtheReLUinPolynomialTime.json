{"1": "Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning sparse polynomial functions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algo- rithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 500-510, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anribit Mukherjee. Understanding deep neural networks with rectified linear units, 2016. URL: https://arxiv.org/abs/1611.01491.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. 2014.  Peter Bartlett, Daniel Kane, and Adam Klivans. Personal communication. 2017.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. JACM: Journal of the ACM, 50, 2003.  Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other  kernel-based learning methods. Cambridge University Press, 2000.  36   GOEL KANADE KLIVANS THALER  Corollary 49 Let A be an algorithm that learns ReLUs on all domains X \u2286 Rn where (w \u00b7 x) may take on values that are \u03c9(1) with respect to the dimension n. Then any algorithm for reli- ably learning C in time g((cid:15)) \u00b7 poly(n) will break the Sparse Learning Parity with Noise hardness assumption.  Finally, we point out Kalai et al. (2012) proved that reliably learning conjunctions is also as hard as PAC Learning DNF formulas. Thus, by our above reduction, any efficient algorithm for reliably learning ReLUs would give an efficient algorithm for PAC learning DNF formulas (again this would be considered a breakthrough result in computational learning theory).  6. Conclusions and Open Problems  We have given the first set of efficient algorithms for ReLUs in a natural learning model. ReLUs are both effective in practice and, unlike linear threshold functions (halfspaces), admit non-trivial learning algorithms for all distributions with respect to adversarial noise. We \u201csidestepped\u201d the hardness results in Boolean function learning by focusing on problems that are not entirely scale- invariant with respect to the choice of domain (e.g., reliably learning ReLUs). The obvious open question is to improve the dependence of our main result on 1/(cid:15). We can handle (cid:15) = 1/ log n, and as mentioned in the introduction, (cid:15) = 1/poly(n) seems difficult. Is it possible to obtain a run-time of poly(n, k) \u00b7 2O(1/(cid:15)) for depth-2 networks of ReLUs with k hidden units?  Acknowledgements. The authors are grateful to Sanjeev Arora and Roi Livni for helpful feedback and useful discussions on this work.  References  Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning sparse polynomial functions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algo- rithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 500-510, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anribit Mukherjee. Understanding deep neural networks with rectified linear units, 2016. URL: https://arxiv.org/abs/1611.01491.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. 2014.  Peter Bartlett, Daniel Kane, and Adam Klivans. Personal communication. 2017.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. JACM: Journal of the ACM, 50, 2003.  Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other  kernel-based learning methods. Cambridge University Press, 2000. RELIABLY LEARNING THE RELU IN POLYNOMIAL TIME  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC, pages 105-117. ACM, 2016. ISBN 978-1-4503-4132-5. URL http://dl.acm.org/citation.cfm?id= 2897518.  Ilias Diakonikolas, Daniel M. Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In FOCS, pages 11-20. IEEE Computer Society, 2010. ISBN 978-0-7695- 4244-7.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In Pro- ceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23- 26, 2016, pages 907-940, 2016. URL http://jmlr.org/proceedings/papers/v49/ eldan16.html.  Bassey Etim. Approve or Reject: Can You Moderate Five New York Times Comments? The New York Times, 2016. URL http://www.nytimes.com/interactive/2016/09/ 20/insider/approve-or-reject-moderation-quiz.html. Originally published September 20, 2016. Retrieved October 4, 2016.  V. Feldman, P. Gopalan, S. Khot, and A. K. Ponnuswami. On agnostic learning of parities, mono- mials, and halfspaces. SIAM J. Comput, 39(2):606-645, 2009. URL http://dx.doi.org/ 10.1137/070684914.  Vitaly Feldman and Pravesh Kothari. Agnostic learning of disjunctions on symmetric distributions.  Journal of Machine Learning Research, 16:3455-3467, 2015.  Jerome H. Friedman. Multivariate adaptive regression splines. Ann. Statist, 1991.  David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Inf. Comput., 100(1):78-150, 1992.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026-1034, 2015.  Thomas Hofmann, Bernhard Sch\u00a8olkopf, and Alexander J Smola. Kernel methods in machine learn-  ing. The annals of statistics, pages 1171-1220, 2008.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:  Risk bounds, margin bounds, and regularization. 2008.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008. doi: 10.1137/ 060649057.  Adam Tauman Kalai, Varun Kanade, and Yishay Mansour. Reliable agnostic learning. Journal of  Computer and System Sciences, 78(5):1481-1495, 2012. GOEL KANADE KLIVANS THALER  Kenji Kawaguchi.  Deep learning without poor  Masashi Sugiyama, Ulrike V. Luxburg, itors, NIPS, pages 586-594, advances-in-neural-information-processing-systems-29-2016.  In Daniel D. Lee, Isabelle Guyon, ed- URL http://papers.nips.cc/book/  and Roman Garnett,  local minima.  2016.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning.  Mach. Learn., 17(2-3):115-141, 1994.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In  RANDOM, 2014.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci, 75(1):2-12, 2009. URL http://dx.doi.org/10.1016/ j.jcss.2008.07.008.  Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7533):436-444, May 2015.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of train- ing neural networks. pages 855-863, 2014. URL http://papers.nips.cc/paper/ 5267-on-the-computational-efficiency-of-training-neural-networks.  Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural  network acoustic models. In Proc. ICML, volume 30, 2013.  Alessandro Magnani and Stephen P. Boyd. Convex piecewise-linear fitting. Optimization and ISSN 1573-2924. doi: 10.1007/s11081-008-9045-3. URL  Engineering, 10(1):1-17, 2009. http://dx.doi.org/10.1007/s11081-008-9045-3.  James Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character, 209:415-446, 1909.  Donald J. Newman. Rational approximation to |x|. Michigan Math. J., 11(1):11-14, 03 1964.  Phillippe Rigollet. High-Dimensional Statistics. MIT, 1st edition, 2015.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with  the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, 2011.  Alexander A. Sherstov. Making polynomials robust to noise. In Proceedings of the Forty-fourth Annual ACM Symposium on Theory of Computing, STOC \u201912, pages 747-758, New York, NY, USA, 2012. ACM. RELIABLY LEARNING THE RELU IN POLYNOMIAL TIME  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. J. ACM, 62(2):13:1-13:45, May 2015. doi: 10.1145/2728167. URL http://doi.acm.org/10.1145/2728167.  Wikipedia. Multinomial theorem \u2014 Wikipedia, the free encyclopedia, 2016a. URL: https:  //en.wikipedia.org/wiki/Multinomial_theorem.  Wikipedia. Polynomial kernel \u2014 Wikipedia, the free encyclopedia, 2016b. URL: https://en.  wikipedia.org/wiki/Polynomial_kernel.  Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and  neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.  Yuchen Zhang, Jason Lee, and Michael Jordan. (cid:96)1 networks are improperly learnable in polynomial-  time. In ICML, 2016a.  Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks.  arXiv preprint arXiv:1609.01000, 2016b."}