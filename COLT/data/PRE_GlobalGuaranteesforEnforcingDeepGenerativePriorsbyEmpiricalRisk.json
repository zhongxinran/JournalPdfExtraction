{"1": "Ali Ahmed, Benjamin Recht, and Justin Romberg. Blind deconvolution using convex programming.  IEEE Transactions on Information Theory, 60(3):1711-1732, 2014.  Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with  implications for training. CoRR, abs/1511.05653, 2015.  Bandeira, Boumal, and Voroninski. On the low-rank approach for semidefinite programs arising in  synchronization and community detection. JMLR, 49:1-22, 2016.  Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253-263, 2008.  Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using genera-  tive models. arXiv:1703.03208, 2017.  6   DEEP GENERATIVE PRIORS  In the case that A \u201c In, the RRIC is trivially satisfied, and we get the following corollary about  inverting multilayer neural networks.  Corollary 3 (Approximate Invertibility of Multilayer Neural Networks) If G is a d-layer neu- ral network such that Wi satisfies the WDC with constant (cid:15) for all i \u201c 1 . . . d, then the function f pxq \u201c }Gpxq \u00b4 Gpx0q}2 has no stationary points outside of a neighborhood around x0 and \u00b4\u03c1dx0.  In the case of a Gaussian network with Gaussian measurements, the WDC and RRIC are satis- fied with high probability if the network is sufficiently expansive and there are a sufficient number of measurements.  Proposition 4 Fix 0 \u0103 (cid:15) \u0103 1. Assume ni \u011b cni\u00b41 log ni\u00b41 for all i \u201c 1 . . . d and m \u0105 cdk log \u03a0d i\u201c1ni. Assume the entires of Wi are i.i.d. N p0, 1{niq, and the entries of A are i.i.d. N p0, 1{mq. Then, Wi satisfies the WDC with constant (cid:15) for all i and A satisfies the RRIC with \u0159 d i\u201c1 \u02dccnie\u00b4\u03b3ni\u00b41 \u00b4 \u02dcce\u00b4\u03b3m. Here, c and respect to G with constant (cid:15) with probability at least 1 \u00b4 \u03b3\u00b41 are constants that depend polynomially on (cid:15)\u00b41, and \u02dcc is a universal constant.  As stated after Theorem 1, no assumption is made on the independence between Wi and Wj for i \u2030 j. While Proposition 4 is stated for A P Rm\u02c6n with i.i.d. Gaussian entries, it also applies in the case of any random matrix that satisfies the following concentration of measure condition:  `  \u02d8  P  |}Ax}2  2 \u00b4 }x}2  2| \u011b (cid:15)}x}2 2  \u010f 2e\u00b4mc0p(cid:15)q,  for any fixed x P Rn, where c0p(cid:15)q is a positive constant depending only on (cid:15). In particular, Proposi- tion 4 and hence Theorem 1 extends to the case of where the entries of A are independent Bernoulli random variables (and the entries of Wi are Gaussian). See Baraniuk et al. (2008) for more.  Acknowledgments  PH is partially supported by NSF DMS-1464525.  References  Ali Ahmed, Benjamin Recht, and Justin Romberg. Blind deconvolution using convex programming.  IEEE Transactions on Information Theory, 60(3):1711-1732, 2014.  Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with  implications for training. CoRR, abs/1511.05653, 2015.  Bandeira, Boumal, and Voroninski. On the low-rank approach for semidefinite programs arising in  synchronization and community detection. JMLR, 49:1-22, 2016.  Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253-263, 2008.  Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using genera-  tive models. arXiv:1703.03208, 2017. DEEP GENERATIVE PRIORS  E. Cand`es, Y. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. SIAM  J. Imaging Sci., 6(1):199-225, 2013a. doi: 10.1137/110848074.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Emmanuel J. Cand`es, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207- 1223, 2006. ISSN 1097-0312. doi: 10.1002/cpa.20124.  Emmanuel J Cand`es, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Comm. Pure Appl. Math., 66(8):1241-1274, 2013b.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. Information Theory, IEEE Transactions on, 61(4):1985-2007, 2015.  Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739-747, 2015.  Yuxin Chen and Emmanuel Candes. The projected power method: An efficient algorithm for joint  alignment from pairwise differences. arXiv preprint arXiv:1609.05820, 2016.  David Donoho. For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution. Communications on Pure and Applied Mathematics, 59(6), 2006.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Wen Huang and Paul Hand. Blind deconvolution by a steepest descent algorithm on a quotient  manifold. arXiv preprint arXiv:1710.03309, 2017.  Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016.  Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. Rapid, robust, and reliable blind  deconvolution via nonconvex optimization. arXiv preprint arXiv:1606.04933, 2016.  Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.  Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.  S. Mallat. Group invariant scattering. Comm. Pure Appl. Math., 65(10):13311398, 2012.  Tyler Maunu, Teng Zhang, and Gilad Lerman. A well-tempered landscape for non-convex robust  subspace recovery. arXiv preprint arXiv:1706.03896, 2017. DEEP GENERATIVE PRIORS  Katta G. Murty and Santosh N. Kabadi. Some np-complete problems in quadratic and nonlinear  programming. Mathematical Programming, 39(2):117-129, Jun 1987.  Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play In Computer generative networks: Conditional iterative generation of images in latent space. Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 3510-3520. IEEE, 2017.  Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving  for simplicity: The all convolutional net. CoRR, abs/1412.6806, 2014.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory  (ISIT), 2016 IEEE International Symposium on, pages 2379-2383. IEEE, 2016."}