{"1": "Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and  application to sparse stochastic bandits. In AISTATS, 2012.  13   CORRALLING A BAND OF BANDIT ALGORITHMS  5.2.2. OTHER EXAMPLES  We brie\ufb02y mention some other examples without giving details. For contextual bandits, if we have different ways to represent the contexts, then each base algorithm can be any existing contextual bandit algorithm with a specific context representation and policy space. The master can then have good performance as long as one of these representations captures the problem well.  For stochastic linear bandits, \u0398 \u2282 Rd is a compact convex set and ft(\u03b8, x) = (cid:104)\u03b8, c\u2217(cid:105) + \u03bet where c\u2217 \u2208 Rd is fixed and unknown, and \u03bet is some zero-mean noise. Previous works have studied cases where c\u2217 is assumed to admit some special structures, such as sparsity, group-sparsity and so on (see for example (Abbasi-Yadkori et al., 2012; Carpentier and Munos, 2012; Johnson et al., 2016)). One can then run CORRAL with different base algorithms assuming different structures of c\u2217. Another related problem is generalized linear bandits, where ft(\u03b8, x) = \u03c3((cid:104)\u03b8, c\u2217(cid:105)) + \u03bet for some link function \u03c3 (such as the logistic function, exponential function and so on, see (Filippi et al., 2010)). It is clear that one can run CORRAL with different base algorithms using different link functions to capture more possibilities of the environments. In all these cases, the number of base algorithms is relatively small.  6. Conclusion and Open Problems  In this work, we presented a master algorithm which can combine a set of base algorithms and per- form as well as the best of them in a very strong sense in the bandit setting. Two major applications of our approach were presented to illustrate how this master algorithm can be used to create more adaptive bandit algorithms in a black-box fashion.  There are two major open problems left in this direction. One is to improve the results of Theorem 7 so that the master can basically inherit the same regret bounds of all the base algorithms, i.e., Eq. (1) holds simultaneously for all base algorithms satisfying stability condition with \u03b1i < 1. Note that this is in general impossible (see (Lattimore and Szepesv\u00b4ari, 2016) for a lower bound in a special case), but it is not clear whether it is possible if we only care about the scaling with T while allowing worse dependence on other parameters. The current approach fails to achieve this mainly because each of these bounds requires a different tuning of the same learning rate \u03b7.  Another open problem is to improve the dependence on M , the number of base algorithms, from polynomial to logarithmic while keeping the same dependence on other parameters (or prove its impossibility). Logarithmic dependence on M can be achieved by using EXP4 as the master, but as was earlier discussed, this leads to poor dependence on other parameters.  The authors would like to thank John Langford for posing the question initially that stimulated this research. Most of the work was completed when Behnam Neyshabur was an intern at Microsoft Research.  Acknowledgments  References  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and  application to sparse stochastic bandits. In AISTATS, 2012. AGARWAL LUO NEYSHABUR SCHAPIRE  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-point methods for full-information and bandit online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smooth-  ness. In Advances in Neural Information Processing Systems, 2015.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming In Proceedings of the 31st  the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning, 2014.  Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11(Oct):2785-2836, 2010.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3(Nov):397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Annual Conference on Learning Theory (COLT), 2016.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002b.  Sbastien Bubeck and Nicol Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. ISSN 1935-8237. doi: 10.1561/2200000024.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex optimization. In Proceedings of the 29th Annual Conference on Learning Theory (COLT), 2016.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial  bandits. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), volume 23, 2012.  S\u00b4ebastien Bubeck, Ofer Dekel, Tomer Koren, and Yuval Peres. Bandit convex optimization:  T In Proceedings of the 28th Annual Conference on Learning Theory  regret in one dimension. (COLT), 2015.  \u221a  S\u00b4ebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016. CORRALLING A BAND OF BANDIT ALGORITHMS  Alexandra Carpentier and R\u00b4emi Munos. Bandit theory meets compressed sensing for high dimen-  sional stochastic linear bandit. In AISTATS, 2012.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff  functions. In AISTATS, volume 15, pages 208-214, 2011.  Uriel Feige, Tomer Koren, and Moshe Tennenholtz. Chasing ghosts: competing with stateful poli-  cies. In Foundations of Computer Science (FOCS), pages 100-109. IEEE, 2014.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth an- nual ACM-SIAM symposium on Discrete algorithms, pages 385-394. Society for Industrial and Applied Mathematics, 2005.  Dylan Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems, 2016.  Nicholas Johnson, Vidyashankar Sivakumar, and Arindam Banerjee. Structured stochastic linear  bandits. arXiv preprint arXiv:1606.05693, 2016.  Emilie Kaufmann, Nathaniel Korda, and R\u00b4emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In International Conference on Algorithmic Learning Theory, pages 199-213. Springer, 2012.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in neural information processing systems, pages 817-824, 2008.  Tor  Lattimore bandits.  Szepesv\u00b4ari. ear at posts lower-bounds-for-stochastic-linear-bandits, 2016.  and Csaba Blog  lin- http://banditalgs.com/2016/10/20/  stochastic  bounds  Lower  for  Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661-670. ACM, 2010.  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. In Foundations of  Computer Science, 1989., 30th Annual Symposium on, pages 256-261. IEEE, 1989.  Odalric-Ambrym Maillard and R\u00b4emi Munos. Adaptive bandits: Towards the best history-dependent  strategy. In AISTATS, pages 570-578, 2011. AGARWAL LUO NEYSHABUR SCHAPIRE  Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-  ming, volume 13. Siam, 1994.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceed-  ings of the 26th Annual Conference on Learning Theory (COLT), pages 993-1019, 2013.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, 2016.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling.  Journal of Machine Learning Research, 2014.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems, 2016.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933."}