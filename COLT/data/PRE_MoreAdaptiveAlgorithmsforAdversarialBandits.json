{"1": "Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008.  12   MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  wt,i  (cid:8) (cid:104)w, mt(cid:105) + D\u03c8t(w, w(cid:48)  We invoke BROAD-OMD with at = 0, \u02c6(cid:96)t,i = (cid:96)t,i1{it=i}  Note that a\u2217 can be different from the empirically best arm i\u2217 defined in Section 2. The expected regret in this setting is still with respect to i\u2217 and further takes into consideration the randomness over losses. In other words, we care about E(cid:96)1,...,(cid:96)T [Ei1,...,iT [RegT ]], abbreviated as E[RegT ] still. being the typical importance-weighted unbiased estimator, and a somewhat special choice of mt: mt,i = (cid:96)t,it for all i. This choice of mt is seemingly invalid since it depends on it, which is drawn after we have constructed wt based on mt itself. However, note that because mt now has identical coordinates, we have wt = argminw\u2208\u2206K t, independent of the actual value of mt. Therefore, the algorithm is still valid and is in fact equivalent to the vanilla log-barrier OMD of (Foster et al., 2016). Also note that we cannot define \u02c6(cid:96)t as in previous sections (in terms of mt) since it is not an unbiased estimator of (cid:96)t anymore (due to the randomness of mt). Although the algorithm is the same, using our analysis framework we actually derive a tighter t,i(\u02c6(cid:96)t,i \u2212 (cid:96)t,it)2 = i=1((cid:96)t,i1{it = i} \u2212 wt,i(cid:96)t,it)2. It turns out that based on this quantity alone, one can derive both a \u201csmall-loss\u201d bound for the adversarial setting and a logarithmic bound for the stochastic setting as shown below. We emphasize that the doubling trick of Algorithm 3 is essential to make the algorithm parameter-free, which is another key difference from (Foster et al., 2016).  bound in terms of the following quantity based on Theorem 7: (cid:80)T (cid:80)T  t)(cid:9) = argminw\u2208\u2206K  (cid:8)D\u03c8t(w, w(cid:48)  t)(cid:9) = w(cid:48)  i=1 w2  (cid:80)K  (cid:80)K  t=1  t=1  Theorem 10 BROAD-OMD with at = 0, mt,i = (cid:96)t,it, \u02c6(cid:96)t,i = (cid:96)t,i1{it=i} (Algorithm 3), guarantees  wt,i  , and the doubling trick  E [RegT ] = O  \uf8ed  \uf8eb  (cid:118) (cid:117) (cid:117) (cid:116)(K ln T )E  (cid:34) T  (cid:88)  K (cid:88)  t=1  i=1  ((cid:96)t,i1{it = i} \u2212 wt,i(cid:96)t,it)2  + K ln T  \uf8f8 .  (9)  (cid:35)  \uf8f6  (cid:1), while in the ad- This bound implies that in the stochastic setting, we have E [RegT ] = O (cid:0) K ln T versarial setting, we have E [RegT ] = O (cid:0)(cid:112)KLT,i\u2217 ln T + K ln T (cid:1) assuming non-negative losses.  \u2206  5. Conclusions and Discussions  In this work we develop and analyze a general bandit algorithm using techniques such as optimistic mirror descent, log-barrier regularizer, increasing learning rate, and so on. We show various appli- cations of this general framework, obtaining several more adaptive algorithms that improve previous works. Future directions include 1) improving the dependence on K for the path-length results; 2) obtaining second-order path-length bounds; 3) generalizing the results to the linear bandit problem.  Acknowledgement. CYW is grateful for the support of NSF Grant #1755781. The authors would like to thank Chi-Jen Lu for posing the problem of bandit path-length, and to thank Chi-Jen Lu and Yi-Te Hong for helpful discussions in this direction.  References  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In Conference on Learning Theory, pages 12-38, 2017.  Chamy Allenberg, Peter Auer, Laszlo Gyorfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In Algorithmic Learning Theory, volume 4264, pages 229-243. Springer, 2006.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, pages 42-1, 2012.  S\u00b4ebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016.  S\u00b4ebastien Bubeck, Michael B. Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-  armed bandits. arXiv preprint arXiv:1711.01037, 2017.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.  Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds.  In Conference on Learning Theory, pages 210-227, 2013.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In Conference  on Learning Theory, 2017.  Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms  for zero-sum games. Games and Economic Behavior, 92:327-348, 2015.  Steven De Rooij, Tim Van Erven, Peter D Gr\u00a8unwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15(1):1281-1316, 2014.  R\u00b4emy Degenne and Vianney Perchet. Anytime optimal algorithms in stochastic multi-armed ban-  dits. In International Conference on Machine Learning, pages 1587-1595, 2016.  Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems, pages 4734-4742, 2016.  Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In  Conference on Learning Theory, pages 176-196, 2014.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The kl-ucb algorithm for bounded stochastic bandits and  beyond. In Conference On Learning Theory, pages 359-376, 2011.  S\u00b4ebastien Gerchinovitz and Tor Lattimore. Refined lower bounds for adversarial bandits. In Ad-  vances in Neural Information Processing Systems, pages 1198-1206, 2016.  Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning  Research, 12(Apr):1287-1311, 2011a.  Elad Hazan and Satyen Kale. A simple multi-armed bandit algorithm with optimal variation- bounded regret. In Proceedings of the 24th Annual Conference on Learning Theory, pages 817- 820, 2011b.  Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Opti-  mization, 2(3-4):157-325, 2016.  Wouter M Koolen and Tim Van Erven. Second-order quantile methods for experts and combinatorial  games. In Conference on Learning Theory, pages 1155-1175, 2015.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Tor Lattimore. Optimally confident ucb: Improved regret for finite-armed bandits. arXiv preprint  arXiv:1507.07880, 2015.  Haipeng Luo and Robert E Schapire. Achieving all with no parameters: Adanormalhedge.  In  Conference on Learning Theory, pages 1286-1304, 2015.  Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Small-loss bounds for online learning with  partial information. arXiv preprint arXiv:1711.03639, 2017.  H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. Journal of  Machine Learning Research, 18(90):1-50, 2017.  Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Conference on Learning  Theory, pages 1360-1375, 2015.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in  Neural Information Processing Systems, pages 577-585, 2016.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-  ence on Learning Theory, pages 993-1019, 2013a.  Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se- quences. In Advances in Neural Information Processing Systems, pages 3066-3074, 2013b.  Yevgeny Seldin and G\u00b4abor Lugosi. An improved parametrization and analysis of the exp3++ algo-  rithm for stochastic and adversarial bandits. In Conference on Learning Theory, 2017. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In International Conference on Machine Learning, pages 1287-1295, 2014.  Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient  algorithm. In International Conference on Machine Learning, pages 1593-1601, 2014.  Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of In Advances in Neural Information Processing Systems, pages  regularized learning in games. 2989-2997, 2015.  Tim van Erven and Wouter M Koolen. Metagrad: Multiple learning rates in online learning. In  Advances in Neural Information Processing Systems, pages 3666-3674, 2016."}