{"1": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  Rong Ge, Sham M. Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure. CoRR, 2019. URL https: //arxiv.org/abs/1904.12838.  Nicholas J. A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. CoRR, 2018. URL http://arxiv.org/abs/ 1812.05217.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par- allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint arXiv:1610.03774, 2016.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-  ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.  Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information  theoretically optimal. CoRR, 2019. URL https://arxiv.org/abs/1904.12443.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Texts in Statistics.  Springer, 1998. ISBN 9780387985022.  Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-  gence results and optimal averaging schemes. CoRR, abs/1212.1824, 2012.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.  4   DO GOOD ALGORITHMS NECESSARILY QUERY BAD POINTS?  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  Rong Ge, Sham M. Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure. CoRR, 2019. URL https: //arxiv.org/abs/1904.12838.  Nicholas J. A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. CoRR, 2018. URL http://arxiv.org/abs/ 1812.05217.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par- allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint arXiv:1610.03774, 2016.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-  ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.  Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information  theoretically optimal. CoRR, 2019. URL https://arxiv.org/abs/1904.12443.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Texts in Statistics.  Springer, 1998. ISBN 9780387985022.  Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-  gence results and optimal averaging schemes. CoRR, abs/1212.1824, 2012.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000."}