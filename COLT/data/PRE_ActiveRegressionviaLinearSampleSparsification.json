{"1": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.  Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta- tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 253-262, 2017.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE transactions on information theory, 59(10):6880-6892, 2013.  Y. Bresler and A. Macovski. Exact maximum likelihood parameter estimation of superimposed exponential signals in noise. IEEE Transactions on Acoustics, Speech, and Signal Processing, 34 (5):1081-1089, Oct 1986. ISSN 0096-3518. doi: 10.1109/TASSP.1986.1164949.  Kamalika Chaudhuri, Sham M Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of active learning for maximum likelihood estimation. In Advances in Neural Information Processing Systems, pages 1090-1098, 2015.  Xue Chen, Daniel M. Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a  frequency gap. In FOCS 2016, 2016.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, 23:493-507, 1952.  11   ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  coefficients and l = \u02dcO(k2). This upper bounds |f (\u22121)|2 in terms of |f (\u22121 + \u2206)|2 + \u00b7 \u00b7 \u00b7 + |f (\u22121 + l \u00b7 \u2206)|2 and then |f (\u22121)|2/(cid:107)f (cid:107)2 D by integrating \u2206 from 0 to 2/l.  The improvement of Theorem 4 contains two steps. In the first step, we show that f (x) can be expressed as a constant-coefficient linear combination of the elements of an O(k)-length arithmetic sequence on both sides of x, namely, {f (x\u22122k \u00b7\u2206), . . . , f (x+2k \u00b7\u2206)}\\f (x). This is much shorter than the (cid:101)O(k2) elements required by Chen et al. (2016) for the one-sided version, and provides an \u02dcO(k2) factor improvement. Next we find k such linear combinations that are almost orthogonal to each other to remove the extra k factor. These two let us show that  |f (x)|2 (cid:107)f (cid:107)2 D  sup f \u2208F  = O  (cid:19)  (cid:18) k log k 1 \u2212 |x|  for any x \u2208 (\u22121, 1). This leads to \u03ba = O(k log2 k), which appears in Theorem 32 of Section G.  The authors would like to thank Adam Klivans and David Zuckerman for many helpful comments about this work.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.  Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta- tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 253-262, 2017.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE transactions on information theory, 59(10):6880-6892, 2013.  Y. Bresler and A. Macovski. Exact maximum likelihood parameter estimation of superimposed exponential signals in noise. IEEE Transactions on Acoustics, Speech, and Signal Processing, 34 (5):1081-1089, Oct 1986. ISSN 0096-3518. doi: 10.1109/TASSP.1986.1164949.  Kamalika Chaudhuri, Sham M Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of active learning for maximum likelihood estimation. In Advances in Neural Information Processing Systems, pages 1090-1098, 2015.  Xue Chen, Daniel M. Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a  frequency gap. In FOCS 2016, 2016.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, 23:493-507, 1952. ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  Albert Cohen, Mark A Davenport, and Dany Leviatan. On the stability and accuracy of least squares  approximations. Foundations of computational mathematics, 13(5):819-834, 2013.  Michal Derezinski and Manfred K Warmuth. Unbiased estimates for linear regression via volume sampling. In Advances in Neural Information Processing Systems, pages 3087-3096, 2017.  Michal Derezinski, Manfred K Warmuth, and Daniel Hsu. Tail bounds for volume sampled linear  regression. arXiv preprint arXiv:1802.06749, 2018.  Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decomposi-  tions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.  Robert Fano. Transmission of information; a statistical theory of communications. Cambridge,  Massachusetts, M.I.T. Press, 1961.  Ralph Hartley. Transmission of information. Bell System Technical Journal, 1928.  Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The  Journal of Machine Learning Research, 17(1):543-582, 2016.  Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time. In Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), FOCS \u201915, pages 250-269. IEEE Computer Society, 2015.  Malik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutative bernstein  bound. arXiv preprint arXiv:1008.0587, 2010.  Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R(cid:13) in  Machine Learning, 3(2):123-224, 2011.  Ankur Moitra. The threshold for super-resolution via extremal functions. In STOC, 2015.  H.L. Montgomery and R.C. Vaughan. Hilbert\u2019s inequality. Journal of the London Mathematical  Society, s2-8(1):73-82, 1974.  Holger Rauhut and Rachel Ward. Sparse legendre expansions via (cid:96)1-minimization. Journal of  approximation theory, 164(5):517-533, 2012.  Sivan Sabato and Remi Munos. Active regression by stratification. In Advances in Neural Informa-  tion Processing Systems, pages 469-477, 2014.  Claude Shannon. Communication in the presence of noise. Proc. Institute of Radio Engineers, 37  (1):10-21, 1949.  Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2772-2789. SIAM, 2019.  Robert E. Tarjan. Lecture 10: More chernoff bounds, sampling, and the chernoff + union bound.  Princeton Class Notes, Probability and Computing, pages 1-9, 2009. ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12:389-434, 2012.  Rachel Ward. Importance sampling in signal processing applications. In Excursions in Harmonic  Analysis, Volume 4, pages 205-228. Springer, 2015.  David P Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,  2014."}