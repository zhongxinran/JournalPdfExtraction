{"1": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret Bounds for the Adaptive Control of Linear  Quadratic Systems. In Conference on Learning Theory, 2011.  Yasin Abbasi-Yadkori, Nevena Lazi\u00b4c, and Csaba Szepesv\u00e1ri. Model-Free Linear Quadratic Control  via Reduction to Expert Prediction. In AISTATS, 2019.  Marc Abeille and Alessandro Lazaric. Thompson Sampling for Linear-Quadratic Control Problems.  In AISTATS, 2017.  Marc Abeille and Alessandro Lazaric. Improved Regret Bounds for Thompson Sampling in Linear  Quadratic Control Problems. In International Conference on Machine Learning, 2018.  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-  case regret bounds. In Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3): 325-349, 2013.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax Regret Bounds for Rein-  forcement Learning. In International Conference on Machine Learning, 2017.  Patrick Billingsley. Probability and Measure. 1995.  Justin Boyan. Least-Squares Temporal Difference Learning. In International Conference on Ma-  chine Learning, 1999.  Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares Algorithms for Temporal Difference  Learning. Machine Learning, 22(1-3):33-57, 1996.  Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization. In Conference on Robot Learning, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017.  13   THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Acknowledgments  We thank John Duchi and Daniel Russo, who both independently suggested studying LQR using asymptotic analysis; this paper is a direct result of their feedback. We also thank Horia Mania for many helpful discussions regarding policy gradient methods. Finally, we thank Nicolas Flammarion and Nilesh Tripuraneni for pointers regarding asymptotic analysis of stochastic gradient methods. ST is supported by a Google PhD fellowship. BR is generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, and an Amazon AWS AI Re- search Award.  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret Bounds for the Adaptive Control of Linear  Quadratic Systems. In Conference on Learning Theory, 2011.  Yasin Abbasi-Yadkori, Nevena Lazi\u00b4c, and Csaba Szepesv\u00e1ri. Model-Free Linear Quadratic Control  via Reduction to Expert Prediction. In AISTATS, 2019.  Marc Abeille and Alessandro Lazaric. Thompson Sampling for Linear-Quadratic Control Problems.  In AISTATS, 2017.  Marc Abeille and Alessandro Lazaric. Improved Regret Bounds for Thompson Sampling in Linear  Quadratic Control Problems. In International Conference on Machine Learning, 2018.  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-  case regret bounds. In Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3): 325-349, 2013.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax Regret Bounds for Rein-  forcement Learning. In International Conference on Machine Learning, 2017.  Patrick Billingsley. Probability and Measure. 1995.  Justin Boyan. Least-Squares Temporal Difference Learning. In International Conference on Ma-  chine Learning, 1999.  Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares Algorithms for Temporal Difference  Learning. Machine Learning, 22(1-3):33-57, 1996.  Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization. In Conference on Robot Learning, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator. In Neural Information Processing Systems, 2018.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Optimism-Based  Adaptive Regulation of Linear-Quadratic Systems. arXiv:1711.07230, 2017.  Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator. In International Conference on Machine Learning, 2018.  Claude-Nicolas Fiechter. PAC Adaptive Control of Linear Systems. In Conference on Learning  Theory, 1997.  Morteza Ibrahimi, Adel Javanmard, and Benjamin Van Roy. Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems. In Neural Information Processing Systems, 2012.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement  Learning. Journal of Machine Learning Research, 11:1563-1600, 2010.  Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query Complexity of Derivative-Free  Optimization. In Neural Information Processing Systems, 2012.  Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan.  Is Q-learning Provably  Efficient? In Neural Information Processing Systems, 2018.  Galin L. Jones. On the Markov chain central limit theorem. Probability Surveys, 1:299-320, 2004.  Harold Kushner and George Yin. Stochastic Approximation and Recursive Algorithms and Appli-  cations. 2003.  Jan R. Magnus. The expectation of products of quadratic forms in normal variables: the practice.  Statistica Neerlandica, 33(3):131-136, 1979.  Dhruv Malik, Kush Bhatia, Koulik Khamaru, Peter L. Bartlett,  , and Martin J. Wainwright. Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. In AISTATS, 2019.  Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive  approach to reinforcement learning. In Neural Information Processing Systems, 2018.  Henry B. Mann and Abraham Wald. On the Statistical Treatment of Linear Stochastic Difference  Equations. Econometrica, 11(3-4):173-220, 1943.  Abdelkader Mokkadem. Mixing properties of ARMA processes. Stochastic Processes and their  Applications, 29(2):309-315, 1988.  B. Molinari. The Stabilizing Solution of the Discrete Algebraic Riccati Equation. IEEE Transac-  tions on Automatic Control, 20(3):396-399, 1975. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural Network Dy- namics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. In Inter- national Conference on Robotics and Automation, 2018.  Yurii Nesterov and Vladimir Spokoiny. Random Gradient-Free Minimization of Convex Functions.  Foundations of Computational Mathematics, 17(2):527-566, 2017.  Yi Ouyang, Mukul Gagrani, and Rahul Jain. Control of unknown linear systems with Thompson In 55th Annual Allerton Conference on Communication, Control, and Computing,  Alain Pajor. Metric Entropy of the Grassmann Manifold. Convex Geometric Analysis, 34:181-188,  sampling. 2017.  1998.  Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural  Networks, 21(4):682-697, 2008.  Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal Difference Models: Model-Free Deep RL for Model-Based Control. In International Conference on Learning Rep- resentations, 2018.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization. In International Conference on Machine Learning, 2012.  Benjamin Recht. A Tour of Reinforcement Learning: The View from Continuous Control.  arXiv:1806.09460, 2018.  Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as a  Scalable Alternative to Reinforcement Learning. arXiv:1703.03864, 2017.  Kathrin Sch\u00e4cke. On the Kronecker Product. Master\u2019s thesis, University of Waterloo, 2004.  John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High- In International  Dimensional Continuous Control Using Generalized Advantage Estimation. Conference on Learning Representations, 2016.  Aaron Sidford, Mengdi Wang, Xian Wu, Lin F. Yang, and Yinyu Ye. Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model. In Neural Information Processes Systems, 2018.  Max Simchowitz, Horia Mania, Stephen Tu, Michael I. Jordan, and Benjamin Recht. Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification. In Conference on Learning Theory, 2018.  Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC Model-Free Reinforcement Learning. In International Conference on Machine Learning, 2006.  Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-Based Reinforcement Learning in Contextual Decision Processes. In Conference on Learning Theory, 2019. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Panos Toulis and Edoardo M. Airoldi. Asymptotic and finite-sample properties of estimators based  on stochastic gradients. The Annals of Statistics, 45(4):1694-1727, 2017.  Stephen Tu and Benjamin Recht. Least-Squares Temporal Difference Learning for the Linear  Quadratic Regulator. In International Conference on Machine Learning, 2018.  Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-  ment learning. Machine Learning, 8(3-4):229-246, 1992.  Fuzhen Zhang. The Schur Complement and its Applications, volume 4 of Numerical Methods and  Algorithms. 2005."}