{"1": "Hirotsugu Akaike. Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory (Tsahkadsor, 1971), pages 267-281. Akad\u00b4emiai Kiad\u00b4o, Budapest, 1973.  Pierre Alquier. Iterative feature selection in least square regression estimation. Ann. Inst.  Henri Poincar\u00b4e Probab. Stat., 44(1):47-88, 2008.  198   Comminges Dalalyan  Let us stress now that, all over this work, we have deliberately avoided any discussion on the computational aspects of the variable selection in nonparametric regression. The goal in this paper was to investigate the possibility of consistent recovery without paying attention to the complexity of the selection procedure. This lead to some conditions that could be considered a benchmark for assessing the properties of sparsity pattern estimators. As for the estimator proposed in Section 3, it is worth noting that its computational complexity is not always prohibitively large. A recommended strategy is to compute the coe\ufb03cients (cid:98)\u03b8k in a stepwise manner; at each step K = 1, 2, . . . , d\u2217 only the coe\ufb03cients (cid:98)\u03b8k with (cid:107)k(cid:107)0 = K need to be computed and compared with the threshold. If some (cid:98)\u03b8k exceeds the threshold, then all the covariates X j corresponding to nonzero coordinates of k are considered as relevant. We can stop this computation as soon as the number of covariates classified as relevant attains d\u2217. While the worst-case complexity of this procedure is exponential, there are many functions f for which the complexity of the procedure will be polynomial in d. For example, this is the case for additive models in which f(x) = f1(xi1) + . . . + fd\u2217(xid\u2217 ) for some univariate functions f1, . . . , fd\u2217.  Note also that in the present study we focused exclusively on the consistency of variable selection without paying any attention to the consistency of regression function estimation. A thorough analysis of the latter problem being left to a future work, let us simply remark that in the case of fixed d\u2217, under the conditions of Theorem 2, it is straightforward to construct a consistent estimator of the regression function. In fact, it su\ufb03ces to use a projection estimator with a properly chosen truncation parameter on the set of relevant variables. The situation is much more delicate in the case when the sparsity d\u2217 grows to infinity along with the sample size n. Presumably, condition (10) is no longer su\ufb03cient for consistently estimating the regression function. The rationale behind this conjecture is that the minimax rate of convergence for estimating f in our context, if we assume in addition that the set of relevant variables is known, is equal n\u22122/(2+d\u2217) = exp(\u22122 log n/(2 + d\u2217)). If the left hand side of (10) is equal to a constant and log log d = o(log n), then the aforementioned minimax rate does not tend to zero, making thus the estimator inconsistent. This heuristical argument shows that there is still some work to do for getting tight conditions ensuring the consistent estimation of the regression function in the high dimensional set-up.  The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR) under the grant PARCIMONIE.  Acknowledgments  References  Hirotsugu Akaike. Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory (Tsahkadsor, 1971), pages 267-281. Akad\u00b4emiai Kiad\u00b4o, Budapest, 1973.  Pierre Alquier. Iterative feature selection in least square regression estimation. Ann. Inst.  Henri Poincar\u00b4e Probab. Stat., 44(1):47-88, 2008. Consistent variable selection in nonparametric regression  Francis Bach. High-dimensional non-linear variable selection through hierarchical kernel  learning. Technical report, arXiv:0909.0844, 2009.  Karine Bertin and Guillaume Lecu\u00b4e. Selection of variables and dimension reduction in  high-dimensional non-parametric regression. Electron. J. Stat., 2:1224-1241, 2008.  Peter J. Bickel, Ya\u2019acov Ritov, and Alexandre B. Tsybakov. Hierarchical selection of vari- ables in sparse high-dimensional regression. Borrowing Strength: Theory Powering Ap- plications - A Festschrift for Lawrence D. Brown. IMS Collections, 6:56-69, 2010.  Florentina Bunea and Adrian Barbu. Dimension reduction and variable selection in case control studies via regularized likelihood optimization. Electron. J. Stat., 3:1257-1287, 2009.  Jean Dieudonn\u00b4e. Calcul infinit\u00b4esimal. Hermann, Paris, 1968.  David Donoho and Jiashun Jin. Feature selection by higher criticism thresholding achieves the optimal phase diagram. Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci., 367(1906):4449-4470, 2009. With electronic supplementary materials available online.  Jianqing Fan, Richard Samworth, and Yichao Wu. Ultrahigh dimensional feature selection:  beyond the linear model. J. Mach. Learn. Res., 10:2013-2038, 2009.  Robert M. Fano. Transmission of information: A statistical theory of communications. The  M.I.T. Press, Cambridge, Mass., 1961.  Rodolphe Jenatton, Jean-Yves Audibert, and Francis Bach. Structured variable selection  with sparsity-inducing norms. Technical report, arXiv:0904.3523, 2009.  John La\ufb00erty and Larry Wasserman. Rodeo: sparse, greedy nonparametric regression. Ann.  Statist., 36(1):28-63, 2008.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by  model selection. Ann. Statist., 28(5):1302-1338, 2000.  Karim Lounici, Massimiliano Pontil, Alexandre B. Tsybakov, and Sara van de Geer. inference under group sparsity. Technical report,  Oracle inequalities and optimal arXiv:1007.1771, 2010.  Colin L. Mallows. Some comments on Cp. Technometrics, 15:661-675, Nov. 1973.  James Mazo and Andrew Odlyzko. Lattice points in high-dimensional spheres. Monatsh.  Math., 110(1):47-61, 1990.  Methodol., 72(4):417-473, 2010.  Nicolai Meinshausen and Peter Bhlmann. Stability selection. J. R. Stat. Soc. Ser. B Stat.  Guillaume Obozinski, Martin J. Wainwright, and Michael I. Jordan. High-dimensional  union support recovery in multivariate. The Annals of Statistics, to appear, 2011. Comminges Dalalyan  Pradeep Ravikumar, Martin J. Wainwright, and John D. La\ufb00erty. High-dimensional Ising model selection using (cid:96)1-regularized logistic regression. Ann. Statist., 38(3):1287-1319, 2010.  Gideon Schwarz. Estimating the dimension of a model. Ann. Statist., 6(2):461-464, 1978.  James G. Scott and James O. Berger. Bayes and empirical-Bayes multiplicity adjustment  in the variable-selection problem. Ann. Statist., 38(5):2587-2619, 2010.  Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc.  Ser. B, 58(1):267-288, 1996.  Jo-Anne Ting, Aaron D\u2019Souza, Sethu Vijayakumar, and Stefan Schaal. E\ufb03cient learning and feature selection in high-dimensional regression. Neural Comput., 22(4):831-886, 2010.  Alexandre B. Tsybakov.  Introduction to nonparametric estimation. Springer Series in  Statistics. Springer, New York, 2009.  Larry Wasserman and Kathryn Roeder. High-dimensional variable selection. Ann. Statist.,  37(5A):2178-2201, 2009.  Statist., 38(2):894-942, 2010.  Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann.  Tong Zhang. On the consistency of feature selection using greedy least squares regression.  J. Mach. Learn. Res., 10:555-568, 2009.  Peng Zhao and Bin Yu. On model selection consistency of Lasso. J. Mach. Learn. Res., 7:  2541-2563, 2006.  Peng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for  grouped and hierarchical variable selection. Ann. Statist., 37(6A):3468-3497, 2009."}