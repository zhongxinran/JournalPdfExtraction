{"1": "Peter L. Bartlett and Mikhail Traskin. AdaBoost is consistent. Journal of Machine Learning  Research, 8:2347-2368, 2007.  Peter J. Bickel, Ya\u2019acov Ritov, and Alon Zakai. Some theory for generalized boosting  algorithms. Journal of Machine Learning Research, 7:705-732, 2006.  553   The Rate of Convergence of AdaBoost  the desired properties.  Applying Lemma 14 to the sequence \u03b7\u2217  (t) yields some convex combination \u03b7\u2020 having margin at least \u03b3 > 0 (for some \u03b3) on A and zero margin on its complement, proving Item 1 of the Decomposition Lemma. The next lemma proves Item 2.  Lemma 15 There is a (finite) combination \u03b7\u2217 which achieves the same margins on F as the optimal solution.  Proof The existence of \u03b7\u2020 with properties as in Lemma 14 implies that the optimal loss is the same whether considering all the examples, or just examples in F . Therefore it su\ufb03ces to show the existence of finite \u03b7\u2217 that achieves loss K on F , that is, (cid:96)\u03b7\u2217(F ) = K.  Recall MF denotes the matrix M restricted to the rows corresponding to examples in F . Let ker MF = {x : MF x = 0} be the null-space of MF . Let \u03b7(t) be the projection of \u03b7\u2217 (t) onto the orthogonal subspace of ker MF . Then the losses (cid:96)\u03b7(t)(F ) = (cid:96)\u03b7\u2217 (t)(F ) converge to the opti- mal loss K. If MF is identically zero, then each \u03b7(t) = 0, and then \u03b7\u2217 = 0 has loss K on F . F MF . Then (cid:107)M \u03b7(t)(cid:107) \u2265 \u03bb(cid:107)\u03b7(t)(cid:107). Otherwise, let \u03bb2 be the smallest positive eigenvalue of M T By the definition of finite margin set, inf t\u2192\u221e (cid:96)\u03b7(t)(F ) = inf t\u2192\u221e (cid:96)\u03b7\u2217 (t)(F ) > 0. Therefore, the margins (cid:107)M \u03b7(t)(cid:107) are bounded, and hence the \u03b7(t) are also bounded in norm. Therefore they have a (finite) limit point \u03b7\u2217 which must have loss K over F .  As a corollary, we prove Item 3.  Lemma 16 There is a constant \u00b5max < \u221e, such that for any combination \u03b7 that achieves bounded loss on the finite-margin set, (cid:96)\u03b7(F ) \u2264 m, the margin (M \u03b7)i for any example i in F lies in the bounded interval [\u2212 ln m, \u00b5max] .  Proof The loss (cid:96)\u03b7(F ) at most m implies no margin may be less than \u2212 ln m. If Item 3 of the Decomposition Lemma were false, then for some example x \u2208 F there exists a sequence of combinations of weak classifiers, whose tth element achieves more than margin t on x but has loss at most m on F . Applying Lemma 13 we can find a subsequence \u03bb(t) whose tail achieves zero-loss on some non-empty subset S of F containing x, and bounded margins in F \\ S. Applying Lemma 14 to \u03bb(t) we get some convex combination \u03bb\u2020 which has positive margins on S and zero margin on F \\ S. Let \u03b7\u2217 be as in Lemma 15, a finite combination achieving the optimal loss on F . Then \u03b7\u2217 + \u221e \u00b7 \u03bb\u2020 achieves the same loss on every example in F \\ S as the optimal solution \u03b7\u2217, but zero loss for examples in S. This solution is strictly better than \u03b7\u2217 on F , a contradiction to the optimality of \u03b7\u2217.  References  Peter L. Bartlett and Mikhail Traskin. AdaBoost is consistent. Journal of Machine Learning  Research, 8:2347-2368, 2007.  Peter J. Bickel, Ya\u2019acov Ritov, and Alon Zakai. Some theory for generalized boosting  algorithms. Journal of Machine Learning Research, 7:705-732, 2006. Mukherjee Rudin Schapire  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  Leo Breiman. Prediction games and arcing classifiers. Neural Computation, 11(7):1493-  Press, 2004.  1517, 1999.  Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learn- ing algorithms. In Proceedings of the 23rd International Conference on Machine Learning, 2006.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and  Bregman distances. Machine Learning, 48(1/2/3), 2002.  Marcus Frean and Tom Downs. A simple cost function for boosting. Technical report, Department of Computer Science and Electrical Engineering, University of Queensland, 1998.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, August 1997.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: A  statistical view of boosting. Annals of Statistics, 28(2):337-374, April 2000.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals  of Statistics, 29(5), October 2001.  David G. Luenberger and Yinyu Ye. Linear and nonlinear programming. Springer, third  edition, 2008.  Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex di\ufb00erentiable minimization. Journal of Optimization Theory and Applications, 72(1): 7-35, January 1992.  Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Functional gradient In Advances in Large Margin Classifiers. MIT  techniques for combining hypotheses. Press, 1999.  Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as  gradient descent. In Advances in Neural Information Processing Systems 12, 2000.  T. Onoda, G. R\u00a8atsch, and K.-R. M\u00a8uller. An asymptotic analysis of AdaBoost in the binary classification case. In Proceedings of the 8th International Conference on Artificial Neural Networks, pages 195-200, 1998.  G. R\u00a8atsch, T. Onoda, and K.-R. M\u00a8uller. Soft margins for AdaBoost. Machine Learning, 42  (3):287-320, 2001.  Gunnar R\u00a8atsch and Manfred K. Warmuth. E\ufb03cient margin maximizing with boosting.  Journal of Machine Learning Research, 6:2131-2152, 2005. The Rate of Convergence of AdaBoost  Gunnar R\u00a8atsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of lever-  aging. In Advances in Neural Information Processing Systems 14, 2002.  Cynthia Rudin, Robert E. Schapire, and Ingrid Daubechies. Analysis of boosting algorithms  using the smooth margin function. Annals of Statistics, 35(6):2723-2768, 2007.  Robert E. Schapire. The convergence rate of AdaBoost. In The 23rd Conference on Learning  Theory, 2010. open problem.  Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated  predictions. Machine Learning, 37(3):297-336, December 1999.  Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separability: New relaxations and e\ufb03cient boosting algorithms. In 21st Annual Confer- ence on Learning Theory, 2008.  Xindong Wu, Vipin Kumar, J. Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda, Geo\ufb00rey J. McLachlan, Angus Ng, Bing Liu, Philip S. Yu, Zhi-Hua Zhou, Michael Stein- bach, David J. Hand, and Dan Steinberg. Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1):1-37, 2008.  Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency.  Annals of Statistics, 33(4):1538-1579, 2005. Mukherjee Rudin Schapire  Lemma 17 To get within \u03b5 < 0.1 of the optimum loss on the dataset in Table 2, AdaBoost takes at least 0.22/\u03b5 steps.  Proof Note that optimum loss is 2/3, and we are bounding the number of rounds necessary to get within (2/3) + \u03b5 loss for \u03b5 < 0.1. We begin by showing that for rounds t \u2265 3, the edge achieved is 1/t. First observe that the edges in rounds 1 and 2 are 1/3 and 1/2. Our claim will follow from the following stronger claim. Let wt c denote the normalized-losses (adding up to 1) or weights on examples a, b, c at the beginning of round t, and \u03b4t the edge in round t. Then for t \u2265 2,  a, wt  b, wt  1. Either 1/2 = wt  a or 1/2 = wt b.  2. \u03b4t+1 = \u03b4t/(1 + \u03b4t).  Proof by induction. Base case may be checked. Suppose the inductive assumption holds for t. Assume without loss of generality that 1/2 = wt c. Then in round t, ha c/(1 + 2wt gets picked, the edge \u03b4t = 2wt c). Hence, in round t + 1 hb gets picked and we get edge \u03b4t+1 = 2wt c) = \u03b4t/(1 + \u03b4t). Proof follows by induction. Note the recurrence on \u03b4t yields \u03b4t = 1/t for t \u2265 3.  b > wt c/2)/(1/2 + wt  b = 1/2, wt+1  c, and wt+1  c/(1 + 2wt  c = (wt  c) = wt  a > wt  Next we find the loss after each iteration. The loss after T rounds is  and can be computed as follows. Notice that in the following list  (cid:112)  1 \u2212 (1/3)2  1 \u2212 1/t2  T (cid:89)  (cid:112)  i=2  1 \u2212 (1/2)2 = (1 \u00b7 3)/(2 \u00b7 2), 1 \u2212 (1/3)2 = (2 \u00b7 4)/(3 \u00b7 3), 1 \u2212 (1/4)2 = (3 \u00b7 5)/(4 \u00b7 4), . . . = . . . ,  the middle denominator (3\u00b73) gets canceled by the right term of the first numerator and the left term of the third denominator. Continuing this way, the product till term 1 \u2212 (1/T )2 is (1/2) {(T + 1)/T }. Therefore the loss after round T is (2/3)(cid:112)1 + 1/T \u2265 (2/3) + (2/9)T , for T \u2265 3. Since the error after 3 rounds is still at least (2/3) + 0.1 the Lemma holds for \u03b5 < 0.1.  Lemma 18 Suppose u0, u1, . . . , are non-negative numbers satisfying  for some non-negative constants c0, c1. Then, for any t,  ut \u2212 ut+1 \u2265 c0u1+c1  ,  t  1 uc1 t  \u2212  1 uc1 0  \u2265 c1c0t. The Rate of Convergence of AdaBoost  Proof By induction on t. The base case is an identity. Assume Lemma holds for t. Then,  1 uc1 t+1  \u2212  \u2265  1 uc1 0  (cid:18) 1 uc1 t+1  \u2212  1 uc1 t  (cid:19)  +  (cid:18) 1 uc1 t  \u2212  1 uc1 0  (cid:19)  \u2265  1 uc1 t+1  \u2212  1 uc1 t  + c0t, (by induction).  Thus it su\ufb03ces to show  1 uc1 t+1  \u2212  1 uc1 t  \u2265 c1c0 \u21d0\u21d2  (cid:19)c1  (cid:18) ut ut+1  \u2265 1 + c1c0uc1  t \u21d0  Since 1 + c1c0uc1  t \u2264 (1 + c0uc1  t )c1, and (1 + c0uc1  t ) (1 \u2212 c0uc1  1 (1 \u2212 c0uc1  t )c1 \u2265 1 + c1c0uc1 t . t ) < 1, the inequality holds."}