{"1": "P. Alquier and K. Lounici. Pac-bayesian bounds for sparse regression estimation with exponen-  tial weights. Electron. J. Statist., 5:127-145, 2010.  647  00.20.40.60.81\u22122024Signal Length: n=102400.20.40.60.81\u22122024Noisy function : \u03c3=1 and PSNR=800.20.40.60.81\u22122024Denoised by EWA (PSNR=19.64)00.20.40.60.81\u22122024Denoised by BJS (PSNR=16.8) URE (PSNR=19.59), ST (PSNR=15.16)  BJSUREST OPTIMAL AGGREGATION OF AFFINE ESTIMATORS  Figure 4: Piece-Regular. The first row is the true signal (left) and a noisy version corrupted by Gaussian noise with standard deviation \u03c3 = 1 (right). The second row gives denoised version obtained by EWA (left) and by BJS, ST and URE (right). The PSNR is com- puted by the formula PSNR = 10 log10  (cid:161)max( f )2/MSE(cid:162).  6. Summary and future work  In this paper, we have addressed the problem of aggregating a set of affine estimators in the context of regression with fixed design and heteroscedastic noise. Under some assumptions on the constituent estimators, we have proven that the EWA with a suitably chosen temperature parameter satisfies PAC-Bayesian type inequality, from which different types of oracle inequal- ities have been deduced. All these inequalities are with leading constant one and with rate- optimal residual term. As a by-product of our results, we have shown that EWA applied to the family of Pinsker\u2019s estimators produces an estimator, which is adaptive in the exact minimax sense. Next in our agenda is carrying out an experimental evaluation of the proposed aggregate using the approximation schemes described by Dalalyan and Tsybakov (2009), Rigollet and Tsy- bakov (2011) and Alquier and Lounici (2010). It will also be interesting to extend the results of this work to the case of the unknown noise variance in the same vein as in Giraud (2008).  The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR) under the grant PARCIMONIE.  Acknowledgments  References  P. Alquier and K. Lounici. Pac-bayesian bounds for sparse regression estimation with exponen-  tial weights. Electron. J. Statist., 5:127-145, 2010.00.20.40.60.81\u22122024Signal Length: n=102400.20.40.60.81\u22122024Noisy function : \u03c3=1 and PSNR=800.20.40.60.81\u22122024Denoised by EWA (PSNR=19.64)00.20.40.60.81\u22122024Denoised by BJS (PSNR=16.8) URE (PSNR=19.59), ST (PSNR=15.16)  BJSUREST SALMON DALALYAN  n  EWA  URE  BJS  ST  EWA  URE  BJS  ST512204851220485122048  Blocks  HeaviSine  0.051 (0.42) -0.052 (0.35) -0.050 (0.36) -0.007 (0.42)  -0.060 (0.19) -0.079 (0.19) -0.059 (0.23) -0.051 (0.25)  0.038 (0.37) 0.010 (0.36) -0.002 (0.30) 0.007 (0.34)  0.245 (0.39) 0.302 (0.50) 0.299 (0.46) 0.362 (0.57)  0.247 (0.42) 0.215 (0.39) 0.240 (0.36) 0.278 (0.48)  0.294 (0.47) 0.293 (0.51) 0.300 (0.45) 0.312 (0.50)  9.617 (1.78) 13.807 (2.16) 19.984 (2.68) 28.948 (3.31)  1.155 (0.57) 2.064 (0.86) 3.120 (1.20) 4.858 (1.42)  6.933 (1.54) 9.712 (1.76) 13.656 (2.25) 19.113 (2.68)  4.846 (1.29) 9.256 (1.70) 17.569 (2.17) 30.447 (2.96)  3.966 (1.12) 5.889 (1.36) 8.685 (1.64) 12.667 (2.03)  5.644 (1.20) 9.977 (1.67) 16.790 (2.06) 27.315 (2.61)  Ramp  Doppler  13.233 0.212 (2.11) (0.31) 17.080 0.205 (2.29) (0.39) 21.862 0.270 (2.92) (0.41) 28.733 0.234 (0.42) (3.19) Piece-Regular 8.883 0.248 (1.76) (0.40) 12.147 0.237 (2.28) (0.37) 15.207 0.291 (2.18) (0.46) 21.543 0.283 (2.47) (0.54) Piece-Polynomial 12.201 (1.81) 17.765 (2.72) 23.321 (2.96) 31.550 (3.05)  0.203 (0.37) 0.312 (0.49) 0.321 (0.48) 0.314 (0.49)  0.062 (0.35) -0.100 (0.30) -0.107 (0.35) -0.150 (0.34)  -0.069 (0.32) -0.105 (0.30) -0.092 (0.34) -0.059 (0.34)  0.017 (0.37) -0.078 (0.35) -0.026 (0.38) -0.007 (0.41)  6.036 (1.23) 12.620 (1.75) 23.006 (2.35) 38.671 (3.02)  4.879 (1.20) 9.793 (1.64) 16.798 (2.13) 27.387 (2.77)  3.988 (1.19) 9.031 (1.62) 17.565 (2.28) 29.461 (2.95)  Table 1: Comparison of several adaptive methods on the six (non-smooth) signals of inter- est. For each signal length n and each method, we give the average value of n \u00d7 (MSE \u2212 MSEOracle) and the corresponding standard deviation below, for 1000 replica- tions of the experiment. Negative values indicate that in some cases the EWA proce- dure has a smaller risk than that of the best linear estimator used for the aggregation, which is possible since the EWA itself is not a linear estimator.  Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural  Comput., 9:1545-1588, October 1997.  S. Arlot and F. Bach. Data-driven calibration of linear estimators with minimal penalties. In  J-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann. Statist., 37  F. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:  Y. Baraud, Ch. Giraud, and S. Huet. Estimator selection in the gaussian setting. submitted, 2010.  NIPS, pages 46-54, 2009.  (4):1591-1646, 2009.  1179-1225, 2008. OPTIMAL AGGREGATION OF AFFINE ESTIMATORS  n  EWA  URE  BJS  ST  EWA  URE  BJS  ST512204851220485122048  Blocks  HeaviSine  0.387 (0.43) 0.170 (0.20) 0.162 (0.18) 0.120 (0.17)  0.217 (0.16) 0.206 (0.18) 0.179 (0.18) 0.162 (0.15)  0.162 (0.16) 0.150 (0.18) 0.146 (0.18) 0.141 (0.20)  0.216 (0.40) 0.209 (0.41) 0.226 (0.41) 0.220 (0.37)  0.207 (0.42) 0.221 (0.43) 0.200 (0.50) 0.189 (0.37)  0.200 (0.38) 0.215 (0.38) 0.211 (0.39) 0.221 (0.43)  0.216 (0.24) 0.650 (0.25) 1.282 (0.44) 1.574 (0.55)  1.399 (0.54) 0.024 (0.26) 0.113 (0.27) 0.421 (0.27)  0.339 (0.24) 0.425 (0.23) 0.935 (0.33) 1.316 (0.42)  2.278 (0.98) 3.193 (1.07) 4.507 (1.28) 6.107 (1.55)  2.496 (0.96) 3.045 (1.10) 3.905 (1.27) 5.019 (1.53)  2.770 (1.00) 3.658 (1.20) 4.815 (1.35) 6.432 (1.54)  Doppler  1.608 0.237 (0.73) (0.40) 1.200 0.250 (0.48) (0.44) 1.842 0.229 (0.86) (0.45) 1.864 0.229 (0.40) (1.07) Piece-Regular 2.120 (1.09) 2.045 (1.17) 1.251 (0.70) 1.650 (1.12)  0.279 (0.49) 0.248 (0.45) 0.228 (0.41) 0.223 (0.42)  0.257 (0.48) 0.243 (0.46) 0.236 (0.47) 0.210 (0.39)  1.486 (0.68) 1.865 (0.84) 1.547 (1.02) 2.246 (1.15)  0.214 (0.23) 0.165 (0.20) 0.147 (0.19) 0.138 (0.20)  0.269 (0.27) 0.216 (0.20) 0.183 (0.20) 0.145 (0.19)  0.215 (0.25) 0.170 (0.20) 0.179 (0.20) 0.165 (0.20)  2.777 (1.04) 3.682 (1.24) 5.043 (1.43) 6.584 (1.58)  2.053 (0.95) 2.883 (1.13) 3.780 (1.37) 4.992 (1.42)  2.649 (1.01) 3.683 (1.20) 5.017 (1.38) 6.628 (1.70)  Ramp  Piece-Polynomial  Table 2: Comparison of several adaptive methods on the six smoothed signals of inter- est. length n and each method, we give the average value of n(MSE \u2212 MSEOracle) and the corresponding standard deviation below, for 1000 repli- cations of the experiment.  For each signal  A. R. Barron, L. Birg\u00e9, and P. Massart. Risk bounds for model selection via penalization. Probab.  Theory Related Fields, 113(3):301-413, 1999.  L. Breiman. Bagging predictors. Mach. Learn., 24(2):123-140, 1996.  A. Buades, B. Coll, and J-M. Morel. A review of image denoising algorithms, with a new one.  Multiscale Model. Simul., 4(2):490-530, 2005.  F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for Gaussian regression. Ann.  Statist., 35(4):1674-1697, 2007.  T. T. Cai. Adaptive wavelet estimation: a block thresholding and oracle inequality approach.  Ann. Statist., 27(3):898-924, 1999. ISSN 0090-5364.  O. Catoni. Statistical learning theory and stochastic optimization, volume 1851 of Lecture Notes  in Mathematics. Springer-Verlag, Berlin, 2004. SALMON DALALYAN  L. Cavalier, G. K. Golubev, D. Picard, and A. B. Tsybakov. Oracle inequalities for inverse prob-  lems. Ann. Statist., 30(3):843-874, 2002.  A. Cohen. All admissible linear estimates of the mean vector. The Annals of Mathematical  Statistics, 37(2):458-463, 1966. ISSN 00034851.  A. S. Dalalyan and J. Salmon. Sharp oracle inequalities for aggregation of affine estimators.  Technical Report arXiv:1104.3969v2 [math.ST], April 2011.  A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp oracle inequal-  ities and sparsity. In COLT, pages 97-111, 2007.  A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp pac-bayesian  bounds and sparsity. Mach. Learn., 72(1-2):39-61, 2008.  A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin  Monte-Carlo. In COLT, 2009.  81(3):425-455, 1994.  D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika,  D. L. Donoho and I. M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage. J.  Amer. Statist. Assoc., 90(432):1200-1224, 1995.  S. Y. Efromovich. On nonparametric regression for IID observations in a general setting. Ann.  Statist., 24(3):1125-1144, 1996.  S. Y. Efromovich and M. S. Pinsker. A self-training algorithm for nonparametric filtering. Av-  tomat. i Telemekh., 1(11):58-65, 1984.  S. Y. Efromovich and M. S. Pinsker. Sharp-optimal and adaptive estimation for heteroscedastic  nonparametric regression. Statist. Sinica, 6(4):925-942, 1996.  Y. Freund. Boosting a weak learning algorithm by majority. In Proceedings of the third annual  workshop on Computational learning theory, COLT, pages 202-216, 1990.  E. I. George. Minimax multiple shrinkage estimation. Ann. Statist., 14(1):188-205, 1986.  Ch. Giraud. Mixing least-squares estimators when the variance is unknown. Bernoulli, 14(4):  1089-1107, 2008.  G. K. Golubev. Nonparametric estimation of smooth densities of a distribution in L_2. Problemy  Peredachi Informatsii, 28(1):52-62, 1992.  Y. Golubev. On universal oracle inequalities related to high-dimensional linear models. Ann.  Statist., 38(5):2751-2780, 2010.  Statist., 28(3):681-712, 2000.  A. B. Juditsky and A. S. Nemirovski. Functional aggregation for nonparametric regression. Ann.  A. B. Juditsky and A. S. Nemirovski. Nonparametric denoising of signals with unknown local  structure. I. Oracle inequalities. Appl. Comput. Harmon. Anal., 27(2):157-179, 2009. OPTIMAL AGGREGATION OF AFFINE ESTIMATORS  G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the kernel matrix with semidefinite programming. J. Mach. Learn. Res., 5:27-72 (electronic), 2003/04.  G. Lecu\u00e9. Optimal rates of aggregation in classification under low noise assumption. Bernoulli,  G. Leung. Information Theory and Mixing Least Squares Regression. PhD thesis, Yale University,  G. Leung and A. R. Barron.  Information theory and mixing least-squares regressions.  IEEE  Trans. Inf. Theory, 52(8):3396-3410, 2006.  K. Lounici. Generalized mirror averaging and D-convex aggregation. Math. Methods Statist., 16  A. S. Nemirovski. Topics in non-parametric statistics, volume 1738 of Lecture Notes in Math.  13(4):1000-1022, 2007.  2004.  (3):246-259, 2007.  Springer, Berlin, 2000.  Inf., 16(2):52-68, 1980.  M. S. Pinsker. Optimal filtration of square-integrable signals in Gaussian noise. Probl. Peredachi  Ph. Rigollet and A. B. Tsybakov. Linear and convex aggregation of density estimators. Math.  Methods Statist., 16(3):260-280, 2007.  Ph. Rigollet and A. B. Tsybakov. Exponential screening and optimal rates of sparse estimation.  Ann. Statist., 39(2):731-471, 2011.  J. Salmon and E. Le Pennec. NL-Means and aggregation procedures. In ICIP, pages 2977-2980,  2009.  J. Shawe-Taylor and N. Cristianini. An introduction to support vector machines : and other  kernel-based learning methods. Cambridge University Press, 2000.  C. M. Stein. Estimation of the mean of a multivariate normal distribution. Ann. Statist., 9(6):  1135-1151, 1981.  A. B. Tsybakov. Optimal rates of aggregation. In COLT, pages 303-313, 2003.  A. B. Tsybakov. Introduction to nonparametric estimation. Springer, New York, 2009.  Y. Yang. Combining different procedures for adaptive regression. J. Multivariate Anal., 74(1):  135-161, 2000.  783-809, 2003.  2004.  Y. Yang. Regression with multiple candidate models: selecting or mixing? Statist. Sinica, 13(3):  Y. Yang. Aggregating regression procedures to improve performance. Bernoulli, 10(1):25-47,  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. R.  Stat. Soc. Ser. B Stat. Methodol., 68(1):49-67, 2006. SALMON DALALYAN  T. Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE Trans.  Inform. Theory, 52(4):1307-1321, 2006. ISSN 1557-9654."}