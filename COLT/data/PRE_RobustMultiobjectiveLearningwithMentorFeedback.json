{"1": "ICML, 2004.  Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119\u2013139, August 1997.  Paul E. Green and V. Srinivasan. Conjoint analysis in marketing: New developments with implica-  tions for research and practice. Journal of Marketing, 54(4):3\u201319, October 1990.  Nathan D. Ratliff, J. Andrew Bagnell, and Martin Zinkevich. Maximum margin planning. In ICML,  pages 729\u2013736, 2006.  Umar Syed and Robert Schapire. A game-theoretic approach to apprenticeship learning. In Ad-  vances in Neural Information Processing Systems 20, NIPS 2007, pages 1449\u20131456, 2008.  Kenneth Train. Discrete Choice Methods with Simulation. Cambridge University Press, New York,  NY, 2nd edition, 2009.  Bin Yu. Assouad, Fano and Le Cam. Festschrift in Honor of L. Le Cam on his 70th Birthday, 1993.  Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy  inverse reinforcement learning. In AAAI, pages 1433\u20131438, 2008.  Appendix A. Application to apprenticeship learning  As noted earlier, our model is a generalization of the apprenticeship learning framework of Abbeel and Ng (2004) as subsequently extended by Syed and Schapire (2008). Here, we brie\ufb02y describe how their framework and algorithm are essentially a special case of ours.  In this setting, an agent called an apprentice is allowed to observe the actions of a so-called expert in a stochastic environment modeled as a Markov Decision Process (MDP). We assume that both the apprentice and the expert have access to the states, actions, and transition function of the MDP, as well as an assumed initial distribution over start states. However, the MDP has no explicit reward function. Rather, each state \u03c3 is associated with a vector of reward features, denoted \u03c6(\u03c3). We suppose that the \u201ctrue\u201d reward function is some convex combination q\u2217 of the reward features so that the (never directly observed) reward at state \u03c3 is q\u2217 \u00b7 \u03c6(\u03c3). Although q\u2217 is unknown, we assume that each of the reward features has known direction, that is, that larger values are always at least as desirable as lower values.  We suppose that the apprentice observes the expert following trajectories through the MDP. On each one of these, an initial state \u03c30 is chosen from the initial state distribution D0, then actions are selected by the expert causing the MDP to progress along a trajectory of states \u03c3 = (\u03c30, \u03c31, . . .) in the usual fashion. For simplicity, we assume these trajectories are in\ufb01nite, although, as with most of our assumptions, this certainly can be relaxed. Further, we assume that the expert\u2019s behavior induces a distribution D over such state sequences, and that these trajectories are sampled independently from D.  14   AGARWAL BADANIDIYURU DUD\u00b4IK SCHAPIRE SLIVKINS  References  ICML, 2004.  Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, August 1997.  Paul E. Green and V. Srinivasan. Conjoint analysis in marketing: New developments with implica-  tions for research and practice. Journal of Marketing, 54(4):3-19, October 1990.  Nathan D. Ratliff, J. Andrew Bagnell, and Martin Zinkevich. Maximum margin planning. In ICML,  pages 729-736, 2006.  Umar Syed and Robert Schapire. A game-theoretic approach to apprenticeship learning. In Ad-  vances in Neural Information Processing Systems 20, NIPS 2007, pages 1449-1456, 2008.  Kenneth Train. Discrete Choice Methods with Simulation. Cambridge University Press, New York,  NY, 2nd edition, 2009.  Bin Yu. Assouad, Fano and Le Cam. Festschrift in Honor of L. Le Cam on his 70th Birthday, 1993.  Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy  inverse reinforcement learning. In AAAI, pages 1433-1438, 2008."}