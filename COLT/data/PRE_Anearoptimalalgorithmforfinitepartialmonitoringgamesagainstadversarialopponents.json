{"1": "bandits. In COLT, 2009.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic  multiarmed bandit problem. SIAM J. Comput., 32(1):48-77, 2002.  G\u00b4abor Bart\u00b4ok, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Toward a classification of finite partial-  monitoring games. In ALT, pages 224-238, 2010.  G\u00b4abor Bart\u00b4ok, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Minimax regret of finite partial-monitoring games in stochastic environments. Journal of Machine Learning Research - Proceedings Track (COLT), 19:133-154, 2011.  13   Near-optimal algorithm for partial monitoring  to the construction of point-local games, this situation is avoided and thus we have the opportunity to play a point-local game of choice without randomizing.  One may notice that our bound contains the value (cid:15)G in the denominator. This value depends on the structure of the game and can get very small in some cases. One might also think that with more actions, (cid:15)G decreases and thus N gets in the bound implicitly. However, there exist game instances with many actions and large (cid:15)G. For an example consider cell decomposition in which every corner of the probability simplex has a local game with many actions, but these local games are far away from each other.  We would also like to note that a value related to (cid:15)G naturally must appear in the regret upper bound. To understand why, consider a game where two non-neighboring actions do not satisfy the local observability condition, but their cells are very close to each other (and thus (cid:15)G is small). Imagine these cells moving towards each other. When the gap becomes zero, the actions become neighbors and the game becomes non-locally observable; thus the regret will scale with T 2/3. Hence, as (cid:15)G shrinks, the regret must go up.  The bound also shows a dependence on the number of outcomes (M ). We conjecture that this dependence can be lifted with a more sophisticated way of tracking qt. Our method of devoting some rounds to exploration seems suboptimal. Improving the bound in this aspect remains future work.  As a final remark we note a fact that we found interesting.  If we use the algorithm LocalExp3 on a bandit game (we can because it is a point-local locally observable game), the algorithm does not reduce to Exp3. This is due to the fact that the expectation of the updates are o\ufb00set by the value (cid:80)N k=1 pk(t)L(k, Jt), which is in turn the expected loss of the algorithm at time step t. This \u201ccentralized\u201d update might even improve upon the performance of Exp3 because it makes the absolute values of the updates smaller.  Acknowledgments  The author thanks the anonymous reviewers for their insightful comments and constructive suggestions. This research was supported in part by DARPA grant MSEE FA8650-11-1- 7156.  References  bandits. In COLT, 2009.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic  multiarmed bandit problem. SIAM J. Comput., 32(1):48-77, 2002.  G\u00b4abor Bart\u00b4ok, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Toward a classification of finite partial-  monitoring games. In ALT, pages 224-238, 2010.  G\u00b4abor Bart\u00b4ok, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Minimax regret of finite partial-monitoring games in stochastic environments. Journal of Machine Learning Research - Proceedings Track (COLT), 19:133-154, 2011. Bart\u00b4ok  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In  Advanced Lectures on Machine Learning, pages 208-240, 2003.  Nicol`o Cesa-Bianchi, G\u00b4abor Lugosi, and Gilles Stoltz. Regret minimization under partial  monitoring. Math. Oper. Res., 31(3):562-580, 2006.  Dean P. Foster and Alexander Rakhlin. No internal regret via neighborhood watch. Journal  of Machine Learning Research - Proceedings Track (AISTATS), 22:382-390, 2012.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput.,  108(2):212-261, 1994.  In NIPS, pages 684-692, 2011.  Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations.  Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary  feedback and loss. In COLT/EuroCOLT, pages 208-223, 2001.  V. G. Vovk. Aggregating strategies. In COLT, pages 371-386, 1990."}