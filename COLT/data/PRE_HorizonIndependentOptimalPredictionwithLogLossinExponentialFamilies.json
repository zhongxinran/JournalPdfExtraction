{"1": "York: John Wiley, 1978.  O. Barndor\ufb00-Nielsen. Information and Exponential Families in Statistical Theory. New  12   Bartlett Gr\u00a8unwald Harremo\u00a8es Hedayati Kot(cid:32)lowski  Now we get to the second case where the variance function is given by Equation 15. If c1 = 0 we get an exponential family where the variance is constant, i.e. the family is the Gaussian translation family. Then the term k corresponds to a translation of the exponential family and we may assume that k = 0. If c1 (cid:54)= 0 we can scale up or down and obtain the equation  V (\u00b5) = 2\u00b53/2.  (16)  There exists an exponential family with this variance function, namely the Tweedie family of order 3/2 with V (\u00b5) = 2\u00b53/2. Since exponential families are uniquely determined by their variance function Morris (1982), the Tweedie family of order 3/2 is the only family satisfying (16).  4. Discussion  The present paper has focused on 1-dimensional exponential families with non-empty inte- rior parameter spaces. Any model that admits a 1-dimensional su\ufb03cient statistic can be embedded in a one dimensional exponential family. One can prove that SNML exchange- ability implies that the parameter space must have non-empty interior, thus strengthening our results further, but the limited space did not allow us to go into this problem here.  We do not have any general results for the multidimensional case, but we can make a few observations: products of models that are SNML exchangeable are also exchangeable. All multidimensional Gaussian location models can be obtained in this way by a suitable choice of coordinate system. The only other SNML exchangeable models we know of in higher dimensions are Gaussian models where the mean is unknown and the scaling of the covariance matrix is unknown. This can be seen from the fact that a sum of squared Gaussian variables has a Gamma distribution. The Tweedie family of order 3/2 does not seem to play any interesting role in higher dimensions, because it cannot be combined with the other distributions.  One of the consequences of this paper is that for 1-dimensional exponential families, NML (if it is defined without conditioning) will always be horizon dependent. We conjecture that this conclusion will hold for arbitrary models. Only conditional versions of NML allow the kind of consistency that we call SNML exchangeability, and even after conditioning, SNML exchangeability is restricted to a few but very important models.  Acknowledgements  Wojciech Kot(cid:32)lowski has been supported by the Foundation of Polish Science under the Homing Plus programme. We gratefully acknowledge the support of the NSF through grant CCF-1115788 and of the Australian Research Council through Australian Laureate Fellowship FL110100281.  References  York: John Wiley, 1978.  O. Barndor\ufb00-Nielsen. Information and Exponential Families in Statistical Theory. New Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families  A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743-2760, 1998. Special Commemorative Issue: Information Theory: 1948-1998.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning and Games. Cambridge University  I. Csisz\u00b4ar and F. Mat\u00b4us. Information projections revisited. IEEE Transactions on Infor-  mation Theory, vol. 49, no. 6, pp. 1474-1490, 2003.  P. Gr\u00a8unwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA,  Press, 2006.  2007.  P. Harremo\u00a8es. Extendable MDL. Accepted for presentation at International Symposium for  Information Theory (ISIT 2013), ArXiv: 1301.6465, Jan. 2013.  F. Hedayati and P. Bartlett. Exchangeability Characterizes Optimality of Sequential Nor- malized Maximum Likelihood and Bayesian Prediction with Je\ufb00reys Prior. In Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS \u201912), 2012a.  F. Hedayati and P. Bartlett. The optimality of Je\ufb00reys prior for online density estimation and the asymptotic normality of maximum likelihood estimators. In Proceedings of the Twenty Fifth Conference on Learning Theory (COLT\u2019 12), 2012b.  A. Hurwitz. \u00a8Uber die angen\u00a8aherte Darstellung der Irrationalzahlen durch rationale Br\u00a8uche (On the approximation of irrational numbers by rational numbers, in German). Mathe- matische Annalen 39 (2): 279-284, 1891.  B. J\u00f8rgensen. The Theory of Dispersion Models. Chapman & Hall, 1997.  S. Kakade, M. Seeger, and D. Foster. Worst-case bounds for Gaussian process models. In Proceedings of the 2005 Neural Information Processing Systems Conference (NIPS 2005), 2006.  W. Kot(cid:32)lowski and P. Gr\u00a8unwald. Maximum likelihood vs. sequential normalized maximum likelihood in on-line density estimation. In Proceedings of the Twenty-Fourth Conference on Learning Theory (COLT\u2019 11), 761-779, Budapest, 2011.  F. Liang and A. R. Barron. Exact minimax strategies for predictive density estimation, data compression, and model selection. IEEE Transactions on Information Theory, 50: 2708-2726, 2004.  C. Morris. Natural exponential families with quadratic variance functions. Ann. Statist.,  10:65-80, 1982.  J. Rissanen. Modeling by the shortest data description. Automatica, 14:465-471, 1978.  J. Rissanen. Fisher information and stochastic complexity. IEEE Transactions on Infor-  mation Theory, 42(1):40-47, 1996. Bartlett Gr\u00a8unwald Harremo\u00a8es Hedayati Kot(cid:32)lowski  J. Rissanen and T. Roos. Conditional NML universal models. In Information Theory and  Applications Workshop (ITA-07), 337-341, 2007.  T. Roos and J. Rissanen. On sequentially normalized maximum likelihood models.  In Workshop on Information Theoretic Methods in Science and Engineering (WITMSE-08), 2008.  Y. Shtarkov. Universal sequential coding of single messages. Problems of Information  Transmission, 23(3):175-186, 1987.  E. Takimoto and M. Warmuth. The last-step minimax algorithm. In Conference on Algo-  rithmic Learning Theory (ALT \u201900), 2000a.  E. Takimoto and M. Warmuth. The minimax strategy for Gaussian density estimation. In  Conference on Learning Theory (COLT \u201900), 100-106, 2000b."}