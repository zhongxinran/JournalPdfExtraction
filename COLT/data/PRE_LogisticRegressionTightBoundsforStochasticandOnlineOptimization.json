{"1": "Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. arXiv preprint arXiv:1303.6149, 2013.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781. 2013.  Yaroslav Bulatov. Log loss or hinge loss? http://yaroslavvb.blogspot.co.il/2007/  06/log-loss-or-hinge-loss.html, June 2007.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman  distances. Machine Learning, 48(1-3):253-285, 2002.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. The Annals of Statistics, 28(2):337-407, 2000.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic and  online optimization. arXiv preprint arXiv:1405.3843, 2014.  John Langford. Optimal proxy loss for classification. http://hunch.net/?p=547, April  2009.  Brendan H McMahan and Matthew J Streeter. Open problem: Better bounds for online logistic  regression. Journal of Machine Learning Research-Proceedings Track, 23:44-1, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  ICML, pages 928-936, 2003.  13   TIGHT BOUNDS FOR LOGISTIC REGRESSION  a distribution over instances such that the induced expected loss function is approximately linear around its optimum.  An interesting feature of our results is that our regret/convergence bounds apply to a finite range of T , and are different than the known asymptotic bounds. Arguably, the range of T for which our results apply is the important one in practice (sub-exponential in the size of the hypothesis class). Are there other natural settings in which regret bounds for bounded number of iterations differ from the asymptotic bound?  The research leading to these results has received funding from the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 336078 - ERC-SUBLRN.  Acknowledgments  References  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. arXiv preprint arXiv:1303.6149, 2013.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781. 2013.  Yaroslav Bulatov. Log loss or hinge loss? http://yaroslavvb.blogspot.co.il/2007/  06/log-loss-or-hinge-loss.html, June 2007.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman  distances. Machine Learning, 48(1-3):253-285, 2002.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. The Annals of Statistics, 28(2):337-407, 2000.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic and  online optimization. arXiv preprint arXiv:1405.3843, 2014.  John Langford. Optimal proxy loss for classification. http://hunch.net/?p=547, April  2009.  Brendan H McMahan and Matthew J Streeter. Open problem: Better bounds for online logistic  regression. Journal of Machine Learning Research-Proceedings Track, 23:44-1, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  ICML, pages 928-936, 2003."}