{"1": "[1] R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applica-  tions to Markov chains. Electron. J. Probab., 1000-1034, 2008.  [2] T. M. Adams, A. B. Nobel. Uniform approximation and bracketing properties of VC classes.  [3] M. Anthony, P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  Bernoulli, 18:1310-1319, 2012.  University Press, 1999.  ing, 66(2-3): 151-163, 2007.  [4] P. Auer, R. Ortner. A new PAC bound for intersection-closed concept classes. Machine Learn-  [5] M.F. Balcan, P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Conference on Learning Theory, 2013.  [6] P. L. Bartlett, O. Bousquet, S. Mendelson. Local Rademacher Complexities. The Annals of  Statistics, 33(4):1497-1537, 08, 2005.  [7] P. L. Bartlett, S. Mendelson. Empirical minimization. Probability Theory and Related Fields,  135(3):311-334, 2006.  2002.  [8] O. Bousquet, A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  [9] S. Boucheron, G. Lugosi, P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Cambridge, 2013.  [10] N. H. Bshouty, Y. Li, P. M. Long. Using the doubling dimension to analyze the generalization  of learning algorithms. Journal of Computer and System Sciences, 2009.  [11] L. Devroye, L. Gy\u00a8orfi, G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, New York, 1996.  [12] A. Ehrenfeucht, D. Haussler, M. Kearns, L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  [13] S. Floyd and M. Warmuth. Sample Compression, learnability, and the Vapnik Chervonenkis  Dimension, Machine Learning, 21, 269-304 (1995).  [14] E. Gassiat, R. van Handel. The local geometry of finite mixtures, Trans. Amer. Math. Soc.  366, 1047-1072, 2014.  16   ZHIVOTOVSKIY  We would like to thank Steve Hanneke for several helpful discussions and anonymous reviewers for their useful suggestions. The author was supported solely by the Russian Science Foundation grant (project 14-50-00150).  Acknowledgments  References  [1] R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applica-  tions to Markov chains. Electron. J. Probab., 1000-1034, 2008.  [2] T. M. Adams, A. B. Nobel. Uniform approximation and bracketing properties of VC classes.  [3] M. Anthony, P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  Bernoulli, 18:1310-1319, 2012.  University Press, 1999.  ing, 66(2-3): 151-163, 2007.  [4] P. Auer, R. Ortner. A new PAC bound for intersection-closed concept classes. Machine Learn-  [5] M.F. Balcan, P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Conference on Learning Theory, 2013.  [6] P. L. Bartlett, O. Bousquet, S. Mendelson. Local Rademacher Complexities. The Annals of  Statistics, 33(4):1497-1537, 08, 2005.  [7] P. L. Bartlett, S. Mendelson. Empirical minimization. Probability Theory and Related Fields,  135(3):311-334, 2006.  2002.  [8] O. Bousquet, A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  [9] S. Boucheron, G. Lugosi, P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Cambridge, 2013.  [10] N. H. Bshouty, Y. Li, P. M. Long. Using the doubling dimension to analyze the generalization  of learning algorithms. Journal of Computer and System Sciences, 2009.  [11] L. Devroye, L. Gy\u00a8orfi, G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, New York, 1996.  [12] A. Ehrenfeucht, D. Haussler, M. Kearns, L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  [13] S. Floyd and M. Warmuth. Sample Compression, learnability, and the Vapnik Chervonenkis  Dimension, Machine Learning, 21, 269-304 (1995).  [14] E. Gassiat, R. van Handel. The local geometry of finite mixtures, Trans. Amer. Math. Soc.  366, 1047-1072, 2014. OPTIMAL LEARNING VIA LOCAL ENTROPIES AND SAMPLE COMPRESSION  [15] E. Gin\u00b4e, V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type em-  pirical processes. The Annals of Probability, 34(3):1143-1216, 2006.  [16] S. Hanneke, L. Yang. Minimax analysis of active learning. Journal of Machine Learning Re-  search, 16 (12): 3487-3602, 2015.  [17] S. Hanneke. Refined error bounds for several learning algorithms. Journal of Machine Learning  [18] S. Hanneke. The Optimal Sample Complexity of PAC Learning. Journal of Machine Learning  Research 17, 1-55, 2016  Research, 17 (38): 1-15, 2016.  38-53, 1973.  Bernoulli 19 2153-2166, 2013.  [19] D. Haussler, N. Littlestone, M. Warmuth. Predicting {0, 1}-functions on randomly drawn  points. Information and Computation, 115:248-292, 1994.  [20] L. M. Le Cam. Convergence of estimates under dimensionality restrictions. Ann. Statist. 1,  [21] G. Lecu\u00b4e. Empirical risk minimization is optimal for the convex aggregation problem.  [22] G. Lecu\u00b4e. Interplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis. Habilitation thesis, Universit\u00b4e Paris-Est, 2011.  [23] G. Lecu\u00b4e, S. Mendelson On the optimality of the aggregate with exponential weights for low  temperature. Bernoulli, 2013.  [24] G. Lecu\u00b4e, S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. http:  //arxiv.org/abs/1305.4825, 2013.  [25] G. Lecu\u00b4e, C. Mitchell. Oracle inequalities for cross-validation type procedures. Electronic  Journal of Statistics, 6, 1803-1837, 2012.  [26] T. Liang, A. Rakhlin, K. Sridharan. Learning with square loss: Localization through offset Rademacher complexity. Proceedings of The 28th Conference on Learning Theory, 2015.  [27] N. Littlestone. From On-line to batch learning. In COLT, 1989.  [28] P. M. Long. On the sample complexity of PAC learning halfspaces against the uniform distri-  bution. IEEE Transactions on Neural Networks, 6(6):1556-1559, 1995.  [29] P. Massart, E. N\u00b4ed\u00b4elec. Risk bounds for statistical learning. Annals of Statistics, 2006.  [30] S. Mendelson. Obtaining fast error rates in nonconvex situations. Journal of Complexity. Vol-  ume 24, Issue 3, 380-397, 2008.  [31] S. Mendelson. \u2018Local\u2019 vs. \u2018global\u2019 parameters - breaking the Gaussian complexity barrier. https://arxiv.org/abs/1504.02191, to appear in Annals of Statisitcs, 2017  [32] S.Mendelson. Learning without concentration. Journal of the ACM, Volume 62, Issue 3, 2015. ZHIVOTOVSKIY  [33] A. Rakhlin, K. Sridharan, A. B. Tsybakov. Empirical entropy, minimax regret and minimax  risk. Bernoulli, 2017.  [34] H. Simon. An almost optimal PAC-algorithm. Proceedings of The 28th Conference on Learn-  ing Theory, pp. 1552-1563, 2015.  [35] A. B. Tsybakov. Optimal rates of aggregation. In Computational Learning Theory and Kernel  Machines 303-313. Lecture Notes in Artificial Intelligence, 2003.  [36] A. B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statis-  tics. Vol. 32, No. 1, 135-166, 2004  [37] A. W. van der Vaart, J. A. Wellner. Weak Convergence and Empirical Processes. Springer,  1996.  [38] V. Vapnik, A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their probabilities. Proc. USSR Acad. Sci. 181(4), 781-783, 1968.  [39] V. Vapnik, A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974.  [40] M. K. Warmuth. The optimal PAC algorithm. In Proceedings of the 17th Conference on Learn-  ing Theory, 2004.  1, 252-273, 2003.  [41] M. Wegkamp. Model selection in nonparametric regression. Annals of Statistics, Vol. 31, No.  [42] Y. Yang, A. Barron. Information-theoretic determination of minimax rates of convergence. An-  nals of Statistics, 27, 1564-1599, 1999.  [43] N. Zhivotovskiy, S. Hanneke. Localization of VC classes: Beyond Local Rademacher com-  plexities. https://arxiv.org/abs/1606.00922, 2016."}