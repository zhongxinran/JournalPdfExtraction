{"1": "Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best Arm Identification in Multi-Armed Bandits. In Proceedings of the 23rd Conference on Learning Theory, COLT \u201910, pages 41-53, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, May 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multi-  armed Bandit Problem. SIAM Journal of Computing, 32:48-77, Jan 2003.  R. E. Bechhofer. A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances. The Annals of Mathematical Statistics, 25:16-39, 1954.  Robert E. Bechhofer, Thomas J. Santner, and David M. Goldsman. Design and Analysis of Ex- periments for Statistical Selection, Screening, and Multiple Comparisons. Wiley-Interscience, 1995.  Donald A. Berry and Bert Fristedt. Bandit Problems: Sequential Allocation of Experiments (Mono-  graphs on Statistics and Applied Probability). Chapman & Hall, Oct 1985.  Justin Boesel, Barry L. Nelson, and Seong-Hee Kim. Using Ranking and Selection to \u201cClean Up\u201d  after Simulation Optimization. Operations Research, 51(5):814-825, 2003.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob- In Proceedings of the 20th international conference on Algorithmic learning theory,  lems. ALT\u201909, pages 23-37, 2009.  Stephen E. Chick and Noah Gans. Economic Analysis of Simulation Selection Problems. Manage-  ment Science, 55(3):421-437, 2009.  Stephen E. Chick and Koichiro Inoue. New Two-Stage and Sequential Procedures for Selecting the  Best Simulated System. Operations Research, 49(5):732-743, 2001.  Vincent Cicirello and Stephen Smith. The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection. In 20th National Conference on Artificial Intelligence, AAAI \u201905, pages 1355-1361, 2005.  Ioana Dumitriu, Prasad Tetali, and Peter Winkler. On Playing Golf with Two Balls. SIAM Journal  of Discrete Mathematics, 16:604-615, Apr 2003.  S. N. Ethier and Davar Khoshnevisan. Bounds on Gambler\u2019s Ruin Probabilities in Terms of Mo-  ments. Methodology and Computing in Applied Probability, 4(1):55-68, Mar 2002.  13   FINDING A MOST BIASED COIN WITH FEWEST FLIPS  Acknowledgments  We thank Santosh Vempala for valuable comments.  References  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best Arm Identification in Multi-Armed Bandits. In Proceedings of the 23rd Conference on Learning Theory, COLT \u201910, pages 41-53, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, May 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multi-  armed Bandit Problem. SIAM Journal of Computing, 32:48-77, Jan 2003.  R. E. Bechhofer. A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances. The Annals of Mathematical Statistics, 25:16-39, 1954.  Robert E. Bechhofer, Thomas J. Santner, and David M. Goldsman. Design and Analysis of Ex- periments for Statistical Selection, Screening, and Multiple Comparisons. Wiley-Interscience, 1995.  Donald A. Berry and Bert Fristedt. Bandit Problems: Sequential Allocation of Experiments (Mono-  graphs on Statistics and Applied Probability). Chapman & Hall, Oct 1985.  Justin Boesel, Barry L. Nelson, and Seong-Hee Kim. Using Ranking and Selection to \u201cClean Up\u201d  after Simulation Optimization. Operations Research, 51(5):814-825, 2003.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob- In Proceedings of the 20th international conference on Algorithmic learning theory,  lems. ALT\u201909, pages 23-37, 2009.  Stephen E. Chick and Noah Gans. Economic Analysis of Simulation Selection Problems. Manage-  ment Science, 55(3):421-437, 2009.  Stephen E. Chick and Koichiro Inoue. New Two-Stage and Sequential Procedures for Selecting the  Best Simulated System. Operations Research, 49(5):732-743, 2001.  Vincent Cicirello and Stephen Smith. The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection. In 20th National Conference on Artificial Intelligence, AAAI \u201905, pages 1355-1361, 2005.  Ioana Dumitriu, Prasad Tetali, and Peter Winkler. On Playing Golf with Two Balls. SIAM Journal  of Discrete Mathematics, 16:604-615, Apr 2003.  S. N. Ethier and Davar Khoshnevisan. Bounds on Gambler\u2019s Ruin Probabilities in Terms of Mo-  ments. Methodology and Computing in Applied Probability, 4(1):55-68, Mar 2002. CHANDRASEKARAN KARP  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC Bounds for Multi-armed Bandit and Markov Decision Processes. In Proceedings of the 15th Annual Conference on Computational Learning Theory, COLT \u201902, pages 255-270, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. J. Mach. Learn. Res., 7: 1079-1105, December 2006.  Peter I. Frazier, Warren B. Powell, and Savas Dayanik. A Knowledge-Gradient Policy for Sequential Information Collection. SIAM Journal on Control and Optimization, 47(5):2410-2439, 2008.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and S\u00b4ebastien Bubeck. Multi- Bandit Best Arm Identification. In Advances in Neural Information Processing Systems, NIPS \u201911, pages 2222-2230, 2011.  John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed Bandit Allocation Indices. Wiley,  2nd edition, 2011.  Shanti S. Gupta and Klaus J. Miescke. Bayesian look ahead one-stage sampling allocations for selection of the best population. Journal of Statistical Planning and Inference, 54(2):229-244, 1996.  S. H. Kim and B. L. Nelson. Selecting the best system. Handbooks in Operations Research and  Management Science: Simulation, pages 501-534, 2006.  T.L. Lai and H. Robbins. Asymptotically Efficient Adaptive Allocation Rules. Advances in Applied  Mathematics, 6(1):4-22, 1985.  Shie Mannor and John N. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed  Bandit Problem. Journal of Machine Learning Research, 5:623-648, Dec 2004.  Oded Maron and Andrew Moore. Hoeffding Races: Accelerating Model Selection Search for Clas- sification and Function Approximation. In Advances in Neural Information Processing Systems, volume 6, pages 59-66, April 1994.  E. Paulson. A Sequential Procedure for Selecting the Population with the Largest Mean from k  Normal Populations. The Annals of Mathematical Statistics, 35:174-180, 1964.  J. Pichitlamken and B. L. Nelson. Selection-of-the-best procedures for optimization via simulation.  In Proceedings of the 2001 Winter Simulation Conference, pages 401-407, 2001."}