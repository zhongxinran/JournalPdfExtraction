{"1": "Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, and Benjamin Taskar. Learning the parameters of determinantal point process kernels. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1224-1232, 2014.  Jennifer Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for In Proceedings of the 27th International Conference learning determinantal point processes. on Neural Information Processing Systems, NIPS\u201914, pages 3149-3157, Cambridge, MA, USA, 2014. MIT Press.  A. Kulesza. Learning with determinantal point processes. PhD thesis, University of Pennsylvania,  2012.  Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publish-  ers Inc., Hanover, MA, USA, 2012. ISBN 1601986289, 9781601986283.  Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2389-2397, 2015.  3   MLE OF DPPS  4. Maximum likelihood estimation  Let Z1, . . . , Zn be n independent copies of Z \u223c DPP(L\u2217), where the kernel L\u2217 is unknown. We assume that L\u2217 is positive definite. Let \u02c6L be the maximum likelihood estimator (MLE) of L\u2217 (unique up to a \ufb02ip of the sign of its columns and its rows).  We measure the performance of the MLE using the loss (cid:96) defined by  (cid:96)( \u02c6L, L\u2217) = min D\u2208D  (cid:107) \u02c6L \u2212 DL\u2217D(cid:107)F  where (cid:107) \u00b7 (cid:107)F denotes the Frobenius norm.  Our statistical results establish asymptotic properties of the MLE. We use OIP for big-O notation in probability. For L \u2208 IRN \u00d7N and J, J (cid:48) \u2286 [N ], we denote by LJ,J (cid:48) the submatrix of L obtained by keeping the rows indexed in J and the columns indexed in J (cid:48).  Theorem 3  \u2022 (cid:96)( \u02c6L, L\u2217) \u2212\u2212\u2212\u2192 n\u2192\u221e  0 ,  in probability.  \u2022 If L\u2217 is irreducible, then, \u02dcL is asymptotically normal. In particular, (cid:96)( \u02c6L, L\u2217) = OIP(n\u22121/2).  \u2022 If L\u2217 is block diagonal, then, for any pair of distinct blocks J, J (cid:48) of L\u2217,  (cid:107) \u02c6LJ,J (cid:48) \u2212 DL\u2217  J,J (cid:48)D(cid:107)F = OIP(n\u22121/4)  (cid:107) \u02c6LJ \u2212 DJ L\u2217  J DJ (cid:107)F = OIP(n\u22121/2).  min D\u2208D  min D\u2208D  (4.1)  (4.2)  and  References  Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, and Benjamin Taskar. Learning the parameters of determinantal point process kernels. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1224-1232, 2014.  Jennifer Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for In Proceedings of the 27th International Conference learning determinantal point processes. on Neural Information Processing Systems, NIPS\u201914, pages 3149-3157, Cambridge, MA, USA, 2014. MIT Press.  A. Kulesza. Learning with determinantal point processes. PhD thesis, University of Pennsylvania,  2012.  Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publish-  ers Inc., Hanover, MA, USA, 2012. ISBN 1601986289, 9781601986283.  Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2389-2397, 2015."}