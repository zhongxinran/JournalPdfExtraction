{"1": "2009.  Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent meth-  ods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.  Robert J Adler and Jonathan E Taylor. Random fields and geometry. Springer Science & Business Media,  10   LEE SIMCHOWITZ  covering of Rn, instead of a naive union bound. It is possible that for the structured problems that arise in machine learning, whether in matrix factorization or convolutional neural networks, that saddle points are isolated after taking a quotient with respect to the associated symmetry group of the problem. Techniques from dynamical systems on manifolds may be applicable to understand the behavior of optimization algorithms on problems with a high degree of symmetry.  It is also important to understand how stringent the strict saddle assumption is. Will a perturba- tion of a function always satisfy the strict saddle property? Adler and Taylor (2009) provide very general sufficient conditions for a random function to be Morse, meaning the eigenvalues at critical points are non-zero, which implies the strict saddle condition. These conditions rely on checking the density of \u22072f (x) has full support conditioned on the event that \u2207f (x) = 0. This can be explicitly verified for functions f that arise from learning problems.  However, we note that there are very difficult unconstrained optimization problems where the strict saddle condition fails. Perhaps the simplest is optimization of quartic polynomials. Indeed, checking if 0 is a local minimizer of the quartic  f (x) =  qijx2  i x2 j  n (cid:88)  i,j=1  is equivalent to checking whether the matrix Q = [qij] is co-positive, a co-NP complete problem. For this f , the Hessian at x = 0 is zero. In concurrent work, Anandkumar and Ge (2016) have proposed an algorithm to avoid third-order saddles. Interestingly, the strict saddle property failing is analogous in dynamical systems to the existence of a slow manifold where complex dynamics may emerge. Slow manifolds give rise to metastability, bifurcation, and other chaotic dynamics, and it would be intriguing to see how the analysis of chaotic systems could be applied to understand the behavior of optimization algorithms around these difficult critical points.  Acknowledgements  The authors would like to thank Chi Jin, Tengyu Ma, Robert Nishihara, Mahdi Soltanolkotabi, Yuekai Sun, Jonathan Taylor, and Yuchen Zhang for their insightful feedback. MS is generously supported by an NSF Graduate Research Fellowship. BR is generously supported by ONR awards N00014-14-1-0024, N00014-15-1-2620, and N00014-13-1-0129, and NSF awards CCF-1148243 and CCF-1217058. MIJ is generously supported by ONR award N00014-11-1-0688 and by the ARL and the ARO under grant number W911NF-11-1-0391. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple Inc., Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm, Virdata and VMware.  References  2009.  Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent meth-  ods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.  Robert J Adler and Jonathan E Taylor. Random fields and geometry. Springer Science & Business Media, GRADIENT DESCENT CONVERGES TO MINIMIZERS  Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in non-convex  optimization. arXiv preprint arXiv:1602.05908, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse  coding. In Proceedings of The 28th Conference on Learning Theory, pages 113-149, 2015.  H\u00b4edy Attouch, J\u00b4er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the Kurdyka-Lojasiewicz inequality. Mathematics of Operations Research, 35(2):438-457, 2010.  Hedy Attouch, J\u00b4er\u02c6ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, 137(1-2):91-129, 2013.  Antonio Auffinger, G\u00b4erard Ben Arous, and Ji\u02c7r\u00b4\u0131 \u02c7Cern`y. Random matrices and complexity of spin glasses.  Communications on Pure and Applied Mathematics, 66(2):165-201, 2013.  Mikhail Belkin, Luis Rademacher, and James Voss. Basis learning as an algorithmic primitive. arXiv preprint  arXiv:1411.1420, 2014.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, Olivier Ley, Laurent Mazet, et al. Characterizations of Lojasiewicz inequali-  ties: subgradient \ufb02ows, talweg, convexity. Trans. Amer. Math. Soc, 362(6):3319-3363, 2010.  T Tony Cai, Xiaodong Li, and Zongming Ma. Optimal rates of convergence for noisy sparse phase retrieval  via thresholded Wirtinger \ufb02ow. arXiv preprint arXiv:1506.03382, 2015.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow: Theory  and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The loss surface  of multilayer networks. arXiv:1412.0233, 2014.  Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. SIAM, 2000.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Ben- gio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems, pages 2933-2941, 2014.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient  for tensor decomposition. arXiv:1503.02101, 2015.  Philip E Gill and Walter Murray. Newton-type methods for unconstrained and linearly constrained optimiza-  tion. Mathematical Programming, 7(1):311-350, 1974.  M.W. Hirsch, C.C. Pugh, and M. Shub. Invariant Manifolds. Number no. 583 in Lecture Notes in Mathemat-  ics. Springer-Verlag, 1977.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries.  IEEE Transactions on Information Theory, 56(6):2980-2998, 2009.  K Lange. Optimization. springer texts in statistics. 2013.  Jorge J Mor\u00b4e and Danny C Sorensen. On the use of directions of negative curvature in a modified Newton  method. Mathematical Programming, 16(1):1-20, 1979. LEE SIMCHOWITZ  Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear programming.  Mathematical programming, 39(2):117-129, 1987.  Yurii Nesterov. Media, 2004.  Introductory lectures on convex optimization, volume 87. Springer Science & Business  Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance.  Mathematical Programming, 108(1):177-205, 2006.  Ioannis Panageas and Georgios Piliouras. Gradient descent converges to minimizers: The case of non-isolated  critical points. arXiv preprint arXiv:1605.00405, 2016.  Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem for  non-convex optimization. arXiv:1405.4604, 2014.  Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The  Annals of Probability, pages 698-712, 1990.  Michael Shub. Global stability of dynamical systems. Springer Science & Business Media, 1987.  Stephen Smale. Differentiable dynamical systems. Bulletin of the American mathematical Society, 73(6):  747-817, 1967.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the  geometric picture. arXiv:1511.03607, 2015a.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere II: Recovery by Riemannian  trust-region method. arXiv:1511.04777, 2015b.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.  Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pages 1260-1268, 2014.  Tuo Zhao, Zhaoran Wang, and Han Liu. Nonconvex low rank matrix factorization via inexact first order  oracle."}