{"1": "Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320, 2011.  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural  Information Processing Systems, pages 41-48, 2008.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Victor H de la Pe\u02dcna and Guodong Pang. Exponential inequalities for self-normalized processes with  applications. Electronic Communications in Probability, 14:372-381, 2009.  Victor H de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: Exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 32(3):1902- 1933, 2004.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118,  1975.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learn- ing Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic In Proceedings of The 27th Conference on Learning Theory, COLT and online optimization. 2014, Barcelona, Spain, June 13-15, 2014, pages 197-209, 2014. URL http://jmlr.org/ proceedings/papers/v35/hazan14a.html.  Anatoli Juditsky and Yuri Nesterov. Primal-dual subgradient methods for minimizing uniformly  convex functions. Technical report, 2010.  13   GENERALIZATION OF STOCHASTIC EXPONENTIALLY CONCAVE OPTIMIZATION  References  Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320, 2011.  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural  Information Processing Systems, pages 41-48, 2008.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Victor H de la Pe\u02dcna and Guodong Pang. Exponential inequalities for self-normalized processes with  applications. Electronic Communications in Probability, 14:372-381, 2009.  Victor H de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Self-normalized processes: Exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 32(3):1902- 1933, 2004.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118,  1975.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learn- ing Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic In Proceedings of The 27th Conference on Learning Theory, COLT and online optimization. 2014, Barcelona, Spain, June 13-15, 2014, pages 197-209, 2014. URL http://jmlr.org/ proceedings/papers/v35/hazan14a.html.  Anatoli Juditsky and Yuri Nesterov. Primal-dual subgradient methods for minimizing uniformly  convex functions. Technical report, 2010. MAHDAVI ZHANG JIN  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex pro- gramming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Tomer Koren. Open problem: Fast stochastic exp-concave optimization. In Proceedings of the 26th  Annual Conference on Learning Theory, pages 1073-1075, 2013.  Mehrdad Mahdavi and Rong Jin. Excess risk bounds for exponentially concave losses. ArXiv  e-prints, arXiv:1401.4566, 2014.  Wiley & Sons Ltd, 1983.  A. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization. John  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course, volume 87 of Applied  optimization. Kluwer Academic Publishers, 2004.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Ohad Shamir. The sample complexity of learning linear predictors with the squared loss. ArXiv  e-prints, arXiv:1406.5143, 2014."}