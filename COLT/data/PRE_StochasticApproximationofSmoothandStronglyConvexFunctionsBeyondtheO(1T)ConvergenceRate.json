{"1": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  Proceedings of the 33rd International Conference on Machine Learning, pages 699-707, 2016.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1-51, 2017.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.  Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems 29, pages 3576-3584, 2016.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, pages 797-842, 2015.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paral- lelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. Journal of Machine Learning Research, 18(223):1-42, 2018.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  13   FAST RATES FOR STOCHASTIC APPROXIMATION  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  Proceedings of the 33rd International Conference on Machine Learning, pages 699-707, 2016.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1-51, 2017.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.  Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems 29, pages 3576-3584, 2016.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, pages 797-842, 2015.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paral- lelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. Journal of Machine Learning Research, 18(223):1-42, 2018.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013. FAST RATES FOR STOCHASTIC APPROXIMATION  Sujin Kim, Raghu Pasupathy, and Shane G. Henderson. A Guide to Sample Average Approximation,  pages 207-243. 2015.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  G\u00b4abor Lugosi. Concentration-of-measure inequalities. Technical report, Department of Economics,  Pompeu Fabra University, 2009.  Mehrdad Mahdavi and Rong Jin. Passive learning with target risk. Annual Conference on Learning Theory, pages 252-269, 2013.  In Proceedings of the 26th  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Proceedings of the 28th Annual Conference on Learning Theory, 2015.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. ArXiv e-  prints, arXiv:1605.01288, 2016.  Eric Moulines and Francis R. Bach. Non-asymptotic analysis of stochastic approximation algo- rithms for machine learning. In Advances in Neural Information Processing Systems 24, pages 451-459, 2011.  I. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for non-strongly  convex optimization. Mathematical Programming, 175(1):69-107, 2019.  Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems 27, pages 1017-1025, 2014.  A. Nemirovski and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. John  Wiley & Sons Ltd, 1983.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Random gradient-free minimization of convex functions. Core discussion papers,  2011.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00b4as P\u00b4ocz\u00b4os, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on Machine Learning, 2016. FAST RATES FOR STOCHASTIC APPROXIMATION  Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo- nential convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25, pages 2672-2680, 2012.  Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong  growth condition. ArXiv e-prints, arXiv:1308.6370, 2013.  Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized  loss minimization. Journal of Machine Learning Research, 14:567-599, 2013.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver- gence results and optimal averaging schemes. In Proceedings of the 30th International Confe- rence on Machine Learning, pages 71-79, 2013.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on Stochastic Pro-  gramming: Modeling and Theory. SIAM, second edition, 2014.  M.V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computa-  tional Optimization and Applications, 11(1):23-35, 1998.  Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth  loss. ArXiv e-prints, arXiv:1009.3896, 2010.  Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In Proceedings of the 30th Annual Conference on Learning Theory, pages 1882-1919, 2017.  Andre Wibisono, Martin J Wainwright, Michael I. Jordan, and John C. Duchi. Finite sample con- vergence rates of zero-order stochastic optimization methods. In Advances in Neural Information Processing Systems 25, pages 1448-1456, 2012.  Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde- pendent access of full gradients. In Advance in Neural Information Processing Systems 26, pages 980-988, 2013a.  Lijun Zhang, Tianbao Yang, Rong Jin, and Xiaofei He. O(log T ) projections for stochastic op- timization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning, 2013b.  Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex opti- mization: O(1/n)- and O(1/n2)-type of risk bounds. In Proceedings of the 30th Annual Confe- rence on Learning Theory, pages 1954-1979, 2017a.  Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent al- In Proceedings of the 21st International Conference on Machine Learning, pages  gorithms. 919-926, 2004. FAST RATES FOR STOCHASTIC APPROXIMATION  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In Proceedings of the 30th Annual Conference on Learning Theory, pages 1980-2022, 2017b.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928-936, 2003."}