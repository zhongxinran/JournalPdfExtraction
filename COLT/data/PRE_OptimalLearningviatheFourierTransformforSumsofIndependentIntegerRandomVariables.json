{"1": "NY, 1992.  2010.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  P. Assouad. Deux remarques sur l\u2019estimation. C. R. Acad. Sci. Paris S\u00b4er. I, 296:1021-1024, 1983.  A.D. Barbour, L. Holst, and S. Janson. Poisson Approximation. Oxford University Press, New York,  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  15   LEARNING SIIRVS VIA THE FOURIER TRANSFORM  some neighborhood of Pn(p). Specifically, there is some \u03b4 > 0 such that P \u22121 x \u2208 Tn with (cid:107)x \u2212 Pn(p)(cid:107)\u221e \u2264 \u03b4.  n is defined at every  Let Q be a distribution over [n] satisfying dK(P, Q) \u2264 \u03b4. Equivalently, if y = (Q(< i))n  i=1 \u2208 Tn is the CDF of Q, then (cid:107)Pn(p) \u2212 y(cid:107)\u221e \u2264 \u03b4. Thus, P \u22121 n (y) \u2208 Tn are the parameters of a 2-SIIRV with distribution Q. It follows that Q is a 2-SIIRV with parameters q, which completes the proof of (i). Note that the proof also implies that Q in this neighborhood can be taken to be Pn(q(cid:48)) for q(cid:48) in some small neighborhood of p.  n is defined at y and q = P \u22121  To prove part (ii) of Lemma 9, we use a geometric argument. Recall that the parameters of 1 4(n+1) . 4(n+1) , and therefore  P0 are p0 = ( 1 We show in the full version that any Q in Pn(\u2202S) satisfies dTV (P0, Q) \u2265 e\u22123n dK(P0, Q) \u2265 e\u22123n  n+1 ). Let S \u2286 Tn be the set of vectors p with (cid:107)p \u2212 p0(cid:107)\u221e \u2264  n+1 , . . . , n  8(n+1)2 \u2265 2\u22129n.  Let B be the set of distributions Q on [n] so that dK(P0, Q) \u2264 2\u22129n. We claim that Pn(S) \u2229 B = B. To begin, note that S is compact, and therefore this intersection is closed. On the other hand, since Pn(\u2202S) is disjoint from B, this intersection is Pn(int(S)) \u2229 B. On the other hand, since Pn has non-singular Jacobian on int(S), the open mapping theorem implies that Pn(int(S)) \u2229 B is an open subset of B. Therefore, Pn(S) \u2229 B is both a closed and open subset of B, and therefore, since B is connected, it must be all of B. This completes the proof of part (ii).  Given Lemma 9, our sample lower bound proceeds as follows: Starting from the 2-SIIRV P0, we perturb its pdf by a small amount to construct the \u201chypercube\u201d distributions Pb satisfying the conditions of Assouad\u2019s lemma. Lemma 9 guarantees that, if the perturbation is small enough, all these distributions are indeed 2-SIIRVs.  Acknowledgements. Part of this work was performed while I.D. and A.S. were at the University of Edinburgh, supported in part by EPSRC grant EP/L021749/1 and a Marie Curie Career Inte- gration Grant (CIG). The research of D.K. was supported in part by NSF Award CCF-1553288 (CAREER).  References  NY, 1992.  2010.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  P. Assouad. Deux remarques sur l\u2019estimation. C. R. Acad. Sci. Paris S\u00b4er. I, 296:1021-1024, 1983.  A.D. Barbour, L. Holst, and S. Janson. Poisson Approximation. Oxford University Press, New York,  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112, DIAKONIKOLAS KANE STEWART  L. Birg\u00b4e. On estimating a density using Hellinger distance and some other strange facts. Probab.  Theory Relat. Fields, 71(2):271-291, 1986.  R. Blei, F. Gao, and W. V. Li. Metric entropy of high dimensional distributions. Proceedings of the  American Mathematical Society (AMS), 135(12):4009 - 4018, 2007.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  B. Carl and I. Stephani. Entropy, compactness and the approximation of operators, volume 98 of  Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, 1990.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  L. Chen, L. Goldstein, and Q.-M. Shao. Normal Approximation by Stein\u2019s Method. Springer, 2011.  L. H. Y. Chen and Y. K. Leong. From zero-bias to discretized normal approximation. 2010.  S.X. Chen and J.S. Liu. Statistical applications of the Poisson-Binomial and Conditional Bernoulli  Distributions. Statistica Sinica, 7:875-892, 1997.  X. Chen, D. Durfee, and A. Orfanou. On the complexity of nash equilibria in anonymous games.  In STOC, 2015.  H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. Ann. Math. Statist., 23:493-507, 1952.  M. Cryan, L. Goldberg, and P. Goldberg. Evolutionary trees can be learned in polynomial time in  the two state general Markov model. SIAM Journal on Computing, 31(2):375-397, 2002.  C. Daskalakis and C. Papadimitriou. On Oblivious PTAS\u2019s for Nash Equilibrium. In STOC, pages  75-84, 2009.  C. Daskalakis and C. Papadimitriou. Sparse covers for sums of indicators. Probability Theory and  Related Fields, pages 1-27, 2014.  C. Daskalakis and C. H. Papadimitriou. Computing equilibria in anonymous games.  In FOCS,  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  pages 83-93, 2007.  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In LEARNING SIIRVS VIA THE FOURIER TRANSFORM  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of Inde-  pendent Integer Random Variables. In FOCS, pages 217-226, 2013.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and  its applications. CoRR, abs/1511.03641, 2015a.  C. Daskalakis, G. Kamath, and C. Tzamos. On the structure, covering, and learning of poisson  multinomial distributions. In FOCS, 2015b. Available as arxiv report abs/1504.08363.  L. Devroye and L. Gy\u00a8orfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in Statis-  1985.  tics, Springer, 2001.  I. Diakonikolas, D. M. Kane, and A. Stewart. Nearly optimal learning and sparse covers for sums  of independent integer random variables. CoRR, abs/1505.00662, 2015a.  I. Diakonikolas, D. M. Kane, and A. Stewart. Properly learning poisson binomial distributions in  almost polynomial time. CoRR, abs/1511.04066, 2015b.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distri-  butions and its algorithmic applications. CoRR, abs/1511.03592, 2015c.  D. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algo-  rithms. Cambridge University Press, Cambridge, 2009.  R.M Dudley. Metric entropy of some classes of sets with differentiable boundaries. Journal of Approximation Theory, 10(3):227 - 236, 1974. ISSN 0021-9045. doi: 10.1016/0021-9045(74) 90120-8.  D. E. Edmunds and H. Triebel. Function spaces, entropy numbers, differential operators, volume  120 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, 1996.  J. Feldman, R. O\u2019Donnell, and R. Servedio. Learning mixtures of product distributions over discrete  domains. In Proc. 46th IEEE FOCS, pages 501-510, 2005.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In Proceedings of the  12th Annual COLT, pages 183-192, 1999.  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  A. Guntuboyina and B. Sen. Covering numbers for convex functions. Information Theory, IEEE  Transactions on, 59(4):1957-1965, April 2013.  S. Har-peled. Geometric Approximation Algorithms. American Mathematical Society, Boston, MA,  USA, 2011.  R. Hasminskii and I. Ibragimov. On density estimation in the view of kolmogorov\u2019s ideas in ap-  proximation theory. Ann. Statist., 18(3):999-1010, 1990. DIAKONIKOLAS KANE STEWART  D. Haussler and M. Opper. Mutual information, metric entropy and cumulative relative entropy  risk. Ann. Statist., 25(6):2451-2492, 1997.  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58:13-30, 1963.  A. J. Izenman. Recent developments in nonparametric density estimation. Journal of the American  Statistical Association, 86(413):205-224, 1991.  A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In STOC,  M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cam-  pages 553-562, 2010.  bridge, MA, 1994.  M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. On the learnability of  discrete distributions. In Proc. 26th STOC, pages 273-282, 1994.  A. N. Kolmogorov and V. M. Tihomirov. \u03b5-entropy and \u03b5-capacity of sets in function spaces. Uspehi  Mat. Nauk, 14:3-86, 1959.  J. Kruopis. Precision of approximation of the generalized binomial distribution by convolutions of  poisson measures. Lithuanian Mathematical Journal, 26(1):37-49, 1986.  G. G. Lorentz. Metric entropy and approximation. Bull. Amer. Math. Soc., 72:903-937, 1966.  Y. Makovoz. On the kolmogorov complexity of functions of finite smoothness. Journal of Com-  plexity, 2(2):121 - 130, 1986.  Bachelier, Paris, 1837.  Appl., 28:393-403, 1983.  S.D. Poisson. Recherches sur la Probabilit`e des jugements en mati\u00b4e criminelle et en mati\u00b4ere civile.  E. L. Presman. Approximation of binomial distributions by infinitely divisible ones. Theory Probab.  B. Roos. Binomial approximation to the Poisson binomial distribution: The Krawtchouk expansion.  Theory Probab. Appl., 45:328-344, 2000.  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  1992.  porated, 2008.  B. W. Silverman. Density Estimation. Chapman and Hall, London, 1986.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incor-  A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer Series  in Statistics. Springer-Verlag, New York, 1996. With applications to statistics.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 113-122, 2002. LEARNING SIIRVS VIA THE FOURIER TRANSFORM  Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Ann.  Statist., 27(5):1564-1599, 1999.  Y. G. Yatracos. Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy.  Annals of Statistics, 13:768-774, 1985."}