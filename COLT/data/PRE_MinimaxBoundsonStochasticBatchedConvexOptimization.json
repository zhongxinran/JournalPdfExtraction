{"1": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. IEEE Transactions on Information Theory, 58 (5):3235-3249, 2012.  G. Ballard, J. Demmel, O. Holtz, and O. Schwartz. Minimizing communication in numerical linear  algebra. SIAM Journal on Matrix Analysis and Applications, 32(3):866-901, 2011.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013a.  N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the Twenty-Fourth ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1190-1204, 2013b.  G. Dantzig. On the non-existence of tests of Student\u2019s hypothesis having power functions indepen-  dent of \u03c3. The Annals of Mathematical Statistics, 11(2):186-192, 1940.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.  J. C. Duchi. Introductory lectures on stochastic convex optimization. In Park City Mathematics Institute Graduate Summer School: Collected Lectures. American Mathematical Society (forth- coming), 2017.  J. C. Duchi, P. L. Bartlett, and M. J. Wainwright. Randomized smoothing for stochastic optimiza-  tion. SIAM Journal on Optimization, 22(2):674-701, 2012.  S. Fuller and L. Millett. The Future of Computing Performance: Game Over or Next Level? Na-  tional Academies Press, 2011.  ence, 104:121-145, 2002.  1983.  J. Hardwick and Q. F. Stout. Optimal few-stage designs. Journal of Statistical Planning and Infer-  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley,  F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochastic  gradient descent. In Advances in Neural Information Processing Systems 24, 2011.  V. Perchet, P. Rigollet, S. Chassang, and E. Snowberg. Batched bandit problems. Annals of Statis-  tics, 44(2):660681, 2016.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.  In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, pages 3-24, 2013.  A. Smith, A. Thakurta, and J. Upadhyay. Is interaction necessary for distributed private learning?  In IEEE Symposium on Security and Privacy, 2017.  13   MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. IEEE Transactions on Information Theory, 58 (5):3235-3249, 2012.  G. Ballard, J. Demmel, O. Holtz, and O. Schwartz. Minimizing communication in numerical linear  algebra. SIAM Journal on Matrix Analysis and Applications, 32(3):866-901, 2011.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013a.  N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the Twenty-Fourth ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1190-1204, 2013b.  G. Dantzig. On the non-existence of tests of Student\u2019s hypothesis having power functions indepen-  dent of \u03c3. The Annals of Mathematical Statistics, 11(2):186-192, 1940.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.  J. C. Duchi. Introductory lectures on stochastic convex optimization. In Park City Mathematics Institute Graduate Summer School: Collected Lectures. American Mathematical Society (forth- coming), 2017.  J. C. Duchi, P. L. Bartlett, and M. J. Wainwright. Randomized smoothing for stochastic optimiza-  tion. SIAM Journal on Optimization, 22(2):674-701, 2012.  S. Fuller and L. Millett. The Future of Computing Performance: Game Over or Next Level? Na-  tional Academies Press, 2011.  ence, 104:121-145, 2002.  1983.  J. Hardwick and Q. F. Stout. Optimal few-stage designs. Journal of Statistical Planning and Infer-  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley,  F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochastic  gradient descent. In Advances in Neural Information Processing Systems 24, 2011.  V. Perchet, P. Rigollet, S. Chassang, and E. Snowberg. Batched bandit problems. Annals of Statis-  tics, 44(2):660681, 2016.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.  In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, pages 3-24, 2013.  A. Smith, A. Thakurta, and J. Upadhyay. Is interaction necessary for distributed private learning?  In IEEE Symposium on Security and Privacy, 2017. MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION  C. Stein. A two-sample test for a linear hypothesis whose power is independent of the variance.  Annals of Mathematical Statistics, 16(3):243-258, 1945.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.  A. Wald. Contributions to the theory of statistical estimation and testing hypotheses. Annals of  Mathematical Statistics, 10(4):299-326, 1939. MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION"}