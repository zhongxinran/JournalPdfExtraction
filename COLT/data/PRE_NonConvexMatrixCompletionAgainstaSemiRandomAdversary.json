{"1": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Rudolf Ahlswede and Andreas J. Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret mini- mization beyond matrix multiplicative updates. In Proc. 46th ACM Symp. on Theory of Comput- ing, pages 237-245, 2015.  Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th ACM-SIAM Symp. on Discrete Algorithms, pages 1824-1831, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient and neural algorithms  for sparse coding. In Proc. 28th Conference on Learning Theory, page 113149, 2015.  Joshua D. Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-Ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-  perspectives. 1828, 2013.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proc. 31st Intl. Conf. on  Machine Learning, pages 1881-1889, 2014.  13   NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  5. Conclusions  In this paper, we showed that even though non-convex approaches for matrix completion are not robust in the semi-random model, but it is possible to fix them using a pre-processing step. The pre-processing step solves a few convex programs (packing SDPs) to ameliorate the in\ufb02uence of the semi-random adversary. Unlike the full convex relaxation for matrix completion, our pre-processing step runs in nearly-linear time. Combining our pre-processing step with non-convex optimization gives an algorithm that is robust in the semi-random model, and at the same time enjoys the effi- ciency of the non-convex approaches.  An immediate open problem is whether we can prove the output of the pre-processing step allows non-convex optimization to recover the ground truth exactly. More broadly, we hope this work will inspire new ideas that make non-convex optimization more robust.  This work is supported by NSF CCF-1704656. We thank Qingqing Huang, Andrej Risteski, Srinadh Bhojanapalli, Yin Tat Lee for discussions at various stages of the work. Yu Cheng is also supported in part by NSF CCF-1527084, CCF-1535972, CCF-1637397, IIS-1447554, and NSF CAREER Award CCF-1750140.  Acknowledgments  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Rudolf Ahlswede and Andreas J. Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret mini- mization beyond matrix multiplicative updates. In Proc. 46th ACM Symp. on Theory of Comput- ing, pages 237-245, 2015.  Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th ACM-SIAM Symp. on Discrete Algorithms, pages 1824-1831, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient and neural algorithms  for sparse coding. In Proc. 28th Conference on Learning Theory, page 113149, 2015.  Joshua D. Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-Ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-  perspectives. 1828, 2013.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proc. 31st Intl. Conf. on  Machine Learning, pages 1881-1889, 2014. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of  Algorithms, 19(2):204-234, 1995.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. Information Theory, IEEE Transactions on, 56(5):2053-2080, 2010.  Ji Chen and Xiaodong Li. Memory-efficient kernel PCA via partial matrix sampling and nonconvex optimization: a model-free analysis of local minima. arXiv preprint arXiv:1711.01742, 2017.  Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent:  General statistical and algorithmic guarantees. arXiv preprint, 1509.03025, 2015.  Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efficient sampling for In Proc. 28th Conference on Learning  Gaussian graphical models via spectral sparsification. Theory, pages 364-390, 2015.  Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving SDD linear systems in nearly m log1/2 n time. In Proc. 45th ACM Symp. on Theory of Computing, pages 343-352, 2014.  Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer and  System Sciences, 63(4):639-671, 2001.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Structures and Algorithms, 16(2):195-208, 2000.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic  gradient for tensor decomposition. arXiv preprint arXiv:1503.02101, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Proc. 28th Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proc. 34th Intl. Conf. on Machine Learning, pages 1233-1242, 2017.  Moritz Hardt. Understanding alternating minimization for matrix completion. In Proc. 55th IEEE  Symp. on Foundations of Computer Science. IEEE, 2014.  Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In Proc.  27th Conference on Learning Theory, pages 638-678, 2014.  Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh. Matrix completion and low-rank SVD  via fast alternating least squares. Journal of Machine Learning Research, 2014.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proc. 44th ACM Symp. on Theory of Computing, pages 665-674. ACM, 2013. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Rahul Jain and Penghui Yao. A parallel approximation algorithm for positive semidefinite pro- In Proc. 52nd IEEE Symp. on Foundations of Computer Science, pages 463-471,  Mark Jerrum. Large cliques elude the metropolis process. Random Struct. Algorithms, 3(4):347-  gramming. 2011.  360, 1992.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.  Jonathan A. Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combi- natorial algorithm for solving SDD systems in nearly-linear time. In Proc. 44th ACM Symp. on Theory of Computing, pages 911-920, 2013.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010a.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. The Journal of Machine Learning Research, 11:2057-2078, 2010b.  Yehuda Koren. The Bellkor solution to the Net\ufb02ix grand prize. Net\ufb02ix prize documentation, 81,  2009.  Ioannis Koutis, Gary L. Miller, and Richard Peng. A nearly-m log n time solver for SDD linear systems. In Proc. 52nd IEEE Symp. on Foundations of Computer Science, pages 590-598, 2011.  Ludek Kucera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics,  57(2-3):193-212, 1995.  Rasmus Kyng and Sushant Sachdeva. Approximate Gaussian elimination for Laplacians - fast, sparse, and simple. In Proc. 57th IEEE Symp. on Foundations of Computer Science, pages 573- 582, 2016.  Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time. In  Proc. 56th IEEE Symp. on Foundations of Computer Science, pages 250-269, 2015.  Yin Tat Lee and He Sun. An SDP-based algorithm for linear-sized spectral sparsification. In Proc.  48th ACM Symp. on Theory of Computing, pages 678-687, 2017.  Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap- proximation via alternating minimization. In Proc. 33rd Intl. Conf. on Machine Learning, pages 2358-2367, 2016.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation algo- rithms for semi-random partitioning problems. In Proc. 43rd ACM Symp. on Theory of Comput- ing, pages 367-384. ACM, 2012.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clustering with noisy partial information. In Proc. 28th Conference on Learning Theory, pages 1321-1342, 2015. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proc. 21st ACM-  SIAM Symp. on Discrete Algorithms, pages 712-728, 2010.  Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11(Aug):2287-2322, 2010.  Ankur Moitra, William Perry, and Alexander S Wein. How robust are reconstruction thresholds for community detection? In Proc. 47th ACM Symp. on Theory of Computing, pages 828-841, 2016.  Sahand Negahban and Martin J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May):1665- 1697, 2012.  Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. arXiv preprint arXiv:1609.03240, 2016.  Richard Peng and Daniel A. Spielman. An efficient parallel solver for SDD linear systems. In Proc.  45th ACM Symp. on Theory of Computing, pages 333-342, 2014.  Richard Peng, Kanat Tangwongsan, and Peng Zhang. Faster and simpler width-independent parallel algorithms for positive semidefinite programming. arXiv preprint arXiv:1201.5135v3, 2016.  Amelia Perry and Alexander S Wein. A semidefinite program for unbalanced multisection in the stochastic block model. In International Conference on Sampling Theory and Applications (SampTA), pages 64-67. IEEE, 2017.  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12:3413-3430, 2011.  Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative  prediction. In Proc. 22nd Intl. Conf. on Machine Learning, pages 713-719. ACM, 2005.  Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geo-  metric functional analysis. Journal of the ACM, 54(4):21, 2007.  Christopher De Sa, Christopher R\u00b4e, and Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. In Proc. 32nd Intl. Conf. on Machine Learning, pages 2332-2341, 2015.  J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117,  2015.  Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM  Journal on Computing, 40(6):1913-1926, 2011.  Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM Journal on  Computing, 40(4):981-1025, 2011. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis Applications, 35(3):835-885, 2014.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proc. 18th Conference on  Learning Theory, pages 545-560, 2005.  Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakkola. Maximum-margin matrix factor- ization. In Proc. 16th Advances in Neural Information Processing Systems, pages 1329-1336, 2004.  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Proc.  56th IEEE Symp. on Foundations of Computer Science, pages 270-289. IEEE, 2015.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In Proc. 27th Advances in Neural Information Processing Systems, pages 559-567, 2015.  Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016."}