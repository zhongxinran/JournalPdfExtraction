{"1": "Martin Arjovsky and L\u00b4eon Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein gan.  arXiv preprint  arXiv:1701.07875, 2017.  Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930-945, 1993. ISSN 00189448. doi: 10.1109/ 18.256500.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994. ISSN 08856125. doi: 10.1007/BF00993164.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. arXiv preprint arXiv:1509.05009, 554, 2015.  George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.  Amit Daniely. Depth separation for neural networks. arXiv preprint arXiv:1702.08489, 2017.  Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. CoRR abs/1506.08473, 2015.  Daniel M Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 633-643. ACM, 2016.  14   LEE GE MA RISTESKI ARORA  References  Martin Arjovsky and L\u00b4eon Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein gan.  arXiv preprint  arXiv:1701.07875, 2017.  Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930-945, 1993. ISSN 00189448. doi: 10.1109/ 18.256500.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994. ISSN 08856125. doi: 10.1007/BF00993164.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. arXiv preprint arXiv:1509.05009, 554, 2015.  George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.  Amit Daniely. Depth separation for neural networks. arXiv preprint arXiv:1702.08489, 2017.  Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. CoRR abs/1506.08473, 2015.  Daniel M Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 633-643. ACM, 2016. ON THE ABILITY OF NEURAL NETS TO EXPRESS DISTRIBUTIONS  Leonid Vasilevich Kantorovich and G Sh Rubinstein. On a space of completely additive functions.  Vestnik Leningrad. Univ, 13(7):52-59, 1958.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. In Proceedings of the twenty-sixth annual ACM  On the learnability of discrete distributions. symposium on Theory of computing, pages 273-282. ACM, 1994.  Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Pro- ceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 180-191. VLDB Endowment, 2004.  Diederik P Kingma and Max Welling. Auto-encoding variational bayes.  arXiv preprint  arXiv:1312.6114, 2013.  Ilia Krasikov. Approximations for the bessel and airy functions with an explicit error term. LMS  Journal of Computation and Mathematics, 17(01):209-225, 2014.  Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and  approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.  J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015. doi: 10.1016/j.neunet.2014.09.003. Published online 2014; based on TR arXiv:1404.7828 [cs.NE].  Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016. LEE GE MA RISTESKI ARORA"}