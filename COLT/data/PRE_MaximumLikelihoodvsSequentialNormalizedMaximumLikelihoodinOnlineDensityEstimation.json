{"1": "K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Journal of Machine Learning, 43(3):211-246, 2001.  O.E. Barndor\ufb00-Nielsen. Information and Exponential Families in Statistical Theory. Wiley,  Chichester, UK, 1978.  A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding  and modeling. IEEE Transactions on Information Theory, 44(6):2743-2760, 1998.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors.  Journal of Machine Learning, 43(3):247-264, 2001.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning and Games. Cambridge University  Press, 2006.  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley, 1991.  Sanjoy Dasgupta and Daniel Hsu. On-line estimation with the multivariate gaussian distri-  bution. In Conference on Learning Theory (COLT \u201907), 2007.  A.P. Dawid. Present position and potential developments: Some personal views, statistical  theory, the prequential approach. J. Royal Stat.Soc., Ser. A, 147(2):278-292, 1984.  Y. Freund. Predicting a binary sequence almost as well as the optimal biased coin.  In  Computational Learning Theory (COLT\u2019 96), pages 89-98, 1996.  P. Gr\u00a8unwald and W. Kot(cid:32)lowski. Prequential plug-in codes that achieve optimal redundancy rates even if the model is wrong. In The IEEE International Symposium on Information Theory (ISIT \u201910), 2010.  P. D. Gr\u00a8unwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA,  2007.  P. D. Gr\u00a8unwald and S. de Rooij. Asymptotic log-loss of prequential maximum likelihood  codes. In Conference on Learning Theory (COLT 2005), pages 652-667, 2005.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex opti-  mization. Machine Learning, 69(2-3):169-192, 2007.  W. Kot(cid:32)lowski, P. Gr\u00a8unwald, and S. de Rooij. Following the \ufb02attened leader. In Conference  on Learning Theory (COLT \u201910), 2010.  474   Kot(cid:32)lowski Gr\u00a8unwald  In future work, we plan to work on the two open questions posed in the paper: (1) Is is possible to relax the condition that the model is constrained to the compact subset of the parameter space by conditioning the regret on the outcome from the first iteration? (2) When is SNML equal to Bayes with Je\ufb00reys\u2019 prior and is there any general relationship between the worst-case regrets of the two?  References  K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Journal of Machine Learning, 43(3):211-246, 2001.  O.E. Barndor\ufb00-Nielsen. Information and Exponential Families in Statistical Theory. Wiley,  Chichester, UK, 1978.  A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding  and modeling. IEEE Transactions on Information Theory, 44(6):2743-2760, 1998.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors.  Journal of Machine Learning, 43(3):247-264, 2001.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning and Games. Cambridge University  Press, 2006.  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley, 1991.  Sanjoy Dasgupta and Daniel Hsu. On-line estimation with the multivariate gaussian distri-  bution. In Conference on Learning Theory (COLT \u201907), 2007.  A.P. Dawid. Present position and potential developments: Some personal views, statistical  theory, the prequential approach. J. Royal Stat.Soc., Ser. A, 147(2):278-292, 1984.  Y. Freund. Predicting a binary sequence almost as well as the optimal biased coin.  In  Computational Learning Theory (COLT\u2019 96), pages 89-98, 1996.  P. Gr\u00a8unwald and W. Kot(cid:32)lowski. Prequential plug-in codes that achieve optimal redundancy rates even if the model is wrong. In The IEEE International Symposium on Information Theory (ISIT \u201910), 2010.  P. D. Gr\u00a8unwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA,  2007.  P. D. Gr\u00a8unwald and S. de Rooij. Asymptotic log-loss of prequential maximum likelihood  codes. In Conference on Learning Theory (COLT 2005), pages 652-667, 2005.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex opti-  mization. Machine Learning, 69(2-3):169-192, 2007.  W. Kot(cid:32)lowski, P. Gr\u00a8unwald, and S. de Rooij. Following the \ufb02attened leader. In Conference  on Learning Theory (COLT \u201910), 2010. ML vs. SNML in On-line Density Estimation  M. Raginsky, R. F. Marcia, S. Jorge, and R. Willett. Sequential probability assignment via online convex programming using exponential families. In IEEE International Symposium on Information Theory, 2009.  J. Rissanen. Fisher information and stochastic complexity. IEEE Trans. Information The-  ory, IT-42(1):40-47, 1996.  J. Rissanen and T. Roos. Conditional NML universal models. In Information Theory and  Applications Workshop (ITA-07), pages 337-341, 2007.  T. Roos and J. Rissanen. On sequentially normalized maximum likelihood models.  In Workshop on Information Theoretic Methods in Science and Engineering (WITMSE-08), 2008.  Y. Shtarkov. Universal sequential coding of single messages. Problems of Information  Transmission, 23(3):175-186, 1987.  E. Takimoto and M. Warmuth. The last-step minimax algorithm. In Conference on Algo-  rithmic Learning Theory (ALT \u201900), 2000."}