{"1": "Shai Ben-David, Nathan Srebro, and Ruth Urner. Universal learning vs. no free lunch results. In  Philosophy and Machine Learning Workshop NIPS, 2011.  Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan. Minimizing the misclassifi-  cation error rate using a surrogate convex loss. In Proceedings ICML, pages 83-90, 2012.  Robert P W Duin. Small sample size generalization. In Proceedings of the Scandinavian Conference  on Image Analysis, volume 2, pages 957-964, 1995.  Wouter M Koolen, Peter Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and  stochastic fast rates in online learning. In NIPS, pages 4457-4465, 2016.  Jesse H Krijthe and Marco Loog. Projected estimators for robust semi-supervised classification.  Machine Learning, 106(7):993-1008, 2017.  Anqi Liu, Lev Reyzin, and Brian D Ziebart. Shift-pessimistic Active Learning Using Robust Bias-  aware Prediction. In Proceedings of AAAI-15, pages 2764-2770, 2015.  Marco Loog. Constrained parameter estimation for semi-supervised learning: the case of the nearest  mean classifier. In ECML PKDD 2010, pages 291-304. Springer, 2010.  Marco Loog and Robert P W Duin. The dipping phenomenon. In Proceedings of the IAPR S+SSPR,  pages 310-317. Springer, 2012.  Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural  networks III, pages 151-209. Springer, 1996.  Sarunas Raudys and Robert P W Duin. Expected classification error of the Fisher linear classifier  with pseudo-inverse covariance matrix. Pattern recognition letters, 19(5-6):385-392, 1998.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  4   OPEN PROBLEM: MONOTONICITY OF LEARNING  of demanding a lower loss, we may require that the loss does not degrade too much. Or we can demand the property to hold with high probability with respect to both samples.  More generally, we may ask: why and how does this behaviour occur? And maybe more impor- tantly: how can we provably avoid non-monotone behaviour? What conditions does a learner need to satisfy to be monotone? Perhaps particular loss functions lead to monotone learners? What if we allow for learning under regularization or other strategies deviating from strict ERM, for example improper learners or randomized decision rules?  Perhaps it is always possible to find a D for a given Z on which learners are non-monotone. In that case, is it possible to avoid non-monotone behaviour under some assumptions on D? Realize- ability or well-specification could be good candidate-assumptions on D. In fact, this raises the issue to what extent well-specified statistical models can actually be proven to behave monotonically. For instance, is Example II monotone if the problem is well-specified?  All in all, we believe the question of monotonicity of learning offers various tantalizing ques-  tions to study, some of which may yet have to be formulated.  References  Shai Ben-David, Nathan Srebro, and Ruth Urner. Universal learning vs. no free lunch results. In  Philosophy and Machine Learning Workshop NIPS, 2011.  Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan. Minimizing the misclassifi-  cation error rate using a surrogate convex loss. In Proceedings ICML, pages 83-90, 2012.  Robert P W Duin. Small sample size generalization. In Proceedings of the Scandinavian Conference  on Image Analysis, volume 2, pages 957-964, 1995.  Wouter M Koolen, Peter Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and  stochastic fast rates in online learning. In NIPS, pages 4457-4465, 2016.  Jesse H Krijthe and Marco Loog. Projected estimators for robust semi-supervised classification.  Machine Learning, 106(7):993-1008, 2017.  Anqi Liu, Lev Reyzin, and Brian D Ziebart. Shift-pessimistic Active Learning Using Robust Bias-  aware Prediction. In Proceedings of AAAI-15, pages 2764-2770, 2015.  Marco Loog. Constrained parameter estimation for semi-supervised learning: the case of the nearest  mean classifier. In ECML PKDD 2010, pages 291-304. Springer, 2010.  Marco Loog and Robert P W Duin. The dipping phenomenon. In Proceedings of the IAPR S+SSPR,  pages 310-317. Springer, 2012.  Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural  networks III, pages 151-209. Springer, 1996.  Sarunas Raudys and Robert P W Duin. Expected classification error of the Fisher linear classifier  with pseudo-inverse covariance matrix. Pattern recognition letters, 19(5-6):385-392, 1998.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014."}