{"1": "J Abernethy and E Hazan. Faster convex optimization: Simulated annealing with an ef\ufb01cient uni-  versal barrier. arxiv 1507.02528, 2015.  Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1\u20139, 2009.  Naman Agarwal, Brian Bullins, and Elad Hazan. Second order stochastic optimization for machine  learning in linear time. arXiv preprint arXiv:1602.03943, 2016.  12   LOWER BOUNDS FOR HIGHER-ORDER CONVEX OPTIMIZATION  Theorem 13 For any integer k, any T > 5k, \u03b4 \u2208 [0, 1], and any k-order (potentially randomized algorithm), there exists a k-differentiable convex function f \u2020 : Bd \u2192 R for d = \u2126(T 3 log(T 2/\u03b4)), such that with probability at least 1 \u2212 \u03b4 (over the randomness of the algorithm) for T steps of the algorithm every point y queried by the algorithm is such that  f \u2020(y) \u2265 min x\u2208Bd  f \u2020(x) +  1 \u221a 2  T  .  Moreover the function f \u2020 is guaranteed to be k-differentiable with Lipschitz constants Li bounded as  \u2200 i \u2264 k Li+1 \u2264 (20kT 2.5)i.  Due to space constraints the proof of Theorem 13 is included in the"}