{"1": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Giulia Desalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In Proceedings of  the 26th International Conference on Algorithmic Learning Theory, pages 254-269, 2015.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. ArXiv e-prints, arXiv:1608.04414, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. ArXiv e-prints, arXiv:1601.04011, 2016.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery  Problems. Springer, 2011.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning In Proceedings of the 9th Annual Conference on Computational Learning  with squared loss. Theory, pages 140-146, 1996.  13   O(1/n)- AND O(1/n2)-TYPE OF RISK BOUNDS OF ERM  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Giulia Desalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In Proceedings of  the 26th International Conference on Algorithmic Learning Theory, pages 254-269, 2015.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. ArXiv e-prints, arXiv:1608.04414, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. ArXiv e-prints, arXiv:1601.04011, 2016.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery  Problems. Springer, 2011.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning In Proceedings of the 9th Annual Conference on Computational Learning  with squared loss. Theory, pages 140-146, 1996. ZHANG YANG JIN  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization In Proceedings of the 28th Conference on  of stochastic exponentially concave optimization. Learning Theory, 2015.  Colin McDiarmid. On the method of bounded differences.  In Surveys in Combinatorics, pages  148-188, 1989.  prints, arXiv:1605.01288, 2016.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. ArXiv e-  Ron Meir and Tong Zhang. Generalization error bounds for bayesian mixture algorithms. Journal  of Machine Learning Research, 4:839-860, 2003.  Eric Moulines and Francis R. Bach. Non-asymptotic analysis of stochastic approximation algo- rithms for machine learning. In Advances in Neural Information Processing Systems 24, pages 451-459, 2011.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course, volume 87 of Applied  optimization. Kluwer Academic Publishers, 2004.  Dmitriy Panchenko. Some extensions of an inequality of vapnik and chervonenkis. Electronic  Communications in Probability, 7:55-65, 2002.  Gilles Pisier. The volume of convex bodies and Banach space geometry. Cambridge Tracts in  Mathematics (No. 94). Cambridge University Press, 1989.  Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming. Communi-  cations on Pure and Applied Mathematics, 66(8):1275-1297, 2013.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Bernhard Sch\u00a8olkopf and Alexander J. Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT Press, 2002.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on Stochastic Pro-  gramming: Modeling and Theory. SIAM, second edition, 2014.  Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their ap-  proximations. Constructive Approximation, 26(2):153-172, 2007. O(1/n)- AND O(1/n2)-TYPE OF RISK BOUNDS OF ERM  Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth  loss. ArXiv e-prints, arXiv:1009.3896, 2010.  Karthik Sridharan, Shai Shalev-shwartz, and Nathan Srebro. Fast rates for regularized objectives.  In Advances in Neural Information Processing Systems 21, pages 1545-1552, 2009.  Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of  Statistics, 32:135-166, 2004.  Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 2000.  Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde- pendent access of full gradients. In Advance in Neural Information Processing Systems 26, pages 980-988, 2013a.  Lijun Zhang, Tianbao Yang, Rong Jin, and Xiaofei He. O(log T ) projections for stochastic op- timization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning, 2013b."}