{"1": "Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211\u2013246, 2001.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. Neural Networks, IEEE Transactions on, 7(3):604\u2013619, 1996.  J\u00a8urgen Forster. On relative loss bounds in generalized linear regression. In Fundamentals of Com-  putation Theory, pages 269\u2013280. Springer, 1999.  Dean P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084\u20131090, 1991.  Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1\u201363, 1997.  Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Ef\ufb01cient minimax strategies for square loss  games. In Advances in Neural Information Processing Systems, pages 3230\u20133238, 2014.  Edward Moroshko and Koby Crammer. Weighted last-step min\u2013max algorithm with improved sub-  logarithmic regret. Theoretical Computer Science, 558:107\u2013124, 2014.  Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In  13th COLT, pages 100\u2013106, 2000.  Volodimir G. Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371\u2013383. Morgan Kaufmann, 1990.  Volodya Vovk. Competitive on-line linear regression. Advances in Neural Information Processing  Systems, pages 364\u2013370, 1998.  Appendix A. Alternative Pt recurrence  Lemma 11 For the Pt matrices de\ufb01ned in Theorem 2, we have  P \u22121  t =  xqx(cid:124)  q +  t (cid:88)  q=1  T (cid:88)  q=t+1  x 1 + x  (cid:124) q Pqxq (cid:124) q Pqxq  xqx(cid:124) q .  Proof The proof is by induction. We start with  P \u22121  T =  xtx  (cid:124) t .  T (cid:88)  t=1  13   MINIMAX FIXED-DESIGN LINEAR REGRESSION  References  Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, 2001.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. Neural Networks, IEEE Transactions on, 7(3):604-619, 1996.  J\u00a8urgen Forster. On relative loss bounds in generalized linear regression. In Fundamentals of Com-  putation Theory, pages 269-280. Springer, 1999.  Dean P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084-1090, 1991.  Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1-63, 1997.  Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Efficient minimax strategies for square loss  games. In Advances in Neural Information Processing Systems, pages 3230-3238, 2014.  Edward Moroshko and Koby Crammer. Weighted last-step min-max algorithm with improved sub-  logarithmic regret. Theoretical Computer Science, 558:107-124, 2014.  Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In  13th COLT, pages 100-106, 2000.  Volodimir G. Vovk. Aggregating strategies. In Proc. Third Workshop on Computational Learning  Theory, pages 371-383. Morgan Kaufmann, 1990.  Volodya Vovk. Competitive on-line linear regression. Advances in Neural Information Processing  Systems, pages 364-370, 1998."}