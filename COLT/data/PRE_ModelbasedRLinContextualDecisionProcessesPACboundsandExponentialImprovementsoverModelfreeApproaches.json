{"1": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear  quadratic systems. In Conference on Learning Theory, 2011.  Eric W Aboaf, Steven M Drucker, and Christopher G Atkeson. Task-level robot learning: Juggling a tennis ball more accurately. In IEEE International Conference on Robotics and Automation, 1989.  Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, and Alexander Russell. A note on the representational incompatibility of function approximation and factored dynamics. In Advances in Neural Information Processing Systems, 2003.  Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforce-  ment learning. arXiv:1703.05449, 2017.  Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear \u03c0 learning using state and action  features. In International Conference on Machine Learning, pages 833-842, 2018.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement  learning. In Advances in Neural Information Processing Systems, pages 2818-2826, 2015.  Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing Systems, 2018.  Marc P Deisenroth, Carl E Rasmussen, and Dieter Fox. Learning to control a low-cost manipulator  using data-efficient reinforcement learning. In Robotics: Science and Systems, 2011.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer Science &  Business Media, 2012.  Carlos Diuk, Lihong Li, and Bethany R Lef\ufb02er. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 249-256. ACM, 2009.  Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for  model-based reinforcement learning. In Artificial Intelligence and Statistics, 2017.  Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu. Robust estimation and generative adversarial nets.  arXiv:1810.02030, 2018.  Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A  kernel two-sample test. Journal of Machine Learning Research, 2012.  13   MODEL-BASED RL IN CDPS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear  quadratic systems. In Conference on Learning Theory, 2011.  Eric W Aboaf, Steven M Drucker, and Christopher G Atkeson. Task-level robot learning: Juggling a tennis ball more accurately. In IEEE International Conference on Robotics and Automation, 1989.  Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, and Alexander Russell. A note on the representational incompatibility of function approximation and factored dynamics. In Advances in Neural Information Processing Systems, 2003.  Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforce-  ment learning. arXiv:1703.05449, 2017.  Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear \u03c0 learning using state and action  features. In International Conference on Machine Learning, pages 833-842, 2018.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement  learning. In Advances in Neural Information Processing Systems, pages 2818-2826, 2015.  Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing Systems, 2018.  Marc P Deisenroth, Carl E Rasmussen, and Dieter Fox. Learning to control a low-cost manipulator  using data-efficient reinforcement learning. In Robotics: Science and Systems, 2011.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer Science &  Business Media, 2012.  Carlos Diuk, Lihong Li, and Bethany R Lef\ufb02er. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 249-256. ACM, 2009.  Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for  model-based reinforcement learning. In Artificial Intelligence and Statistics, 2017.  Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu. Robust estimation and generative adversarial nets.  arXiv:1810.02030, 2018.  Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A  kernel two-sample test. Journal of Machine Learning Research, 2012. MODEL-BASED RL IN CDPS  Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algo-  rithms for factored MDPs. Journal of Aprtificial Intelligence Research, 2003.  Zhaohan Daniel Guo and Emma Brunskill. Sample efficient feature selection for factored MDPs.  arXiv preprint arXiv:1703.03454, 2017.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement  learning. Journal of Machine Learning Research, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex- tual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?  In Advances in Neural Information Processing Systems, 2018.  Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces.  In  International Conference on Machine Learning, 2003.  Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored MDPs. In Interna-  tional Joint Conference on Artificial Intelligence, 1999.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.  Machine learning, 2002.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich  observations. In Advances in Neural Information Processing Systems, 2016.  Kailasam Lakshmanan, Ronald Ortner, and Daniil Ryabko. Improved regret bounds for undiscounted continuous reinforcement learning. In International Conference on Machine Learning, 2015.  Tor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general reinforcement  learning. In International Conference on Machine Learning, 2013.  Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite- horizon off-policy estimation. In Advances in Neural Information Processing Systems, pages 5356-5366, 2018.  Alfred M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in  Applied Probability, 1997.  Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity of finite-  horizon Markov decision process problems. Journal of the ACM, 2000.  Trung Nguyen, Zhuoru Li, Tomi Silander, and Tze Yun Leong. Online feature selection for model- based reinforcement learning. In International Conference on Machine Learning, pages 498-506, 2013.  Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement learning. In Advances in Neural Information Processing Systems 31, pages 8888-8896, 2018. MODEL-BASED RL IN CDPS  Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement  learning. In Advances in Neural Information Processing Systems, 2012.  Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.  In Advances in Neural Information Processing Systems, 2014a.  Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored MDPs. In  Advances in Neural Information Processing Systems, pages 604-612, 2014b.  Jason Pazis and Ronald Parr. PAC Optimal Exploration in Continuous Space Markov Decision  Processes. In AAAI, 2013.  Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-free reinforcement learning. In International Conference on Machine Learning, 2006.  Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.  Istv\u00e1n Szita and Csaba Szepesv\u00e1ri. Model-based reinforcement learning with nearly tight exploration  complexity bounds. In International Conference on Machine Learning, 2010.  Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018.  Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 1992.  Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in  deterministic systems. In Advances in Neural Information Processing Systems, 2013.  Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine learning, 8(3-4):229-256, 1992.  Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based reinforcement learning with theoretical guarantees. arXiv:1807.03858, 2018."}