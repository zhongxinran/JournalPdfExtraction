{"1": "Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. Tech-  nical Report cs.AI/0004001, 2000. http://arxiv.org/abs/cs.AI/0004001.  Marcus Hutter. New error bounds for Solomonoff prediction. Journal of Computer and System  Sciences, 62(4):653-667, 2001.  12   LEIKE HUTTER  Name \u00b5-optimal policy Pareto optimality Balanced Pareto optimality Self-optimizing Strong asymptotic optimality Weak asymptotic optimality  Issue/Comment requires to know the true environment \u00b5 in advance trivial (Theorem 18) dependent on UTM (Corollary 13 and Corollary 14) does not apply to MCCS LSC impossible (Lattimore and Hutter, 2011, Thm. 8) BayesExp (Lattimore, 2013, Ch. 5), but not AIXI (Orseau, 2010)  Table 1: Proposed notions of optimality (Hutter, 2002; Orseau, 2010; Lattimore and Hutter, 2011) and their issues. Weak asymptotic optimality stands out to be the only possible nontrivial optimality notion.  timality) is highly subjective, because it depends on the choice of the UTM: AIXI is not balanced Pareto optimal with respect to all universal mixtures. Moreover, according to Corollary 15, any com- putable policy is nearly balanced Pareto optimal, save some \u03b5 > 0. For finite lifetime discounting, there are UTMs such that every policy has maximal intelligence (Theorem 6). The self-optimizing theorem (Hutter, 2002, Thm. 4 & Thm. 7) is not applicable to the class of all computable envi- ronments MCCS LSC that we consider here, since this class does not allow for self-optimizing policies. Therefore no nontrivial and non-subjective optimality results for AIXI remain (see Table 1). We have to regard AIXI as a relative theory of intelligence, dependent on the choice of the UTM (Sune- hag and Hutter, 2014).  (cid:80)t  (cid:0)V \u2217  The underlying problem is that a discounting Bayesian agent such as AIXI does not have enough time to explore sufficiently; exploitation has to start as soon as possible. In the beginning the agent does not know enough about its environment and therefore relies heavily on its prior. Lack of ex- ploration then retains the prior\u2019s biases. This fundamental problem can be alleviated by adding an extra exploration component. Lattimore (2013) defines BayesExp, a weakly asymptotically op- timal policy \u03c0 that converges (independent of the UTM) to the optimal value in Ces`aro mean: 1 t  \u03bd (\u00e6 <k)(cid:1) \u2192 0 as t \u2192 \u221e \u03bd-almost surely for all \u03bd \u2208 MCCS LSC .  k=1 But it is not clear that weak asymptotic optimality is a good optimality criterion. For example, weak asymptotic optimality can be achieved by navigating into traps (parts of the environment with a simple optimal policy but possibly very low rewards that cannot be escaped). Furthermore, to be weakly asymptotically optimal requires an excessive amount of exploration: BayesExp needs to take exploratory actions that it itself knows to very likely be extremely costly or dangerous. This leaves us with the following open question: What are good optimality criteria for generally intelligent agents (Hutter, 2009, Sec. 5)?  \u03bd (\u00e6 <k) \u2212 V \u03c0  References  Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. Tech-  nical Report cs.AI/0004001, 2000. http://arxiv.org/abs/cs.AI/0004001.  Marcus Hutter. New error bounds for Solomonoff prediction. Journal of Computer and System  Sciences, 62(4):653-667, 2001. BAD UNIVERSAL PRIORS AND NOTIONS OF OPTIMALITY  Marcus Hutter. Self-optimizing and Pareto-optimal policies in general environments based on  Bayes-mixtures. In Computational Learning Theory, pages 364-379. Springer, 2002.  Marcus Hutter. Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Prob-  Marcus Hutter. Open problems in universal induction & intelligence. Algorithms, 3(2):879-906,  ability. Springer, 2005.  2009.  versity, 2013.  pages 368-382. Springer, 2011.  Science, 519:140-154, 2014.  Tor Lattimore. Theory of General Reinforcement Learning. PhD thesis, Australian National Uni-  Tor Lattimore and Marcus Hutter. Asymptotically optimal agents. In Algorithmic Learning Theory,  Tor Lattimore and Marcus Hutter. General time consistent discounting. Theoretical Computer  Shane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. Minds  & Machines, 17(4):391-444, 2007.  Shane Legg and Joel Veness. An approximation of the universal intelligence measure. In Algo- rithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, pages 236-249. Springer, 2013.  Jan Leike and Marcus Hutter. On the computability of AIXI. In Uncertainty in Artificial Intelli-  gence, 2015.  2010.  Ming Li and Paul M. B. Vit\u00b4anyi. An Introduction to Kolmogorov Complexity and Its Applications.  Texts in Computer Science. Springer, 3rd edition, 2008.  Markus M\u00a8uller. Stationary algorithmic probability. Theoretical Computer Science, 411(1):113-130,  Laurent Orseau. Optimality issues of universal greedy agents with static priors.  In Algorithmic  Learning Theory, pages 345-359. Springer, 2010.  Laurent Orseau. Asymptotic non-learnability of universal agents with computable horizon func-  tions. Theoretical Computer Science, 473:149-156, 2013.  Laurent Orseau. Universal knowledge-seeking agents. Theoretical Computer Science, 519:127-  139, 2014.  Laurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for  stochastic environments. In Algorithmic Learning Theory, pages 158-172. Springer, 2013.  Ray Solomonoff. A formal theory of inductive inference. Parts 1 and 2. Information and Control, 7  (1):1-22 and 224-254, 1964.  Ray Solomonoff. Complexity-based induction systems: Comparisons and convergence theorems.  IEEE Transactions on Information Theory, 24(4):422-432, 1978. LEIKE HUTTER  Peter Sunehag and Marcus Hutter. Optimistic agents are asymptotically optimal. In Australasian  Joint Conference on Artificial Intelligence, pages 15-26. Springer, 2012.  Peter Sunehag and Marcus Hutter. Intelligence as inference or forcing Occam on the world. In  Artificial General Intelligence, pages 186-195. Springer, 2014.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,  Cambridge, MA, 1998.  Ian Wood, Peter Sunehag, and Marcus Hutter. (Non-)equivalence of universal priors. In Solomonoff  85th Memorial Conference, pages 417-425. Springer, 2011. BAD UNIVERSAL PRIORS AND NOTIONS OF OPTIMALITY"}