{"1": "J.-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann.  Statist., 37(4):1591-1646, 2009.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning  algorithms. J. Comp. Sys. Sci., 64:48-75, 2002.  393   Sparsity Regret Bounds for Individual Sequences  Proposition 10 (A consequence of Prop. 1 of Dalalyan and Tsybakov 2011) Assume that sup1(cid:54)j(cid:54)d (cid:107)\u03d5j(cid:107)\u221e < \u221e and that the set of assumptions (21) above hold true. Then, for every R > 0 and \u03b7 (cid:54) , the mirror averaging aggregate (cid:98)fT : X \u2192 R defined in Dalalyan and Tsybakov (2011, Equations (1) and (3)) satisfies  (cid:54)R (cid:107)u \u00b7 \u03d5 \u2212 f (cid:107)2 \u221e  2\u03c32 + 2 sup(cid:107)u(cid:107)1  (cid:17)\u22121  (cid:16)  E  (cid:20)(cid:119) (cid:119) (cid:119)f \u2212 (cid:98)fT  (cid:21)  (cid:119) 2 (cid:119) (cid:119)  L2  (cid:54)  (cid:40)  (cid:107)f \u2212 u \u00b7 \u03d5(cid:107)2  L2 +  4 (cid:107)u(cid:107)0 \u03b7(T + 1)  (cid:32)  \u221a  ln  1 +  (cid:33)(cid:41)  dT (cid:107)u(cid:107)1 (cid:107)u(cid:107)0  inf (cid:54)R\u22122d\u03c4  (cid:107)u(cid:107)1  +  4 dT  d (cid:88)  j=1  (cid:107)\u03d5j(cid:107)2  L2 +  1 \u03b7(T + 1)  .  We can now discuss the two questions left open by Dalalyan and Tsybakov (2011). Despite the similarity of the two bounds, the sparsity oracle inequality stated in Propo- sition 10 above only holds for vectors u within (cid:96)1-balls of finite radii. The authors thus asked in Dalalyan and Tsybakov (2011, Section 4.2) whether it was possible to extend the infimum to the whole Rd space. Our results show that, thanks to data-driven truncation, the answer is positive.  The second open question, which was raised in Dalalyan and Tsybakov (2011, Section 5.1, Remark 6), deals with the prior knowledge of the variance factor \u03c32 of the noise. The latter is indeed required by their algorithm for the choice of the inverse temperature parameter \u03b7. The authors thus asked whether adaptivity to \u03c32 was possible. Our sparsity oracle inequality (22) above provides a positive answer (up to a ln T factor).  Remark 11 Similar adaptivity results hold in the regression model with fixed design; see the full version of this paper (Gerchinovitz, 2011, Section 5.2). The framework of prediction of individual sequences thus seems to o\ufb00er a unifying setting to address tuning issues both in the random and in the fixed design regression models.  Acknowledgments  The author would like to thank Arnak Dalalyan, Gilles Stoltz, and Pascal Massart for their helpful comments and suggestions. The author acknowledges the support of the French Agence Nationale de la Recherche (ANR), under grant PARCIMONIE (http://www.proba. jussieu.fr/ANR/Parcimonie), and of the IST Programme of the European Community, un- der the PASCAL2 Network of Excellence, IST-2007-216886.  References  J.-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann.  Statist., 37(4):1591-1646, 2009.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning  algorithms. J. Comp. Sys. Sci., 64:48-75, 2002. Gerchinovitz  K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Mach. Learn., 43(3):211-246, 2001.  G. Biau, K. Bleakley, L. Gy\u00a8orfi, and G. Ottucs\u00b4ak. Nonparametric sequential prediction of  time series. J. Nonparametr. Stat., 22(3-4):297-317, 2010.  L. Birg\u00b4e and P. Massart. Gaussian model selection. J. Eur. Math. Soc., 3:203-268, 2001.  F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for regression learning.  Technical report, 2004. Available at http://arxiv.org/abs/math/0410214.  F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation and sparsity via (cid:96)1 penal- In Proceedings of the 19th Annual Conference on Learning Theory  ized least squares. (COLT\u201906), pages 379-391, 2006.  F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for Gaussian regression. Ann.  Statist., 35(4):1674-1697, 2007.  O. Catoni. Statistical learning theory and stochastic optimization. Springer, New York,  2004.  Press, 2006.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction  with expert advice. Mach. Learn., 66(2/3):321-352, 2007.  A. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting and sharp oracle in- equalities. In Proceedings of the 20th Annual Conference on Learning Theory (COLT\u201907), pages 97-111, 2007.  A. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp PAC-  Bayesian bounds and sparsity. Mach. Learn., 72(1-2):39-61, 2008.  A. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin In Proceedings of the 22nd Annual Conference on Learning Theory  Monte-Carlo. (COLT\u201909), pages 83-92, 2009.  A. Dalalyan and A. B. Tsybakov. Mirror averaging with sparsity priors. Bernoulli, 2011.  To appear. Available at http://hal.archives-ouvertes.fr/hal-00461580/.  J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT\u201910), pages 14-26, 2010.  Y. Freund, R. E. Schapire, Y. Singer, and M. K. Warmuth. Using and combining predic- tors that specialize. In Proceedings of the 29th annual ACM Symposium on Theory of Computing (STOC\u201997), pages 334-343, 1997.  S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression.  Technical report, 2011. Available at http://arxiv.org/abs/1101.1057. Sparsity Regret Bounds for Individual Sequences  L. Gy\u00a8orfi and G. Ottucs\u00b4ak. Sequential prediction of unbounded stationary time series. IEEE  Trans. Inform. Theory, 53(5):1866-1872, 2007.  L. Gy\u00a8orfi, M. Kohler, A. Krzy\u02d9zak, and H. Walk. A distribution-free theory of nonparametric  regression. Springer Series in Statistics. Springer-Verlag, New York, 2002.  S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex In Advances in Neural Information Processing Systems 21  programming algorithms. (NIPS\u201908), pages 801-808. 2009.  J. Kivinen and M. K. Warmuth. Averaging expert predictions. In Proceedings of the 4th European Conference on Computational Learning Theory (EuroCOLT\u201999), pages 153- 167, 1999.  J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. J. Mach.  Learn. Res., 10:777-801, 2009.  G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of convergence for high- dimensional regression under (cid:96)q-ball sparsity. In Proceedings of the 47th annual Allerton conference on communication, control, and computing (Allerton\u201909), pages 251-257, 2009.  M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. J. Mach.  Learn. Res., 9:759-813, 2008.  S. Shalev-Shwartz and A. Tewari. Stochastic methods for (cid:96)1-regularized loss minimiza- tion. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML\u201909), pages 929-936, 2009.  V. Vovk. Competitive on-line statistics. Internat. Statist. Rev., 69:213-248, 2001.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.  J. Mach. Learn. Res., 11:2543-2596, 2010."}