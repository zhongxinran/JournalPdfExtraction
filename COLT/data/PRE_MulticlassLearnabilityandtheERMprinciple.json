{"1": "P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2):235-256, 2002.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed  bandit problem. SICOMP: SIAM Journal on Computing, 32, 2003.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50: 74-86, 1995.  S. Ben-David, D. Pal, , and S. Shalev-Shwartz. Agnostic online learning. In COLT, 2009.  A. Beygelzimer, J. Langford, and P. Ravikumar. Multiclass classification with filter trees.  Preprint, June, 2007.  CoRR, 2009.  Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments.  M. Collins. Discriminative training methods for hidden Markov models: Theory and ex- periments with perceptron algorithms. In Conference on Empirical Methods in Natural Language Processing, 2002.  224   Daniely Sabato Ben-David Shalev-Shwartz  Conjecture 26 There exists a constant C such that, for every hypothesis class H \u2286 Y X ,  mr  H((cid:15), \u03b4) \u2264 C  (cid:32)  (cid:15) ) + ln( 1 dN (H) ln( 1 \u03b4 ) (cid:15)  (cid:33)  In light of theorem 7 and the fact that there are cases where dG \u2265 log2(|Y| \u2212 1)dN , in order to prove the conjecture we will have to find a learning algorithm that is not just an arbitrary ERM learner. So far, all the general upper bounds that we are aware of are valid for any ERM learner. Understanding how to select among ERM learners is fundamental as it teaches us what is the correct way to learn. We suspect that such an understanding might lead to improved bounds in the binary case as well. We hope that our examples from section 2.4 and our result for symmetric classes will prove to be the first steps in the search for the best ERM.  Another direction is the study of learnability conditions for additional hypotheses classes. Section 5 shows that some well known multiclass constructions have surprisingly similar sample complexity properties. It is of practical significance and theoretical interest to study learnability conditions for other constructions, and especially to develop a fuller understanding of the relationship between di\ufb00erent constructions, in a manner that could guide an informed choice of a hypothesis class.  Sivan Sabato is supported by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities.  Acknowledgments  References  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2):235-256, 2002.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed  bandit problem. SICOMP: SIAM Journal on Computing, 32, 2003.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50: 74-86, 1995.  S. Ben-David, D. Pal, , and S. Shalev-Shwartz. Agnostic online learning. In COLT, 2009.  A. Beygelzimer, J. Langford, and P. Ravikumar. Multiclass classification with filter trees.  Preprint, June, 2007.  CoRR, 2009.  Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments.  M. Collins. Discriminative training methods for hidden Markov models: Theory and ex- periments with perceptron algorithms. In Conference on Empirical Methods in Natural Language Processing, 2002. Multiclass Learnability and the ERM principle  K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems.  Journal of Machine Learning Research, 3:951-991, 2003.  R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. Wiley, 1973.  Michael Fink, Shai Shalev-Shwartz, Yoram Singer, and Shimon Ullman. Online multi- class learning by interclass hypothesis sharing. In International Conference on Machine Learning, 2006.  J. Fox. Applied Regression Analysis, Linear Models, and Related Methods. SAGE Publica-  tions, 1997.  Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, August 1997.  T.J. Hastie and R.J. Tibshirani. Generalized additive models. Chapman & Hall, 1995.  S.M. Kakade, S. Shalev-Shwartz, and A. Tewari. E\ufb03cient bandit algorithms for online  multiclass prediction. In International Conference on Machine Learning, 2008.  N. Littlestone. Learning when irrelevant attributes abound. In FOCS, pages 68-77, October  1987.  B. K. Natarajan. On learning sets and functions. Mach. Learn., 4:67-97, 1989.  R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predic-  tions. Machine Learning, 37(3):1-40, 1999.  S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.  V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.  V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995."}