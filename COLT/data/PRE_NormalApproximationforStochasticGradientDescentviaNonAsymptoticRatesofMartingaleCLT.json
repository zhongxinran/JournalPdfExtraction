{"1": "Andreas Anastasiou. Assessing the multivariate normal approximation of the maximum likelihood estimator from high-dimensional, heterogeneous data. Electronic Journal of Statistics, 12:3794- 3828, 2018.  Andrew D Barbour. Stein\u2019s method for diffusion approximations. Probability theory and related  fields, 84(3):297-322, 1990.  Vidmantas Bentkus. On the dependence of the berry-esseen bound on dimension. Journal of  Statistical Planning and Inference, 113(2):385-402, 2003.  Vidmantas Bentkus. A lyapunov-type bound in Rd. Theory of Probability & Its Applications, 49  (2):311-323, 2005.  Erwin Bolthausen. Exact convergence rates in some martingale central limit theorems. The Annals  of Probability, 10(3):672-688, 1982.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in  high dimensions. The Annals of Probability, 45(4):2309-2352, 2017.  Yuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-  gales. Springer Science & Business Media, 2012.  Kai Lai Chung. On a stochastic approximation method. The Annals of Mathematical Statistics,  pages 463-483, 1954.  ton University Press, 1946.  Harald Cram\u00b4er. Mathematical methods of statistics. Princeton Mathematical Series, vol. 9. Prince-  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  John Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. Arxiv Preprint, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 3052-3060, 2015.  Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with dis- cretized diffusions. In Advances in Neural Information Processing Systems 31, pages 9694-9703, 2018.  10   NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  References  Andreas Anastasiou. Assessing the multivariate normal approximation of the maximum likelihood estimator from high-dimensional, heterogeneous data. Electronic Journal of Statistics, 12:3794- 3828, 2018.  Andrew D Barbour. Stein\u2019s method for diffusion approximations. Probability theory and related  fields, 84(3):297-322, 1990.  Vidmantas Bentkus. On the dependence of the berry-esseen bound on dimension. Journal of  Statistical Planning and Inference, 113(2):385-402, 2003.  Vidmantas Bentkus. A lyapunov-type bound in Rd. Theory of Probability & Its Applications, 49  (2):311-323, 2005.  Erwin Bolthausen. Exact convergence rates in some martingale central limit theorems. The Annals  of Probability, 10(3):672-688, 1982.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in  high dimensions. The Annals of Probability, 45(4):2309-2352, 2017.  Yuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-  gales. Springer Science & Business Media, 2012.  Kai Lai Chung. On a stochastic approximation method. The Annals of Mathematical Statistics,  pages 463-483, 1954.  ton University Press, 1946.  Harald Cram\u00b4er. Mathematical methods of statistics. Princeton Mathematical Series, vol. 9. Prince-  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  John Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. Arxiv Preprint, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 3052-3060, 2015.  Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with dis- cretized diffusions. In Advances in Neural Information Processing Systems 31, pages 9694-9703, 2018. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  Vaclav Fabian. On asymptotic normality in stochastic approximation. The Annals of Mathematical  Statistics, 39(4):1327-1332, 1968.  Xiequan Fan. Exact rates of convergence in some martingale central limit theorems. Journal of  Mathematical Analysis and Applications, 469(2):1028-1044, 2019.  Yixin Fang, Jinfeng Xu, and Lei Yang. Online bootstrap confidence intervals for the stochastic gradient descent estimator. The Journal of Machine Learning Research, 19(1):3053-3073, 2018.  Robert E Gaunt. Rates of convergence in normal approximation under moment conditions via new bounds on solutions of the stein equation. Journal of Theoretical Probability, 29(1):231-247, 2016.  Charles Geyer. Asymptotics of maximum likelihood without the lln or clt or sample size going to infinity. In Advances in Modern Statistical Theory and Applications: A Festschrift in honor of Morris L. Eaton, pages 1-24. Institute of Mathematical Statistics, 2013.  Alexander Goldenshluger, Anatoli Juditsky, and Arkadi Nemirovski. Hypothesis testing by convex  optimization. Electronic journal of statistics, 9(2):1645-1712, 2015.  Larry Goldstein and Yosef Rinott. Multivariate normal approximations by stein\u2019s method and size  bias couplings. Journal of Applied Probability, 33(1):1-17, 1996.  Jack Gorham, Andrew B Duncan, Sebastian J Vollmer, and Lester Mackey. Measuring sample  quality with diffusions. arXiv preprint arXiv:1611.06972, 2016.  Peter Hall and Christopher C Heyde. Martingale limit theory and its application. Academic press,  2014.  2019.  Anatoli Juditsky and Arkadi Nemirovski. Lectures on statistical inferences via convex optimization.  Arun Kumar Kuchibhotla. Deterministic inequalities for smooth m-estimators. arXiv preprint  arXiv:1809.05172, 2018.  Guanghui Lan, Arkadi Nemirovski, and Alexander Shapiro. Validation analysis of mirror descent  stochastic approximation method. Mathematical programming, 134(2):425-458, 2012.  Tianyang Li, Anastasios Kyrillidis, Liu Liu, and Constantine Caramanis. Approximate newton- based statistical inference using only stochastic gradients. arXiv preprint arXiv:1805.08920, 2018.  Robert Liptser and Albert Nikolaevich Shiryayev. Theory of martingales, volume 49. Springer  Science & Business Media, 2012.  James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna- tional Conference on International Conference on Machine Learning, pages 735-742, 2010.  Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451-459, 2011. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  Jean-Christophe Mourrat. On the rate of convergence in the martingale central limit theorem.  Bernoulli, 19(2):633-645, 2013.  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574- 1609, 2009.  Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  Martin Rai\u02c7c. A multivariate clt for decomposable random vectors with finite second moments.  Journal of Theoretical Probability, 17(3):573-603, 2004.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for  strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.  Gesine Reinert and Adrian R\u00a8ollin. Multivariate normal approximation with steins method of ex- changeable pairs under a general linearity condition. The Annals of Probability, 37(6):2150- 2173, 2009.  Yosef Rinott and V Rotar. Some bounds on the rate of convergence in the clt for martingales. i.  Theory of Probability & Its Applications, 43(4):604-619, 1999.  Adrian R\u00a8ollin. On quantitative bounds in the mean martingale central limit theorem. Statistics &  Probability Letters, 138:171-176, 2018.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical  report, Cornell University Operations Research and Industrial Engineering, 1988.  Jerome Sacks. Asymptotic distribution of stochastic approximation procedures. The Annals of  Mathematical Statistics, 29(2):373-405, 1958.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on stochastic program-  ming: modeling and theory. SIAM, 2009.  Charles Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:  1-164, 1986.  Weijie Su and Yuancheng Zhu. Statistical inference for online learning and stochastic approximation  via hierarchical incremental gradient descent. arXiv preprint arXiv:1802.04876, 2018.  Panos Toulis and Edoardo M Airoldi. Asymptotic and finite-sample properties of estimators based  on stochastic gradients. The Annals of Statistics, 45(4):1694-1727, 2017.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT"}