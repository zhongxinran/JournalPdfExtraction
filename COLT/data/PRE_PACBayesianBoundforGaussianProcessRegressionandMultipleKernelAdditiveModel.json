{"1": "R. A. Adams and J. J. Fournier. Sobolev Spaces. Academic Press, New York, 2003. second edition.  P. Alquier and G. Biau. Sparse single-index model. Technical report, 2011. arXiv:1101.3229.  P. Alquier and K. Lounici. PAC-Bayesian bounds for sparse regression estimation with exponential  weights. Electronic Journal of Statistics, 5:127-145, 2011.  C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 2010 Workshop on New  Directions in Multiple Kernel Learning, Whistler, 2010.  F. R. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO  algorithm. In the 21st International Conference on Machine Learning, pages 41-48, 2004.  C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, Boston, 1988.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.  The Annals of Statistics, 37(4):1705-1732, 2009.  8.12   SUZUKI  5. Conclusion and Discussion  In this paper, we developed a PAC-Bayesian bound for Gaussian process model and generalized it to sparse additive model. Important notion was that the optimal rate is achieved without any conditions on the design. Interpolations of spaces gave a nice characterization of the convergence rate on the misspecified situation. We have observed that Gaussian processes with scale mixture adaptively achieve the minimax optimal rate on both correctly-specified and misspecified situations.  We bounded the empirical L2-norm (cid:107)\u00b7(cid:107)n in this paper. However, the evaluation of the population L2(PX ) = (cid:82) f (X)2dPX , between the estimator and the true function is also of interest L2-norm, (cid:107)f (cid:107)2 from the view point of generalization error. For the analysis of the population L2-norm, the L\u221e- norm in the metric entropy condition (6) and the definition (4) of \u03c6(m) could be replaced with the f \u2217 m population L2-morm (cid:107)\u00b7(cid:107)L2(PX ). To bound the population L2-norm, we would need to impose some smoothness condition on the prior (see Theorem 2 and the following discussions in van der Vaart and van Zanten (2011)). Our future work includes developing a PAC-Bayesian bound that is also applicable to the population L2-norm.  Another interesting topic is to compare Bayesian-MKL with a model selection type method that minimizes a penalized risk like the BIC estimator. Rigollet and Tsybakov (2011a) discussed benefits of a model averaging type estimator comparing to a BIC type estimator in a finite dimensional linear model. It is interesting to argue an analogous thing also in a nonparametric regression situation.  We would like to thank Alexandre B. Tsybakov and Pierre Alquier for their suggestive advices. TS was partially supported by MEXT Kakenhi 22700289, Global COE Program \u201cThe Research and Training Center for New Development in Mathematics,\u201d and the Aihara Project, the FIRST program from JSPS, initiated by CSTP.  Acknowledgments  References  R. A. Adams and J. J. Fournier. Sobolev Spaces. Academic Press, New York, 2003. second edition.  P. Alquier and G. Biau. Sparse single-index model. Technical report, 2011. arXiv:1101.3229.  P. Alquier and K. Lounici. PAC-Bayesian bounds for sparse regression estimation with exponential  weights. Electronic Journal of Statistics, 5:127-145, 2011.  C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 2010 Workshop on New  Directions in Multiple Kernel Learning, Whistler, 2010.  F. R. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO  algorithm. In the 21st International Conference on Machine Learning, pages 41-48, 2004.  C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, Boston, 1988.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.  The Annals of Statistics, 37(4):1705-1732, 2009.  8.12   PAC-BAYESIAN MKL  H. J. Brascamp and E. H. Lieb. On extensions of the brunn-minkowski and pr\u00b4ekopa-leindler the- orem, including inequalities for log concave functions, and with an application to the diffusion equation. Journal of Functional Analysis, 22(4):366-389, 1976.  I. Castillo. Lower bounds for posterior rates with Gaussian process priors. Electronic Journal of  Statistics, 2:1281-1299, 2008.  O. Catoni. Statistical Learning Theory and Stochastic Optimization. Lecture Notes in Mathematics.  Springer, 2004. Saint-Flour Summer School on Probability Theory 2001.  A. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting sharp PAC-Bayesian  bounds and sparsity. Machine Learning, 72:39-61, 2008.  A. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin Monte-  Carlo. Journal of Computer and System Sciences, in press, 2011.  D. E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential Operators. Cam-  bridge University Press, Cambridge, 1996.  M. N. Gibbs. Bayesian Gaussian Processes for Regression and Classification. PhD thesis, Univer-  sity of Cambridge, 1997.  1995.  P. J. Green. Reversible jump markov chain monte carlo computation. Biometrika, 82(4):711-732,  L. Gross. Measurable functions on Hilbert space. Transactions of the American Mathematical  Society, 105(3):372-390, 1962.  G. Harg\u00b4e. A particular case of correlation inequality for the gaussian measure. The Annals of  Probability, 27(4):1939-1951, 1999.  G. Harg\u00b4e. A convex/log-concave correlation inequality for gaussian measure and an application to  abstract wiener spaces. Probability Theory and Related Fields, 130(3):415-440, 2004.  T. Hastie and R. Tibshirani. Generalized additive models. Chapman & Hall Ltd, 1999.  V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics, 38(6):  3660-3695, 2010.  J. Kuelbs and W. V. Li. Metric entropy and the small ball problem for gaussian measures. Journal  of Functional Analysis, 116(1):133-157, 1993.  G. Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix  with semi-definite programming. Journal of Machine Learning Research, 5:27-72, 2004.  W. V. Li and Q.-M. Shao. Gaussian processes: inequalities, small ball probabilities and applications.  Stochastic Processes: Theory and Methods, 19:533-597, 2001.  J.-M. Marin and C. Robert. Bayesian Core: A Practical Approach to Computational Bayesian  Statistics. Springer, 2007.  8.13   SUZUKI  D. McAllester. Some PAC-Bayesian theorems. In the Anual Conference on Computational Learning  Theory, pages 230-234, 1998.  ing Theory, pages 164-170, 1999.  D. McAllester. PAC-Bayesian model averaging. In the Anual Conference on Computational Learn-  L. Meier, S. van de Geer, and P. B\u00a8uhlmann. High-dimensional additive modeling. The Annals of  Statistics, 37(6B):3779-3821, 2009.  G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Journal of Machine Learning Research, 13:389-427, 2012.  C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.  P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the Royal  Statistical Society: Series B, 71(5):1009-1030, 2009.  P. Rigollet and A. B. Tsybakov. Exponential screening and optimal rates of sparse estimation. The  Annals of Statistics, 39(2):731-771, 2011a.  P. Rigollet and A. B. Tsybakov. Sparse estimation by exponential weighting. Technical report,  2011b. arXiv:1108.5116.  (2), 2004.  M. Seeger. Gaussian processes for machine learning. International Journal of Neural Systems, 14  I. Steinwart. Support Vector Machines. Springer, 2008.  I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In  Proceedings of the Annual Conference on Learning Theory, pages 79-93, 2009.  T. Suzuki and M. Sugiyama. Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness. In JMLR Workshop and Conference Proceedings 22, pages 1152-1183, 2012. Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS2012).  R. Tomioka and T. Suzuki. Regularization strategies and empirical bayesian learning for mkl. In  NIPS 2010 Workshop: New Directions in Multiple Kernel Learning, Whistler, 2010.  A. W. van der Vaart and J. H. van Zanten. Rates of contraction of posterior distributions based on  Gaussian process priors. The Annals of Statistics, 36(3):1435-1463, 2008a.  A. W. van der Vaart and J. H. van Zanten. Reproducing kernel Hilbert spaces of Gaussian priors. Pushing the Limits of Contemporary Statistics: Contributions in Honor of Jayanta K. Ghosh, 3: 200-222, 2008b. IMS Collections.  A. W. van der Vaart and J. H. van Zanten. Adaptive Bayesian estimation using a Gaussian random  field with inverse Gamma bandwidth. The Annals of Statistics, 37(5B):2655-2675, 2009.  A. W. van der Vaart and J. H. van Zanten.  Information rates of nonparametric gaussian process  methods. Journal of Machine Learning Research, 12:2095-2119, 2011.  8.14   PAC-BAYESIAN MKL  A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-  tions to Statistics. Springer, New York, 1996.  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal  of The Royal Statistical Society Series B, 68(1):49-67, 2006."}