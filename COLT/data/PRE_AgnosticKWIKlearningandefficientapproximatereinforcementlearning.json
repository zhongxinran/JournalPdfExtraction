{"1": "Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforce-  ment learning. In NIPS, pages 89-96, 2008.  Ronen I. Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning in certain classes of stochastic games. Artificial Intelligence, 121(1-2):31-47, 2000.  Ronen I. Brafman and Moshe Tennenholtz. R-MAX - a general polynomial time algorithm  for near-optimal reinforcement learning. In IJCAI, pages 953-958, 2001.  Kai Lai Chung. A course in probability theory. Academic Press, 3 edition, 2001.  Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for  e\ufb03cient reinforcement learning. In ICML, pages 240-247, 2008.  Joseph L. Doob. Stochastic processes. John Wiley & Sons, 1953.  Claude-Nicolas Fiechter. E\ufb03cient reinforcement learning. In COLT, pages 88-97, 1994.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):  100-118, 1975.  Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis,  University College London, 2003.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial  time. Machine Learning, 49(2-3):209-232, 2002.  770   Szita Szepesv\u00b4ari  bound on the expected immediate rewards (condition (a)) holds by assumption, just like the measurability condition (b) and that the action selected at time t is sampled from \u03c0t(\u00b7|st) (condition (c)). The condition on the accuracy of the planner (condition (d)) was assumed as a condition of this theorem. The accuracy condition (e) holds with emodel = rD + (cid:15) on G by the choice of G, while the optimism condition (f) is met because of the use of the optimistic wrapper (in fact, because of this wrapper, Q\u03c0 (s, a) = Vmax holds for any \u02c6Mt (s, a) (cid:54)\u2208 Kt). Also, condition (g) is met because the learn method of MDPLearner is not called when (st, at) \u2208 Kt, hence in that case gt+1 = gt and thus \u03c0t+1 = \u03c0t. Finally, on G, B = B(\u03b4) bounds the number of times (st, at) (cid:54)\u2208 Kt happens. Therefore, by the conclusion of Theorem 5.1, with probability at least 1 \u2212 2\u03b4, the number of 5emodel/(1 \u2212 \u03b3) + eplan- \u221a mistakes is bounded by 2Vmax(1\u2212\u03b3)L  , where L =  2B + 3)  B + (  (cid:113)  (cid:27)  (cid:26)  (cid:1)  log (cid:0) L \u03b4  (cid:1) + 6 log (cid:0) L \u03b4  emodel  max(1, (cid:100)(1 \u2212 \u03b3)\u22121 log(Vmax(1 \u2212 \u03b3)/emodel)(cid:101)). Plugging in the value emodel = rD + (cid:15) gives the final bound.  References  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforce-  ment learning. In NIPS, pages 89-96, 2008.  Ronen I. Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning in certain classes of stochastic games. Artificial Intelligence, 121(1-2):31-47, 2000.  Ronen I. Brafman and Moshe Tennenholtz. R-MAX - a general polynomial time algorithm  for near-optimal reinforcement learning. In IJCAI, pages 953-958, 2001.  Kai Lai Chung. A course in probability theory. Academic Press, 3 edition, 2001.  Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for  e\ufb03cient reinforcement learning. In ICML, pages 240-247, 2008.  Joseph L. Doob. Stochastic processes. John Wiley & Sons, 1953.  Claude-Nicolas Fiechter. E\ufb03cient reinforcement learning. In COLT, pages 88-97, 1994.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):  100-118, 1975.  Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis,  University College London, 2003.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial  time. Machine Learning, 49(2-3):209-232, 2002. Agnostic KWIK learning  Lihong Li. A Unifying Framework for Computational Reinforcement Learning Theory. PhD thesis, Department of Computer Science, Rutgers University, New Brunswick, NJ, USA, 2009.  Lihong Li and Michael L. Littman. Reducing reinforcement learning to kwik online regres-  sion. Annals of Mathematics and Artificial Intelligence, 58(3-4):217-237, 2010.  Lihong Li, Michael L. Littman, and Thomas J. Walsh. Knows what it knows: A framework  for self-aware learning. In ICML, pages 568-575, 2008.  Lihong Li, Michael L. Littman, Thomas J. Walsh, and Alexander L. Strehl. Knows what it knows: A framework for self-aware learning. Machine Learning, 82(3):399-443, 2011a.  Lihong Li, Michael L. Littman, Thomas J. Walsh, and Alexander L. Strehl. Knows what it  knows: a framework for self-aware learning. Machine learning, 82:399-443, 2011b.  Martin .L. Puterman. Markov Decision Processes \u2014 Discrete Stochastic Dynamic Program-  ming. John Wiley & Sons, Inc., New York, NY, 2005.  Alexander L. Strehl. Model-based reinforcement learning in factored-state MDPs. In IEEE  ADPRL, pages 103-110, 2007.  Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval In Proceedings of the 22nd international conference on Machine learning,  estimation. pages 856-863, 2005.  Alexander L. Strehl and Michael L. Littman. Online linear regression and its application  to model-based reinforcement learning. In NIPS, 2007.  Alexander L. Strehl, Lihong Li, and Michael L. Littman. Incremental model-based learners  with formal learning-time guarantees. In UAI, pages 485-493, 2006.  Alexander L. Strehl, Carlos Diuk, and Michael L. Littman. E\ufb03cient structure learning in  factored-state MDPs. In AAAI, pages 645-650, 2007.  Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite MDPs: PAC analysis. The Journal of Machine Learning Research, 10:2413-2444, 2009.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT  Press, Cambridge, 1998.  Csaba Szepesv\u00b4ari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial  Intelligence and Machine Learning. Morgan & Claypool Publishers, 2010.  Istv\u00b4an Szita and Andr\u00b4as L\u02ddorincz. The many faces of optimism: a unifying approach. In  ICML, pages 1048-1055, 2008.  Istv\u00b4an Szita and Csaba Szepesv\u00b4ari. Model-based reinforcement learning with nearly tight  exploration complexity bounds. In ICML, pages 1031-1038, June 2010. Szita Szepesv\u00b4ari  Thomas J. Walsh, Sergiu Goschin, and Michael Littman. Integrating sample-based planning and model-based reinforcement learning. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, 2010."}