{"1": "David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Advances  in neural information processing systems, pages 494-501, 1989.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected Langevin Monte Carlo. In Advances in Neural Information Processing Systems, pages 1243-1251, 2015.  Ruobing Chen. Stochastic derivative-free optimization of noisy functions. Ph.D. Thesis, Lehigh  University, 2015.  Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region  method and random models. Mathematical Programming, pages 1-41, 2015.  K Andrew Cliffe, Mike B Giles, Robert Scheichl, and Aretha L Teckentrup. Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients. Computing and Visualization in Science, 14(1):3, 2011.  Patrick R Conrad, Andrew D Davis, Youssef M Marzouk, Natesh S Pillai, and Aaron Smith. Parallel local approximation MCMC for expensive models. SIAM/ASA Journal on Uncertainty Quantifi- cation, 6(1):339-373, 2018.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic In International Conference on Machine Learning, pages 1833-1841,  non-convex problems. 2016.  Mohamed Jebalia and Anne Auger. On multiplicative noise models for stochastic search. In Inter- national Conference on Parallel Problem Solving from Nature, pages 52-61. Springer, 2008.  Mohamed Jebalia, Anne Auger, and Nikolaus Hansen. Log-linear convergence and divergence of  the scale-invariant (1 + 1)-ES in noisy environments. Algorithmica, 59(3):425-460, 2011.  Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.  science, 220(4598):671-680, 1983.  13   CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES  References  David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Advances  in neural information processing systems, pages 494-501, 1989.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected Langevin Monte Carlo. In Advances in Neural Information Processing Systems, pages 1243-1251, 2015.  Ruobing Chen. Stochastic derivative-free optimization of noisy functions. Ph.D. Thesis, Lehigh  University, 2015.  Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region  method and random models. Mathematical Programming, pages 1-41, 2015.  K Andrew Cliffe, Mike B Giles, Robert Scheichl, and Aretha L Teckentrup. Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients. Computing and Visualization in Science, 14(1):3, 2011.  Patrick R Conrad, Andrew D Davis, Youssef M Marzouk, Natesh S Pillai, and Aaron Smith. Parallel local approximation MCMC for expensive models. SIAM/ASA Journal on Uncertainty Quantifi- cation, 6(1):339-373, 2018.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic In International Conference on Machine Learning, pages 1833-1841,  non-convex problems. 2016.  Mohamed Jebalia and Anne Auger. On multiplicative noise models for stochastic search. In Inter- national Conference on Parallel Problem Solving from Nature, pages 52-61. Springer, 2008.  Mohamed Jebalia, Anne Auger, and Nikolaus Hansen. Log-linear convergence and divergence of  the scale-invariant (1 + 1)-ES in noisy environments. Algorithmica, 59(3):425-460, 2011.  Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.  science, 220(4598):671-680, 1983. CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. To appear in Proceedings of STOC 2018, arXiv preprint arXiv:1710.06261, 2017.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic In Conference on Learning Theory,  gradient Langevin dynamics: a nonasymptotic analysis. pages 1674-1703, 2017.  Andrej Risteski and Yuanzhi Li. Algorithms and matching lower bounds for approximately-convex optimization. In Advances in Neural Information Processing Systems, pages 4745-4753, 2016.  Mark Rudelson and Roman Vershynin. Hanson-Wright inequality and sub-Gaussian concentration.  Electron. Commun. Probab, 18(82):1-9, 2013.  Yaron Singer and Jan Vondr\u00b4ak. Information-theoretic lower bounds for convex optimization with erroneous oracles. In Advances in Neural Information Processing Systems, pages 3204-3212, 2015.  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681- 688, 2011.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient  Langevin dynamics. In Conference on Learning Theory, pages 1980-2022, 2017. CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES"}