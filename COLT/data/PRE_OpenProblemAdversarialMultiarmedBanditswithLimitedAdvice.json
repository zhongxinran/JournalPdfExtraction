{"1": "Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal of Computing, 32(1), 2002.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings on the International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5, 2012.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. E\ufb03cient learning with partially ob-  served attributes. Journal of Machine Learning Research, 2011.  Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to In Proceedings of the International Conference on  personalized news article recommendation. World Wide Web (WWW), 2010.  Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori.  with limited advice and multiarmed bandits with paid observations. http://arxiv.org/abs/1304.3708, 2013.  Prediction Technical report,  3   Adversarial Multiarmed Bandits with Limited Advice  (cid:16)(cid:112)(N \u2212 M + 1)KT ln N  (cid:17)  O regret bound, which has a bit disappointing dependence on M (even though it would provide a continuous interpolation between asking one expert and asking all experts).  The problem of prediction with limited advice is related to label-e\ufb03cient prediction (Cesa-Bianchi and Lugosi, 2006; Audibert and Bubeck, 2010). In label-e\ufb03cient prediction all experts are queried on a subset of game rounds and in prediction with limited advice a subset of experts is queried on all game rounds. We note that the formulation of pre- diction with limited advice is substantially di\ufb00erent from learning with partially observed attributes (Cesa-Bianchi et al., 2011), but possibly some tools could be transferred between the settings.  Acknowledgements  This research was supported by an Australian Research Council Australian Laureate Fel- lowship (FL110100281). We gratefully acknowledge the support of the NSF through grant CCF-1115788.  References  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal of Computing, 32(1), 2002.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings on the International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5, 2012.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. E\ufb03cient learning with partially ob-  served attributes. Journal of Machine Learning Research, 2011.  Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to In Proceedings of the International Conference on  personalized news article recommendation. World Wide Web (WWW), 2010.  Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori.  with limited advice and multiarmed bandits with paid observations. http://arxiv.org/abs/1304.3708, 2013.  Prediction Technical report, \u2200h: \u02c6L0(h) = 0. for i = 1, 2, ... do  Let  Seldin Crammer Bartlett  qt(h) =  e\u2212\u03b7t \u02c6Lt\u22121(h) h e\u2212\u03b7t \u02c6Lt\u22121(h)  .  (cid:80)  Sample M experts without replacement, such that the probability of sampling expert h is \u02dcqt(h). (\u02dcqi(h) is specified in the analysis of the algorithm.) Let 1h t = 1 if expert h was sampled and 1h  t = 0 otherwise.  Get advice vectors \u03beh  t for the experts sampled.  Let  Draw action At according to pt and receive reward Rt \u2208 [0, 1].  pt(a) =  (cid:80) qt(h) \u02dcqt(h) \u03beh h (cid:80) qt(h) h \u02dcqt(h)  t (a)1h t 1h t  .  \u2200a : La  t =  1 \u2212 Rt pt(a)  1{At=a}.  \u2200h : Y h  t = \u03beh  t (At)LAt t  1 \u02dcqt(h)  1h t .  \u2200h : \u02c6Lt(h) =  t (cid:88)  i=1  Y h i .  end  Analysis  Algorithm 1: A general algorithm for multiarmed bandits with limited advice."}