{"1": "We study learning of initial intervals in the prediction model. We show that for each distribution \\emphD over the domain, there is an algorithm \\emphA_D, whose probability of a mistake in round m is at most \\emph(\u00bd + o(1))/m. We also show that the best possible bound that can be achieved in the case in which the same algorithm \\emphA must be applied for all distributions \\emphD is at least (^1\u2044_\u221a\\emphe - o(1))^1\u2044_\\emphm > (^3\u2044_5-o(1))^1\u2044_\\emphm. Informally, \u201cknowing\u201d the distribution \\emphD enables an algorithm to reduce its error rate by a constant factor strictly greater than 1. As advocated by Ben-David et al. (2008), knowledge of \\emphD can be viewed as an idealized proxy for a large number of unlabeled examples."}