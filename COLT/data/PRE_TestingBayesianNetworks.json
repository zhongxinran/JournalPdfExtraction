{"1": "P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.  J. Mach. Learn. Res., 7:1743-1788, 2006.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing. Journal of  Machine Learning Research - Proceedings Track, 19:47-68, 2011.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-linear  time. CoRR, abs/1506.00671, 2015b.  M. Adamaszek, A. Czumaj, and C. Sohler. Testing monotone continuous distributions on high- dimensional real cubes. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, pages 56-65, 2010.  A. Almudevar. A hypothesis test for equality of bayesian network models. EURASIP J. Bioinformatics  and Systems Biology, 2010, 2010.  NIPS, pages 1061-1069, 2012.  A. Anandkumar, D. J. Hsu, F. Huang, and S. Kakade. Learning mixtures of tree graphical models. In  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  Z. Bai and H. Saranadasa. Effect of high dimension: by an example of a two sample problem. Statist.  Sinica,, 6:311-329, 1996.  T. Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001.  62   CANONNE DIAKONIKOLAS KANE STEWART  But because of the d-way probability similarities, the terms Pr[ PSi = x ] and Pr[ QSi = x ] terms are very close, within an additive poly((cid:15)/n).  (Here we use the extra assumption that P and Q use the same ordering.) Denote by Ti the parents of i under the topology of Q. Then H(Qi | Q1,...,i\u22121 = y) depends only on the values of the coordinates in Ti. Thus the last part of the sum is a sum over z of Pr[ QTi = z ] H(Qi | QTi = z) and Pr[ PTi = z ] H(Qi | QTi = z), which are also close by a similar argument. Thus, EP1,...,Pi\u22121[D(Qi | PSi(cid:107)Qi | P1, . . . , Pi\u22121)] = I (Qi; Q1, . . . , Qi\u22121 | QSi)+poly(cid:0) (cid:15) n  (cid:1) = poly(cid:0) (cid:15) n  (cid:1).  This implies that P, Q are close in KL divergence, and therefore in L1.  The second part of the theorem, asserting the existence of a closeness testing algorithm with optimal dependence on d, will be very similar. Indeed, by the proof above it suffices to check that the restrictions of P and Q to any set of (d + 3)-coordinates are poly((cid:15)/n)-close. Using known results Chan et al. (2014c), this can be done for any specific collection of d + 3 coordinates with N samples in poly(N ) time, and high probability of success, implying the second part of the theorem.  References  P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.  J. Mach. Learn. Res., 7:1743-1788, 2006.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing. Journal of  Machine Learning Research - Proceedings Track, 19:47-68, 2011.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-linear  time. CoRR, abs/1506.00671, 2015b.  M. Adamaszek, A. Czumaj, and C. Sohler. Testing monotone continuous distributions on high- dimensional real cubes. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, pages 56-65, 2010.  A. Almudevar. A hypothesis test for equality of bayesian network models. EURASIP J. Bioinformatics  and Systems Biology, 2010, 2010.  NIPS, pages 1061-1069, 2012.  A. Anandkumar, D. J. Hsu, F. Huang, and S. Kakade. Learning mixtures of tree graphical models. In  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  Z. Bai and H. Saranadasa. Effect of high dimension: by an example of a two sample problem. Statist.  Sinica,, 6:311-329, 1996.  T. Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001. TESTING BAYESIAN NETWORKS  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000. URL citeseer.ist.psu.edu/batu00testing.html.  T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White. Testing random variables for independence and identity. In Proc. 42nd IEEE Symposium on Foundations of Computer Science, pages 442-451, 2001.  T. Batu, S. Dasgupta, R. Kumar, and R. Rubinfeld. The complexity of approximating entropy. In  ACM Symposium on Theory of Computing, pages 678-687, 2002.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  2010.  A. Bhattacharyya, E. Fischer, R. Rubinfeld, and P. Valiant. Testing monotonicity of distributions  over general partial orders. In ICS, pages 239-252, 2011.  P. J. Bickel. A distribution free version of the smirnov two sample test in the p-variate case. Ann.  Math. Statist., 40(1):1-23, 02 1969.  T. T. Cai and Z. Ma. Optimal hypothesis testing for high dimensional covariance matrices. Bernoulli,  19(5B):2359-2388, 2013.  C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016, pages 25:1-25:14, 2016.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S. Chan, I. Diakonikolas, P. Valiant, and G. Valiant. Optimal algorithms for testing closeness of  discrete distributions. In SODA, pages 1193-1203, 2014c.  S. X. Chen and Y. L. Qin. A two-sample test for high-dimensional data with applications to gene-set  testing. Ann. Statist., 38(2):808-835, 04 2010.  J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning bayesian networks from data: An  information-theory based approach. Artificial Intelligence, 137(1):43 - 90, 2002. CANONNE DIAKONIKOLAS KANE STEWART  D. M. Chickering. Learning equivalence classes of bayesian-network structures. J. Mach. Learn.  Res., 2:445-498, 2002.  C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE  Trans. Inf. Theor., 14(3):462-467, 1968.  M. Cryan, L. Goldberg, and P. Goldberg. Evolutionary trees can be learned in polynomial time in the  two state general Markov model. SIAM Journal on Computing, 31(2):375-397, 2002.  R. Daly, Q. Shen, and S. Aitken. Learning bayesian networks: approaches and issues. The Knowledge  Engineering Review, 26:99-157, 2011. ISSN 1469-8005.  S. Dasgupta. The sample complexity of learning fixed-structure bayesian networks. Machine  Learning, 29(2-3):165-180, 1997.  S. Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual Symposium on  Foundations of Computer Science, pages 634-644, 1999.  C. Daskalakis and Q. Pan. Square hellinger subadditivity for bayesian networks and its applications  to identity testing. CoRR, abs/1612.03164, 2016.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of  Independent Integer Random Variables. In FOCS, pages 217-226, 2013a.  C. Daskalakis, I. Diakonikolas, R. Servedio, G. Valiant, and P. Valiant. Testing k-modal distributions:  Optimal algorithms via reductions. In SODA, pages 1833-1852, 2013b.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and its  applications. In Proceedings of STOC\u201916, 2016a.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing ising models. CoRR,  abs/1612.03147, 2016b. URL http://arxiv.org/abs/1612.03147.  A. De, I. Diakonikolas, and R. Servedio. Learning from satisfying assignments. In Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 478-497, 2015.  I. Diakonikolas and D. M. Kane. A new approach for testing properties of discrete distributions. In  FOCS, pages 685-694, 2016. Full version available at abs/1601.05557.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Testing Identity of Structured Distributions. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, 2015a. TESTING BAYESIAN NETWORKS  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Optimal algorithms and lower bounds for testing closeness of structured distributions. In 56th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2015, 2015b.  I. Diakonikolas, D. M. Kane, and A. Stewart. Nearly optimal learning and sparse covers for sums of  independent integer random variables. CoRR, abs/1505.00662, 2015c.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Collision-based testers are optimal for uniformity and closeness. Electronic Colloquium on Computational Complexity (ECCC), 23:178, 2016a.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distributions and its algorithmic applications. In Proceedings of STOC\u201916, 2016b. Available at https://arxiv.org/abs/1511.03592.  I. Diakonikolas, D. M. Kane, and A. Stewart. Robust learning of fixed-structure bayesian networks.  CoRR, abs/1606.07384, 2016c.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In Proceedings of the  12th Annual COLT, pages 183-192, 1999.  N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence, UAI\u201996, pages 274-282, 1996.  N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine Learning, 29  (2):131-163, 1997.  N. Friedman, M. Linial, and I. Nachman. Using bayesian networks to analyze expression data.  Journal of Computational Biology, 7:601-620, 2000.  O. Goldreich and D. Ron. On testing expansion in bounded-degree graphs. Technical Report  TR00-020, Electronic Colloquium in Computational Complexity, 2000.  M. Gonen, P. H. Westfall, and W. O. Johnson. Bayesian multiple testing for two-sample multivariate  endpoints. Biometrics, 59(1):76-82, 2003. ISSN 1541-0420.  M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, pages 753-760, 2015.  H. Hotelling. The generalization of student\u2019s ratio. Ann. Math. Statist., 2(3):360-378, 08 1931.  P. Indyk, R. Levi, and R. Rubinfeld. Approximating and Testing k-Histogram Distributions in  Sub-linear Time. In PODS, pages 15-22, 2012.  A. Javanmard and A. Montanari. Confidence intervals and hypothesis testing for high-dimensional  regression. J. Mach. Learn. Res., 15(1):2869-2909, 2014.  F. V. Jensen and T. D. Nielsen. Bayesian Networks and Decision Graphs. Springer Publishing  Company, Incorporated, 2nd edition, 2007. CANONNE DIAKONIKOLAS KANE STEWART  D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive  Computation and Machine Learning. The MIT Press, 2009.  E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Texts in Statistics. Springer,  R. Levi, D. Ron, and R. Rubinfeld. Testing properties of collections of distributions. In ICS, pages  2005.  179-194, 2011.  R. Y. Liu and K. Singh. A quality index based on data depth and multivariate rank tests. Journal of the American Statistical Association, 88(421):252-260, 1993. ISSN 01621459. URL http: //www.jstor.org/stable/2290720.  P. L. Loh and M. J. Wainwright. Structure estimation for discrete graphical models: Generalized  covariance matrices and their inverses. In NIPS, pages 2096-2104, 2012.  D. Margaritis. Learning Bayesian Network Model Structure From Data. PhD thesis, CMU, 2003.  A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS,  pages 93-102, 2010.  E. Mossel and S. Roch. Learning nonsingular phylogenies and Hidden Markov Models. In To appear  in Proceedings of the 37th Annual Symposium on Theory of Computing (STOC), 2005.  R. E. Neapolitan. Learning Bayesian Networks. Prentice-Hall, Inc., 2003.  J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypothe- ses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933. doi: 10.1098/rsta. 1933.0009. URL http://rsta.royalsocietypublishing.org/content/231/ 694-706/289.short.  H.-T. Nguyen, P. Leray, and G. Ramstein. Multiple Hypothesis Testing and Quasi Essential Graph for Comparing Two Sets of Bayesian Networks, pages 176-185. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.  L. Paninski. A coincidence-based test for uniformity given very sparsely-sampled discrete data.  IEEE Transactions on Information Theory, 54:4750-4755, 2008.  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan  Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.  D. Pollard.  Asymptopia.  Asymptopia, 2003. Manuscript.  http://www.stat.yale.edu/\u02dcpollard/Books/  Y. Polyanskiy and Y. Wu. Strong data-processing inequalities for channels and Bayesian networks.  ArXiv e-prints, August 2015.  Y. Rahmatallah, F. Emmert-Streib, and G. Glazko. Gene Sets Net Correlations Analysis (GSNCA): a multivariate differential coexpression test for gene sets. Bioinformatics, 30(3):360-368, 2014. TESTING BAYESIAN NETWORKS  A. Ramdas, D. Isenberg, A. Singh, and L. A. Wasserman. Minimax lower bounds for linear independence testing. In IEEE International Symposium on Information Theory, ISIT 2016, pages 965-969, 2016.  R. Rubinfeld. Taming big probability distributions. XRDS, 19(1):24-28, 2012.  R. Rubinfeld and R. Servedio. Testing monotone high-dimensional distributions. In Proc. 37th  Annual ACM Symposium on Theory of Computing (STOC), pages 147-156, 2005.  N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical  models in high dimensions. IEEE Trans. Information Theory, 58(7):4117-4134, 2012.  D. M. Sobel and T. Kushnir. Interventions do not solely benefit causal learning: Being told what to do results in worse learning than doing it yourself. In Proceedings of the twenty-fifth annual meeting of the Cognitive Science Society, pages 1100-1105, 2003.  M. S. Srivastava and M. Du. A test for the mean vector with fewer observations than the dimension.  Journal of Multivariate Analysis, 99(3):386 - 402, 2008.  N. St\u00a8adler and S. Mukherjee. Multivariate gene-set testing based on graphical models. Biostatistics,  16(1):47-59, 2015.  G. Valiant and P. Valiant. Estimating the unseen: an n/ log(n)-sample estimator for entropy and  support size, shown optimal via new CLTs. In STOC, pages 685-694, 2011.  G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity testing. In  FOCS, 2014.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 113-122, 2002.  T. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence, UAI \u201990, pages 255-270, 1991.  M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.  Found. Trends Mach. Learn., 1(1-2):1-305, 2008.  M. J. Wainwright, P. Ravikumar, and J. D. Lafferty. High-dimensional graphical model selection  using (cid:96)1-regularized logistic regression. In NIPS, pages 1465-1472, 2006.  L. Weiss. Two-sample tests for multivariate distributions. Ann. Math. Statist., 31(1):159-164, 03  1960.  W. Yin, S. Garimalla, A. Moreno, M. R. Galinski, and M. P. Styczynski. A tree-like bayesian structure learning algorithm for small-sample datasets from complex biological model systems. BMC Systems Biology, 9(1):1-18, 2015. CANONNE DIAKONIKOLAS KANE STEWART  APPENDIX"}