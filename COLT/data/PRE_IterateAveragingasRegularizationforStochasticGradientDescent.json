{"1": "Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning Journal, 43(3):211\u2013246, 2001.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26 (NIPS), pages 773\u2013781, 2013.  Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.  Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.  Ernesto De Vito, Andrea Caponnetto, and Lorenzo Rosasco. Model selection for regularized least- squares algorithm in learning theory. Foundations of Computational Mathematics, 5(1):59\u201385, 2005.  Alexandre D\u00b4efossez and Francis Bach. Constant step size least-mean-square: Bias-variance trade- offs and optimal sampling distributions. In Proceedings of the 25th International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), pages 205\u2013213, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363\u20131399, 2016.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1\u201351, 2017.  Amir-massoud Farahmand. Regularization in reinforcement learning. PhD thesis, University of  Alberta, 2011.  12   ITERATE AVERAGING AS REGULARIZATION FOR SGD  The extremely technical proof of this theorem is presented in"}