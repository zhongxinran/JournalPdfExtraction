{"1": "We give the first dimension-efficient algorithms for learning Rectified Linear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto \\mathsf{max}(0, \u00a0\\mathbf{w} \u22c5\\mathbf{x})$ with $\\mathbf{w} \u2208\\mathbb{S}^n-1$. Our algorithm works in the challenging Reliable Agnostic learning model of Kalai, Kanade and Mansour (2012) where the learner is given access to a distribution $\\mathcal{D}$ on labeled examples but the labeling may be arbitrary.  We construct a hypothesis that simultaneously minimizes the false-positive rate and the loss on inputs given positive labels by $\\mathcal{D}$, for any convex, bounded, and Lipschitz loss function. The algorithm runs in polynomial-time (in $n$) with respect to \\em any distribution on $\\mathbb{S}^n-1$ (the unit sphere in $n$ dimensions) and for any error parameter $\u03b5= \u03a9(1 / \\log n)$ (this yields a PTAS for a question raised by F. Bach on the complexity of maximizing ReLUs).  These results are in contrast to known efficient algorithms for reliably learning linear threshold functions, where $\u03b5$ must be $\u03a9(1)$ and strong assumptions are required on the marginal distribution. We can compose our results to obtain the first set of efficient algorithms for learning constant-depth networks of ReLU with fixed polynomial-dependence in the dimension. For depth-2 networks of sigmoids, we obtain the first algorithms that have a polynomial dependency in \\em all parameters. Our techniques combine kernel methods and polynomial approximations with a \u201cdual-loss\u201d approach to convex programming. As a byproduct we obtain a number of applications including the first set of efficient algorithms for \u201cconvex piecewise-linear fitting\u201d and the first efficient algorithms for noisy polynomial reconstruction of low-weight polynomials on the unit sphere."}