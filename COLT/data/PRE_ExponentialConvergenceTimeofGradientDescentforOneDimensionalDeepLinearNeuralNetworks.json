{"1": "Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient  descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.  Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit  acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.  Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization effi- ciently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520-529, 2018.  Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous  models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,  2016.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv  preprint arXiv:1810.02032, 2018.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information  Processing Systems, pages 586-594, 2016.  Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are  global. In International Conference on Machine Learning, pages 2908-2913, 2018.  Haihao Lu and Kenji Kawaguchi.  Depth creates no bad local minima.  arXiv preprint  arXiv:1702.08580, 2017.  Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4785-4795, 2017.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-  ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.  12   EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS  This research was partially supported by an ERC grant 754705.  Acknowledgments  References  Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient  descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.  Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit  acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.  Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization effi- ciently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520-529, 2018.  Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous  models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,  2016.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv  preprint arXiv:1810.02032, 2018.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information  Processing Systems, pages 586-594, 2016.  Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are  global. In International Conference on Machine Learning, pages 2908-2913, 2018.  Haihao Lu and Kenji Kawaguchi.  Depth creates no bad local minima.  arXiv preprint  arXiv:1702.08580, 2017.  Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4785-4795, 2017.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-  ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS  J Michael Steele. The Cauchy-Schwarz master class: an introduction to the art of mathematical  inequalities. Cambridge University Press, 2004. EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS"}