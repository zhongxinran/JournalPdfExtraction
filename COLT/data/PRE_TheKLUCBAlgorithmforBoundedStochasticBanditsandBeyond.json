{"1": "R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed  bandit problem. Advances in Applied Probability, 27(4):1054-1078, 1995.  J-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitor-  ing. Journal of Machine Learning Resaerch, 11:2785-2836, 2010.  J-Y. Audibert, R. Munos, and Cs. Szepesv\u00b4ari. Exploration-exploitation trade-o\ufb00 using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19), 2009.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2):235-256, 2002.  A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for Markov decision pro-  cesses. Mathematics of Operations Research, pages 222-255, 1997.  C.J. Clopper and E.S. Pearson. The use of confidence of fiducial limits illustration in the  case of the binomial. Biometrika, 26:404-413, 1934.  E. Even-Dar, S. Mannor, and Y. Mansour. PAC bounds for multi-armed bandit and Markov decision processes. In Conf. Comput. Learning Theory (Sydney, Australia, 2002), volume 2375 of Lecture Notes in Comput. Sci., pages 255-270. Springer, Berlin, 2002.  S. Filippi. Optimistic strategies in Reinforcement Learning (in French). PhD thesis, Telecom  ParisTech, 2010. URL http://tel.archives-ouvertes.fr/tel-00551401/.  S. Filippi, O. Capp\u00b4e, and A. Garivier. Optimism in reinforcement learning and Kullback- Leibler divergence. In Allerton Conf. on Communication, Control, and Computing, Mon- ticello, US, 2010.  J.C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statis-  tical Society, Series B, 41(2):148-177, 1979.  J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support models. In T. Kalai and M. Mohri, editors, Conf. Comput. Learning Theory, Haifa, Israel, 2010.  T.L. Lai and H. Robbins. Asymptotically e\ufb03cient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985.  373   KL-UCB: bounded bandits, and beyond  7. Conclusion  The self-normalized deviation bound of Theorems 10 and 11, together with the new analysis In presented in Section 6, allowed us to design and analyze improved UCB algorithms. this approach, only an upper-bound of the deviations (more precisely, of the exponential moments) of the rewards is required, which makes it possible to obtain versatile policies satisfying interesting regret bounds for large classes of reward distributions. The resulting index policies are simple, fast, and very e\ufb03cient in practice, even for small time horizons.  References  R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed  bandit problem. Advances in Applied Probability, 27(4):1054-1078, 1995.  J-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitor-  ing. Journal of Machine Learning Resaerch, 11:2785-2836, 2010.  J-Y. Audibert, R. Munos, and Cs. Szepesv\u00b4ari. Exploration-exploitation trade-o\ufb00 using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19), 2009.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2):235-256, 2002.  A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for Markov decision pro-  cesses. Mathematics of Operations Research, pages 222-255, 1997.  C.J. Clopper and E.S. Pearson. The use of confidence of fiducial limits illustration in the  case of the binomial. Biometrika, 26:404-413, 1934.  E. Even-Dar, S. Mannor, and Y. Mansour. PAC bounds for multi-armed bandit and Markov decision processes. In Conf. Comput. Learning Theory (Sydney, Australia, 2002), volume 2375 of Lecture Notes in Comput. Sci., pages 255-270. Springer, Berlin, 2002.  S. Filippi. Optimistic strategies in Reinforcement Learning (in French). PhD thesis, Telecom  ParisTech, 2010. URL http://tel.archives-ouvertes.fr/tel-00551401/.  S. Filippi, O. Capp\u00b4e, and A. Garivier. Optimism in reinforcement learning and Kullback- Leibler divergence. In Allerton Conf. on Communication, Control, and Computing, Mon- ticello, US, 2010.  J.C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statis-  tical Society, Series B, 41(2):148-177, 1979.  J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support models. In T. Kalai and M. Mohri, editors, Conf. Comput. Learning Theory, Haifa, Israel, 2010.  T.L. Lai and H. Robbins. Asymptotically e\ufb03cient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985. Garivier Capp\u00b4e  O-A. Maillard, R. Munos, and G. Stoltz. A finite-time analysis of multi-armed bandits problems with kullback-leibler divergences. In Conf. Comput. Learning Theory, Budapest, Hungary, 2011.  P. Massart. Concentration inequalities and model selection, volume 1896 of Lecture Notes in Mathematics. Springer, Berlin, 2007. Lectures from the 33rd Summer School on Probability Theory held in Saint-Flour, July 6-23, 2003."}