{"1": "Mikhail Alekhnovich, Sanjeev Arora, and Iannis Tourlakis. Towards strong nonapproximability results in the lov\u00b4asz-schrijver hierarchy. In Proceedings of the thirty-seventh annual ACM sym- posium on Theory of computing, pages 294-303. ACM, 2005.  12   DANIELY SHALEV-SHWARTZ  consider the formula h : {\u00b11}2Kn \u2192 {0, 1} defined by h(x) = \u2228T x \u2208 Xn,K we have,  t=1 \u2227Rt  r=1 \u2227n  i=1xjt,r,\u03c8ibt,r,i. For  h(g(x)) = 1 \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], i \u2208 [n], gjt,r,\u03c8ibt,r,i(x) = 1  \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], i \u2208 [n], x(jt,r) (cid:54)= (\u2212\u03c8ibt,r, i) \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], x1(jt,r) (cid:54)= \u2212\u03c8x2(jt,r)bt,r \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], x1(jt,r)\u03c8x2(jt,r) = bt,r \u21d0\u21d2 h\u03c8(x) = x(\u03c8) = P (x1(1)\u03c8x2(1), . . . , x1(K)\u03c8x2(K)) = 1 .  (cid:3)  3.5. Wrapping up - concluding theorem 3 We are now ready to conclude the proof. Let q : N \u2192 N be any function such that q(n) = \u03c9(log(n)). W.l.o.g., we assume that q(n) = O (cid:0)log2(n)(cid:1). By theorem 12 it is enough to show that for every d, it is hard to distinguish samples that are realizable by DNFq(n) and (cid:0)nd, 1/4(cid:1)-scattered samples.  By assumption 1, there is K such that CSPrand  nd+2(SATK) is hard. Denote q(cid:48)(n) = q(2Kn). By  nd+1(TK,q(cid:48)(n)) is hard. By lemma 15, the problem CSPrand  lemma 13, the problem CSPrand is hard. Now, since \u00acTK,q(cid:48)(n) can be realized by a DNF formula with q(cid:48)(n) clauses, by lemma 16, the problem CSPrand nd+1(TK,q(cid:48)(n), \u00acTK,q(cid:48)(n)) can be reduced to a problem of distinguishing samples 8 nd+1, 1/4(cid:1)- that are realizable by a DNF formula with 2Kn variables and q(cid:48)(n) clauses, from (cid:0) 1 scattered samples. Changing variables (i.e., replacing 2Kn with n(cid:48)), we conclude that it is hard to distinguish samples that are realizable by DNFq(n) from 1 -scattered samples, which are in particular (cid:0)nd, 1/4(cid:1)-scattered. The theorem follows.  8(2K)d\u22121 nd+1, 1/4  (cid:16)  (cid:17)  nd+1(TK,q(cid:48)(n), \u00acTK,q(cid:48)(n))  Basic learning problems that we are unable to resolve even under the random K-SAT (or K-XOR) assumption include decision trees and intersections of a constantly many halfspaces. (It is worth noting that no known algorithm can learn even intersections of 2 halfspaces).  Amit Daniely was a recipient of the Google Europe Fellowship in Learning Theory, and this research was supported in part by this Google Fellowship. Shai Shalev-Shwartz is supported by the Israeli Science Foundation grant number 590-10. We thank Uri Feige, Guy Kindler and Nati Linial for valuable discussions.  4. Open questions  Acknowledgments  References  Mikhail Alekhnovich, Sanjeev Arora, and Iannis Tourlakis. Towards strong nonapproximability results in the lov\u00b4asz-schrijver hierarchy. In Proceedings of the thirty-seventh annual ACM sym- posium on Theory of computing, pages 294-303. ACM, 2005. COMPLEXITY THEORETIC LIMITATIONS ON LEARNING DNF\u2019S  Sarah Allen, Ryan O\u2019Donnell, and David Witmer. How to refute a random csp? In FOCS, 2015.  Dana Angluin and Michael Kharitonov. When won\u2019t membership queries help? In STOC, pages  444-454, May 1991.  B. Applebaum, B. Barak, and D. Xiao. On basing lower-bounds for learning on worst-case assump- tions. In Foundations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 211-220. IEEE, 2008.  Paul Beame and Toniann Pitassi. Simplified and improved resolution lower bounds. In Foundations of Computer Science, 1996. Proceedings., 37th Annual Symposium on, pages 274-282. IEEE, 1996.  Paul Beame, Richard Karp, Toniann Pitassi, and Michael Saks. On the complexity of unsatisfiability proofs for random k-cnf formulas. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 561-571. ACM, 1998.  Eli Ben-Sasson. Expansion in proof complexity. In Hebrew University. Citeseer, 2001.  Eli Ben-Sasson and Avi Wigderson. Short proofs are narrowresolution made simple. In Proceedings of the thirty-first annual ACM symposium on Theory of computing, pages 517-526. ACM, 1999.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.  Joshua Buresh-Oppenheim, Nicola Galesi, Shlomo Hoory, Avner Magen, and Toniann Pitassi. Rank bounds and integrality gaps for cutting planes procedures. In Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 318-327. IEEE, 2003.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech- niques, pages 310-321. Springer, 2004.  Amin Coja-Oghlan, Colin Cooper, and Alan Frieze. An efficient sparse regularity concept. SIAM  Journal on Discrete Mathematics, 23(4):2000-2034, 2010.  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC, 2016.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning  halfspaces over sparse vectors. In NIPS, 2013.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. In STOC, 2014.  Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem-proving.  Communications of the ACM, 5(7):394-397, 1962.  Uriel Feige. Relations between average case complexity and approximation complexity. In Pro- ceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 534-543. ACM, 2002. DANIELY SHALEV-SHWARTZ  Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. In Au-  tomata, languages and programming, pages 519-530. Springer, 2004.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, 2006.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. In STOC, 2015.  Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to construct random functions. Journal  of the Association for Computing Machinery, 33(4):792-807, October 1986.  Dima Grigoriev. Linear lower bound on degrees of positivstellensatz calculus proofs for the parity.  Theoretical Computer Science, 259(1):613-622, 2001.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proceedings of  the 47th Foundations of Computer Science (FOCS), 2006.  Armin Haken. The intractability of resolution. Theoretical Computer Science, 39:297-308, 1985.  Johan H\u02daastad. Some optimal inapproximability results. Journal of the ACM (JACM), 48(4):798-  859, 2001.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In Pro-  ceedings of the 46th Foundations of Computer Science (FOCS), 2005.  Michael Kearns and Leslie G. Valiant. Cryptographic limitations on learning Boolean formulae and  finite automata. In STOC, pages 433-444, May 1989.  Michael Kharitonov. Cryptographic hardness of distribution-specific learning. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 372-381. ACM, 1993.  Subhash Khot and Rishi Saket. Hardness of minimizing and learning dnf expressions. In Foun- dations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 231-240. IEEE, 2008.  Subhash Khot and Rishi Saket. On the hardness of learning intersections of two halfspaces. Journal  of Computer and System Sciences, 77(1):129-141, 2011.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In  RANDOM, 2014.  Adam R Klivans and Rocco Servedio. Learning dnf in time 2O(n1/3). In Proceedings of the thirty-  third annual ACM symposium on Theory of computing, pages 258-265. ACM, 2001.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of  halfspaces. In FOCS, 2006.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. Efficient agnostic learning of neural IEEE Transactions on Information Theory, 42(6):2118-2132,  networks with bounded fan-in. 1996. COMPLEXITY THEORETIC LIMITATIONS ON LEARNING DNF\u2019S  N. Linial and Z. Luria. Chernoff\u2019s Inequality - A very elementary proof. Arxiv preprint  arXiv:1403.7739 v2, 2014.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier transform, and  learnability. In FOCS, pages 574-579, October 1989.  Yishay Mansour. An o(n log log n) learning algorithm for dnf under the uniform distribution. Jour-  nal of Computer and System Sciences, 50(3):543-550, 1995.  L. Pitt and L.G. Valiant. Computational limitations on learning from examples. Journal of the  Association for Computing Machinery, 35(4):965-984, October 1988.  Leonard Pitt and Manfred K. Warmuth. Prediction preserving reducibility. Technical Report UCSC- CRL-88-26, University of California Santa Cruz, Computer Research Laboratory, November 1988.  Prasad Raghavendra. Optimal algorithms and inapproximability results for every csp?  In Pro- ceedings of the 40th annual ACM symposium on Theory of computing, pages 245-254. ACM, 2008.  R.E. Schapire. The strength of weak learnability. In FOCS, pages 28-33, October 1989.  Grant Schoenebeck. Linear level lasserre lower bounds for certain k-csps. In Foundations of Com- puter Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 593-602. IEEE, 2008.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, Novem-  ber 1984."}