{"1": "Dana Angluin and Philip D. Laird. Learning from noisy examples. Machine Learning, 2 (4):343\u2013370, 1987. doi: 10.1007/BF00116829. URL http://dx.doi.org/10.1007/ BF00116829.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians.  In Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing, STOC \u201901, pages 247\u2013257, New York, NY, USA, 2001. ACM. ISBN 1-58113-349-9. doi: 10.1145/380752.380808. URL http://doi.acm.org/10.1145/380752.380808.  Avrim Blum and Tom M. Mitchell. Combining labeled and unlabeled data with co-training.  In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, Madison, Wisconsin, USA, July 24-26, 1998., pages 92\u2013100, 1998. doi: 10.1145/279943.279962. URL http://doi.acm.org/10.1145/279943.279962.  Sanjoy Dasgupta. Learning mixtures of gaussians.  In 40th Annual Symposium on Foundations of Computer Science, FOCS \u201999, 17-18 October, 1999, New York, NY, USA, pages 634\u2013644, 1999. doi: 10.1109/SFFCS.1999.814639. URL http://dx.doi.org/10.1109/SFFCS. 1999.814639.  14   KEARNS WU  Lemma 22 Suppose the class P satisfies the parametric mixture learning assumption (Assump- tion 18), the class C is CN learnable, and mixture distribution over Y is \u03b3-healthy for some \u03b3 > 0. Then there exists an algorithm L that given \u03b5, \u03b4 and \u03b3 as inputs and sample access from Gen, halts in time bounded by poly(1/\u03b5, 1/\u03b4, 1/\u03b3, n, k), and with probability at least 1 \u2212 \u03b4, outputs a list of probability models T that contains some \u02c6T with err( \u02c6T ) \u2264 \u03b5.  Finally, to wrap up and prove Theorem 19, we also need to handle the case where healthy mixture condition in Theorem 16 does not hold. We will again appeal to the robust distribution learner in Lemma 17 to learn the distributions directly, and construct hypothesis models based on the output distributions. To guarantee that the output hypothesis model is accurate, we will again use the maximum likelihood method to select the model with the minimum empirical log-loss (formal proof deferred to the"}