{"1": "P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal  of the American Statistical Association, 101:138-156, 2006.  K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classifiers. In Proceedings of  International Conference on Machine Learning, pages 57-64, 2000.  D. Bertsekas, A. Nedic, and A. Ozdaglar. Convex Analysis and Optimization. Athena Scientific,  Belmont, MA, 2003.  D. J. Crisp and C. J. C. Burges. A geometric interpretation of \u03bd-SVM classifiers. In S. A. Solla, T. K. Leen, and K.-R. M\u00a8uller, editors, Advances in Neural Information Processing Systems 12, pages 244-250. MIT Press, 2000.  F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American  Mathematical Society, 39:1-49, 2002.  G. R.G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach  to classification. Journal of Machine Learning Research, 3:555-582, 2003.  M. E. Mavroforakis and S. Theodoridis. A geometric approach to support vector machine (svm)  classification. IEEE Transactions on Neural Networks, 17(3):671-682, 2006.  J. S. Nath and C. Bhattacharyya. Maximum margin classifiers with specified false positive and false negative error rates. In C. Apte, B. Liu, S. Parthasarathy, and D. Skillicorn, editors, Proceedings of the seventh SIAM International Conference on Data mining, pages 35-46. SIAM, 2007.  R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, USA, 1970.  29.12   KANAMORI TAKEDA SUZUKI  the derivative \u00af(cid:96)(cid:48)(z) is Lipschitz continuous and the Lipschitz constant is equal to 1/(2w), we have \u00af(cid:96)(cid:48)(\u03c1 + z) \u2212 \u00af(cid:96)(cid:48)(\u03c1 \u2212 z) \u2264 z/w. Therefore, the inequality sup\u03c1\u2265\u2212\u00af(cid:96)(0)/2 \u03be(z, \u03c1) \u2264 sup\u03c1\u2265\u2212\u00af(cid:96)(0)/2 = z/w 4w\u22121 z \u2264 2z holds. We see that \u00af\u03be(z) = 2z satisfies the sufficient conditions in Lemma \u00af(cid:96)(cid:48)(\u2212\u00af(cid:96)(0)/2) 7. Hence, the loss function corresponding to the revised uncertainty set satisfies the conditions for statistical consistency, though the original uncertainty set with the estimation error does not correspond to the empirical mean of a loss function.  \u2264 4w  z/w \u00af(cid:96)(cid:48)(\u03c1)  5. Conclusion  In this paper, we studied the relation between the loss function approach and the minimum distance approach in binary classification problems. We proposed the learning algorithm based on the revised minimum distance problem, and proved the statistical consistency. In our proof, the hinge loss used in \u03bd-SVM is excluded, though Steinwart (2003) proved the statistical consistency of \u03bd-SVM with a nice choice of the regularization parameter. A future work is to relax the assumptions of our theoretical result so as to include the hinge loss function and other popular loss functions such as the logistic loss. Also, it is important to derive the convergence rate of the proposed learning method. Developing an optimization algorithm is needed for practical data analysis by the statistical learning with uncertainty sets.  References  P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal  of the American Statistical Association, 101:138-156, 2006.  K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classifiers. In Proceedings of  International Conference on Machine Learning, pages 57-64, 2000.  D. Bertsekas, A. Nedic, and A. Ozdaglar. Convex Analysis and Optimization. Athena Scientific,  Belmont, MA, 2003.  D. J. Crisp and C. J. C. Burges. A geometric interpretation of \u03bd-SVM classifiers. In S. A. Solla, T. K. Leen, and K.-R. M\u00a8uller, editors, Advances in Neural Information Processing Systems 12, pages 244-250. MIT Press, 2000.  F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American  Mathematical Society, 39:1-49, 2002.  G. R.G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach  to classification. Journal of Machine Learning Research, 3:555-582, 2003.  M. E. Mavroforakis and S. Theodoridis. A geometric approach to support vector machine (svm)  classification. IEEE Transactions on Neural Networks, 17(3):671-682, 2006.  J. S. Nath and C. Bhattacharyya. Maximum margin classifiers with specified false positive and false negative error rates. In C. Apte, B. Liu, S. Parthasarathy, and D. Skillicorn, editors, Proceedings of the seventh SIAM International Conference on Data mining, pages 35-46. SIAM, 2007.  R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, USA, 1970.  29.12   CONJUGATE PROPERTY IN CLASSIFICATION  B. Sch\u00a8olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization,  Optimization, and Beyond. MIT Press, 2001.  B. Sch\u00a8olkopf, A. Smola, R. Williamson, and P. Bartlett. New support vector algorithms. Neural  Computation, 12(5):1207-1245, 2000.  I. Steinwart. On the optimal parameter choice for v-support vector machines. IEEE Trans. Pattern  Anal. Mach. Intell., 25(10):1274-1284, 2003.  I. Steinwart. Consistency of support vector machines and other regularized kernel classifiers. IEEE  Transactions on Information Theory, 51(1):128-142, 2005.  I. Steinwart and A. Christmann. Support Vector Machines. Springer Publishing Company, Incorpo-  rated, 1st edition, 2008.  V. Vapnik. Statistical Learning Theory. Wiley, 1998.  D.-X. Zhou. The covering number in learning theory. Journal of Complexity, 18(3):739-767, 2002."}