{"1": "384-414, 2010.  Katy S. Azoury and Manfred K. Warmuth. Relative Loss Bounds for On-Line Density Estimation  with the Exponential Family of Distributions. Machine Learning, 43:211-246, 2001.  Francis Bach. Self-concordant analysis for logistic regression. Electric Journal of Statisitics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15:595627, 2014.  Stephan Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Lester R. Ford, Jr. Solution of a ranking problem from binary comparisons. American Mathematical  Monthly, 64(8):28\u2014-33, 1957.  Yoav Freund. Predicting a binary sequence almost as well as the optimal biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT \u201996), pages 89-98, 1996.  12   MATSUMOTO HATANO TAKIMOTO  Bayesian Approach For online density estimation problems, Bayesian approaches have been quite effective for obtaining O(ln T ) regret bounds (e.g., Freund (1996); Azoury and Warmuth (2001); Kotlowski et al. (2010)). The typical Bayesian approach assumes a prior distribution over parameters and predicts the average of parameters w.r.t. the posterior distribution. Unfortunately, unlike the exponential family, it is not straightforward to obtain a regret bound for Bradly-Terry and logistic models using this approach. Note that FTRL has a natural Bayesian interpretation that, FTRL predicts the maximum a posterior estimate of parameters w.r.t. the posterior distribution, where the prior distribution is defined as the likelihood of the virtual 2\u03bb matches between each two players. It is an interesting open question whether a Bayesian approach achieves O(ln T ) regret bound for these models.  6. Conclusion  We considered the online density estimation problem for Bradley-Terry models. We derived match- ing upper and lower bounds of the radius, analyzed FTRL with virtual match regularization, and showed better regret bounds than standard algorithms.  There are some interesting open questions to explore. An obvious open problem is to obtain better upper/lower regret bounds for Bradley-Terry models. It might be possible to design Bayesian- based algorithms for Bradley-Terry models as well. In addition, minimax analyses for Bradley-Terry models would be an interesting approach.  Acknowledgments  We thank anonymous reviewers and Hiroe Inoue for many helpful comments. Hatano is grate- ful to the supports from JSPS KAKENHI Grant Number 25330261 and CORE Project Grant of Microsoft Research Asia. Takimoto is grateful to the supports from JSPS KAKENHI Grant Num- ber 15H02667. In addition, both authors acknowledge the support from MEXT KAKENHI Grant Number 24106010 (the ELC project).  References  384-414, 2010.  Katy S. Azoury and Manfred K. Warmuth. Relative Loss Bounds for On-Line Density Estimation  with the Exponential Family of Distributions. Machine Learning, 43:211-246, 2001.  Francis Bach. Self-concordant analysis for logistic regression. Electric Journal of Statisitics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15:595627, 2014.  Stephan Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Lester R. Ford, Jr. Solution of a ranking problem from binary comparisons. American Mathematical  Monthly, 64(8):28\u2014-33, 1957.  Yoav Freund. Predicting a binary sequence almost as well as the optimal biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT \u201996), pages 89-98, 1996. ONLINE DENSITY ESTIMATION OF BRADLEY-TERRY MODELS  Elad Hazan. The convex optimization approach to regret minimization. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning, chapter 10, pages 287-304. MIT Press, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Levy. Logistic Regression: Tight Bounds for Stochastic and Online Optimization. In Proceedings of the 27th Conference on Learning Theory (COLT\u201914), pages 197-209, 2014.  David R. Hunter. MM algorithms for generalized Bradley-Terry models. Annals of Statistics, 32  (1):384-406, 2004.  Wojciech Kotlowski, Peter Gr\u00a8unwald, and Steven de Rooij. Following the Flattened Leader. In Proceedings of 23th Annual Conference on Learning Theory (COLT 2010), pages 106-118, 2010.  John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall, 1995.  H. Brendan McMahan and Matthew J. Streeter. Open Problem: Better Bounds for Online Logistic Regression. In Proceeding of the 25th Annual Conference on Learning Theory (COLT 2012), pages 44.1-44.3, 2012.  Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends  in Machine Learning, 4(2):107-194, 2011.  Yu. M. Shtar\u2019kov. Universal Sequential Coding of Single Messages. Problems of Information  Transmission, 23(3):175-186, 1987.  Eiji Takimoto and Manfred K. Warmuth. The Minimax Strategy for Gaussian Density Estimation. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory (COLT 2000), pages 100-106, 2000.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2003), pages 928-936, 2003."}