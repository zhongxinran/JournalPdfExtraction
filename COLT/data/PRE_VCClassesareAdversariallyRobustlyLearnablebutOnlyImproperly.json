{"1": "University Press, 1999.  M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  P. Assouad. Densit\u00b4e et dimension. Annales de l\u2019Institut Fourier (Grenoble), 33(3):233-282, 1983.  Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust  learning. arXiv preprint arXiv:1810.02180, 2018.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50:74-86, 1995.  Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Gior- In Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pages 387- 402. Springer, 2013.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik- Chervonenkis dimension. Journal of the Association for Computing Machinery, 36(4):929-965, 1989.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of evasion  adversaries. arXiv preprint arXiv:1806.01471, 2018.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. Journal of Machine Learning Research, 16:2377-2404, 2015.  O. David, S. Moran, and A. Yehudayoff. Supervised learning through the lens of compression. In  Advances in Neural Information Processing Systems 29, pages 2784-2792, 2016.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimen-  sion. Machine Learning, 21(3):269-304, 1995.  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial  examples. arXiv preprint arXiv:1412.6572, 2014.  T. Graepel, R. Herbrich, and J. Shawe-Taylor. PAC-Bayesian compression bounds on the prediction  error of learning algorithms for classification. Machine Learning, 59(1-2):55-76, 2005.  S. Hanneke, A. Kontorovich, and M. Sadigurschi. Sample compression for real-valued learners. In  Proceedings of the 30th International Conference on Algorithmic Learning Theory, 2019.  12   ADVERSARIALLY ROBUST LEARNABILITY  References  University Press, 1999.  M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  P. Assouad. Densit\u00b4e et dimension. Annales de l\u2019Institut Fourier (Grenoble), 33(3):233-282, 1983.  Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust  learning. arXiv preprint arXiv:1810.02180, 2018.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50:74-86, 1995.  Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Gior- In Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pages 387- 402. Springer, 2013.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik- Chervonenkis dimension. Journal of the Association for Computing Machinery, 36(4):929-965, 1989.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of evasion  adversaries. arXiv preprint arXiv:1806.01471, 2018.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. Journal of Machine Learning Research, 16:2377-2404, 2015.  O. David, S. Moran, and A. Yehudayoff. Supervised learning through the lens of compression. In  Advances in Neural Information Processing Systems 29, pages 2784-2792, 2016.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimen-  sion. Machine Learning, 21(3):269-304, 1995.  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial  examples. arXiv preprint arXiv:1412.6572, 2014.  T. Graepel, R. Herbrich, and J. Shawe-Taylor. PAC-Bayesian compression bounds on the prediction  error of learning algorithms for classification. Machine Learning, 59(1-2):55-76, 2005.  S. Hanneke, A. Kontorovich, and M. Sadigurschi. Sample compression for real-valued learners. In  Proceedings of the 30th International Conference on Algorithmic Learning Theory, 2019. ADVERSARIALLY ROBUST LEARNABILITY  Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classification via function trans-  formation. arXiv preprint arXiv:1810.09519, 2018.  N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished  manuscript, 1986.  Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.  MIT press, 2018.  63(3):21:1-21:10, 2016.  S. Moran and A. Yehudayoff. Sample compression schemes for VC classes. Journal of the ACM,  B. K. Natarajan. On learning sets and functions. Machine Learning, 4:67-97, 1989.  Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial ex-  amples. arXiv preprint arXiv:1801.09344, 2018a.  Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-  bustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018b.  N. Sauer. On the density of families of sets. Journal of Combinatorial Theory (A), 13(1):145-147,  1972.  Cambridge, MA, 2012.  R. E. Schapire and Y. Freund. Boosting. Adaptive Computation and Machine Learning. MIT Press,  Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad- versarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge university press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In COLT, 2009.  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.  A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer, 1996.  V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York,  1982.  V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their probabilities. Theory of Probability and its Applications, 16(2):264-280, 1971.  V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974. ADVERSARIALLY ROBUST LEARNABILITY  M. Warmuth. Compressing to VC dimension many points. In Proceedings of the 16th Conference  on Learning Theory, 2003.  Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5283-5292, 2018.  Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,  2012.  Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adversarially  robust generalization. arXiv preprint arXiv:1810.11914, 2018."}