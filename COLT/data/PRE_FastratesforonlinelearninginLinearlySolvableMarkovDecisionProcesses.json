{"1": "Y. Abbasi-Yadkori, P. L. Bartlett, X. Chen, and A. Malek. Large-scale Markov decision problems with KL control cost and its application to crowdsourcing. In 32nd International Conference on Machine Learning (ICML) 2015, pages 1053-1062, 2015.  Y. Abbasi-Yadkori and Cs. Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic  systems. In COLT, 2011.  369-377, 2014.  Y. Abbasi-Yadkori, P. Bartlett, and V. Kanade. Tracking adversarial targets. In ICML 2014, pages  Y. Ariki, T. Matsubara, and S. H. Hyon. Latent Kullback-Leibler control for dynamic imitation In 2016 IEEE-RAS 16th International  learning of whole-body behaviors in humanoid robots. Conference on Humanoid Robots (Humanoids), pages 946-951, 2016.  P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In UAI 2009, 2009.  D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, Bel-  mont, MA, 3 edition, 2007.  MA, 1996.  New York, NY, USA, 2006.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. Accepted to the Journal of Machine Learning Research, 2014.  T. Dick, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. Online learning in markov decision processes with chang-  ing cost sequences. In ICML 2014, 2014.  K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable mdps. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 335-342, 2010.  E. Even-Dar, S. M. Kakade, and Y. Mansour. Online Markov decision processes. Mathematics of  Operations Research, 34(3):726-736, 2009.  V. G\u00b4omez, H. J. Kappen, J. Peters, and G. Neumann. Policy search for path integral control. Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases, 8724 LNAI (PART 1):482-497, 2014.  V. G\u00b4omez, S. Thijssen, A. C. Symington, S. Hailes, and H. J. Kappen. Real-time stochastic opti- mal control for multi-agent quadrotor systems. In 26th International Conference on Automated Planning and Scheduling, 2016.  P. Guan, M. Raginsky, and R. M. Willett. Online markov decision processes with kullback-leibler  control cost. Automatic Control, IEEE Transactions on, 59(6):1423-1438, 2014.  13   FAST RATES FOR ONLINE LEARNING IN LMDPS  References  Y. Abbasi-Yadkori, P. L. Bartlett, X. Chen, and A. Malek. Large-scale Markov decision problems with KL control cost and its application to crowdsourcing. In 32nd International Conference on Machine Learning (ICML) 2015, pages 1053-1062, 2015.  Y. Abbasi-Yadkori and Cs. Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic  systems. In COLT, 2011.  369-377, 2014.  Y. Abbasi-Yadkori, P. Bartlett, and V. Kanade. Tracking adversarial targets. In ICML 2014, pages  Y. Ariki, T. Matsubara, and S. H. Hyon. Latent Kullback-Leibler control for dynamic imitation In 2016 IEEE-RAS 16th International  learning of whole-body behaviors in humanoid robots. Conference on Humanoid Robots (Humanoids), pages 946-951, 2016.  P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In UAI 2009, 2009.  D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, Bel-  mont, MA, 3 edition, 2007.  MA, 1996.  New York, NY, USA, 2006.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. Accepted to the Journal of Machine Learning Research, 2014.  T. Dick, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. Online learning in markov decision processes with chang-  ing cost sequences. In ICML 2014, 2014.  K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable mdps. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 335-342, 2010.  E. Even-Dar, S. M. Kakade, and Y. Mansour. Online Markov decision processes. Mathematics of  Operations Research, 34(3):726-736, 2009.  V. G\u00b4omez, H. J. Kappen, J. Peters, and G. Neumann. Policy search for path integral control. Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases, 8724 LNAI (PART 1):482-497, 2014.  V. G\u00b4omez, S. Thijssen, A. C. Symington, S. Hailes, and H. J. Kappen. Real-time stochastic opti- mal control for multi-agent quadrotor systems. In 26th International Conference on Automated Planning and Scheduling, 2016.  P. Guan, M. Raginsky, and R. M. Willett. Online markov decision processes with kullback-leibler  control cost. Automatic Control, IEEE Transactions on, 59(6):1423-1438, 2014. NEU AND G \u00b4OMEZ  E. Hazan. The convex optimization approach to regret minimization. In S. Sra, S. Nowozin, and  S. Wright, editors, Optimization for Machine Learning, pages 287-303. MIT press, 2011.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69:169-192, 2007.  E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization,  2(3-4):157-325, 2016.  T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal  of Machine Learning Research, 99:1563-1600, August 2010. ISSN 1532-4435.  H. J. Kappen. Linear theory for control of nonlinear stochastic systems. Physical review letters, 95  (20):200201, 2005.  H. J. Kappen, V. G\u00b4omez, and M. Opper. Optimal control as a graphical model inference problem.  Machine learning, 87(2):159-182, 2012.  K. Kinjo, E. Uchibe, and K. Doya. Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task. Frontiers in Neurorobotics, 7:1-13, 2013.  J. Kivinen and M. Warmuth. Averaging expert predictions.  In Proceedings of the Fourth Euro- pean Conference on Computational Learning Theory, pages 153-167. Lecture Notes in Artificial Intelligence, Vol. 1572. Springer, 1999.  W. Kot\u0142owski. On minimaxity of follow the leader strategy in the stochastic setting. In International  Conference on Algorithmic Learning Theory, pages 261-275, 2016.  T. Matsubara, V. G\u00b4omez, and H. J. Kappen. Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models. 30th Conference on Uncertainty in Artificial Intel- ligence (UAI), 2014.  N. Merhav and M. Feder. Universal sequential learning and decision from individual data sequences. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory. ACM Press, 1992.  C. D. Meyer. Matrix analysis and applied linear algebra, volume 2. Siam, 2000.  G. Neu, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. The online loop-free stochastic shortest-path problem. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 231-243, 2010.  G. Neu, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. The adversarial stochastic shortest path problem with  unknown transition probabilities. In AISTATS 2012, pages 805-813, 2012.  G. Neu, A. Gy\u00a8orgy, Cs. Szepesv\u00b4ari, and A. Antos. Online Markov decision processes under bandit  feedback. IEEE Transactions on Automatic Control, 59:676-691, 2014.  M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-  Interscience, April 1994. FAST RATES FOR ONLINE LEARNING IN LMDPS  E. Rombokas, M. Malhotra, E. A. Theodorou, E. Todorov, and Y. Matsuoka. Reinforcement learning and synergistic control of the act hand. IEEE/ASME Transactions on Mechatronics, 18(2):569- 577, 2013.  A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in online optimization. In NIPS-27, pages  810-818, 2014.  E. Seneta. Non-negative matrices and Markov chains. Springer Science & Business Media, 2006.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012.  R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Cs. Szepesv\u00b4ari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelli-  gence and Machine Learning. Morgan & Claypool Publishers, 2010.  D. Thalmeier, V. G\u00b4omez, and H. J. Kappen. Action selection in growing state spaces: control of network structure growth. Journal of Physics A: Mathematical and Theoretical, 50(3):034006, 2017.  E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforce-  ment learning. Journal of Machine Learning Research, 11:3137-3181, 2010.  E. Todorov. Linearly-solvable Markov decision problems. In NIPS-18, pages 1369-1376, 2006.  ISBN 0-262-23253-7.  E. Todorov. General duality between optimal control and estimation.  In Decision and Control,  2008. CDC 2008. 47th IEEE Conference on, pages 4286-4292. IEEE, 2008.  E. Todorov. Compositionality of optimal control laws. In NIPS-22, pages 1856-1864, 2009.  E. Todorov. Policy gradients in linearly-solvable mdps. In NIPS-23, pages 2298-2306. CURRAN,  2010.  T. van Erven, P. D. Gr\u00a8unwald, N. A. Mehta, M. D. Reid, and R. C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793-1861, 2015.  G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou. Aggressive driving with model predictive path integral control. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 1433-1440, May 2016. doi: 10.1109/ICRA.2016.7487277.  J. Y. Yu, S. Mannor, and N. Shimkin. Markov decision processes with arbitrary reward processes.  Mathematics of Operations Research, 34(3):737-757, 2009.  A. Zimin and G. Neu. Online learning in episodic Markovian decision processes by relative entropy  policy search. In NIPS-26, pages 1583-1591, 2013. NEU AND G \u00b4OMEZ"}