{"1": "Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 464-473. IEEE, 2014.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use little information. In Proceedings of the 29th international conference on algorithmic learning theory. To appear, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Mark Braverman. Interactive information complexity. In In Proceedings of the 44th annual ACM  Symposium on Theory of Computing, STOC12, 2012.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2ed  edition, 2006.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265-284. Springer, 2006.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 117-126. ACM, 2015.  11   A DIRECT SUM RESULT FOR THE INFORMATION COMPLEXITY OF LEARNING  sample, h(x) = y. For any x that did not appear in the sample, h(x) is sampled uniformly from {0, 1}. This algorithm has mutual information that does not grow with the size of the domain (it is O(m)). This is not too meaningful, as this algorithm is not a PAC learner. But it illustrates that the lower bound breaks somewhere, and it would be worthwhile to identify exactly how far the assumptions can be pushed before it breaks.  A different and interesting direction is to prove upper bounds on information complexity. First, we would like to understand whether the lower bound presented here is sharp. Better yet: Can we provide explicit general constructions for learning algorithms that obtain the information complex- ity, i.e., retain the minimal amount of information possible? Following the theorem of Bassily et al. (2018) stating that compression entails learning, this would yield a novel class of learning algo- rithms for all hypothesis classes in which the information complexity is o(m) - a strong result that might even have practical applications.  Lastly, and perhaps most interestingly, one may also consider the converse of that theorem: Is there a sense in which low information complexity is a necessary condition for learnability? Are the concepts equivalent?  References  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 464-473. IEEE, 2014.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use little information. In Proceedings of the 29th international conference on algorithmic learning theory. To appear, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Mark Braverman. Interactive information complexity. In In Proceedings of the 44th annual ACM  Symposium on Theory of Computing, STOC12, 2012.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2ed  edition, 2006.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265-284. Springer, 2006.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 117-126. ACM, 2015. A DIRECT SUM RESULT FOR THE INFORMATION COMPLEXITY OF LEARNING  Peter D Gr\u00a8unwald. The minimum description length principle. MIT press, 2007.  Marcus Hutter. Universal algorithmic intelligence: A mathematical top down approach. In Artificial  General Intelligence, pages 227-290. Springer, 2007.  Mauricio Karchmer, Ran Raz, and Avi Wigderson. Super-logarithmic depth lower bounds via the direct sum in communication complexity. Computational Complexity, 5(3-4):191-204, 1995.  Eyal Kushilevitz and Noam Nisan. Communication Complexity. Cambridge University Press, 1997.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical  report, Technical report, University of California, Santa Cruz, 1986.  Li Ming and Paul Vit\u00b4anyi. An introduction to Kolmogorov complexity and its applications. Springer  Heidelberg, 1997.  ACM (JACM), 63(3):21, 2016.  Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. Journal of the  Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465-471, 1978.  Ryan Rogers, Aaron Roth, Adam Smith, and Om Thakkar. Max-information, differential privacy, and post-selection hypothesis testing. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 487-494. IEEE, 2016.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Maurice Sion. On general minimax theorems. Pacific journal of mathematics, 8(1):171-176, 1958.  Ray J Solomonoff. A formal theory of inductive inference, part I. Information and control, 7(1):  1-22, 1964.  1928.  Vladimir N Vapnik and Alexey Ya Chervonenkis. On the uniform convergence of relative frequen-  cies of events to their probabilities. Measures of Complexity, 16(2):11, 1971.  John Von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320,  John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. 1944.  Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn- ing algorithms. In Advances in Neural Information Processing Systems, pages 2521-2530, 2017."}