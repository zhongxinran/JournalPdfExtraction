{"1": "Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak and David Steurer. Sum-of-squares:proofs, beliefs, and algorithms. 2016. URL https:  //www.sumofsquares.org/public/lec-definitions-general.html.  R\u00b4emi Bardenet, Odalric-Ambrym Maillard, et al. Concentration inequalities for sampling without  replacement. Bernoulli, 21(3):1361-1385, 2015.  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Peter L Bartlett, Shahar Mendelson, and Joseph Neeman. (cid:96)1-regularized linear regression: persistence  and oracle inequalities. Probability theory and related fields, 154(1-2):193-224, 2012.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix  completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 310-321. Springer, 2004.  Amit Daniely. Complexity theoretic limitations on learning halfspaces.  In Proceedings of the  forty-eighth annual ACM symposium on Theory of Computing, pages 105-117. ACM, 2016.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018.  12   FAST RATES FOR AGNOSTIC TENSOR COMPLETION  5. Conclusion  Our results demonstrate the power of the sum-of-squares hierarchy for agnostic statistical learning, and show for the first time that sum-of-squares algorithms can obtain fast rates for prediction. We hope our work will serve as a starting point for applying sum-of-squares to obtain polynomial time algorithms with fast rates in statistical learning for broader classes of models.  A few immediate technical questions emerge. Can the dependence on rank in our results be improved? Can the subgradient results be extended to the general undercomplete or even overcomplete case? Can similar agnostic learning results be obtained with a more practical algorithm that does not rely on solving the full sum-of-squares SDP?  Acknowledgements We thank Sasha Rakhlin and Ankur Moitra for helpful discussions and thank Matthew J. Telgarsky for being a constant source of inspiration.  References  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak and David Steurer. Sum-of-squares:proofs, beliefs, and algorithms. 2016. URL https:  //www.sumofsquares.org/public/lec-definitions-general.html.  R\u00b4emi Bardenet, Odalric-Ambrym Maillard, et al. Concentration inequalities for sampling without  replacement. Bernoulli, 21(3):1361-1385, 2015.  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Peter L Bartlett, Shahar Mendelson, and Joseph Neeman. (cid:96)1-regularized linear regression: persistence  and oracle inequalities. Probability theory and related fields, 154(1-2):193-224, 2012.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix  completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 310-321. Springer, 2004.  Amit Daniely. Complexity theoretic limitations on learning halfspaces.  In Proceedings of the  forty-eighth annual ACM symposium on Theory of Computing, pages 105-117. ACM, 2016.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Shmuel Friedland and Lek-Heng Lim. Nuclear norm of higher-order tensors. Mathematics of  Computation, 87(311):1255-1281, 2018.  St\u00b4ephane Gaiffas and Guillaume Lecu\u00b4e. Sharp oracle inequalities for high-dimensional matrix  prediction. IEEE Transactions on Information Theory, 57(10):6942-6957, 2011.  David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions  on Information Theory, 57(3):1548-1566, 2011.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. The ellipsoid method and its consequences  in combinatorial optimization. Combinatorica, 1(2):169-197, 1981.  David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Information and Computation, 100(1):78-150, 1992.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via  sum-of-square proofs. In Conference on Learning Theory, pages 956-1006, 2015.  Prateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In Advances in  Neural Information Processing Systems, pages 1431-1439, 2014.  Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. Journal of Machine Learning Research, 11(Jul):2057-2078, 2010.  Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression.  Proceedings of The 31st Conference on Learning Theory, 2018.  Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):  455-500, 2009.  Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302-2329, 2011.  Pravesh K Kothari, Ryuhei Mori, Ryan O\u2019Donnell, and David Witmer. Sum of squares lower bounds for refuting any csp. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 132-145. ACM, 2017.  Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal  on Optimization, 11(3):796-817, 2001.  Guillaume Lecu\u00b4e and Shahar Mendelson. Learning subgaussian classes: Upper and minimax bounds.  arXiv preprint arXiv:1305.4825, 2013. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Guillaume Lecu\u00b4e and Shahar Mendelson. Regularization and the small-ball method ii: complexity dependent error rates. The Journal of Machine Learning Research, 18(1):5356-5403, 2017.  Guillaume Lecu\u00b4e and Shahar Mendelson. Regularization and the small-ball method i: sparse recovery.  The Annals of Statistics, 46(2):611-641, 2018.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag, New York,  1991.  Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localization through offset rademacher complexity. In Proceedings of The 28th Conference on Learning Theory, pages 1260-1285, 2015.  Shahar Mendelson. Learning without concentration. In Proceedings of The 27th Conference on  Learning Theory, pages 25-39, 2014.  Vitali D Milman and Gideon Schechtman. Asymptotic theory of finite dimensional normed spaces.  Springer-Verlag New York, Inc., 1986.  Andrea Montanari and Nike Sun. Spectral algorithms for tensor completion. Communications on  Pure and Applied Mathematics, 2016.  Cun Mu, Bo Huang, John Wright, and Donald Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In International Conference on Machine Learning, pages 73-81, 2014.  Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and  high-dimensional scaling. The Annals of Statistics, pages 1069-1097, 2011.  Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May): 1665-1697, 2012.  Sahand Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, 2012.  Ryan O\u2019Donnell and Yuan Zhou. Approximability and proof complexity. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1537-1556. Society for Industrial and Applied Mathematics, 2013.  Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness  and optimization. PhD thesis, California Institute of Technology, 2000.  Aaron Potechin and David Steurer. Exact tensor completion with sum-of-squares. Proceedings of  the 30th Annual Conference on Learning Theory, 2017.  Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random csps below the spectral threshold. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 121-131. ACM, 2017. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12(Dec):3413-3430, 2011.  R Tyrrell Rockafellar. Convex Analysis, volume 28. Princeton University Press, 1970.  Bernardino Romera-Paredes and Massimiliano Pontil. A new convex relaxation for tensor completion.  In Advances in Neural Information Processing Systems, pages 2967-2975, 2013.  Naum Zuselevich Shor. An approach to obtaining global extremums in polynomial mathematical  programming problems. Cybernetics, 23(5):695-700, 1987.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference  on Computational Learning Theory, pages 545-560. Springer, 2005.  Ryota Tomioka and Taiji Suzuki. Convex tensor decomposition via structured schatten norm regularization. In Advances in neural information processing systems, pages 1331-1339, 2013.  Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex tensor decomposition. In Advances in Neural Information Processing Systems, pages 972-980, 2011.  Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series  in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.  G Alistair Watson. Characterization of the subdifferential of some matrix norms. Linear algebra and  its applications, 170:33-45, 1992.  Dong Xia, Ming Yuan, and Cun-Hui Zhang. Statistically optimal and computationally efficient low  rank tensor completion from noisy entries. arXiv preprint arXiv:1711.04934, 2017.  Ming Yuan and Cun-Hui Zhang. On tensor completion via nuclear norm minimization. Foundations  of Computational Mathematics, 16(4):1031-1068, 2016."}