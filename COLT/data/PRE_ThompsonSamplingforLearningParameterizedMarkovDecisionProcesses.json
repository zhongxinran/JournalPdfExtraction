{"1": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm. CoRR, abs/1406.3926, 2014. URL http: //arxiv.org/abs/1406.3926.  R. Agrawal, D. Teneketzis, and V. Anantharam. Asymptotically efficient adaptive allocation schemes for controlled Markov chains: finite parameter space. IEEE Trans. Aut. Cont., 34(12): 1249-1259, Dec 1989.  Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit  problem. In COLT, volume 23 of Proc. JMLR, pages 39.1-39.26, 2012.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In Proc. ICML, 2013.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. J. Mach. Learn. Res., 11:2785-2836, December 2010.  Andrew R. Barron. Information-theoretic characterization of Bayes Performance and the Choice of  Priors in Parametric and Nonparametric Problems. Bayesian Statistics, 6:27-52, 1998.  P.L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In Proc. UAI, pages 35-42, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures in Machine Learning, pages 208-240. Springer, 2004.  Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-  optimal reinforcement learning. JMLR, 3:213-231, 2003.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision  processes. Math. Oper. Res., 22(1):pp. 222-255, 1997.  Taeryon Choi and R. V. Ramamoorthi. Remarks on consistency of posterior distributions, volume 3  of Collections. Institute of Mathematical Statistics, 2008.  Victor H. de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Pseudo-maximization and self-  normalized processes. Probab. Surveys, 4:172-192, 2007.  Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In Proc.  UAI, 1999.  S. Ghosal, J. K. Ghosh, and R. V. Ramamoorthi. Posterior consistency of Dirichlet mixtures in  density estimation. Ann. Statist., 27(1):143-158, 03 1999.  Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior  distributions. Ann. Statist., 28(2):500-531, 04 2000.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson Sampling for Complex Online  Problems. In Proc. ICML, 2014.  13   THOMPSON SAMPLING FOR LEARNING PARAMETERIZED MDPS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm. CoRR, abs/1406.3926, 2014. URL http: //arxiv.org/abs/1406.3926.  R. Agrawal, D. Teneketzis, and V. Anantharam. Asymptotically efficient adaptive allocation schemes for controlled Markov chains: finite parameter space. IEEE Trans. Aut. Cont., 34(12): 1249-1259, Dec 1989.  Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit  problem. In COLT, volume 23 of Proc. JMLR, pages 39.1-39.26, 2012.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In Proc. ICML, 2013.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. J. Mach. Learn. Res., 11:2785-2836, December 2010.  Andrew R. Barron. Information-theoretic characterization of Bayes Performance and the Choice of  Priors in Parametric and Nonparametric Problems. Bayesian Statistics, 6:27-52, 1998.  P.L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In Proc. UAI, pages 35-42, 2009.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced  Lectures in Machine Learning, pages 208-240. Springer, 2004.  Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-  optimal reinforcement learning. JMLR, 3:213-231, 2003.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision  processes. Math. Oper. Res., 22(1):pp. 222-255, 1997.  Taeryon Choi and R. V. Ramamoorthi. Remarks on consistency of posterior distributions, volume 3  of Collections. Institute of Mathematical Statistics, 2008.  Victor H. de la Pe\u02dcna, Michael J. Klass, and Tze Leung Lai. Pseudo-maximization and self-  normalized processes. Probab. Surveys, 4:172-192, 2007.  Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In Proc.  UAI, 1999.  S. Ghosal, J. K. Ghosh, and R. V. Ramamoorthi. Posterior consistency of Dirichlet mixtures in  density estimation. Ann. Statist., 27(1):143-158, 03 1999.  Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior  distributions. Ann. Statist., 28(2):500-531, 04 2000.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson Sampling for Complex Online  Problems. In Proc. ICML, 2014. GOPALAN MANNOR  Geoffrey Grimmett and David Stirzaker. Probability and Random Processes. Oxford University  Press, 1992.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement  Learning. JMLR, 11:1563-1600, 2010.  Emilie Kaufmann, Nathaniel Korda, and R\u00b4emi Munos. Thompson Sampling: An Asymptotically  Optimal Finite-time Analysis. In Proc. ALT, 2012.  Ger Koole. A simple proof of the optimality of a threshold policy in a two-server queueing system.  Syst. Control Lett., 26(5):301-303, December 1995.  Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson Sampling for 1-Dimensional  Exponential Family Bandits. In Proc. NIPS, 2013.  Christina E Lee, Asuman Ozdaglar, and Devavrat Shah. Computing the Stationary Distribution  Locally. In Proc. NIPS, pages 1376-1384. Curran Associates, Inc., 2013.  David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. Amer.  Math. Soc., 2006.  Woei Lin and P.R. Kumar. Optimal control of a queueing system with two heterogeneous servers.  Automatic Control, IEEE Transactions on, 29(8):696-703, Aug 1984.  Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with  function approximation. In Proc. ICML, pages 664-671, 2008.  P A Ortega and D A Braun. A Minimum Relative Entropy Principle for Learning and Acting. JAIR,  38:475-511, 2010.  Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder Dimen- sion. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1466-1474. Curran Associates, Inc., 2014.  Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in Factored MDPs. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 604-612. Curran Associates, Inc., 2014.  Ian Osband, Dan Russo, and Benjamin Van Roy.  (More) Efficient Reinforcement Learning via  Posterior Sampling. In Proc. NIPS, pages 3003-3011. Curran Associates, Inc., 2013.  Herbert Robbins and David Siegmund. Boundary crossing probabilities for the Wiener process and  sample sums. Ann. Math. Statist., 41(5):1410-1429, 1970.  Dan Russo and Benjamin Van Roy. Eluder Dimension and the Sample Complexity of Optimistic  Exploration. In Proc. NIPS, pages 2256-2264. Curran Associates, Inc., 2013.  Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior distributions. Ann. Stat.,  29(3):687-714, 06 2001. THOMPSON SAMPLING FOR LEARNING PARAMETERIZED MDPS  Ambuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for  irreducible MDPs. In Proc. NIPS, 2008.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 24(3-4):285-294, 1933.  Appendices for the paper Thompson Sampling for Learning Parameterized Markov Decision Processes"}