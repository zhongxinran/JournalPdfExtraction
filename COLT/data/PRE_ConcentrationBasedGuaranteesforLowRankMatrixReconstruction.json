{"1": "925-936, 2010.  P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. In Computational Learning Theory, pages 224-240. Springer, 2001.  E.J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):  E.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations  of Computational Mathematics, 9(6):717-772, 2009.  E.J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion.  IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  A. Chistov and D. Grigoriev. Complexity of quantifier elimination in the theory of alge- braically closed fields. In Proceedings of the 11th Symposium on Mathematical Foun- dations of Computer Science, volume 176 of Lecture Notes in Computer Science, pages 17-31. Springer, 1984.  M. Fazel, H. Hindi, and S.P. Boyd. A rank minimization heuristic with application to In Proceedings of the 2001 American Control  minimum order system approximation. Conference, volume 6, pages 4734-4739. IEEE, 2002.  R.H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal  of Machine Learning Research, 11:2057-2078, 2010.  V. Koltchinskii, A.B. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal  rates for noisy low rank matrix completion. arXiv:1011.6256, 2010.  T. Lee, A. Shraibman, and R. Spalek. A direct product theorem for discrepancy. In 23rd Annual IEEE Conference on Computational Complexity (CCC\u201908), pages 71-80. IEEE, 2008.  331   Concentration-Based Guaranteesfor Low-Rank Matrix Reconstruction  \u2022 Pointing out that the max-norm can yield superior reconstruction guarantees over the  more commonly used trace-norm.  \u2022 Studying the issue of sampling with and without replacement, and establishing rig- orous generic results relating the two settings. This has been done before for exact recovery (Recht, 2009), but is done here for the more delicate situation of approximate recovery of either M or Y .  The main deficiency of our approach is a worse dependence on the approximation parameter (cid:15), when \u03c3 > 0 (i.e. the approximately low rank case) and (cid:15) = o(\u03c32) (i.e. estimation error less then approximation error). Although this dependence is tight for general classes with bounded Rademacher complexity, we do not know if it can be improved in Theorem 6. In particular, we do not know whether the less favorable dependence is a consequence of not relying on zero-mean i.i.d. noise, or not relying on M having low-rank (instead of only assuming low max-norm), or on relying only on the Rademacher complexity of the class of low max-norm matrices\u2014perhaps better bounds can be obtained with a more careful analysis.  References  925-936, 2010.  P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. In Computational Learning Theory, pages 224-240. Springer, 2001.  E.J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):  E.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations  of Computational Mathematics, 9(6):717-772, 2009.  E.J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion.  IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  A. Chistov and D. Grigoriev. Complexity of quantifier elimination in the theory of alge- braically closed fields. In Proceedings of the 11th Symposium on Mathematical Foun- dations of Computer Science, volume 176 of Lecture Notes in Computer Science, pages 17-31. Springer, 1984.  M. Fazel, H. Hindi, and S.P. Boyd. A rank minimization heuristic with application to In Proceedings of the 2001 American Control  minimum order system approximation. Conference, volume 6, pages 4734-4739. IEEE, 2002.  R.H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal  of Machine Learning Research, 11:2057-2078, 2010.  V. Koltchinskii, A.B. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal  rates for noisy low rank matrix completion. arXiv:1011.6256, 2010.  T. Lee, A. Shraibman, and R. Spalek. A direct product theorem for discrepancy. In 23rd Annual IEEE Conference on Computational Complexity (CCC\u201908), pages 71-80. IEEE, 2008. Foygel Srebro  S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix com-  pletion: Optimal bounds with noise. arXiv:1009.2118, 2010.  B. Recht. A simpler approach to matrix completion. arXiv:0910.0651, 2009.  R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm. Advances in Neural Information Processing Systems, 23: 2056-2064, 2010.  Y. Seginer. The expected norm of random matrices. Combinatorics, Probability and Com-  puting, 9(2):149-166, 2000.  A.A. Sherstov. Halfspace matrices. In 22nd Annual IEEE Conference on Computational  Complexity (CCC\u201907), pages 83-95. IEEE, 2007.  N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. 18th Annual Conference  on Learning Theory (COLT), pages 545-560, 2005.  N. Srebro, J.D.M. Rennie, and T.S. Jaakkola. Maximum-margin matrix factorization. Ad-  vances in Neural Information Processing Systems, 17:1329-1336, 2005.  N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. Advances  in Neural Information Processing Systems, 23:2199-2207, 2010.  J.A. Tropp. User-friendly tail bounds for sums of random matrices. arXiv:1004.4389, 2010. Concentration-Based Guaranteesfor Low-Rank Matrix Reconstruction"}