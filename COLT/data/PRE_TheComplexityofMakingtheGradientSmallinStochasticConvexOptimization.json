{"1": "Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1-9, 2009.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205. ACM, 2017.  Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1165-1175. 2018.  G\u00b4abor Braun, Crist\u00b4obal Guzm\u00b4an, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Transactions on Information Theory, 63(7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points i. arXiv preprint arXiv:1710.11606, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017b.  Damek Davis and Dmitriy Drusvyatskiy. Complexity of finding near-stationary points of convex  functions stochastically. arXiv preprint arXiv:1802.08556, 2018.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti- mization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 687-697, 2018.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information.  SIAM Journal on Computing, 23(5):1001-1018, 1994.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-  tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlin- ear and stochastic programming. Math. Program., 156(1-2):59-99, 2016. doi: 10.1007/ s10107-015-0871-8.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724-1732, 2017.  13   THE COMPLEXITY OF MAKING THE GRADIENT SMALL  References  Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1-9, 2009.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205. ACM, 2017.  Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1165-1175. 2018.  G\u00b4abor Braun, Crist\u00b4obal Guzm\u00b4an, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Transactions on Information Theory, 63(7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points i. arXiv preprint arXiv:1710.11606, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017b.  Damek Davis and Dmitriy Drusvyatskiy. Complexity of finding near-stationary points of convex  functions stochastically. arXiv preprint arXiv:1802.08556, 2018.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti- mization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 687-697, 2018.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information.  SIAM Journal on Computing, 23(5):1001-1018, 1994.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-  tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlin- ear and stochastic programming. Math. Program., 156(1-2):59-99, 2016. doi: 10.1007/ s10107-015-0871-8.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724-1732, 2017. THE COMPLEXITY OF MAKING THE GRADIENT SMALL  Richard M Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 881-890. Society for Industrial and Applied Mathematics, 2007.  Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2348-2358, 2017.  Arkadii Semenovich Nemirovski and David Borisovich Yudin. Problem complexity and method  efficiency in optimization. 1983.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course. 2004.  Yurii Nesterov. How to make the gradients small. Optima, 88:10-11, 2012.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00b4as P\u00b4ocz\u00b4os, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, pages 314-323, 2016.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex  optimization. In Conference on Learning Theory, 2009.  Joseph F Traub, Grzegorz W Wasilkowski, and Henryk Wo\u00b4zniakowski. Information-based complexity.  1988.  Blake Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives.  In Advances in Neural Information Processing Systems 29, pages 3639-3647. 2016.  Blake Woodworth, Jialei Wang, Brendan McMahan, and Nathan Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Advances in Neural Information Processing Systems 31, pages 8505-8515, 2018.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent for nonconvex optimization. In Advances in Neural Information Processing Systems 31, pages 3925-3936. 2018."}