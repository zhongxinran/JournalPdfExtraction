{"1": "Michal Aharon, Michael Elad, and Alfred M. Bruckstein. K-SVD: An algorithm for de- signing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):4311-4322, 2006.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations.  Cambridge University Press, 1999.  Peter L. Bartlett, Tam\u00b4as Linder, and G\u00b4abor Lugosi. The minimax distortion redundancy in empirical quantizer design. IEEE Transactions on Information theory, 44(5):1802-1813, 1998.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. Ann. Statist., 33:1497-1537, 2005.  G\u00b4erard Biau, Luc Devroye, and G\u00b4abor Lugosi. On the performance of clustering in Hilbert  spaces. IEEE Transactions on Information Theory, 54(2):781-790, 2008.  Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel  principal component analysis. Machine Learning, 66(2):259-294, 2007.  788   Vainsencher Mannor Bruckstein  only logarithmically dependent on the l1 norm of the coe\ufb03cient vector widens the set of applicable approaches to penalization. Second, in the particular case of k sparse represen- tation, we have shown that the Babel function is a key property for the generalization of dictionaries. It might thus be useful to modify dictionary learning algorithms so that they obtain dictionaries with low Babel functions, possibly through regularization or through certain convex relaxations. Third, mistake bounds (e.g., Mairal et al. 2010) on the quality of the solution to the coe\ufb03cient finding optimization problem may lead to generalization bounds for practical algorithms, by tying such algorithms to k sparse representation.  The upper bounds presented here invite complementary lower bounds. The existing lower bounds for k = 1 (vector quantization) and for k = p (representation using PCA di- rections) are applicable, but do not capture the geometry of general k sparse representation, and in particular do not clarify the e\ufb00ective dimension of the unrestricted class of dictio- naries for it. We have not excluded the possibility that the class of unrestricted dictionaries has the same dimension as that of those with a small Babel function. The best upper bound we know for the larger class, being the trivial one of order O (cid:0)(cid:0)p (cid:1)n2(cid:14) m), leaves a significant k gap for future exploration.  We view the dependence on \u00b5k\u22121 from an \u201calgorithmic luckiness\u201d perspective (Herbrich and Williamson, 2003): if the data is described by a dictionary with low Babel function the generalization bounds are encouraging.  We thank Shahar Mendelson for helpful discussions. A.B. was partly supported by the European Communitys FP7-FET program, SMALL project, under grant agreement no. 225913. S.M and D.V. were partially supported by the ISF under contract 890015.  Acknowledgments  References  Michal Aharon, Michael Elad, and Alfred M. Bruckstein. K-SVD: An algorithm for de- signing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):4311-4322, 2006.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations.  Cambridge University Press, 1999.  Peter L. Bartlett, Tam\u00b4as Linder, and G\u00b4abor Lugosi. The minimax distortion redundancy in empirical quantizer design. IEEE Transactions on Information theory, 44(5):1802-1813, 1998.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. Ann. Statist., 33:1497-1537, 2005.  G\u00b4erard Biau, Luc Devroye, and G\u00b4abor Lugosi. On the performance of clustering in Hilbert  spaces. IEEE Transactions on Information Theory, 54(2):781-790, 2008.  Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel  principal component analysis. Machine Learning, 66(2):259-294, 2007. The Sample Complexity of Dictionary Learning  Alfred M. Bruckstein, David L. Donoho, and Michael Elad. From sparse solutions of systems of equations to sparse modeling of signals and images. SIAM Review, 51(1):34-81, 2009.  Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projec- tions: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12): 5406-5425, 2006.  Scott S. Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis  pursuit. SIAM Review, 43(1):129-159, 2001.  Sheng Chen, Stephen A. Billings, and Wan Luo. Orthogonal least squares methods and their application to non-linear system identification. International Journal of Control, 50 (5):1873-1896, 1989.  Felipe Cucker and Stephen Smale. On the mathematical foundations of learning. Bull.  Amer. Math. Soc, 39(1):1-50, 2002.  Geo\ufb00 Davis, St`ephane Mallat, and Marco Avellaneda. Adaptive greedy approximations.  Constructive approximation, 13(1):57-98, 1997.  David L. Donoho and Michael Elad. Optimally sparse representation in general (nonorthog- onal) dictionaries via (cid:96)1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197-2202, 2003.  Richard J. Du\ufb03n and Albert C. Schaeer. A class of nonharmonic Fourier series. Trans.  Amer. Math. Soc, 72:341-366, 1952.  Ralf Herbrich and Robert Williamson. Algorithmic luckiness. Journal of Machine Learning  Research, 3:175-212, 2003.  Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge University Press, 1990.  Kenneth Kreutz-Delgado, Joseph F. Murray, Bhaskar D. Rao, Kjersti Engan, Te-Won Lee, and Terrance J. Sejnowski. Dictionary learning algorithms for sparse representation. Neural computation, 15(2):349-396, 2003.  Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. E\ufb03cient sparse coding algorithms. In Advances in Neural Information Processing Systems 19, pages 801-808. MIT Press, Cambridge, MA, 2007.  Michael S. Lewicki, Terrence J. Sejnowski, and Howard Hughes. Learning overcomplete  representations. Neural Computation, 12:337-365, 1998.  Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11:19-60, 2010.  Andreas Maurer and Massimiliano Pontil. K-dimensional coding schemes in hilbert spaces.  IEEE Transactions on Information Theory, 56:5839-5846, November 2010.  Shahar Mendelson. A few notes on statistical learning theory. Advanced Lectures on Machine  Learning, pages 1-40, 2003. Vainsencher Mannor Bruckstein  Bruno A. Olshausen and David J. Fieldt. Sparse coding with an overcomplete basis set: a  strategy employed by V1. Vision Research, 37:3311-3325, 1997.  Gabriel Peyr\u00b4e. Sparse modeling of textures. Journal of Mathematical Imaging and Vision,  34(1):17-31, 2009.  Matan Protter and Michael Elad. Sparse and redundant representations and motion- estimation-free algorithm for video denoising. Wavelets XII. Proceedings of the SPIE, 6701:43, 2007.  John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge  University Press, 2004.  John Shawe-Taylor, Christopher K. I. Williams, Nello Cristianini, and Jaz Kandola. On the eigenspectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE Transactions on Information Theory, 51(7):2510-2522, 2005.  Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans-  actions on Information Theory, 50:2231-2242, 2004.  John Wright, Allen Y. Yang, Arvind Ganesh, S. Shankar Sastry, and Yi Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 210-227, 2008."}