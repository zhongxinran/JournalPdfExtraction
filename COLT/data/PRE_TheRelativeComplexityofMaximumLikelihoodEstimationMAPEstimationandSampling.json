{"1": "D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clus-  tering. Machine Learning, 75(2):245-248, 2009.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond SVD. In Proceedings of the  53rd IEEE Annual Symposium on Foundations of Computer Science, pages 1-10, 2012.  A. Baker. Transcendental Number Theory. Cambridge University Press, 1975.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A  non-asymptotic analysis. In Conference on Learning Theory, pages 300-323, 2018.  B. Chor and T. Tuller. Finding a maximum likelihood tree is hard. Journal of the ACM, 53(5):  722-744, 2006.  V. Guruswami and A. Vardy. Maximum-likelihood decoding of Reed-Solomon codes is NP-hard. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 470-478, 2005.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Opera-  tions Research, 31(2):253-266, 2006.  L. M. Le Cam. On some asymptotic properties of maximum likelihood estimates and related Bayes  estimates. University of California Publications in Statistics, 1:277-330, 1953.  K. W. Ng, G.-L. Tian, and M.-L. Tang. Dirichlet and Related Distributions: Theory, Methods and  Applications. Wiley, 2011.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin In Conference on Learning Theory, pages 1674-1703,  dynamics: a nonasymptotic analysis. 2017.  D. Sontag and D. Roy. Complexity of inference in latent Dirichlet allocation. In Advances in Neural  Information Processing Systems, pages 1008-1016, 2011.  F. Tops\u00f8e. Some bounds for the logarithmic function. Inequal. Theory Appl., pages 137-151, 2004.  C. Tosh and S. Dasgupta. Maximum likelihood estimation for mixtures of spherical Gaussians is  NP-hard. JMLR, 18(1):1-11, 2018.  13   RELATING ML ESTIMATION, MAP ESTIMATION, SAMPLING  References  D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clus-  tering. Machine Learning, 75(2):245-248, 2009.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond SVD. In Proceedings of the  53rd IEEE Annual Symposium on Foundations of Computer Science, pages 1-10, 2012.  A. Baker. Transcendental Number Theory. Cambridge University Press, 1975.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A  non-asymptotic analysis. In Conference on Learning Theory, pages 300-323, 2018.  B. Chor and T. Tuller. Finding a maximum likelihood tree is hard. Journal of the ACM, 53(5):  722-744, 2006.  V. Guruswami and A. Vardy. Maximum-likelihood decoding of Reed-Solomon codes is NP-hard. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 470-478, 2005.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Opera-  tions Research, 31(2):253-266, 2006.  L. M. Le Cam. On some asymptotic properties of maximum likelihood estimates and related Bayes  estimates. University of California Publications in Statistics, 1:277-330, 1953.  K. W. Ng, G.-L. Tian, and M.-L. Tang. Dirichlet and Related Distributions: Theory, Methods and  Applications. Wiley, 2011.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin In Conference on Learning Theory, pages 1674-1703,  dynamics: a nonasymptotic analysis. 2017.  D. Sontag and D. Roy. Complexity of inference in latent Dirichlet allocation. In Advances in Neural  Information Processing Systems, pages 1008-1016, 2011.  F. Tops\u00f8e. Some bounds for the logarithmic function. Inequal. Theory Appl., pages 137-151, 2004.  C. Tosh and S. Dasgupta. Maximum likelihood estimation for mixtures of spherical Gaussians is  NP-hard. JMLR, 18(1):1-11, 2018. RELATING ML ESTIMATION, MAP ESTIMATION, SAMPLING"}