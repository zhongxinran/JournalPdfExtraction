{"1": "Michael Kearns and Umesh Vazirani. An Introduction to Computational Learning Theory.  The MIT Press, Cambridge, Massachusetts, U.S.A., 1994.  Loizos Michael. Autodidactic Learning and Reasoning. PhD thesis, School of Engineering  and Applied Sciences, Harvard University, U.S.A., May 2008.  Loizos Michael. Partial Observability and Learnability. Artificial Intelligence, 174(11):  639-669, July 2010.  Dale Schuurmans and Russell Greiner. Learning Default Concepts. In Proceedings of the Tenth Canadian Conference on Artificial Intelligence (AI\u201994), pages 99-106, May 1994.  Leslie Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134-1142,  November 1984.  827   Missing Information Impediments to Learnability  Thus, the concept classes of conjunctions and linear thresholds (Kearns and Vazirani, 1994) are consistently learnable. Unlike what holds in the PAC learning model, a learning reduction cannot be readily employed to establish the learnability of k-CNFs for constant values of k \u2265 2. This holds because k-CNFs cannot be evaluated modularly on observations (unlike on examples). Indeed, the value of a certain conjunction of two subformulas on some observation may not be determinable only by the values of the subformulas (e.g., when they are undefined), but may require knowledge of the subformulas themselves. Hence:  Problem 3 Is the concept class C of 2-CNFs consistently learnable by a hypothesis class H? Is the question true for any concept class of formulas that are not modularly evaluatable?  The case of learning 3-CNFs presents an additional challenge when compared to the case of learning 2-CNFs, since the former formulas are not believed to be evaluatable e\ufb03ciently. Indeed, their evaluation on the observation \u2217n implies deciding their satisfiability. Hence:  Problem 4 Is the concept class C of 3-CNFs consistently learnable by a hypothesis class H? Is the question true for any concept class of formulas that are not e\ufb03ciently evaluatable?  Despite being a necessary condition, PAC learnability is not, by itself, a su\ufb03cient condi- tion for consistent learnability \u2014 at least not when the hypothesis class H and the concept class C are required to coincide, and the complexity condition RP (cid:54)= NP is assumed.  Theorem 5 The concept class C that comprises either parities or monotone-term 1-decision lists is not consistently learnable by the hypothesis class H = C, unless RP = NP.  The negative result holds despite C being PAC learnable by H = C (Kearns and Vazirani,  1994), and even when at most three attributes are masked in each observation. Hence:  Problem 6 Is the concept class C that comprises either parities or monotone-term 1- decision lists consistently learnable by a hypothesis class H that di\ufb00ers from C?  Refining the necessary and su\ufb03cient conditions for consistent learnability would help clarify which PAC learnability results remain true when information is missing arbitrarily, and, hence, which can be applied in realistic settings where the masking process is unknown.  References  Michael Kearns and Umesh Vazirani. An Introduction to Computational Learning Theory.  The MIT Press, Cambridge, Massachusetts, U.S.A., 1994.  Loizos Michael. Autodidactic Learning and Reasoning. PhD thesis, School of Engineering  and Applied Sciences, Harvard University, U.S.A., May 2008.  Loizos Michael. Partial Observability and Learnability. Artificial Intelligence, 174(11):  639-669, July 2010.  Dale Schuurmans and Russell Greiner. Learning Default Concepts. In Proceedings of the Tenth Canadian Conference on Artificial Intelligence (AI\u201994), pages 99-106, May 1994.  Leslie Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134-1142,  November 1984."}