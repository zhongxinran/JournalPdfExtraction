{"1": "Peyman Afshani, Ehsan Chiniforooshan, Reza Dorrigiv, Arash Farzan, Mehdi Mirzazadeh, Narges Simjour, and Hamid Zarrabi-Zadeh. On the complexity of finding an unknown cut via vertex queries. In Computing and Combinatorics, pages 459-469. Springer, 2007.  K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.  ics.uci.edu/ml.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Neural Information Processing Systems, 2011.  12   DASARATHY NOWAK ZHU  6. Experiments  We performed some preliminary experiments on the following data sets: (a) Digits: This dataset is from the Cedar Buffalo binary digits database originally Hull (1994). We preprocessed the dig- its by reducing the size of each image down to a 16x16 grid with down-sampling and Gaussian smoothing Le Cun et al. (1990). Each image is thus a 256-dimensional vector with elements being gray-scale pixel values in 0-255. We considered two separate binary classification tasks on this data set: 1 vs 2 and 4 vs 9. Intuitively one might expect the former task to be much simpler than the latter. For each task, we randomly chose 200 digits in the positive class and 200 in the negative. We computed the Euclidean distance between these 400 digits based on their feature vectors. We then constructed a symmetrized 10-nearest-neighbor graph, with an edge between images i, j iff i is among j\u2019s 10 nearest neighbors or vice versa. Each task is thus represented by a graph with exactly 400 nodes and about 3000 undirected unweighted edges. Nonetheless, due to the intrinsic confusability, the cut size and the boundary (i.e., edges connecting the two classes) varies drastically across the tasks: 1 vs 2 has a boundary of 92, while 4 vs 9 has a boundary of 290. (b) Congressional Voting Records (CVR): This is the congressional voting records data set from the UCI machine learning repository (Bache and Lichman, 2013). We created a graph out of this by thresholding (at 0.5) the Euclidean distance between the data points. This was then processed to retain the largest connected component which had 380 vertices and a boundary size of 234. (c) Grid: This is a syn- thetic example of a 15x15 grid of vertices with a positive core in the center. The core was generated from a square by randomly dithering its boundary. See Figure 3.  We compared the performance of four algorithms: (a) S2 (b) AFS - the active learning algo- rithm from Afshani et al. (2007); (c) ZLG - the algorithm from Zhu et al. (2003b); and (d) BND - the experiment design-like algorithm from Gu and Han (2012). We show the number of queries needed before all nodes in \u2202C have been queried. This number, which we call \u2202C-query com- plexity, is by definition no smaller than |\u2202C|. Notice that before completely querying \u2202C, it is impossible for any algorithm to guarantee zero error without prior assumptions. Thus we posit that \u2202C-query complexity is a sensible measure for the setting considered in this paper. In fact \u2202C-query complexity can be thought of as the experimental analogue of the theoretical query complexity of Section 4. These results are shown in Table 1. The bold figures show the best performance in each experiment. As can be seen, S2 clearly outperforms AFS and BOUND as suggested by our theory. It is quite surprising to see how well ZLG performs given that it was not designed with this objective in mind. We believe that trying to understanding this will be a fruitful avenue for future work.  References  Peyman Afshani, Ehsan Chiniforooshan, Reza Dorrigiv, Arash Farzan, Mehdi Mirzazadeh, Narges Simjour, and Hamid Zarrabi-Zadeh. On the complexity of finding an unknown cut via vertex queries. In Computing and Combinatorics, pages 459-469. Springer, 2007.  K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.  ics.uci.edu/ml.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Neural Information Processing Systems, 2011. S2 : ACTIVE LEARNING ON GRAPHS  Christopher M Bishop et al. Pattern recognition and machine learning, volume 4. springer New  York, 2006.  Avrim Blum and Shuchi Chawla. Learning from Labeled and Unlabeled Data using Graph Mincuts.  In International Conference on Machine Learning, pages 19-26, 2001.  R. Castro and R. Nowak. Minimax bounds for active learning. IEEE Transactions on Information  Theory, pages 2339-2353, 2008.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, pages 493-507, 1952.  Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active Clustering: Robust and Efficient Hierarchical Clustering using Adaptively Selected Similarities. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Quanquan Gu and Jiawei Han. Towards active learning on graphs: An error bound minimization  approach. In ICDM, pages 882-887, 2012.  S. Hanneke. Rates of convergence in active learning. The Annals of Statistics, pages 333-361, 2011.  Jonathan J. Hull. A database for handwritten text recognition research.  IEEE Transactions on  Pattern Analysis and Machine Intelligence, 16(5), 1994.  V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. Journal  of Machine Learning Research, pages 2457-2485, 2010.  Y. Le Cun, B. Boser, J. S Denker, D. Henderson, R. E.. Howard, W. Howard, and L. D. Jackel. Hand- written digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems, 2, 1990.  S. Minsker. Plug-in approach to active learning. Journal of Machine Learning Research, pages  67-90, 2012.  Clayton Scott and Robert D Nowak. Minimax-optimal classification with dyadic decision trees.  Information Theory, IEEE Transactions on, 52(4):1335-1353, 2006.  L. Wang. Smoothness, disagreement coefficient, and the label complexity of agnostic active learn-  ing. Journal of Machine Learning Research, pages 2269-2292, 2011.  Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. In International Conference on Machine Learning, pages 912- 919, 2003a.  Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. Combining active learning and semi- supervised learning using Gaussian fields and harmonic functions. In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, 2003b. DASARATHY NOWAK ZHU  Acknowledgments  RN is supported in part by the National Science Foundation grant CCF1218189 and the National Institutes of Health grant 1 U54 AI117924-01; XZ is supported in part by National Science Foun- dation grants IIS 0916038, IIS 0953219, IIS 1216758, and the National Institutes of Health grant 1 U54 AI117924-01."}