{"1": "Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In  Advances in Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00b4emi Munos. Minimax regret bounds for reinforcement learning. In  Proceedings of the 34th International Conference on Machine Learning, pages 263-272, 2017.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances  in Neural Information Processing Systems, 2015.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of  Machine Learning Research, 11(Apr):1563-1600, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision pro-  cesses with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations.  In  Advances in Neural Information Processing Systems, 2016.  Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In Algorithmic Learning Theory (ALT), 2012.  Andrew Ng. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings  of the 16th International Conference on Machine Learning, 1999.  Ian Osband and Benjamin Van Roy. On lower bounds for regret  in reinforcement learning.  arXiv preprint  arXiv:1608.02732, 2016.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstrac-  tion in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.  4   OPEN PROBLEM: THE DEPENDENCE OF SAMPLE COMPLEXITY LOWER BOUNDS ON PLANNING HORIZON  varies in the family of MDPs of interest, the upper bound argument will be broken as there are HAS \u201c1-switch\u201d non-stationary policies. In fact, we find that such a situation can be created by adding a small amount of instantaneous reward to the lazy-chain style constructions. The difficulty is that the algorithm may not need to know the switching timing precisely. In the cases we have inspected, the algorithm can basically discretize [H] into intervals of length O((cid:15)H) and guarantee that one of those O(1/(cid:15)) switching timings is (cid:15)-optimal. Thus, the construction is still subject to a variant of the Monte-Carlo upper bound argument.  2.4. Alternative formulations  The results for regret minimization are in similar situations. The lower and upper bounds for the episodic setting are closed under reward uniformity and asymptotic assumptions (Azar et al., 2017). In the average-reward case, the notion of horizon is replaced by the MDP\u2019s diameter, D; here the lower and the upper bounds still have a gap of  D (Jaksch et al., 2010; Agrawal and Jia, 2017).  \u221a  We also welcome resolution of our problem in more realistic and challenging settings beyond tabular RL, such as rich observations and function approximation (Krishnamurthy et al., 2016; Jiang et al., 2017). While a richer setting enables more powerful lower bounds, existing work have not leveraged the power yet (e.g., Jiang et al., 2017, Thm 6 still uses a tabular construction).  Acknowledgements We thank Christoph Dann and John Langford for insightful discussions.  References  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In  Advances in Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00b4emi Munos. Minimax regret bounds for reinforcement learning. In  Proceedings of the 34th International Conference on Machine Learning, pages 263-272, 2017.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances  in Neural Information Processing Systems, 2015.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of  Machine Learning Research, 11(Apr):1563-1600, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision pro-  cesses with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations.  In  Advances in Neural Information Processing Systems, 2016.  Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In Algorithmic Learning Theory (ALT), 2012.  Andrew Ng. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings  of the 16th International Conference on Machine Learning, 1999.  Ian Osband and Benjamin Van Roy. On lower bounds for regret  in reinforcement learning.  arXiv preprint  arXiv:1608.02732, 2016.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstrac-  tion in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999."}