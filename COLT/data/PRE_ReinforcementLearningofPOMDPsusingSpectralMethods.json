{"1": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear  quadratic systems. In COLT, pages 1-26, 2011.  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pages 2312-2320, 2011.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  A. Atrash and J. Pineau. Efficient planning and tracking in pomdps with large observation spaces. In AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems, 2006.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement  learning. In Advances in neural information processing systems, pages 89-96, 2009.  J. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic programming. In S. Thrun, L.K. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems 16, pages 831-838. MIT Press, 2004.  Sevi Baltaoglu, Lang Tong, and Qing Zhao. Online learning and optimization of markov jump affine  models. arXiv preprint arXiv:1605.02213, 2016.  Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement In Proceedings of the 25th Annual Conference on  learning in weakly communicating MDPs. Uncertainty in Artificial Intelligence, 2009.  A.G. Barto, R.S. Sutton, and C.W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, SMC-13(5): 834-846, Sept 1983. ISSN 0018-9472. doi: 10.1109/TSMC.1983.6313077.  Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. J. Artif. Int. Res.,  15(1):319-350, November 2001. ISSN 1076-9757.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954-966, 2011.  Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near- optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003.  23   REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear  quadratic systems. In COLT, pages 1-26, 2011.  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pages 2312-2320, 2011.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  A. Atrash and J. Pineau. Efficient planning and tracking in pomdps with large observation spaces. In AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems, 2006.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement  learning. In Advances in neural information processing systems, pages 89-96, 2009.  J. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic programming. In S. Thrun, L.K. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems 16, pages 831-838. MIT Press, 2004.  Sevi Baltaoglu, Lang Tong, and Qing Zhao. Online learning and optimization of markov jump affine  models. arXiv preprint arXiv:1605.02213, 2016.  Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement In Proceedings of the 25th Annual Conference on  learning in weakly communicating MDPs. Uncertainty in Artificial Intelligence, 2009.  A.G. Barto, R.S. Sutton, and C.W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, SMC-13(5): 834-846, Sept 1983. ISSN 0018-9472. doi: 10.1109/TSMC.1983.6313077.  Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. J. Artif. Int. Res.,  15(1):319-350, November 2001. ISSN 1076-9757.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954-966, 2011.  Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near- optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003. AZIZZADENESHELI LAZARIC ANANDKUMAR  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework In Sanjoy Dasgupta and David Mcallester, editors, Proceedings of the 30th and applications. International Conference on Machine Learning (ICML-13), volume 28, pages 151-159. JMLR Workshop and Conference Proceedings, 2013.  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1-38, 1977.  M. Gheshlaghi-Azar, A. Lazaric, and E. Brunskill. Regret bounds for reinforcement learning with policy advice. In Proceedings of the European Conference on Machine Learning (ECML\u201913), 2013.  M. Gheshlaghi-Azar, A. Lazaric, and E. Brunskill. Resource-efficient stochastic optimization of a locally smooth function under correlated bandit feedback. In Proceedings of the Thirty-First International Conference on Machine Learning (ICML\u201914), 2014.  Mohammad Gheshlaghi azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra- mani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2220-2228. Curran Associates, Inc., 2013.  Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A pac rl algorithm for episodic pomdps. In Proceedings of the 19th International Conference on Artificial Intelligence and Statis- tics, pages 510-518, 2016.  William Hamilton, Mahdi Milani Fard, and Joelle Pineau. Efficient learning and planning with compressed predictive states. The Journal of Machine Learning Research, 15(1):3395-3439, 2014.  Milos Hauskrecht and Hamish Fraser. Planning treatment of ischemic heart disease with partially observable markov decision processes. Artificial Intelligence in Medicine, 18(3):221 - 244, 2000. ISSN 0933-3657.  Daniel J Hsu, Aryeh Kontorovich, and Csaba Szepesv\u00b4ari. Mixing time estimation in reversible markov chains from a single sample path. In Advances in Neural Information Processing Systems, pages 1459-1467, 2015.  Tommi Jaakkola, Satinder P. Singh, and Michael I. Jordan. Reinforcement learning algorithm for partially observable markov decision problems. In Advances in Neural Information Processing Systems 7, pages 345-352. MIT Press, 1995.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement  learning. J. Mach. Learn. Res., 11:1563-1600, August 2010. ISSN 1532-4435.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-  chine Learning, 49(2-3):209-232, 2002.  Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In Machine Learning:  ECML 2006, pages 282-293. Springer, 2006. REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS  Aryeh Kontorovich, Boaz Nadler, and Roi Weiss. On learning parametric-output hmms. arXiv  preprint arXiv:1302.6009, 2013.  Aryeh Kontorovich, Roi Weiss, et al. Uniform chernoff and dvoretzky-kiefer-wolfowitz-type in- equalities for markov chains and related processes. Journal of Applied Probability, 51(4):1100- 1113, 2014.  Leonid Aryeh Kontorovich, Kavita Ramanan, et al. Concentration inequalities for dependent ran- dom variables via the martingale method. The Annals of Probability, 36(6):2126-2158, 2008.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford.  Contextual-mdps for pac-  reinforcement learning with rich observations. arXiv preprint arXiv:1602.02722v1, 2016.  Steven M LaValle. Planning algorithms. Cambridge university press, 2006.  Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of pomdps under the expected average reward criterion. European Journal of Operational Research, 211(3):556- 567, 2011.  Michael L. Littman. Memoryless policies: Theoretical limitations and practical results. In Proceed- ings of the Third International Conference on Simulation of Adaptive Behavior : From Animals to Animats 3: From Animals to Animats 3, SAB94, pages 238-245, Cambridge, MA, USA, 1994. MIT Press. ISBN 0-262-53122-4.  Michael L. Littman, Richard S. Sutton, and Satinder Singh. Predictive representations of state. In In Advances In Neural Information Processing Systems 14, pages 1555-1561. MIT Press, 2001.  John Loch and Satinder P Singh. Using eligibility traces to find the best memoryless policy in  partially observable markov decision processes. In ICML, pages 323-331, 1998.  Omid Madani. On the computability of infinite-horizon partially observable markov decision pro-  cesses. In AAAI98 Fall Symposium on Planning with POMDPs, Orlando, FL, 1998.  Lingsheng Meng and Bing Zheng. The optimal perturbation bounds of the moore-penrose inverse  under the frobenius norm. Linear Algebra and its Applications, 432(4):956-963, 2010.  Andrew Y. Ng and Michael Jordan. Pegasus: A policy search method for large mdps and pomdps. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201900, pages 406-415, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860- 709-9.  P Ortner and R Auer. Logarithmic online regret bounds for undiscounted reinforcement learning.  Advances in Neural Information Processing Systems, 19:49, 2007.  Ronald Ortner, Odalric-Ambrym Maillard, and Daniil Ryabko. Selecting near-optimal approximate state representations in reinforcement learning. In Peter Auer, Alexander Clark, Thomas Zeug- mann, and Sandra Zilles, editors, Algorithmic Learning Theory, volume 8776 of Lecture Notes in Computer Science, pages 140-154. Springer International Publishing, 2014. ISBN 978-3-319- 11661-7. AZIZZADENESHELI LAZARIC ANANDKUMAR  Christos Papadimitriou and John N. Tsitsiklis. The complexity of markov decision processes. Math.  Oper. Res., 12(3):441-450, August 1987. ISSN 0364-765X.  Theodore J. Perkins. Reinforcement learning for POMDPs based on action values and stochastic optimization. In Proceedings of the Eighteenth National Conference on Artificial Intelligence and Fourteenth Conference on Innovative Applications of Artificial Intelligence (AAAI/IAAI 2002), pages 199-204. AAAI Press, 2002.  Shaowei Png, J. Pineau, and B. Chaib-draa. Building adaptive dialogue systems via bayes-adaptive pomdps. Selected Topics in Signal Processing, IEEE Journal of, 6(8):917-927, Dec 2012. ISSN 1932-4553. doi: 10.1109/JSTSP.2012.2229962.  P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2008.  Pascal Poupart and Craig Boutilier. Bounded finite state controllers.  In Sebastian Thrun,  Lawrence K. Saul, and Bernhard Sch\u00a8olkopf, editors, NIPS, pages 823-830. MIT Press, 2003.  Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. In Advances in  neural information processing systems, pages 1225-1232, 2007.  Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in partially observable markovian decision processes. In ICML, pages 284-292. Citeseer, 1994.  E. J. Sondik. The optimal control of partially observable Markov processes. PhD thesis, Stanford  University, 1971.  Le Song, Animashree Anandkumar, Bo Dai, and Bo Xie. Nonparametric estimation of multi-view  latent variable models. arXiv preprint arXiv:1311.3287, 2013.  Matthijs T.J. Spaan. Partially observable markov decision processes. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning, volume 12 of Adaptation, Learning, and Optimiza- tion, pages 387-414. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-27644-6.  Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press, 1998.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.  John K. Williams and Satinder P. Singh. Experimental results on learning stochastic memoryless policies for partially observable markov decision processes. In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, NIPS, pages 1073-1080. The MIT Press, 1998. REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS"}