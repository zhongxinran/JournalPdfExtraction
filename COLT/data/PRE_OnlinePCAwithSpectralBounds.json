{"1": "Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1565-1573, 2013.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of pca with capped msg. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1815-1823. 2013.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,  pca. Advances in Neural Information Processing Systems 26, pages 3174-3182. 2013.  11   ONLINE PCA  It follows that (cid:107)(I \u2212 Utm\u22121U T  (cid:107)(I \u2212 Utm\u22121U T tm\u22121)Btm(cid:107)2 > \u2206 and a direction entered U at time tm \u2264 t(cid:48).  F /d \u2265 \u2206 + \u03c1  tm\u22121)Xt:tm(cid:107)2  Theorem 11 Combining the above we get the following: assume Algorithm 3 received as input pa- rameters k, \u03b5 and a sketching algorithm with guarantee \u03c1, update time Tsketch(X, \u03c1) and a memory requirement of Ssketch(X, \u03c1). We have  1. The target dimension of the sketch is O(k log(n)/\u03b52). 2. The running time is bounded by Tsketch(X, \u03c1) + O(nnz(X)k log(n)/\u03b52) + O (cid:0)kd log(n)/\u03b53(cid:1) where nnz(X) is the number of non-zero entries in X. For sufficiently large n (as common in streaming scenarios) this quantity is in fact Tsketch(X, \u03c1) + O(nnz(X)k log(n)/\u03b52).  3. The space requirement of the algorithm is Ssketch(X, \u03c1) + O(kd log(n)/\u03b52).  4. The error of the output is bounded by  (cid:107)X\u2212(XY +)Y (cid:107)2 \u2264 \u03c32  k+1+5\u03b5\u03c32  1+O  (cid:16)(cid:112)  k log(n)/\u03b52 (cid:16)  \u03c1 + max  t  (cid:107)xt(cid:107)2(cid:17)(cid:17)  = \u03c32  k+1+O(\u03b5\u03c32  1) .  k+1 + \u03b5\u03c32  In some cases we have a crude approximation for \u2206\u2217 = \u03c32  Possible improvements: Having prior knowledge about the matrix, two potential improvements 1. By this can be made. we mean having knowledge of a scalar \u22060 such that \u22060 \u2264 \u2206\u2217 but \u22060 \u2265 \u2206\u2217/c, for some large constant c. If this happens to be the case we can initialize \u2206 to be \u22060 and the log(n) terms in the above theorems become log(c). The second improvement can be made when we have some lower bound 1 < \u03ba \u2264 \u03c32 k+1. First, notice that typically it makes sense to have an input k for which k+1 (cid:28) \u03c32 \u03c32 1, hence \u03ba can be potentially large. When having knowledge of such a parameter we can set the multiplicative update of \u2206 to grow by 1 + \u03b5\u03ba rather than 1 + \u03b5. The results stated above regarding the error guarantee remain the same as long as \u03ba \u2264 \u03c32 k+1; however, the running time, memory complexity and target dimension are decrease by a factor of max{1/\u03b5, \u03ba}. To conclude, in an optimistic, yet not unlikely scenario where we have knowledge of \u22060 = \u2126(\u2206\u2217) and \u03ba = \u2126(\u03b5\u22121) we get a target dimension of O(k/\u03b5).  1/\u03c32  1/\u03c32  References  Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1565-1573, 2013.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of pca with capped msg. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1815-1823. 2013.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,  pca. Advances in Neural Information Processing Systems 26, pages 3174-3182. 2013. KARNIN LIBERTY  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal components anal- ysis. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 887-901, 2015.  Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.  Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix approximation.  In APPROX-RANDOM, pages 292-303, 2006.  Petros Drineas and Ravi Kannan. Pass efficient algorithms for approximating large matrices. In Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms, pages 223- 232, 2003.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank In Proceedings of the 39th Annual Symposium on Foundations of Computer  approximations. Science, pages 370-, 1998.  Mina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima- tions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 707-717, 2014a.  Mina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima-  tions. In SODA, 2014b.  Mina Ghashami, Edo Liberty, Jeff M. Phillips, and David P. Woodruff. Frequent directions : Simple  and deterministic matrix sketching. CoRR, abs/1501.01711, 2015.  Edo Liberty. Simple and deterministic matrix sketching. In KDD, pages 581-588, 2013.  Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert. Ran- domized algorithms for the low-rank approximation of matrices. Proceedings of the National Academy of Sciences,, 104(51):20167-20172, December 2007.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming PCA. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2886-2894. 2013.  Jiazhong Nie, Wojciech Kot\u0142owski, and Manfred K. Warmuth. Online PCA with optimal regrets. In  ALT, pages 98-112, 2013.  Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geo-  metric functional analysis. J. ACM, 54(4), July 2007. ISSN 0004-5411.  Tam\u00b4as Sarl\u00b4os. Improved approximation algorithms for large matrices via random projections. In  FOCS, pages 143-152, 2006.  Manfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret bounds  that are logarithmic in the dimension, 2007."}