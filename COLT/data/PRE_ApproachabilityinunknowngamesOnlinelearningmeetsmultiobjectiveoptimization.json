{"1": "Y. Azar, U. Feige, M. Feldman, and M. Tennenholtz. Sequential decision making with vector  outcomes. In Proceedings of ITCS, 2014.  W. Beibei and K.J.R. Liu. Advances in cognitive radio networks: a survey. IEEE Journal of Selected  Topics in Signal Processing, 5(1):5-23, 2011.  A. Bernstein and N. Shimkin. Response-based approachability and its application to generalized  no-regret algorithms. arXiv:1312.7658 [cs.LG], 2014.  A. Bernstein, S. Mannor, and N. Shimkin. Opportunistic strategies for generalized no-regret prob-  lems. In Proceedings of COLT, pages 158-171, 2013.  D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathemat-  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK,  ics, 6:1-8, 1956.  2004.  13   APPROACHABILITY IN UNKNOWN GAMES  c : \u2206(A) \u00d7 \u2206(B) \u2192 Rg. (With no loss of generality we can assume that the payoff function takes values in a bounded nonnegative interval.) The vector m of our general formulation corresponds to  m(y) =  (cid:20) u( \u00b7 , y) c( \u00b7 , y)  (cid:21)  .  The payoff set P to be be approached given the constraints is [M, +\u221e), that is, payoffs are to be maximized given the constraints: P\u03b1 = [M \u2212 \u03b1, +\u221e). Abusing the notation by not distinguishing between m(y) and y, we denote the maximal payoff under the constraint by \u03c6(cid:63)(y) = max(cid:8)u(x, y) : x \u2208 \u2206(A) s.t. c(x, y) \u2208 \u0393(cid:9) .  This target function corresponds to (4) in the same way as \u03c6\u03a8 corresponds to (9). Mannor et al. (2009) exactly proceed as we did in Section 3: they first show that \u03c6(cid:63) is unachievable in general and then show that the relaxed goal cav[\u03c6(cid:63)] can be achieved. They propose a computationally complex strategy to do so (based on calibration) but Bernstein and Shimkin (2014) already noted that simpler and more tractable strategies could achieve cav[\u03c6(cid:63)] as well.  The target function \u03c6x(cid:63)  , which we proved above to be achievable, improves on cav[\u03c6(cid:63)], even though, as in the remark concluding Section 5.1, it is difficult to quantify in general how much we gain. One should look at specific examples to quantify the improvement from cav[\u03c6(cid:63)] to \u03d5\u03c8 (as we do in Mannor et al., 2014, Section C). The added value in our approach mostly lies in the versatility: we do not need to assume that some known game is taking place.  Vianney Perchet acknowledges funding from the ANR, under grants ANR-10-BLAN-0112 and ANR-13-JS01-0004-01. Shie Mannor was partially supported by the ISF under contract 890015. Gilles Stoltz would like to thank Investissements d\u2019Avenir (ANR-11-IDEX-0003 / Labex Ecodec / ANR-11-LABX-0047) for financial support.  Acknowledgments  References  Y. Azar, U. Feige, M. Feldman, and M. Tennenholtz. Sequential decision making with vector  outcomes. In Proceedings of ITCS, 2014.  W. Beibei and K.J.R. Liu. Advances in cognitive radio networks: a survey. IEEE Journal of Selected  Topics in Signal Processing, 5(1):5-23, 2011.  A. Bernstein and N. Shimkin. Response-based approachability and its application to generalized  no-regret algorithms. arXiv:1312.7658 [cs.LG], 2014.  A. Bernstein, S. Mannor, and N. Shimkin. Opportunistic strategies for generalized no-regret prob-  lems. In Proceedings of COLT, pages 158-171, 2013.  D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathemat-  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK,  ics, 6:1-8, 1956.  2004. N. Cesa-Bianchi and G. Lugosi. Potential-based algorithms in on-line prediction and game theory.  Machine Learning, 3(51):239-261, 2003.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with  expert advice. Machine Learning, 66(2/3):321-352, 2007.  S. de Rooij, T. van Erven, P.D. Gr\u00a8unwald, and W. Koolen. Follow the leader if you can, hedge if  you must. Journal of Machine Learning Research, 2014. In press.  E. Even-Dar, R. Kleinberg, S. Mannor, and Y. Mansour. Online learning for global cost functions.  In Proceedings of COLT, 2009.  J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer-Verlag, 2001.  T.-F. Hou. Approachability in a two-person game. The Annals of Mathematical Statistics, 42:  735-744, 1971.  C.-L. Hwang and A.S. Md Masud. Multiple Objective Decision Making, Methods and Applications:  a state-of-the-art survey. Springer-Verlag, 1979.  S. Mannor and J. N. Tsitsiklis. Approachability in repeated games: Computational aspects and a  Stackelberg variant. Games and Economic Behavior, 66(1):315-325, 2009.  S. Mannor, J.N. Tsitsiklis, and J.Y. Yu. Online learning with sample path constraints. Journal of  Machine Learning Research, 10:569-590, 2009.  S. Mannor, V. Perchet, and G. Stoltz. Approachability in unknown games: Online learning meets  multi-objective optimization. arXiv:1402.2043 [stat.ML], 2014.  K. Miettinen. Nonlinear Multiobjective Optimization. Springer, 1999.  H. Simon. Cognitive radio: brain-empowered wireless communications. IEEE Journal on Selected  Areas in Communications, 23(2):201-220, 2005.  X. Spinat. A necessary and sufficient condition for approachability. Mathematics of Operations  Research, 27:31-44, 2002. APPROACHABILITY IN UNKNOWN GAMES"}