{"1": "R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and  unlabeled data. Technical Report RC23462, IBM T.J. Watson Research Center, 2004.  M.-F. Balcan, S. Hanneke, and J. Wortman Vaughan. The true sample complexity of active learning.  Machine Learning, 80(2-3):111-139, September 2010.  J. Baxter. A bayesian/information theoretic model of learning to learn via multiple task sampling.  Machine Learning, 28:7-39, 1997.  J. Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:  149-198, 2000.  on Learning Theory, 2003.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In Conference  J. G. Carbonell. Learning by analogy: Formulating and generalizing plans from past experience. In R. S. Michalski, J. G.Carbonell, and T. M. Mitchell, editors, Machine Learning, An Artificial Intelligence Approach. Tioga Press, Palo Alto, CA, 1983.  J. G. Carbonell. Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, Machine Learning, An Artificial Intelligence Approach, Volume II. Morgan Kaufmann, 1986.  R. Caruana. Multitask learning. Machine Learning, 28:41-75, 1997.  L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. Springer, New York,  NY, USA, 2001.  805   TRANSFER LEARNING  5. Conclusions  We have shown that when learning a sequence of i.i.d. target concepts from a known VC class, with an unknown distribution from a known totally bounded family, transfer learning can lead to amortized expected sample complexity close to that achievable by an algorithm with direct knowl- edge of the the targets\u2019 distribution. Furthermore, the number of extra labeled examples per task, beyond what is needed for learning that task, is bounded by the VC dimension of the class. The key insight leading to this result is that the prior distribution is uniquely identifiable based on the joint distribution over the first VC dimension number of points. This is not necessarily the case for the distribution over any number of points less than the VC dimension. As a particularly interesting application, we note that in the context of active learning, transfer learning of this type can even lead to improvements in the asymptotic dependence on the desired error rate guarantee \u03b5 in the average expected sample complexity, and in particular can guarantee this average is o(1/\u03b5).  We extend our sincere thanks to Avrim Blum for several thought-provoking discussions on this topic.  Acknowledgments  References  R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and  unlabeled data. Technical Report RC23462, IBM T.J. Watson Research Center, 2004.  M.-F. Balcan, S. Hanneke, and J. Wortman Vaughan. The true sample complexity of active learning.  Machine Learning, 80(2-3):111-139, September 2010.  J. Baxter. A bayesian/information theoretic model of learning to learn via multiple task sampling.  Machine Learning, 28:7-39, 1997.  J. Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:  149-198, 2000.  on Learning Theory, 2003.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In Conference  J. G. Carbonell. Learning by analogy: Formulating and generalizing plans from past experience. In R. S. Michalski, J. G.Carbonell, and T. M. Mitchell, editors, Machine Learning, An Artificial Intelligence Approach. Tioga Press, Palo Alto, CA, 1983.  J. G. Carbonell. Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, Machine Learning, An Artificial Intelligence Approach, Volume II. Morgan Kaufmann, 1986.  R. Caruana. Multitask learning. Machine Learning, 28:41-75, 1997.  L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. Springer, New York,  NY, USA, 2001. YANG HANNEKE CARBONELL  T. Evgeniou and M. Pontil. Regularized multi-task learning.  In ACM SIGKDD Conference on  Knowledge Discovery and Data Mining, 2004.  T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of  Machine Learning Research, 6:615-637, 2005.  S. Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Machine Learning Depart-  ment, School of Computer Science, Carnegie Mellon University, 2009.  J. Kolodner (Ed). Case-Based Learning. Kluwer Academic Publishers, The Netherlands, 1993.  C. Micchelli and M. Pontil. Kernels for multi-task learning. In Advances in Neural Information  Processing 18, 2004.  M. J. Schervish. Theory of Statistics. Springer, New York, NY, USA, 1995.  D. L. Silver. Selective Transfer of Neural Network Task Knowledge. PhD thesis, Computer Science,  University of Western Ontario, 2000.  S. Thrun. Is learning the n-th thing any easier than learning the first? In In Advances in Neural  Information Processing Systems 8, 1996.  V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York,  1982.  M. M. Veloso and J. G. Carbonell. Derivational analogy in prodigy: Automating case acquisition,  storage and utilization. Machine Learning, 10:249-278, 1993.  L. Yang, S. Hanneke, and J. Carbonell. The sample complexity of self-verifying bayesian active  learning. Technical Report CMU-ML-10-105, Carnegie Mellon University, 2010.  Y. G. Yatracos. Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy.  The Annals of Statistics, 13:768-774, 1985."}