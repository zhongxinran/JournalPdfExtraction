{"1": "MA, 1996.  bridge, 2008.  L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc.  The 12th Int. Conf. Machine Learning, pages 30-37, 1995.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  D. P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large linear  systems. Journal of Computational and Applied Mathematics, 227(1):27-50, 2009.  V. S. Borkar. Stochastic Approximation: A Dynamic Viewpoint. Cambridge University Press, Cam-  V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation and  reinforcement learning. SIAM J. Control Optim., 38:447-469, 2000.  J. A. Boyan. Least-squares temporal difference learning.  In Proc. The 16th Int. Conf. Machine  Learning, pages 49-56, 1999.  C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: A survey and  comparison. Journal of Machine Learning Res., 15:809-883, 2014.  J. L. Doob. Stochastic Processes. John Wiley & Sons, New York, 1953.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.  M. Geist and B. Scherrer. Off-policy learning with eligibility traces: A survey. Journal of Machine  Learning Res., 15:289-333, 2014.  P. W. Glynn and D. L. Iglehart. Science, 35:1367-1392, 1989.  Importance sampling for stochastic simulations. Management  H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Uncon-  strained Systems. Springer-Verlag, New York, 1978.  H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer-Verlag, New York, 2nd edition, 2003.  H. R. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta,  2011.  A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy In Proc. Conf. Advances in Neural Information  learning with linear function approximation. Processing Systems (NIPS) 27, 2014.  A. R. Mahmood, H. Yu, M. White, and R. S. Sutton. Emphatic temporal-difference learning. In  European Workshops on Reinforcement Learning, Lille, France, 2015.  S. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press,  Cambridge, 2nd edition, 2009.  13   ON CONVERGENCE OF EMPHATIC TEMPORAL-DIFFERENCE LEARNING  References  MA, 1996.  bridge, 2008.  L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc.  The 12th Int. Conf. Machine Learning, pages 30-37, 1995.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  D. P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large linear  systems. Journal of Computational and Applied Mathematics, 227(1):27-50, 2009.  V. S. Borkar. Stochastic Approximation: A Dynamic Viewpoint. Cambridge University Press, Cam-  V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation and  reinforcement learning. SIAM J. Control Optim., 38:447-469, 2000.  J. A. Boyan. Least-squares temporal difference learning.  In Proc. The 16th Int. Conf. Machine  Learning, pages 49-56, 1999.  C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal differences: A survey and  comparison. Journal of Machine Learning Res., 15:809-883, 2014.  J. L. Doob. Stochastic Processes. John Wiley & Sons, New York, 1953.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.  M. Geist and B. Scherrer. Off-policy learning with eligibility traces: A survey. Journal of Machine  Learning Res., 15:289-333, 2014.  P. W. Glynn and D. L. Iglehart. Science, 35:1367-1392, 1989.  Importance sampling for stochastic simulations. Management  H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Uncon-  strained Systems. Springer-Verlag, New York, 1978.  H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer-Verlag, New York, 2nd edition, 2003.  H. R. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta,  2011.  A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy In Proc. Conf. Advances in Neural Information  learning with linear function approximation. Processing Systems (NIPS) 27, 2014.  A. R. Mahmood, H. Yu, M. White, and R. S. Sutton. Emphatic temporal-difference learning. In  European Workshops on Reinforcement Learning, Lille, France, 2015.  S. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press,  Cambridge, 2nd edition, 2009. J. Neveu. Discrete-Parameter Martingales. North-Holland, Amsterdam, 1975.  D. Precup, R. S. Sutton, and S. Dasgupta. Off-policy temporal-difference learning with function  approximation. In Proc. The 18th Int. Conf. Machine Learning, pages 417-424, 2001.  M. L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. John  Wiley & Sons, 1994.  R. S. Randhawa and S. Juneja. Combining importance sampling and temporal difference control variates to simulate Markov chains. ACM Trans. Modeling and Computer Simulation, 14(1): 1-30, 2004.  B. Scherrer. Should one compute the temporal difference fix point or minimize the Bellman resid- ual? The unified oblique projection view. In Proc. The 27th Int. Conf. Machine Learning, pages 959-966, 2010.  R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:  9-44, 1988.  R. S. Sutton. TD models: Modeling the world at a mixture of time scales. In Proc. The 12th Int.  Conf. Machine Learning, pages 531-539, 1995.  R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press, Cambridge, MA, 1998.  R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy  temporal-difference learning, 2015. http://arxiv.org/abs/1503.04269.  J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxi-  mation. IEEE Trans. Automat. Contr., 42(5):674-690, 1997.  R. S. Varga. Matrix Iterative Analysis. Springer-Verlag, Berlin, 2nd edition, 2000.  H. Yu. Least squares temporal difference methods: An analysis under general conditions. SIAM J.  Control Optim., 50:3310-3343, 2012.  H. Yu. On convergence of emphatic temporal-difference larning. Technical report, University of  Alberta, 2015. http://arxiv.org/abs/1506.02582.  H. Yu and D. P. Bertsekas. Weighted Bellman equations and their applications in approximate  dynamic programming. LIDS Technical Report 2876, MIT, 2012."}