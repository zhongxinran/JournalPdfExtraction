{"1": "Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4):267-283, 2017.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3:397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 116-120, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, EWRL 2018, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non- stationary rewards. In Advances in Neural Information Processing Systems 27, NIPS 2014, pages 199-207, 2014.  Lilian Besson and Emilie Kaufmann. What doubling tricks can and can\u2019t do for multi-armed bandits.  CoRR, abs/1803.06971, 2018. URL http://arxiv.org/abs/1803.06971.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  13   ADAPTIVELY TRACKING THE BEST ARM  5. Conclusion  Extending the work in (Auer et al., 2018), we have constructed the first algorithm for the stochastic multi-armed bandit problem with abrupt changes of the reward distributions that achieves optimal regret bounds without knowing the number of changes in advance. The main technical contribution is the delicate testing schedule of the apparently inferior arms. This testing is necessary to detect when a previously inferior arm becomes the best arm.  We note that our algorithm (without any change) also provides optimal regret bounds in terms of total variation. These optimal bounds have also been achieved in (Chen et al., 2019), which provides also optimal bounds for the more general stochastic contextual bandits setting.  Regarding the adversarial bandit setting, it remains an open problem to construct an algorithm  with optimal regret bounds without a priori tuning in respect to the number of arm changes.  This work has been supported by the Austrian Science Fund (FWF): I 3437-N33 in the framework of the CHIST-ERA ERA-NET (DELTA project).  Acknowledgments  References  Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4):267-283, 2017.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3:397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 116-120, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, EWRL 2018, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non- stationary rewards. In Advances in Neural Information Processing Systems 27, NIPS 2014, pages 199-207, 2014.  Lilian Besson and Emilie Kaufmann. What doubling tricks can and can\u2019t do for multi-armed bandits.  CoRR, abs/1803.06971, 2018. URL http://arxiv.org/abs/1803.06971.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. AUER GAJANE ORTNER  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In COLT 2012 - The 25th Annual Conference on Learning Theory, pages 42.1-42.23, 2012.  Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In 32nd Annual Conference on Learn- ing Theory (COLT), 2019.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, pages 1079-1087, 2019.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In Proceedings of the 22nd International Conference on Algorithmic Learning Theory, ALT 2011, pages 174-188. Springer, 2011.  C\u00b4edric Hartland, Sylvain Gelly, Nicolas Baskiotis, Olivier Teytaud, and Mich`ele Sebag. Multi- armed bandit, dynamic environments and meta-bandits. NIPS-2006 workshop, Online trading between exploration and exploitation, 2006.  Levente Kocsis and Csaba Szepesv\u00b4ari. Discounted UCB. 2nd PASCAL Challenges Workshop, 2006.  Dimitris E. Koulouriotis and A.S. Xanthopoulos. Reinforcement learning and evolutionary algo- rithms for non-stationary multi-armed bandit problems. Applied Mathematics and Computation, 196(2):913 - 922, 2008.  Haipeng Luo. Personal communication, 2019.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in non-stationary worlds. In Proceedings of the 31st Conference On Learning Theory, COLT 2018, pages 1739-1776, 2018.  Ronald Ortner, Daniil Ryabko, Peter Auer, and R\u00b4emi Munos. Regret bounds for restless Markov  bandits. Theoretical Computer Science, 558:62-76, 2014.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In Proceedings of the 31st International Conference on Machine Learning, ICML 2014, pages 1287-1295, 2014.  Alex Slivkins and Eli Upfal. Adapting to a changing environment: The Brownian restless bandits.  In 21st Conference on Learning Theory, COLT 2008, pages 343-354, 2008.  Jia Yuan Yu and Shie Mannor. Piecewise-stationary bandit problems with side observations. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, pages 1177-1184, 2009. ADAPTIVELY TRACKING THE BEST ARM"}