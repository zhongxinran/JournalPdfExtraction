{"1": "S. Agarwal and P. Niyogi. Stability and generalization of bipartite ranking algorithms.  In 18th  Annual Conference on Learning Theory, 2005.  S. Agarwal and P. Niyogi. Generalization bounds for ranking algorithms via algorithmic stability.  Journal of Machine Learning Research, 10:441-474, 2009.  13.12   WANG KHARDON PECHYONY JONES  \u03b3 \u2212 yt(cid:104)u, xt(cid:105), which implies yt(cid:104)u, xt(cid:105) (cid:62) \u03b3 \u2212 (cid:96)t.. We therefore have the lower bound  (cid:104)wt+1, u(cid:105) = (cid:104)wt, u(cid:105) + yt(cid:104)xt, u(cid:105)mt (cid:62) (cid:104)wt, u(cid:105) + (\u03b3 \u2212 (cid:96)t)mt  = (cid:104)wt, u(cid:105) + \u03b3mt \u2212 (cid:96)tmt (cid:62) (cid:104)wt, u(cid:105) + \u03b3mt \u2212 (cid:96)t  (\u2235 mt (cid:54) 1)  \u21d2 (cid:104)wn, u(cid:105) (cid:62)  \u03b3mt \u2212  (cid:96)t = \u03b3M \u2212 D1.  n (cid:88)  t=1  n (cid:88)  t=1  (21)  Combing the upper bound R2M with (21), we get (\u03b3M \u2212 D1)2 (cid:54) R2M. Solving the quadratic equation, we obtain the desired bound.  6. Conclusion and Future work  In this paper, we provide generalization bounds for online learners using pairwise loss functions and apply these to analyze the risk of an online Bipartite ranking algorithm. There are several directions for possible future work. From an empirical perspective, although the random Online AUC Maximization (OAM) is simple and easy to implement, it seems that it does not maintain buffers in an optimal way. Intuitively, one might want to store \u201csupport ranking vectors\u201d that help to build the correct ranker instead of using a random buffer. We are currently exploring ideas on building a smart buffer to improve its performance.  From the theoretical point of view, one direction is to improve the current bounds to achieve faster convergence rates. Alternatively, one can analyze Algorithm 5 from a totally different point of view. Under the batch setting, Clemenc\u00b8on et al. (2008) already provide fast convergence rates for the empirical risk minimizer. Since Algorithm 5 is in fact a stochastic gradient descent algo- rithm to minimize the U -statistic, using online convex programming techniques (Zinkevich, 2003; Shalev-Shwartz, 2007), one can show that the regret is small. Combining this with the batch results automatically yields risk bounds for the algorithm. It is interesting to compare this approach to the one proposed in this paper in terms of the risk bounds that can be obtained. However, it is important to note that the approach in this paper is more general in two ways. First we only assume that the loss function is Lipschitz instead of convex. Second, the ensemble of hypotheses can be produced by an arbitrary online learning algorithm, not just stochastic gradient descent.  We would like to thank anonymous reviewers for their constructive comments. YW and DP also thank Nicol`o Cesa-Bianchi for early discussion. YW and RK were partly supported by NSF grant IIS-0803409. Part of this research was done when YW was an intern at Akamai Technologies in 2011.  Acknowledgments  References  S. Agarwal and P. Niyogi. Stability and generalization of bipartite ranking algorithms.  In 18th  Annual Conference on Learning Theory, 2005.  S. Agarwal and P. Niyogi. Generalization bounds for ranking algorithms via algorithmic stability.  Journal of Machine Learning Research, 10:441-474, 2009.  13.12   ONLINE LEARNING WITH PAIRWISE LOSS FUNCTIONS  S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds for the area  under the ROC curve. Journal of Machine Learning Research, 6:393-425, 2005.  U. Brefeld and T. Scheffer. AUC maximizing support vector learning. In ICML 2005 Workshop on  ROC Analysis in Machine Learning, 2005.  N. Cesa-Bianchi and C. Gentile. Improved risk tail bounds for on-line algorithms. IEEE Transac-  tions on Information Theory, 54(1):386-390, 2008.  N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning  algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  S. Clemenc\u00b8on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of u-statistics.  Annals of Statistics, 36(2):844-874, 2008.  F. Cucker and S. Smale. On the mathematical foundations of learning. Bull. Am. Math. Soc., 39(1):  1-49, 2002.  Press, 2007.  Verlag, 1996.  F. Cucker and D. Zhou. Learning theory: an approximation theory viewpoint. Cambridge University  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer  Y. Freund and R. Schapire. Large margin classification using the perceptron algorithm. Machine  Learning, 37:277-296, 1999.  Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efficient boosting algorithm for combining  preferences. Journal of Machine Learning Research, 4:933-969, 2003.  C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265-299, 2003.  T. Joachims. Optimizing search engines using clickthrough data. In Eighth ACM SIGKDD interna-  tional conference on Knowledge discovery and data mining, pages 133-142. ACM, 2002.  S. Kakade and A. Tewari. On the generalization ability of online strongly convex programming  algorithms. Advances in Neural Information Processing Systems, 2009.  M. Kearns, M. Li, L. Pitt, and L. G. Valiant. Recent results on boolean concept learning.  In Proceedings of the Fourth International Workshop on Machine Learning, pages 337-352, 1987.  N. Littlestone. Mistake bounds and logarithmic linear-threshold learning algorithms. PhD thesis,  University of California at Santa Cruz, 1990.  T. Peel, S. Anthoine, L. Ralaivola, et al. Empirical Bernstein inequalities for u-statistics. In Neural  Information Processing Systems (NIPS), 2010.  C. Rudin. The p-norm push: A simple convex ranking algorithm that concentrates at the top of the  list. Journal of Machine Learning Research, 10:2233-2271, 2009.  C. Rudin, C. Cortes, M. Mohri, and R. Schapire. Margin-based ranking meets boosting in the  middle. 18th Annual Conference on Learning Theory, 2005.  13.13   WANG KHARDON PECHYONY JONES  S. Shalev-Shwartz. Online learning: Theory, algorithms, and applications. PhD thesis, Hebrew  University, 2007.  S. Shalev-Shwartz and Y. Singer. A new perspective on an old perceptron algorithm. 18th Anual  Conference on Learning Theory, 2005.  T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In 18th Annual  Conference on Learning Theory, 2005.  P. Zhao, S. Hoi, R. Jin, and T. Yang. Online AUC Maximization. In 28th international conference  on Machine learning, 2011.  M. Zinkevich. Online Convex Programming and Generalized Infinitesimal Gradient Ascent. In 20th  international conference on Machine learning, 2003.  13.14   ONLINE LEARNING WITH PAIRWISE LOSS FUNCTIONS"}