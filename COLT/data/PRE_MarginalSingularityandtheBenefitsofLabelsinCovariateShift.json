{"1": "Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of Statistics, 35(2):608-633, 2007.  Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target samples. In International Conference on Algorithmic Learning Theory, pages 139-153. Springer, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort- man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010a.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain adap- tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129-136, 2010b.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In Inter-  national Conference on Machine Learning, pages 1870-1879, 2015.  Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye.  Joint transfer and batch-mode active learning. In Sanjoy Dasgupta and David McAllester, editors, Pro- ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 253-261, 2013.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation.  In  Advances in neural information processing systems, pages 2456-2464, 2011.  Corinna Cortes, Mehryar Mohri, and Andr\u00b4es Munoz Medina. Adaptation based on general- ized discrepancy. Machine Learning Research, forthcoming. URL http://www. cs. nyu. edu/\u02dc mohri/pub/daj. pdf.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions. arXiv preprint arXiv:1411.0894, 2014.  Pascal Germain, Amaury Habrard, Franc\u00b8ois Laviolette, and Emilie Morvant. A pac-bayesian ap- proach for domain adaptation with specialization to linear classifiers. In International Conference on Machine Learning, pages 738-746, 2013.  Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Sch\u00a8olkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pages 601-608, 2007.  Samory Kpotufe. Lipschitz density-ratios, structured data, and data-driven tuning.  In Artificial  Intelligence and Statistics, pages 1320-1328, 2017.  4   MINIMAX TRANSFER  References  Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of Statistics, 35(2):608-633, 2007.  Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target samples. In International Conference on Algorithmic Learning Theory, pages 139-153. Springer, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort- man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010a.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain adap- tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129-136, 2010b.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In Inter-  national Conference on Machine Learning, pages 1870-1879, 2015.  Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye.  Joint transfer and batch-mode active learning. In Sanjoy Dasgupta and David McAllester, editors, Pro- ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 253-261, 2013.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation.  In  Advances in neural information processing systems, pages 2456-2464, 2011.  Corinna Cortes, Mehryar Mohri, and Andr\u00b4es Munoz Medina. Adaptation based on general- ized discrepancy. Machine Learning Research, forthcoming. URL http://www. cs. nyu. edu/\u02dc mohri/pub/daj. pdf.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions. arXiv preprint arXiv:1411.0894, 2014.  Pascal Germain, Amaury Habrard, Franc\u00b8ois Laviolette, and Emilie Morvant. A pac-bayesian ap- proach for domain adaptation with specialization to linear classifiers. In International Conference on Machine Learning, pages 738-746, 2013.  Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Sch\u00a8olkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pages 601-608, 2007.  Samory Kpotufe. Lipschitz density-ratios, structured data, and data-driven tuning.  In Artificial  Intelligence and Statistics, pages 1320-1328, 2017. MINIMAX TRANSFER  Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the benefits of labels in  covariate-shift. arXiv preprint arXiv:1803.01833, 2018.  Oleg V Lepski and VG Spokoiny. Optimal pointwise adaptive methods in nonparametric estimation.  The Annals of Statistics, pages 2512-2546, 1997.  Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds  and algorithms. arXiv preprint arXiv:0902.3430, 2009a.  Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial  r\u00b4enyi divergence. Intelligence, pages 367-374. AUAI Press, 2009b.  Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with drift- ing distributions. In International Conference on Algorithmic Learning Theory, pages 124-138. Springer, 2012.  Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence.  Dataset shift in machine learning. The MIT Press, 2009.  Avishek Saha, Piyush Rai, Hal Daum\u00b4e, Suresh Venkatasubramanian, and Scott L DuVall. Active su- pervised domain adaptation. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 97-112. Springer, 2011.  Richard J Samworth et al. Optimal weighted nearest neighbour classifiers. The Annals of Statistics,  40(5):2733-2763, 2012.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adapta- tion. In Advances in neural information processing systems, pages 1433-1440, 2008.  Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine  learning. Cambridge University Press, 2012.  Liu Yang, Steve Hanneke, and Jaime Carbonell. A theory of transfer learning with applications to  active learning. Machine learning, 90(2):161-189, 2013."}