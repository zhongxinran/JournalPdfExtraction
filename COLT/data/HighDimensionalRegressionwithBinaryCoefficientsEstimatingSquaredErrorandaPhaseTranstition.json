{"1": "We consider a sparse linear regression model $Y=X\u03b2^*+W$ where $X$ is $n\\times p$ matrix Gaussian i.i.d.  entries, $W$ is $n\\times 1$ noise vector with i.i.d. mean zero Gaussian entries and standard deviation $\u03c3$, and $\u03b2^*$ is $p\\times 1$ binary vector with support size (sparsity)  $k$. Using a  novel conditional second moment method  we obtain a tight up to a multiplicative constant approximation of the optimal squared error $\\min_\u03b2\\|Y-X\u03b2\\|_2$, where the minimization is over all $k$-sparse binary vectors $\u03b2$. The approximation reveals interesting structural properties of the underlying regression problem. In particular, \\beginenumerate \\item [(a)] We establish that $n^*=2k\\log p/\\log (2k/\u03c3^2+1)$ is a phase transition point with the following \u201call-or-nothing\u201d property. When $n$ exceeds $n^*$,  $(2k)^-1\\|\\beta_2-\u03b2^*\\|_0\u22480$, and when $n$ is  below $n^*$, $(2k)^-1\\|\\beta_2-\u03b2^*\\|_0\u22481$, where $\\beta_2$ is the optimal solution achieving the smallest squared error. As a corollary  $n^*$ is the asymptotic threshold for recovering $\u03b2^*$  information theoretically. Note that $n^*$ is asymptotically below the threshold  $n_\\text{LASSO}/CS=(2k+\u03c3^2)\\log p$, above which the LASSO and Compressive Sensing methods are able to recover $\u03b2^*$. \\item [(b)] We compute the squared error for an intermediate problem $\\min_\u03b2\\|Y-X\u03b2\\|_2$ where the minimization is restricted to vectors $\u03b2$ with $\\|\u03b2-\u03b2^*\\|_0=2k \u03b6$, for some fixed ratio $\u03b6\u2208[0,1]$. We show that a lower bound part $\u0393(\u03b6)$ of the estimate, which essentially corresponds to the estimate based on the first moment method, undergoes a phase transition  at three different thresholds, namely $n_\\text{inf},1=\u03c3^2\\log p$, which is information theoretic bound for recovering $\u03b2^*$ when $k=1$ and $\u03c3$ is large, then at $n^*$ and finally at $n_\\text{LASSO}/CS$. \\item [(c)] We establish a certain Overlap Gap Property (OGP) on the space of all $k$-sparse binary vectors $\u03b2$ when $n\\le ck\\log p$ for sufficiently small constant $c$. By drawing a connection with a similar OGP exhibited by many randomly generated constraint satisfaction problems and statistical physics models, we conjecture that OGP is the source of algorithmic hardness of solving the minimization problem $\\min_\u03b2\\|Y-X\u03b2\\|_2$ in the regime $n"}