{"1": "bridge University Press, 2009.  HAL-01098505, 2014.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Technical report,  Maria-Florina Balcan and Christopher Berlind. A new perspective on learning linear separators with large lqlp margins. Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 68-76, 2014.  Peter L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on information theory, 44(2):525-536, 1998.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, pages 463-482, 2003.  Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. Advances in neural information processing systems, pages 123-130, 2005.  Xavier Glorot Antoine Bordes and Yoshua Bengio. Deep sparse rectifier networks. AISTATS, 2011.  Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. STOC, 2014.  Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231-  283, 1981.  Sham M Kakade, Karthik Sridharan, and AmbujTewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. Advances in neural information processing systems, pages 793-800, 2009.  Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of  halfspaces. FOCS, pages 553-562, 2006.  Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the  generalization error of combined classifiers. Annals of Statistics, pages 1-50, 2002.  13   NORM-BASED CAPACITY CONTROL IN NEURAL NETWORKS  Acknowledgments  This research was partially supported by NSF grant IIS-1302662 and an Intel ICRI-CI award. We thank the COLT anonymous reviewers for pointing out an error in the statement of Lemma 15 and suggesting other corrections.  References  bridge University Press, 2009.  HAL-01098505, 2014.  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Technical report,  Maria-Florina Balcan and Christopher Berlind. A new perspective on learning linear separators with large lqlp margins. Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 68-76, 2014.  Peter L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on information theory, 44(2):525-536, 1998.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, pages 463-482, 2003.  Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. Advances in neural information processing systems, pages 123-130, 2005.  Xavier Glorot Antoine Bordes and Yoshua Bengio. Deep sparse rectifier networks. AISTATS, 2011.  Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. STOC, 2014.  Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231-  283, 1981.  Sham M Kakade, Karthik Sridharan, and AmbujTewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. Advances in neural information processing systems, pages 793-800, 2009.  Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of  halfspaces. FOCS, pages 553-562, 2006.  Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the  generalization error of combined classifiers. Annals of Statistics, pages 1-50, 2002. NEYSHABUR TOMIOKA SREBRO  Wee Sun Lee, Peter L Bartlett, and Robert C Williamson. Efficient agnostic learning of neural Information Theory, IEEE Transactions on, 42(6):2118-2132,  networks with bounded fan-in. 1996.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. Advances in Neural Information Processing Systems, pages 855-863, 2014.  Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.  ICML, 2010.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q.V. Le, P. Nguyen, A. Senior, V. Van- ICASSP,  houcke, J. Dean, and G.E. Hinton. On rectified linear units for speech processing. 2013."}