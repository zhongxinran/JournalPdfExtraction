{"1": "J. Abernethy, E. Hazan, and A. Rakhlin.  Interior-point methods for full-information and bandit  online learning. Information Theory, IEEE Transactions on, 58(7):4164-4175, July 2012.  J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M.-F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 807-823. JMLR.org, 2014.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and Gy. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In J. L. Balc\u00b4azar, P. M. Long, and F. Stephan, editors, Proceedings of the 17th International Conference on Algorithmic Learning Theory (ALT 2006), volume 4264 of Lecture Notes in Computer Science, pages 229-243, Berlin, Heidelberg, October 7-10 2006. Springer. ISBN 978-3-540-46649-9.  J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceed-  ings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Jour-  nal of Machine Learning Research, 11:2635-2686, 2010.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Regret in online combinatorial optimization. Mathematics  of Operations Research, 39:31-45, 2014.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM J. Comput., 32(1):48-77, 2002a. ISSN 0097-5397.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64:48-75, 2002b. doi: doi:10.1006/jcss.2001.1795.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  New York, NY, USA, 2006.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. In S. Dasgupta and A. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, pages 237-246. Omnipress, June 18-21 2009.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,  78:1404-1422, 2012.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. In Proceedings of the 18th Annual Conference on Learning Theory (COLT-2005), pages 217-232. Springer, 2005.  W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and ap- In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International plications. Conference on Machine Learning (ICML 2013), volume 28 of JMLR Workshop and Conference Proceedings, pages 151-159, 2013.  13   FIRST-ORDER REGRET BOUNDS FOR COMBINATORIAL SEMI-BANDITS  References  J. Abernethy, E. Hazan, and A. Rakhlin.  Interior-point methods for full-information and bandit  online learning. Information Theory, IEEE Transactions on, 58(7):4164-4175, July 2012.  J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M.-F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 807-823. JMLR.org, 2014.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and Gy. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In J. L. Balc\u00b4azar, P. M. Long, and F. Stephan, editors, Proceedings of the 17th International Conference on Algorithmic Learning Theory (ALT 2006), volume 4264 of Lecture Notes in Computer Science, pages 229-243, Berlin, Heidelberg, October 7-10 2006. Springer. ISBN 978-3-540-46649-9.  J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceed-  ings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Jour-  nal of Machine Learning Research, 11:2635-2686, 2010.  J.-Y. Audibert, S. Bubeck, and G. Lugosi. Regret in online combinatorial optimization. Mathematics  of Operations Research, 39:31-45, 2014.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM J. Comput., 32(1):48-77, 2002a. ISSN 0097-5397.  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64:48-75, 2002b. doi: doi:10.1006/jcss.2001.1795.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  New York, NY, USA, 2006.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. In S. Dasgupta and A. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, pages 237-246. Omnipress, June 18-21 2009.  N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,  78:1404-1422, 2012.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. In Proceedings of the 18th Annual Conference on Learning Theory (COLT-2005), pages 217-232. Springer, 2005.  W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and ap- In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International plications. Conference on Machine Learning (ICML 2013), volume 28 of JMLR Workshop and Conference Proceedings, pages 151-159, 2013. NEU  L. Devroye, G. Lugosi, and G. Neu. Prediction by random-walk perturbation.  In S. I. Shalev- Shwartz, S., editor, Proceedings of the 25th Annual Conference on Learning Theory, pages 460- 473, 2013.  Y. Gai, B. Krishnamachari, and R. Jain. Combinatorial network optimization with unknown vari- ables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Trans- actions on Networking, 20(5):1466-1478, Oct 2012.  A. Gy\u00a8orgy, T. Linder, G. Lugosi, and Gy.. Ottucs\u00b4ak. The on-line shortest path problem under partial monitoring. Journal of Machine Learning Research, 8:2369-2403, 2007. ISSN 1532-4435.  J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the theory of games, 3:  97-139, 1957.  E. Hazan and S. Kale. Extracting certainty from uncertainty: regret bounded byvariation incosts.  Machine Learning, 80(2-3):165-188, 2010.  E. Hazan and S. Kale. Better algorithms for benign bandits. The Journal of Machine Learning  Research, 12:1287-1311, 2011.  M. Hutter and J. Poland. Prediction with expert advice by following the perturbed leader for general weights. In S. Ben-David, J. Case, and A. Maruoka, editors, Proceedings of the 15th International Conference on Algorithmic Learning Theory (ALT), volume 3244 of Lecture Notes in Computer Science, pages 279-293. Springer, 2004.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71:291-307, 2005.  T. Koc\u00b4ak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 613-621, 2014.  W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging structured concepts. In Proceedings of the  23rd Annual Conference on Learning Theory (COLT), pages 93-105, 2010.  B. Kveton, Z. Wen, A. Ashkan, and Cs. Szepesv\u00b4ari. Tight regret bounds for stochastic combinatorial  semi-bandits. In AISTATS, 2015.  G. Neu and G. Bart\u00b4ok. An efficient algorithm for learning with semi-bandit feedback. In S. Jain, R. Munos, F. Stephan, and T. Zeugmann, editors, Proceedings of the 24th International Confer- ence on Algorithmic Learning Theory, volume 8139 of Lecture Notes in Computer Science, pages 234-248. Springer, 2013.  J. Poland. FPL analysis for adaptive bandits.  In In 3rd Symposium on Stochastic Algorithms,  Foundations and Applications (SAGA\u201905), pages 58-69, 2005.  A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In S. I. Shalev-Shwartz, S., editor, Proceedings of the 25th Annual Conference on Learning Theory, pages 993-1019, 2013. FIRST-ORDER REGRET BOUNDS FOR COMBINATORIAL SEMI-BANDITS  S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize : From value to algorithms. In  Advances in Neural Information Processing Systems 25, pages 2150-2158. 2012.  A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in online optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 810-818, 2014.  G. Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD  thesis, Universit\u00b4e Paris-Sud, 2005.  T. Van Erven, M. Warmuth, and W. Kot\u0142owski. Follow the leader with dropout perturbations. In M.- F. Balcan and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of JMLR Proceedings, pages 949-974. JMLR.org, 2014."}