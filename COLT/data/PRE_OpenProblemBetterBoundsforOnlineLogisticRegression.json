{"1": "Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Mach. Learn., 69, December 2007.  H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-  mization. In COLT, 2010.  ICML, 2003.  Martin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In  Appendix A. The Exp-Concavity of the Logistic Loss  Theorem 1 The logistic loss function (cid:96)(wt; xt, yt) = log(1 + exp(\u2212ytwt \u00b7 xt)), from Eq. (1), is \u03b1-exp-concave with \u03b1 = exp(\u2212D/2) over set W = {w | (cid:107)w(cid:107)2 \u2264 D/2} when (cid:107)xt(cid:107)2 \u2264 1 and yt \u2208 {\u22121, 1}.  Proof Recall that a function (cid:96) is \u03b1-exp-concave if (cid:79)2 exp(\u2212\u03b1(cid:96)(w)) (cid:22) 0. When (cid:96)(w) = g(w \u00b7 x) for x \u2208 Rn, we have (cid:79)2 exp(\u2212\u03b1(cid:96)(w)) = (cid:79)2f (cid:48)(cid:48)(z)xx(cid:62), where f (z) = exp(\u2212\u03b1g(z)). For the logistic loss, we have g(z) = log(1 + exp(z)) (without loss of generality, we consider a negative example), and so f (z) = (1 + exp(z))\u2212\u03b1. Then,  f (cid:48)(cid:48)(z) = \u03b1ez(1 + ez)\u2212\u03b1\u22122(\u03b1ez \u2212 1).  We need the largest \u03b1 such that f (cid:48)(cid:48)(z) \u2264 0, given a \ufb01xed z. We can see by inspection that \u03b1 = 0 is a zero. Since ez(1 + ez)\u2212\u03b1\u22122 > 0, from the term (\u03b1ez \u2212 1) we conclude \u03b1 = e\u2212z is the largest value of \u03b1 where f (cid:48)(cid:48)(z) \u2264 0. Note that z = wt \u00b7 xt, and so |z| \u2264 D/2 since (cid:107)xt(cid:107)2 \u2264 1, and so taking the worst case over wt \u2208 W and xt with (cid:107)xt(cid:107)2 \u2264 1, we have \u03b1 = exp(\u2212D/2).  44.3   OPEN PROBLEM: ONLINE LOGISTIC REGRESSION  Thus, if we let T \u2212 = {t | yt = \u22121}, we have  (cid:88)  t\u2208T \u2212  ft(wt) \u2212 ft(wt+1) \u2264  NT(cid:88)  N =0  1 N + \u03bb  \u2264  +  1 \u03bb  NT(cid:88)  N =1  1 N  1 \u03bb  \u2264  + log(NT ) + 1.  Applying a similar argument to rounds with positive labels and summing over the rounds with  positive and negative labels independently gives  Regret \u2264 \u03bb(|w\u2217| + 2 log 2) + log(PT ) + log(NT ) +  + 2.  2 \u03bb  Note log(PT ) + log(NT ) \u2264 2 log T . We wish to compete with w\u2217 where |w\u2217| \u2264 D/2, so we can choose \u03bb = 1\u221a  which gives  D/2  \u221a  Regret \u2264 O(  D + log T ).  References  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Mach. Learn., 69, December 2007.  H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-  mization. In COLT, 2010.  ICML, 2003.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In"}