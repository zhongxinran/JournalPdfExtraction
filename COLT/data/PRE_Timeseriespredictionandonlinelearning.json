{"1": "A. Agarwal and J.C. Duchi. The generalization ability of online algorithms for dependent data.  IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5:13-51, 1995.  Pierre Alquier and Olivier Wintenberger. Model selection for weakly dependent time series fore-  casting. Technical Report 2010-39, Centre de Recherche en Economie et Statistique, 2010.  Pierre Alquier, Xiaoyin Li, and Olivier Wintenberger. Prediction of time series by statistical learn-  ing: general losses and fast rates. Dependence Modelling, 1:65-93, 2014.  Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction.  In proceedings of COLT, 2013.  proceedings of ICML, 2015.  243-252, 1992.  Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In  Peter L. Bartlett. Learning with a slowly changing distribution. In proceedings of COLT, pages  Rakesh D. Barve and Phil M. Long. On the complexity of learning from drifting distributions.  Information and Computation, 138(2):101-123, 1997.  15   TIME SERIES PREDICTION AND ONLINE LEARNING  As in Section 4.1, the convex optimization problem in (10) can be solved using a standard projected subgradient algorithm where at each iteration a DC-algorithm of Tao and An (1998) is used to compute the discrepancy, if H and L are convex. As before, this DC-algorithm is guaranteed to be optimal if L is the squared loss and H is a set of linear hypothesis. Furthermore, for linear hypotheses with the squared loss and (cid:100)discHA(q) in the objective, the same analysis as in Section 4.1 can be used.  5. Conclusion  Time series prediction is a fundamental learning problem. We presented a series of results exploiting its recent analysis in statistical learning theory in the general scenario of non-stationary non-mixing Kuznetsov and Mohri (2015) and other existing regret-based analysis and guarantees from the broad on-line learning literature. This combination of the benefits of different approaches can lead to a variety of rich problems and solutions in learning theory that we hope this work will promote and stimulate.  This work was partly funded by NSF IIS-1117591 and CCF-1535987.  Acknowledgments  References  A. Agarwal and J.C. Duchi. The generalization ability of online algorithms for dependent data.  IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5:13-51, 1995.  Pierre Alquier and Olivier Wintenberger. Model selection for weakly dependent time series fore-  casting. Technical Report 2010-39, Centre de Recherche en Economie et Statistique, 2010.  Pierre Alquier, Xiaoyin Li, and Olivier Wintenberger. Prediction of time series by statistical learn-  ing: general losses and fast rates. Dependence Modelling, 1:65-93, 2014.  Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction.  In proceedings of COLT, 2013.  proceedings of ICML, 2015.  243-252, 1992.  Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In  Peter L. Bartlett. Learning with a slowly changing distribution. In proceedings of COLT, pages  Rakesh D. Barve and Phil M. Long. On the complexity of learning from drifting distributions.  Information and Computation, 138(2):101-123, 1997. KUZNETSOV MOHRI  Shai Ben-David, Gyora M. Benedek, and Yishay Mansour. A parametrization scheme for classifying  models of learnability. In Proceedings of COLT, pages 285-302, 1989.  Patrizia Berti and Pietro Rigo. A Glivenko-Cantelli theorem for exchangeable random variables.  Statistics & Probability Letters, 32(4):385 - 391, 1997.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2:499-526, 2002.  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past posteri-  ors. In proceedings of COLT, 2001.  George Edward Pelham Box and Gwilym Jenkins. Time Series Analysis, Forecasting and Control.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Holden-Day, 1990.  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Trans. Inf. Theory, 50(9), 2004.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. Mirror descent meets fixed  share (and feels no regret). In proceedings of NIPS, pages 989-997, 2012.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. An online learning-based framework for track-  ing. In UAI, 2010.  Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and  algorithm for regression. Theor. Comput. Sci., 2014.  Koby Crammer, Yishay Mansour, Eyal Even-Dar, and Jennifer Wortman Vaughan. Regret mini-  mization with concept drift. In proceedings of COLT, 2010.  Eyal Even-Dar, Yishay Mansour, and Jennifer Wortman. Regret minimization with concept drift.  In Proceedings of COLT, 2010.  Opt., 23(4):493-513, 1985.  R. Fletcher. On minimizing the maximum eigenvalue of a symmetric matrix. SIAM J. Control and  C. Helmberg and F. Oustry. Bundle methods to minimize the maximum eigenvalue function. In Handbook of Semidefinite Programming: Theory, Algorithms, and Applications. Kluwer Aca- demic Publishers, 2000.  Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32(2), 1998.  Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. JMLR, 2001.  Florian Jarre. An interior-point method for minimizing the maximum eigenvalue of a linear combi-  nation of matrices. SIAM J. Control Optim., 31(5):1360-1377, 1993.  Wouter M Koolen, Alan Malek, Peter L Bartlett, and Yasin Abbasi. Minimax time series prediction.  In proceedings of NIPS, 2015. TIME SERIES PREDICTION AND ONLINE LEARNING  Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for time series prediction with non-  stationary processes. In proceedings of ALT, 2014.  Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non-  stationary time series. In proceedings of NIPS, 2015.  Nick Littlestone. From on-line to batch learning. In Proceedings of COLT, pages 269-284, 1989.  Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learn-  ing, pages 5-34, 2000.  D.S. Modha and E. Masry. Memory-universal prediction of stationary random processes. Informa-  tion Theory, IEEE Transactions on, 44(1):117-133, Jan 1998.  Mehryar Mohri and Andres Mu\u02dcnoz Medina. New analysis and algorithm for learning with drifting  distributions. In proceedings of ALT, 2012.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.  In proceedings of NIPS, 2009.  Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary \u03d5-mixing and \u03b2-mixing  processes. Journal of Machine Learning Research, 11:789-814, 2010.  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.  The MIT Press, 2012.  Edward Moroshko and Koby Crammer. Weighted last-step min-max algorithm with improved sub-  logarithmic regret. In proceedings of ALT, 2012.  Edward Moroshko and Koby Crammer. A last-step regression algorithm for non-stationary online  Edward Moroshko, Nina Vaits, and Koby Crammer. Second-order non-stationary online learning  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Math. Pro-  Michael L. Overton. On minimizing the maximum eigenvalue of a symmetric matrix. SIAM J.  learning. In AISTATS, 2013.  for regression. JMLR, 2015.  gram., 110:245-259, 2007.  Matrix Anal. Appl., 9(2), 1988.  data. In GRC, 2010.  Vladimir Pestov. Predictive PAC learnability: A paradigm for learning from exchangeable input  Alexander Rakhlin and Karthik Sridharan. Hierarchies of relaxations for online prediction problems  with evolving constraints. In proceedings of COLT, 2015.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages,  combinatorial parameters, and learnability. In proceedings of NIPS, 2010.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic, constrained,  and smoothed adversaries. In proceedings of NIPS, 2011. KUZNETSOV MOHRI  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform  martingale laws of large numbers. Probability Theory and Related Fields, 2015a.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complex-  ities. JMLR, 16(1), January 2015b.  Cosma Shalizi and Aryeh Kontorovitch. Predictive PAC learning and process decompositions. In  proceedings of NIPS, 2013.  of NIPS, 2009.  Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. In proceedings  Pham Dinh Tao and Le Thi Hoai An. A D.C. optimization algorithm for solving the trust-region  subproblem. SIAM Journal on Optimization, 8(2):476-505, 1998.  M. Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Networks  and Control Systems. Springer-Verlag New York, Inc., 1997.  Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals  of Probability, 22(1):94-116, 1994."}