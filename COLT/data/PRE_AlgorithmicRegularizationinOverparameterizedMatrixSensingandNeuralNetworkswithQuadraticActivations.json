{"1": "Rados\u0142aw Adamczak, Alexander Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Quantita- tive estimates of the convergence of the empirical covariance matrix in log-concave ensembles. Journal of the American Mathematical Society, 23(2):535-561, 2010.  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. Technical report, Technical Report Preprint, 2017.  S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global Optimality of Local Search for Low Rank  Matrix Recovery. ArXiv e-prints, May 2016.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.  Sgd learns over- parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.  Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.  Comptes Rendus Mathematique, 346(9-10):589-592, 2008.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Yuxin Chen, Yuejie Chi, and Andrea J Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Transactions on Information Theory, 61(7): 4034-4059, 2015.  Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Par- seval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854-863, 2017.  Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize  for deep nets. arXiv preprint arXiv:1703.04933, 2017.  Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.  R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June  2017.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in Neural Information Processing Systems (NIPS), 2016. URL http://arxiv. org/abs/1605.07272.  13   ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  References  Rados\u0142aw Adamczak, Alexander Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Quantita- tive estimates of the convergence of the empirical covariance matrix in log-concave ensembles. Journal of the American Mathematical Society, 23(2):535-561, 2010.  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. Technical report, Technical Report Preprint, 2017.  S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global Optimality of Local Search for Low Rank  Matrix Recovery. ArXiv e-prints, May 2016.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.  Sgd learns over- parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.  Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.  Comptes Rendus Mathematique, 346(9-10):589-592, 2008.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Yuxin Chen, Yuejie Chi, and Andrea J Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Transactions on Information Theory, 61(7): 4034-4059, 2015.  Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Par- seval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854-863, 2017.  Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize  for deep nets. arXiv preprint arXiv:1703.04933, 2017.  Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.  R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June  2017.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in Neural Information Processing Systems (NIPS), 2016. URL http://arxiv. org/abs/1605.07272. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.  Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre- bro. Implicit regularization in matrix factorization. arXiv preprint arXiv:1705.09280, 2017.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In 5th International Conference on  Learning Representations (ICLR 2017), 2017.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.  CoRR, abs/1609.05191, 2016. URL http://arxiv.org/abs/1609.05191.  Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. 2017.  Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe- ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.  Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances  in neural information processing systems, pages 950-957, 1992.  Richard Kueng, Holger Rauhut, and Ulrich Terstiege. Low rank matrix recovery from rank one  measurements. Applied and Computational Harmonic Analysis, 42(1):88-116, 2017.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014.  Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk mini- mization. Advances in Computational Mathematics, 25(1):161-193, 2006.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On  the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac- bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.  Behnam Neyshabur, Srinadh Bhojanapalli, and Nati Srebro. Exploring generalization in deep learn-  ing. In Advances in Neural Information Processing Systems, pages 5943-5952, 2017b.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning  Research, 12:3413-3430, 2011. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635-2670, 2010.  M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape  of over-parameterized shallow neural networks. ArXiv e-prints, July 2017.  Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees  for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.  Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable  data. arXiv preprint arXiv:1710.10345, 2017.  Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In ICML, 2013.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference  on Computational Learning Theory, pages 545-560. Springer, 2005.  Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In  Advances in neural information processing systems, pages 2645-2653, 2011.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. arXiv preprint  The marginal value of adaptive gradient methods in machine learning. arXiv:1705.08292, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.  Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-  connected neural networks. In Artificial Intelligence and Statistics, pages 83-91, 2017.  Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using  burer-monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016.  Kai Zhong, Prateek Jain, and Inderjit S Dhillon. Efficient matrix sensing using rank-1 gaussian mea- surements. In International Conference on Algorithmic Learning Theory, pages 3-18. Springer, 2015. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS"}