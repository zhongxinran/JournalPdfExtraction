{"1": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information  12  012345\u22124\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA FLAMMARION BACH  Figure 3: Quadratic optimization with regression noise. Left \u03c3 = 1, r = 1. Right \u03c3 = 0.1, r = 10.  Figure 4: Least-Square Regression. Left \u03c3 = 1, r = 1. Right \u03c3 = 0.1, r = 10.  6. Conclusion  We have provided a joint analysis of averaging and acceleration for non-strongly-convex quadratic functions in a single framework, both with noiseless and noisy gradients. This allows us to define a class of algorithms that can benefit simultaneously from the known improvements of averaging and acceleration: faster forgetting of initial conditions (for acceleration), and better robustness to noise when the noise covariance is proportional to the Hessian (for averaging).  Our current analysis of our class of algorithms in Eq. (5), that considers two different affine combinations of previous iterates (instead of one for traditional acceleration), is limited to quadratic functions; an extension of its analysis to all smooth or self-concordant-like functions would widen its applicability. Similarly, an extension to least-squares regression with natural heteroscedastic stochastic gradient, as suggested by our simulations, would be an interesting development.  This work was partially supported by the MSR-Inria Joint Centre and a grant by the European Research Council (SIERRA project 239993). The authors would like to thank Aymeric Dieuleveut for helpful discussions.  Acknowledgments  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information012345\u22124\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Structured noisy gradients, d=20  OAOA\u2212atAGDAccGDAC\u2212SASAGEAccRDA012345\u22123\u22122\u221210log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA012345\u22124\u221220log10(n)log10[f(\u03b8)\u2212f(\u03b8*)]Least\u2212Square Regression, d=20  OAOA\u2212atAGDAC\u2212SASAGEAccRDA FROM AVERAGING TO ACCELERATION  Theory, 58(5):3235-3249, 2012.  1998.  L. Arnold. Random dynamical systems. Springer Monographs in Mathematics. Springer-Verlag,  F. Bach and E. Moulines. Non-Asymptotic Analysis of Stochastic Approximation Algorithms for  Machine Learning. In Advances in Neural Information Processing Systems, 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems, December 2013.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  A. d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM J. Optim., 19(3):1171-  1183, 2008.  A. D\u00b4efossez and F. Bach. Constant step size least-mean-square: Bias-variance trade-offs and opti-  mal sampling distributions. Technical Report 1412.0156, arXiv, 2014.  O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with  inexact oracle. Math. Program., 146(1-2, Ser. A):37-75, 2014.  A. Dieuleveut and F. Bach. Non-parametric Stochastic Approximation with Large Step sizes. Tech-  nical Report 1408.0361, arXiv, August 2014.  C. Hu, W. Pan, and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online  learning. In Advances in Neural Information Processing Systems, 2009.  G. Lan. An optimal method for stochastic composite optimization. Math. Program., 133(1-2, Ser.  A):365-397, 2012.  Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).  Soviet Mathematics Doklady, 27(2):372-376, 1983.  Y. Nesterov. Introductory Lectures on Convex Optimization, volume 87 of Applied Optimization.  Kluwer Academic Publishers, Boston, MA, 2004. A basic course.  Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser.  B):125-161, 2013.  B. O\u2019Donoghue and E. Cand`es. Adaptive restart for accelerated gradient schemes. Foundations of  Computational Mathematics, pages 1-18, 2013.  J. M. Ortega and W. C. Rheinboldt. Iterative solution of nonlinear equations in several variables, volume 30 of Classics in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2000.  B. T. Polyak. Some methods of speeding up the convergence of iteration methods. {USSR} Com-  putational Mathematics and Mathematical Physics, 4(5):1-17, 1964. FLAMMARION BACH  B. T. Polyak. Introduction to Optimization. Translations Series in Mathematics and Engineering.  Optimization Software, Inc., Publications Division, New York, 1987.  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J.  Control Optim., 30(4):838-855, 1992.  M. Schmidt, N. Le Roux, and F. Bach. Convergence Rates of Inexact Proximal-Gradient Methods In Advances in Neural Information Processing Systems, December  for Convex Optimization. 2011.  W. Su, S. Boyd, and E. Cand`es. A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient Method: Theory and Insights. In Advances in Neural Information Processing Systems, 2014.  A. B. Tsybakov. Optimal rates of aggregation. In Proceedings of the Annual Conference on Com-  putational Learning Theory, 2003.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J.  Mach. Learn. Res., 11:2543-2596, 2010."}