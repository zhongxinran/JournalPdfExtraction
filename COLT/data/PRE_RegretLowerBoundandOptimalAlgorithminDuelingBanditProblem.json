{"1": "R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, 27:1054-1078, 1995.  Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-  dits. In ICML, pages 856-864, 2014.  Peter Auer, Nicol\u00b4o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, 2002.  Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach In Proceedings of the 2010 Eurographics/ACM SIGGRAPH  to procedural animation design. Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103-112, 2010.  S\u00b4ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit\u00b4e des Sciences et  Technologie de Lille - Lille I, June 2010.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and  beyond. In COLT, pages 359-376, 2011.  Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni In In Preference Learning (PL-09)  Semeraro. Preference learning in recommender systems. ECML/PKDD-09 Workshop, 2009.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. Transactions on Information Systems, 31(4):17:1-43, 2013.  Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded  Support Models. In COLT, pages 67-79, 2010.  Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order responses.  In KDD, pages 583-588, 2003.  1091-1114, 09 1987.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):  T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985.  Microsoft Research. Microsoft Learning to Rank Datasets, 2010. URL http://research.  microsoft.com/en-us/projects/mslr/.  Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on  learning to rank for information retrieval. Inf. Retr., 13(4):346-374, 2010.  Tanguy Urvoy, Fabrice Cl\u00b4erot, Rapha\u00a8el Feraud, and Sami Naamane. Generic exploration and k-  armed voting bandits. In ICML, pages 91-99, 2013.  Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241-248, 2011.  13   REGRET LOWER BOUND AND OPTIMAL ALGORITHM IN DUELING BANDIT PROBLEM  References  R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, 27:1054-1078, 1995.  Nir Ailon, Zohar Shay Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal ban-  dits. In ICML, pages 856-864, 2014.  Peter Auer, Nicol\u00b4o Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, 2002.  Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach In Proceedings of the 2010 Eurographics/ACM SIGGRAPH  to procedural animation design. Symposium on Computer Animation, SCA 2010, Madrid, Spain, 2010, pages 103-112, 2010.  S\u00b4ebastien Bubeck. Bandits Games and Clustering Foundations. Theses, Universit\u00b4e des Sciences et  Technologie de Lille - Lille I, June 2010.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and  beyond. In COLT, pages 359-376, 2011.  Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Giovanni In In Preference Learning (PL-09)  Semeraro. Preference learning in recommender systems. ECML/PKDD-09 Workshop, 2009.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. Transactions on Information Systems, 31(4):17:1-43, 2013.  Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded  Support Models. In COLT, pages 67-79, 2010.  Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order responses.  In KDD, pages 583-588, 2003.  1091-1114, 09 1987.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist., 15(3):  T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in  Applied Mathematics, 6(1):4-22, 1985.  Microsoft Research. Microsoft Learning to Rank Datasets, 2010. URL http://research.  microsoft.com/en-us/projects/mslr/.  Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. LETOR: A benchmark collection for research on  learning to rank for information retrieval. Inf. Retr., 13(4):346-374, 2010.  Tanguy Urvoy, Fabrice Cl\u00b4erot, Rapha\u00a8el Feraud, and Sami Naamane. Generic exploration and k-  armed voting bandits. In ICML, pages 91-99, 2013.  Yisong Yue and Thorsten Joachims. Beat the mean bandit. In ICML, pages 241-248, 2011. KOMIYAMA HONDA KASHIMA NAKAGAWA  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits  problem. In COLT, 2009.  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits  problem. J. Comput. Syst. Sci., 78(5):1538-1556, 2012.  Omar Zaidan and Chris Callison-Burch. Crowdsourcing translation: Professional quality from non- In The 49th Annual Meeting of the Association for Computational Linguistics professionals. (ACL): Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Port- land, Oregon, USA, pages 1220-1229, 2011.  Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R\u00b4emi Munos. Relative confidence sam-  pling for efficient on-line ranker evaluation. In WSDM, pages 73-82, 2014a.  Masrour Zoghi, Shimon Whiteson, R\u00b4emi Munos, and Maarten de Rijke. Relative upper confidence  bound for the k-armed dueling bandit problem. In ICML, pages 10-18, 2014b.  Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale  online ranker evaluation. In WSDM, 2015."}