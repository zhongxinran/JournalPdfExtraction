{"1": "Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  S\u00b4eebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed bandits. In Proceedings of The 30th International Conference on Machine Learning, pages 258- 265, 2013.  20   r \u2265 s, we have:  (cid:32) +\u221e (cid:88)  O  r=s+1  |BADr\u22121,s| \u00b7 (ln |Scur| + ln \u03b4\u22121  r )\u03b5\u22122 r  = O  (cid:33)  (cid:32) +\u221e (cid:88)  r=s+1  1 8r\u2212s \u00b7 |BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln r)\u03b5\u22122  r  (cid:33)  = O (cid:0)|BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s(cid:1)  Putting them together, we can see the number of samples incurred by UniformSample is bounded by:  (cid:32)+\u221e (cid:88)  O  (|BADs| + |OPTs|) \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s  ,  (cid:33)  s=1 which simplifies to O (cid:0)(cid:80) e )(cid:1) . Finally, we consider the number of samples taken by PAC-SamplePrune. Noticing nopt = rank(Mcur), the number of samples is O(|Scur|(ln nopt + ln \u03b4\u22121 r ). So PAC-SamplePrune does not affect the sample complexity, and we finish our proof.  e (ln k + ln \u03b4\u22121 + ln ln \u2206\u22121  e\u2208S \u2206\u22122  r )\u03b5\u22122  5. Future Work  In this paper, we present nearly-optimal algorithms for both the exact and PAC versions of the pure- exploration problem subject to a matroid constraint in a stochastic multi-armed bandit game: given a set of arms with a matroid constraint on them, pick a basis of the matroid whose weight (the sum of expectations over arms in this basis) is as large as possible, with high probability.  An immediate direction for investiation is to extend our results to other polynomial-time-computable combinatorial constraints: s-t paths, matchings (or more generally, the intersection of two matroid- s), etc. The model also extends to NP-hard combinatorial constraints, but there we would likely compare our solution against \u03b1-approximate solutions, instead of the optimal solution. Considering non-linear functions of the means is another natural next step. Yet another, perhaps more challeng- ing, direction is to consider stochastic optimization problems, where the solution may depend on other details of the distributions than just the means.  References  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  S\u00b4eebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed bandits. In Proceedings of The 30th International Conference on Machine Learning, pages 258- 265, 2013. PURE EXPLORATION OF MULTI-ARMED BANDIT UNDER MATROID CONSTRAINTS [EXTENDED ABSTRACT]  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Sciences, 78(5):1404-1422, 2012.  arXiv:1511.03774, 2015.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and System  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- In Advances in Neural Information Processing Systems, pages  ration of multi-armed bandits. 379-387, 2014.  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework In Proceedings of the 30th International Conference on Machine Learning,  and applications. pages 151-159, 2013.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research, 7:1079-1105, 2006.  RH Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals of  Mathematical Statistics, pages 36-72, 1964.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and S\u00b4ebastien Bubeck. Multi- In Advances in Neural Information Processing Systems, pages  bandit best arm identification. 2222-2230, 2011.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Pro- cessing Systems, pages 3212-3220, 2012.  Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett. Improved learning complexity in combinatorial pure exploration bandits. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 1004-1012, 2016.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. COLT, 2014.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  David R Karger. Random sampling and greedy sparsification for matroid optimization problems.  Mathematical Programming, 82(1-2):41-81, 1998. David R Karger, Philip N Klein, and Robert E Tarjan. A randomized linear-time algorithm to find  minimum spanning trees. Journal of the ACM (JACM), 42(2):321-328, 1995.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. arXiv preprint arXiv:1407.4443, 2014.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004.  Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms. Chapman & Hall/CRC, 2010.  Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Select-  ed Papers, pages 169-177. Springer, 1985.  Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identification with applications In Proceedings of the 31st International Conference on Machine Learning  to crowdsourcing. (ICML-14), pages 217-225, 2014."}