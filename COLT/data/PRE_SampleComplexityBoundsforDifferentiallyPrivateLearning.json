{"1": "R. Agrawal and R. Srikant. Privacy-preserving data mining. SIGMOD Rec., 29(2):439\u2013450,  2000. ISSN 0163-5808. doi: http://doi.acm.org/10.1145/335191.335438.  Lars Backstrom, Cynthia Dwork, and Jon M. Kleinberg. Wherefore art thou r3579x?: anonymized social networks, hidden patterns, and structural steganography. In Carey L. Williamson, Mary Ellen Zurko, Peter F. Patel-Schneider, and Prashant J. Shenoy, editors, WWW, pages 181\u2013190. ACM, 2007. ISBN 978-1-59593-654-7.  K. Ball. An elementary introduction to modern convex geometry. In Silvio Levy, editor,  Flavors of Geometry, volume 31. 1997.  171   Sample Complexity Bounds for Differentially Private Learning  Theorem 12 Let H be a hypothesis class, D be a distribution over X , X be an i.i.d. sample from D of size m, and A be a learning algorithm that guarantees \u03b1-label privacy and outputs a hypothesis in H. Let d(cid:48)(cid:48) := ddim4(cid:15)(H, \u03c1D) \u2265 1. If (cid:15) \u2264 \u2206/2 and  m \u2264  (d(cid:48)(cid:48) \u2212 1) log 2 \u03b1  where \u2206 is the diameter of (H, \u03c1D), then there exists h\u2217 \u2208 H such that with probability at least 1/2 over the random choice of X and internal randomness of A, the hypothesis hA returned by A(SX,h\u2217) has classification error  Prx\u223cD [hA(x) (cid:54)= h\u2217(x)] > (cid:15).  In other words, any \u03b1-label private algorithm for learning a hypothesis in H with error at most (cid:15) \u2264 \u2206/2 must use at least (d(cid:48)(cid:48) \u2212 1) log(2)/\u03b1 examples. Theorem 12 uses ideas similar to those in (Beimel et al., 2010), but the result is stronger in that it applies to \u03b1-label privacy and continuous data domains. A detailed proof is provided in"}