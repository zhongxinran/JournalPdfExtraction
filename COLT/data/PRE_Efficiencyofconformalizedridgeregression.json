{"1": "577-580, 1966.  We are grateful to Albert Shiryaev for inviting us in September 2013 to Kolmogorov\u2019s dacha in Komarovka, where this project was conceived, and to Glenn Shafer for his advice about terminol- ogy. Thanks to the reviewers for their comments. The first author has been partially supported by the Laboratory for Structural Methods of Data Analysis in Predictive Modeling, Moscow Institute of Physics and Technology, Russian Government grant (agreement 11.G34.31.0073). The second author has been partially supported by EPSRC (grant EP/K033344/1).  R. Raj Bahadur. A note on quantiles in large samples. Annals of Mathematical Statistics, 37:  Raymond J. Carroll. On the distribution of quantiles of residuals in a linear model. Technical Report Mimeo Series No. 1161, Department of Statistics, University of North Carolina at Chapel Hill, March 1978. Available from http://www.stat.ncsu.edu/information/library/mimeo.php.  Samprit Chatterjee and Ali S. Hadi. Sensitivity Analysis in Linear Regression. Wiley, New York,  1988.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzy\u02d9zak, and Harro Walk. A Distribution-Free Theory of  Nonparametric Regression. Springer, New York, 2002.  Harold V. Henderson and Shayle R. Searle. On deriving the inverse of a sum of matrices. SIAM  Review, 23:53-60, 1981.  12   BURNAEV VOVK  coefficients whose maximum is o(1) as n \u2192 \u221e (this uses the assumption 1 earlier).  n X (cid:48)  nXn \u2192 \u03a3 made  A more intuitive (but not necessarily simpler) proof can be obtained by noticing that \u02c6wn \u2212 w  and the residuals are asymptotically (precisely when a = 0) independent.  7. Conclusion  The results of this paper are asymptotic; it would be very interesting to obtain their non-asymptotic counterparts. In non-asymptotic settings, however, it is not always true that conformalized ridge re- gression loses little in efficiency as compared with the Bayesian prediction interval; this is illustrated in Vovk et al. (2005), Section 8.5, and illustrated and explained in Vovk et al. (2009). The main dif- ference is that CRR and Bayesian predictor start producing informative predictions after seeing a different number of observations. CRR, like any other conformal predictor (or any other method whose validity depends only on the IID assumption), starts producing informative predictions only after the number of observations exceeds the inverse significance level 1/(cid:15). After this theoretical lower bound is exceeded, however, the difference between CRR and Bayesian predictions quickly becomes very small.  Another interesting direction of further research is to extend our results to kernel ridge regres-  sion.  Acknowledgements  References  577-580, 1966.  We are grateful to Albert Shiryaev for inviting us in September 2013 to Kolmogorov\u2019s dacha in Komarovka, where this project was conceived, and to Glenn Shafer for his advice about terminol- ogy. Thanks to the reviewers for their comments. The first author has been partially supported by the Laboratory for Structural Methods of Data Analysis in Predictive Modeling, Moscow Institute of Physics and Technology, Russian Government grant (agreement 11.G34.31.0073). The second author has been partially supported by EPSRC (grant EP/K033344/1).  R. Raj Bahadur. A note on quantiles in large samples. Annals of Mathematical Statistics, 37:  Raymond J. Carroll. On the distribution of quantiles of residuals in a linear model. Technical Report Mimeo Series No. 1161, Department of Statistics, University of North Carolina at Chapel Hill, March 1978. Available from http://www.stat.ncsu.edu/information/library/mimeo.php.  Samprit Chatterjee and Ali S. Hadi. Sensitivity Analysis in Linear Regression. Wiley, New York,  1988.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzy\u02d9zak, and Harro Walk. A Distribution-Free Theory of  Nonparametric Regression. Springer, New York, 2002.  Harold V. Henderson and Shayle R. Searle. On deriving the inverse of a sum of matrices. SIAM  Review, 23:53-60, 1981. EFFICIENCY OF CONFORMALIZED RIDGE REGRESSION  Jing Lei and Larry Wasserman. Distribution free prediction bands for nonparametric regression.  Journal of the Royal Statistical Society B, 76:71-96, 2014.  Ronald H. Randles, Thomas P. Hettmansperger, and George Casella. Introduction to the Special  Issue: Nonparametric statistics. Statistical Science, 19:561, 2004.  George A. F. Seber and Alan J. Lee. Linear Regression Analysis. Wiley, Hoboken, NJ, second  edition, 2003.  349-376, 2013a.  Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Press, Cambridge, 1998.  Vladimir Vovk. Conditional validity of inductive conformal predictors. Machine Learning, 92:  Vladimir Vovk. Kernel ridge regression. In Bernhard Sch\u00a8olkopf, Zhiyuan Luo, and Vladimir Vovk, editors, Empirical Inference: Festschrift in Honour of Vladimir N. Vapnik, chapter 11, pages 105-116. Springer, Berlin, 2013b.  Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World.  Springer, New York, 2005.  Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. On-line predictive linear regression. An-  nals of Statistics, 37:1566-1590, 2009.  Larry Wasserman. Frasian inference. Statistical Science, 26:322-325, 2011."}