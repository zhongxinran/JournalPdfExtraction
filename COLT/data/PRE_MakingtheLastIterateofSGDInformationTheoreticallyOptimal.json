{"1": "Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-50  on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in  Machine Learning, 8(3-4):231-357, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for  non-smooth stochastic gradient descent. arXiv preprint arXiv:1812.05217, 2018.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1): 2489-2512, 2014.  3   LAST ITERATE OF SGD  shown to achieve information theoretically optimal error rates in the convex and strongly convex settings when averaging of iterates is used (Nemirovsky and Yudin (1983),Zinkevich (2003),Cesa- Bianchi et al. (2004), Kakade and Tewari (2009), Epoch GD in Hazan and Kale (2014) , SGD Rakhlin et al. (2012) and Lacoste-Julien et al. (2012)). The question of the last iterate was first considered in Shamir and Zhang (2013) and it gives a bound of O( log T\u221a T ) in expectation T for the general case and strongly convex case respectively. Harvey et al. (2018) show matching high (cid:1) in the general case and O (cid:0) 1 probability bounds and show that for the standard step sizes (O t in the strongly convex case), the logarithmic-suboptimal bounds are tight.  ) and O( log T  (cid:16) 1\u221a  (cid:17)  t  2. Conclusions and Discussion  We studied the fundamental question of sub-optimality of the last point of SGD/GD for general non-smooth convex functions as well as for strongly-convex functions. We proposed a novel step-size sequence that leads to information theoretically optimal rates in both the above mentioned settings. Our result proves a more general result for any \u201cmodified step-size\u201d of a decaying standard step-size, and uses a novel technique of tracking best iterate in each time-interval and ensuring that the later iterates do not significantly deviate from the best iterate in the previous time interval. We also provide a high-probability bound using a super-martingale technique from Harvey et al. (2018). Simulations show that our step-size indeed leads to better last point than the standard step-size sequences.  Our approach fundamentally exploits an assumption that we apriori know the total number of iterations T . Hence, our result does not provide an any-time algorithm. In contrast, existing any-time results have an extra log T multiplicative factor in the sub-optimality. We conjecture that this gap is fundamental and every any-time algorithm would suffer from the extra log T factor.  This research was partially supported by ONR N00014-17-1-2147 and MIT-IBM Watson AI Lab.  Acknowledgments  References  Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-50  on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in  Machine Learning, 8(3-4):231-357, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for  non-smooth stochastic gradient descent. arXiv preprint arXiv:1812.05217, 2018.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1): 2489-2512, 2014. LAST ITERATE OF SGD  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.  Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method  efficiency in optimization. 1983.  Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  Alexander Rakhlin, Ohad Shamir, Karthik Sridharan, et al. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, volume 12, pages 1571-1578. Citeseer, 2012.  Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated  sub-gradient solver for svm. Mathematical programming, 127(1):3-30, 2011.  Ohad Shamir. Open problem: Is averaging needed for strongly convex stochastic gradient descent?  In Conference on Learning Theory, pages 47-1, 2012.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver- gence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71-79, 2013.  Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM, 2004.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003."}