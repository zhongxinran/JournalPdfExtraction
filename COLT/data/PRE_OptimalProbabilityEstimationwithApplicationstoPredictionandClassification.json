{"1": "J. Acharya, H. Das, H. Mohimani, A. Orlitsky, and Shengjun Pan. Exact calculation of  pattern probabilities. In ISIT, pages 1498 -1502, June 2010.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing.  JMLR - Proceedings Track, 19:47-68, 2011.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, S. Pan, and A.T. Suresh. Competitive classification and closeness testing. JMLR - Proceedings Track, 23:22.1-22.18, 2012a.  J. Acharya, H. Das, and A. Orlitsky. Tight bounds on profile redundancy and distinguisha-  bility. In NIPS, 2012b.  T. Batu. Testing properties of distributions. PhD thesis, Cornell University, 2001.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions  are close. In FOCS, pages 259-269, 2000.  S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification : A survey of some  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Ulisses Braga-Neto. Classification and error estimation for discrete data. Pattern Recogni-  tion, 10(7):446-462, 2009.  13   Optimal Probability Estimation with Applications to Prediction and Classification  5.7. Lower bounds on estimation  We now lower bound the rate of convergence. We construct an explicit distribution such that with probability \u2265 1 \u2212 n\u22121 the total variation distance is (cid:101)\u2126(n\u22121/4). By Pinsker\u2019s inequality, this implies that the KL divergence is (cid:101)\u2126(n\u22121/2). Note that since distance is (cid:101)\u2126(n\u22121/4) with probability close to 1, the expected distance is also (cid:101)\u2126(n\u22121/4). def= (cid:112) \u03c0 2 i log1.5 n symbols with probability pi  def= (cid:98)i2 log3 n(cid:99) Let p be a distribution with ni , and ni symbols with probability pi + i . c1 and c2 are constants such that the sum of probabilities is 1. We sketch the proof and leave the details to the full version of the paper. Proof [sketch of Theorem 3] The distribution p has the following properties.  n1/4 log9/8 n  n1/4 log9/8 n  n , for c1  \u2264 i \u2264 c2  n  \u2022 Let R = \u222ai{npi, npi + 1 . . . npi + i} for c1  n1/4 log9/8 n  \u2264 i \u2264 c2  n1/4 log9/8 n  . For every \u00b5 \u2208 R,  Pr(\u03a6\u00b5 = 1) \u2265 1/3.  \u2022 If \u03a6\u00b5 = 1, then the symbol that has appeared \u00b5 times has probability pi or pi + i n  with almost equal probability.  \u2022 Label-invariant estimators cannot distinguish between the two cases, and hence incur  an error of (cid:101)\u2126(i/n) = (cid:101)\u2126(n\u22123/4) for a constant fraction of multiplicities \u00b5 \u2208 R.  The total number of multiplicities in R is n1/4 \u00b7 n1/4 = n1/2. Multiplying by the error for each multiplicity yields the bound (cid:101)\u2126(n\u22121/4).  References  J. Acharya, H. Das, H. Mohimani, A. Orlitsky, and Shengjun Pan. Exact calculation of  pattern probabilities. In ISIT, pages 1498 -1502, June 2010.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing.  JMLR - Proceedings Track, 19:47-68, 2011.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, S. Pan, and A.T. Suresh. Competitive classification and closeness testing. JMLR - Proceedings Track, 23:22.1-22.18, 2012a.  J. Acharya, H. Das, and A. Orlitsky. Tight bounds on profile redundancy and distinguisha-  bility. In NIPS, 2012b.  T. Batu. Testing properties of distributions. PhD thesis, Cornell University, 2001.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions  are close. In FOCS, pages 259-269, 2000.  S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification : A survey of some  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Ulisses Braga-Neto. Classification and error estimation for discrete data. Pattern Recogni-  tion, 10(7):446-462, 2009. Acharya Jafarpour Orlitsky Suresh  N. Cesa-Bianchi and G. Lugosi. Minimax regret under log loss for general classes of experts.  COLT, pages 12-18, New York, NY, USA, 1999. ACM.  S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL \u201996, pages 310-318. Association for Computational Linguistics, 1996.  H. Das. Competitive Tests and Estimators for Properties of Distributions. PhD thesis,  E. Drukh and Y. Mansour. Concentration bounds for unigrams language model. In COLT,  UCSD, 2012.  2004.  W. A. Gale and G. Sampson. Good-turing frequency estimation without tears. Journal of  Quantitative Linguistics, 2(3):217-237, 1995.  I. J. Good. The population frequencies of species and the estimation of population param-  eters. 40(3-4):237-264, 1953.  P. D. Gr\u00a8unwald. The Minimum Description Length Principle. The MIT Press, 2007.  R. Krichevsky and V. Trofimov. The performance of universal encoding.  Information  Theory, IEEE Transactions on, 27(2):199 - 207, March 1981.  L. LeCam. Asymptotic methods in statistical decision theory. Springer series in statistics.  Springer, New York, 1986.  G.G. Lorentz. Bernstein polynomials. Chelsea Publishing Company, Incorporated, 1986.  D. A. McAllester and R. E. Schapire. On the convergence rate of good-turing estimators.  In COLT, 2000.  C. McDiarmid. On the method of bounded di\ufb00erences. In Surveys in Combinatorics, London  Mathematical Society Lecture Note Series. Cambridge University Press, 1989.  N. Merhav and M. Feder. Universal prediction. IEEE Transactions on Information Theory,  44(6):2124-2147, 1998.  M. Mitzenmacher and E. Upfal. Probability and computing: Randomized algorithms and  probabilistic analysis. Cambridge University Press, 2005.  M. I. Ohannessian and M A. Dahleh. Rare probability estimation under regularly varying  heavy tails. JMLR- Proceedings Track, 23:21.1-21.24, 2012.  A. Orlitsky, N. P. Santhanam, and J. Zhang. Always good turing: Asymptotically optimal  probability estimation. In FOCS, 2003.  A. Orlitsky, N.P. Santhanam, and J. Zhang. Universal compression of memoryless sources over unknown alphabets. IEEE Transactions on Information Theory, 50(7):1469- 1481, July 2004. Optimal Probability Estimation with Applications to Prediction and Classification  A. Orlitsky, N. Santhanam, K. Viswanathan, and J. Zhang. Convergence of profile based  estimators. In ISIT 2005, pages 1843 -1847, September 2005.  L. Paninski. Variational minimax estimation of discrete distributions under kl loss.  In  NIPS, 2004.  L. Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete  data. IEEE Transactions on Information Theory, 54(10), 2008.  J. Rissanen. Universal coding, information, prediction, and estimation. IEEE Transactions  on Information Theory, 30(4):629-636, July 1984.  G. Shamir. A new upper bound on the redundancy of unknown alphabets. In Proceedings of The Annual Conference on Information Sciences and Systems, Princeton, New-Jersey, 2004.  G. Valiant and P. Valiant. Estimating the unseen: an n/log(n)-sample estimator for entropy  and support size, shown optimal via new clts. STOC. ACM, 2011.  V. G. Vovk. A game of prediction with expert advice. COLT, pages 51-60, New York, NY,  A. B. Wagner, P. Viswanath, and S. R. Kulkarni. Strong consistency of the good-turing  USA, 1995. ACM.  estimator. In ISIT, 2006."}