{"1": "Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer Pub-  lishing Company, Incorporated, 2000.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in  Machine Learning, 2015.  Robert M. Freund, Paul Grigas, and Rahul Mazumder. Condition number analysis of logistic re-  gression, and its implications for first-order solution methods. INFORMS, 2017.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.  Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in  terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Conference on Learning Theory, pages 1305- 1320, 2015.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  Publishers, 2004.  Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-  gence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic  Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on  the Mathematical Theory of Automata, 12:615-622, 1962.  Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances in  neural information processing systems, 2016.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  13   THE IMPLICIT BIAS OF GRADIENT DESCENT ON NONSEPARABLE DATA  References  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer Pub-  lishing Company, Incorporated, 2000.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in  Machine Learning, 2015.  Robert M. Freund, Paul Grigas, and Rahul Mazumder. Condition number analysis of logistic re-  gression, and its implications for first-order solution methods. INFORMS, 2017.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.  Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in  terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Conference on Learning Theory, pages 1305- 1320, 2015.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  Publishers, 2004.  Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-  gence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic  Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on  the Mathematical Theory of Automata, 12:615-622, 1962.  Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances in  neural information processing systems, 2016.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012. THE IMPLICIT BIAS OF GRADIENT DESCENT ON NONSEPARABLE DATA  Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.  Matthew Streeter and Brendan McMahan. No-regret algorithms for unconstrained online convex  optimization. In Advances in neural information processing systems, 2012.  Matus Telgarsky. A primal-dual convergence analysis of boosting. JMLR, 13:561-606, 2012.  Matus Telgarsky. Margins, shrinkage, and boosting. In ICML, 2013."}