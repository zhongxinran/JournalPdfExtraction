{"1": "Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected langevin monte carlo. arXiv preprint arXiv:1507.02564, 2015.  Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing  gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin  mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.  Imre Csisz\u00b4ar, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and  Trends R(cid:13) in Communications and Information Theory, 1(4):417-528, 2004.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Arnak S Dalalyan and Alexandre B Tsybakov. Sparse regression learning by aggregation and  langevin monte-carlo. Journal of Computer and System Sciences, 78(5):1423-1443, 2012.  Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning  algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pages 1884- 1892, 2016.  Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061-  1083, 1975.  13   GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  We thank Zhou Lu, Feicheng Wang and Xiang Wang for helpful discussions. This work was par- tially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and Center for Data Science, Beijing Institute of Big Data Research in Peking University. This work was done when X.Z. was visiting Peking University.  Acknowledgments  References  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected langevin monte carlo. arXiv preprint arXiv:1507.02564, 2015.  Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing  gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin  mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.  Imre Csisz\u00b4ar, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and  Trends R(cid:13) in Communications and Information Theory, 1(4):417-528, 2004.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Arnak S Dalalyan and Alexandre B Tsybakov. Sparse regression learning by aggregation and  langevin monte-carlo. Journal of Computer and System Sciences, 78(5):1423-1443, 2012.  Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning  algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pages 1884- 1892, 2016.  Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061-  1083, 1975. GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  Istv\u00b4an Gy\u00a8ongy. Mimicking the one-dimensional marginal distributions of processes having an it\u02c6o  differential. Probability theory and related fields, 71(4):501-516, 1986.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.  Chris Junchi Li, Lei Li, Junyang Qian, and Jian-Guo Liu. Batch size matters: A diffusion approxi- mation framework on nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.  Qianxiao Li, Cheng Tai, and E Weinan. Dynamics of stochastic gradient algorithms. arXiv preprint  arXiv:1511.06251, 2015.  Junhong Lin and Lorenzo Rosasco. Optimal learning for multi-pass stochastic gradient methods. In  Advances in Neural Information Processing Systems, pages 4556-4564, 2016.  Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit regularization for multiple passes sgm. In International Conference on Machine Learning, pages 2340-2348, 2016.  Ben London. Generalization bounds for randomized learning with application to stochastic gradient  descent. In NIPS Workshop on Optimizing the Optimizers, 2016.  Peter A Markowich and C\u00b4edric Villani. On the trend to equilibrium for the fokker-planck equation:  an interplay between physics and functional analysis. Mat. Contemp, 19:1-29, 2000.  David A McAllester. Pac-bayesian stochastic model selection. Machine Learning, 51(1):5-21,  2003.  Tigran Nagapetyan, Andrew B Duncan, Leonard Hasenclever, Sebastian J Vollmer, Lukasz Szpruch, and Konstantinos Zygalakis. The true cost of stochastic gradient langevin dynamics. arXiv preprint arXiv:1706.02692, 2017.  Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.  Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-  rithms. arXiv preprint arXiv:1801.04295, 2018.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.  Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.  Analysis and Applications, 3(04):397-417, 2005.  Hannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pages 63-95. Springer,  1996. GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In Advances in Neural Information Processing Systems, pages 6067-6077, 2017.  Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn- ing algorithms. In Advances in Neural Information Processing Systems, pages 2521-2530, 2017.  Nanyang Ye, Zhanxing Zhu, and Rafal K Mantiuk. Langevin dynamics with continuous tempering  for training deep neural networks. arXiv preprint arXiv:1703.04379, 2017.  Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Karthik Sridharan, Brando Miranda, Noah Golowich, and Tomaso Poggio. Theory of deep learning iii: Generalization properties of sgd. Technical report, Center for Brains, Minds and Machines (CBMM), 2017a.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient  langevin dynamics. arXiv preprint arXiv:1702.05575, 2017b."}