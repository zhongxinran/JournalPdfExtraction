{"1": "A. Baranes and P.-Y. Oudeyer. R-IAC: Robust Intrinsically Motivated Exploration and Active Learn- ing. IEEE Transactions on Autonomous Mental Development, 1(3):155-169, Oct. 2009. ISSN 1943-0604. doi: 10.1109/TAMD.2009.2037513.  R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal  reinforcement learning. Journal of Machine Learning Research, 3:213-231, 2002.  T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. J. Mach.  Learn. Res., 99:1563-1600, August 2010. ISSN 1532-4435.  S. M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Compu-  tationel Neuroscience Unit, University College London, 2003.  M. J. Kearns and S. P. Singh. Near-optimal reinforcement learning in polynominal time. In ICML,  pages 260-268, 1998.  P.-Y. Oudeyer and F. Kaplan. What is Intrinsic Motivation? A Typology of Computational Ap- doi:  proaches. Frontiers in neurorobotics, 1(November):6, Jan. 2007. 10.3389/neuro.12.006.2007.  ISSN 1662-5218.  P.-Y. Oudeyer, F. Kaplan, and V. Hafner. Intrinsic motivation systems for autonomous mental de-  velopment. IEEE Transactions on Evolutionary Computation, 11:265-286, 2007.  J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proceedings of the first international conference on simulation of adaptive behav- ior on From animals to animats, pages 222-227, Cambridge, MA, USA, 1991. MIT Press. ISBN 0-262-63138-5.  40.12   LIM AUER  4.7. Discussion  Our algorithm employs the idea of optimism under uncertainty, which underlies many PAC-MDP algorithms (Kearns and Singh, 1998; Brafman and Tennenholtz, 2002; Kakade, 2003; Strehl et al., 2006; Szita and Szepesv\u00b4ari, 2010). A particular point that we need to clarify is regarding the notion of \u201cknown\u201d states. The meaning of a \u201cknown\u201d state in UcbExplore is very different from that in the R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003). In UcbExplore, a state is \u201cknown\u201d if we have learned a good policy to reach it. On the other hand, in R-MAX, a state is \u201cknown\u201d if we have sampled its actions sufficiently often.  It remains an open question if the exploration bound for our algorithm is optimal. One would (cid:17) , but this has not been achieved. New methods  (cid:16) SAL2 expect that the bounds can be improved to \u02dcO (cid:15)2 will be necessary to obtain such an improvement.  We thank the anonymous reviewers for their very valuable comments. The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 231495 (CompLACS) and n\u25e6 216886 (PASCAL2).  Acknowledgements  References  A. Baranes and P.-Y. Oudeyer. R-IAC: Robust Intrinsically Motivated Exploration and Active Learn- ing. IEEE Transactions on Autonomous Mental Development, 1(3):155-169, Oct. 2009. ISSN 1943-0604. doi: 10.1109/TAMD.2009.2037513.  R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal  reinforcement learning. Journal of Machine Learning Research, 3:213-231, 2002.  T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. J. Mach.  Learn. Res., 99:1563-1600, August 2010. ISSN 1532-4435.  S. M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Compu-  tationel Neuroscience Unit, University College London, 2003.  M. J. Kearns and S. P. Singh. Near-optimal reinforcement learning in polynominal time. In ICML,  pages 260-268, 1998.  P.-Y. Oudeyer and F. Kaplan. What is Intrinsic Motivation? A Typology of Computational Ap- doi:  proaches. Frontiers in neurorobotics, 1(November):6, Jan. 2007. 10.3389/neuro.12.006.2007.  ISSN 1662-5218.  P.-Y. Oudeyer, F. Kaplan, and V. Hafner. Intrinsic motivation systems for autonomous mental de-  velopment. IEEE Transactions on Evolutionary Computation, 11:265-286, 2007.  J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proceedings of the first international conference on simulation of adaptive behav- ior on From animals to animats, pages 222-227, Cambridge, MA, USA, 1991. MIT Press. ISBN 0-262-63138-5.  40.12   AUTONOMOUS EXPLORATION FOR NAVIGATING IN MDPS  J. Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (19902010). Autonomous  Mental Development, IEEE Transactions on, 2(3):230-247, 2010.  S. P. Singh, A. G. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. In NIPS,  2004.  S. P. Singh, R. L. Lewis, A. G. Barto, and J. Sorg. Intrinsically motivated reinforcement learning:  An evolutionary perspective. IEEE T. Autonomous Mental Development, 2(2):70-82, 2010.  A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman. Pac model-free reinforcement  learning. In ICML, pages 881-888, 2006.  I. Szita and C. Szepesv\u00b4ari. Model-based reinforcement learning with nearly tight exploration com-  plexity bounds. In ICML, pages 1031-1038, 2010."}