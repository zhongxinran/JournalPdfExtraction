{"1": "T. Bayes. An essay towards solving a problem in the doctrine of chances. Philosophi- cal Transactions of the Royal Society, 53:370\u2013418, 1763. doi: 10.1098/rstl.1763.0053. [Reprinted in Biometrika, 45, 296\u2013315, 1958].  R. Begleiter and R. El-Yaniv. Superior guarantees for sequential prediction and lossless com- pression via alphabet decomposition. Journal of Machine Learning Research, 7:379411, 2006.  J. M. Bernardo. Reference posterior distributions for Bayesian inference (with discussion).  Journal of the Royal Statistical Society, B41:113\u2013147, 1979.  C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.  13   Sparse Adaptive Dirichlet-Multinomial-like Processes  8. Conclusion  I introduced and analyzed a model, closely related to the Dirichlet-multinomial distribution, which predicts an Old symbol with its past frequency scaled down by t t+\u03b2 and a new symbol with its weight, scaled down by \u03b2 t+\u03b2 . Natural weight choices are uniform and 2\u2212CodeLength. I derived exact expressions and for small m rather tight bounds for the code length and redundancy. The bounds were data-dependent rather then expected or worst-case bounds. This led to an (approximately) optimal choice of \u03b2 di\ufb00erent from traditional recommendations. The constant o\ufb04ine \u03b2\u2217 (16) depends on the total sequence length n and number of di\ufb00erent used symbols m. The variable online (cid:126)\u03b2\u2217 (21) depends on the current sequence length t and number of di\ufb00erent symbols observed so far mt.  The redundancy bounds additionally depend on the individual symbol counts ni them- selves. They show that S\u03b2\u2217 has (at most) zero redundancy for unused symbols and finite redundancy for symbols occurring only finitely often, unlike the KT estimator and compan- ions which have redundancy 1 2 lnn+O(1) per base symbol, whether it occurs or not. Indeed, my bounds are independent of the base alphabet size D, therefore also hold for denumerable and with suitable reinterpretation for continuous X .  There seems to be not much leeway in choosing a globally good \u03b2. Experimentally it seems that even slight changes in \u03b2\u2217 can significantly deteriorate performance in some (m,n,D)-regime, but can only marginally and locally improve performance in others. Empir- ically S (cid:126)\u03b2\u2217 seems superior to the other fast online estimators I compared it to. See"}