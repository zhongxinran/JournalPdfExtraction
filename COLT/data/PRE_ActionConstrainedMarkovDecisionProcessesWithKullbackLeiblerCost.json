{"1": "W. H. Al-Sabban, L. F. Gonzalez, and R. N. Smith. Wind-energy based path planning for unmanned aerial vehicles using Markov Decision Processes. In Proc. IEEE Conf. Robotics and Automation (ICRA), pages 784-789. IEEE, 2013.  12  01-0.50ImRe0.5\u03b6=0\u03b6=1\u03b6=2\u03bb(\u02c7P\u03b6) MDPS WITH K-L COST  in which l P XL is the position on the grid. The figure shows only the values n \u201c 2 and n \u201c 4 (the most interesting to view because of obvious spatial variability).  If the position l \u201c pla, loq is far from the boundary of XL,  say, minpla, loq \u011b 4 and minpda \u00b4 la, do \u00b4 loq \u011b 4, then  ErUt | Lt \u201c l Nt \u201c ns \u00ab 0 and vpl, nq \u00ab \u03c9pl, nq,  \u03b6 \u201c 0  For the case \u03b6 \u201c 1 the vector field is transformed so that vectors near the target state point in this direction; for \u03b6 \u201c 2 this behavior is more apparent. For states far from the target the control effort seems to be lower - most likely the optimal policy waits for more favorable weather that will push the UAV in the North-East direction.  The eigenvalues of \u02c7P\u03b6 are shown in Fig. 4 for \u03b6 \u201c 0, 1, 2. Most of the eigenvalues are driven near zero for \u03b6 \u201c 2. Those  Figure 4: Eigenvalues of \u02c7P\u03b6  three that are independent of \u03b6 are the three eigenvalues of Q0, t0.9095, 0.9655, 1u.  While the vector field and eigenvalues change significantly when \u03b6 is doubled from 1 to 2, the cost to go J \u02da defined in (36) grows relatively slowly with \u03b6. Shown on the right hand side of Fig. 2 are comparisons for these two values of \u03b6. One plot with n \u201c 2 and the other n \u201c 4. The plot on the far right shows J \u02da  \u03b6 pl, \u03b6q for 0 \u010f \u03b6 \u010f 1 and l \u201c p1, 1q (the location farthest from l\u201a).  These plots are easily obtained because of the nature of the algorithm: the optimal policy and  value function are generated for any range of \u03b6 of interest.  4. Conclusions  The ODE approach for solving MDPs has simple structure for the class of models considered in this paper. We are currently looking at approaches to approximate dynamic programming as has been successful in the unconstrained model Todorov (2009).  It is likely that the ODE has special structure for other classes of MDPs, such as the \u201crational inattention\u201d framework of Sims (2006); Shafieepoorfard et al. (2016). The computational efficiency of this approach will depend in part on numerical properties of the ODE, such as its sensitivity for complex models. Applications to distributed control were the original motivation for this work, with particular attention to \u201cdemand dispatch\u201d Chen et al. (2017). It is believed that this paper will offer new computational tools in this ongoing research.  Funding from the ANR under grant ANR-16-CE05-0008, and NSF under awards EPCN 1609131, CPS 1646229 is gratefully acknowledged.  Acknowledgments  References  W. H. Al-Sabban, L. F. Gonzalez, and R. N. Smith. Wind-energy based path planning for unmanned aerial vehicles using Markov Decision Processes. In Proc. IEEE Conf. Robotics and Automation (ICRA), pages 784-789. IEEE, 2013.01-0.50ImRe0.5\u03b6=0\u03b6=1\u03b6=2\u03bb(\u02c7P\u03b6) MDPS WITH K-L COST  D. P. Bertsekas and S. E. Shreve. Stochastic Optimal Control: The Discrete-Time Case. Athena  Scientific, 1996.  A. Bu\u02c7si\u00b4c and S. Meyn. Ordinary differential equation methods for Markov decision processes and application to Kullback-Leibler control cost. SIAM Journal on Control and Optimization, 56(1): 343-366, 2018.  Y. Chen, U. Hashmi, J. Mathias, A. Bu\u02c7si\u00b4c, and S. Meyn. Distributed control design for balancing the grid using \ufb02exible loads. In IMA volume on the control of energy markets and grids. Springer, 2017.  A. Dembo and O. Zeitouni. Large Deviations Techniques And Applications. Springer-Verlag, New  York, second edition, 1998.  K. Doya. How can we learn efficiently to act optimally and \ufb02exibly? Proceedings of the National  Academy of Sciences, 106(28):11429-11430, 2009.  G. F. Franklin, M. L. Workman, and D. Powell. Digital Control of Dynamic Systems. Addison-  Wesley Longman Publishing Co., Inc., Boston, MA, USA, 3rd edition, 1997.  P. Guan, M. Raginsky, and R. M. Willett. Online Markov decision processes with Kullback-Leibler  control cost. IEEE Trans. Automat. Control, 59(6):1423-1438, June 2014.  M. K\u00b4arn\u00b4y. Towards fully probabilistic control design. Automatica, 32(12):1719 -1722, 1996.  I. Kontoyiannis and S. P. Meyn. Spectral theory and limit theorems for geometrically ergodic  Markov processes. Ann. Appl. Probab., 13:304-362, 2003.  S. Meyn, P. Barooah, A. Bu\u02c7si\u00b4c, Y. Chen, and J. Ehren. Ancillary service to the grid using intelligent  deferrable loads. IEEE Trans. Automat. Control, 60(11):2847-2862, Nov 2015.  M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley  & Sons, 2014.  P. J. Schweitzer. Perturbation theory and finite Markov chains. J. Appl. Prob., 5:401-403, 1968.  E. Shafieepoorfard, M. Raginsky, and S. P. Meyn. Rationally inattentive control of Markov pro-  cesses. SIAM J. Control Optim., 54(2):987-1016, 2016.  C. A. Sims. Rational inattention: Beyond the linear-quadratic case. The American economic review,  pages 158-163, 2006.  R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057-1063, Cambridge, MA, USA, 1999. MIT Press.  E. Todorov. Linearly-solvable Markov decision problems. In B. Sch\u00a8olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1369-1376. MIT Press, Cambridge, MA, 2007. MDPS WITH K-L COST  E. Todorov. Efficient computation of optimal actions. Proceedings of the National Academy of  Sciences, 106(28):11478-11483, 2009."}