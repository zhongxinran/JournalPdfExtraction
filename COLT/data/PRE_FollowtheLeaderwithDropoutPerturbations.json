{"1": "Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Neural Information Processing Systems (NIPS), pages 1-9, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64(1):48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds IEEE Transactions on Neural  for on-line prediction of linear functions by gradient descent. Networks, 7(2):604-619, May 1996.  Nicol`o Cesa-Bianchi, Yaov Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research. To appear., 2014.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Con-  ference on Learning Theory (COLT), pages 460-473, 2013.  13   DROPOUT PERTURBATIONS  4. Constant Regret on IID Losses  In this section we show that our BDP algorithm has optimal O(ln K) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a fixed gap \u03b3 between the expected loss of the best expert and all others.  Theorem 4.1 Let \u03b3 \u2208 (0, 1] and \u03b4 \u2208 (0, 1] be constants, and let k\u2217 be a fixed expert. Suppose the loss vectors (cid:96)t are independent random variables such that the expected differences in loss satisfy  Then, with probability at least 1 \u2212 \u03b4, the regret of the BDP algorithm is bounded by a constant:  E[(cid:96)t,k \u2212 (cid:96)t,k\u2217] \u2265 \u03b3  for all t.  min k(cid:54)=k\u2217  RT \u2264  8 (1 \u2212 \u03b1)2\u03b32 ln  8K (1 \u2212 \u03b1)2\u03b32\u03b4  + 3.  (4.1)  (4.2)  As discussed in the proof in Section B.1, it is possible to improve the dependence on \u03b3 at the  cost of getting a more complicated expression.  Tim van Erven was supported by NWO Rubicon grant 680-50-1112, Wojciech Kot\u0142owski by the Foundation for Polish Science under the Homing Plus Program, and Manfred K. Warmuth by NSF grant IIS-1118028.  Acknowledgments  References  Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Neural Information Processing Systems (NIPS), pages 1-9, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64(1):48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds IEEE Transactions on Neural  for on-line prediction of linear functions by gradient descent. Networks, 7(2):604-619, May 1996.  Nicol`o Cesa-Bianchi, Yaov Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research. To appear., 2014.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Con-  ference on Learning Theory (COLT), pages 460-473, 2013. VAN ERVEN KOT\u0141OWSKI WARMUTH  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  James Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of  Games, 3:97-139, 1957.  Elad Hazan, Satyen Kale, and Manfred K. Warmuth. On-line variance minimization in O(n2) per  trial? In Conference on Learning Theory (COLT), pages 314-315, 2010.  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. CoRR,  dinov. abs/1207.0580, 2012.  Nie Jiazhong, Wojciech Kot\u0142owski, and Manfred K. Warmuth. On-line PCA with optimal regrets.  In Algorithmic Learning Theory (ALT), pages 98-112, 2013.  Adam Kalai. A perturbation that makes Follow the Leader equivalent to Randomized Weighted  Majority. Private communication, December 2005.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  Jyrki Kivinen and Manfred K. Warmuth. Additive versus Exponentiated Gradient updates for linear  prediction. Information and Computation, 132(1):1-64, 1997.  Wouter M. Koolen and Manfred K. Warmuth. Hedging structured concepts. In 23rd Annual Con-  ference on Learning Theory - COLT 2010, pages 93-104. Omnipress, June 2010.  Dima Kuzmin and Manfred K. Warmuth. Optimum follow the leader algorithm. In Conference on  Learning Theory (COLT), pages 684-686, 2005. Open problem.  Nick Littlestone and Manfred K. Warmuth. The Weighted Majority algorithm. Information and  Computation, 108(2):212-261, 1994.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Ma-  chine Learning Research, 4:773-818, 2003.  Tim van Erven, Peter D. Gr\u00a8unwald, Wouter Koolen, and Steven de Rooij. Adaptive hedge.  In  Neural Information Processing Systems (NIPS), pages 1656-1664, 2011.  Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, 1998.  Stefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Neural  Information Processing Systems (NIPS), pages 351-359, 2013.  Sida I. Wang and Christopher D. Manning. Fast dropout training. In International Conference on  Machine Learning (ICML), pages 118-126, 2013. DROPOUT PERTURBATIONS  Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. Machine Learning, 87(1):  1-32, 2011.  Additional Material for \u201cFollow the Leader with Dropout Perturbations\u201d"}