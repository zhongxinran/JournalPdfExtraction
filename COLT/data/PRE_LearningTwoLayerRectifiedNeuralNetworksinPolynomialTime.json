{"1": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural  networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with unknown gaus- In Advances in Neural  sian noise, with implications for gaussian mixtures and autoencoders. Information Processing Systems, pages 2375-2383, 2012.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1057-1066. ACM, 2017.  Robert B. Ash. Lecture notes 21-25 in statistics, finding the density. https://faculty.math.  illinois.edu/\u02dcr-ash/Stat/StatLec21-25.pdf.  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy.  In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analy- sis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594-603. ACM, 2014.  Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural  Networks, 5(1):117-127, 1992.  Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.  arXiv preprint arXiv:1809.10787, 2018.  13   The authors thank the partial support by the National Science Foundation under Grant No. CCF- 1815840. Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing. The authors would also like to thank Anima Anandkumar, Mark Bun, Rong Ge, Sam Hopkins and Rina Panigrahy for useful discussions.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural  networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with unknown gaus- In Advances in Neural  sian noise, with implications for gaussian mixtures and autoencoders. Information Processing Systems, pages 2375-2383, 2012.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1057-1066. ACM, 2017.  Robert B. Ash. Lecture notes 21-25 in statistics, finding the density. https://faculty.math.  illinois.edu/\u02dcr-ash/Stat/StatLec21-25.pdf.  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy.  In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analy- sis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594-603. ACM, 2014.  Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural  Networks, 5(1):117-127, 1992.  Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.  arXiv preprint arXiv:1809.10787, 2018. Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian  inputs. arXiv preprint arXiv:1702.07966, 2017.  Wlodzimierz Bryc. The normal distribution: characterizations with applications, volume 100.  Springer Science & Business Media, 2012.  Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling. Inverse  problems, 23(3):969, 2007.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Michael B Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and balancing via box constrained newton\u2019s method and interior point methods. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 902-913. IEEE, 2017.  Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287-314,  1994.  Keith Conrad. Expository papers: Universal identities. http://www.math.uconn.edu/  \u02dckconrad/blurbs/linmultialg/univid.pdf.  A. DasGupta. Asymptotic Theory of Statistics and Probability. Springer Texts in Statistics. Springer New York, 2008. ISBN 9780387759708. URL https://books.google.com/books? id=9ByccYe5aI4C.  Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep  learning. In Advances in neural information processing systems, pages 2148-2156, 2013.  Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in neural informa- tion processing systems, pages 1269-1277, 2014.  Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks  with overlaps. arXiv preprint arXiv:1805.07798, 2018.  Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global  minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.  Alan Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In Foundations of Computer Science, 1996. Proceedings., 37th Annual Symposium on, pages 359-368. IEEE, 1996.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank  approximations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  Rong Ge. Personal communication. October, 2018. Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares  algorithms. arXiv preprint arXiv:1504.05287, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with  symmetric inputs. arXiv preprint arXiv:1810.06793, 2018.  Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv  preprint arXiv:1709.06010, 2017.  Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-  mial time. arXiv preprint arXiv:1611.10258, 2016.  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping  patches. arXiv preprint arXiv:1802.02547, 2018.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decomposition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 584-593. ACM, 2014.  David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions  on Information Theory, 57(3):1548-1566, 2011.  Allan Gut. An intermediate course in probability. chapter 5. Springer Publishing Company, Incor-  porated, 2009.  Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 651-660. IEEE, 2014.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and In Proceedings of the 4th conference on Innovations in Theoretical  spectral decompositions. Computer Science, pages 11-20. ACM, 2013.  Aapo Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE  transactions on Neural Networks, 10(3):626-634, 1999.  Aapo Hyv\u00a8arinen and Erkki Oja.  Independent component analysis: algorithms and applications.  Neural networks, 13(4-5):411-430, 2000.  Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream compu-  tation. Journal of the ACM (JACM), 53(3):307-323, 2006. Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alter- In Proceedings of the forty-fifth annual ACM symposium on Theory of  nating minimization. computing, pages 665-674. ACM, 2013.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Score function features for discriminative  learning: Matrix and tensor framework. arXiv preprint arXiv:1412.2863, 2014.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  J Stephen Judd. Neural network design and the complexity of learning. Technical report, CALI-  FORNIA INST OF TECH PASADENA DEPT OF COMPUTER SCIENCE, 1988.  Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized In Advances in Neural Information  linear and single index models with isotonic regression. Processing Systems, pages 927-935, 2011.  Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically  learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE transactions on information theory, 56(6):2980-2998, 2010.  B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann. Statist., 28(5):1302-1338, 10 2000. doi: 10.1214/aos/1015957395. URL https://doi.org/ 10.1214/aos/1015957395.  Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its impli- cations for combinatorial and convex optimization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1049-1065. IEEE, 2015.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 597-607. Curran Associates, Inc., 2017a. URL http://papers.nips.cc/paper/ 6662-convergence-analysis-of-two-layer-neural-networks-with-relu-activation. pdf.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.  In Advances in Neural Information Processing Systems, pages 597-607, 2017b.  Yi-Kai Liu, Animashree Anandkumar, Dean P Foster, Daniel Hsu, and Sham M Kakade. Two svds su ffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. In Neural Information Processing Systems (NIPS), 2012.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014. Michael W Mahoney et al. Randomized algorithms for matrices and data. Foundations and Trends  in Machine Learning, 3(2):123-224, 2011.  Pasin Manurangsi and Daniel Reichman. The computational complexity of training relu(s). arXiv  preprint arXiv:1810.04207, 2018.  etry, 3(4):325-337, 1988.  Nimrod Megiddo. On the complexity of polyhedral separability. Discrete & Computational Geom-  Lingsheng Meng and Bing Zheng. The optimal perturbation bounds of the moorepenrose inverse un- der the frobenius norm. Linear Algebra and its Applications, 432(4):956 - 963, 2010. ISSN 0024- 3795. doi: https://doi.org/10.1016/j.laa.2009.10.009. URL http://www.sciencedirect. com/science/article/pii/S0024379509005230.  Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural  networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.  Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, 2013.  Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu- lar values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pages 1576- 1602. World Scientific, 2010.  Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 143-152. IEEE, 2006.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223-1231, 2016.  Mahdi Soltanolkotabi. Learning relus via gradient descent.  In Advances in Neural Information  Processing Systems, pages 2007-2017, 2017.  Zhao Song, David Woodruff, and Huan Zhang. Sublinear time orthogonal tensor decomposition. In  Advances in Neural Information Processing Systems, pages 793-801, 2016.  Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees  for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.  Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In  Conference on Learning Theory, pages 37-1, 2012.  Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017a. Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks  with relu nonlinearity. 2017b.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training one- hidden-layer neural networks. CoRR, abs/1805.02677, 2018. URL http://arxiv.org/ abs/1805.02677.  Santosh S Vempala. Learning convex concepts from gaussian distributions with pca. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 124-130. IEEE, 2010.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Roman Vershynin. High-dimensional probability: An introduction with applications in data sci-  ence, volume 47. Cambridge University Press, 2018.  M.J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Se- ISBN  ries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. 9781108498029. URL https://books.google.com/books?id=8C8nuQEACAAJ.  Yining Wang and Anima Anandkumar. Online and differentially-private tensor decomposition. In  Advances in Neural Information Processing Systems, pages 3531-3539, 2016.  David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends  in Theoretical Computer Science, 10(1-2):1-157, 2014.  Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly In International Conference on Machine Learning, pages 993-  learnable in polynomial time. 1001, 2016.  Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees  for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.  Roadmap  In Section A we introduce our nO(k) time exact algorithm when rank(A) = k and arbitrary X, for recovery of rank-k matrices U\u2217, V\u2217 such that U\u2217f (V\u2217X) = A. In this section, we also demon- strate that for a very wide class of distributions for random matices X, the matrix U\u2217f (V\u2217X) is in fact full rank with high probability, and therefore can be solved with our exact algorithm. Then, in Section B, we prove NP-hardness of the learning problem when rank(A) < k. Next, in Section C, we give a polynomial time algorithm for exact recovery of U\u2217, V\u2217 in the case when X has Gaus- sian marginals in the realizable setting. Section C.1 develops our Independenct Component Analysis Based algorithm, whereas Section C.2 develops our more general exact recovery algorithm. In Sec- tion C.3, we show how recent concurrent results can be bootstrapped via our technqiues to obtain exact recovery for a wider class of distributions.  In Section D, we demonstrate how to extend our algorithm to the case where A = U\u2217f (V\u2217X)+ E where E is mean 0 i.i.d. sub-Gaussian noise. Then in Section E, we give a fixed-paramater tractable (FPT) (in k and \u03ba(V\u2217)) for the exact recovery of U\u2217, V\u2217 in the case where U\u2217 does not have full column rank. We give our second FPT algorithm in Section F, which finds weights which approximate the optimal network for arbitrary error matrices E that are independent of X. In Section G, we demonstrate how the weights of certain low-rank networks, where k < d, m, can be recovered exactly in the presence of a class of arbitrary sparse noise in polynomial time. Finally, in"}