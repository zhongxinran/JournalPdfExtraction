{"1": "Y. Abbasi-Yadkori, D.P\u00b4al, and C.Szepesv\u00b4ari. Improved Algorithms for Linear Stochastic Bandits.  In Advances in Neural Information Processing Systems, 2011.  S. Agrawal and N. Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs. In  International Conference on Machine Learning (ICML), 2013.  A. Antos, V. Grover, and C. Szepesv\u00b4ari. Active learning in multi-armed bandits. In Algorithmic  Learning Theory, 2008.  A. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. Information Theory, IEEE Transactions on, 44(6):2743-2760, Oct 1998. ISSN 0018- 9448. doi: 10.1109/18.720554.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. Journal of Machine Learning  Research, 12:1587-1627, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  Antoine Chambaz, Aur\u00b4elien Garivier, and Elisabeth Gassiat. A MDL approach to HMM with pois- son and gaussian emissions. application to order identification. Journal of Statistical Planning and Inference, 139(3):962-977, 2009.  H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  R. Combes and A. Prouti`ere. Unimodal Bandits without Smoothness. Technical report, 2014.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Re- search, 7:1079-1105, 2006.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to In Advances in Neural Information Processing Systems,  Fixed Budget and Fixed Confidence. 2012.  Aurlien Garivier. Consistency of the unlimited BIC context tree estimator. IEEE Transactions on  Information Theory, 52(10):4630-4635, 2006.  T.L. Graves and T.L. Lai. Asymptotically Efficient adaptive choice of control laws in controlled  markov chains. SIAM Journal on Control and Optimization, 35(3):715-743, 1997.  Peter D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive Computation and Ma-  chine Learning). The MIT Press, 2007. ISBN 0262072815.  15   OPTIMAL BEST ARM IDENTIFICATION WITH FIXED CONFIDENCE  References  Y. Abbasi-Yadkori, D.P\u00b4al, and C.Szepesv\u00b4ari. Improved Algorithms for Linear Stochastic Bandits.  In Advances in Neural Information Processing Systems, 2011.  S. Agrawal and N. Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs. In  International Conference on Machine Learning (ICML), 2013.  A. Antos, V. Grover, and C. Szepesv\u00b4ari. Active learning in multi-armed bandits. In Algorithmic  Learning Theory, 2008.  A. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. Information Theory, IEEE Transactions on, 44(6):2743-2760, Oct 1998. ISSN 0018- 9448. doi: 10.1109/18.720554.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. Journal of Machine Learning  Research, 12:1587-1627, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  Antoine Chambaz, Aur\u00b4elien Garivier, and Elisabeth Gassiat. A MDL approach to HMM with pois- son and gaussian emissions. application to order identification. Journal of Statistical Planning and Inference, 139(3):962-977, 2009.  H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  R. Combes and A. Prouti`ere. Unimodal Bandits without Smoothness. Technical report, 2014.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Re- search, 7:1079-1105, 2006.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to In Advances in Neural Information Processing Systems,  Fixed Budget and Fixed Confidence. 2012.  Aurlien Garivier. Consistency of the unlimited BIC context tree estimator. IEEE Transactions on  Information Theory, 52(10):4630-4635, 2006.  T.L. Graves and T.L. Lai. Asymptotically Efficient adaptive choice of control laws in controlled  markov chains. SIAM Journal on Control and Optimization, 35(3):715-743, 1997.  Peter D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive Computation and Ma-  chine Learning). The MIT Press, 2007. ISBN 0262072815. GARIVIER KAUFMANN  K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil\u2019UCB: an Optimal Exploration Algorithm for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014.  S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-  armed bandits. In International Conference on Machine Learning (ICML), 2012.  E. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Pro-  ceeding of the 26th Conference On Learning Theory., 2013.  E. Kaufmann, O. Capp\u00b4e, and A. Garivier. On the Complexity of A/B Testing. In Proceedings of the  27th Conference On Learning Theory, 2014.  E. Kaufmann, O. Capp\u00b4e, and A. Garivier. On the Complexity of Best Arm Identification in Multi-  Armed Bandit Models. Journal of Machine Learning Research (to appear), 2015.  Raphail E. Krichevsky and Victor K. Trofimov. The performance of universal encoding.  IEEE Transactions on Information Theory, 27(2):199-206, 1981. doi: 10.1109/TIT.1981.1056331. URL http://dx.doi.org/10.1109/TIT.1981.1056331.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6(1):4-22, 1985.  S. Magureanu, R. Combes, and A. Prouti`ere. Lipschitz Bandits: Regret lower bounds and optimal  algorithms. In Proceedings on the 27th Conference On Learning Theory, 2014.  S. Mannor and J. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit  Problem. Journal of Machine Learning Research, pages 623-648, 2004.  R. Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimiza-  tion and planning., volume 7. Foundations and Trends in Machine Learning, 2014.  J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465 - 471, 1978.  ISSN doi: http://dx.doi.org/10.1016/0005-1098(78)90005-5. URL http://www.  0005-1098. sciencedirect.com/science/article/pii/0005109878900055.  N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian Process Optimization in the Bandit Setting : No Regret and Experimental Design. In Proceedings of the International Conference on Machine Learning, 2010.  N.K. Vaidhyan and R. Sundaresan. Learning to detect an oddball target. arXiv:1508.05572, 2015.  Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. The context tree weighting method:  Basic properties. IEEE Transactions on Information Theory, 41:653-664, 1995. OPTIMAL BEST ARM IDENTIFICATION WITH FIXED CONFIDENCE"}