{"1": "R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applications to  markov chains. Electronic Journal of Probability, 34(13), 2008.  R. Bardenet and O.-A. Maillard. Concentration inequalities for sampling without replacement.  http://arxiv.org/abs/1309.4029, 2013.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statis-  tics, 33(4):14971537, 2005.  13   LOCALIZED COMPLEXITIES FOR TRANSDUCTIVE LEARNING  Remark 16 The question of transductive convergence rates is somewhat delicate, since all results stated here assume a fixed set XN , as re\ufb02ected for instance in the bound of Corollary 15 depending on the eigenvalues of the kernel Gram matrix of the set XN . In order to give a precise meaning to rates, one has to specify how XN evolves as N grows. A natural setting for this is Vapnik (1998)\u2019s second transductive setting where XN is i.i.d. from some generating distribution. In that case we think it is possible to adapt once again the results of Bartlett et al. (2005) in order to relate the quantities r\u2217 m(N ) to asymptotic counterparts as N \u2192 \u221e, though we do not pursue this avenue in the present work.  5. Conclusion  In this paper, we have considered the setting of transductive learning over a broad class of bounded and nonnegative loss functions. We provide excess risk bounds for the transductive learning setting based on the localized complexity of the hypothesis class, which hold under general assumptions on the loss function and the hypothesis class. When applied to kernel classes, the transductive excess risk bound can be formulated in terms of the tailsum of the eigenvalues of the kernels, similar to the best known estimates in inductive learning. The localized excess risk bound is achieved by proving two novel and very general concentration inequalities for suprema of empirical processes when sampling without replacement, which are of potential interest also in various other application areas in machine learning and learning theory, where they may serve as a fundamental mathematical tool. For instance, sampling without replacement is commonly employed in the Nystr\u00a8om method (Kumar et al., 2012), which is an efficient technique to generate low-rank matrix approximations in large-scale machine learning. Another potential application area of our novel concentration in- equalities could be the analysis of randomized sequential algorithms such as stochastic gradient descent and randomized coordinate descent, practical implementations of which often deploy sam- pling without replacement (Recht and Re, 2012). Very interesting also would be to explore whether the proposed techniques could be used to generalize matrix Bernstein inequalities (Tropp, 2012) to the case of sampling without replacement, which could be used to analyze matrix completion problems (Koltchinskii et al., 2011). The investigation of application areas beyond the transductive learning setting is, however, outside of the scope of the present paper.  The authors are thankful to Sergey Bobkov, Stanislav Minsker, and Mehryar Mohri for stimulating discussions and to the anonymous reviewers for their helpful comments. Marius Kloft acknowledges a postdoctoral fellowship by the German Research Foundation (DFG).  Acknowledgments  References  R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applications to  markov chains. Electronic Journal of Probability, 34(13), 2008.  R. Bardenet and O.-A. Maillard. Concentration inequalities for sampling without replacement.  http://arxiv.org/abs/1309.4029, 2013.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statis-  tics, 33(4):14971537, 2005. TOLSTIKHIN BLANCHARD KLOFT  P. L. Bartlett, S. Mendelson, and P. Phillips. On the optimality of sample-based estimates of the  expectation of the empirical minimizer. ESAIM: Probability and Statistics, 2010.  A. Blum and J. Langford. PAC-MDL bounds. In Proceedings of the International Conference on  Computational Learning Theory (COLT), 2003.  S. Bobkov. Concentration of normalized sums and a central limit theorem for noncorrelated random  variables. Annals of Probability, 32, 2004.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Independence. Oxford University Press, 2013.  O. Bousquet. A Bennett concentration inequality and its application to suprema of empirical pro-  cesses. C. R. Acad. Sci. Paris, Ser. I, 334:495-500, 2002a.  O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis  of Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002b.  C. Cortes and M. Mohri. On transductive regression. In Advances in Neural Information Processing  C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn., 20:273-297, September 1995.  Systems (NIPS), 2006.  ISSN 0885-6125.  C. Cortes, M. Mohri, D. Pechyony, and A. Rastogi. Stability analysis and learning bounds for  transductive regression algorithms. http://arxiv.org/abs/0904.0814, 2009.  P. Derbeko, R. El-Yaniv, and R. Meir. Explicit learning curves for transduction and application to clustering and compression algorithms. Journal of Artificial Intelligence Research, 22, 2004.  R. El-Yaniv and D. Pechyony. Stable transductive learning. In Proceedings of the International  Conference on Computational Learning Theory (COLT), 2006.  R. El-Yaniv and D. Pechyony. Transductive rademacher complexity and its applications. Journal of  Artificial Intelligence Research, 2009.  D. Gross and V. Nesme. Note on sampling without replacing from a fnite collection of matrices.  http://arxiv.org/abs/1001.2738v2, 2010.  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58(301):13-30, 1963.  T. Klein and E. Rio. Concentration around the mean for maxima of empirical processes. The Annals  of Probability, 33(3):10601077, 2005.  V. Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. The  Annals of Statistics, 34(6):25932656, 2006.  V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: \u00b4Ecole D \u00b4Et\u00b4e de Probabilit\u00b4es de Saint-Flour XXXVIII-2008. Ecole d\u2019\u00b4et\u00b4e de probabilit\u00b4es de Saint- Flour. Springer, 2011a. LOCALIZED COMPLEXITIES FOR TRANSDUCTIVE LEARNING  V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems.  \u00b4Ecole d\u2019\u00b4et\u00b4e de probabilit\u00b4es de Saint-Flour XXXVIII-2008. Springer Verlag, 2011b.  V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learn- In D. E. Gine and J.Wellner, editors, High Dimensional Probability, II, pages 443-457.  ing. Birkhauser, 1999.  V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. Annals of Statistics, 39:2302-2329, 2011. doi: 10.1214/ 11-AOS894.  S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the nystr&#246;m method. Journal  of Machine Learning Research, 13(1):981-1006, Apr. 2012.  P. Massart. Some applications of concentration inequalities to statistics. Ann. Fac. Sci. Toulouse  Math., 9(6):245-303, 2000.  2003.  S. Mendelson. On the performance of kernel classes. J. Mach. Learn. Res., 4:759-771, December  D. Pechyony. Theory and Practice of Transductive Learning. PhD thesis, Technion, 2008.  B. Recht and C. Re. Toward a noncommutative arithmetic-geometric mean inequality: Conjectures,  case-studies, and consequences. In COLT, 2012.  R. J. Ser\ufb02ing. Probability inequalities for the sum in sampling without replacement. The Annuals  of Statistics, 2(1):39-48, 1974.  I. Steinwart and A. Christmann. Support Vector Machines. Springer Publishing Company, Incorpo-  rated, 1st edition, 2008. ISBN 0387772413.  I. Steinwart, D. R. Hush, and C. Scovel. Learning from dependent observations. J. Multivariate  Analysis, 100(1):175-194, 2009.  M. Stone. Cross-validatory choice and assessment of statistical predictors (with discussion). Journal  of the Royal Statistical Society, B36:111-147, 1974.  M. Talagrand. New concentration inequalities in product spaces. Inventiones Mathematicae, 126,  J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  V. N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag New York,  1996.  Inc., 1982.  V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998. TOLSTIKHIN BLANCHARD KLOFT"}