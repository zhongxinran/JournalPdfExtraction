{"1": "2016.  Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. In Proceedings of the 48th Annual ACM on  Algorithmic stability for adaptive data analysis. Symposium on Theory of Computing, STOC, 2016.  25   LEARNING UNDER ROBUST GENERALIZATION  Theorems 39 and Theorem 40 only show a relationship between (\u03b5, 0)-differential privacy and strong generalization. To show such a relationship when \u03b4 > 0, we appeal to group privacy, first studied by Dwork et al. (2006a), which says that if M is (\u03b5, \u03b4)-differentially private and two samples S, S(cid:48) differ on k entries, then M(S) \u2248k\u03b5,ke(k\u22121)\u03b5\u03b4 M(S(cid:48)). Using simulator SIMD = M(S\u2217) for any fixed sample S\u2217 \u223ci.i.d. Dn and by the fact that any sample S can differ from S\u2217 in an most n samples, we see that M is (0, n\u03b5, ne(n\u22121)\u03b5\u03b4)-perfectly generalizing.  Unfortunately, this blowup in parameters is generally unacceptable for most tasks. We suspect as with (\u03b5, 0)-differential  (cid:16)(cid:112)n ln(1/\u03b2)  that the necessary blowup in the \u03b5 parameter is closer to \u0398 privacy, but leave a formal proof as an open question for future work.  (cid:17)  On the positive side, most known (\u03b5, \u03b4)-differentially private algorithms are designed by com- posing several (\u03b5(cid:48), 0)-differentially private algorithms, where the \u03b4 > 0 is an artifact of the compo- sition (see, e.g., Theorem 3.20 of Dwork and Roth (2014) for more details). Since perfect general- ization enjoys adaptive composition (as shown in Bassily and Freund (2016)), we could also obtain (\u03b2, \u03b5, \u03b4)-perfectly generalizing algorithms by composing a collection of (\u03b2, \u03b5, 0)-perfectly general- izing algorithms together. This will give better generalization parameters than a direct reduction via group privacy.  Acknowledgments  We thank Adam Smith and Raef Bassily for helpful comments about adaptive composition of per- fectly generalizing mechanisms and pointing out an error in an earlier version of this paper. We thank Shay Moran for telling us about variable length compression schemes and sharing with us his manuscript David et al. (2016). We thank our anonymous reviewers for numerous helpful com- ments.  The first author is supported in part by NSF grant 1254169, US-Israel Binational Science Foun- dation grant 2012348, and a Simons Graduate Fellowship. The second author is supported in part by NSF grants 1254169 and 1518941, US-Israel Binational Science Foundation Grant 2012348, the Charles Lee Powell Foundation, a Google Faculty Research Award, an Okawa Foundation Research Grant, a subcontract through the DARPA Brandeis project, a grant from the HUJI Cyber Security Research Center, and a startup grant from Hebrew University\u2019s School of Computer Science. Part of this work was completed when the second author was visiting the Simons Institute for the The- ory of Computing at Berkeley. The third author is supported by grants from the Sloan Foundation, a Simons Investigator grant to Salil Vadhan, and NSF grant CNS-1237235. The fourth author is supported in part by an NSF CAREER award, NSF grant CNS-1513694, a subcontract through the DARPA Brandeis project, and a grant from the Sloan Foundation.  References  2016.  Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. In Proceedings of the 48th Annual ACM on  Algorithmic stability for adaptive data analysis. Symposium on Theory of Computing, STOC, 2016. CUMMINGS LIGETT NISSIM ROTH WU  Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competi- tions. In Proceedings of the 32nd International Conference on Machine Learning, ICML, pages 1006-1014, 2015.  Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to noninteractive database privacy. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, STOC, pages 609-618, 2008.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occams razor.  Readings in machine learning, pages 201-204, 1990.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2:499-526, 2002.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and learning of threshold functions. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pages 634-649, 2015.  Ofir David, Shay Moran, and Amir Yehudayof. Supervised learning through the lens of compres-  sion. Preprint, 2016.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations  and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Advances in Cryptology - EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques. Proceedings, chapter Our Data, Ourselves: Privacy Via Distributed Noise Generation, pages 486-503. Springer Berlin Heidelberg, 2006a.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC, pages 265-284, 2006b.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toni Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. In Advances in Neural Information Processing Systems, NIPS, pages 2341-2349, 2015a.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638, 2015b.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the 47th Annual ACM on Symposium on Theory of Computing, STOC, pages 117-126, 2015c.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011. LEARNING UNDER ROBUST GENERALIZATION  Michael J Kearns and Umesh Virkumar Vazirani. An introduction to computational learning theory.  MIT press, 1994.  report, 1986.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy.  In 48th Annual  IEEE Symposium on Foundations of Computer Science, FOCS, pages 94-103, 2007.  Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predic-  tivity in learning theory. Nature, 428(6981):419-422, 2004.  Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information the- ory. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS, 2016.  S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-  rithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of Complexity, pages 11-30. Springer International Publishing, 1971.  Manfred K. Warmuth. Compressing to VC Dimension Many Points, volume 2777, pages 743-744.  Springer Berlin Heidelberg, 2003."}