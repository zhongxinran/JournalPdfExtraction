{"1": "Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak. Online learning of quantum states. In Advances in Neural Information Processing Systems 31 (NIPS), pages 8976\u2013 8986. 2018.  Jacob D. Abernethy and Alexander Rakhlin. Beating the adaptive bandit with high probability. In  Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  Jacob D. Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smooth- ness. In Advances in Neural Information Processing Systems 28 (NIPS), pages 2197\u20132205. 2015.  Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 28\u201340, 2010.  12   BANDIT PCA  Unfortunately, our attempts to prove such bounds were unsuccessful due to a limitation common to all known techniques for proving high-probability bounds. Brie\ufb02y put, all known approaches (Auer et al., 2002; Bartlett et al., 2008; Audibert and Bubeck, 2010; Beygelzimer et al., 2011; Neu, 2015) are based on adjusting the unbiased loss estimates so that the loss of every action v is (cid:11)(cid:3) for some small \u03b2 of order T \u22121/2 (see, e.g., (cid:2)(cid:10)vvT, (cid:101)L slightly underestimated by a margin of \u03b2Et Abernethy and Rakhlin, 2009 for a general discussion). While it is straightforward to bias our own (cid:11)(cid:3) in the bound, estimates in the same way, this eventually leads to extra terms of order \u03b2Et which are impossible to control by a small enough upper bound, as shown in"}