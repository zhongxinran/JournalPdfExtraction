{"1": "Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Karl Johan \u00c5str\u00f6m and Peter Eykhoff. System identification\u2014a survey. Automatica, 7(2):123-162, 1971.  Karl Johan \u00c5str\u00f6m and Tore H\u00e4gglund. PID controllers: theory, design, and tuning, volume 2. Instrument  society of America Research Triangle Park, NC, 1995.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to  align and translate. arXiv preprint arXiv:1409.0473, 2014.  Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks.  arXiv preprint  arXiv:1901.00434, 2019.  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural  networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.  SIAM journal on imaging sciences, 2(1):183-202, 2009.  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the  bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.  Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering,  volume 3. Wiley New York, 1992.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.  arXiv preprint arXiv:1702.07966, 2017.  Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix  completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models  using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.  Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint  arXiv:1709.06129, 2017.  Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in  unstable linear systems. Automatica, 96:342-353, 2018.  Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex  learning and optimization. NIPS, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.  arXiv preprint arXiv:1711.00501, 2017.  13   STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  References  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Karl Johan \u00c5str\u00f6m and Peter Eykhoff. System identification\u2014a survey. Automatica, 7(2):123-162, 1971.  Karl Johan \u00c5str\u00f6m and Tore H\u00e4gglund. PID controllers: theory, design, and tuning, volume 2. Instrument  society of America Research Triangle Park, NC, 1995.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to  align and translate. arXiv preprint arXiv:1409.0473, 2014.  Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks.  arXiv preprint  arXiv:1901.00434, 2019.  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural  networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.  SIAM journal on imaging sciences, 2(1):183-202, 2009.  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the  bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.  Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering,  volume 3. Wiley New York, 1992.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.  arXiv preprint arXiv:1702.07966, 2017.  Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix  completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models  using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.  Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint  arXiv:1709.06129, 2017.  Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in  unstable linear systems. Automatica, 96:342-353, 2018.  Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex  learning and optimization. NIPS, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.  arXiv preprint arXiv:1711.00501, 2017. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches.  arXiv preprint arXiv:1802.02547, 2018.  Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645-6649. IEEE, 2013.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv  preprint arXiv:1609.05191, 2016.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human- level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015.  BL Ho and Rudolph E Kalman. Effective construction of linear state-variable models from input/output  functions. at-Automatisierungstechnik, 14(1-12):545-548, 1966.  Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,  1997.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed  training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv preprint  arXiv:1810.02032, 2018.  Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927-935, 2011.  Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks.  arXiv preprint arXiv:1711.00811, 2017.  Michel Ledoux. The concentration of measure phenomenon. American Mathematical Soc., 2001.  Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680, 2019.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In  Advances in Neural Information Processing Systems, pages 597-607, 2017.  Lennart Ljung. System identification: theory for the user. Prentice-hall, 1987.  Lennart Ljung. System identification. In Signal analysis and prediction, pages 163-173. Springer, 1998.  Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses. The Annals  of Statistics, 46(6A):2747-2774, 2018a.  Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers  neural networks. arXiv preprint arXiv:1804.06561, 2018b.  John Miller and Moritz Hardt. When recurrent models don\u2019t need to be recurrent.  arXiv preprint  arXiv:1805.10369, 2018.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to  spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understand- ing the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018.  Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. arXiv  preprint arXiv:1806.05722, 2018.  Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the  shortest path? International Conference on Machine Learning, 2019.  Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time-data tradeoffs for linear inverse  problems. IEEE Transactions on Information Theory, 64(6):4129-4158, 2018.  Jos\u00e9 Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic differential equations.  In Advances in Neural Information Processing Systems, pages 172-180, 2010.  Borhan M Sanandaji, Tyrone L Vincent, and Michael B Wakin. Exact topology identification of large-scale interconnected dynamical systems from compressive observations. In American Control Conference (ACC), 2011, pages 649-656. IEEE, 2011a.  Borhan M Sanandaji, Tyrone L Vincent, Michael B Wakin, Roland T\u00f3th, and Kameshwar Poolla. Compressive system identification of lti and ltv arx models. In Decision and Control and European Control Conference (CDC-ECC), 2011 50th IEEE Conference on, pages 791-798. IEEE, 2011b.  Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral  methods. arXiv preprint arXiv:1603.00954, 2016.  Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing:  Towards a sharp analysis of linear system identification. arXiv preprint arXiv:1802.08334, 2018.  Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.  Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape  of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.  Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic  Processes, pages 13-73. Springer, 2014.  Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust control  from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.  Stephen Tu, Ross Boczar, and Benjamin Recht. On the approximation of toeplitz operators for nonparametric (cid:104)\u221e-norm estimation. In 2018 Annual American Control Conference (ACC), pages 1867-1872. IEEE, 2018.  Roman Vershynin.  Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with  multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.  Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for  one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS"}