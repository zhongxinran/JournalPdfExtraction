{"1": "Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In Proceedings of The 33rd International Conference on Machine Learning, pages 699-707, 2016.  S. Bittanti and P. Colaneri. Periodic systems: filtering and control. Springer Science & Business  Media, 2008.  systems, pages 217-224, 2003.  L. Bottou and Y. LeCun. Large scale online learning. In Advances in neural information processing  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Ma- chine Learning, 3(1):1-122, 2011.  C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent. Mathematical Programming, 155(1-2): 57-79, 2016.  O. Costa, M. Fragoso, and R. Marques. Discrete-time Markov jump linear systems. Springer Science  & Business Media, 2006.  Inc. CVX Research. CVX: Matlab software for disciplined convex programming, version 2.0.  http://cvxr.com/cvx, August 2012.  A. Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Informa-  tion Processing Systems, pages 676-684, 2016.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014a.  A. Defazio, J. Domke, and T. Caetano. Finito: A faster, permutable incremental gradient method for big data problems. In Proceedings of the 31st International Conference on Machine Learning, pages 1125-1133, 2014b.  V. Dragan, T. Morozan, and A. Stoica. Mathematical methods in robust control of discrete-time  linear stochastic systems. Springer, 2010.  Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: a  novel approach. Mathematical Programming, 145(1-2):451-482, 2014.  M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In Recent Ad- vances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95- 110. Springer-Verlag Limited, 2008.  J. Hespanha. Linear systems theory. Princeton university press, 2009.  M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers.  arXiv preprint arXiv:1208.3922, 2012.  25   ANALYSIS OF STOCHASTIC OPTIMIZATION METHODS USING JUMP SYSTEM THEORY  References  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In Proceedings of The 33rd International Conference on Machine Learning, pages 699-707, 2016.  S. Bittanti and P. Colaneri. Periodic systems: filtering and control. Springer Science & Business  Media, 2008.  systems, pages 217-224, 2003.  L. Bottou and Y. LeCun. Large scale online learning. In Advances in neural information processing  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Ma- chine Learning, 3(1):1-122, 2011.  C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent. Mathematical Programming, 155(1-2): 57-79, 2016.  O. Costa, M. Fragoso, and R. Marques. Discrete-time Markov jump linear systems. Springer Science  & Business Media, 2006.  Inc. CVX Research. CVX: Matlab software for disciplined convex programming, version 2.0.  http://cvxr.com/cvx, August 2012.  A. Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Informa-  tion Processing Systems, pages 676-684, 2016.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014a.  A. Defazio, J. Domke, and T. Caetano. Finito: A faster, permutable incremental gradient method for big data problems. In Proceedings of the 31st International Conference on Machine Learning, pages 1125-1133, 2014b.  V. Dragan, T. Morozan, and A. Stoica. Mathematical methods in robust control of discrete-time  linear stochastic systems. Springer, 2010.  Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: a  novel approach. Mathematical Programming, 145(1-2):451-482, 2014.  M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In Recent Ad- vances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95- 110. Springer-Verlag Limited, 2008.  J. Hespanha. Linear systems theory. Princeton university press, 2009.  M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers.  arXiv preprint arXiv:1208.3922, 2012. HU SEILER RANTZER  B. Hu. A Robust Control Perspective on Optimization of Strongly-Convex Functions. PhD thesis,  University of Minnesota, 2016.  B. Hu and L. Lessard. Dissipativity theory for Nesterov\u2019s accelerated method. In Proceedings of  the 34th International Conference on Machine Learning, 2017.  B. Hu and P. Seiler. Exponential decay rate conditions for uncertain linear systems using integral  quadratic constraints. IEEE Transactions on Automatic Control, 61(11):3561-3567, 2016.  R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-  tion. In Advances in Neural Information Processing Systems, pages 315-323, 2013.  C. Kao. On stability of discrete-time LTI systems with varying time delays. IEEE Transactions on  Automatic Control, 57:1243-1248, 2012.  C. Kao and A. Rantzer. Stability analysis of systems with uncertain time-varying delays. Automat-  ica, 43(6):959-970, 2007.  40:1429-1434, 2004.  C.Y. Kao and B. Lincoln. Simple stability criteria for systems with time-varying delays. Automatica,  H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient meth- ods under the Polyak-Lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795-811, 2016.  D. Kim and J. Fessler. Optimized first-order methods for smooth convex minimization. Mathemat-  ical programming, 159(1-2):81-107, 2016.  L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral  quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.  H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances  in Neural Information Processing Systems, pages 3384-3392, 2015.  A. Megretski and A. Rantzer. System analysis via integral quadratic constraints. IEEE Transactions  on Automatic Control, 42:819-830, 1997.  R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan. A general analysis of the conver- gence of ADMM. In Proceedings of the 32nd International Conference on Machine Learning, pages 343-352, 2015.  A. Nitanda. Stochastic proximal gradient descent with acceleration techniques.  In Advances in  Neural Information Processing Systems, pages 1574-1582, 2014.  H. Pfifer and P. Seiler. Integral quadratic constraints for delayed nonlinear and parameter-varying  systems. Automatica, 56:36 - 43, 2015.  B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693-701, 2011. ANALYSIS OF STOCHASTIC OPTIMIZATION METHODS USING JUMP SYSTEM THEORY  S. Reddi, A. Hefny, S. Sra, B. P\u00b4ocz\u00b4os, and A. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of The 33rd International Conference on Machine Learning, pages 314-323, 2016a.  S. Reddi, S. Sra, B. P\u00b4ocz\u00b4os, and A. Smola. Fast incremental method for nonconvex optimization.  In IEEE Conf. on Decision and Control, pages 1971-1977, 2016b.  H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-  tics, 22(3):400-407, 1951.  N. Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. In Advances in Neural Information Processing Systems, 2012.  M. Schmidt, N. Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.  ArXiv preprint, 2013.  S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss.  The Journal of Machine Learning Research, 14(1):567-599, 2013.  S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-  larized loss minimization. Mathematical Programming, 155:105-145, 2016.  Shai Shalev-Shwartz. SDCA without duality. arXiv preprint arXiv:1502.06177, 2015.  Shai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In Proceed-  ings of the 33rd International Conference on Machine Learning, pages 747-754, 2016.  R. Sun, Z. Luo, and Y. Ye. On the expected convergence of randomly permuted ADMM. arXiv  preprint arXiv:1503.06387, 2015.  A. Taylor, J. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case  performance of first-order methods. Mathematical Programming, 161(1-2):307-345, 2017.  C. Teo, A. Smola, S. Vishwanathan, and Q. Le. A scalable modular convex solver for regular- ized risk minimization. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 727-736, 2007.  K.C. Toh, M.J. Todd, and R.H. Tutuncu. SDPT3 - a matlab software package for semidefinite  programming. Optimization Methods and Software, 11:545-581, 1999.  R.H Tutuncu, K.C. Toh, and M.J. Todd. Solving semidefinite-quadratic-linear programs using  SDPT3. Mathematical Programming Ser. B, 95:189-217, 2003.  R. Zhang and J. Kwok. Asynchronous distributed ADMM for consensus optimization. In Proceed-  ings of the 31st International Conference on Machine Learning, pages 1701-1709, 2014. HU SEILER RANTZER"}