{"1": "Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artifi-  cial Intelligence, 201:81-105, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum and Adam Kalai. A note on learning from multiple-instance examples. Machine  Learning, 30(1):23-29, 1998.  Mathematics, 3(2):168-177, 1990.  Jehoshua Bruck. Harmonic analysis of polynomial threshold functions. SIAM Journal on Discrete  Mahdi Cheraghchi, Adam R. Klivans, Pravesh Kothari, and Homin K. Lee. Submodular functions  are noise stable. In Yuval Rabani, editor, SODA, pages 1586-1592. SIAM, 2012.  Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely. A ptas for agnostically learning halfspaces.  In Conference on Learning Theory,  pages 484-502, 2015.  Amit Daniely. Sgd learns the conjugate kernel class of the network.  In Isabelle Guyon, Ul- rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2419-2427, 2017. URL http://papers.nips.cc/paper/ 6836-sgd-learns-the-conjugate-kernel-class-of-the-network.  Thomas G Dietterich, Richard H Lathrop, and Tom\u00b4as Lozano-P\u00b4erez. Solving the multiple instance  problem with axis-parallel rectangles. Artificial intelligence, 89(1):31-71, 1997.  Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de- scent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.  Vitaly Feldman. Learning dnf expressions from fourier spectrum. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT, volume 23 of JMLR Proceedings, pages 17.1-17.19. JMLR.org, 2012. URL http://jmlr.org/proceedings/papers/v23/.  Vitaly Feldman. Statistical query learning. In Encyclopedia of Algorithms, pages 2090-2095. 2016.  Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one- hidden-layer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  13   LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  References  Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artifi-  cial Intelligence, 201:81-105, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum and Adam Kalai. A note on learning from multiple-instance examples. Machine  Learning, 30(1):23-29, 1998.  Mathematics, 3(2):168-177, 1990.  Jehoshua Bruck. Harmonic analysis of polynomial threshold functions. SIAM Journal on Discrete  Mahdi Cheraghchi, Adam R. Klivans, Pravesh Kothari, and Homin K. Lee. Submodular functions  are noise stable. In Yuval Rabani, editor, SODA, pages 1586-1592. SIAM, 2012.  Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely. A ptas for agnostically learning halfspaces.  In Conference on Learning Theory,  pages 484-502, 2015.  Amit Daniely. Sgd learns the conjugate kernel class of the network.  In Isabelle Guyon, Ul- rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2419-2427, 2017. URL http://papers.nips.cc/paper/ 6836-sgd-learns-the-conjugate-kernel-class-of-the-network.  Thomas G Dietterich, Richard H Lathrop, and Tom\u00b4as Lozano-P\u00b4erez. Solving the multiple instance  problem with axis-parallel rectangles. Artificial intelligence, 89(1):31-71, 1997.  Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de- scent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.  Vitaly Feldman. Learning dnf expressions from fourier spectrum. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT, volume 23 of JMLR Proceedings, pages 17.1-17.19. JMLR.org, 2012. URL http://jmlr.org/proceedings/papers/v23/.  Vitaly Feldman. Statistical query learning. In Encyclopedia of Algorithms, pages 2090-2095. 2016.  Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one- hidden-layer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with  symmetric inputs. arxiv preprint arxiv:1810.06793, 2018.  Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability of neural  networks. In NIPS, 2017.  Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-  mial time. arXiv preprint arXiv:1611.10258, 2016.  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping  patches. arXiv preprint arXiv:1802.02547, 2018.  Parikshit Gopalan, Adam Tauman Kalai, and Adam R Klivans. Agnostically learning decision trees. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 527-536. ACM, 2008.  Francisco Herrera, Sebasti\u00b4an Ventura, Rafael Bello, Chris Cornelis, Amelia Zafra, D\u00b4anel S\u00b4anchez- Tarrag\u00b4o, and Sarah Vluymans. Multiple instance learning. In Multiple Instance Learning, pages 17-33. Springer, 2016.  Jeffrey C. Jackson. An efficient membership-query algorithm for learning dnf with respect to the  uniform distribution. J. Comput. Syst. Sci, 55(3):414-440, 1997.  Jeffrey C. Jackson, Adam R. Klivans, and Rocco A. Servedio. Learnability beyond AC\u02c60. In Pro- ceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC-02), pages 776- 784, New York, May 19-21 2002. ACM Press.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793-800, 2009.  of  Sham M. Kakade, Adam Kalai, Varun Kanade, and Ohad Shamir. and 2011.  ing In NIPS, advances-in-neural-information-processing-systems-24-2011.  learn- index models with regression. URL http://papers.nips.cc/book/  generalized pages  linear 927-935,  Efficient  isotonic  single  Adam Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009. URL http://www.cs.mcgill.ca/\u02dccolt2009/papers/001.pdf# page=1.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically  learning halfspaces. SIAM J. Comput., 37(6):1777-1805, 2008.  Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic con- In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on,  cepts. pages 382-391. IEEE, 1990. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  A. Klivans, R. O\u2019Donnell, and R. Servedio. Learning intersections and thresholds of halfspaces.  JCSS: Journal of Computer and System Sciences, 68, 2004.  Adam R. Klivans and Raghu Meka. Moment-matching polynomials. Electronic Colloquium on Computational Complexity (ECCC), 20:8, 2013. URL http://eccc.hpi-web.de/ report/2013/008.  Adam R Klivans and Rocco A Servedio. Learning intersections of halfspaces with a margin. Journal  of Computer and System Sciences, 74(1):35-48, 2008.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci, 75(1):2-12, 2009. URL http://dx.doi.org/10.1016/ j.jcss.2008.07.008.  Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the fourier spectrum. SIAM  Journal on Computing, 22(6):1331-1348, 1993.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Lee, Bartlett, and Williamson. Lower bounds on the VC-dimension of smoothly parametrized function classes. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 1994.  Linial, Mansour, and Nisan. Constant depth circuits, fourier transform, and learnability. JACM:  Journal of the ACM, 40, 1993.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014.  Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.  In Advances in neural information processing systems, pages 2627-2635, 2014.  Mossel, O\u2019Donnell, and Servedio. Learning juntas.  In STOC: ACM Symposium on Theory of  Computing (STOC), 2003.  Anirbit Mukherjee and Amitabh Basu. Lower bounds over boolean inputs for deep neural networks  with relu gates. arXiv preprint arXiv:1711.03073, 2017.  Ramamohan Paturi. On the degree of polynomials that approximate symmetric Boolean functions In Proceedings of the Twenty-Fourth Annual ACM Symposium on the  (preliminary version). Theory of Computing, pages 468-474, Victoria, British Columbia, Canada, 4-6 May 1992.  Sivan Sabato and Naftali Tishby. Multi-instance learning with any hypothesis class. Journal of  Machine Learning Research, 13(Oct):2999-3039, 2012.  Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non- linear functions. CoRR, abs/1610.09887, 2016. URL http://arxiv.org/abs/1610. 09887. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, and Aurelio Uncini. Kafnets: kernel- based non-parametric activation functions for neural networks. arXiv preprint arXiv:1707.04035, 2017.  Bernhard Sch\u00a8olkopf and Alexander J Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT press, 2002.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with  the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, 2011.  Ohad Shamir. Distribution-specific hardness of learning neural networks.  arXiv preprint  arXiv:1609.01037, 2016.  Alexander A Sherstov. Making polynomials robust to noise.  In Proceedings of the forty-fourth  annual ACM symposium on Theory of computing, pages 747-758. ACM, 2012.  Alex Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00a8olkopf. A hilbert space embedding for dis- tributions. In International Conference on Algorithmic Learning Theory, pages 13-31. Springer, 2007.  Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591,  2017.  Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural net-  works. arXiv preprint arXiv:1707.04615, 2017.  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities  and the closest pair problem. Journal of the ACM (JACM), 62(2):13, 2015.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Wikipedia. Multinomial theorem \u2014 Wikipedia, the free encyclopedia, 2016. [Online; accessed  30-October-2016].  Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva. Electron-proton dynamics in deep learning.  CoRR, abs/1702.00458, 2017. URL http://arxiv.org/abs/1702.00458.  Yuchen Zhang, Jason Lee, and Michael Jordan. \u21131 networks are improperly learnable in polynomial-  time. In ICML, 2016.  Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In ICML, volume 70, pages 4140-4149. JMLR.org, 2017. URL http://jmlr.org/proceedings/papers/v70/. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME"}