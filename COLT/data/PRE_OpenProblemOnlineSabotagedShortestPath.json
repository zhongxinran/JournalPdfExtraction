{"1": "Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial optimization.  Math. Oper. Res., 39(1):31-45, 2014.  Nicol`o Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and  feels no regret). In Advances in Neural Information Processing Systems 25, pages 980-988, 2012.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proc. 29th Annual ACM Symposium on Theory of Computing, pages 334-343, 1997.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. Transactions on Computation  Theory, 6(3):11:1-11:16, 2014.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts  and bandits. Machine Learning, 80(2-3):245-272, 2010.  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In Proceedings  of the 23rd Annual Conference on Learning Theory, pages 93-105, 2010.  Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets and adver-  sarial losses. In Advances in Neural Information Processing Systems 27, pages 2780-2788, 2014.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine  Learning Research, 4:773-818, 2003.  1. Fancier bounds with mixture comparators and relative entropy are also available, but outside the scope of this note.  3   OPEN PROBLEM: ONLINE SABOTAGED SHORTEST PATH  \u221a  t:i\u2208At  (cid:1) \u2264  (cid:0)(cid:96)t,it \u2212 (cid:96)t,i  2012)1 that the the regret compared to each reference specialist i \u2208 {1, . . . , N } is bounded by E (cid:80) T ln N where the sum is taken over the rounds where expert i was awake, and the expectation E scopes the algorithm\u2019s random choice of expert it. Note that this regret has the same shape as (1). The algorithm runs in time O(N ) per round and uses O(N ) space. Sabotage can be reduced to specialists quite naturally as follows. We enumerate all source-sink paths P1, . . . , PD (note that D can be exponential in the size of the graph). We then use these paths as specialists, where a path P is awake at time t iff all its edges e \u2208 P are awake e \u2208 At. Disregarding gross inefficiency, the specialists algorithm would deliver the following regret bound. For any comparator path P , the expected regret (1) is bounded by RegretT (P ) \u2264 K T ln D where K is the length of the longest path (i.e. the loss range).  \u221a  Discussion This regret bound has two issues. First, we know that its regret bound is sub-optimal. In the simple case where no edges are sabotaged, the Component Hedge algorithm (instance of \u221a K for many graphs. This is due to the so-called mirror descent) improves the regret by a factor range factor problem discussed by Koolen et al. (2010) that arises from the fact that the algorithm does not take advantage of the fact that paths may overlap, and that their losses cannot be controlled independently. Second, and perhaps more importantly, the running time is horrible as one parameter is maintained per path. We have efficient algorithms for combinatorial settings without sleeping. These algorithms are \u201ccollapsed\u201d: they maintain one parameter per edge. The question is hence whether and how this methodology can be extended to incorporate sabotaged / sleeping edges.  We believe that Online Sabotaged Shortest Path a core problem which will become a starting point for various orthogonal extension such as switching between paths, semi-bandit feedback, etc.  References  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial optimization.  Math. Oper. Res., 39(1):31-45, 2014.  Nicol`o Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and  feels no regret). In Advances in Neural Information Processing Systems 25, pages 980-988, 2012.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proc. 29th Annual ACM Symposium on Theory of Computing, pages 334-343, 1997.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. Transactions on Computation  Theory, 6(3):11:1-11:16, 2014.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts  and bandits. Machine Learning, 80(2-3):245-272, 2010.  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In Proceedings  of the 23rd Annual Conference on Learning Theory, pages 93-105, 2010.  Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets and adver-  sarial losses. In Advances in Neural Information Processing Systems 27, pages 2780-2788, 2014.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine  Learning Research, 4:773-818, 2003.  1. Fancier bounds with mixture comparators and relative entropy are also available, but outside the scope of this note."}