{"1": "Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classi-  fication and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018a.  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-  stand kernel learning. arXiv preprint arXiv:1802.01396, 2018b.  Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict  statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-  parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-  mation Processing Systems, pages 2422-2430, 2017.  Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes  over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.  Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):  1-50, 2010.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of  nonparametric regression. Springer Science & Business Media, 2006.  Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler. Neural tangent kernel: Convergence and gen-  eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.  10   INTERPOLATION LOWER BOUNDS  allows, under a number of additional assumptions, the estimation error to be small. The interaction of dimensionality, sample size, and eigenvalue decays for the population and sample covariance matrices is complex, and identifying all the regimes when interpolation succeeds is still a largely unexplored area. In particular, our lower bound becomes vacuous as soon as d starts to scale with n. It would be interesting to understand the minimal scaling of d along with assumptions on the underlying distribution that allow minimum-norm interpolation to succeed.  Partial motivation for the study of interpolation methods comes from the recent successes of neural networks. These overparametrized models are typically trained to achieve zero error on the training data (Zhang et al., 2016; Belkin et al., 2018b), yet perform well out-of-sample. Recent work connecting sufficiently wide neural networks and the effective kernel (Mei et al., 2018; Chizat and Bach, 2018; Daniely, 2017; Jacot et al., 2018; Du et al., 2018) suggests that interpolating neural networks can be studied through the lens of kernel methods. In particular, it can be shown that the limiting solutions in such cases are, in fact, minimum-norm interpolants with respect to the corre- sponding kernel. Hence, further study of strengths and limitations of minimum-norm interpolation can shed light on performance of neural networks.  This work was partly supported by the DARPA Lagrange program and the MIT-Sensetime Alliance on Artificial Intelligence.  Acknowledgments  References  Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classi-  fication and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018a.  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-  stand kernel learning. arXiv preprint arXiv:1802.01396, 2018b.  Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict  statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-  parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-  mation Processing Systems, pages 2422-2430, 2017.  Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes  over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.  Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):  1-50, 2010.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of  nonparametric regression. Springer Science & Business Media, 2006.  Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler. Neural tangent kernel: Convergence and gen-  eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018. INTERPOLATION LOWER BOUNDS  Giovanni Leoni. A First Course in Sobolev Spaces, Second Edition. American Mathematical Soci-  ety, 2017.  Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel\u201d ridgeless\u201d regression can gener-  alize. arXiv preprint arXiv:1808.00387, 2018.  Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of  two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.  Elias M Stein and Guido Weiss. Introduction to Fourier analysis on Euclidean spaces (PMS-32).  Princeton University Press, 1971.  Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of adaboost and random forests as interpolating classifiers. The Journal of Machine Learning Re- search, 18(1):1558-1590, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."}