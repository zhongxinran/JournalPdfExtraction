{"1": "Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. Electronic Journal of  Statistics, 11(2):5165-5197, 2017.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. In Conference on Learning Theory, pages 583-589, 2016.  Shi Dong and Benjamin Van Roy. An information-theoretic analysis for Thompson sampling with  many actions. In Advances in Neural Information Processing Systems, 2018.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-  tual bandits. In International Conference on Machine Learning, pages 2071-2080, 2017.  Fang Liu, Swapna Buccapatnam, and Ness Shroff.  Information directed sampling for stochastic bandits with graph feedback. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.  Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In  Advances in Neural Information Processing Systems, pages 1583-1591, 2014a.  Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of  Operations Research, 39(4):1221-1243, 2014b.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.  The Journal of Machine Learning Research, 17(1):2442-2471, 2016.  Daniel Russo and Benjamin Van Roy. Satisficing in time-sensitive bandit learning. arXiv preprint  arXiv:1803.02855, 2018.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.  3   ON THE PERFORMANCE OF THOMPSON SAMPLING ON LOGISTIC BANDITS  The assumption that the worst-case optimal log-odds are positive may be restrictive. This is equivalent to assuming that the for each possible model, the optimal action yields more than 50% probability of success. However, this assumption is essential, since it ensures that the fragility di- mension is well-defined. When the worst-case optimal log-odds are negative, the geometry of action and parameter sets plays a less significant role than parameter \u03b2, therefore we conjecture that the exponential dependence on \u03b2 is inevitable. This could be an interesting direction for future research.  References  Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. Electronic Journal of  Statistics, 11(2):5165-5197, 2017.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. In Conference on Learning Theory, pages 583-589, 2016.  Shi Dong and Benjamin Van Roy. An information-theoretic analysis for Thompson sampling with  many actions. In Advances in Neural Information Processing Systems, 2018.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-  tual bandits. In International Conference on Machine Learning, pages 2071-2080, 2017.  Fang Liu, Swapna Buccapatnam, and Ness Shroff.  Information directed sampling for stochastic bandits with graph feedback. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.  Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In  Advances in Neural Information Processing Systems, pages 1583-1591, 2014a.  Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of  Operations Research, 39(4):1221-1243, 2014b.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.  The Journal of Machine Learning Research, 17(1):2442-2471, 2016.  Daniel Russo and Benjamin Van Roy. Satisficing in time-sensitive bandit learning. arXiv preprint  arXiv:1803.02855, 2018.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933."}