{"1": "N. Bshouty and V. Feldman. On using extended statistical queries to avoid membership  queries. Journal of Machine Learning Research, 2:359-395, 2002. ISSN 1533-7928.  H. Buhrman, N. Vereshchagin, and R. de Wolf. On computation and communication with In Proceedings of IEEE Conference on Computational Complexity, pages  small bias. 24-32, 2007.  D. Diochnos and G. Tur\u00b4an. On evolvability: The swapping algorithm, product distributions, and covariance. In Proceedings of Stochastic Algorithms: Foundations and Applications (SAGA), pages 74-88, 2009.  270   Feldman  initiated by Forster (2002) and Forster et al. (2003) (see Linial et al., 2007; Sherstov, 2007; Linial and Shraibman, 2009, for some recent results). The inverse of the optimal margin is referred to as the margin complexity of a concept class (Linial et al., 2007). Besides its importance to machine learning, it has several connections to fundamental quantities in communication complexity (Goldmann et al., 1992; Sherstov, 2007; Linial and Shraibman, 2009). We cannot invoke this measure directly to upper-bound the complexity of using our evolution algorithm since margin complexity disregards the computational complexity of the embedding function. But given an e\ufb03cient embedding function the application of our evolution algorithm becomes straightforward.  Corollary 19 Let C be a concept class over domain X, X (cid:48) \u2286 Bn and \u03b3 > 1/q(n) for some polynomial q(\u00b7). If there exists an e\ufb03ciently computable embedding of C over X to HS\u03b3(X (cid:48)) over X (cid:48), then C is evolvable monotonically with any well-behaved loss function.  5. Conclusions and Open Problems  Our lower bound in Section 3 provides strong hardness results for learning that is limited to observing the accuracy (or alternatively, Boolean loss performance) of hypotheses. In particular, it implies that evolvability in Valiant\u2019s original model is severely limited unless the distribution over the domain is strongly restricted. An interesting direction for fur- ther work would be to find a complete characterization of distribution independent CSQ learnability (as was recently achieved for SQ learnability (Simon, 2007; Feldman, 2009b; Sz\u00a8or\u00b4enyi, 2009)). Potentially simpler questions left open in this work are whether conjunc- tion are CSQ learnable to constant accuracy (say 1/4) and whether the lower bound for conjunctions can be strengthened from a quasi-polynomial to an exponential number of queries.  At the same time results of Section 4 demonstrate that the limitation of Boolean loss can be overcome by using a real-valued hypotheses with a non-linear loss function. The evolution algorithm we described is based on the first mutation algorithm that is simple, general and robust enough to be a plausible candidate for biological evolution. It would be interesting to know if similar result can be proved for other SQ learnable concept classes (e.g. general linear threshold functions) and whether the result can be extended to more general loss functions.  References  N. Bshouty and V. Feldman. On using extended statistical queries to avoid membership  queries. Journal of Machine Learning Research, 2:359-395, 2002. ISSN 1533-7928.  H. Buhrman, N. Vereshchagin, and R. de Wolf. On computation and communication with In Proceedings of IEEE Conference on Computational Complexity, pages  small bias. 24-32, 2007.  D. Diochnos and G. Tur\u00b4an. On evolvability: The swapping algorithm, product distributions, and covariance. In Proceedings of Stochastic Algorithms: Foundations and Applications (SAGA), pages 74-88, 2009. Evolvability of LTF  V. Feldman. Evolvability from learning algorithms. In Proceedings of STOC, pages 619-628,  2008.  V. Feldman. Robustness of evolvability. In Proceedings of COLT, pages 277-292, 2009a.  V. Feldman. A complete characterization of statistical query learning with applications to  evolvability. In Proceedings of FOCS, pages 375-384, 2009b.  V. Feldman and L. G. Valiant. The learning power of evolution. In Proceedings of COLT,  pages 513-514, 2008.  J. Forster. A linear lower bound on the unbounded error probabilistic communication  complexity. Journal of Computer and System Sciences, 65(4):612-625, 2002.  J. Forster, N. Schmitt, H.U. Simon, and T. Suttorp. Estimating the optimal margins of  embeddings in euclidean half spaces. Machine Learning, 51(3):263-281, 2003.  Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation,  121(2):256-285, 1995.  M. Goldmann, J. H\u02daastad, and A. Razborov. Majority gates vs. general weighted threshold  gates. Computational Complexity, 2:277-300, 1992.  J. Jackson. An e\ufb03cient membership-query algorithm for learning DNF with respect to the  uniform distribution. Journal of Computer and System Sciences, 55:414-440, 1997.  B. Jacobson, 2007. Personal communication with L. Valiant.  V. Kanade, L. G. Valiant, and J. Wortman Vaughan. Evolution with drifting targets. In  Proceedings of COLT, pages 155-167, 2010.  M. Kearns. E\ufb03cient noise-tolerant learning from statistical queries. Journal of the ACM,  45(6):983-1006, 1998.  M. Kearns and L. Valiant. Cryptographic limitations on learning boolean formulae and  finite automata. Journal of the ACM, 41(1):67-95, 1994.  M. Kearns, R. Schapire, and L. Sellie. Toward e\ufb03cient agnostic learning. Machine Learning,  17(2-3):115-141, 1994.  A. Klivans and R. Servedio. Learning DNF in time 2  System Sciences, 68(2):303-318, 2004.  \u02dcO(n1/3). Journal of Computer and  N. Linial and A. Shraibman. Learning complexity vs communication complexity. Combi-  natorics, Probability & Computing, 18(1-2):227-245, 2009.  N. Linial, S. Mendelson, G. Schechtman, and A. Shraibman. Complexity measures of sign  matrices. Combinatorica, 27(4):439-463, 2007.  N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold  algorithm. Machine Learning, 2:285-318, 1987. Feldman  L. Michael. Evolving decision lists. Manuscript, 2007.  F. Rosenblatt. The perceptron: a probabilistic model for information storage and organi-  zation in the brain. Psychological Review, 65:386-407, 1958.  R. Schapire. The strength of weak learnability. Machine Learning, 5(2):197-227, 1990.  A. A. Sherstov. Halfspace matrices. In Proceedings of Conference on Computational Com-  plexity, pages 83-95, 2007.  H. Simon. A characterization of strong learnability in the statistical query model.  In Proceedings of Symposium on Theoretical Aspects of Computer Science, pages 393-404, 2007.  B. Sz\u00a8or\u00b4enyi. Characterizing statistical query learning:simplified notions and proofs. In ALT,  pages 186-200, 2009.  1984.  ECCC, 2006.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142,  L. G. Valiant. Evolvability. Journal of the ACM, 56(1):3.1-3.21, 2009. Earlier version in  P. Valiant. Distribution free evolvability of real functions over all convex loss functions.  Unpublished manuscript, 2011."}