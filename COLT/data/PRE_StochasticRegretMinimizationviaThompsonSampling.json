{"1": "S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem.  Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. CoRR,  abs/1209.3353, 2012b.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Babu, N. Borisov, S. Duan, H. Herodotou, and V. Thummala. Automated experiment-driven  management of (database) systems. Proc. of HotOS, 2009.  S\u00b4ebastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson  sampling. NIPS, pages 638-646, 2013.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for sequential alloca-  tion problems. Advances in Applied Mathematics, 17(2):122 - 142, 1996.  Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages  2249-2257, 2011.  B. C. Dean, M. X. Goemans, and J. Vondrak. Approximating the stochastic knapsack problem: The benefit of adaptivity. In FOCS \u201904: Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science, pages 208-217, 2004.  A. Demberel, J. Chase, and S. Babu. Re\ufb02ective control for an elastic cloud appliation: An automated  experiment workbench. Proc. of HotCloud, 2009.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient  optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.  A. Garivier and O. Capp\u00b4e. The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond.  In Proc. COLT, 2011.  J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments.  Progress in statistics (European Meeting of Statisticians), 1972.  A. Goel, S. Khanna, and B. Null. The ratio index for budgeted learning, with applications. In SODA,  pages 18-27, 2009.  ICML, 2014.  58(1), 2010.  A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex bandit problems. In  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. J. ACM,  H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based optimization of mapreduce  programs. Proc. of VLDB, 2011.  13   THOMPSON SAMPLING  References  S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem.  Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. CoRR,  abs/1209.3353, 2012b.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Babu, N. Borisov, S. Duan, H. Herodotou, and V. Thummala. Automated experiment-driven  management of (database) systems. Proc. of HotOS, 2009.  S\u00b4ebastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson  sampling. NIPS, pages 638-646, 2013.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for sequential alloca-  tion problems. Advances in Applied Mathematics, 17(2):122 - 142, 1996.  Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages  2249-2257, 2011.  B. C. Dean, M. X. Goemans, and J. Vondrak. Approximating the stochastic knapsack problem: The benefit of adaptivity. In FOCS \u201904: Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science, pages 208-217, 2004.  A. Demberel, J. Chase, and S. Babu. Re\ufb02ective control for an elastic cloud appliation: An automated  experiment workbench. Proc. of HotCloud, 2009.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient  optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.  A. Garivier and O. Capp\u00b4e. The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond.  In Proc. COLT, 2011.  J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments.  Progress in statistics (European Meeting of Statisticians), 1972.  A. Goel, S. Khanna, and B. Null. The ratio index for budgeted learning, with applications. In SODA,  pages 18-27, 2009.  ICML, 2014.  58(1), 2010.  A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex bandit problems. In  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. J. ACM,  H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based optimization of mapreduce  programs. Proc. of VLDB, 2011. GUHA MUNAGALA  Emilie Kaufmann, Nathaniel Korda, and Remi Munos. Thompson sampling: An asymptotically  optimal finite-time analysis. Proceedings of ALT, 2012.  T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6:4-22, 1985.  15(3):1091-1114, 1987.  Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist.,  L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news  article recommendation. In WWW, pages 661-670, 2010.  H. Robbins. Some aspects of the sequential design of experiments. Bulletin American Mathematical  Society, 55:527-535, 1952.  Daniel Russo and Benjamin Van-Roy. Learning to optimize via posterior sampling. CORR; http:  //arxiv.org/abs/1301.2609, 2013.  Steven L. Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in  Business and Industry, 26(6):639-658, 2010.  William R. Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):pp. 285-294, 1933.  N. Vulkan. An economist\u2019s perspective on probability matching. Journal of Economic Surveys,  1992."}