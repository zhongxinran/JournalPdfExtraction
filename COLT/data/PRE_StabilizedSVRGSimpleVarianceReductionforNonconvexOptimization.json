{"1": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arX-  iv:1708.08694, 2017.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zhi-Dong Bai and Yong-Qua Yin. Necessary and sufficient conditions for almost sure convergence of the largest eigenvalue of a wigner matrix. The Annals of Probability, pages 1729-1741, 1988.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57 (4):2342-2359, 2011.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646-1654, 2014.  Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067-1077, 2017.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Infor- mation Processing Systems, pages 687-697, 2018.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a.  13   STABILIZED SVRG  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arX-  iv:1708.08694, 2017.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zhi-Dong Bai and Yong-Qua Yin. Necessary and sufficient conditions for almost sure convergence of the largest eigenvalue of a wigner matrix. The Annals of Probability, pages 1729-1741, 1988.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57 (4):2342-2359, 2011.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646-1654, 2014.  Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067-1077, 2017.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Infor- mation Processing Systems, pages 687-697, 2018.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a. STABILIZED SVRG  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017b.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017a.  Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle  points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance  reduction. In Advances in neural information processing systems, pages 315-323, 2013.  Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345-2355, 2017.  Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex  optimization. arXiv preprint arXiv:1802.04477, 2018.  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314-323, 2016.  Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an expo- nential convergence rate for finite training sets. In Advances in neural information processing systems, pages 2663-2671, 2012.  Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized  loss minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.  Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.  Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2904-2913, 2018.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Yi Xu, Jing Rong, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. In Advances in Neural Information Processing Systems, pages 5535- 5545, 2018.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Finding local minima via stochastic nested variance  reduction. arXiv preprint arXiv:1806.08782, 2018a.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex  optimization. arXiv preprint arXiv:1806.07811, 2018b. STABILIZED SVRG"}