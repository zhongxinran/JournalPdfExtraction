{"1": "Emmanuel Abbe, Tamir Bendory, William Leeb, Jo\u02dcao M Pereira, Nir Sharon, and Amit Singer. Multireference alignment is easier with an aperiodic translation distribution. IEEE Transactions on Information Theory, 2018a.  Emmanuel Abbe, Jo\u02dcao M Pereira, and Amit Singer. Estimation in the group action channel. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 561-565. IEEE, 2018b.  Carlos Am\u00b4endola, Kristian Ranestad, and Bernd Sturmfels. Algebraic identifiability of gaussian  mixtures. International Mathematics Research Notices, 2016.  12   GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Therefore, the second assumption of Lemma 11 is satisfied, thanks to (Vaart, 1998, Lemma 19.38). We have proven that all the assumptions of Lemma 11 are satisfied, which yields Theorem 12.  Perhaps surprinsingly, these rates do not depend on the size of H and this theorem shows that any \u03b8\u2217 could be estimated, up to identifiability, at least at the rate n\u22121/4 via the MLE. However, Theorem 12 does not provide uniform bounds. In fact, the OP\u03b8\u2217 signs contain constants that depend on \u03b8\u2217 and may become arbitrarily large. For instance, one expects that the constants hidden in the n1/2 should blow up when two modes \u03b8\u2217 and g\u03b8\u2217, for g /\u2208 H, are distinct but arbitrarily close to each other.  4. Conclusion  In this work, we have exhibited two different pointwise rates for the estimation of the parameter of a mixture of Gaussian distributions with uniform weights, under the invariance of an isometry group action: n\u22121/2 and n\u22121/4. Even in the second regime, we have shown that some components of \u03b8\u2217 could still be estimated at the fast rate n\u22121/2, and we have provided an algebraic description and a geometric interpretation of this fact, in terms of colliding modes of the population log-likelihood. These rates are consistent with the usual pointwise rates obtained known in the literature, even though here, we focused on parameter estimation (as opposed to distribution learning) with respect to the Euclidean loss.  As expected for general mixtures Chen (1995), the slow regime n\u22121/4 occurs when the actual number of components in the mixture is strictly less than the number predicted by the model, here, |G|. In other words, for general mixtures, slower rates occur when the model is overparametrized. However, here, this analogy should be made carefully because the presence of symmetries in \u03b8\u2217 is not necessarily implying an overparametrization.  The projection \u00afH depends on \u03b8\u2217. Therefore, even if Theorem 12 states that some components of \u03b8\u2217 are estimated at the parametric rate n\u22121/2 while the other components are estimated at the slower rate n\u22121/4, the linear subspaces corresponding to these components are unknown. The problem of recovering \u00afH or, more generally, H, is somewhat equivalent to learning the symmetries of \u03b8\u2217. If one assumes that inf g /\u2208H (cid:107)g\u03b8\u2217 \u2212 \u03b8\u2217(cid:107) is bounded away from zero by some known constant, then H can be recovered easily. In general, the estimation of H is a more challenging problem, which we leave for further work.  References  Emmanuel Abbe, Tamir Bendory, William Leeb, Jo\u02dcao M Pereira, Nir Sharon, and Amit Singer. Multireference alignment is easier with an aperiodic translation distribution. IEEE Transactions on Information Theory, 2018a.  Emmanuel Abbe, Jo\u02dcao M Pereira, and Amit Singer. Estimation in the group action channel. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 561-565. IEEE, 2018b.  Carlos Am\u00b4endola, Kristian Ranestad, and Bernd Sturmfels. Algebraic identifiability of gaussian  mixtures. International Mathematics Research Notices, 2016. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Martin Azizyan, Aarti Singh, and Larry Wasserman. Minimax theory for high-dimensional gaussian mixtures with sparse mean separation. In Advances in Neural Information Processing Systems, pages 2139-2147, 2013.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Afonso Bandeira, Philippe Rigollet, and Jonathan Weed. Optimal rates of estimation for multi-  reference alignment. arXiv preprint arXiv:1702.08546, 2017a.  Afonso S Bandeira, Moses Charikar, Amit Singer, and Andy Zhu. Multireference alignment using semidefinite programming. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 459-470. ACM, 2014.  Afonso S Bandeira, Ben Blum-Smith, Joe Kileel, Amelia Perry, Jonathan Weed, and Alexan- der S Wein. Estimation under group actions: recovering orbits from invariants. arXiv preprint arXiv:1712.10163, 2017b.  Jiahua Chen. Optimal rate of convergence for finite mixture models. The Annals of Statistics, pages  221-233, 1995.  Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of computer science, 1999. 40th  annual symposium on, pages 634-644. IEEE, 1999.  Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753-760. ACM, 2015.  Philippe Heinrich and Jonas Kahn. Optimal rates for finite mixture estimation. arXiv preprint  arXiv:1507.04313, 2015.  Nhat Ho and XuanLong Nguyen. Singularity structures and impacts on parameter estimation in  finite mixtures of distributions. arXiv preprint arXiv:1609.02655, 2016.  Nhat Ho, XuanLong Nguyen, et al. Convergence rates of parameter estimation for some weakly  identifiable finite mixtures. The Annals of Statistics, 44(6):2726-2755, 2016.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and In Proceedings of the 4th conference on Innovations in Theoretical  spectral decompositions. Computer Science, pages 11-20. ACM, 2013.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553-562. ACM, 2010.  Michael R. Kosorok. Introduction to empirical processes and semiparametric inference. Springer  Series in Statistics. Springer, New York, 2008. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93- 102. IEEE, 2010.  XuanLong Nguyen et al. Convergence of latent mixing measures in finite and infinite mixture  models. The Annals of Statistics, 41(1):370-400, 2013.  Amelia Perry, Jonathan Weed, Afonso Bandeira, Philippe Rigollet, and Amit Singer. The sample  complexity of multi-reference alignment. arXiv preprint arXiv:1707.00943, 2017.  Arora Sanjeev and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the  thirty-third annual ACM symposium on Theory of computing, pages 247-257. ACM, 2001.  Amit Singer and Yoel Shkolnisky. Three-dimensional structure determination from common lines in cryo-em by eigenvectors and semidefinite programming. SIAM journal on imaging sciences, 4 (2):543-572, 2011.  COS Sorzano, JR Bilbao-Castro, Y Shkolnisky, M Alcorlo, R Melero, G Caffarena-Fern\u00b4andez, M Li, G Xu, R Marabini, and JM Carazo. A clustering approach to multireference alignment of single-particle projections in electron microscopy. Journal of structural biology, 171(2):197-206, 2010.  A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Math-  ematics. Cambridge University Press, 1998.  Alexander Spence Wein. Statistical estimation in the presence of group actions. PhD thesis, Mas-  sachusetts Institute of Technology, 2018.  Yihong Wu and Pengkun Yang. Optimal estimation of gaussian mixtures via denoised method of  moments. Technical report, working paper, 2018. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE"}