{"1": "arXiv:1003.1129, 2010.  A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses.  A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of  multilayer networks. In AISTATS, 2015.  Y. Dauphin, R. Pascanu, C\u00b8 . G\u00a8ulc\u00b8ehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking  the saddle point problem in high-dimensional non-convex optimization. In NIPS. 2014.  M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Freitas. Predicting parameters in deep  Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the  V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,  learning. In NIPS. 2013.  trade. Springer, 1998.  2010.  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning  in deep linear neural networks. In ICLR. 2014.  1. Index of \u22072L at w is the number of negative eigenvalues of the Hessian \u22072L at w. Local minima have index 0.  3   OPEN PROBLEM: THE LANDSCAPE OF THE LOSS SURFACES OF MULTILAYER NETWORKS  EM,I1,I2,...,I\u03a8[L(w)] = \u03c1  (cid:48)  + \u03c1  Zi1,i2,...,iHwi1wi2 . . . wiH.  1 \u039b(H\u22121)/2  \u039b (cid:88)  i1,i2,...,iH=1  (cid:80)\u039b  i = 1.  i=1 w2  It is also assumed that Z\u2019s are independent (A6u) . Finally, the spherical assumption (A7p) imposes that 1 \u039b Note that the term in bold is a Hamiltonian of the spherical spin-glass model Auffinger et al. (2010). It was recently shown Auffinger et al. (2010) that the Hamiltonian of this model has inter- esting properties when the size of the model (\u039b) goes to \u221e. We next list these properties along with the possible interpretation for neural networks: (i) critical points form an ordered structure such that there exists an energy barrier (a certain value of the Hamiltonian) below which with overwhelming probability one can find only low-index1 critical points, most of which are concentrated close to the barrier (this would explain why in case of large networks recovered local minima are typically cor- responding to the same test performance which is not the case for small networks, (ii) Recovering the ground state, i.e. global minimum, takes exponentially long time, (iii) with overwhelming prob- ability one can find only high-index saddle points above energy E\u2212\u221e and there are exponentially many of those (this would explain the importance of saddle points in the optimization problem), (iv) low-index critical points are \u2019geometrically\u2019 lying closer to the ground state than high-index critical points (this would explain why recovering poor quality local minima, which are \u2019far\u2019 from the global minimum, is more likely for small-size networks than for large-size networks).  Open problem: Is it possible to establish a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models under milder assumptions? The central problem is to eliminate unrealistic assumptions of variable independence (A5-6u). Note that assumption A5u implies that the activation mechanism of any path (for the ith path it is denoted as Ii) is independent of the input data, which clearly cannot be true. Similarly, assumption A6u implies all paths have independent inputs, which cannot be true since many paths share the same input. Alternatively, it would also be desired to find network architectures for which the connection to spin-glass models can be established explicitly with only mild (plausible), if any, assumptions.  References  arXiv:1003.1129, 2010.  A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses.  A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of  multilayer networks. In AISTATS, 2015.  Y. Dauphin, R. Pascanu, C\u00b8 . G\u00a8ulc\u00b8ehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking  the saddle point problem in high-dimensional non-convex optimization. In NIPS. 2014.  M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Freitas. Predicting parameters in deep  Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the  V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,  learning. In NIPS. 2013.  trade. Springer, 1998.  2010.  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning  in deep linear neural networks. In ICLR. 2014.  1. Index of \u22072L at w is the number of negative eigenvalues of the Hessian \u22072L at w. Local minima have index 0. CHOROMANSKA LECUN BEN AROUS"}