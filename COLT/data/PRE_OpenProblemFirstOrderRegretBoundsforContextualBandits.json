{"1": "A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A fast and simple  algorithm for contextual bandits. In ICML, 2014.  A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas,  S. Sen, and A. Slivkins. A multiworld testing decision service. arXiv:1606.03966, 2016.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and G. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of un-  bounded losses under partial monitoring. In ALT, 2006.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem.  SIAM Journal on Computing, 2002.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient optimal  learning for contextual bandits. In UAI, 2011.  D. Foster, Z. Li, T. Lykouris, K. Sridharan, and E. Tardos. Learning in games: Robustness of fast convergence.  In NIPS, 2016.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to  boosting. Journal of Computer and System Sciences, 1997.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In STOC, 2016.  J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In  G. Neu. First-order regret bounds for combinatorial semi-bandits. In COLT, 2015.  A. Rakhlin and K. Sridharan. Bistro: An efficient relaxation-based method for contextual bandits. In ICML,  V. Syrgkanis, A. Krishnamurthy, and R. E. Schapire. Efficient algorithms for adversarial contextual learning.  V. Syrgkanis, H. Luo, A. Krishnamurthy, and R. E. Schapire.  Improved regret bounds for oracle-based  adversarial contextual bandits. In NIPS, 2016b.  NIPS, 2008.  2016.  In ICML, 2016a.  4   AGARWAL KRISHNAMURTHY LANGFORD LUO SCHAPIRE  proportional to 1{p\u00b5 0} so that policies that are observed to make a mistake are eliminated. Now one can verify that  t (a|xt); finally update \u03a0t = {\u03c0 \u2208 \u03a0t\u22121 : \u03c0(xt) (cid:54)= at \u2228 (cid:96)t(at) =  t (a|xt) \u2265 \u03b3}p\u00b5  pt =  1\u03c0i +  1 2  1 2(|\u03a0t\u22121| \u2212 2)  (cid:88)  1\u03c0j ,  j(cid:54)=0,i \u03c0j \u2208\u03a0t\u22121  for any i (cid:54)= 0 s.t. \u03c0i \u2208 \u03a0t\u22121 satisfies the constraint (1\u03c0 denotes the point mass at \u03c0). However, since p\u00b5 t (ab|xi) = 1/2, no matter what \u03b3 is on each round the algorithm either makes a mistake with probability at least 1/(2 T mistakes to eliminate all bad policies. Therefore the \u221a T ). Several variants of this algorithm were also considered and all of them expected regret is \u2126( \u221a were shown to have \u2126(  T ) regret, indicating that some very different techniques are needed.  T ), or has made  \u221a  \u221a  Prize We are offering a prize of $250 for positive or negative resolutions to (Q1) or (Q2).  Acknowledgements We thank Gergely Neu for formative discussions about these problems.  References  A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A fast and simple  algorithm for contextual bandits. In ICML, 2014.  A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas,  S. Sen, and A. Slivkins. A multiworld testing decision service. arXiv:1606.03966, 2016.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and G. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of un-  bounded losses under partial monitoring. In ALT, 2006.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem.  SIAM Journal on Computing, 2002.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient optimal  learning for contextual bandits. In UAI, 2011.  D. Foster, Z. Li, T. Lykouris, K. Sridharan, and E. Tardos. Learning in games: Robustness of fast convergence.  In NIPS, 2016.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to  boosting. Journal of Computer and System Sciences, 1997.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In STOC, 2016.  J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In  G. Neu. First-order regret bounds for combinatorial semi-bandits. In COLT, 2015.  A. Rakhlin and K. Sridharan. Bistro: An efficient relaxation-based method for contextual bandits. In ICML,  V. Syrgkanis, A. Krishnamurthy, and R. E. Schapire. Efficient algorithms for adversarial contextual learning.  V. Syrgkanis, H. Luo, A. Krishnamurthy, and R. E. Schapire.  Improved regret bounds for oracle-based  adversarial contextual bandits. In NIPS, 2016b.  NIPS, 2008.  2016.  In ICML, 2016a."}