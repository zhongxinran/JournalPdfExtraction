{"1": "Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774\u2013792. PMLR, 06\u201309 Jul 2018. URL http://proceedings.mlr.press/v75/agarwal18a. html.  12  (Nesterov,2018)Algorithm110110210310400.20.40.60.8Iterationsf(xk)Syntheticn=10,d=10010110210310400.20.40.60.8Iterationsf(xk)Syntheticn=100,d=100010110210310400.20.40.60.8Iterationsf(xk)mushroomdataset1011021030.40.60.8Iterationsf(xk)a9adataset OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION  (a)  (b)  (c)  (d)  Figure 2: A performance comparison for the non-regularized logistic regression problem between the accelerated tensor method from Nesterov (2018a) and Algorithm 1. (a) Uses synthetic data with n = 10 and d = 100, (b) uses synthetic data with n = 100 and d = 1000, (c) uses the mushroom dataset (d = 8124 and n = 112) Dheeru and Karra Taniskidou (2017), and (d) uses the a9a dataset (d = 32561 and n = 123) Dheeru and Karra Taniskidou (2017).  d = 112) Dheeru and Karra Taniskidou (2017), and Figure 2(d) uses the a9a dataset (n = 32561 and d = 123) Dheeru and Karra Taniskidou (2017).  For the logistic regression problem, we don\u2019t have access to the optimal value function in gen- eral, thus, we plot only the cost function evaluated at the current iterate. As expected by the theo- retic results, Algorithm 1 requires one order of magnitude less iterations than the accelerated tensor method from Nesterov (2018a) to achieve the same function value.  In"}