{"1": "J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse  superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, 2017.  J. Barbier, M. Dia, N. Macris, and F. Krzakala. The mutual information in random linear estimation.  In 54th Annual Allerton Conf. on Communication, Control, and Computing, page 625, a.  Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove  replica formulas in bayesian inference. arXiv:1705.02780[v3], 2017.  Jean Barbier, Nicolas Macris, Mohamad Dia, and Florent Krzakala. Mutual information and opti-  mality of approximate message-passing in random linear estimation. arXiv:1701.05823, b.  M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications  to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785, 2011.  M. Bayati and A. Montanari. The lasso risk for gaussian matrices. IEEE Transactions on Informa-  tion Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Marc Lelarge, and Andrea Montanari. Universality in polytope phase transitions  and message passing algorithms. The Annals of Applied Probability, 25(2):753-822, 2015.  Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherrington-  kirkpatrick model. Communications in Mathematical Physics, 325(1):333366, 2014.  Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406, 2006.  3   PH. TRANSITIONS, OPT. ERRORS AND AMP IN HIGH-D. GENERALIZED LINEAR MODELS  A second object of focus is the algorithmic complexity: When is it possible to efficiently per- form these optimal estimations? To answer this question, we compare our information-theoretic results to the performance of the GAMP algorithm and its state evolution Rangan (2011). We deter- mine regions of parameters where this algorithm is or is not information-theoretically optimal. Up to technical assumptions, our results apply to all activation functions \u03d5 and priors P0, thus unifying a large volume of previous work where many particular functions have been analyzed on a case by case basis. This generality allows us to provide a unifying understanding of the types of phase transitions and phase diagrams that we can encounter in GLMs. Among other, we discuss the per- ceptron problem, one-bit compressed sensing, real valued-phase retrievial (or sign-less compressed sensing) and Relu-type measurements.  Acknowledgments  This work has been supported by funding from the SNSF (grant 200021-156672), from the ERC un- der the European Unions FP7 Grant Agreement 307087-SPARCS and the European Union\u2019s Hori- zon 2020 Research and Innovation Program 714608-SMiLe, as well as by the French Agence Na- tionale de la Recherche under grant ANR-17-CE23-0023-01 PAIL. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU and the Chaire de recherche sur les mod`eles et sciences des donn\u00b4ees, Fondation CFM pour la Recherche-ENS. Part of this work was done while L\u00b4eo Miolane was visiting EPFL  References  J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse  superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, 2017.  J. Barbier, M. Dia, N. Macris, and F. Krzakala. The mutual information in random linear estimation.  In 54th Annual Allerton Conf. on Communication, Control, and Computing, page 625, a.  Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove  replica formulas in bayesian inference. arXiv:1705.02780[v3], 2017.  Jean Barbier, Nicolas Macris, Mohamad Dia, and Florent Krzakala. Mutual information and opti-  mality of approximate message-passing in random linear estimation. arXiv:1701.05823, b.  M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications  to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785, 2011.  M. Bayati and A. Montanari. The lasso risk for gaussian matrices. IEEE Transactions on Informa-  tion Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Marc Lelarge, and Andrea Montanari. Universality in polytope phase transitions  and message passing algorithms. The Annals of Applied Probability, 25(2):753-822, 2015.  Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherrington-  kirkpatrick model. Communications in Mathematical Physics, 325(1):333366, 2014.  Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406, 2006. PH. TRANSITIONS, OPT. ERRORS AND AMP IN HIGH-D. GENERALIZED LINEAR MODELS  David Donoho and Andrea Montanari. High dimensional robust m-estimation: asymptotic variance via approximate message passing. Probability Theory and Related Fields, 166:935-969, 2016.  David L Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations  by linear programming. Proc. Nat. Acad. Sci., 102(27):9446-9451, 2005.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com-  pressed sensing. Proc. Nat. Acad. Sci., 106(45):18914-18919, Nov 2009.  N. El Karoui, D. Bean, P. J. Bickel, C. Lim, and B. Yu. On robust regression with high-dimensional  predictors. Proc. Nat. Acad. Sci., 110(36):14557, 2013.  Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of  networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.  Francesco Guerra and Fabio Lucio Toninelli. The thermodynamic limit in mean field spin glass  models. Communications in Mathematical Physics, 230(1):71-79, 2002.  Dongning Guo and Sergio Verd\u00b4u. Randomly spread cdma: Asymptotics via statistical physics.  IEEE Transactions on Information Theory, 51(6):1983-2010, June 2005. ISSN 0018-9448.  G\u00b4eza Gy\u00a8orgyi. First-order transition to perfect generalization in a neural network with binary  synapses. Physical Review A, 41(12):7097, 1990.  P. McCullagh. Generalized linear models. Euro. Journal of Operational Research, 16(3):285, 1984.  M. M\u00b4ezard, G. Parisi, and MA Virasoro. Spin glass theory and beyond. World Sci. Publish., 1987.  Marc M\u00b4ezard. The space of interactions in neural networks: Gardner\u2019s computation with the cavity  method. Journal of Physics A: Mathematical and General, 22(12):2181-2190, 1989.  John Ashworth Nelder and R Jacob Baker. Generalized linear models. Wiley Online Library, 1972.  Sundeep Rangan. Generalized approximate message passing for estimation with random linear  mixing. In IEEE ISIT, pages 2168-2172, July 2011.  G. Reeves and H. Pfister. The replica-symmetric prediction for compressed sensing with gaussian  matrices is exact. In Inf. Theory (ISIT), 2016 IEEE International Symposium on, page 665.  Sebastian H. Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from  examples. Phys. Rev. A, 45:6056-6091, Apr 1992.  Claude E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:623, 1948.  Toshiyuki Tanaka. A statistical-mechanics approach to large-system analysis of cdma multiuser  detectors. IEEE Transactions on Information Theory, 48(11):2888-2910, Nov 2002.  Timothy L. H. Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a  rule. Rev. Mod. Phys., 65:499-556, Apr 1993.  Lenka Zdeborov\u00b4a and Florent Krzakala. Statistical physics of inference: thresholds and algorithms.  Advances in Physics, 65(5):453-552, 2016."}