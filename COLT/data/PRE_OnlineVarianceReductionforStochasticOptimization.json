{"1": "Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo-  rithm for bandit linear optimization. In COLT, pages 263-274, 2008.  Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.  Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00b4arik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110-1119, 2016.  David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed- ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial and Applied Mathematics, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Leon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. In Advances  in neural information processing systems, pages 585-592, 1995.  Guillaume Bouchard, Th\u00b4eo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.  arXiv preprint arXiv:1506.09016, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.  Dominik Csiba and Peter Richt\u00b4arik.  Importance sampling for minibatches.  arXiv preprint  arXiv:1602.02283, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.  Matthew Faulkner, Michael Olson, Rishi Chandy, Jonathan Krause, K Mani Chandy, and Andreas Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors. In Information Processing in Sensor Networks (IPSN), 2011 10th International Confer- ence on, pages 13-24. IEEE, 2011.  13   ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  References  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo-  rithm for bandit linear optimization. In COLT, pages 263-274, 2008.  Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.  Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00b4arik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110-1119, 2016.  David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed- ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial and Applied Mathematics, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Leon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. In Advances  in neural information processing systems, pages 585-592, 1995.  Guillaume Bouchard, Th\u00b4eo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.  arXiv preprint arXiv:1506.09016, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.  Dominik Csiba and Peter Richt\u00b4arik.  Importance sampling for minibatches.  arXiv preprint  arXiv:1602.02283, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.  Matthew Faulkner, Michael Olson, Rishi Chandy, Jonathan Krause, K Mani Chandy, and Andreas Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors. In Information Processing in Sensor Networks (IPSN), 2011 10th International Confer- ence on, pages 13-24. IEEE, 2011. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  David A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100-118,  1975.  Elad Hazan. A survey: The convex optimization approach to regret minimization. Optimization for  machine learning, pages 287-302, 2011.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance  reduction. In Advances in neural information processing systems, pages 315-323, 2013.  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex pro- gramming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  KDD Cup 2004. KDD Cup 2004. Protein Homology Dataset. http://osmot.cs.cornell.  edu/kddcup/, 2004. Accessed: 10.11.2016.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-  mization. COLT 2010, page 244, 2010.  Hongseok Namkoong, Aman Sinha, Steve Yadlowsky, and John C. Duchi. Adaptive sampling probabilities for non-smooth optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2574- 2583, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.  I Necoara, Y Nesterov, and F Glineur. A random coordinate descent method on large optimization  problems with linear constraints. 2011.  Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems, pages 1017-1025, 2014.  Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM  Journal on Optimization, 22(2):341-362, 2012.  Dmytro Perekrestenko, Volkan Cevher, and Martin Jaggi. Faster coordinate descent via adaptive im- portance sampling. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54. PMLR, 2017.  F. Salehi, L. E. Celis, and P. Thiran. Stochastic Optimization with Bandit Sampling. ArXiv e-prints,  August 2017.  Farnood Salehi, Patrick Thiran, and L Elisa Celis. Stochastic dual coordinate descent with bandit  sampling. arXiv preprint arXiv:1712.03010, 2017. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  David Sculley. Web-scale k-means clustering. In Proceedings of the 19th international conference  on World wide web, pages 1177-1178. ACM, 2010.  Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and  Trends R(cid:13) in Machine Learning, 4(2):107-194, 2012.  Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 761-769, 2016.  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image  recognition. ICLR, 2015.  Sebastian U Stich, Anant Raj, and Martin Jaggi. Safe adaptive importance sampling. In Advances in Neural Information Processing Systems 30, pages 4384-4394. Curran Associates, Inc., 2017.  Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1-9, 2015. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION"}