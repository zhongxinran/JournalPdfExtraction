{"1": "University Press, 1999.  M. Anthony and Peter Bartlett. Learning in Neural Networks: Theoretical Foundations. Cambridge  8   MAURER  so  2 (B (\u2206) /3 + J (\u2206)) \u2264  68 max {1, c, c(cid:48)(cid:48)}3 \u03bb3n  +  24c(cid:48)(cid:48)(cid:48) max {1, c(cid:48), c(cid:48)(cid:48)}3 \u03bb4n  =  \u03b12 ((cid:96), \u03bb) n  ,  which proves (4).  Also, for any given x \u2208 B we have  (cid:96) ((cid:104)x, g (x)(cid:105)) \u2212 (cid:96) ((cid:104)x, g (x1, ..., xk\u22121, x, xk+1, ..., xn)(cid:105)) = Dk  xk,x(cid:96) ((cid:104)x, g (x)(cid:105)) .  But the expectation of the second term on the left, as x \u223c \u00b5 and x \u223c \u00b5n, is equal to Ex\u223c\u00b5n [(cid:96) ((cid:104)xi, g (x)(cid:105))], so  E [\u2206]) =  E [(cid:96) ((cid:104)x, g (x)(cid:105)) \u2212 (cid:96) ((cid:104)xk, g (x)(cid:105))] =  Ex\u223c\u00b5,x\u223c\u00b5n  1 n  (cid:88)  k  (cid:104)  Dk  (cid:105) xk,x(cid:96) ((cid:104)x, g (x)(cid:105))  \u2264  sup x\u2208Bn  sup y,y(cid:48)\u2208B  Ex\u223c\u00b5  (cid:104)  (cid:105) y,y(cid:48)(cid:96) ((cid:104)x, g (x)(cid:105))  Dk  \u2264 max  k  sup x\u2208Bn  sup y,y(cid:48)\u2208B  (cid:12) (cid:12)L(cid:48) (x) (cid:12)  (cid:104)\u02c6k (cid:0)y \u2212 y(cid:48)(cid:1)(cid:105)(cid:12) (cid:12) (cid:12) \u2264  k  = max  sup x\u2208Bn 2 max {1, c(cid:48), c(cid:48)(cid:48)}2 \u03bb3/2n  sup y,y(cid:48)\u2208B  Dk  y,y(cid:48)L (x)  =  \u03b11 ((cid:96), \u03bb) n  ,  1 n  1 n  (cid:88)  k (cid:88)  k  which proves (3). This completes the proof of Theorem 2.  Proof [of Theorem 3] Substitution of the above bound on B (\u2206) into the first concentration in- equality of Theorem 4, solving for the deviation and using the bound (3) on E [\u2206] gives the first inequality. For any x \u2208 Bn we have  \u03a32 (\u2206) (x) =  E(y,y(cid:48))\u223c\u00b52  Dk  y,y(cid:48)\u2206 (x)  (cid:17)2(cid:21)  (cid:20)(cid:16)  (cid:20)  sup x\u2208Bn  \uf8ee  (cid:32)  (cid:88)  k (cid:88)  k  (cid:88)  1 2  1 2  1 2  \u2264  \u2264  =  E(y,y(cid:48))\u223c\u00b52  \uf8f0  k 1 ((cid:96), \u03bb) \u03c32 9\u03b12  x\u223c\u00b5 (x)  .  n  E(y,y(cid:48))\u223c\u00b52  (cid:12) (cid:12)  (cid:12)\u2206(cid:48) (x) \u02c6k (cid:0)y \u2212 y(cid:48)(cid:1)(cid:12)  2(cid:21) (cid:12) (cid:12)  6 max {1, c(cid:48), c(cid:48)(cid:48)}2 \u03bb3/2n  (cid:33)2  (cid:13)y \u2212 y(cid:48)(cid:13) (cid:13) 2 (cid:13)  \uf8f9  \uf8fb  Substitution in the first inequality then gives the second inequality.  References  University Press, 1999.  M. Anthony and Peter Bartlett. Learning in Neural Networks: Theoretical Foundations. Cambridge SECOND-ORDER STABILITY  P. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural  results. Journal of Machine Learning Research, 3:463-482, 2002.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities. Oxford University Press,  O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Founda-  tions of Computational Mathematics, 7(3):331-368, 2007.  F. Cucker and S. Smale. Best choices for regularization parameters in learning theory: on the  bias-variance problem. Foundations of Computational Mathematics, 2(4):413-428, 2002.  B. Efron and C. Stein. The jackknife estimate of variance. The Annals of Statistics, pages 586-596,  M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient  descent. arXiv preprint arXiv:1509.01240, 2015.  C. Houdr\u00b4e. The iterated jackknife estimate of variance. Statistics and probability letters, 35(2):  A. Maurer. A Bernstein-type inequality for functions of bounded interaction. ArXiv e-prints, Jan-  C. McDiarmid. Concentration.  In Probabilistic Methods of Algorithmic Discrete Mathematics,  pages 195-248, Berlin, 1998. Springer.  T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of the IEEE, 78(9):  W. Rudin. Principles of mathematical analysis. McGraw-Hill New York, 1964.  J. M. Steele. An efron-stein inequality for nonsymmetric statistics. The Annals of Statistics, pages  2013.  2:499-526, 2002.  1981.  197-201, 1997.  uary 2017.  1481-1497, 1990.  753-758, 1986."}