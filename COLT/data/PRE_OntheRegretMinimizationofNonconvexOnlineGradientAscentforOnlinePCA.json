{"1": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1195-1199, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055464. URL http://doi.acm.org/10.1145/3055399.3055464.  Zeyuan Allen-Zhu.  Natasha 2:  In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- Information Processing Systems 31, pages 2680- nett, URL http://papers.nips.cc/paper/ 2691. Curran Associates, 7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf.  Faster non-convex optimization than sgd.  editors, Advances in Neural  Inc., 2018.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap- free, and near-optimal rate. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 487-492. IEEE, 2017a.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- vectors and faster mmwu. In International Conference on Machine Learning, pages 116-125, 2017b.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 3174-3182, 2013.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6- 11 August 2017, pages 654-663, 2017. URL http://proceedings.mlr.press/v70/ carmon17a.html.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332-2341, 2015.  13   NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1195-1199, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055464. URL http://doi.acm.org/10.1145/3055399.3055464.  Zeyuan Allen-Zhu.  Natasha 2:  In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- Information Processing Systems 31, pages 2680- nett, URL http://papers.nips.cc/paper/ 2691. Curran Associates, 7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf.  Faster non-convex optimization than sgd.  editors, Advances in Neural  Inc., 2018.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap- free, and near-optimal rate. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 487-492. IEEE, 2017a.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- vectors and faster mmwu. In International Conference on Machine Learning, pages 116-125, 2017b.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 3174-3182, 2013.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6- 11 August 2017, pages 654-663, 2017. URL http://proceedings.mlr.press/v70/ carmon17a.html.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332-2341, 2015. NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal bounds for privacy-preserving principal component analysis. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 11-20. ACM, 2014.  Dan Garber and Elad Hazan. Fast and simple pca via convex optimization.  arXiv preprint  arXiv:1509.05647, 2015.  568, 2015.  Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In ICML, pages 560-  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,  2(3-4):157-325, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012.  H. Hotelling. Analysis of a complex of statistical variables into principal components. J. Educ.  Psych., 24, 1933.  Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm. arXiv preprint arXiv:1602.06929, 2016.  Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix completion via non-convex stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4520-4528, 2016.  Ian Jolliffe. Principal component analysis.  In International encyclopedia of statistical science,  pages 1094-1096. Springer, 2011.  Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  Chris Junchi Li, Mengdi Wang, Han Liu, and Tong Zhang. Near-optimal stochastic approximation for online principal component estimation. Mathematical Programming, 167(1):75-97, 2018.  Teodor Vanislavov Marinov, Poorya Mianjy, and Raman Arora. Streaming principal component analysis in noisy setting. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3413-3422, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  Poorya Mianjy and Raman Arora. Stochastic PCA with (cid:96)2 and (cid:96)1 regularization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3531-3539, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/ v80/mianjy18a.html.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming pca. In  Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  Jiazhong Nie, Wojciech Kotlowski, and Manfred K. Warmuth. Online PCA with optimal regrets. In  24th International Conference on Algorithmic Learning Theory, ALT, 2013.  K. Pearson. On lines and planes of closest fit to systems of points in space. Philosophical Magazine,  2(6):559-572, 1901.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012.  Ohad Shamir. Convergence of stochastic gradient descent for PCA:. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19- 24, 2016, pages 257-265, 2016.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. In 19th Annual Conference  on Learning Theory, COLT, 2006a.  Manfred K. Warmuth and Dima Kuzmin. Randomized PCA algorithms with regret bounds that are logarithmic in the dimension. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, NIPS, 2006b.  Peng Xu, Bryan He, Christopher De Sa, Ioannis Mitliagkas, and Chris Re. Accelerated stochastic power iteration. In International Conference on Artificial Intelligence and Statistics, pages 58-67, 2018.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003."}