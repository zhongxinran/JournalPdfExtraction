{"1": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. CoRR,  abs/1603.05953, 2016.  Dan Anbar.  On Optimal Estimation Methods Using Stochastic Approximation Proce- dures. University of California, 1971. URL http://books.google.com/books?id= MmpHJwAACAAJ.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.  Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algo-  rithms for machine learning. In NIPS 24, 2011.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  L\u00b4eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.  Louis Augustin Cauchy. M\u00b4ethode g\u00b4en\u00b4erale pour la r\u00b4esolution des syst\u00b4emes d\u2019\u00b4equations simultanees.  C. R. Acad. Sci. Paris, 1847.  mization, 19(3):1171-1183, 2008.  Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-  Alexandre D\u00b4efossez and Francis R. Bach. Averaged least-mean-squares: Bias-variance trade-offs  and optimal sampling distributions. In AISTATS, volume 38, 2015.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods with inexact oracle:  the strongly convex case. CORE Discussion Papers 2013016, 2013.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex  optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.  Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation with large step  sizes. The Annals of Statistics, 2015.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger  convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.  Vaclav Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics,  1(3), 1973.  Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal  point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015a.  13   ACCELERATING STOCHASTIC GRADIENT DESCENT  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. CoRR,  abs/1603.05953, 2016.  Dan Anbar.  On Optimal Estimation Methods Using Stochastic Approximation Proce- dures. University of California, 1971. URL http://books.google.com/books?id= MmpHJwAACAAJ.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.  Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algo-  rithms for machine learning. In NIPS 24, 2011.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  L\u00b4eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.  Louis Augustin Cauchy. M\u00b4ethode g\u00b4en\u00b4erale pour la r\u00b4esolution des syst\u00b4emes d\u2019\u00b4equations simultanees.  C. R. Acad. Sci. Paris, 1847.  mization, 19(3):1171-1183, 2008.  Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-  Alexandre D\u00b4efossez and Francis R. Bach. Averaged least-mean-squares: Bias-variance trade-offs  and optimal sampling distributions. In AISTATS, volume 38, 2015.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods with inexact oracle:  the strongly convex case. CORE Discussion Papers 2013016, 2013.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex  optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.  Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation with large step  sizes. The Annals of Statistics, 2015.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger  convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.  Vaclav Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics,  1(3), 1973.  Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal  point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015a. ACCELERATING STOCHASTIC GRADIENT DESCENT  Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Competing with the empirical risk  minimizer in a single pass. In COLT, 2015b.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 2012.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 2013.  Anne Greenbaum. Behavior of slightly perturbed lanczos and conjugate-gradient recurrences. Lin-  ear Algebra and its Applications, 1989.  Magnus R. Hestenes and Eduard Stiefel. Methods of conjuate gradients for solving linear systems.  Journal of Research of the National Bureau of Standards, 1952.  Daniel J. Hsu, Sham M. Kakade, and Tong Zhang. Random design analysis of ridge regression.  Foundations of Computational Mathematics, 14(3):569-600, 2014.  Chonghai Hu, James T. Kwok, and Weike Pan. Accelerated gradient methods for stochastic opti-  mization and online learning. In NIPS 22, 2009.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paralleliz- ing stochastic approximation through mini-batching and tail-averaging. CoRR, abs/1610.03774, 2016.  Harold J. Kushner and Dean S. Clark. Stochastic Approximation Methods for Constrained and  Unconstrained Systems. Springer-Verlag, 1978.  Harold J. Kushner and George Yin. Stochastic approximation and recursive algorithms and appli-  cations. Springer-Verlag, 2003.  G. Lan. An optimal method for stochastic composite optimization. Tech. Report, GaTech., 2008.  Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. CoRR,  abs/1507.02000, 2015.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer, 1998.  Hongzhou Lin, Julien Mairal, and Za\u00a8\u0131d Harchaoui. A universal catalyst for first-order optimization.  In NIPS, 2015.  Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling,  and the randomized kaczmarz algorithm. Mathematical Programming, 2016.  Arkadii S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Yurii E. Nesterov. A method for unconstrained convex minimization problem with the rate of  convergence O(1/k2). Doklady AN SSSR, 269, 1983. ACCELERATING STOCHASTIC GRADIENT DESCENT  Yurii E. Nesterov.  Introductory lectures on convex optimization: A basic course, volume 87 of  Applied Optimization. Kluwer Academic Publishers, 2004.  Yurii E. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.  SIAM Journal on Optimization, 22(2):341-362, 2012.  Christopher C. Paige. The computation of eigenvalues and eigenvectors of very large sparse matri-  ces. PhD Thesis, University of London, 1971.  Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-  putational Mathematics and Mathematical Physics, 4, 1964.  Boris T. Polyak. Introduction to Optimization. Optimization Software, 1987.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  John G. Proakis. Channel identification for high speed digital communications. IEEE Transactions  on Automatic Control, 1974.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  Sumit Roy and John J. Shynk. Analysis of the momentum lms algorithm. IEEE Transactions on  Acoustics, Speech and Signal Processing, 1990.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for  regularized loss minimization. In ICML, 2014.  Rajesh Sharma, William A. Sethares, and James A. Bucklew. Analysis of momentum adaptive  filtering algorithms. IEEE Transactions on Signal Processing, 1998.  Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Publishers, 2000.  Bernard Widrow and Samuel D. Stearns. Adaptive Signal Processing. Englewood Cliffs, NJ:  Prentice-Hall, 1985.  Ashia C. Wilson, Benjamin Recht, and Michael I. Jordan. A lyapunov analysis of momentum  methods in optimization. CoRR, abs/1611.02635, 2016.  Blake Woodworth and Nathan Srebro. Tight complexity bounds for optimizing composite objec-  tives. CoRR, abs/1605.08003, 2016.  Kun Yuan, Bicheng Ying, and Ali H. Sayed. On the in\ufb02uence of momentum acceleration on online  learning. Journal of Machine Learning Research (JMLR), volume 17, 2016. ACCELERATING STOCHASTIC GRADIENT DESCENT"}