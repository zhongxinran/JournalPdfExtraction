{"1": "1991.  K.S. Alexander. Rates of growth and sample moduli for weighted empirical processes indexed by  sets. Probability Theory and Related Fields, 1987.  N. Alon. A non-linear lower bound for planar epsilon-nets. FOCS, pages 341\u2013346, 2010.  D. Applegate and R. Kannan. Sampling and integration of near log-concave functions. In STOC,  P. Assouad. Plongements lipschitziens dans. R . Bull. Soc. Math. France, 111(4):429\u2013448, 1983.  13   ACTIVE AND PASSIVE LEARNING OF LINEAR SEPARATORS UNDER LOG-CONCAVE DISTRIBUTIONS  \u2208  [0, 1) and a  Theorem 15 Let s = O(log(1/\u01eb)). Assume that the distribution DXY satisfies the Tsybakov noise 0, and that the marginal D on Rd is isotropic log- condition for constants \u03b1 \u03b4 using concave. (1) If \u03b1 = 0, we can find a separator with excess error O(log(1/\u01eb))(d + log(s/\u03b4)) labeled examples in the active learning model, and O (cid:17) labeled examples in the passive learning model. (2) If \u03b1 > 0, we can find a separator with excess \u03b4 using O((1/\u01eb)2\u03b1 log2(1/\u01eb))(d + log(s/\u03b4)) labeled examples in the error active learning model.  \u01eb with probability 1  \u01eb with probability 1  d+log(1/\u03b4) \u01eb  \u2264  \u2212  \u2265  \u2264  \u2212  (cid:16)  In the case \u03b1 = 0 (that is more general than the Massart noise condition) our analysis leads to optimal bounds for active and passive learning of linear separators under log-concave distribu- tions, improving the dependence on d over previous best known results (Hanneke and Yang, 2012; Gin\u00b4e and Koltchinskii, 2006). Our analysis for Tsybakov noise (\u03b1 0) leads to bounds on active learning with improved dependence on d over previous known results (Hanneke and Yang, 2012) in this case as well. Proofs and further details appear in"}