{"1": "J. M. Adams and G. White. A versatile pulse shape discriminator for charged particle sepa- ration and its application to fast neutron time-of-\ufb02ight spectroscopy. Nuclear Instruments and Methods in Physics Research, 1978.  S. Ambers, M. Flaska, and S. Pozzi. A hybrid pulse shape discrimination technique with en- hanced performance at neutron energies below 500 kev. Nuclear Instruments and Methods in Physics Research A, 638:116-121, 2011.  D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343-370,  1988.  (cid:15) 8 (cid:15) 8  (cid:15) 8  21   Classification with Asymmetric Label Noise  Assume that both  |R0(f ) \u2212 (cid:98)R0(f )| <  for all f \u2208 Fk  |R1(f ) \u2212 (cid:98)R1(f )| <  for all f \u2208 Fk,  which by the result just stated, occurs with probability at least 1\u2212\u03b4 for m and n su\ufb03ciently large. It follows that  (cid:15) 8  (cid:15) 8  .  max{R0( (cid:98)fk), R1( (cid:98)fk)} < max{ (cid:98)R0( (cid:98)fk), (cid:98)R1( (cid:98)fk)} +  and  max{R0(f \u2217  k ), R1(f \u2217  k )} > max{ (cid:98)R0(f \u2217  k ), (cid:98)R1(f \u2217  k )} \u2212  Using these inequalities in Equation (19) yields  R( (cid:98)fk) \u2212 R(Fk) < max{ (cid:98)R0( (cid:98)fk), (cid:98)R1( (cid:98)fk)} +  \u2212 (max{ (cid:98)R0(f \u2217  k ), (cid:98)R1(f \u2217  k )} \u2212  ) +  (cid:15) 8  (cid:15) 8  .  From our definition of (cid:98)fk in Equation (16), for m and n su\ufb03ciently large we have  max{ (cid:98)R0( (cid:98)fk), (cid:98)R1( (cid:98)fk)} \u2264 max{ (cid:98)R0(f \u2217  k ), (cid:98)R1(f \u2217  k )} +  (cid:15) 8  .  Therefore, we can conclude that  R( (cid:98)fk) \u2212 R(Fk) <  (cid:15) 2  ,  with probability at least 1 \u2212 \u03b4. Thus, we conclude that  0 \u2297 \u02dcP n \u02dcP m  1 (R( (cid:98)fk) \u2212 R\u2217 < (cid:15)) > 1 \u2212 \u03b4,  for m and n su\ufb03ciently large.  References  J. M. Adams and G. White. A versatile pulse shape discriminator for charged particle sepa- ration and its application to fast neutron time-of-\ufb02ight spectroscopy. Nuclear Instruments and Methods in Physics Research, 1978.  S. Ambers, M. Flaska, and S. Pozzi. A hybrid pulse shape discrimination technique with en- hanced performance at neutron energies below 500 kev. Nuclear Instruments and Methods in Physics Research A, 638:116-121, 2011.  D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343-370,  1988.  (cid:15) 8 (cid:15) 8  (cid:15) 8 Scott Blanchard Handy  J. Aslam and S. Decatur. On the sample complexity of noise-tolerant learning. Inf. Process.  Lett., 57:189-195, 1996.  G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. Journal of Machine  Learning Research, 11:2973-3009, 2010.  A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training.  In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92-100, 1998.  C. Bouveyron and S. Girard. Robust supervised classification with mixture models: Learn- ing from data with uncertain labels. Journal of Pattern Recognition, 42:2649-2658, 2009.  C. Brodley and M. Friedl. Identifying mislabeled training data. Journal of Artifcial Intel-  ligence Research, pages 131-167, 1999.  N. H. Bshouty, S. A. Goldman, H. D. Mathias, S. Suri, and H. Tamaki. Noise-tolerant distribution-free learning of general geometric concepts. J. ACM, 45(5):863-890, 1998.  N. Cesa-Bianchi, P. Fischer, E. Shamir, and H.-U. Simon. Randomized hypotheses and In Proc. Third European  minimum disagreement hypotheses for learning with noise. Conf. on Computational Learning Theory, pages 119-133, 1997.  V. Denchev, N. Ding, S. V. N. Vishwanathan, and H. Neven. Robust classification with adiabatic quantum optimization. In J. Langford and J. Pineau, editors, Proc. 29th Int. Conf. on Machine Learning, pages 863-870, 2012.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition.  Springer, 1996.  N. Ding and S. V. N. Vishwanathan. t-logistic regression. In J. La\ufb00erty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 514-522. 2010.  S. Jabbari. PAC-learning with label noise. Master\u2019s thesis, University of Alberta, December  2010.  A. Kalai and R. Servedio. Boosting in the presence of noise. Symposium on Theory of  Computing, pages 196-205, 2003.  M. Kearns. E\ufb03cient noise-tolerant learning from statistical queries. Proceedings of the Twenty-Fifth Annual ACM Symposium on THeory of Computing, pages 392-401, 1993.  N. Lawrence and B. Sch\u00a8olkopf. Estimating a kernel Fisher discriminant in the presence of label noise. Proceedings of the International Conference in Machine Learning, 2001.  P. Long and R. Servido. Random classification noise defeats all convex potential boosters.  Machine Learning, 78:287-304, 2010.  N. Manwani and P. S. Sastry. Noise tolerance under risk minimization. Technical Report  arXiv:1109.5231, 2011. Classification with Asymmetric Label Noise  H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In Y. Bengio D. Koller, D. Schuurmans and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1049-1056. 2009.  L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In In Advances in Neural Information Processing Systems 12, pages 512-518. MIT Press, 2000.  U. Rebbapragada and C. Brodley. Class noise mitigation through instance weighting. Eu-  ropean Conference on Machine Learning, pages 708-715, 2007.  S. Sabato and N. Tishby. Multi-instance learning with any hypothesis class. J. Machine  Learning Research, 13:2999-3039, 2012.  G. Stempfel and L. Ralaivola. Learning SVMs from sloppily labeled data. In Proc. 19th  Int. Conf. on Artificial Neural Networks: Part I, pages 884-893, 2009.  L. Xu, K. Crammer, and D. Schuurmans. Robust support vector machine training via convex outlier ablation. Proceedings of the 21st National Conference on Artificial Intelligence (AAAI), 2006."}