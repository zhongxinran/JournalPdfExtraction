{"1": "Scott Aaronson. The learnability of quantum states. In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 463, pages 3089-3114. The Royal Society, 2007.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227- 236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis.  Sparse PCA via bipartite matchings. arXiv preprint arXiv:1508.00625, 2015.  Stephen Becker, Volkan Cevher, and Anastasios Kyrillidis. Randomized low-memory singular value projection. In 10th International Conference on Sampling Theory and Applications (Sampta), 2013.  Rajendra Bhatia. Perturbation bounds for matrix eigenvalues, volume 53. SIAM, 1987.  Nicolas Boumal. Optimization and estimation on manifolds. PhD thesis, UC Louvain, Belgium,  2014.  20   BHOJANAPALLI KYRILLIDIS SANGHAVI  a non-trivial step size selection results in linear convergence when f is smooth and (restricted) strongly convex, even though the problem is now non-convex. In the case where f is only smooth, only sublinear rate is guaranteed. In addition, we present initialization schemes that use only first order information and guarantee to find a starting point with small relative distance from optimum. There are many possible directions for future work, extending the idea of using non-convex formulation for semi-definite optimization. Showing convergence under weaker initialization con- dition or without any initialization requirement is definitely of great interest. Another interesting direction is to improve the convergence rates presented in this work, by using acceleration tech- niques and thus, extend ideas used in the case of convex gradient descent Nesterov (2004). Finally, it would be valuable to see how the techniques presented in this paper can be generalized to other standard algorithms like stochastic gradient descent and coordinate descent.  Furthermore, we identify applications, such as sparse PCA Vu et al. (2013); Asteris et al. (2015), that require non-smooth constraints on the factors U . That being said, an extension of this work to proximal techniques for the non-convex case is a very interesting future research direction.  References  Scott Aaronson. The learnability of quantum states. In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 463, pages 3089-3114. The Royal Society, 2007.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227- 236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis.  Sparse PCA via bipartite matchings. arXiv preprint arXiv:1508.00625, 2015.  Stephen Becker, Volkan Cevher, and Anastasios Kyrillidis. Randomized low-memory singular value projection. In 10th International Conference on Sampling Theory and Applications (Sampta), 2013.  Rajendra Bhatia. Perturbation bounds for matrix eigenvalues, volume 53. SIAM, 1987.  Nicolas Boumal. Optimization and estimation on manifolds. PhD thesis, UC Louvain, Belgium,  2014. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Nicolas Boumal. A riemannian low-rank method for optimization over semidefinite matrices with  block-diagonal constraints. arXiv preprint arXiv:1506.00575, 2015.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  S\u00b4ebastien Bubeck.  Theory of convex optimization for machine learning.  arXiv preprint  arXiv:1405.4980, 2014.  Samuel Burer. Semidefinite programming in the space of partial positive semidefinite matrices.  SIAM Journal on Optimization, 14(1):139-172, 2003.  Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefi- nite programs via low-rank factorization. Mathematical Programming, 95(2):329-357, 2003.  Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite  programming. Mathematical Programming, 103(3):427-444, 2005.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. Information Theory, IEEE Transactions on, 57 (4):2342-2359, 2011.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval  via matrix completion. SIAM Review, 57(2):225-251, 2015a.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. Information Theory, IEEE Transactions on, 61(4):1985-2007, 2015b.  Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:  General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.  Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple- tion. In Proceedings of The 31st International Conference on Machine Learning, pages 674-682, 2014.  Yuxin Chen and Sujay Sanghavi. A general framework for high-dimensional estimation in the In Communication, Control, and Computing (Allerton), 2010 48th  presence of incoherence. Annual Allerton Conference on, pages 1570-1576. IEEE, 2010.  Kenneth L Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM  Transactions on Algorithms (TALG), 6(4):63, 2010.  Alexandre d\u2019Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert RG Lanckriet. A direct for- mulation for sparse PCA using semidefinite programming. SIAM review, 49(3):434-448, 2007.  Quoc Tran Dinh, Anastasios Kyrillidis, and Volkan Cevher. Composite self-concordant minimiza-  tion. Journal of Machine Learning Research, 16:371-416, 2015. BHOJANAPALLI KYRILLIDIS SANGHAVI  Alan Edelman, Tom\u00b4as A Arias, and Steven T Smith. The geometry of algorithms with orthogonality  constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303-353, 1998.  Mituhiro Fukuda, Masakazu Kojima, Kazuo Murota, and Kazuhide Nakata. Exploiting sparsity in semidefinite programming via matrix completion I: General framework. SIAM Journal on Optimization, 11(3):647-674, 2001.  Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42 (6):1115-1145, 1995.  Elad Hazan. Sparse approximate solutions to semidefinite programs. In LATIN 2008: Theoretical  Informatics, pages 306-316. Springer, 2008.  Christoph Helmberg and Franz Rendl. A spectral bundle method for semidefinite programming.  SIAM Journal on Optimization, 10(3):673-696, 2000.  Christoph Helmberg, Michael L Overton, and Franz Rendl. The spectral bundle method with  second-order information. Optimization Methods and Software, 29(4):855-876, 2014.  Roger A Horn and Charles R Johnson. Topics in matrix analysis. Cambridge University Presss,  Cambridge, 37:39, 1991.  Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and M\u00b4aty\u00b4as A Sustik. Sparse inverse covariance matrix estimation using quadratic approximation. In Advances in Neural Information Processing Systems, pages 2330-2338, 2011.  Martin Jaggi. Convex optimization without projection steps. arXiv preprint arXiv:1108.1170, 2011.  Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value  projection. In Advances in Neural Information Processing Systems, pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 665-674. ACM, 2013.  Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Computing matrix squareroot via  non convex local search. arXiv preprint arXiv:1507.05854, 2015.  Adel Javanmard and Andrea Montanari. Localization from incomplete noisy distance measure-  ments. Foundations of Computational Mathematics, 13(3):297-345, 2013.  Kaifeng Jiang, Defeng Sun, and Kim-Chuan Toh. An inexact accelerated proximal gradient method for large scale linearly constrained convex SDP. SIAM Journal on Optimization, 22(3):1042- 1064, 2012.  Michel Journ\u00b4ee, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327-2351, 2010.  Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the sixteenth annual ACM symposium on Theory of computing, pages 302-311. ACM, 1984. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. Information Theory, IEEE Transactions on, 56(6):2980-2998, 2010.  Philip Klein and Hsueh-I Lu. Efficient approximation algorithms for semidefinite programs arising from MAX CUT and COLORING. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pages 338-347. ACM, 1996.  Anastasios Kyrillidis and Volkan Cevher. Matrix recipes for hard thresholding methods. Journal of  mathematical imaging and vision, 48(2):235-265, 2014.  Anastasios Kyrillidis, Rabeeh Karimi, Quoc Tran Dinh, and Volkan Cevher. Scalable sparse co- variance estimation via self-concordance. In Twenty-Eighth AAAI Conference on Artificial Intel- ligence, 2014.  Soeren Laue. A hybrid algorithm for convex semidefinite optimization. In Proceedings of the 29th  International Conference on Machine Learning (ICML-12), pages 177-184, 2012.  Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Ad-  vances in neural information processing systems, pages 556-562, 2001.  Jason Lee, Yuekai Sun, and Michael Saunders. Proximal newton-type methods for convex opti-  mization. In Advances in Neural Information Processing Systems, pages 836-844, 2012.  Yi-Kai Liu. Universal low-rank matrix recovery from Pauli measurements. In Advances in Neural  Information Processing Systems, pages 1638-1646, 2011.  Leon Mirsky. A trace inequality of John von Neumann. Monatshefte f\u00a8ur Mathematik, 79(4):303-  306, 1975.  Bamdev Mishra, Gilles Meyer, and Rodolphe Sepulchre. Low-rank optimization for distance matrix completion. In Decision and control and European control conference (CDC-ECC), 2011 50th IEEE conference on, pages 4455-4460. IEEE, 2011.  Renato DC Monteiro. First-and second-order methods for semidefinite programming. Mathematical  Programming, 97(1-2):209-244, 2003.  Kazuhide Nakata, Katsuki Fujisawa, Mituhiro Fukuda, Masakazu Kojima, and Kazuo Murota. Ex- ploiting sparsity in semidefinite programming via matrix completion II: Implementation and nu- merical results. Mathematical Programming, 95(2):303-327, 2003.  Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix com- pletion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1):1665- 1697, 2012.  Yurii Nesterov.  Introductory lectures on convex optimization, volume 87. Springer Science &  Business Media, 2004.  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Mathemati-  cal Programming, 110(2):245-259, 2007. BHOJANAPALLI KYRILLIDIS SANGHAVI  Yurii Nesterov and Arkadi Nemirovski. A general approach to polynomial-time algorithms de- sign for convex programming. Report, Central Economical and Mathematical Institute, USSR Academy of Sciences, Moscow, 1988.  Yurii Nesterov and Arkadi Nemirovski. Self-concordant functions and polynomial-time methods in convex programming. USSR Academy of Sciences, Central Economic & Mathematic Institute, 1989.  Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-  tion. In Advances in Neural Information Processing Systems, pages 2796-2804, 2013.  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Christopher D Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2332-2341, 2015.  Shai Shalev-shwartz, Alon Gonen, and Ohad Shamir. Large-scale convex minimization with a low-rank constraint. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 329-336, 2011.  Suvrit Sra. On the matrix square root via geometric optimization. arXiv preprint arXiv:1507.08366,  2015.  arXiv:1602.06664, 2016.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. arXiv preprint  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv  preprint arXiv:1411.8003, 2014.  Kim-Chuan Toh. Solving large scale semidefinite programs via an iterative solver on the augmented  systems. SIAM Journal on Optimization, 14(3):670-698, 2004.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Andre Uschmajew and Bart Vandereycken. Greedy rank updates combined with riemannian descent methods for low-rank optimization. In 12th International Conference on Sampling Theory and Applications (Sampta), 2015.  Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In Founda- tions of Computer Science, 1989., 30th Annual Symposium on, pages 338-343. IEEE, 1989.  Vincent Q Vu, Jing Lei, et al. Minimax sparse principal subspace estimation in high dimensions.  The Annals of Statistics, 41(6):2905-2947, 2013.  Andrew E Waters, Aswin C Sankaranarayanan, and Richard Baraniuk. Sparcs: Recovering low- rank and sparse matrices from compressive measurements. In Advances in neural information processing systems, pages 1089-1097, 2011. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Zaiwen Wen, Donald Goldfarb, and Wotao Yin. Alternating direction augmented Lagrangian meth- ods for semidefinite programming. Mathematical Programming Computation, 2(3-4):203-230, 2010.  Chris D White, Sujay Sanghavi, and Rachel Ward. The local convexity of solving systems of  quadratic equations. arXiv preprint arXiv:1506.07868, 2015.  Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of The 31st International Conference on Machine Learning, pages 593-601, 2014.  Dejiao Zhang and Laura Balzano. Global convergence of a grassmannian gradient descent algorithm  for subspace estimation. arXiv preprint arXiv:1506.07405, 2015.  Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix  estimation. In Advances in Neural Information Processing Systems, pages 559-567, 2015.  Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank mini- arXiv preprint  mization and semidefinite programming from random linear measurements. arXiv:1506.06081, 2015."}