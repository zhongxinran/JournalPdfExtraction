{"1": "We provide an algorithm for properly learning mixtures of two single-dimensional Gaussians without any separability assumptions. Given \\tildeO(1/\\varepsilon^2) samples from an unknown mixture, our algorithm outputs a mixture that is \\varepsilon-close in total variation distance, in time \\tildeO(1/\\varepsilon^5). Our sample complexity is optimal up to logarithmic factors, and significantly improves upon both Kalai et al., whose algorithm has a prohibitive dependence on 1/\\varepsilon, and Feldman et al., whose algorithm requires bounds on the mixture parameters and depends pseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm for selecting a good candidate distribution from among competing hypotheses. Namely, given a collection of N hypotheses containing at least one candidate that is \\varepsilon-close to an unknown distribution, our algorithm outputs a candidate which is O(\\varepsilon)-close to the  distribution. The algorithm requires O(\\logN/\\varepsilon^2) samples from the unknown distribution and O(N \\log N/\\varepsilon^2) time, which improves previous such results (such as the Scheff\u00e9 estimator) from a quadratic dependence of the running time on N to quasilinear. Given the wide use of such results for the purpose of hypothesis selection, our improved algorithm implies immediate improvements to any such use."}