{"1": "J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the Dark: An Efficient Algorithm for Ban- In Proceedings of the International Conference on Learning Theory  dit Linear Optimization. (COLT), pages 263-274, 2008.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Duality between subgradient and conditional gradient methods. SIAM J. Optim., 25(1):  115-129, 2015.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  H. H. Bauschke and J. M. Borwein. Legendre functions and the method of random Bregman pro-  jections. J. Convex Anal., 4(1):27-67, 1997.  H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011.  H. H. Bauschke, J. Bolte, and M. Teboulle. A descent Lemma beyond Lipschitz gradient continuity:  first-order methods revisited and applications. Mathematics of Operations Research, 2016.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, 2003.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  J. Bolte and M. Teboulle. Smooth optimization with approximate gradient. SIAM J. Optim., 43(3):  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge,  L. M. Bregman. A relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming. \u02c7Z. Vy\u02c7cisl. Mat. i Mat. Fiz., 7:620-631, 1967.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  1266-1292, 2003.  2004.  Cambridge, 2006.  G. Chen and M. Teboule. Convergence analysis of a proximal-like minimization algorithm using  Bregman functions. SIAM J. Optim., 3(3):538-543, 1993.  I. Colin, A. Bellet, J. Salmon, and S. Cl\u00b4emenc\u00b8on. Gossip Dual Averaging for Decentralized Opti- mization of Pairwise Functions. In Proceedings of the Conference on Machine Learning (ICML), 2016.  14   FLAMMARION BACH  References  J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the Dark: An Efficient Algorithm for Ban- In Proceedings of the International Conference on Learning Theory  dit Linear Optimization. (COLT), pages 263-274, 2008.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Duality between subgradient and conditional gradient methods. SIAM J. Optim., 25(1):  115-129, 2015.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  H. H. Bauschke and J. M. Borwein. Legendre functions and the method of random Bregman pro-  jections. J. Convex Anal., 4(1):27-67, 1997.  H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011.  H. H. Bauschke, J. Bolte, and M. Teboulle. A descent Lemma beyond Lipschitz gradient continuity:  first-order methods revisited and applications. Mathematics of Operations Research, 2016.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, 2003.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  J. Bolte and M. Teboulle. Smooth optimization with approximate gradient. SIAM J. Optim., 43(3):  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge,  L. M. Bregman. A relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming. \u02c7Z. Vy\u02c7cisl. Mat. i Mat. Fiz., 7:620-631, 1967.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  1266-1292, 2003.  2004.  Cambridge, 2006.  G. Chen and M. Teboule. Convergence analysis of a proximal-like minimization algorithm using  Bregman functions. SIAM J. Optim., 3(3):538-543, 1993.  I. Colin, A. Bellet, J. Salmon, and S. Cl\u00b4emenc\u00b8on. Gossip Dual Averaging for Decentralized Opti- mization of Pairwise Functions. In Proceedings of the Conference on Machine Learning (ICML), 2016. STOCHASTIC COMPOSITE LEAST-SQUARES REGRESSION  P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, volume 49 of Springer Optim. Appl., pages 185-212. Springer, New York, 2011.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. In Advances in Neural Information Processing Systems (NIPS), pages 1646-1654, 2014.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. J. Mach. Learn. Res., 13:165-202, 2012.  O. Devolder, F. Glineur, and Y. Nesterov. First-order methods with inexact oracle: the strongly  convex case. CORE Discussion Papers, 2013016, 2013.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, 1996.  A. Dieuleveut, N. Flammarion, and F. Bach. Harder, Better, Faster, Stronger Convergence Rates for  Least-Squares Regression. arXiv preprint arXiv:1602.05419v2, 2016.  J. Duchi and F. Ruan. Local Asymptotics for some Stochastic Optimization Problems: Optimality,  Constraint Identification, and Dual Averaging. arXiv preprint arXiv:1612.05612, 2016.  J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite Objective Mirror Descent. In Proceedings of the International Conference on Learning Theory (COLT), pages 14-26, 2010.  J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: convergence  analysis and network scaling. IEEE Trans. Automat. Control, 57(3):592-606, 2012.  N. Flammarion and F. Bach. From averaging to acceleration, there is only a step-size. In Proceed-  ings of the International Conference on Learning Theory (COLT), 2015.  C. Gentile and N. Littlestone. The robustness of the p-norm algorithms. International Conference on Learning Theory (COLT), pages 1-11, 1999.  In Proceedings of the  O. Hanner. On the uniform convexity of Lp and lp. Ark. Mat., 3:239-244, 1956.  J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Fundamentals of Convex Analysis. Grundlehren Text  Editions. Springer-Verlag, Berlin, 2001.  P. Jain, S. M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Parallelizing Stochastic Approxi- mation Through Mini-Batching and Tail-Averaging. arXiv preprint arXiv:1610.03774, 2016.  A. Juditsky and A. S. Nemirovski. Functional aggregation for nonparametric regression. Ann.  Statist., 28(3):681-712, 2000.  71(3):291-307, 2005.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. J. Comput. System Sci.,  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Inform. and Comput., 132(1):1-63, 1997. FLAMMARION BACH  K. C. Kiwiel. Proximal minimization methods with generalized Bregman functions. SIAM J. Con-  trol Optim., 35(4):1142-1168, 1997.  W. Krichene, A. Bayen, and P. L. Bartlett. Accelerated mirror descent in continuous and discrete time. In Advances in Neural Information Processing Systems (NIPS), pages 2845-2853, 2015.  H. Kushner and G G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer, 2003.  G. Lecu\u00b4e. Optimal oracle inequality for aggregation of classifiers under low noise condition. In Learning theory, volume 4005 of Lecture Notes in Comput. Sci., pages 364-378. Springer, Berlin, 2006.  G. Lecu\u00b4e. Optimal rates of aggregation in classification under low noise assumption. Bernoulli, 13  (4):1000-1022, 2007.  S. Lee and S. J. Wright. Manifold identification in dual averaging for regularized stochastic online  learning. J. Mach. Learn. Res., 13:1705-1744, 2012.  H. Lu, R. Freund, and Y. Nesterov. Relatively-Smooth Convex Optimization by First-Order Meth-  ods, and Applications. arXiv preprint arXiv:1610.05708, 2016.  O. Macchi. Adaptive Processing: the Least-Mean-Squares Approach with Applications in Trans-  mission. Wiley West Sussex, 1995.  B. Martinet. Breve communication. R\u00b4egularisation d\u2019in\u00b4equations variationnelles par approxima- tions successives. ESAIM: Mathematical Modelling and Numerical Analysis - Mod\u00b4elisation Math\u00b4ematique et Analyse Num\u00b4erique, 4:154-158, 1970.  H. B. McMahan. Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and  L1 Regularization. In AISTATS, pages 525-533, 2011.  J.-J. Moreau. Fonctions convexes duales et points proximaux dans un espace Hilbertien. C. R. Acad.  Sci. Paris, 255:2897-2899, 1962.  A. S. Nemirovski and D. B. Yudin. Effective methods for the solution of convex programming  problems of large dimensions. `Ekonom. i Mat. Metody, 15(1):135-152, 1979.  A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience Publication. John Wiley & Sons, Inc., New York, 1983. Translated from the Russian and with a preface by E. R. Dawson, Wiley-Interscience Series in Discrete Mathematics.  Y. Nesterov. Introductory Lectures on Convex Optimization, volume 87 of Applied Optimization.  Kluwer Academic Publishers, Boston, MA, 2004. A basic course.  Y. Nesterov. Primal-dual subgradient methods for convex problems. Math. Program., 120(1, Ser.  Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser.  B):221-259, 2009.  B):125-161, 2013. STOCHASTIC COMPOSITE LEAST-SQUARES REGRESSION  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J.  Control Optim., 30(4):838-855, 1992.  M. Raginsky and A. Rakhlin.  Information-based complexity, feedback and dynamics in convex  programming. IEEE Trans. Inform. Theory, 57(10):7036-7056, 2011.  I. Rish and G. Grabarnik. Sparse Modeling: Theory, Algorithms, and Applications. CRC press,  2014.  Press, Princeton, N.J., 1970.  R. T. Rockafellar. Convex Analysis. Princeton Mathematical Series, No. 28. Princeton University  S. Shalev-Shwartz and S. m. Kakade. Mind the duality gap: Logarithmic regret algorithms for online optimization. In Advances in Neural Information Processing Systems (NIPS), pages 1457-1464, 2009.  S. Shalev-Shwartz and Y. Singer. Online learning meets optimization in the dual.  In Learning  theory, volume 4005 of Lecture Notes in Comput. Sci., pages 423-437. Springer, Berlin, 2006.  T. Suzuki. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multi- plier Method. In Proceedings of the Conference on Machine Learning (ICML), pages 392-400, 2013.  A. B. Tsybakov. Optimal rates of aggregation. In Proceedings of the Annual Conference on Com-  putational Learning Theory, 2003.  J.-P. Vial. Strong and weak convexity of sets and functions. Math. Oper. Res., 8(2):231-259, 1983.  A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in  optimization. Proceedings of the National Academy of Sciences, 113(47), 2016.  A. I. Wilson, B. Recht, and M. I. Jordan. A Lyapunov Analysis of Momentum Methods in Opti-  mization. arXiv preprint arXiv:1611.02635v3, 2016.  S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo. Sparse reconstruction by separable approxi-  mation. IEEE Trans. Signal Process., 57(7):2479-2493, 2009.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J.  Mach. Learn. Res., 11:2543-2596, 2010.  M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Pro-  ceedings of the Conference on Machine Learning (ICML), 2003. FLAMMARION BACH"}