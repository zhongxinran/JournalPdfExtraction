{"1": "Dmitry Adamskiy, Manfred K Warmuth, and Wouter M Koolen. Putting bayes to sleep. In Advances  in neural information processing systems 25, 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit  problems. In International Conference on Algorithmic Learning Theory, 2011.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016.  13   A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  The authors would like to thank Peter Auer for the discussion about the possibility of getting optimal bounds for our problem, and to thank Peter Auer, Pratik Gajane, Ronald Ortner for kindly sharing their manuscript of (Auer et al., 2018) before it was public. HL and CYW are supported by NSF Grant #1755781.  Acknowledgments  References  Dmitry Adamskiy, Manfred K Warmuth, and Wouter M Koolen. Putting bayes to sleep. In Advances  in neural information processing systems 25, 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit  problems. In International Conference on Algorithmic Learning Theory, 2011.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016. A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  Elad Hazan and Comandur Seshadhri. Efficient learning algorithms for changing environments. In Proceedings of the 26th International Conference on Machine Learning, pages 393-400, 2009.  Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151-  178, 1998.  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza- tion: Competing with dynamic comparators. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, 2015.  Kwang-Sung Jun, Francesco Orabona, Stephen Wright, Rebecca Willett, et al. Online learning for changing environments using coin betting. Electronic Journal of Statistics, 11(2):5282-5310, 2017.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in Neural Information Processing Systems 21, 2008.  Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise- stationary multi-armed bandit problem. In Thirty-Second AAAI Conference on Artificial Intelli- gence, 2018.  Haipeng Luo and Robert E. Schapire. Achieving All with No Parameters: AdaNormalHedge. In  28th Annual Conference on Learning Theory (COLT), 2015.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in  non-stationary worlds. In 31st Annual Conference on Learning Theory (COLT), 2018.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, 2016.  Aleksandrs Slivkins and Eli Upfal. Adapting to a changing environment: the brownian restless  bandits. In 21st Annual Conference on Learning Theory (COLT), pages 343-354, 2008.  Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efficient algorithms for ad- versarial contextual learning. In Proceedings of the 33rd International Conference on Machine Learning, 2016a.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems 29, 2016b.  Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic  environments. In Advances in Neural Information Processing Systems 29, 2016.  Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: opti- mal dynamic regret of online learning with true and noisy gradient. In Proceedings of the 33rd International Conference on Machine Learning, pages 449-457, 2016. A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In Advances in Neural Information Processing Systems 30, 2017.  Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou. Dynamic regret of strongly adaptive methods. In Proceedings of the 35th International Conference on Machine Learning, 2018.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  Proceedings of the 20th International Conference on Machine Learning, 2003."}