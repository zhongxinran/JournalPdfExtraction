{"1": "K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An Intro-  duction, volume 2044. Springer, 2012.  Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. In 29th Annual Conference on Learning Theory, pages 698-728, 2016.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 1989.  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in  Neural Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In 29th  Annual Conference on Learning Theory, pages 907-940, 2016.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Sys- tems, pages 2877-2885, 2013.  6   DANIELY  weights bounded by 4. For each i \u2208 [d] we can compose the linear function (x, x(cid:48)) (cid:55)\u2192 xi + x(cid:48) i with Nsquare to get a depth-2 network Ni that calculates (xi+x(cid:48) i)2 (cid:15) 2dL and has the same width and weight bound as Nsquare. Summing the networks Ni and subtracting 1 results with a depth-2 network Ninner that calculates (cid:104)x, x(cid:48)(cid:105) with an error of (cid:15) and hidden layer weights bounded by 2, and prediction layer weights bounded by 4.  2L and has width 16d2L  with an error of(cid:15)  Now, again by lemma 5 there is a depth-2 network Nf that calculates f in [\u22121, 1], with an error of (cid:15) 2 , has width at most 2L (cid:15) , hidden layer weights bounded by 1 and prediction layer weights bounded by 2L, and is L-Lipschitz. Finally, consider the depth-3 network NF that is the composition of Ninner and Nf . NF has width at most 16d2L  (cid:15) weight bound of max(4, 2L), and it satisfies  |NF (x, x(cid:48)) \u2212 F (x, x(cid:48))| = |Nf (Ninner(x, x(cid:48))) \u2212 f ((cid:104)x, x(cid:48)(cid:105))|  \u2264 |Nf (Ninner(x, x(cid:48))) \u2212 Nf ((cid:104)x, x(cid:48)(cid:105))| + |Nf ((cid:104)x, x(cid:48)(cid:105)) \u2212 f ((cid:104)x, x(cid:48)(cid:105))| (cid:15) \u2264 L|Ninner(x, x(cid:48)) \u2212 (cid:104)x, x(cid:48)(cid:105)| + 2  \u2264 L  +  = (cid:15)  (cid:15) 2L  (cid:15) 2  (cid:3)  References  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An Intro-  duction, volume 2044. Springer, 2012.  Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. In 29th Annual Conference on Learning Theory, pages 698-728, 2016.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 1989.  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in  Neural Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In 29th  Annual Conference on Learning Theory, pages 907-940, 2016.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Sys- tems, pages 2877-2885, 2013. DEPTH SEPARATION FOR NEURAL NETWORKS  Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-  linear functions. arXiv preprint arXiv:1610.09887, 2016.  Matus Telgarsky. Representation benefits of deep feedforward networks. In COLT, 2016."}