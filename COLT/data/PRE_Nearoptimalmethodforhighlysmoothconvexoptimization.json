{"1": "N. Agarwal and E. Hazan. Lower bounds for higher-order convex optimization. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774\u2013792. PMLR, 2018.  Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, 2018.  A. Gasnikov, E. Gorbunov, D. Kovalev, A. Mohhamed, and E. Chernousova. The global rate of convergence for optimal tensor methods in smooth convex optimization. Arxiv preprint arXiv:1809.00382, 2018.  B. Jiang, H. Wang, and S. Zhang. An optimal high-order tensor method for convex optimization.  Arxiv preprint arXiv:1812.06557, 2018.  R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for con- vex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092\u20131125, 2013.  A. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Ser. Tekhnich-  eskaya Kibernetika, 2, 1982.  Interscience, 1983.  A. Nemirovski and D. Yudin. Problem Complexity and Method Ef\ufb01ciency in Optimization. Wiley  Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).  Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.  Y. Nesterov.  Introductory lectures on convex optimization: A basic course. Kluwer Academic  Publishers, 2004.  Nesterov Y. Implementable tensor methods in unconstrained convex optimization. Core discussion papers, 2018. URL https://ideas.repec.org/p/cor/louvco/2018005.html.  Appendix A. Proofs from Section 4  Here we provide proofs of lemmas from Section 4. Proof [Proof of Lemma 13] To compute the derivative of z\u03b8, we note by optimality condition that  Taking derivatives with respect to \u03b8 on both sides gives  \u2207zF (z\u03b8, (cid:101)x\u03b8) = 0.  \u22072  zzF (z\u03b8, (cid:101)x\u03b8) \u00b7  z\u03b8 + \u22072  zxF (z\u03b8, (cid:101)x\u03b8) \u00b7  d d\u03b8  d d\u03b8 (cid:101)x\u03b8 = 0.  Hence, we have  d d\u03b8  z\u03b8 = \u2212 (cid:0)\u22072  zzF (z\u03b8, (cid:101)x\u03b8)(cid:1)\u22121  \u22072  zxF (z\u03b8, (cid:101)x\u03b8) \u00b7 (yk \u2212 xk).  (17)  13   NEAR-OPTIMAL HIGHLY SMOOTH CONVEX OPTIMIZATION  References  N. Agarwal and E. Hazan. Lower bounds for higher-order convex optimization. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774-792. PMLR, 2018.  Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, 2018.  A. Gasnikov, E. Gorbunov, D. Kovalev, A. Mohhamed, and E. Chernousova. The global rate of convergence for optimal tensor methods in smooth convex optimization. Arxiv preprint arXiv:1809.00382, 2018.  B. Jiang, H. Wang, and S. Zhang. An optimal high-order tensor method for convex optimization.  Arxiv preprint arXiv:1812.06557, 2018.  R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for con- vex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092-1125, 2013.  A. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Ser. Tekhnich-  eskaya Kibernetika, 2, 1982.  Interscience, 1983.  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley  Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).  Soviet Mathematics Doklady, 27(2):372-376, 1983.  Y. Nesterov.  Introductory lectures on convex optimization: A basic course. Kluwer Academic  Publishers, 2004.  Nesterov Y. Implementable tensor methods in unconstrained convex optimization. Core discussion papers, 2018. URL https://ideas.repec.org/p/cor/louvco/2018005.html."}