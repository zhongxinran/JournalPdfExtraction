{"1": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima for nonconvex optimization in linear time. Symposium on Theory of Computing, 2017.  Ernesto G Birgin, JL Gardenghi, Jos\u00b4e Mario Mart\u00b4\u0131nez, Sandra Augusta Santos, and Ph L Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order reg- ularized models. Mathematical Programming, 163(1-2):359-368, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford.  \u2018Convex until proven guilty\u2019: dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of 34th International Conference on Machine Learning, pages 654-663, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017b.  2   HINDER  Our result can be contrasted with the results of Birgin, Gardenghi, Mart\u00b4\u0131nez, Santos, and Toint (2017) who gives a runtime of O((T3+?)(cid:15)\u2212(p+1)/p) where the ? denotes the cost of finding a sta- tionary point of a pth order regularized problem. Letting p = 1 gives gradient descent and p = 2 cubic regularized Newton. However, for p > 2 all known methods for solving pth order have (cid:15)- dependencies that cause the computational runtime to scale at best with O((cid:15)\u22123/2) corresponding to cubic regularized Newton. Therefore our major contribution is to show it is possible to obtain an O((cid:15)\u22124/3) runtime \u2014 including computational cost \u2014 for finding (cid:15)-stationary points of nonconvex functions. See Table 1 for a comparison of our results with existing results.  Lipschitz method  \u2207f  gradient descent  \u2207f, \u22072f Carmon et al. (2017a) \u2207f, \u22073f Carmon et al. (2017a)  \u22072f  cubic reg. Nesterov and Polyak (2006)  Birgin et al.  \u2207pf  pth reg. (2017). \u2207f, \u22073f This paper. Thm 1. \u2207f, \u22073f This paper. Thm 2.  runtime  dimension-free lower bound (Car- mon et al., 2017b,c)  T1(cid:15)\u22122 T1(cid:15)\u22122 T1(cid:15)\u22127/4 T1(cid:15)\u221212/7 T1(cid:15)\u22125/3 T1(cid:15)\u22128/5 (T2 + d\u03c9)(cid:15)\u22123/2 T2(cid:15)\u22123/2  (Tp+?)(cid:15)\u2212 p+1  p  Tp(cid:15)\u2212(p+1)/p  ((T1 + d\u03c9)d + T2)(cid:15)\u22124/3  (T3 + d4)(cid:15)\u22124/3 T3(cid:15)\u22124/3  Table 1: Comparison of the runtime of different algorithms for finding stationary points of non- convex functions. The question mark is a placeholder for the time to solve a pth order regularization problem.  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima for nonconvex optimization in linear time. Symposium on Theory of Computing, 2017.  Ernesto G Birgin, JL Gardenghi, Jos\u00b4e Mario Mart\u00b4\u0131nez, Sandra Augusta Santos, and Ph L Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order reg- ularized models. Mathematical Programming, 163(1-2):359-368, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford.  \u2018Convex until proven guilty\u2019: dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of 34th International Conference on Machine Learning, pages 654-663, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017b. CUTTING PLANE METHODS CAN BE EXTENDED INTO NONCONVEX OPTIMIZATION1  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017c.  Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle  points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.  Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global perfor-  mance. Mathematical Programming, 108(1):177-205, 2006.  Cl\u00b4ement W Royer and Stephen J Wright. Complexity analysis of second-order line-search algo-  rithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017."}