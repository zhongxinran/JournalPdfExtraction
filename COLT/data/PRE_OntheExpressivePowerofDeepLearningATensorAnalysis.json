{"1": "Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom- positions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Richard Bellman, Richard Ernest Bellman, Richard Ernest Bellman, and Richard Ernest Bellman. Introduc-  tion to matrix analysis, volume 960. SIAM, 1970.  Yoshua Bengio. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1):  1-127, 2009.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. Neural Networks and Learning Systems, IEEE Transactions on, 25(8):1553-1565, 2014.  Joan Bruna and St\u00b4ephane Mallat. Invariant Scattering Convolution Networks. IEEE TPAMI, 2012.  Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report 05-02, 2005.  13   ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS  we show in app. A, pooling over large windows and trimming down a network\u2019s depth may bring to an exponential decrease in expressive efficiency.  The second point our theory sheds light on is sharing. As discussed in sec. 3.3, introducing weight sharing to a shallow network (CP model) considerably limits its expressive power. The net- work can only represent symmetric tensors, which in turn means that it is location invariant w.r.t. input vectors (patches). In the case of a deep network (HT model) the limitation posed by sharing is not as strict. Generated tensors need not be symmetric, implying that the network is capable of mod- eling location - a crucial ability in almost any real-world task. The above findings suggest that the sharing constraint is increasingly limiting as a network gets shallower, to the point where it causes complete ignorance to location. This could serve as an argument supporting the empirical success of deep convolutional networks - they bind together the statistical and computational advantages of sharing with many layers that mitigate its expressive limitations.  Lastly, our construction advocates locality, or more specifically, 1 \u00d7 1 receptive fields. Recent convolutional networks providing state of the art recognition performance (e.g. Lin et al. (2014); Szegedy et al. (2015)) make extensive use of 1 \u00d7 1 linear transformations, proving them to be very successful in practice. In view of our model, such 1 \u00d7 1 operators factorize tensors while providing universality with a minimal number of parameters. It seems reasonable to conjecture that for this task of factorizing coefficient tensors, larger receptive fields are not significantly helpful, as they lead to redundancy which may deteriorate performance in presence of limited training data. Investigation of this conjecture is left for future work.  Acknowledgments  Amnon Shashua would like to thank Tomaso Poggio and Shai S. Shwartz for illuminating discus- sions during the preparation of this manuscript. We would also like to thank Tomer Galanti, Tamir Hazan and Lior Wolf for commenting on draft versions of the paper. The work is partly funded by Intel grant ICRI-CI no. 9-2012-6133 and by ISF Center grant 1790/12. Nadav Cohen is supported by a Google Fellowship in Machine Learning.  References  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom- positions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Richard Bellman, Richard Ernest Bellman, Richard Ernest Bellman, and Richard Ernest Bellman. Introduc-  tion to matrix analysis, volume 960. SIAM, 1970.  Yoshua Bengio. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1):  1-127, 2009.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. Neural Networks and Learning Systems, IEEE Transactions on, 25(8):1553-1565, 2014.  Joan Bruna and St\u00b4ephane Mallat. Invariant Scattering Convolution Networks. IEEE TPAMI, 2012.  Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report 05-02, 2005. COHEN SHARIR SHASHUA  Nadav Cohen and Amnon Shashua. Simnets: A generalization of convolutional networks. Advances in  Neural Information Processing Systems (NIPS), Deep Learning Workshop, 2014.  Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions.  International Conference on Machine Learning (ICML), 2016.  Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), 2016.  Systems, 2(4):303-314, 1989.  G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks.  In Advances in Neural  Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  176, 1990.  F Girosi and T Poggio. Networks and the best approximation property. Biological cybernetics, 63(3):169-  W Hackbusch and S K\u00a8uhn. A New Scheme for the Tensor Representation. Journal of Fourier Analysis and  Applications, 15(5):706-722, 2009.  Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics. Springer Science & Business Media, Berlin, Heidelberg, February 2012.  Benjamin D Haeffele and Ren\u00b4e Vidal. Global Optimality in Tensor Factorization, Deep Learning, and Be-  yond. CoRR abs/1202.2745, cs.NA, 2015.  Andr\u00b4as Hajnal, Wolfgang Maass, Pavel Pudl\u00b4ak, M\u00b4arl\u00b4o Szegedy, and Gy\u00a8orgy Tur\u00b4an. Threshold circuits of bounded depth. In Foundations of Computer Science, 1987., 28th Annual Symposium on, pages 99-110. IEEE, 1987.  Johan Hastad. Almost optimal lower bounds for small depth circuits. In Proceedings of the eighteenth annual  ACM symposium on Theory of computing, pages 6-20. ACM, 1986.  Johan H\u02daastad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational Com-  plexity, 1(2):113-129, 1991.  Kurt Hornik, Maxwell B Stinchcombe, and Halbert White. Multilayer feedforward networks are universal  approximators. Neural networks, 2(5):359-366, 1989.  Brian Hutchinson, Li Deng, and Dong Yu. Tensor Deep Stacking Networks. IEEE Trans. Pattern Anal. Mach.  Intell. (), 35(8):1944-1957, 2013.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the Perils of Non-Convexity: Guaranteed  Training of Neural Networks using Tensor Methods. CoRR abs/1506.08473, 2015.  Frank Jones. Lebesgue integration on Euclidean space. Jones & Bartlett Learning, 2001.  Mauricio Karchmer. Communication complexity a new approach to circuit depth. 1989.  Tamara G Kolda and Brett W Bader. Tensor Decompositions and Applications. SIAM Review (), 51(3):  455-500, 2009. ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS  Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V Oseledets, and Victor S Lempitsky. Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition. CoRR abs/1202.2745, cs.CV, 2014.  Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook  of brain theory and neural networks, 3361(10), 1995.  Min Lin, Qiang Chen, and Shuicheng Yan. Network In Network.  International Conference on Learning  Representations, 2014.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural net-  works. Advances in Neural Information Processing Systems, 2014.  Wolfgang Maass, Georg Schnitger, and Eduardo D Sontag. A comparison of the computational power of  sigmoid and Boolean threshold circuits. Springer, 1994.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks. arXiv  preprint arXiv:1411.7717, 2014.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages 2877- 2885, 2013.  Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems, pages 2924-2932, 2014.  Alexander Novikov, Anton Rodomanov, Anton Osokin, and Dmitry Vetrov. Putting MRFs on a Tensor Train.  ICML, pages 811-819, 2014.  Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of inference regions of deep feed  forward networks with piece-wise linear activations. arXiv preprint arXiv, 1312, 2013.  Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 8:143-195,  January 1999.  Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 689-690. IEEE, 2011.  Ran Raz and Amir Yehudayoff. Lower bounds and separations for constant depth multilinear circuits. Com-  putational Complexity, 18(2):171-207, 2009.  Benjamin Rossman, Rocco A Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem for  boolean circuits. arXiv preprint arXiv:1504.03398, 2015.  Walter Rudin. Functional analysis. international series in pure and applied mathematics, 1991.  Thomas Serre, Lior Wolf, and Tomaso Poggio. Object Recognition with Features Inspired by Visual Cortex.  CVPR, 2:994-1000, 2005.  Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib, Richard M Schwartz, and John Makhoul. Statistical Machine Translation Features with Multitask Tensor Networks. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, cs.CL, 2015.  Amir Shpilka and Amir Yehudayoff. Arithmetic circuits: A survey of recent results and open questions.  Foundations and Trends in Theoretical Computer Science, 5(3-4):207-388, 2010. COHEN SHARIR SHASHUA  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-  tion. arXiv preprint arXiv:1409.1556, 2014.  Michael Sipser. Borel sets and circuit complexity. ACM, New York, New York, USA, December 1983.  Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Y Ng. Reasoning With Neural Tensor Networks for Knowledge Base Completion. Advances in Neural Information Processing Systems, pages 926-934, 2013.  Le Song, Mariya Ishteva, Ankur P Parikh, Eric P Xing, and Haesun Park. Hierarchical Tensor Decomposition  of Latent Tree Graphical Models. ICML, pages 334-342, 2013.  Maxwell Stinchcombe and Halbert White. Universal approximation using feedforward networks with non- International Joint Conference on Neural Networks, pages  sigmoid hidden layer activation functions. 613-617 vol.1, 1989.  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,  Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. CVPR, 2015.  Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. DeepFace: Closing the Gap to Human- In CVPR \u201914: Proceedings of the 2014 IEEE Conference on  Level Performance in Face Verification. Computer Vision and Pattern Recognition. IEEE Computer Society, June 2014.  Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101,  2015.  Y Yang and D B Dunson. Bayesian conditional tensor factorizations for high-dimensional classification.  Journal of the American Statistical, 2015.  Dong Yu, Li Deng, and Frank Seide. Large Vocabulary Speech Recognition Using Deep Tensor Neural  Networks. INTERSPEECH, pages 6-9, 2012.  Daniel Zoran and Yair Weiss. \u201dNatural Images, Gaussian Mixtures and Dead Leaves\u201d. Advances in Neural  Information Processing Systems, pages 1745-1753, 2012. ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS"}