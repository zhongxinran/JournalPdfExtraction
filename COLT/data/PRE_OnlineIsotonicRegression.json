{"1": "M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4): 641-647, 1955.  K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponen-  tial family of distributions. Journal of Machine Learning, 43(3):211-246, 2001.  14   KOT\u0141OWSKI, KOOLEN AND MALEK  8. Conclusions and open problem  We introduced the online version of the isotonic regression problem, in which the learner must sequentially predict the labels as well as the best isotonic function. We gave a computationally efficient version of the Exponential Weights algorithm which plays on a covering net for the set of isotonic functions and proved that its regret is bounded by O(T 1/3 log2/3(T )). We also showed an \u2126(T 1/3) lower bound on the regret of any algorithm, essentially closing the gap.  There are some interesting directions for future research. First, we believe that the discretization (covering net) is not needed in the algorithm, and a carefully devised continuous prior would work as well. We were, however, unable to find a prior that would produce the optimal regret bound and remain computationally efficient. Second, we are interested to see whether some regularized version of FTL (e.g., by means of relative entropy), or the forward algorithm (Vovk-Azoury-Warmuth) (Azoury and Warmuth, 2001) could work for this problem. However, the most interesting research direction is the extension to the partial order case. In this setting, the learner is given a set of points X = {x1, . . . , xT }, together with a partial order relation (cid:22) on X. The goal of the learner is to sequentially predict the labels not much worse than the best function which respects the isotonic constraints: xi (cid:22) xj \u2192 f (xi) \u2264 f (xj). A typical application would be nonparametric data modeling with multiple features, where domain knowledge may tell us that increasing the value of any of the features is likely to increase the value of the label. The off-line counterpart has been extensively studied in the statistics literature (Robertson et al., 1998), and the optimal isotonic function shares many properties (e.g., averaging within level sets) with the linear order case. The discretized Exponential Weights algorithm, which was presented in this paper, can be extended to deal with partial orders. The analysis closely follows the proof of Theorem 4 except that the size of the covering net FK is no longer O(T K) but now depends on the structure of (cid:22). We believe that |FK| is the right quantity to measure the complexity of the problem and the algorithm will remain competitive in this more general setting. Unfortunately, the algorithm is no longer efficiently implementable and suffers from the same problems that plague inference in graphical models on general graphs. It thus remains an open problem to find an efficient algorithm for the partial order case.  Acknowledgments  We thank the anonymous reviewers for suggestions which improved the quality of our work. Wouter Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO, Veni grant 639.021.439), Wojciech Kot\u0142owski acknowledges support from the Polish National Sci- ence Centre (grant no. 2013/11/D/ST6/03050), and Alan Malek acknowledges support from Adobe through a Digital Marketing Research Award.  References  M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4): 641-647, 1955.  K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponen-  tial family of distributions. Journal of Machine Learning, 43(3):211-246, 2001. ONLINE ISOTONIC REGRESSION  L. Birg\u00b4e and P. Massart. Rates of convergence for minimum contrast estimators. Probability Theory  and Related Fields, 97:113-150, 1993.  H. D. Brunk. Maximum likelihood estimates of monotone parameters. Annals of Mathematical  N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors. Machine  Statistics, 26(4):607-616, 1955.  Learning, 43(3):247-264, 2001.  2006.  Wiley & Sons, 2014.  rica, 86(2):429-438, 1999.  97-106, 2007.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  D. DeTemple and W. Webb. Combinatorial reasoning: An introduction to the art of counting. John  R. Dykstra, J. Hewett, and T. Robertson. Nonparametric, isotonic discriminant procedures. Biomet-  T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1):  P. Gaillard and S. Gerchinovitz. A chaining algorithm for online nonparametric regression.  In  Conference on Learning Theory (COLT), pages 764-796, 2015.  S. van de Geer. Estimating a regression function. Annals of Statistics, 18:907-924, 1990.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69(2-3):169-192, 2007.  S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and  single index models with isotonic regression. In NIPS, pages 927-935. 2011.  A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT,  2009.  537-544, 2009.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Information and Computation, 132(1):1-63, 1997.  W. Kot\u0142owski and R. S\u0142owi\u00b4nski. Rule learning with monotonicity constraints.  In ICML, pages  W. Kot\u0142owski and R. S\u0142owinski. On nonparametric ordinal classification with monotonicity con- straints. IEEE Transactions on Knowledge and Data Engineering, 25(11):2576-2589, 2013.  J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.  Psychometrika, 29(1):1-27, 1964.  R. Kyng, A. Rao, and S. Sachdeva. Fast, provable algorithms for isotonic regression in all (cid:96)p-norms.  In NIPS, 2015.  J. de Leeuw, K. Hornik, and P. Mair. Isotone optimization in R: Pool-adjacent-violators algorithm  (PAVA) and active set methods. Journal of Statistical Software, 32:1-24, 2009. KOT\u0141OWSKI, KOOLEN AND MALEK  R. Luss, S. Rosset, and M. Shahar. Efficient regularized isotonic regression with application to  gene-gene interaction search. Annals of Applied Statistics, 6(1):253-283, 2012.  A. K. Menon, X. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabili-  ties with a ranking loss. In ICML, 2012.  T. Moon, A. Smola, Y. Chang, and Z. Zheng. Intervalrank: Isotonic regression with listwise and  pairwise constraint. In WSDM, pages 151-160. ACM, 2010.  H. Narasimhan and S. Agarwal. On the relationship between binary classification, bipartite ranking,  and binary class probability estimation. In NIPS, pages 2913-2921. 2013.  A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning.  In  ICML, pages 625-632, 2005.  G. Obozinski, C. E. Grant, G. R. G. Lanckriet, M. I. Jordan, and W. W. Noble. Consistent proba-  bilistic outputs for protein function prediction. Genome Biology, 2008 2008.  A. Rakhlin and K. Sridharan. Online nonparametric regression. In COLT, pages 1232-1264, 2014.  T. Robertson, F. T. Wright, and R. L. Dykstra. Order Restricted Statistical Inference. John Wiley &  Sons, 1998.  8:45-114, 2014.  S. de Rooij and T. van Erven. Learning the switching rate by discretising Bernoulli sources online.  In AISTATS, pages 432-439, 2009.  A. Saumard and J. A. Wellner. Log-concavity and strong log-concavity: A review. Statistics Surveys,  S. Shalev-Shwartz. Online learning and online convex optimization. In Foundations and Trends in  Machine Learning, volume 4, pages 107-194. 2012.  M. Stylianou and N. Flournoy. Dose finding using the biased coin up-and-down design and isotonic  regression. Biometrics, 58(1):171-177, 2002.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2009.  V. Vovk. Aggregating strategies. In COLT, pages 371-386, 1990.  V. Vovk, I. Petej, and V. Fedorova. Large-scale probabilistic predictors with and without guarantees  of validity. In NIPS, pages 892-900. 2015.  Bin Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer-  Verlag, 1997.  B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability esti-  mates. In KDD, pages 694-699, 2002.  C.-H. Zhang. Risk bounds in isotonic regression. The Annals of Statistics, 30(2):528-555, 2002.  M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML,  pages 928-936, 2003. ONLINE ISOTONIC REGRESSION"}