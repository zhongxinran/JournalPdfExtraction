{"1": "We give a polynomial-time algorithm for learning neural networks with one layer of sigmoids feeding into any Lipschitz, monotone activation function (e.g., sigmoid or ReLU).  The algorithm succeeds with respect to {\\em any} distribution on the unit ball in $n$ dimensions (hidden weight vectors in the first layer have unit norm).  This is the first efficient algorithm for learning a general class of neural networks with more than one nonlinear layer that makes no restrictions on the VC-dimension of the network. Algorithms for learning relaxations of our model (e.g., allowing larger weight vectors in the first layer) would lead to breakthroughs on notoriously hard problems in Boolean function learning.  Thus, our results are \u201cbest possible\u201d with respect to current techniques. Our algorithm\u2013 {\\em Alphatron}\u2013 is an iterative update rule that combines isotonic regression with kernel methods.  We use this algorithm to give a simple reduction for translating PAC learning algorithms to the more general, real-valued setting of {\\em probabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d. noise-tolerance.  This substantially improves many longstanding results for PAC learning Boolean functions."}