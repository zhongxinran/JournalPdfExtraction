{"1": "convex duality. 2006.  Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. JMLR, 3:463-482, Nov 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  St\u00b4ephane Boucheron, Olivier Bousquet, and G\u00b4abor Lugosi. Theory of classification: a survey of  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Joseph T. Chang and David Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):  287-317, 1997.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Breg-  man distances. Machine Learning, 48(1-3):253-285, 2002.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer, 1996.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. Annals of Statistics, 28(2):337-407, 2000.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of  Statistics, 29:1189-1232, 2000.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. In  FOCS, 2006.  1994.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Michael Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT Press,  Christian L\u00b4eonard. Orlicz spaces. http://www.cmap.polytechnique.fr/\u02dcleonard/  papers/orlicz.pdf, 2007. Accessed 2015-04-28.  Christian L\u00b4eonard. Minimization of entropy functionals. J. Math. Anal. Appl., 346:183-204, 2008.  Kfir Levy, Elad Hazan, and Tomer Koren. Logistic regression: Tight bounds for stochastic and  online optimization. In COLT, 2014.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  R. Tyrrell Rockafellar. Integrals which are convex functionals I. Pacific J. Math., 24:525-539, 1968.  13   CONVEX RISK MINIMIZATION AND CONDITIONAL PROBABILITY ESTIMATION  References  convex duality. 2006.  Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. JMLR, 3:463-482, Nov 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  St\u00b4ephane Boucheron, Olivier Bousquet, and G\u00b4abor Lugosi. Theory of classification: a survey of  recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  Joseph T. Chang and David Pollard. Conditioning as disintegration. Statistica Neerlandica, 51(3):  287-317, 1997.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Breg-  man distances. Machine Learning, 48(1-3):253-285, 2002.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer, 1996.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. Annals of Statistics, 28(2):337-407, 2000.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of  Statistics, 29:1189-1232, 2000.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. In  FOCS, 2006.  1994.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Michael Kearns and Umesh Vazirani. An introduction to computational learning theory. MIT Press,  Christian L\u00b4eonard. Orlicz spaces. http://www.cmap.polytechnique.fr/\u02dcleonard/  papers/orlicz.pdf, 2007. Accessed 2015-04-28.  Christian L\u00b4eonard. Minimization of entropy functionals. J. Math. Anal. Appl., 346:183-204, 2008.  Kfir Levy, Elad Hazan, and Tomer Koren. Logistic regression: Tight bounds for stochastic and  online optimization. In COLT, 2014.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  R. Tyrrell Rockafellar. Integrals which are convex functionals I. Pacific J. Math., 24:525-539, 1968. TELGARSKY DUD\u00b4IK SCHAPIRE  R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.  R. Tyrrell Rockafellar. Conjugate Duality and Optimization. SIAM Publications, 1974.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new  explanation for the effectiveness of voting methods. In ICML, pages 322-330, 1997.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Nathan Srebro, and Karthik Sridharan. Fast rates for regularized objectives.  In NIPS, 2008.  Matus Telgarsky. A primal-dual convergence analysis of boosting. JMLR, 13:561-606, 2012.  Matus Telgarsky. Boosting with the logistic loss is consistent. In COLT, 2013.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. The Annals of Statistics, 32:56-85, 2004.  Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals  of Statistics, 33:1538-1579, 2005."}