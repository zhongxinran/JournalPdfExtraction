{"1": "society, 68(3):337-404, 1950.  384-414, 2010.  Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  M. S. Bartlett. Approximate confidence intervals. Biometrika, 40(1/2):12-19, 1953.  Gilles Blanchard and Nicole M\u00fccke. Optimal rates for regularization of statistical inverse learning  problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.  L\u00e9on Bottou and Olivier Bousquet. The trade-offs of large scale learning. In Advances in Neural  Information Processing systems, pages 161-168, 2008.  St\u00e9phane Boucheron and Pascal Massart. A high-dimensional Wilks phenomenon. Probability  Theory and Related Fields, 150(3-4):405-433, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Found.  Comput. Math., 7(3):331-368, July 2007.  Nicolo Cesa-Bianchi, Yishay Mansour, and Ohad Shamir. On the complexity of learning with  kernels. In Conference on Learning Theory, pages 297-325, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-  rithm. arXiv preprint arXiv:1702.07254, 2017.  Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic re-  gression: The importance of being improper. Proceedings of COLT, 2018.  Stuart Geman, Elie Bienenstock, and Ren\u00e9 Doursat. Neural networks and the bias/variance  dilemma. Neural Computation, 4(1):1-58, 1992.  L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E. De Vito, and Alessandro Verri. Spectral  algorithms for supervised learning. Neural Computation, 20(7):1873-1897, 2008.  Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statis-  tics: the approach based on in\ufb02uence functions, volume 196. John Wiley & Sons, 2011.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: tight bounds for stochastic and online optimization. In Proceedings of The 27th Conference on Learning Theory, volume 35, pages 197-209, 2014.  Arthur E. Hoerl and Robert W. Kennard. Ridge regression iterative estimation of the biasing param-  eter. Communications in Statistics-Theory and Methods, 5(1):77-88, 1976.  13   FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  References  society, 68(3):337-404, 1950.  384-414, 2010.  Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  M. S. Bartlett. Approximate confidence intervals. Biometrika, 40(1/2):12-19, 1953.  Gilles Blanchard and Nicole M\u00fccke. Optimal rates for regularization of statistical inverse learning  problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.  L\u00e9on Bottou and Olivier Bousquet. The trade-offs of large scale learning. In Advances in Neural  Information Processing systems, pages 161-168, 2008.  St\u00e9phane Boucheron and Pascal Massart. A high-dimensional Wilks phenomenon. Probability  Theory and Related Fields, 150(3-4):405-433, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Found.  Comput. Math., 7(3):331-368, July 2007.  Nicolo Cesa-Bianchi, Yishay Mansour, and Ohad Shamir. On the complexity of learning with  kernels. In Conference on Learning Theory, pages 297-325, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-  rithm. arXiv preprint arXiv:1702.07254, 2017.  Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic re-  gression: The importance of being improper. Proceedings of COLT, 2018.  Stuart Geman, Elie Bienenstock, and Ren\u00e9 Doursat. Neural networks and the bias/variance  dilemma. Neural Computation, 4(1):1-58, 1992.  L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E. De Vito, and Alessandro Verri. Spectral  algorithms for supervised learning. Neural Computation, 20(7):1873-1897, 2008.  Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statis-  tics: the approach based on in\ufb02uence functions, volume 196. John Wiley & Sons, 2011.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: tight bounds for stochastic and online optimization. In Proceedings of The 27th Conference on Learning Theory, volume 35, pages 197-209, 2014.  Arthur E. Hoerl and Robert W. Kennard. Ridge regression iterative estimation of the biasing param-  eter. Communications in Statistics-Theory and Methods, 5(1):77-88, 1976. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  S. Sathiya Keerthi, K. B. Duan, Shirish Krishnaj Shevade, and Aun Neow Poo. A fast dual algorithm  for kernel logistic regression. Machine learning, 61(1-3):151-165, 2005.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems, pages 1477-1485, 2015.  John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Confer- ence on Machine Learning (ICML), 2001.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Science & Business  Media, 2006.  P. McCullagh and J.A. Nelder. Generalized Linear Models. Chapman and Hall, 1989.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. Technical  Report 1605.01288, ArXiv, 2016.  ming, volume 13. SIAM, 1994.  Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-  Dmitrii Ostrovskii and Francis Bach.  Finite-sample analysis of M-estimators using self-  concordance. Technical Report 1810.06838, arXiv, 2018.  Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Advances in Neural In- formation Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada., pages 8125-8135, 2018.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss func-  tions. arXiv preprint arXiv:1501.06598, 2015.  Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems 30, pages 3215-3225. Curran Associates, Inc., 2017.  Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel  method. In Advances in Neural Information Processing Systems, pages 3888-3898, 2017.  Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated  sub-gradient solver for SVM. Mathematical programming, 127(1):3-30, 2011.  John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-  sity Press, 2004.  Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their ap-  proximations. Constructive Approximation, 26(2):153-172, 2007.  Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. Fast rates for regularized objectives.  In Advances in Neural Information Processing Systems 21, pages 1545-1552. 2009. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  Ingo Steinwart and Clint Scovel. Fast rates for support vector machines using gaussian kernels. The  Annals of Statistics, 35(2):575-607, 2007.  Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regres-  sion. In Proc. COLT, 2009.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht. Large scale kernel  learning using block coordinate descent. arXiv preprint arXiv:1602.05310, 2016.  Sara A. Van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of  Statistics, 36(2):614-645, 2008.  Aad W. Van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.  Tim Van Erven, Peter D. Gr\u00fcnwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793- 1861, 2015.  Grace Wahba. Spline Models for Observational Data, volume 59. SIAM, 1990.  Vadim Yurinsky. Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics.  Springer-Verlag, Berlin, 1995. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  Organization of the"}