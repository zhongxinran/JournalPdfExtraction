{"Conference on Learning Theory 2019: Preface": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Conference on Learning Theory 2019: Preface", "abstract": "Preface to the proceedings of the 32nd Conference on Learning Theory.", "pdf_url": "http://proceedings.mlr.press/v99/beygelzimer19a/beygelzimer19a.pdf", "keywords": []}, "Inference under Information Constraints: Lower Bounds from Chi-Square Contraction": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Inference under Information Constraints: Lower Bounds from Chi-Square Contraction", "abstract": "Multiple users getting one sample each from an unknown  distribution seek to enable a central server to conduct statistical inference. However, each player can only provide limited amount of information about its sample to the server.  We propose a unified framework to study such distributed inference problems under local information constraints.  We model the local information constraints by a set of channels $\\mathcal{W}$: each player chooses a channel from $\\mathcal{W}$, and then passes their data through this channel before transmitting the output to the server. The goal in this distributed setting is to understand the blow-up in data requirement imposed by the information constraints, compared to the centralized setting where all data samples are available to the server.  We introduce two notions of \\emph{chi-square fluctuations} which provide bounds for the  average distance and the distance to the  average of a local perturbation. When information constraints are imposed, by the standard data-processing inequality, pairwise distances contract and so do our chi-square fluctuations. We provide a precise characterization of this contraction for discrete $k$-ary distributions and use it to obtain to general lower bounds for distribution learning and testing under information constraints. Our results involve  notions of minmax and maxmin chi-square fluctuations, where  the maximum is over the choice of channels and the minimum is over perturbations. The former emerges when considering public-coin protocols for testing and is bounded in terms of Frobenius norm of a positive semidefinite matrix $H$, a function of the channel family $\\mathcal{W}$. The latter appears for private-coin protocols and is bounded by the nuclear norm of $H$ which is smaller than its Frobenius norm, establishing a separation between the sample complexity of testing using public and private coins.", "pdf_url": "http://proceedings.mlr.press/v99/acharya19a/acharya19a.pdf", "keywords": ["statistical inference", "distribution testing", "distribution learning", "distributed algorithms"], "reference": "Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Distributed simulation and distributed  inference. ArXiV, abs/1804.06952, 2018a.  Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Inference under information constraints I: Lower bounds from chi-square contraction, 2018b. Preprint available at arXiv:abs/1812.11476.  Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Hadamard response: Estimating distributions  privately, efficiently, and with little communication. ArXiV, abs/1802.04705, 2018c.  Jayadev Acharya, Cl\u00e9ment L. Canonne, Cody Freitag, and Himanshu Tyagi. Test without trust: Optimal locally private distribution testing. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS\u201919), 2019. To appear. Full version available on arXiv (abs/1808.02174).  Rudolf Ahlswede and Imre Csisz\u00e1r. Hypothesis testing with communication constraints. IEEE  Transactions on Information Theory, 32(4):533-542, July 1986.  Sivaraman Balakrishnan and Larry Wasserman. Hypothesis testing for high-dimensional multinomi- als: A selective review. The Annals of Applied Statistics, 12(2):727-749, 2018. ISSN 1932-6157. doi: 10.1214/18-AOAS1155SF. URL https://doi.org/10.1214/18-AOAS1155SF.  Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, communi- cation complexity and privacy. In Proceedings of the 25th Conference on Learning Theory, COLT 2012, volume 23 of JMLR Proceedings, pages 26.1-26.22. JMLR.org, 2012.  Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Symposium on Theory of Computing Conference, STOC\u201916, pages 1011-1020. ACM, 2016.  Cl\u00e9ment L. Canonne. A Survey on Distribution Testing: your data is Big. But is it Blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, April 2015.  Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience [John Wiley & Sons], Hoboken, NJ, second edition, 2006. ISBN 978-0-471-24195-9; 0-471-24195-4.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial Methods in Density Estimation. Springer Series in Statistics. Springer New York, 2001. ISBN 9780387951171. URL http://books.google. com/books?id=jvT-sUt1HZYC.  Ilias Diakonikolas. Learning structured distributions. In Handbook of Big Data. CRC Press, 2016.  Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, and Ludwig Schmidt. Communication-efficient distributed learning of discrete distributions. In Advances in Neural Information Processing Systems 30, pages 6394-6404, 2017.  13   INFERENCE UNDER INFORMATION CONSTRAINTS  References  Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Distributed simulation and distributed  inference. ArXiV, abs/1804.06952, 2018a.  Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Inference under information constraints I: Lower bounds from chi-square contraction, 2018b. Preprint available at arXiv:abs/1812.11476.  Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Hadamard response: Estimating distributions  privately, efficiently, and with little communication. ArXiV, abs/1802.04705, 2018c.  Jayadev Acharya, Cl\u00e9ment L. Canonne, Cody Freitag, and Himanshu Tyagi. Test without trust: Optimal locally private distribution testing. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS\u201919), 2019. To appear. Full version available on arXiv (abs/1808.02174).  Rudolf Ahlswede and Imre Csisz\u00e1r. Hypothesis testing with communication constraints. IEEE  Transactions on Information Theory, 32(4):533-542, July 1986.  Sivaraman Balakrishnan and Larry Wasserman. Hypothesis testing for high-dimensional multinomi- als: A selective review. The Annals of Applied Statistics, 12(2):727-749, 2018. ISSN 1932-6157. doi: 10.1214/18-AOAS1155SF. URL https://doi.org/10.1214/18-AOAS1155SF.  Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, communi- cation complexity and privacy. In Proceedings of the 25th Conference on Learning Theory, COLT 2012, volume 23 of JMLR Proceedings, pages 26.1-26.22. JMLR.org, 2012.  Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Symposium on Theory of Computing Conference, STOC\u201916, pages 1011-1020. ACM, 2016.  Cl\u00e9ment L. Canonne. A Survey on Distribution Testing: your data is Big. But is it Blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, April 2015.  Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience [John Wiley & Sons], Hoboken, NJ, second edition, 2006. ISBN 978-0-471-24195-9; 0-471-24195-4.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial Methods in Density Estimation. Springer Series in Statistics. Springer New York, 2001. ISBN 9780387951171. URL http://books.google. com/books?id=jvT-sUt1HZYC.  Ilias Diakonikolas. Learning structured distributions. In Handbook of Big Data. CRC Press, 2016.  Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, and Ludwig Schmidt. Communication-efficient distributed learning of discrete distributions. In Advances in Neural Information Processing Systems 30, pages 6394-6404, 2017. INFERENCE UNDER INFORMATION CONSTRAINTS  John C. Duchi and Martin J. Wainwright. Distance-based and continuum Fano inequalities with  applications to statistical estimation. ArXiV, abs/1311.2669, 2013.  John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local privacy and statistical minimax rates. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, pages 429-438. IEEE Computer Society, 2013.  Cynthia Dwork. Differential privacy: A survey of results. In Theory and Applications of Models of  Computation, volume 4978, pages 1-19. Springer, 2008. ISBN 978-3-540-79227-7.  Orr Fischer, Uri Meir, and Rotem Oshman. Distributed uniformity testing. In Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing, PODC 2018, pages 455-464. ACM, 2018.  Te Sun Han. Hypothesis testing with multiterminal data compression.  IEEE Transactions on  Information Theory, 33(6):759-772, November 1987.  Te Sun Han and Shun-Ichi Amari. Statistical inference under multiterminal data compression. IEEE  Transactions on Information Theory, 44(6):2300-2324, October 1998.  Yanjun Han, Ayfer \u00d6zg\u00fcr, and Tsachy Weissman. Geometric lower bounds for distributed parameter estimation under communication constraints. In Proceedings of the 31st Conference on Learning Theory, COLT 2018, volume 75 of Proceedings of Machine Learning Research, pages 3163-3188. PMLR, 2018.  Yanjun Han, Ayfer \u00d6zg\u00fcr, and Tsachy Weissman. Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints. ArXiv e-prints, abs/1802.08417v1, February 2018. First version (https://arxiv.org/abs/1802.08417v1).  Peter Kairouz, Keith Bonawitz, and Daniel Ramage. Discrete distribution estimation under local privacy. In Proceedings of the 33rd International Conference on Machine Learning, ICML 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 2436-2444. JMLR.org, 2016.  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data.  IEEE Transactions on Information Theory, 54(10):4750-4755, 2008.  David Pollard. Asymptopia, 2003. URL http://www.stat.yale.edu/~pollard/Books/  Asymptopia/. Manuscript.  Ronitt Rubinfeld. Taming big probability distributions. XRDS: Crossroads, The ACM Magazine for Students, 19(1):24, sep 2012. doi: 10.1145/2331042.2331052. URL http://dx.doi.org/ 10.1145/2331042.2331052.  Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In Advances in Neural Information Processing Systems 27, pages 163-171, 2014.  Or Sheffet. Locally private hypothesis testing. In Jennifer Dy and Andreas Krause, editors, Proceed- ings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4612-4621, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. INFERENCE UNDER INFORMATION CONSTRAINTS  Shaowei Wang, Liusheng Huang, Pengzhan Wang, Yiwen Nie, Hongli Xu, Wei Yang, Xiang-Yang Li, and Chunming Qiao. Mutual information optimally local private discrete distribution estimation. ArXiV, abs/1607.08025, 2016.  Thomas Watson. Communication complexity of statistical distance. TOCT, 10(1):2:1-2:11, 2018.  Michele Wigger and Roy Timo. Testing against independence with multiple decision centers. IEEE International Conference on Signal Processing and Communications, IISc, Bangalore, June 2016.  Yu Xiang and Young Han Kim. Interactive hypothesis testing against independence. In Proceedings of the 2013 IEEE International Symposium on Information Theory (ISIT\u201913), pages 1782-1786, 2013.  Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds on Bayes risk in decentralized  estimation. IEEE Transactions on Information Theory, 63(3):1580-1600, 2017.  Min Ye and Alexander Barg. Optimal schemes for discrete distribution estimation under locally  differential privacy. ArXiV, abs/1702.00610, 2017.  Bin Yu. Assouad, Fano, and Le Cam.  In Festschrift for Lucien Le Cam, pages 423-435. Springer, 1997. doi: 10.1007/978-1-4612-1880-7_29. URL http://dx.doi.org/10. 1007/978-1-4612-1880-7_29.  Yuchen Zhang, John Duchi, Michael I. Jordan, and Martin J. Wainwright. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems 26, pages 2328-2336, 2013. "}, "Learning in Non-convex Games with an Optimization Oracle": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning in Non-convex Games with an Optimization Oracle", "abstract": "We consider  online learning in an adversarial, non-convex setting under the assumption that the learner has an access to an offline optimization oracle.  In the general setting of prediction with expert advice, Hazan and Koren established that in the optimization-oracle model, online learning requires exponentially more computation than statistical learning.  In this paper we show that by slightly strengthening the oracle model, the online and the statistical learning models become computationally equivalent. Our result holds for any Lipschitz and bounded (but not necessarily convex) function.  As an application we demonstrate how the offline oracle enables efficient computation of an equilibrium in non-convex games, that include GAN (generative adversarial networks) as a special case.", "pdf_url": "http://proceedings.mlr.press/v99/agarwal19a/agarwal19a.pdf", "keywords": ["Online Learning", "Online Convex Optimization"], "reference": "S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochas- tic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1): 1-122, 2012.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge  university press, 2006. ISBN 9780511546921. doi: 10.1017/CBO9780511546921.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions. IEEE Transactions on Infor- mation Theory, 50:2050\u2014-2057, 2004. URL http://arxiv.org/abs/1305.2505.  Alon Cohen and Tamir Hazan. Following the perturbed leader for online structured learning.  In International Conference on Machine Learning, pages 1034-1042, 2015.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation.  In Conference on Learning Theory, pages 460-473, 2013.  Miroslav Dudik, Nika Haghtalab, Haipeng Luo, Robert E. Schapire, Vasilis Syrgkanis, and Jennifer Wortman Vaughan. Oracle-e\ufb03cient online learning and auction design.  11   Learning in Non-convex Games with an Optimization Oracle  scores to samples re\ufb02ecting the probability of being generated from the true distribution. Formally, by choosing a parameter x \u2208 X and drawing a random noise z, the x-th player produces a sample denote Gx(z). Conversely, the y-th player chooses a parameter y \u2208 Y and assign the score Dy(Gx(z)) \u2208 [0, 1] to the sample Gx(z). The function F usually corresponds to the log-likelihood of mistakenly assigning an high score to a synthetic example and vice versa. the network parameters. As a result, e\ufb03cient convergence to GANs is established by assuming an access to an o\ufb04ine oracle.  It is reasonable to assume that F is Lipschitz and bounded w.r.t.  5. Discussion  Our work establishes a computational equivalence between online and statistical learning in the non-convex setting. We shed light on the hardness result of (Hazan and Koren, 2016) by demonstrating that online learning is significantly more di\ufb03cult than statistical learning only when no structure is assumed.  One interesting direction for further investigation is to refine the comparison model and study the polynomial dependencies more carefully. One obvious question is to un- derstand the gap in terms of the horizon parameter T between the regret bounds for the one-dimensional and the multidimensional settings.  Acknowledgements  We thank Karan Singh for recognizing a bug in our original proof and several discussions. We also thank Alon Cohen and Roi Livni for fruitful discussions. Elad Hazan acknowledges funding from NSF award Number 1704860.  References  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochas- tic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1): 1-122, 2012.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge  university press, 2006. ISBN 9780511546921. doi: 10.1017/CBO9780511546921.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions. IEEE Transactions on Infor- mation Theory, 50:2050\u2014-2057, 2004. URL http://arxiv.org/abs/1305.2505.  Alon Cohen and Tamir Hazan. Following the perturbed leader for online structured learning.  In International Conference on Machine Learning, pages 1034-1042, 2015.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation.  In Conference on Learning Theory, pages 460-473, 2013.  Miroslav Dudik, Nika Haghtalab, Haipeng Luo, Robert E. Schapire, Vasilis Syrgkanis, and Jennifer Wortman Vaughan. Oracle-e\ufb03cient online learning and auction design. Learning in Non-convex Games with an Optimization Oracle  In Annual Symposium on Foundations of Computer Science - Proceedings, 2017. ISBN 9781538634646. doi: 10.1109/FOCS.2017.55.  Alon Gonen and Shai Shalev-Shwartz. Fast Rates for Empirical Risk Minimization of Strict Saddle Problems. In Ohad Shamir and Satyen Kale, editors, Proceedings of the 2017 Conference on Learning Theory, pages 1043\u2014-1063. PMLR, 2017. URL http: //arxiv.org/abs/1701.04271.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory  of Games, 3:97-139, 1957.  Elad Hazan.  Introduction to Online Convex Optimization. Foundations and Trends R(cid:13) in Optimization, 2(3-4):157-325, 2016. ISSN 2167-3888. doi: 10.1561/2400000013. URL http://ocobook.cs.princeton.edu/OCObook.pdfhttp://www.nowpublishers. com/article/Details/OPT-013.  Elad Hazan and Satyen Kale. Online submodular minimization. Journal of Machine Learn-  ing Research, 13(Oct):2903-2922, 2012.  Elad Hazan and Tomer Koren. The Computational Power of Optimization in Online Learn- ing. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 128-141. ACM, 2016. URL https://arxiv.org/pdf/1504.02089.pdf.  Elad Hazan, Karan Singh, and Cyril Zhang. E\ufb03cient regret minimization in non-convex  games. In International Conference on Machine Learning, pages 1433-1441, 2017.  Adam Kalai and Santosh Vempala. E\ufb03cient Algorithms for Online Decision Problems.  Journal of Computer and System Sciences, 2004.  Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On Convergence and Sta- bility of GANs. arXiv preprint arXiv:1705.07215, 2017. URL https://github.com/ kodalinaveen3/DRAGANhttp://arxiv.org/abs/1705.07215.  Dale Schuurmans and Martin A Zinkevich. Deep learning games. In Advances in Neural  Information Processing Systems, pages 1678-1686, 2016.  Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and ISSN 1935-8237. doi: 10.1561/  Trends R(cid:13) in Machine Learning, 4(2):107-194, 2011. 2200000018. URL http://www.nowpublishers.com/article/Details/MAL-018.  Tim Van Erven, Wojciech Kot(cid:32)lowski, and Manfred K Warmuth. Follow the leader with  dropout perturbations. In Conference on Learning Theory, pages 949-974, 2014.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928-936, 2003. "}, "Learning to Prune: Speeding up Repeated Computations": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning to Prune: Speeding up Repeated Computations", "abstract": "Algorithms often must solve sequences of closely related problems. If the algorithm runs a standard procedure with worst-case runtime guarantees on each instance, it will fail to take advantage of valuable structure shared across the problem instances. When a commuter drives from work to home, for example, there are typically only a handful of routes that will ever be the shortest path. A na\u00efve algorithm that does not exploit this common structure may spend most of its time checking roads that will never be in the shortest path. More generally, we can often ignore large swaths of the search space that will likely never contain an optimal solution. We present an algorithm that learns to maximally prune the search space on repeated computations, thereby reducing runtime while provably outputting the correct solution each period with high probability. Our algorithm employs a simple explore-exploit technique resembling those used in online algorithms, though our setting is quite different. We prove that, with respect to our model of pruning search spaces, our approach is optimal up to constant factors. Finally, we illustrate the applicability of our model and algorithm to three classic problems: shortest-path routing, string search, and linear programming. We present experiments confirming that our simple algorithm is effective at significantly reducing the runtime of solving repeated computations.", "pdf_url": "http://proceedings.mlr.press/v99/alabi19a/alabi19a.pdf", "keywords": [], "reference": "Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic foundations of algorithm configuration for combinatorial partitioning problems. Proceedings of the Conference on Learning Theory (COLT), 2017.  Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.  Proceedings of the International Conference on Machine Learning (ICML), 2018a.  Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design, online learning, and private optimization. Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), 2018b.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection.  SIAM Journal on Computing, 46(3):992-1017, 2017.  4   LEARNING TO PRUNE: SPEEDING UP REPEATED COMPUTATIONS  This work was supported in part by Israel Science Foundation (ISF) grant #1044/16, a subcontract on the DARPA Brandeis Project, and the Federmann Cyber Security Center in conjunction with the Israel national cyber directorate.  Acknowledgments  References  Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic foundations of algorithm configuration for combinatorial partitioning problems. Proceedings of the Conference on Learning Theory (COLT), 2017.  Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.  Proceedings of the International Conference on Machine Learning (ICML), 2018a.  Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design, online learning, and private optimization. Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), 2018b.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection.  SIAM Journal on Computing, 46(3):992-1017, 2017. "}, "Towards Testing Monotonicity of Distributions Over General Posets": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Towards Testing Monotonicity of Distributions Over General Posets", "abstract": "In this work, we consider the sample complexity required for testing the monotonicity of distributions over partial orders.  A distribution $p$ over a poset is {\\em monotone} if, for any pair of domain elements $x$ and $y$ such that $x \\preceq y$, $p(x) \\leq p(y)$. To understand the sample complexity of this problem, we introduce a new property called \\emph{bigness} over a finite domain, where the distribution is $T$-big if the minimum probability for any domain element is at least $T$. We establish a lower bound of $\\Omega(n/\\log n)$ for testing bigness of distributions on  domains of size $n$. We then build on  these lower bounds to give $\\Omega(n/\\log{n})$ lower bounds for  testing monotonicity over a matching poset of size $n$ and significantly improved lower bounds over the hypercube poset. We give sublinear sample complexity bounds for testing bigness and for testing monotonicity over the matching poset.   We then give a number of tools for analyzing upper bounds on the sample complexity of the monotonicity testing problem.", "pdf_url": "http://proceedings.mlr.press/v99/aliakbarpour19a/aliakbarpour19a.pdf", "keywords": ["Property Testing", "Monotone Distributions", "Partially Ordered Sets;"], "reference": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. A com- petitive test for uniformity of monotone distributions. In Proceedings of the Sixteenth International Conference on Arti(cid:12)cial Intelligence and Statistics, AISTATS 2013, Scotts- dale, AZ, USA, April 29 - May 1, 2013, pages 57{65, 2013.  Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for prop- erties of distributions. In Advances in Neural Information Processing Systems 28: Annual  12   Testing Monotonicity of Distributions  dition is su\ufb03cient for the distribution to be monotone in the induced subgraph G[B [ T ]. The tester accepts when it cannot rule out the possibility that T has the maximum possible probability mass. Recall that if the distribution is \u03f5-far from monotone, there must exist a large matching of \\violated\" edges. To this end, we show that the induced subgraph G[B [ T ] contains many disjoint violated edges, implying that there are many vertices in T outside of the support: the probability mass on T will be noticeably small and the tester will reject.  Upper bound via trying all matchings. In Section E.5 we give another upper bound for testing monotonicity of a distribution with respect to a bipartite graph which, in this case, has a small number of induced subgraphs that contains a perfect matching of their vertices. In particular, we show that O( log M ) samples are su\ufb03cient for this task, where M \u03f52 is the number of such induced subgraphs. We note that this bound matches the general learning upper bound of O(n=\u03f52) when M attains its maximum value of 2(cid:2)(n), but can potentially be better when M is asymptotically smaller. The main idea of our tester is as follows: if the distribution is \u03f5-far from monotone, there exists a matching of violated edges that is (cid:2)(\u03f5)-far from monotone. Hence, for each subgraph of G that admits a perfect matching, we may approximate the weight (violation amount) of this matching by simply comparing the total probability masses between the top part and the bottom part of the subgraph. We approximate these masses with error probability O(1=M ) for each subgraph, which allows us to apply a union bound over all subgraphs at the end. Our tester rejects if the weight of one such subgraph exceeds \u03f5, or accepts otherwise.  Acknowledgments  MA is supported by funds from the MIT-IBM Watson AI Lab (Agreement No. W1771646), the NSF grants IIS-1741137, and CCF-1733808. TG is supported by the NSF grants CCF- 1740751, CCF-1650733, CCF-1733808, and IIS-1741137. Part of this work was done while TG was a postdoctoral researcher at USC supported by Ilias Diakonikolas\u2019 USC startup grant. JP is supported by the NSF grants CCF-1565235, CCF-1650733, CCF-1733808, and IIS-1741137. RR is supported by by funds from the MIT-IBM Watson AI Lab (Agree- ment No. W1771646), the NSF grants CCF-1650733, CCF-1733808, IIS-1741137 and CCF- 1740751. AY is supported by the NSF grants CCF-1650733, CCF-1733808, IIS-1741137 and the DPST scholarship, Royal Thai Government. This work was completed while AY was at CSAIL, MIT.  References  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. A com- petitive test for uniformity of monotone distributions. In Proceedings of the Sixteenth International Conference on Arti(cid:12)cial Intelligence and Statistics, AISTATS 2013, Scotts- dale, AZ, USA, April 29 - May 1, 2013, pages 57{65, 2013.  Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for prop- erties of distributions. In Advances in Neural Information Processing Systems 28: Annual Testing Monotonicity of Distributions  Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 3591{3599, 2015.  Michal Adamaszek, Artur Czumaj, and Christian Sohler. Testing monotone continuous distributions on high-dimensional real cubes. In Proceedings of the Twenty-First An- nual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010, pages 56{65, 2010.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing mono- tone and unimodal distributions. In Proceedings of the Thirty-sixth Annual ACM Sym- posium on Theory of Computing, STOC \u201904, pages 381{390, New York, NY, USA, 2004. ACM. ISBN 1-58113-852-0.  Tugkan Batu, Sanjoy Dasgupta, Ravi Kumar, and Ronitt Rubinfeld. The complex- doi:  ity of approximating the entropy. 10.1137/S0097539702403645. URL https://doi.org/10.1137/S0097539702403645.  SIAM J. Comput., 35(1):132{150, 2005.  Aleksandrs Belovs and Eric Blais. A polynomial lower bound for testing monotonicity. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1021{1032, 2016.  Arnab Bhattacharyya, Eldar Fischer, Ronitt Rubinfeld, and Paul Valiant. Testing mono- tonicity of distributions over general partial orders. Electronic Colloquium on Computa- tional Complexity (ECCC), 17:27, 2010.  Hadley Black, Deeparnab Chakrabarty, and C. Seshadhri. A o(d ) (cid:1) polylog n monotonicity tester for boolean functions over the hypergrid [n]d . In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2133{2151, 2018.  Cl(cid:19)ement L. Canonne. Big data on the rise: Testing monotonicity of distributions. In ICALP,  2015.  Cl(cid:19)ement L. Canonne. A short note on Poisson tail bounds.  2017. URL http: //www.cs.columbia.edu/~ccanonne/files/misc/2017-poissonconcentration.pdf. ccanonne/(cid:12)les/misc/2017- http://www.cs.columbia.edu/ at Available poissonconcentration.pdf.  online  Cl(cid:19)ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt Rubinfeld. Testing shape restrictions of discrete distributions. Theory Comput. Syst., 62(1):4{62, 2018. doi: 10.1007/s00224-017-9785-6. URL https://doi.org/10.1007/s00224-017-9785-6.  Deeparnab Chakrabarty and C. Seshadhri. Optimal bounds for monotonicity and lips- chitz testing over hypercubes and hypergrids. In Symposium on Theory of Computing Conference (STOC), pages 419{428, 2013.  Deeparnab Chakrabarty and C. Seshadhri. An optimal lower bound for monotonicity testing  over hypergrids. Theory of Computing, 10:453{464, 2014. Testing Monotonicity of Distributions  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning k-modal dis- tributions via testing. In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201912, pages 1371{1385, Philadelphia, PA, USA, 2012. Soci- ety for Industrial and Applied Mathematics. URL http://dl.acm.org/citation.cfm? id=2095116.2095224.  Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, and Paul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of the Twenty-fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201913, pages 1833{1852, Philadelphia, PA, USA, 2013. Society for Industrial and Applied Mathematics. ISBN 978-1-611972-51-1. URL http://dl.acm.org/citation.cfm?id= 2627817.2627948.  Yevgeniy Dodis, Oded Goldreich, Eric Lehman, Sofya Raskhodnikova, Dana Ron, and Alex Samorodnitsky. Improved testing algorithms for monotonicity. In Randomization, Ap- proximation, and Combinatorial Algorithms and Techniques, Third International Work- shop on Randomization and Approximation Techniques in Computer Science, and Second International Workshop on Approximation (RANDOM-APPROX), pages 97{108, 1999.  Eldar Fischer, Eric Lehman, Ilan Newman, Sofya Raskhodnikova, Ronitt Rubinfeld, and Alex Samorodnitsky. Monotonicity testing over general poset domains. In Proceedings on 34th Annual ACM Symposium on Theory of Computing (STOC), pages 474{483, 2002.  Alain Ghouila-Houri. Caract(cid:19)erisation des matrices totalement unimodulaires. C. R. Acad.  Sci. Paris, 254:1192{1194, 1962.  Oded Goldreich, Sha(cid:12) Goldwasser, Eric Lehman, and Dana Ron. Testing monotonicity. In 39th Annual Symposium on Foundations of Computer Science (FOCS), pages 426{435, 1998.  Johannes Kraus, Panayot S. Vassilevski, and Ludmil Zikatanov. Polynomial of best uniform approximation to 1/x and smoothing in two-level methods. Comput. Meth. in Appl. Math., 12(4):448{468, 2012. URL http://www.degruyter.com/view/j/cmam.2012.12. issue-4/cmam-2012-0026/cmam-2012-0026.xml.  Eric Lehman and Dana Ron. On disjoint chains of subsets. J. Comb. Theory, Ser. A, 94  (2):399{404, 2001.  Sofya Raskhodnikova, Dana Ron, Amir Shpilka, and Adam D. Smith. Strong lower bounds for approximating distribution support size and the distinct elements problem. SIAM J. Comput., 39(3):813{842, 2009.  Ronitt Rubinfeld and Rocco A. Servedio. Testing monotone high-dimensional distributions. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing, Baltimore, MD, USA, May 22-24, 2005, pages 147{156, 2005. doi: 10.1145/1060590.1060613. URL http://doi.acm.org/10.1145/1060590.1060613.  A.F. Timan. International Series of Monographs in Pure and Applied Mathematics. Number v. 34 in International Series of Monographs in Pure and Applied Mathematics. Pergamon Testing Monotonicity of Distributions  Press; [distributed in the Western Hemisphere by Macmillan, New York], 1963. URL https://books.google.com/books?id=2R-4AAAAIAAJ.  Gregory Valiant and Paul Valiant. Instance optimal learning of discrete distributions. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, (STOC), pages 142{155, 2016.  Gregory Valiant and Paul Valiant. Estimating the unseen: Improved estimators for entropy and other properties. J. ACM, 64(6):37:1{37:41, 2017. doi: 10.1145/3125643. URL http://doi.acm.org/10.1145/3125643.  Paul Valiant. Testing symmetric properties of distributions.  In Proceedings of the 40th  Annual ACM Symposium on Theory of Computing (STOC), pages 383{392, 2008.  Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. IEEE Transactions on Information Theory, 62(6): 3702{3720, 2016a.  Yihong Wu and Pengkun Yang. Chebyshev polynomials, moment matching, and optimal  estimation of the unseen. arXiv preprint arXiv:1504.01227v2, 2016b.  "}, "Testing Mixtures of Discrete Distributions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Testing Mixtures of Discrete Distributions", "abstract": "There has been significant study on the sample complexity of testing properties of distributions over large domains.  For many properties, it is known that the sample complexity can be substantially smaller than the domain size. For example, over a domain of size $n$, distinguishing the uniform distribution from distributions that are far from uniform in $\\ell_1$-distance uses only $O(\\sqrt{n})$ samples.  However, the picture is very different in the presence of arbitrary noise, even when the amount of noise is quite small.  In this case, one must distinguish if samples are coming from a distribution that is $\\epsilon$-close to uniform from the case where the distribution is $(1-\\epsilon)$-far from uniform.  The latter task requires nearly linear in $n$ samples (Valiant, 2008; Valiant and Valiant, 2017a). In this work, we present a noise model that on one hand is more tractable for the testing problem, and on the other hand represents a rich class of noise families.   In our model, the noisy distribution is a mixture of the original distribution and noise, where the latter is known to the tester either explicitly or via sample access; the form of the noise is also known \\emph{a priori}.  Focusing on the identity and closeness testing problems leads to the following mixture testing question:  Given samples of distributions $p, q_1,q_2$, can we test if $p$ is a mixture of $q_1$ and $q_2$?  We consider this general question in various scenarios that differ in terms of how the tester can access the distributions, and show that indeed this problem is more tractable.  Our results  show that the sample complexity of our testers are exactly the same as for the classical non-mixture case.", "pdf_url": "http://proceedings.mlr.press/v99/aliakbarpour19b/aliakbarpour19b.pdf", "keywords": [], "reference": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda T. Suresh. Sublinear algorithms for outlier detection and generalized closeness testing. In IEEE ISIT, pages 3200-3204, 2014.  Jayadev Acharya, Costantinoss Daskalakis, and Gautam Kamath. Optimal testing for properties of  distributions. In NIPS, pages 3591-3599, 2015.  Maryam Aliakbarpour, Eric Blais, and Ronitt Rubinfeld. Learning and testing junta distributions.  In COLT, pages 19-46, 2016.  Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and Anak Yodpinya- In COLT, 2019 (this  nee. Towards testing monotonicity of distributions over general posets. proceedings).  Tugkan Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White.  Testing random variables for independence and identity. In FOCS, pages 442-451, 2001.  Tugkan Batu, Sanjoy Dasgupta, Ravi Kumar, and Ronitt Rubinfeld. The complexity of approximat-  ing entropy. In STOC, pages 678-687, 2002.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and  unimodal distributions. In STOC, pages 381-390, 2004.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing  closeness of discrete distributions. JACM, 60(1):4:1-4:25, 2013.  Avrim Blum and Lunjia Hu. Active tolerant testing. In COLT, pages 474-497, 2018.  Cl\u00b4ement L Canonne. A survey on distribution testing: Your data is big. but is it blue? ECCC, 22:  63, 2015.  Cl\u00b4ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt Rubinfeld. Testing shape  restrictions of discrete distributions. In STACS, pages 25:1-25:14, 2016.  Cl\u00b4ement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing Bayesian  networks. In COLT, pages 370-448, 2017.  Siu-on Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing  closeness of discrete distributions. In SODA, pages 1193-1203, 2014.  13   TESTING MIXTURES OF DISCRETE DISTRIBUTIONS  MA is supported by funds from the MIT-IBM Watson AI Lab (Agreement No. W1771646), the NSF grants IIS-1741137, and CCF-1733808. RR is supported by by funds from the MIT-IBM Watson AI Lab (Agreement No. W1771646) the NSF grants CCF-1650733, CCF-1733808, IIS-1741137 and CCF-1740751.  Acknowledgments  References  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda T. Suresh. Sublinear algorithms for outlier detection and generalized closeness testing. In IEEE ISIT, pages 3200-3204, 2014.  Jayadev Acharya, Costantinoss Daskalakis, and Gautam Kamath. Optimal testing for properties of  distributions. In NIPS, pages 3591-3599, 2015.  Maryam Aliakbarpour, Eric Blais, and Ronitt Rubinfeld. Learning and testing junta distributions.  In COLT, pages 19-46, 2016.  Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and Anak Yodpinya- In COLT, 2019 (this  nee. Towards testing monotonicity of distributions over general posets. proceedings).  Tugkan Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White.  Testing random variables for independence and identity. In FOCS, pages 442-451, 2001.  Tugkan Batu, Sanjoy Dasgupta, Ravi Kumar, and Ronitt Rubinfeld. The complexity of approximat-  ing entropy. In STOC, pages 678-687, 2002.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and  unimodal distributions. In STOC, pages 381-390, 2004.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing  closeness of discrete distributions. JACM, 60(1):4:1-4:25, 2013.  Avrim Blum and Lunjia Hu. Active tolerant testing. In COLT, pages 474-497, 2018.  Cl\u00b4ement L Canonne. A survey on distribution testing: Your data is big. but is it blue? ECCC, 22:  63, 2015.  Cl\u00b4ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt Rubinfeld. Testing shape  restrictions of discrete distributions. In STACS, pages 25:1-25:14, 2016.  Cl\u00b4ement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing Bayesian  networks. In COLT, pages 370-448, 2017.  Siu-on Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing  closeness of discrete distributions. In SODA, pages 1193-1203, 2014. TESTING MIXTURES OF DISCRETE DISTRIBUTIONS  Constantinos Daskalakis and Qinxuan Pan. Square Hellinger subadditivity for bayesian networks  and its applications to identity testing. In COLT, pages 697-703, 2017.  Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, and Paul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In SODA, pages 1833-1852, 2013.  Ilias Diakonikolas and Daniel M. Kane. A new approach for testing properties of discrete distribu-  tions. In FOCS, pages 685-694, 2016.  Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Testing identity of structured distri-  butions. In SODA, pages 1841-1854, 2015a.  Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Optimal algorithms and lower bounds  for testing closeness of structured distributions. In FOCS, pages 1183-1202, 2015b.  Ilias Diakonikolas, Themis Gouleakis, J. Peebles, and Eric Price. Collision-based testers are optimal  for uniformity and closeness. ECCC, 23:178, 2016.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Sharp bounds for generalized uniformity  testing. In NeurIPS, pages 6204-6213, 2018.  Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, Venkatadheeraj Pichapati, Faster algorithms for testing under conditional sampling.  and In  Ananda Theertha Suresh. COLT, pages 607-636, 2015.  Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. In Studies in Com- plexity and Cryptography. Miscellanea on the Interplay between Randomness and Computation, pages 68-75. Springer, 2011.  Oded Goldreich, Shafi Goldwasser, and Dana Ron. Property testing and its connection to learning  and approximation. JACM, 45:653-750, 1998.  Piotr Indyk, Reut Levi, and Ronitt Rubinfeld. Approximating and testing k-histogram distributions  in sub-linear time. In PODS, pages 15-22, 2012.  Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distributions.  Theory of Computing, 9(8):295-347, 2013.  Liam Paninski. A coincidence-based test for uniformity given very sparsely-sampled discrete data.  IEEE TOIT, 54:4750-4755, 2008.  Ronitt Rubinfeld. Taming big probability distributions. XRDS, 19(1):24-28, 2012.  Alistair Stewart, Ilias Diakonikolas, and Cl\u00b4ement L. Canonne. Testing for families of distributions  via the Fourier transform. In NeurIPS, pages 10084-10095, 2018.  Gregory Valiant and Paul Valiant. Estimating the unseen: Improved estimators for entropy and other  properties. JACM, 64(6):37:1-37:41, 2017a. TESTING MIXTURES OF DISCRETE DISTRIBUTIONS  Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity  testing. SICOMP, 46(1):429-455, 2017b.  Paul Valiant. Testing symmetric properties of distributions. In STOC, pages 383-392, 2008.  "}, "Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates of Martingale CLT": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates of Martingale CLT", "abstract": "We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged stochastic gradient descent (SGD) to a normal random vector for a class of twice-differentiable test functions. A crucial intermediate step is proving a non-asymptotic martingale central limit theorem (CLT), i.e., establishing the rates of convergence of a multivariate martingale difference sequence to a normal random vector, which might be of independent interest. We obtain the explicit rates for the multivariate martingale CLT using a combination of Stein?s method and Lindeberg?s argument, which is then used in conjunction with a non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our results have potentially interesting consequences for computing confidence intervals for parameter estimation with SGD and constructing hypothesis tests with SGD that are valid in a non-asymptotic sense", "pdf_url": "http://proceedings.mlr.press/v99/anastasiou19a/anastasiou19a.pdf", "keywords": ["Rates of Convergence for Martingale CLT", "Non-Asymptotic Normality", "Stochastic Gradient Descent", "Stein\u2019s Method"], "reference": "Andreas Anastasiou. Assessing the multivariate normal approximation of the maximum likelihood estimator from high-dimensional, heterogeneous data. Electronic Journal of Statistics, 12:3794- 3828, 2018.  Andrew D Barbour. Stein\u2019s method for diffusion approximations. Probability theory and related  fields, 84(3):297-322, 1990.  Vidmantas Bentkus. On the dependence of the berry-esseen bound on dimension. Journal of  Statistical Planning and Inference, 113(2):385-402, 2003.  Vidmantas Bentkus. A lyapunov-type bound in Rd. Theory of Probability & Its Applications, 49  (2):311-323, 2005.  Erwin Bolthausen. Exact convergence rates in some martingale central limit theorems. The Annals  of Probability, 10(3):672-688, 1982.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in  high dimensions. The Annals of Probability, 45(4):2309-2352, 2017.  Yuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-  gales. Springer Science & Business Media, 2012.  Kai Lai Chung. On a stochastic approximation method. The Annals of Mathematical Statistics,  pages 463-483, 1954.  ton University Press, 1946.  Harald Cram\u00b4er. Mathematical methods of statistics. Princeton Mathematical Series, vol. 9. Prince-  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  John Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. Arxiv Preprint, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 3052-3060, 2015.  Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with dis- cretized diffusions. In Advances in Neural Information Processing Systems 31, pages 9694-9703, 2018.  10   NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  References  Andreas Anastasiou. Assessing the multivariate normal approximation of the maximum likelihood estimator from high-dimensional, heterogeneous data. Electronic Journal of Statistics, 12:3794- 3828, 2018.  Andrew D Barbour. Stein\u2019s method for diffusion approximations. Probability theory and related  fields, 84(3):297-322, 1990.  Vidmantas Bentkus. On the dependence of the berry-esseen bound on dimension. Journal of  Statistical Planning and Inference, 113(2):385-402, 2003.  Vidmantas Bentkus. A lyapunov-type bound in Rd. Theory of Probability & Its Applications, 49  (2):311-323, 2005.  Erwin Bolthausen. Exact convergence rates in some martingale central limit theorems. The Annals  of Probability, 10(3):672-688, 1982.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in  high dimensions. The Annals of Probability, 45(4):2309-2352, 2017.  Yuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-  gales. Springer Science & Business Media, 2012.  Kai Lai Chung. On a stochastic approximation method. The Annals of Mathematical Statistics,  pages 463-483, 1954.  ton University Press, 1946.  Harald Cram\u00b4er. Mathematical methods of statistics. Princeton Mathematical Series, vol. 9. Prince-  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  John Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. Arxiv Preprint, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 3052-3060, 2015.  Murat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with dis- cretized diffusions. In Advances in Neural Information Processing Systems 31, pages 9694-9703, 2018. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  Vaclav Fabian. On asymptotic normality in stochastic approximation. The Annals of Mathematical  Statistics, 39(4):1327-1332, 1968.  Xiequan Fan. Exact rates of convergence in some martingale central limit theorems. Journal of  Mathematical Analysis and Applications, 469(2):1028-1044, 2019.  Yixin Fang, Jinfeng Xu, and Lei Yang. Online bootstrap confidence intervals for the stochastic gradient descent estimator. The Journal of Machine Learning Research, 19(1):3053-3073, 2018.  Robert E Gaunt. Rates of convergence in normal approximation under moment conditions via new bounds on solutions of the stein equation. Journal of Theoretical Probability, 29(1):231-247, 2016.  Charles Geyer. Asymptotics of maximum likelihood without the lln or clt or sample size going to infinity. In Advances in Modern Statistical Theory and Applications: A Festschrift in honor of Morris L. Eaton, pages 1-24. Institute of Mathematical Statistics, 2013.  Alexander Goldenshluger, Anatoli Juditsky, and Arkadi Nemirovski. Hypothesis testing by convex  optimization. Electronic journal of statistics, 9(2):1645-1712, 2015.  Larry Goldstein and Yosef Rinott. Multivariate normal approximations by stein\u2019s method and size  bias couplings. Journal of Applied Probability, 33(1):1-17, 1996.  Jack Gorham, Andrew B Duncan, Sebastian J Vollmer, and Lester Mackey. Measuring sample  quality with diffusions. arXiv preprint arXiv:1611.06972, 2016.  Peter Hall and Christopher C Heyde. Martingale limit theory and its application. Academic press,  2014.  2019.  Anatoli Juditsky and Arkadi Nemirovski. Lectures on statistical inferences via convex optimization.  Arun Kumar Kuchibhotla. Deterministic inequalities for smooth m-estimators. arXiv preprint  arXiv:1809.05172, 2018.  Guanghui Lan, Arkadi Nemirovski, and Alexander Shapiro. Validation analysis of mirror descent  stochastic approximation method. Mathematical programming, 134(2):425-458, 2012.  Tianyang Li, Anastasios Kyrillidis, Liu Liu, and Constantine Caramanis. Approximate newton- based statistical inference using only stochastic gradients. arXiv preprint arXiv:1805.08920, 2018.  Robert Liptser and Albert Nikolaevich Shiryayev. Theory of martingales, volume 49. Springer  Science & Business Media, 2012.  James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna- tional Conference on International Conference on Machine Learning, pages 735-742, 2010.  Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451-459, 2011. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  Jean-Christophe Mourrat. On the rate of convergence in the martingale central limit theorem.  Bernoulli, 19(2):633-645, 2013.  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574- 1609, 2009.  Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  Martin Rai\u02c7c. A multivariate clt for decomposable random vectors with finite second moments.  Journal of Theoretical Probability, 17(3):573-603, 2004.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for  strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.  Gesine Reinert and Adrian R\u00a8ollin. Multivariate normal approximation with steins method of ex- changeable pairs under a general linearity condition. The Annals of Probability, 37(6):2150- 2173, 2009.  Yosef Rinott and V Rotar. Some bounds on the rate of convergence in the clt for martingales. i.  Theory of Probability & Its Applications, 43(4):604-619, 1999.  Adrian R\u00a8ollin. On quantitative bounds in the mean martingale central limit theorem. Statistics &  Probability Letters, 138:171-176, 2018.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical  report, Cornell University Operations Research and Industrial Engineering, 1988.  Jerome Sacks. Asymptotic distribution of stochastic approximation procedures. The Annals of  Mathematical Statistics, 29(2):373-405, 1958.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on stochastic program-  ming: modeling and theory. SIAM, 2009.  Charles Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:  1-164, 1986.  Weijie Su and Yuancheng Zhu. Statistical inference for online learning and stochastic approximation  via hierarchical incremental gradient descent. arXiv preprint arXiv:1802.04876, 2018.  Panos Toulis and Edoardo M Airoldi. Asymptotic and finite-sample properties of estimators based  on stochastic gradients. The Annals of Statistics, 45(4):1694-1727, 2017.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. NORMAL APPROXIMATION FOR SGD AND MARTINGALE CLT  "}, "Adaptively Tracking the Best Bandit Arm with an Unknown Number of Distribution Changes": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Adaptively Tracking the Best Bandit Arm with an Unknown Number of Distribution Changes", "abstract": "We consider the variant of the stochastic multi-armed bandit problem where the stochastic reward distributions may change abruptly several times. In contrast to previous work, we are able to achieve (nearly) optimal mini-max regret bounds without knowing the number of changes. For this setting, we propose an algorithm called ADSWITCH and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. Our regret bound is the first optimal bound for an algorithm that is not tuned with respect to the number of changes.", "pdf_url": "http://proceedings.mlr.press/v99/auer19a/auer19a.pdf", "keywords": ["multi-armed stochastic bandits", "non-stationary rewards", "switching bandits"], "reference": "Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4):267-283, 2017.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3:397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 116-120, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, EWRL 2018, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non- stationary rewards. In Advances in Neural Information Processing Systems 27, NIPS 2014, pages 199-207, 2014.  Lilian Besson and Emilie Kaufmann. What doubling tricks can and can\u2019t do for multi-armed bandits.  CoRR, abs/1803.06971, 2018. URL http://arxiv.org/abs/1803.06971.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  13   ADAPTIVELY TRACKING THE BEST ARM  5. Conclusion  Extending the work in (Auer et al., 2018), we have constructed the first algorithm for the stochastic multi-armed bandit problem with abrupt changes of the reward distributions that achieves optimal regret bounds without knowing the number of changes in advance. The main technical contribution is the delicate testing schedule of the apparently inferior arms. This testing is necessary to detect when a previously inferior arm becomes the best arm.  We note that our algorithm (without any change) also provides optimal regret bounds in terms of total variation. These optimal bounds have also been achieved in (Chen et al., 2019), which provides also optimal bounds for the more general stochastic contextual bandits setting.  Regarding the adversarial bandit setting, it remains an open problem to construct an algorithm  with optimal regret bounds without a priori tuning in respect to the number of arm changes.  This work has been supported by the Austrian Science Fund (FWF): I 3437-N33 in the framework of the CHIST-ERA ERA-NET (DELTA project).  Acknowledgments  References  Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4):267-283, 2017.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3:397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 116-120, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, EWRL 2018, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non- stationary rewards. In Advances in Neural Information Processing Systems 27, NIPS 2014, pages 199-207, 2014.  Lilian Besson and Emilie Kaufmann. What doubling tricks can and can\u2019t do for multi-armed bandits.  CoRR, abs/1803.06971, 2018. URL http://arxiv.org/abs/1803.06971.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. AUER GAJANE ORTNER  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In COLT 2012 - The 25th Annual Conference on Learning Theory, pages 42.1-42.23, 2012.  Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In 32nd Annual Conference on Learn- ing Theory (COLT), 2019.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, pages 1079-1087, 2019.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In Proceedings of the 22nd International Conference on Algorithmic Learning Theory, ALT 2011, pages 174-188. Springer, 2011.  C\u00b4edric Hartland, Sylvain Gelly, Nicolas Baskiotis, Olivier Teytaud, and Mich`ele Sebag. Multi- armed bandit, dynamic environments and meta-bandits. NIPS-2006 workshop, Online trading between exploration and exploitation, 2006.  Levente Kocsis and Csaba Szepesv\u00b4ari. Discounted UCB. 2nd PASCAL Challenges Workshop, 2006.  Dimitris E. Koulouriotis and A.S. Xanthopoulos. Reinforcement learning and evolutionary algo- rithms for non-stationary multi-armed bandit problems. Applied Mathematics and Computation, 196(2):913 - 922, 2008.  Haipeng Luo. Personal communication, 2019.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in non-stationary worlds. In Proceedings of the 31st Conference On Learning Theory, COLT 2018, pages 1739-1776, 2018.  Ronald Ortner, Daniil Ryabko, Peter Auer, and R\u00b4emi Munos. Regret bounds for restless Markov  bandits. Theoretical Computer Science, 558:62-76, 2014.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In Proceedings of the 31st International Conference on Machine Learning, ICML 2014, pages 1287-1295, 2014.  Alex Slivkins and Eli Upfal. Adapting to a changing environment: The Brownian restless bandits.  In 21st Conference on Learning Theory, COLT 2008, pages 343-354, 2008.  Jia Yuan Yu and Shie Mannor. Piecewise-stationary bandit problems with side observations. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, pages 1177-1184, 2009. ADAPTIVELY TRACKING THE BEST ARM  "}, "Achieving Optimal Dynamic Regret for Non-stationary Bandits without Prior Information": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Achieving Optimal Dynamic Regret for Non-stationary Bandits without Prior Information", "abstract": "This joint extended abstract introduces and compares the results of (Auer et al., 2019) and (Chen et al., 2019), both of which resolve the problem of achieving optimal dynamic regret for non-stationary bandits without prior information on the non-stationarity. Specifically, Auer et al. (2019) resolve the problem for the traditional multi-armed bandits setting, while Chen et al. (2019) give a solution for the more general contextual bandits setting. Both works extend the key idea of (Auer et al., 2018) developed for a simpler two-armed setting.", "pdf_url": "http://proceedings.mlr.press/v99/auer19b/auer19b.pdf", "keywords": [], "reference": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an un- known number of distribution changes. In 32nd Annual Conference on Learning Theory (COLT), 2019.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In 32nd Annual Conference on Learn- ing Theory (COLT), 2019.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019. Full version available at https://ssrn.com/abstract=3261050.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in Neural Information Processing Systems 21, 2008.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in  non-stationary worlds. In 31st Annual Conference on Learning Theory (COLT), 2018.  5   OPTIMAL DYNAMIC REGRET FOR BANDITS WITHOUT PRIOR INFORMATION  Acknowledgments. We thank Dylan Foster, Akshay Krishnamurthy, and Ruihao Zhu for in-depth discussions on the Bandit-over-Bandits approach of (Cheung et al., 2019). The work of (Auer et al., 2019) has been supported by the Austrian Science Fund (FWF): I 3437-N33 in the framework of the CHIST-ERA ERA-NET (DELTA project). The work of (Chen et al., 2019) is supported by NSF Grant #1755781.  References  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an un- known number of distribution changes. In 32nd Annual Conference on Learning Theory (COLT), 2019.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In 32nd Annual Conference on Learn- ing Theory (COLT), 2019.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019. Full version available at https://ssrn.com/abstract=3261050.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in Neural Information Processing Systems 21, 2008.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in  non-stationary worlds. In 31st Annual Conference on Learning Theory (COLT), 2018. "}, "A Universal Algorithm for Variational Inequalities Adaptive to Smoothness and Noise": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A Universal Algorithm for Variational Inequalities Adaptive to Smoothness and Noise", "abstract": "We consider variational inequalities coming from monotone operators, a setting that includes convex minimization and convex-concave saddle-point problems.  We assume an access to potentially noisy unbiased values of the monotone operators and assess convergence through a compatible gap function which corresponds to the standard optimality criteria in the aforementioned subcases. We present a  universal algorithm for these inequalities based on the Mirror-Prox algorithm. Concretely, our algorithm \\emph{simultaneously} achieves the optimal rates for the smooth/non-smooth, and noisy/noiseless settings. This is done without any prior knowledge of these properties, and in the  general set-up of arbitrary norms and compatible Bregman divergences. For convex minimization and convex-concave saddle-point problems, this leads to new adaptive algorithms. Our method relies on a novel yet simple adaptive choice of the step-size, which can be seen as  the appropriate  extension of AdaGrad to  handle constrained problems.", "pdf_url": "http://proceedings.mlr.press/v99/bach19a/bach19a.pdf", "keywords": ["Online learning", "convex optimization", "first-order methods", "universal methods", "minimax games"], "reference": "Heinz H. Bauschke and Patrick L. Combettes. Convex analysis and monotone operator theory in  Hilbert spaces, volume 408. Springer, 2011.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, pages 6-1, 2012.  Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 235-254, 2011.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Pavel Dvurechensky, Alexander Gasnikov, Fedor Stonyakin, and Alexander Titov. Generalized mir- ror prox: Solving variational inequalities with monotone operator, inexact oracle, and unknown H\u00a8older parameters. Technical Report 1806.05140, arXiv, 2018.  Yoav Freund, Robert E Schapire, et al. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999.  Anatoli Juditsky and Arkadi Nemirovski. First order methods for nonsmooth convex large-scale optimization, ii: utilizing problems structure. Optimization for Machine Learning, pages 149- 183, 2011.  Anatoli Juditsky and Arkadi Nemirovski. Solving variational inequalities with monotone operators on domains given by linear minimization oracles. Mathematical Programming, 156(1-2):221- 256, 2016.  Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with  stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.  Anatoli Juditsky, Fatma K\u0131l\u0131nc\u00b8 Karzan, and Arkadi Nemirovski. Randomized first order algorithms with applications to 1-minimization. Mathematical Programming, 142(1-2):269-310, 2013.  Anatoli B Juditsky and Arkadi S Nemirovski. Large deviations of vector-valued martingales in  2-smooth normed spaces. arXiv preprint arXiv:0809.0813, 2008.  Sham Kakade. Lecture notes in multivariate analysis, dimensionality reduction, and spectral meth- ods. http://stat.wharton.upenn.edu/\u02dcskakade/courses/stat991_mult/ lectures/MatrixConcen.pdf, April 2010.  G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Mate-  con, 12:747-756, 1976.  G. M. Korpelevich. Extrapolational gradient methods and their connection with modified la-  grangians. Ehkon. Mat. Metody, 19:694-703, 1983.  13   A UNIVERSAL ALGORITHM FOR VARIATIONAL INEQUALITIES  References  Heinz H. Bauschke and Patrick L. Combettes. Convex analysis and monotone operator theory in  Hilbert spaces, volume 408. Springer, 2011.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, pages 6-1, 2012.  Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 235-254, 2011.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Pavel Dvurechensky, Alexander Gasnikov, Fedor Stonyakin, and Alexander Titov. Generalized mir- ror prox: Solving variational inequalities with monotone operator, inexact oracle, and unknown H\u00a8older parameters. Technical Report 1806.05140, arXiv, 2018.  Yoav Freund, Robert E Schapire, et al. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999.  Anatoli Juditsky and Arkadi Nemirovski. First order methods for nonsmooth convex large-scale optimization, ii: utilizing problems structure. Optimization for Machine Learning, pages 149- 183, 2011.  Anatoli Juditsky and Arkadi Nemirovski. Solving variational inequalities with monotone operators on domains given by linear minimization oracles. Mathematical Programming, 156(1-2):221- 256, 2016.  Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with  stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.  Anatoli Juditsky, Fatma K\u0131l\u0131nc\u00b8 Karzan, and Arkadi Nemirovski. Randomized first order algorithms with applications to 1-minimization. Mathematical Programming, 142(1-2):269-310, 2013.  Anatoli B Juditsky and Arkadi S Nemirovski. Large deviations of vector-valued martingales in  2-smooth normed spaces. arXiv preprint arXiv:0809.0813, 2008.  Sham Kakade. Lecture notes in multivariate analysis, dimensionality reduction, and spectral meth- ods. http://stat.wharton.upenn.edu/\u02dcskakade/courses/stat991_mult/ lectures/MatrixConcen.pdf, April 2010.  G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Mate-  con, 12:747-756, 1976.  G. M. Korpelevich. Extrapolational gradient methods and their connection with modified la-  grangians. Ehkon. Mat. Metody, 19:694-703, 1983. A UNIVERSAL ALGORITHM FOR VARIATIONAL INEQUALITIES  Kfir Levy. Online to of\ufb02ine conversions, universality and adaptive minibatch sizes. In Advances in  Neural Information Processing Systems, pages 1612-1621, 2017.  Yehuda Kfir Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and acceleration. In Advances in Neural Information Processing Systems, pages 6501-6510, 2018.  H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-  mization. COLT 2010, page 244, 2010.  Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229-251, 2004.  Arkadii Nemirovskii and David Borisovich Yudin. Problem complexity and method efficiency in  optimization. 1983.  ity, 8(2):153-175, 1992.  A. S. Nemirovsky. Information-based complexity of linear operator equations. Journal of Complex-  Yurii Nesterov. Universal gradient methods for convex optimization problems. Mathematical Pro-  gramming, 152(1-2):381-404, 2015.  Muhammad Aslam Noor. New extragradient-type methods for general variational inequalities. Jour-  nal of Mathematical Analysis and Applications, 277(2):379-394, 2003.  Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-  quences. In Advances in Neural Information Processing Systems, pages 3066-3074, 2013.  Alp Yurtsever, Quoc Tran Dinh, and Volkan Cevher. A universal primal-dual convex optimization framework. In Advances in Neural Information Processing Systems, pages 3150-3158, 2015. A UNIVERSAL ALGORITHM FOR VARIATIONAL INEQUALITIES  "}, "Learning Two Layer Rectified Neural Networks in Polynomial Time": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "abstract": "We consider the following fundamental problem in the study of neural networks: given input examples $x \\in \\mathbb{R}^d$ and their vector-valued labels, as defined by an underlying generative neural network, recover the weight matrices of this network. We consider two-layer networks, mapping $\\mathbb{R}^d$ to $\\mathbb{R}^m$, with a single hidden layer and $k$ non-linear activation units $f(\\cdot)$, where $f(x) = \\max \\{x , 0\\}$ is the ReLU activation function. Such a network is specified by two weight matrices, $\\mathbf{U}^* \\in \\mathbb{R}^{m \\times k}, \\mathbf{V}^* \\in \\mathbb{R}^{k \\times d}$, such that the label of an example $x \\in \\mathbb{R}^{d}$ is given by $\\mathbf{U}^* f(\\mathbf{V}^* x)$, where $f(\\cdot)$ is applied coordinate-wise. Given $n$ samples $x^1,\u2026,x^n \\in \\mathbb{R}^d$ as a matrix $\\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and the label $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X})$ of the network on these samples, our goal is to recover the weight matrices $\\mathbf{U}^*$ and $\\mathbf{V}^*$. More generally, our labels $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X})$  may be corrupted by noise, and instead we observe $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X}) + \\mathbf{E}$ where $\\mathbf{E}$ is some noise matrix. Even in this case, we may still be interested in recovering good approximations to the weight matrices $\\mathbf{U}^*$ and $\\mathbf{V}^*$.  In this work, we develop algorithms and hardness results under varying assumptions on the input and noise. Although the problem is NP-hard even for $k=2$, by assuming Gaussian marginals over the input $\\mathbf{X}$ we are able to develop polynomial time algorithms for the approximate recovery of $\\mathbf{U}^*$ and $\\mathbf{V}^*$. Perhaps surprisingly, in the noiseless case our algorithms recover $\\mathbf{U}^*,\\mathbf{V}^*$ \\textit{exactly}, i.e. with no error, in \\textit{strongly} polynomial time. To the best of the our knowledge, this is the first algorithm to accomplish exact recovery for the ReLU activation function. For the noisy case, we give the first polynomial time algorithm that approximately recovers the weights in the presence of mean-zero noise $\\mathbf{E}$. Our algorithms generalize to a larger class of \\textit{rectified} activation functions, $f(x) = 0$ when $x\\leq 0$, and $f(x) > 0$ otherwise. Although our polynomial time results require $\\mathbf{U}^*$ to have full column rank, we also give a fixed-parameter tractable algorithm (in $k$) when $\\mathbf{U}^*$ does not have this property. Lastly, we give a  fixed-parameter tractable algorithm for more arbitrary noise matrices $\\mathbf{E}$, so long as they are independent of $\\mathbf{X}$.", "pdf_url": "http://proceedings.mlr.press/v99/bakshi19a/bakshi19a.pdf", "keywords": ["neural networks", "learning", "polynomial time", "ReLU", "computational linear algebra"], "reference": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural  networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with unknown gaus- In Advances in Neural  sian noise, with implications for gaussian mixtures and autoencoders. Information Processing Systems, pages 2375-2383, 2012.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1057-1066. ACM, 2017.  Robert B. Ash. Lecture notes 21-25 in statistics, finding the density. https://faculty.math.  illinois.edu/\u02dcr-ash/Stat/StatLec21-25.pdf.  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy.  In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analy- sis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594-603. ACM, 2014.  Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural  Networks, 5(1):117-127, 1992.  Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.  arXiv preprint arXiv:1809.10787, 2018.  13   The authors thank the partial support by the National Science Foundation under Grant No. CCF- 1815840. Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing. The authors would also like to thank Anima Anandkumar, Mark Bun, Rong Ge, Sam Hopkins and Rina Panigrahy for useful discussions.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural  networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with unknown gaus- In Advances in Neural  sian noise, with implications for gaussian mixtures and autoencoders. Information Processing Systems, pages 2375-2383, 2012.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1057-1066. ACM, 2017.  Robert B. Ash. Lecture notes 21-25 in statistics, finding the density. https://faculty.math.  illinois.edu/\u02dcr-ash/Stat/StatLec21-25.pdf.  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy.  In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analy- sis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594-603. ACM, 2014.  Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural  Networks, 5(1):117-127, 1992.  Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.  arXiv preprint arXiv:1809.10787, 2018. Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian  inputs. arXiv preprint arXiv:1702.07966, 2017.  Wlodzimierz Bryc. The normal distribution: characterizations with applications, volume 100.  Springer Science & Business Media, 2012.  Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling. Inverse  problems, 23(3):969, 2007.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Michael B Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and balancing via box constrained newton\u2019s method and interior point methods. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 902-913. IEEE, 2017.  Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287-314,  1994.  Keith Conrad. Expository papers: Universal identities. http://www.math.uconn.edu/  \u02dckconrad/blurbs/linmultialg/univid.pdf.  A. DasGupta. Asymptotic Theory of Statistics and Probability. Springer Texts in Statistics. Springer New York, 2008. ISBN 9780387759708. URL https://books.google.com/books? id=9ByccYe5aI4C.  Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep  learning. In Advances in neural information processing systems, pages 2148-2156, 2013.  Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in neural informa- tion processing systems, pages 1269-1277, 2014.  Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks  with overlaps. arXiv preprint arXiv:1805.07798, 2018.  Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global  minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.  Alan Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In Foundations of Computer Science, 1996. Proceedings., 37th Annual Symposium on, pages 359-368. IEEE, 1996.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank  approximations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  Rong Ge. Personal communication. October, 2018. Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares  algorithms. arXiv preprint arXiv:1504.05287, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with  symmetric inputs. arXiv preprint arXiv:1810.06793, 2018.  Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv  preprint arXiv:1709.06010, 2017.  Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-  mial time. arXiv preprint arXiv:1611.10258, 2016.  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping  patches. arXiv preprint arXiv:1802.02547, 2018.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decomposition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 584-593. ACM, 2014.  David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions  on Information Theory, 57(3):1548-1566, 2011.  Allan Gut. An intermediate course in probability. chapter 5. Springer Publishing Company, Incor-  porated, 2009.  Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 651-660. IEEE, 2014.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and In Proceedings of the 4th conference on Innovations in Theoretical  spectral decompositions. Computer Science, pages 11-20. ACM, 2013.  Aapo Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE  transactions on Neural Networks, 10(3):626-634, 1999.  Aapo Hyv\u00a8arinen and Erkki Oja.  Independent component analysis: algorithms and applications.  Neural networks, 13(4-5):411-430, 2000.  Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream compu-  tation. Journal of the ACM (JACM), 53(3):307-323, 2006. Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alter- In Proceedings of the forty-fifth annual ACM symposium on Theory of  nating minimization. computing, pages 665-674. ACM, 2013.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Score function features for discriminative  learning: Matrix and tensor framework. arXiv preprint arXiv:1412.2863, 2014.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  J Stephen Judd. Neural network design and the complexity of learning. Technical report, CALI-  FORNIA INST OF TECH PASADENA DEPT OF COMPUTER SCIENCE, 1988.  Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized In Advances in Neural Information  linear and single index models with isotonic regression. Processing Systems, pages 927-935, 2011.  Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically  learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE transactions on information theory, 56(6):2980-2998, 2010.  B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann. Statist., 28(5):1302-1338, 10 2000. doi: 10.1214/aos/1015957395. URL https://doi.org/ 10.1214/aos/1015957395.  Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its impli- cations for combinatorial and convex optimization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1049-1065. IEEE, 2015.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 597-607. Curran Associates, Inc., 2017a. URL http://papers.nips.cc/paper/ 6662-convergence-analysis-of-two-layer-neural-networks-with-relu-activation. pdf.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.  In Advances in Neural Information Processing Systems, pages 597-607, 2017b.  Yi-Kai Liu, Animashree Anandkumar, Dean P Foster, Daniel Hsu, and Sham M Kakade. Two svds su ffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. In Neural Information Processing Systems (NIPS), 2012.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014. Michael W Mahoney et al. Randomized algorithms for matrices and data. Foundations and Trends  in Machine Learning, 3(2):123-224, 2011.  Pasin Manurangsi and Daniel Reichman. The computational complexity of training relu(s). arXiv  preprint arXiv:1810.04207, 2018.  etry, 3(4):325-337, 1988.  Nimrod Megiddo. On the complexity of polyhedral separability. Discrete & Computational Geom-  Lingsheng Meng and Bing Zheng. The optimal perturbation bounds of the moorepenrose inverse un- der the frobenius norm. Linear Algebra and its Applications, 432(4):956 - 963, 2010. ISSN 0024- 3795. doi: https://doi.org/10.1016/j.laa.2009.10.009. URL http://www.sciencedirect. com/science/article/pii/S0024379509005230.  Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural  networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.  Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, 2013.  Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu- lar values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pages 1576- 1602. World Scientific, 2010.  Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 143-152. IEEE, 2006.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223-1231, 2016.  Mahdi Soltanolkotabi. Learning relus via gradient descent.  In Advances in Neural Information  Processing Systems, pages 2007-2017, 2017.  Zhao Song, David Woodruff, and Huan Zhang. Sublinear time orthogonal tensor decomposition. In  Advances in Neural Information Processing Systems, pages 793-801, 2016.  Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees  for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.  Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In  Conference on Learning Theory, pages 37-1, 2012.  Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017a. Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks  with relu nonlinearity. 2017b.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training one- hidden-layer neural networks. CoRR, abs/1805.02677, 2018. URL http://arxiv.org/ abs/1805.02677.  Santosh S Vempala. Learning convex concepts from gaussian distributions with pca. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 124-130. IEEE, 2010.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Roman Vershynin. High-dimensional probability: An introduction with applications in data sci-  ence, volume 47. Cambridge University Press, 2018.  M.J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Se- ISBN  ries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. 9781108498029. URL https://books.google.com/books?id=8C8nuQEACAAJ.  Yining Wang and Anima Anandkumar. Online and differentially-private tensor decomposition. In  Advances in Neural Information Processing Systems, pages 3531-3539, 2016.  David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends  in Theoretical Computer Science, 10(1-2):1-157, 2014.  Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly In International Conference on Machine Learning, pages 993-  learnable in polynomial time. 1001, 2016.  Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees  for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.  Roadmap  In Section A we introduce our nO(k) time exact algorithm when rank(A) = k and arbitrary X, for recovery of rank-k matrices U\u2217, V\u2217 such that U\u2217f (V\u2217X) = A. In this section, we also demon- strate that for a very wide class of distributions for random matices X, the matrix U\u2217f (V\u2217X) is in fact full rank with high probability, and therefore can be solved with our exact algorithm. Then, in Section B, we prove NP-hardness of the learning problem when rank(A) < k. Next, in Section C, we give a polynomial time algorithm for exact recovery of U\u2217, V\u2217 in the case when X has Gaus- sian marginals in the realizable setting. Section C.1 develops our Independenct Component Analysis Based algorithm, whereas Section C.2 develops our more general exact recovery algorithm. In Sec- tion C.3, we show how recent concurrent results can be bootstrapped via our technqiues to obtain exact recovery for a wider class of distributions.  In Section D, we demonstrate how to extend our algorithm to the case where A = U\u2217f (V\u2217X)+ E where E is mean 0 i.i.d. sub-Gaussian noise. Then in Section E, we give a fixed-paramater tractable (FPT) (in k and \u03ba(V\u2217)) for the exact recovery of U\u2217, V\u2217 in the case where U\u2217 does not have full column rank. We give our second FPT algorithm in Section F, which finds weights which approximate the optimal network for arbitrary error matrices E that are independent of X. In Section G, we demonstrate how the weights of certain low-rank networks, where k < d, m, can be recovered exactly in the presence of a class of arbitrary sparse noise in polynomial time. Finally, in "}, "Private Center Points and Learning of Halfspaces": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Private Center Points and Learning of Halfspaces", "abstract": "We present a private agnostic learner for halfspaces over an arbitrary finite domain $X\\subset \\R^d$ with sample complexity $\\mathsf{poly}(d,2^{\\log^*|X|})$. The building block for this learner is a differentially private algorithm for locating an approximate center point of $m>\\mathsf{poly}(d,2^{\\log^*|X|})$ points \u2013 a high dimensional generalization of the median function. Our construction establishes a relationship between these two problems that is reminiscent of the relation between the median and learning one-dimensional thresholds [Bun et al. FOCS \u201915]. This relationship suggests that the problem of privately locating a center point may have further applications in the design of differentially private algorithms. We also provide a lower bound on the sample complexity for privately finding a point in the convex hull. For approximate  differential privacy, we show a lower bound of $m=\\Omega(d+\\log^*|X|)$, whereas for pure differential privacy $m=\\Omega(d\\log|X|)$.", "pdf_url": "http://proceedings.mlr.press/v99/beimel19a/beimel19a.pdf", "keywords": ["Differential privacy", "Private PAC learning", "Halfspaces", "Quasi-concave functions"], "reference": "Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies fi- nite littlestone dimension. CoRR, abs/1806.00949, 2018. URL http://arxiv.org/abs/ 1806.00949. To appear at STOC\u201919.  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In FOCS, pages 464-473, 2014. URL http://dx.doi. org/10.1109/FOCS.2014.56.  Raef Bassily, Abhradeep Guha Thakurta, and Om Dipakbhai Thakkar. Model-agnostic private In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o learning. Cesa-Bianchi, and Roman Garnett, editors, NeurIPS, pages 7102-7112, 2018. URL http: //papers.nips.cc/paper/7941-model-agnostic-private-learning.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private  learners. In ITCS, pages 97-110. ACM, 2013a.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approx- imate differential privacy. In APPROX-RANDOM, volume 8096 of Lecture Notes in Computer  12   PRIVATE CENTER POINTS AND LEARNING OF HALFSPACES  \u03b4 + log\u2217 T ).  By Bun et al. (2015), finding a point in the convex hull (even for d = 1) requires sam- ple complexity \u2126(log\u2217 T ). Thus, together we get a lower bound on the sample complexity of \u2126( d  \u03b5 log 1 It may be tempting to guess that, even with pure (\u03b5, 0)-differential privacy, a sample complexity of O(log |X|) = O(d log T ) should suffice for solving this task, as the size of the output space is T d, because S \u2286 [T ]d, and hence (it seems) that one could privately solve this problem using the exponential mechanism of McSherry and Talwar (2007) with sample complexity that depends logarithmically on the size of the output space. We show that this is not the case, and that any (\u03b5, 0)-differentially private algorithm for this task must have sample complexity \u2126( d2  \u03b5 log T ).  Theorem 24 Let T \u2265 2, and d \u2265 10. Let A be an (\u03b5, \u03b4)-differentially private algorithm that takes a database S \u2286 [T ]d of size m and returns, with probability at least 1/2, a non-trivial linear combination of the points in S. Then, m = \u2126 min  (cid:110) d2  (cid:111)(cid:17)  (cid:16)  .  \u03b5 log T, d  \u03b5 log 1  \u03b4  The proof of Theorem 24 builds on the analysis of Blum et al. (2008) for lower bounding the sample complexity of releasing approximate answers for counting queries. See the full version of this work for more details.  Acknowledgments  A. B. and K. N. were supported by NSF grant no. 1565387. TWC: Large: Collaborative: Computing Over Distributed Sensitive Data. A. B. was supported by ISF grant no. 152/17. Work done while A. B. was visiting Georgetown University. S. M. was supported by the Simons Foundation and the NSF; part of this project was carried while Shay was at the Institute for Advanced Study and was supported by the National Science Foundation under agreement No. CCF-1412958. U. S. was supported by a gift from Google Ltd.  References  Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies fi- nite littlestone dimension. CoRR, abs/1806.00949, 2018. URL http://arxiv.org/abs/ 1806.00949. To appear at STOC\u201919.  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In FOCS, pages 464-473, 2014. URL http://dx.doi. org/10.1109/FOCS.2014.56.  Raef Bassily, Abhradeep Guha Thakurta, and Om Dipakbhai Thakkar. Model-agnostic private In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o learning. Cesa-Bianchi, and Roman Garnett, editors, NeurIPS, pages 7102-7112, 2018. URL http: //papers.nips.cc/paper/7941-model-agnostic-private-learning.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private  learners. In ITCS, pages 97-110. ACM, 2013a.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approx- imate differential privacy. In APPROX-RANDOM, volume 8096 of Lecture Notes in Computer PRIVATE CENTER POINTS AND LEARNING OF HALFSPACES  Science, pages 363-378. Springer, 2013b. Journal version:Theory of Computing, 12(1):1-61, 2016.  Amos Beimel, Hai Brenner, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. Machine Learning, 94(3):401- 437, 2014.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled  examples. In SODA, pages 461-477. SIAM, 2015.  Shai Ben-David and Ami Litman. Combinatorial variability of vapnik-chervonenkis classes with applications to sample compression schemes. Discrete Applied Mathematics, 86(1):3-25, 1998. URL https://doi.org/10.1016/S0166-218X(98)00000-6.  Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: The SuLQ  framework. In Chen Li, editor, PODS, pages 128-138. ACM, 2005.  Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to non-interactive  database privacy. In STOC, pages 609-618. ACM, 2008.  Mark Bun. New Separations in the Complexity of Differential Privacy. PhD thesis, Harvard Uni-  versity, 2016. Supervisor-Salil Vadhan.  Mark Bun and Mark Zhandry. Order-revealing encryption and the hardness of private learning. In Eyal Kushilevitz and Tal Malkin, editors, TCC 2016-A, volume 9562 of Lecture Notes in Computer Science, pages 176-206. Springer, 2016. URL https://doi.org/10.1007/ 978-3-662-49096-9_8.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and  learning of threshold functions. In FOCS, pages 634-649, 2015.  Mark Bun, Kobbi Nissim, and Uri Stemmer. Simultaneous private learning of multiple concepts. In  ITCS, pages 369-380. ACM, 2016.  Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning. In Sham M. Kakade and Ulrike von Luxburg, editors, COLT, volume 19 of JMLR Proceedings, pages 155-186. JMLR.org, 2011.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. Journal of Machine Learning Research, 12:1069-1109, 2011.  John Dunagan and Santosh Vempala. A simple polynomial-time rescaling algorithm for solving linear programs. Mathematical Programming, 114(1):101-114, Jul 2008. ISSN 1436-4646.  Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In COLT 2018, pages 1693-  1702, 2018. URL http://proceedings.mlr.press/v75/dwork18a.html.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity In TCC, volume 3876 of Lecture Notes in Computer Science, pages  in private data analysis. 265-284. Springer, 2006. PRIVATE CENTER POINTS AND LEARNING OF HALFSPACES  Herbert Edelsbrunner. Algorithms in Combinatorial Geometry. Springer-Verlag, Berlin, Heidelberg,  1987. ISBN 0-387-13722-X.  Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. SIAM J. Comput., 44(6):1740-1764, 2015. URL http://dx. doi.org/10.1137/140991844.  Justin Hsu, Aaron Roth, Tim Roughgarden, and Jonathan Ullman.  Privately solving lin- In ICALP, pages 612-624, 2014. URL https://doi.org/10.1007/  ear programs. 978-3-662-43948-7_51.  Haim Kaplan, Yishay Mansour, Yossi Matias, and Uri Stemmer. Differentially private learning of geometric concepts. CoRR, abs/1902.05017, 2019. URL http://arxiv.org/abs/1902. 05017.  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, 2011. URL https: //doi.org/10.1137/090756090.  Michael J. Kearns. Efficient noise-tolerant learning from statistical queries. J. ACM, 45(6):983-  1006, 1998.  Xiaohui Liu, Karl Mosler, and Pavlo Mozharovskyi. Fast computation of Tukey trimmed regions  and median in dimension p > 2. arXiv e-prints, arXiv:1412.5122:arXiv:1412.5122, 2014.  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS, pages  94-103. IEEE Computer Society, 2007.  Gary L. Miller and Donald R. Sheehy. Approximate centerpoints with proofs. Comput. Geom., 43 (8):647-654, 2010. URL https://doi.org/10.1016/j.comgeo.2010.04.006.  Huy L. Nguyen, Jonathan Ullman, and Lydia Zakynthinou. Efficient private algorithms for learning halfspaces. CoRR, abs/1902.09009, 2019. URL http://arxiv.org/abs/1902.09009.  John W. Tukey. Mathematics and the picturing of data. In Proc. Int. Congress of Mathematicians,  volume 2, pages 523-532, 1975.  and Winston, 1961.  Isaac Moiseevich Yaglom and Vladimir Grigorevich Boltyanski\u02c7\u0131. Convex figures. Holt, Rinehart "}, "Lower bounds for testing graphical models: colorings and antiferromagnetic Ising models": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Lower bounds for testing graphical models: colorings and antiferromagnetic Ising models", "abstract": "We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model $M$ and a sampling oracle for the  distribution $\\mu_{M^*}$ of an unknown model $M^*$, can we efficiently determine if the two models $M$ and $M^*$ are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the \\emph{Ising model} and \\emph{proper colorings}, and explore whether identity testing is easier than structure learning. For the ferromagnetic (attractive) Ising model, Daskalasis et al. (2018) presented a polynomial time algorithm for identity testing. We prove hardness results in the antiferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. Specifically, for $n$-vertex graphs of maximum degree $d$, we prove that if $|\\beta| d = \\omega(\\log{n})$ (where $\\beta$ is the inverse temperature parameter), then there is no identity testing algorithm  for the antiferromagnetic Ising model  that runs in polynomial time unless $RP\\!=\\!NP$. We also establish computational lower bounds for a broader set of parameters under the (randomized) exponential time hypothesis. In our proofs, we  use random graphs as gadgets; this is inspired by similar constructions in seminal works on the hardness of approximate\u00a0counting. In the hard-constraint setting, we present hardness results  for identity testing for proper colorings.   Our results are based on the presumed hardness of \\textsc{#BIS}, the problem of (approximately) counting independent sets in bipartite graphs. In particular, we prove that identity testing for colorings is hard in the same range of parameters  where structure learning is known to be hard, which in turn matches the parameter regime for  NP-hardness of the corresponding decision problem.", "pdf_url": "http://proceedings.mlr.press/v99/bezakova19a/bezakova19a.pdf", "keywords": ["distribution testing", "structure learning", "graphical models", "Ising model", "colorings"], "reference": "[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A Learning Algorithm for Boltzmann Ma-  chines. Cognitive Science, 9(1):147-169, 1985.  [2] A. Anandkumar, D.J. Hsu, F. Huang, and S.M. Kakade. Learning mixtures of tree graphical models. In Advances in Neural Information Processing Systems (NeurIPS), pages 1052-1060, 2012.  [3] T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White. Testing random variables for independence and identity. In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 442-451, 2001.  [4] I. Bez\u00b4akov\u00b4a, A. Blanca, Z. Chen, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. Lower bounds for testing graphical models: colorings and antiferromagnetic Ising models. ArXiv preprint ArXiv:1901.07361, 2019.  [5] A. Blanca, Z. Chen, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. Structure Learning of H-colorings. In Proceedings of the 29th International Conference on Algorithmic Learning Theory (ALT), volume 83, pages 152-185, 2018.  [6] G. Bresler. Efficiently learning Ising models on arbitrary graphs. In Proceedings of the 47th  Annual ACM Symposium on Theory of Computing (STOC), pages 771-782, 2015.  [7] G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random fields from samples: some observations and algorithms. SIAM Journal on Computing, 42(2):563-578, 2013.  [8] G. Bresler, D. Gamarnik, and D. Shah. Structure learning of antiferromagnetic Ising models. In Advances in Neural Information Processing Systems (NeurIPS), pages 2852-2860, 2014.  [9] G. Bresler, D. Gamarnik, and D. Shah. Hardness of parameter estimation in graphical models. In Advances in Neural Information Processing Systems (NeurIPS), pages 1062-1070, 2014.  [10] G. Brito, I. Dumitriu, and K.D. Harris. Spectral gap in random bipartite biregular graphs and  its applications. ArXiv preprint ArXiv:1804.07808, 2018.  [11] A.A. Bulatov, M. Dyer, L.A. Goldberg, M. Jerrum, and C. McQuillan. The expressibility of functions on the Boolean domain, with applications to Counting CSPs. Journal of the ACM (JACM), 60(5):32, 2013.  [12] J.-Y. Cai, A. Galanis, L.A. Goldberg, H. Guo, M. Jerrum, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. #BIS-hardness for 2-spin systems on bipartite bounded degree graphs in the tree non- uniqueness region. Journal of Computer and System Sciences, 82(5):690-711, 2016.  [13] C. Calabro, R. Impagliazzo, V. Kabanets, and R. Paturi. The complexity of Unique k-SAT: An Isolation Lemma for k-CNFs. Journal of Computer and System Sciences, 74(3):386-393, 2008.  13   LOWER BOUNDS FOR TESTING GRAPHICAL MODELS  Research supported by NSF grants 1819546, CCF-1617306, CCF-1563838 and CCF-1563757.  Acknowledgments  References  [1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A Learning Algorithm for Boltzmann Ma-  chines. Cognitive Science, 9(1):147-169, 1985.  [2] A. Anandkumar, D.J. Hsu, F. Huang, and S.M. Kakade. Learning mixtures of tree graphical models. In Advances in Neural Information Processing Systems (NeurIPS), pages 1052-1060, 2012.  [3] T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White. Testing random variables for independence and identity. In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 442-451, 2001.  [4] I. Bez\u00b4akov\u00b4a, A. Blanca, Z. Chen, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. Lower bounds for testing graphical models: colorings and antiferromagnetic Ising models. ArXiv preprint ArXiv:1901.07361, 2019.  [5] A. Blanca, Z. Chen, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. Structure Learning of H-colorings. In Proceedings of the 29th International Conference on Algorithmic Learning Theory (ALT), volume 83, pages 152-185, 2018.  [6] G. Bresler. Efficiently learning Ising models on arbitrary graphs. In Proceedings of the 47th  Annual ACM Symposium on Theory of Computing (STOC), pages 771-782, 2015.  [7] G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random fields from samples: some observations and algorithms. SIAM Journal on Computing, 42(2):563-578, 2013.  [8] G. Bresler, D. Gamarnik, and D. Shah. Structure learning of antiferromagnetic Ising models. In Advances in Neural Information Processing Systems (NeurIPS), pages 2852-2860, 2014.  [9] G. Bresler, D. Gamarnik, and D. Shah. Hardness of parameter estimation in graphical models. In Advances in Neural Information Processing Systems (NeurIPS), pages 1062-1070, 2014.  [10] G. Brito, I. Dumitriu, and K.D. Harris. Spectral gap in random bipartite biregular graphs and  its applications. ArXiv preprint ArXiv:1804.07808, 2018.  [11] A.A. Bulatov, M. Dyer, L.A. Goldberg, M. Jerrum, and C. McQuillan. The expressibility of functions on the Boolean domain, with applications to Counting CSPs. Journal of the ACM (JACM), 60(5):32, 2013.  [12] J.-Y. Cai, A. Galanis, L.A. Goldberg, H. Guo, M. Jerrum, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. #BIS-hardness for 2-spin systems on bipartite bounded degree graphs in the tree non- uniqueness region. Journal of Computer and System Sciences, 82(5):690-711, 2016.  [13] C. Calabro, R. Impagliazzo, V. Kabanets, and R. Paturi. The complexity of Unique k-SAT: An Isolation Lemma for k-CNFs. Journal of Computer and System Sciences, 74(3):386-393, 2008. LOWER BOUNDS FOR TESTING GRAPHICAL MODELS  [14] C.L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing Shape Restrictions of  Discrete Distributions. Theory of Computing Systems, 62(1):4-62, 2018.  [15] X. Chen, M. Dyer, L.A. Goldberg, M. Jerrum, P. Lu, C. McQuillan, and D. Richerby. The complexity of approximating conservative counting CSPs. Journal of Computer and System Sciences, 81(1):311-329, 2015.  [16] C.K. Chow and C. Liu. Approximating discrete probability distributions with dependence  trees. IEEE Transactions on Information Theory, 14(3):462-467, 1968.  [17] A. Collevecchio, T.M. Garoni, T. Hyndman, and D. Tokarev. The Worm process for the Ising  model is rapidly mixing. Journal of Statistical Physics, 164(5):1082-1102, 2016.  [18] S. Dasgupta. Learning polytrees. In Proceedings of the 15th Conference on Uncertainty in  Artificial Intelligence (UAI), pages 134-141, 1999.  [19] C. Daskalakis, E. Mossel, and S. Roch. Evolutionary trees and the Ising model on the Bethe lattice: a proof of Steel\u2019s conjecture. Probability Theory and Related Fields, 149(1-2):149- 189, 2011.  [20] C. Daskalakis, N. Dikkala, and G. Kamath. Testing Ising models. In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1989-2007, 2018.  [21] I. Diakonikolas and D.M. Kane. A new approach for testing properties of discrete distributions. In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 685-694, 2016.  [22] I. Diakonikolas, D.M. Kane, and V. Nikishkin. Optimal algorithms and lower bounds for test- ing closeness of structured distributions. In Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 1183-1202, 2015.  [23] I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Sample-Optimal Identity Testing In Proceedings of the 45th International Colloquium on Automata,  with High Probability. Languages, and Programming (ICALP), volume 1, pages 1-41, 2018.  [24] M. Dyer, L.A. Goldberg, C. Greenhill, and M. Jerrum. The relative complexity of approximate  counting problems. Algorithmica, 38(3):471-500, 2004.  [25] M. Dyer, L.A. Goldberg, and M. Jerrum. An approximation trichotomy for Boolean #CSP.  Journal of Computer and System Sciences, 76(3-4):267-277, 2010.  [26] T. Emden-Weinert, S. Hougardy, and B. Kreuter. Uniquely colourable graphs and the hardness of colouring graphs of large girth. Combinatorics, Probability and Computing, 7(4):375-386, 1998.  [27] J. Felsenstein. Inferring phylogenies, volume 2. Sinauer Associates, Inc., Sunderland, MA,  2004.  [28] S. Friedli and Y. Velenik. Statistical mechanics of lattice systems: a concrete mathematical  introduction. Cambridge University Press, 2017. LOWER BOUNDS FOR TESTING GRAPHICAL MODELS  [29] A. Galanis, L.A. Goldberg, and M. Jerrum. Approximately Counting H-Colourings is #BIS-  Hard. SIAM Journal on Computing, 45(3):680-711, 2016.  [30] A. Galanis, D. \u02c7Stefankovi\u02c7c, and E. Vigoda. Inapproximability of the partition function for the antiferromagnetic Ising and hard-core models. Combinatorics, Probability and Computing, 25(4):500-559, 2016.  [31] S. Geman and C. Graffigne. Markov random field image models and their applications to computer vision. In Proceedings of the International Congress of Mathematicians, volume 1, pages 1496-1517. Berkeley, CA, 1986.  [32] H.-O. Georgii. Gibbs measures and phase transitions, volume 9. Walter de Gruyter, 2011.  [33] L.A. Goldberg and M. Jerrum. The complexity of ferromagnetic Ising with local fields. Com-  binatorics, Probability and Computing, 16(1):43-61, 2007.  [34] L.A. Goldberg and M. Jerrum. Approximating the partition function of the ferromagnetic Potts  model. Journal of the ACM, 59(5):25, 2012.  [35] L.A. Goldberg and M. Jerrum. Approximating pairwise correlations in the Ising Model. ACM  Transactions on Computation Theory. To appear, 2019.  [36] O. Goldreich, S. Goldwasser, and D. Ron. Property testing and its connection to learning and  approximation. Journal of the ACM, 45(4):653-750, 1998.  [37] H. Guo and M. Jerrum. Random cluster dynamics for the Ising model is rapidly mixing. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1818-1827, 2017.  [38] L. Hamilton, F. Koehler, and A. Moitra. Information theoretic properties of Markov random fields, and their algorithmic applications. In Advances in Neural Information Processing Sys- tems (NeurIPS), pages 2460-2469, 2017.  [39] S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. Bulletin of  the American Mathematical Society, 43(4):439-561, 2006.  [40] R. Impagliazzo and R. Paturi. On the Complexity of k-SAT. Journal of Computer and System  Sciences, 62(2):367-375, 2001.  [41] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model.  SIAM Journal on computing, 22(5):1087-1116, 1993.  [42] A. Klivans and R. Meka. Learning graphical models using multiplicative weights. In Pro- ceedings of the 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 343-354. IEEE, 2017.  [43] S.-I. Lee, V. Ganapathi, and D. Koller. Efficient Structure Learning of Markov Networks using L1-Regularization. In Advances in Neural Information Processing Systems (NeurIPS), pages 817-824, 2007. LOWER BOUNDS FOR TESTING GRAPHICAL MODELS  [44] M. Molloy and B. Reed. Colouring graphs when the number of colours is nearly the maximum degree. In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing (STOC), pages 462-470, 2001.  [45] D. Randall and D. Wilson. Sampling spin configurations of an Ising system. In Proceedings of the 10th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 959-960, 1999.  [46] P. Ravikumar, M.J. Wainwright, and J.D. Lafferty. High-dimensional Ising model selection using (cid:96)1-regularized logistic regression. The Annals of Statistics, 38(3):1287-1319, 2010.  [47] S. Roth and M.J. Black. Fields of experts: A framework for learning image priors. In Pro- ceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 860-867, 2005.  [48] R. Rubinfeld and M. Sudan. Robust characterizations of polynomials with applications to  program testing. SIAM Journal on Computing, 25(2):252-271, 1996.  [49] R. Salakhutdinov and G. Hinton. An efficient learning procedure for Deep Boltzmann Ma-  chines. Neural Computation, 24(8):1967-2006, 2012.  [50] R. Salakhutdinov and H. Larochelle. Efficient learning of deep Boltzmann machines. In Pro- ceedings of the 13th International Conference on Artificial Intelligence and Statistics (AIS- TATS), pages 693-700, 2010.  [51] N.P. Santhanam and M.J. Wainwright. Information-theoretic limits of selecting binary graph- ical models in high dimensions. IEEE Trans. Information Theory, 58(7):4117-4134, 2012.  [52] A. Sly. Computational transition at the uniqueness threshold.  In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 287-296, 2010.  [53] A. Sly and N. Sun. The computational hardness of counting in two-spin models on d-regular graphs. In Proceedings of the 53rd Annual Symposium on Foundations of Computer Science (FOCS), pages 361-369, 2012.  [54] G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity testing.  SIAM Journal on Computing, 46(1):429-455, 2017.  [55] M. Vuffray, S. Misra, A. Lokhov, and M. Chertkov.  Interaction screening: Efficient and sample-optimal learning of Ising models. In Advances in Neural Information Processing Sys- tems (NeurIPS), pages 2595-2603, 2016. "}, "Approximate Guarantees for Dictionary Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Approximate Guarantees for Dictionary Learning", "abstract": "In the dictionary learning (or sparse coding) problem, we are given a collection of signals (vectors in $\\mathbb{R}^d$), and the goal is to find a \"basis\" in which the signals have a sparse (approximate) representation. The problem has received a lot of attention in signal processing, learning, and theoretical computer science. The problem is formalized as factorizing a matrix $X (d \\times n)$ (whose columns are the signals) as $X = AY$, where $A$ has a prescribed number $m$ of columns (typically $m \\ll n$), and $Y$ has columns that are $k$-sparse (typically $k \\ll d$). Most of the known theoretical results involve assuming that the columns of the unknown $A$ have certain incoherence properties, and that the coefficient matrix $Y$ has random (or partly random) structure.  The goal of our work is to understand what can be said in the absence of such assumptions. Can we still find $A$ and $Y$ such that $X \\approx AY$? We show that this is possible, if we allow violating the bounds on $m$ and $k$ by appropriate factors that depend on $k$ and the desired approximation. Our results rely on an algorithm for what we call the threshold correlation problem, which turns out to be related to hypercontractive norms of matrices. We also show that our algorithmic ideas apply to a setting in which some of the columns of $X$ are outliers, thus giving similar guarantees even in this challenging setting.", "pdf_url": "http://proceedings.mlr.press/v99/bhaskara19a/bhaskara19a.pdf", "keywords": ["Dictionary learning", "sparse coding", "approximation algorithms", "pursuit algorithms"], "reference": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. In Proceedings of The 27th Conference on Learning sparsely used overcomplete dictionaries. Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, pages 123\u2013137, 2014. URL http://jmlr.org/proceedings/papers/v35/agarwal14a.html.  M. Aharon, M. Elad, and A. Bruckstein. rmk-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):4311\u2013 4322, Nov 2006. ISSN 1053-587X. doi: 10.1109/TSP.2006.881199.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and over- In Proceedings of The 27th Conference on Learning Theory, COLT complete dictionaries. 2014, Barcelona, Spain, June 13-15, 2014, pages 779\u2013806, 2014. URL http://jmlr.org/ proceedings/papers/v35/arora14.html.  Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dictionaries with arbitrary supports. In 59th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, pages 283\u2013296, 2018. doi: 10.1109/FOCS.2018.00035. URL https://doi.org/10.1109/FOCS.2018.00035.  13   APPROXIMATE GUARANTEES FOR DICTIONARY LEARNING  Theorem 17 Suppose we have an (\u03b1, \u03b2)-approximation to the \u03c4 -TC problem, where \u03b1, \u03b2 are in- dependent of \u03c4 (e.g., constants). Then there exists an \u2126 approximation to the 2 (cid:55)\u2192 p norm problem.  (cid:16) \u03b11/p\u22121/2\u03b21/p log1/p n  (cid:17)  We defer the proof to "}, "The Optimal Approximation Factor in Density Estimation": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The Optimal Approximation Factor in Density Estimation", "abstract": "Consider the following problem: given two arbitrary densities $q_1,q_2$ and a sample-access to an unknown target density $p$, find which of the $q_i$\u2019s is closer to $p$ in total variation. A remarkable result due to Yatracos shows that this problem is tractable in the following sense: there exists an algorithm that uses  $O(\\epsilon^{-2})$ samples from $p$ and outputs\u00a0$q_i$ such that with high probability, $TV(q_i,p) \\leq 3\\cdot OPT + \\epsilon$, where $OPT= \\min\\{TV(q_1,p),TV(q_2,p)\\}$. Moreover, this result extends to any finite class of densities $\\mathcal{Q}$: there exists an algorithm that outputs the best density in $\\mathcal{Q}$ up to a multiplicative approximation factor of 3. We complement and extend this result by showing that: (i) the factor 3 can not be improved if one restricts the algorithm to output a density from $\\mathcal{Q}$, and (ii) if one allows the algorithm to output arbitrary densities (e.g. a mixture of densities from $\\mathcal{Q}$),  then the approximation factor can be reduced to 2, which is optimal. In particular this demonstrates an advantage of improper learning over proper in this setup. We develop two approaches to achieve the optimal approximation factor of $2$: an adaptive one and a static one. Both approaches are based on a geometric point of view of the problem and rely on estimating surrogate metrics to the total variation. Our sample complexity bounds exploit techniques from {\\it Adaptive Data Analysis}.", "pdf_url": "http://proceedings.mlr.press/v99/bousquet19a/bousquet19a.pdf", "keywords": ["List of keywords"], "reference": "Noga Alon, Shay Moran, and Amir Yehudayoff. Sign rank versus VC dimension. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop and Conference Proceedings, pages 47-80. JMLR.org, 2016.  Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of gaussians via sample compression schemes. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3416- 3425. Curran Associates, Inc., 2018a.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 2679-2686, 2018b.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 1046-1059, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4132-5. doi: 10.1145/2897518.2897566.  Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Near-optimal density esti- mation in near-linear time using variable-width histograms. In NIPS, pages 1844-1852, 2014.  Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In Maria-Florina Balcan, Vitaly Feldman, and Csaba Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, volume 35 of JMLR Workshop and Conference Proceedings, pages 287-316. JMLR.org, 2014.  L. Devroye and L. Gyorfi. Nonparametric Density Estimation: The L1 View. Wiley Interscience  Series in Discrete Mathematics. Wiley, 1985. ISBN 9780471816461.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  Luc Devroye and G\u00b4abor Lugosi. Bin width selection in multivariate histograms by the combinatorial  method. Test, 13(1):129-145, Jun 2004. ISSN 1863-8260. doi: 10.1007/BF02603004.  Ilias Diakonikolas. Learning structured distributions. In Handbook of Big Data, pages 267-283.  Chapman and Hall/CRC, 2016.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In FOCS, pages 73-84. IEEE Computer Society, 2017.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. List-decodable robust mean estimation  and learning mixtures of spherical gaussians. In STOC, pages 1047-1060. ACM, 2018a.  13   THE OPTIMAL APPROXIMATION FACTOR IN DENSITY ESTIMATION  References  Noga Alon, Shay Moran, and Amir Yehudayoff. Sign rank versus VC dimension. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop and Conference Proceedings, pages 47-80. JMLR.org, 2016.  Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of gaussians via sample compression schemes. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3416- 3425. Curran Associates, Inc., 2018a.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 2679-2686, 2018b.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 1046-1059, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4132-5. doi: 10.1145/2897518.2897566.  Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Near-optimal density esti- mation in near-linear time using variable-width histograms. In NIPS, pages 1844-1852, 2014.  Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In Maria-Florina Balcan, Vitaly Feldman, and Csaba Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, volume 35 of JMLR Workshop and Conference Proceedings, pages 287-316. JMLR.org, 2014.  L. Devroye and L. Gyorfi. Nonparametric Density Estimation: The L1 View. Wiley Interscience  Series in Discrete Mathematics. Wiley, 1985. ISBN 9780471816461.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  Luc Devroye and G\u00b4abor Lugosi. Bin width selection in multivariate histograms by the combinatorial  method. Test, 13(1):129-145, Jun 2004. ISSN 1863-8260. doi: 10.1007/BF02603004.  Ilias Diakonikolas. Learning structured distributions. In Handbook of Big Data, pages 267-283.  Chapman and Hall/CRC, 2016.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In FOCS, pages 73-84. IEEE Computer Society, 2017.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. List-decodable robust mean estimation  and learning mixtures of spherical gaussians. In STOC, pages 1047-1060. ACM, 2018a. BOUSQUET KANE MORAN  Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Fast and sample near-optimal algorithms for In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe learning multidimensional histograms. Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Pro- ceedings of Machine Learning Research, pages 819-842. PMLR, 06-09 Jul 2018b.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638, 2015. ISSN 0036-8075. doi: 10.1126/science.aaa9375.  J. Jiao, Y. Han, and T. Weissman. Minimax estimation of the l1 distance. IEEE Transactions on Information Theory, 64(10):6672-6706, Oct 2018. ISSN 0018-9448. doi: 10.1109/TIT.2018. 2846245.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Commun.  ACM, 55(2):113-120, 2012. doi: 10.1145/2076450.2076474.  Pravesh K. Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 1035-1046, 2018. doi: 10.1145/3188745.3188970.  G\u00b4abor Lugosi and Andrew Nobel. Consistency of data-driven histogram methods for density esti- mation and classification. Ann. Statist., 24(2):687-706, 04 1996. doi: 10.1214/aos/1032894460.  Satyaki Mahalanabis and Daniel Stefankovic. Density estimation in linear time.  In Rocco A. Servedio and Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008, pages 503-512. Omnipress, 2008.  Alfred M\u00a8uller. Integral probability metrics and their generating classes of functions. Advances in  Applied Probability, 29(2):429-443, 1997.  K. Pearson. Contributions to the mathematical theory of evolution. ii. skew variation in homoge-  neous material. Philosophical Trans. of the Royal Society of London, 186:343-414, 1895.  N. Sauer. On the density of families of sets. J. Comb. Theory, Ser. A, 13:145-147, 1972. ISSN  0097-3165. doi: 10.1016/0097-3165(72)90019-2.  Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press, 2012.  V.N. Vapnik and A.Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory Probab. Appl., 16:264-280, 1971. ISSN 0040-585X; 1095-7219/e. doi: 10.1137/1116025.  J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100:295-320, 1928.  Yannis G. Yatracos. Rates of convergence of minimum distance estimators and kolmogorov\u2019s en-  tropy. Ann. Statist., 13(2):768-774, 06 1985. doi: 10.1214/aos/1176349553.  Bin Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer,  1997. THE OPTIMAL APPROXIMATION FACTOR IN DENSITY ESTIMATION  Binary search  Input: vectors y, h, and n functions Fi as in Lemma 12, and a sample access to the target distribution p. Output: an index j such that y + (cid:15)  2 ej \u2264 v\u2217.  1. Set nmin = 1, nmax = n. While nmin < nmax:  (a) Set nmid = (cid:98) nmin+nmax(cid:99), (cid:96) = (cid:80)nmid  i=nmin  hi, u = (cid:80)nmax  i=nmid+1 hi, and  L(x) = (1/(cid:96))  (cid:0)Fi(x) \u2212 Eqi[Fi] \u2212 yi  (cid:1)  hi  U (x) = (1/u)  (cid:0)Fi(x) \u2212 Eqi[Fi] \u2212 yi  (cid:1).  hi  nmid(cid:88)  i=nmin  nmax(cid:88)  i=nmid+1  (b) Submit statistical queries to derive estimates \u02c6\u00b5L, \u02c6\u00b5U of Ep[L(x)], Ep[U (x)] re- (cid:15) 2 log n .  spectively up to an additive error of  (c) If \u02c6\u00b5L \u2265 \u02c6\u00b5U then set nmin = nmin, nmax = nmid, and normalize hi = hi  (cid:96) for nmin \u2264 i \u2264 nmax and else set nmin = nmid + 1, nmax = nmax, and normalize hi = hi  u for nmin \u2264 i \u2264 nmax.  2. Output nmin (= nmax).  Figure 2: Binary search for an appropriate index i  "}, "Sorted Top-k in Rounds": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sorted Top-k in Rounds", "abstract": "We consider the sorted top-$k$ problem whose goal is to recover the top-$k$ items with the correct order out of $n$ items using pairwise comparisons. In many applications, multiple rounds of interaction can be costly. We restrict our attention to algorithms with a constant number of rounds $r$ and try to minimize the sample complexity, i.e. the number of comparisons. When the comparisons are noiseless, we characterize how the optimal sample complexity depends on the number of rounds (up to a polylogarithmic factor for general $r$ and up to a constant factor for $r=1$ or 2). In particular, the sample complexity is $\\Theta(n^2)$ for $r=1$, $\\Theta(n\\sqrt{k} + n^{4/3})$ for $r=2$ and $\\tilde{\\Theta}\\left(n^{2/r} k^{(r-1)/r} + n\\right)$ for $r \\geq 3$.  We extend our results of sorted top-$k$ to the noisy case where each comparison is correct with probability $2/3$. When $r=1$ or 2, we show that the sample complexity gets an extra $\\Theta(\\log(k))$ factor when we transition from the noiseless case to the noisy case. We also prove new results for top-$k$ and sorting in the noisy case. We believe our techniques can be generally useful for understanding the trade-off between round complexities and sample complexities of rank aggregation problems.", "pdf_url": "http://proceedings.mlr.press/v99/braverman19a/braverman19a.pdf", "keywords": ["rank aggregation", "sorting", "top-k ranking", "round complexity", "noisy comparisons"], "reference": "Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. In Pro- ceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages 39-75, 2017.  N. Ailon. Active learning ranking from pairwise preferences with almost optimal query complexity.  In Advances in Neural Information Processing Systems, 2011.  N Ailon, M. Charikar, and A. Newman. Aggregating inconsistent information: ranking and cluster-  ing. Journal of the ACM, 55(5):23:1-23:27, 2008.  M Ajtai, J Komlos, W L Steiger, and E Szemeredi. Deterministic selection in o(loglog n) parallel time. In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, STOC \u201986, pages 188-195, New York, NY, USA, 1986. ACM. ISBN 0-89791-193-8. doi: 10.1145/ 12130.12149. URL http://doi.acm.org/10.1145/12130.12149.  12   SORTED TOP-k IN ROUNDS  the blow up in the sample complexity when we transition from the noiseless case to the noisy case. Interestingly, for r = 1 or 2, this blow up is different in different rank aggregation problems: \u0398(1) in top-k, \u0398(log(k)) in sorted top-k and \u0398(log(n)) in sorting.  There are mainly two obstacles to getting tighter bounds for top-k, sorting and sorted top-k when we have more than 2 rounds. We list them as open problems here. The first one is that we don\u2019t have tight (up to a constant factor) sample complexity bounds even in the noiseless case.  Open Problem 6 Get tight (up to a constant factor) sample complexity bounds for the noiseless case when r > 2.  In particular, the first step is to consider 3-round top-k in the noiseless case. Braverman et al. (2016) shows its sample complexity is O(n \u00b7 polylog(n)). Bollob\u00b4as and Brightwell (1990) shows that no 3-round algorithm with \u0398(n) comparisons can find top-k correctly with probability 1 \u2212 o(1). If we only want to succeed with constant probability (for example 2/3), the best lower bound is the trivial one: \u2126(n).  Once we have a good understanding of the noiseless case, we can start to think about the noisy  case for r > 2.  Open Problem 7 Extend our techniques for r = 1 or 2 in the noisy case to the case when we have more than 2 rounds.  In the noisy case, our 2-round bounds are very different and more complicated compared to 1-round bounds. Even if we have tight bounds in the noiseless case, getting tight bounds for more than 2 rounds could be more difficult and might require new techniques.  We would like to thank Claire Mathieu for earlier discussions of this problem.  Acknowledgments  References  Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. In Pro- ceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages 39-75, 2017.  N. Ailon. Active learning ranking from pairwise preferences with almost optimal query complexity.  In Advances in Neural Information Processing Systems, 2011.  N Ailon, M. Charikar, and A. Newman. Aggregating inconsistent information: ranking and cluster-  ing. Journal of the ACM, 55(5):23:1-23:27, 2008.  M Ajtai, J Komlos, W L Steiger, and E Szemeredi. Deterministic selection in o(loglog n) parallel time. In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, STOC \u201986, pages 188-195, New York, NY, USA, 1986. ACM. ISBN 0-89791-193-8. doi: 10.1145/ 12130.12149. URL http://doi.acm.org/10.1145/12130.12149. SORTED TOP-k IN ROUNDS  Mikl\u00b4os Ajtai, J\u00b4anos Koml\u00b4os, and Endre Szemer\u00b4edi. An o(n log n) sorting network.  In Pro- ceedings of the 15th Annual ACM Symposium on Theory of Computing, 25-27 April, 1983, Boston, Massachusetts, USA, pages 1-9, 1983. doi: 10.1145/800061.808726. URL http: //doi.acm.org/10.1145/800061.808726.  Selim G. Akl. Parallel Sorting Algorithms. Academic Press, Inc., Orlando, FL, USA, 1990. ISBN  0120476800.  Noga Alon. Expanders, sorting in rounds and superconcentrators of limited depth. In Proceedings of the 17th Annual ACM Symposium on Theory of Computing, May 6-8, 1985, Providence, Rhode Island, USA, pages 98-102, 1985. doi: 10.1145/22145.22156. URL http://doi.acm.org/ 10.1145/22145.22156.  Noga Alon and Yossi Azar. Sorting, approximate sorting, and searching in rounds. SIAM J. Dis- crete Math., 1(3):269-280, 1988a. doi: 10.1137/0401028. URL http://dx.doi.org/10. 1137/0401028.  Noga Alon and Yossi Azar. The average complexity of deterministic and randomized parallel comparison-sorting algorithms. SIAM J. Comput., 17(6):1178-1192, 1988b. doi: 10.1137/ 0217074. URL http://dx.doi.org/10.1137/0217074.  Noga Alon, Yossi Azar, and Uzi Vishkin. Tight complexity bounds for parallel comparison sorting. In 27th Annual Symposium on Foundations of Computer Science, Toronto, Canada, 27-29 Octo- ber 1986, pages 502-510, 1986. doi: 10.1109/SFCS.1986.57. URL http://dx.doi.org/ 10.1109/SFCS.1986.57.  Yossi Azar and Nicholas Pippenger. Parallel selection. Discrete Applied Mathematics, 27(1-2): 49-58, 1990. doi: 10.1016/0166-218X(90)90128-Y. URL https://doi.org/10.1016/ 0166-218X(90)90128-Y.  Yossi Azar and Uzi Vishkin. Tight comparison bounds on the complexity of parallel sorting. SIAM J. Comput., 16(3):458-464, 1987. doi: 10.1137/0216032. URL https://doi.org/10. 1137/0216032.  B\u00b4ela Bollob\u00b4as and Graham Brightwell. Parallel selection with high probability. SIAM J. Discrete Math., 3(1):21-31, 1990. doi: 10.1137/0403003. URL http://dx.doi.org/10.1137/ 0403003.  B\u00b4ela Bollob\u00b4as and Pavol Hell. Sorting and graphs.  In Ivan Rival, editor, Graphs and Order, volume 147 of NATO ASI Series, pages 169-184. Springer Netherlands, 1985. ISBN 978-94- 010-8848-0. doi: 10.1007/978-94-009-5315-4 5. URL http://dx.doi.org/10.1007/ 978-94-009-5315-4_5.  B\u00b4ela Bollob\u00b4as and Andrew Thomason. Parallel sorting. Discrete Applied Mathematics, 6(1):1 - 11, 1983. ISSN 0166-218X. doi: http://dx.doi.org/10.1016/0166-218X(83)90095-1. URL http: //www.sciencedirect.com/science/article/pii/0166218X83900951.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201908, pages 268-276, SORTED TOP-k IN ROUNDS  Philadelphia, PA, USA, 2008. Society for Industrial and Applied Mathematics. URL http: //dl.acm.org/citation.cfm?id=1347082.1347112.  Mark Braverman and Elchanan Mossel. Sorting from noisy information. CoRR, abs/0910.1191,  2009. URL http://arxiv.org/abs/0910.1191.  Mark Braverman, Jieming Mao, and S. Matthew Weinberg. Parallel algorithms for select and par- tition with noisy comparisons. In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 851-862, New York, NY, USA, 2016. ACM. ISBN 978- 1-4503-4132-5. doi: 10.1145/2897518.2897642. URL http://doi.acm.org/10.1145/ 2897518.2897642.  J. M. Chambers. Algorithm 410: Partial sorting. Commun. ACM, 14(5):357-358, May 1971. ISSN 0001-0782. doi: 10.1145/362588.362602. URL http://doi.acm.org/10.1145/ 362588.362602.  X. Chen, S. Gopi, J. Mao, and J. Schneider. Competitive analysis of the top-k ranking problem. In  Proceedings of ACM-SIAM Symposium on Discrete Algorithms (SODA), 2017.  Xi Chen, Yuanzhi Li, and Jieming Mao. A nearly instance optimal algorithm for top-k ranking In Proceedings of the Twenty-Eighth Annual ACM-SIAM  under the multinomial logit model. Symposium on Discrete Algorithms, SODA, 2018.  Yuxin Chen and Changho Suh. Spectral MLE: top-k rank aggregation from pairwise comparisons. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 371-380, 2015. URL http://jmlr.org/proceedings/ papers/v37/chena15.html.  Vincent Cohen-Addad, Frederik Mallmann-Trenn, and Claire Mathieu. Instance-optimality in the noisy value-and comparison-model \u2014 accept, accept, strong accept: Which papers get in? arXiv preprint arXiv:1806.08182, 2018.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information. SIAM J. Comput., 23(5):1001-1018, 1994. doi: 10.1137/S0097539791195877. URL https: //doi.org/10.1137/S0097539791195877.  Roland H\u00a8aggkvist and Pavol Hell. Parallel sorting with constant time for comparisons. SIAM J. Comput., 10(3):465-472, 1981. doi: 10.1137/0210034. URL http://dx.doi.org/10. 1137/0210034.  K. Jamieson and R. Nowak. Active ranking using pairwise comparisons. In Advances in Neural  Information Processing Systems, 2011.  C. Kenyon-Mathieu and W. Schudy. How to rank with few errors. In Proceedings of the Symposium  on Theory of computing (STOC), 2007.  Clyde P. Kruskal. Searching, merging, and sorting in parallel computation. IEEE Trans. Comput- ers, 32(10):942-946, 1983. doi: 10.1109/TC.1983.1676138. URL https://doi.org/10. 1109/TC.1983.1676138. SORTED TOP-k IN ROUNDS  Frank Thomson Leighton. Tight bounds on the complexity of parallel sorting. In Proceedings of the 16th Annual ACM Symposium on Theory of Computing, April 30 - May 2, 1984, Washington, DC, USA, pages 71-80, 1984. doi: 10.1145/800057.808667. URL http://doi.acm.org/ 10.1145/800057.808667.  T. Lu and C. Boutilier. Learning mallows models with pairwise preferences. In Proceedings of the  International Conference on Machine Learning (ICML), 2011.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Sorting noisy data with partial information. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 515-528, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1859- 4. doi: 10.1145/2422436.2422492. URL http://doi.acm.org/10.1145/2422436. 2422492.  S. Mohajer and C. Suh. Active top-k ranking from noisy comparisons. In Proceedings of the 54th  Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2016.  S. Negahban, S. Oh, and D. Shah. Rank centrality: Ranking from pair-wise comparisons. Opera-  tions Research, 65(1):266-287, 2017.  Alessandro Panconesi and Aravind Srinivasan. Randomized distributed edge coloring via an extension of the chernoff-hoeffding bounds. SIAM J. Comput., 26(2):350-368, April 1997. ISSN 0097-5397. doi: 10.1137/S0097539793250767. URL https://doi.org/10.1137/ S0097539793250767.  Nicholas Pippenger. Sorting and selecting in rounds. SIAM J. Comput., 16(6):1032-1038, 1987.  doi: 10.1137/0216066. URL https://doi.org/10.1137/0216066.  A. Rajkumar and S. Agarwal. A statistical convergence perspective of algorithms for rank aggre- gation from pairwise data. In Proceedings of the International Conference on Machine Learning (ICML), 2014.  R\u00a8udiger Reischuk. A fast probabilistic parallel sorting algorithm.  In 22nd Annual Symposium on Foundations of Computer Science, Nashville, Tennessee, USA, 28-30 October 1981, pages 212-219, 1981. doi: 10.1109/SFCS.1981.6. URL http://dx.doi.org/10.1109/SFCS. 1981.6.  N. B. Shah and M. Wainwright. Simple, robust and optimal ranking from pairwise comparisons.  arXiv preprint arXiv:1512.08949, 2015.  N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. IEEE Transactions on Informa- tion Theory, 63(2):934-959, 2017.  C. Suh, V. Tan, and R. Zhao. Adversarial top-k ranking. IEEE Transactions on Information Theory,  63(4):2201-2225, 2017.  Leslie G. Valiant. Parallelism in comparison problems. SIAM J. Comput., 4(3):348-355, 1975. doi:  10.1137/0204030. URL http://dx.doi.org/10.1137/0204030. SORTED TOP-k IN ROUNDS  F. Wauthier, M.Jordan, and N. Jojic. Efficient ranking from pairwise comparisons. In Proceedings  of the International Conference on Machine Learning (ICML), 2013.  "}, "Multi-armed Bandit Problems with Strategic Arms": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Multi-armed Bandit Problems with Strategic Arms", "abstract": "We study a strategic version of the multi-armed bandit problem, where each arm is an individual strategic agent and we, the principal, pull one arm each round. When pulled, the arm receives some private reward $v_a$ and can choose an amount $x_a$ to pass on to the principal (keeping $v_a-x_a$ for itself). All non-pulled arms get reward $0$. Each strategic arm tries to maximize its own utility over the course of $T$ rounds. Our goal is to design an algorithm for the principal incentivizing these arms to pass on as much of their private rewards as possible. When private rewards are stochastically drawn each round ($v_a^t \\leftarrow D_a$), we show that: \\begin{itemize} \\item Algorithms that perform well in the classic adversarial multi-armed bandit setting necessarily perform poorly: For all algorithms that guarantee low regret in an adversarial setting, there exist distributions $D_1,\\ldots,D_k$ and an $o(T)$-approximate Nash equilibrium for the arms where the principal receives reward $o(T)$.  \\item There exists an algorithm for the principal that induces a game among the arms where each arm has a dominant strategy. Moreover, for every $o(T)$-approximate Nash equilibrium, the principal receives expected reward $\\mu\u2019T - o(T)$, where $\\mu\u2019$ is the second-largest of the means $\\mathbb{E}[D_{a}]$. This algorithm maintains its guarantee if the arms are non-strategic ($x_a = v_a$), and also if there is a mix of strategic and non-strategic arms. \\end{itemize}", "pdf_url": "http://proceedings.mlr.press/v99/braverman19b/braverman19b.pdf", "keywords": ["multi-armed bandit", "strategic learning", "auction design"], "reference": "Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning prices for repeated auctions with strategic buyers. In Advances in Neural Information Processing Systems, pages 1169-1177, 2013.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic  buyers. In Advances in Neural Information Processing Systems, pages 622-630, 2014.  Masaki Aoyagi. Bid rotation and collusion in repeated auctions. Journal of Economic Theory, 112 (1):79-105, 2003. URL http://EconPapers.repec.org/RePEc:eee:jetheo:v: 112:y:2003:i:1:p:79-105.  Masaki Aoyagi. Efficient collusion in repeated auctions with communication. Journal of Eco- nomic Theory, 134(1):61-92, 2007. URL http://EconPapers.repec.org/RePEc: eee:jetheo:v:134:y:2007:i:1:p:61-92.  Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1503-1510, New York, NY, USA, 2012. ACM. URL http://icml.cc/2012/papers/749.pdf.  Susan Athey and Kyle Bagwell. Optimal collusion with private information. RAND Journal of Economics, 32(3):428-65, 2001. URL http://EconPapers.repec.org/RePEc:rje: randje:v:32:y:2001:i:3:p:428-65.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire.  The nonstochas- tic multiarmed bandit problem. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77, January 2003.  Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. Characterizing truthful multi-armed bandit mechanisms: Extended abstract. In Proceedings of the 10th ACM Conference on Elec- tronic Commerce, EC \u201909, pages 79-88, New York, NY, USA, 2009. ACM. ISBN 978-1- 60558-458-4. doi: 10.1145/1566374.1566386. URL http://doi.acm.org/10.1145/ 1566374.1566386.  Moshe Babaioff, Robert D. Kleinberg, and Aleksandrs Slivkins. Truthful mechanisms with implicit payment computation. In Proceedings of the 11th ACM Conference on Electronic Commerce, EC \u201910, pages 43-52, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-822-3. doi: 10.1145/ 1807342.1807349. URL http://doi.acm.org/10.1145/1807342.1807349.  Dirk Bergemann and Juuso Vlimki. Learning and strategic pricing. Econometrica, 64(5):1125- 49, 1996. URL http://EconPapers.repec.org/RePEc:ecm:emetrp:v:64:y: 1996:i:5:p:1125-49.  Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,  78(1):1-3, 1950.  13   MULTI-ARMED BANDIT PROBLEMS WITH STRATEGIC ARMS  References  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning prices for repeated auctions with strategic buyers. In Advances in Neural Information Processing Systems, pages 1169-1177, 2013.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic  buyers. In Advances in Neural Information Processing Systems, pages 622-630, 2014.  Masaki Aoyagi. Bid rotation and collusion in repeated auctions. Journal of Economic Theory, 112 (1):79-105, 2003. URL http://EconPapers.repec.org/RePEc:eee:jetheo:v: 112:y:2003:i:1:p:79-105.  Masaki Aoyagi. Efficient collusion in repeated auctions with communication. Journal of Eco- nomic Theory, 134(1):61-92, 2007. URL http://EconPapers.repec.org/RePEc: eee:jetheo:v:134:y:2007:i:1:p:61-92.  Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1503-1510, New York, NY, USA, 2012. ACM. URL http://icml.cc/2012/papers/749.pdf.  Susan Athey and Kyle Bagwell. Optimal collusion with private information. RAND Journal of Economics, 32(3):428-65, 2001. URL http://EconPapers.repec.org/RePEc:rje: randje:v:32:y:2001:i:3:p:428-65.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire.  The nonstochas- tic multiarmed bandit problem. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77, January 2003.  Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. Characterizing truthful multi-armed bandit mechanisms: Extended abstract. In Proceedings of the 10th ACM Conference on Elec- tronic Commerce, EC \u201909, pages 79-88, New York, NY, USA, 2009. ACM. ISBN 978-1- 60558-458-4. doi: 10.1145/1566374.1566386. URL http://doi.acm.org/10.1145/ 1566374.1566386.  Moshe Babaioff, Robert D. Kleinberg, and Aleksandrs Slivkins. Truthful mechanisms with implicit payment computation. In Proceedings of the 11th ACM Conference on Electronic Commerce, EC \u201910, pages 43-52, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-822-3. doi: 10.1145/ 1807342.1807349. URL http://doi.acm.org/10.1145/1807342.1807349.  Dirk Bergemann and Juuso Vlimki. Learning and strategic pricing. Econometrica, 64(5):1125- 49, 1996. URL http://EconPapers.repec.org/RePEc:ecm:emetrp:v:64:y: 1996:i:5:p:1125-49.  Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,  78(1):1-3, 1950. MULTI-ARMED BANDIT PROBLEMS WITH STRATEGIC ARMS  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. doi: 10.1561/2200000024. URL http://dx.doi.org/10.1561/2200000024.  Sylvain Chassang. Calibrated incentive contracts. Econometrica, 81(5):1935-1971, 2013.  Nikhil R. Devanur and Sham M. Kakade. The price of truthfulness for pay-per-click auctions. In Proceedings of the 10th ACM Conference on Electronic Commerce, EC \u201909, pages 99-106, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-458-4. doi: 10.1145/1566374.1566388. URL http://doi.acm.org/10.1145/1566374.1566388.  Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 5-22, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2565-3. doi: 10.1145/2600057. 2602897. URL http://doi.acm.org/10.1145/2600057.2602897.  J.C. Gittins and D.M. Jones. A dynamic allocation index for the sequential design of experiments.  In J. Gani, editor, Progress in Statistics, pages 241-266. North-Holland, Amsterdam, 1974.  Paul Johnson and Jacques Robert. Collusion in a model of repeated auctions. Cahiers de recherche, Universite de Montreal, Departement de sciences economiques, 1999. URL http: //EconPapers.repec.org/RePEc:mtl:montde:9909.  Sham M. Kakade, Ilan Lobel, and Hamid Nazerzadeh. Optimal dynamic mechanism design and the  virtual-pivot mechanism. Operations Research, 61(4):837-854, 2013.  Ilan Kremer, Yishay Mansour, and Motty Perry. Implementing the \u201dwisdom of the crowd\u201d. Journal of Political Economy, 122(5):988 - 1012, 2014. URL http://EconPapers.repec.org/ RePEc:ucp:jpolec:doi:10.1086/676597.  Jean-Jacques Laffont and David Martimort. The Theory of Incentives: The Principal-Agent Model.  2002.  T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Adv. Appl. Math., 6(1):4-22, March 1985. ISSN 0196-8858. doi: 10.1016/0196-8858(85)90002-8. URL http: //dx.doi.org/10.1016/0196-8858(85)90002-8.  Randolph McAfee and John McMillan. Bidding rings. American Economic Review, 82(3):579- 99, 1992. URL http://EconPapers.repec.org/RePEc:aea:aecrev:v:82:y: 1992:i:3:p:579-99.  John McCarthy. Measures of the value of information. Proceedings of the National Academy of  Sciences, 42(9):654-655, 1956.  Hamid Nazerzadeh, Amin Saberi, and Rakesh Vohra. Dynamic cost-per-action mechanisms and In Proceedings of the 17th International Conference on applications to online advertising. World Wide Web, WWW \u201908, pages 179-188, New York, NY, USA, 2008. ACM. ISBN 978- 1-60558-085-2. doi: 10.1145/1367497.1367522. URL http://doi.acm.org/10.1145/ 1367497.1367522. MULTI-ARMED BANDIT PROBLEMS WITH STRATEGIC ARMS  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American URL http://www.projecteuclid.  Mathematical Society, 58(5):527-535, 1952. org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle= euclid.bams/1183517370.  Andrzej Skrzypacz and Hugo Hopenhayn. Tacit collusion in repeated auctions. Journal of Economic Theory, 114(1):153-169, 2004. URL http://EconPapers.repec.org/RePEc:eee: jetheo:v:114:y:2004:i:1:p:153-169.  "}, "Universality of Computational Lower Bounds for Submatrix Detection": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Universality of Computational Lower Bounds for Submatrix Detection", "abstract": "In the general submatrix detection problem, the task is to detect the presence of a small $k \\times k$ submatrix with entries sampled from a distribution $\\mathcal{P}$ in an $n \\times n$ matrix of samples from $\\mathcal{Q}$. This formulation includes a number of well-studied problems, such as biclustering when $\\mathcal{P}$ and $\\mathcal{Q}$ are Gaussians and the planted dense subgraph formulation of community detection when the submatrix is a principal minor and $\\mathcal{P}$ and $\\mathcal{Q}$ are Bernoulli random variables. These problems all seem to exhibit a universal phenomenon: there is a statistical-computational gap depending on $\\mathcal{P}$ and $\\mathcal{Q}$ between the minimum $k$ at which this task can be solved and the minimum $k$ at which it can be solved in polynomial time. Our main result is to tightly characterize this computational barrier as a tradeoff between $k$ and the KL divergences between $\\mathcal{P}$ and $\\mathcal{Q}$ through average-case reductions from the planted clique conjecture. These computational lower bounds hold given mild assumptions on $\\mathcal{P}$ and $\\mathcal{Q}$ arising naturally from classical binary hypothesis testing. Our results recover and generalize the planted clique lower bounds for Gaussian biclustering in Ma and Wu (2015) and Brennan et al. (2018) and for the sparse and general regimes of planted dense subgraph in Hajek et al. (2015) and Brennan et al. (2018). This yields the first universality principle for computational lower bounds obtained through average-case reductions.", "pdf_url": "http://proceedings.mlr.press/v99/brennan19a/brennan19a.pdf", "keywords": ["statistical-computational gaps", "average-case reductions", "planted clique conjecture", "universality", "submatrix detection", "community detection"], "reference": "Louigi Addario-Berry, Nicolas Broutin, Luc Devroye, and G\u00b4abor Lugosi. On combinatorial testing  problems. The Annals of Statistics, 38(5):3063-3092, 2010.  Noga Alon, Alexandr Andoni, Tali Kaufman, Kevin Matulef, Ronitt Rubinfeld, and Ning Xie. Test- In Proceedings of the thirty-ninth annual ACM  ing k-wise and almost k-wise independence. symposium on Theory of computing, pages 496-505. ACM, 2007.  Brendan PW Ames and Stephen A Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Ery Arias-Castro, Nicolas Verzelen, et al. Community detection in dense random networks. The  Annals of Statistics, 42(3):940-969, 2014.  Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Statistical and computational tradeoffs in biclustering. In NIPS 2011 workshop on computational trade-offs in statistical learning, volume 4, 2011.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013a.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013b.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an o(n1/4) approximation for densest k-subgraph. Proceedings of the forty-second ACM symposium on Theory of computing, pages 201-210, 2010.  Matthew Brennan and Guy Bresler. Optimal average-case reductions to sparse pca: From weak  assumptions to strong hardness. arXiv preprint arXiv:1902.07380, 2019.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds  for problems with planted sparse structure. In COLT, pages 48-166, 2018.  Cristina Butucea and Yuri I Ingster. Detection of a sparse submatrix of a high-dimensional noisy  matrix. Bernoulli, 19(5B):2652-2688, 2013.  T Tony Cai, Tengyuan Liang, Alexander Rakhlin, et al. Computational and statistical boundaries for submatrix localization in a large noisy matrix. The Annals of Statistics, 45(4):1403-1430, 2017.  Utkan Onur Candogan and Venkat Chandrasekaran. Finding planted subgraphs with few eigenval-  ues using the schur-horn relaxation. SIAM Journal on Optimization, 28(1):735-759, 2018.  Yudong Chen. Incoherence-optimal matrix completion. IEEE Transactions on Information Theory,  61(5):2909-2923, 2015.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. Journal of Machine Learning Research, 17(27):1-57, 2016.  13   UNIVERSAL REDUCTIONS TO SUBMATRIX DETECTION  References  Louigi Addario-Berry, Nicolas Broutin, Luc Devroye, and G\u00b4abor Lugosi. On combinatorial testing  problems. The Annals of Statistics, 38(5):3063-3092, 2010.  Noga Alon, Alexandr Andoni, Tali Kaufman, Kevin Matulef, Ronitt Rubinfeld, and Ning Xie. Test- In Proceedings of the thirty-ninth annual ACM  ing k-wise and almost k-wise independence. symposium on Theory of computing, pages 496-505. ACM, 2007.  Brendan PW Ames and Stephen A Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Ery Arias-Castro, Nicolas Verzelen, et al. Community detection in dense random networks. The  Annals of Statistics, 42(3):940-969, 2014.  Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Statistical and computational tradeoffs in biclustering. In NIPS 2011 workshop on computational trade-offs in statistical learning, volume 4, 2011.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013a.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013b.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an o(n1/4) approximation for densest k-subgraph. Proceedings of the forty-second ACM symposium on Theory of computing, pages 201-210, 2010.  Matthew Brennan and Guy Bresler. Optimal average-case reductions to sparse pca: From weak  assumptions to strong hardness. arXiv preprint arXiv:1902.07380, 2019.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds  for problems with planted sparse structure. In COLT, pages 48-166, 2018.  Cristina Butucea and Yuri I Ingster. Detection of a sparse submatrix of a high-dimensional noisy  matrix. Bernoulli, 19(5B):2652-2688, 2013.  T Tony Cai, Tengyuan Liang, Alexander Rakhlin, et al. Computational and statistical boundaries for submatrix localization in a large noisy matrix. The Annals of Statistics, 45(4):1403-1430, 2017.  Utkan Onur Candogan and Venkat Chandrasekaran. Finding planted subgraphs with few eigenval-  ues using the schur-horn relaxation. SIAM Journal on Optimization, 28(1):735-759, 2018.  Yudong Chen. Incoherence-optimal matrix completion. IEEE Transactions on Information Theory,  61(5):2909-2923, 2015.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. Journal of Machine Learning Research, 17(27):1-57, 2016. UNIVERSAL REDUCTIONS TO SUBMATRIX DETECTION  Eden Chlamtac, Michael Dinitz, and Robert Krauthgamer. Everywhere-sparse spanners via dense subgraphs. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 758-767. IEEE, 2012.  Eden Chlamt\u00b4a\u02c7c, Michael Dinitz, and Yury Makarychev. Minimizing the union: Tight approxi- mations for small set bipartite vertex expansion. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 881-899. SIAM, 2017.  E. Chlamt\u00b4a\u02c7c and P. Manurangsi. Sherali-adams integrality gaps matching the log-density threshold.  arXiv preprint arXiv:1804.07842, 2018.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high  probability. Combinatorics, Probability and Computing, 23(1):29-49, 2014.  Yash Deshpande and Andrea Montanari. Finding hidden cliques of size(cid:112)N/e in nearly linear time.  Foundations of Computational Mathematics, 15(4):1069-1128, 2015.  Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for the  two-groups stochastic block model. arXiv preprint arXiv:1507.08685, 2015.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Structures and Algorithms, 16(2):195-208, 2000.  Uriel Feige and Dorit Ron. Finding hidden cliques in linear time. In 21st International Meeting on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms (AofA\u201910), pages 189-204. Discrete Mathematics and Theoretical Computer Science, 2010.  Chao Gao, Zongming Ma, and Harrison H Zhou. Sparse cca: Adaptive estimation and computa-  tional barriers. The Annals of Statistics, 45(5):2074-2101, 2017.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef-  inite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, 2016a.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Semidefinite programs for exact recovery of a hidden  community. In Conference on Learning Theory, pages 1051-1095, 2016b.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Information limits for recovering a hidden community.  IEEE Transactions on Information Theory, 63(8):4729-4745, 2017.  Bruce E Hajek, Yihong Wu, and Jiaming Xu. Computational lower bounds for community detection  on random graphs. In COLT, pages 899-928, 2015.  Kumar Joag-Dev and Frank Proschan. Negative association of random variables with applications.  The Annals of Statistics, pages 286-295, 1983.  Pascal Koiran and Anastasios Zouzias. Hidden cliques and the certification of the restricted isometry  property. IEEE Transactions on Information Theory, 60(8):4999-5006, 2014.  Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, and Aarti Singh. Minimax localization of structural information in large noisy matrices. In Advances in Neural Information Processing Systems, pages 909-917, 2011. UNIVERSAL REDUCTIONS TO SUBMATRIX DETECTION  Florent Krzakala, Jiaming Xu, and Lenka Zdeborov\u00b4a. Mutual information in rank-one matrix esti-  mation. In 2016 IEEE Information Theory Workshop (ITW), pages 71-75. IEEE, 2016.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. Mmse of probabilistic low-rank ma- trix estimation: Universality with respect to the output channel. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 680-687. IEEE, 2015.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. The Annals  of Statistics, 43(3):1089-1116, 2015.  Frank McSherry. Spectral partitioning of random graphs.  In Foundations of Computer Science,  2001. Proceedings. 42nd IEEE Symposium on, pages 529-537. IEEE, 2001.  Andrea Montanari. Finding one community in a sparse graph. Journal of Statistical Physics, 161  (2):273-299, 2015.  Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral methods: From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors. In Ad- vances in Neural Information Processing Systems, pages 217-225, 2015.  Andrey A Shabalin, Victor J Weigman, Charles M Perou, Andrew B Nobel, et al. Finding large average submatrices in high dimensional data. The Annals of Applied Statistics, 3(3):985-1012, 2009.  Nicolas Verzelen, Ery Arias-Castro, et al. Community detection in sparse random networks. The  Annals of Applied Probability, 25(6):3465-3510, 2015.  Tengyao Wang, Quentin Berthet, and Yaniv Plan. Average-case hardness of rip certification. In  Advances in Neural Information Processing Systems, pages 3819-3827, 2016a.  Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896-1930, 2016b.  Yihong Wu and Jiaming Xu. Statistical problems with planted structures: Information-theoretical  and computational limits. arXiv preprint arXiv:1806.00118, 2018. UNIVERSAL REDUCTIONS TO SUBMATRIX DETECTION  ContentsIntroduction  2 Submatrix Problems and Conditions for Universality  3 Summary of Results  4 Overview of Average-Case Reduction Techniques  A Notation  B Average-Case Reductions in Total Variation  C Multivariate Rejection Kernels  D Average-Case Reduction to Submatrix Detection  E Computational Barriers in Submatrix Detection  F Statistical Limit of Submatrix Detection  G The Universality Classes UC-A, UC-B and UC-C  H Further Questions41017264451 UNIVERSAL REDUCTIONS TO SUBMATRIX DETECTION  "}, "Optimal Average-Case Reductions to Sparse PCA: From Weak Assumptions to Strong Hardness": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Optimal Average-Case Reductions to Sparse PCA: From Weak Assumptions to Strong Hardness", "abstract": "In the past decade, sparse principal component analysis has emerged as an archetypal problem for illustrating statistical-computational tradeoffs. This trend has largely been driven by a line of research aiming to characterize the average-case complexity of sparse PCA through reductions from the planted clique ($\\textsc{pc}$) conjecture. All previous reductions either fail to show tight computational lower bounds matching existing algorithms or show lower bounds for formulations of sparse PCA other than its canonical generative model, the spiked covariance model. Also, these lower bounds all quickly degrade given weak forms of the $\\textsc{pc}$ conjecture. We give a reduction from $\\textsc{pc}$ that yields the first full characterization of the computational barrier in the spiked covariance model, providing tight lower bounds at all sparsities $k$. We also show the surprising result that even a mild improvement in the signal strength needed by the best known polynomial-time sparse PCA algorithms would imply that the hardness threshold for $\\textsc{pc}$ is $\\text{polylog}(N)$, rather than on the order $N^{1/2}$ as is widely conjectured. This is the first instance of a suboptimal hardness assumption implying optimal lower bounds for another problem in unsupervised learning.", "pdf_url": "http://proceedings.mlr.press/v99/brennan19b/brennan19b.pdf", "keywords": ["statistical-computational gaps", "average-case reductions", "sparse PCA", "planted clique conjecture", "planted dense subgraph"], "reference": "Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013a.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013b.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds  for problems with planted sparse structure. In COLT, pages 48-166, 2018.  Chao Gao, Zongming Ma, and Harrison H Zhou. Sparse CCA: adaptive estimation and computa-  tional barriers. The Annals of Statistics, 45(5):2074-2101, 2017.  Iain M Johnstone and Arthur Yu Lu.  Sparse principal components analysis. Unpublished  manuscript, 2004.  Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896-1930, 2016.  2   STRONG REDUCTIONS TO SPARSE PCA  canonical generative model, the spiked covariance model. Also, these lower bounds all quickly de- grade with the exponent in the PC conjecture. When only given the PC conjecture up to K = o(N \u03b1) where \u03b1 < 1/2, there is no sparsity k at which they remain tight. If \u03b1 \u2264 1/3 these reductions fail to even show the existence of a statistical-computational gap at any sparsity k. Our main results are:  \u2022 We give a reduction from PC that yields the first full characterization of the computational barrier in the spiked covariance model, providing tight lower bounds at all sparsities k. This partially resolves a question raised in Brennan et al. (2018).  \u2022 We show the surprising result that weaker forms of the PC conjecture up to clique size K = o(N \u03b1) for any given \u03b1 \u2208 (0, 1/2] imply tight computational lower bounds for sparse PCA at sparsities k = o(n\u03b1/3). Our reduction also shows that even a mild improvement in the signal strength needed by the best known polynomial-time sparse PCA algorithms would imply that the hardness threshold for PC is polylog(N ), rather than on the order N 1/2.  Our second result essentially shows that whether or not there are better efficient algorithms for PC is irrelevant to the statistical-computational gap for sparse PCA in the practically relevant highly sparse regime. This is the first instance of a suboptimal hardness assumption implying optimal lower bounds for another problem in unsupervised learning.  The reduction proving this result is more algorithmically involved than prior reductions to sparse PCA, making crucial use of a collection of average-case reduction primitives and introducing new techniques based on several decomposition and comparison properties of random matrices. Our lower bounds remain unchanged assuming hardness of planted dense subgraph instead of PC, which also has implications for the existence of algorithms slower than polynomial time. As a key step, our reduction maps an instance of PC to the empirical covariance matrix of sparse PCA samples, which proves to be a delicate task because of dependence among the entries of this matrix.  References  Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013a.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013b.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds  for problems with planted sparse structure. In COLT, pages 48-166, 2018.  Chao Gao, Zongming Ma, and Harrison H Zhou. Sparse CCA: adaptive estimation and computa-  tional barriers. The Annals of Statistics, 45(5):2074-2101, 2017.  Iain M Johnstone and Arthur Yu Lu.  Sparse principal components analysis. Unpublished  manuscript, 2004.  Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896-1930, 2016. "}, "Learning rates for Gaussian mixtures under group action": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning rates for Gaussian mixtures under group action", "abstract": "We study the pointwise maximum likelihood estimation rates for a class of Gaussian mixtures that are invariant under the action of some isometry group. This model is also known as multi-reference alignment, where random rotations of a given vector are observed, up to Gaussian noise. We completely characterize the speed of the maximum likelihood estimator, by giving a comprehensive description of the likelihood geometry of the model. We show that the unknown parameter can always be decomposed into two components, one of which can be estimated at the fast rate $n^{-1/2}$, the other one being estimated at the slower rate $n^{-1/4}$. We provide an algebraic description and a geometric interpretation of these facts.", "pdf_url": "http://proceedings.mlr.press/v99/brunel19a/brunel19a.pdf", "keywords": ["Asymptotic rates", "Gaussian mixtures", "Maximum likelihood", "Group actions"], "reference": "Emmanuel Abbe, Tamir Bendory, William Leeb, Jo\u02dcao M Pereira, Nir Sharon, and Amit Singer. Multireference alignment is easier with an aperiodic translation distribution. IEEE Transactions on Information Theory, 2018a.  Emmanuel Abbe, Jo\u02dcao M Pereira, and Amit Singer. Estimation in the group action channel. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 561-565. IEEE, 2018b.  Carlos Am\u00b4endola, Kristian Ranestad, and Bernd Sturmfels. Algebraic identifiability of gaussian  mixtures. International Mathematics Research Notices, 2016.  12   GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Therefore, the second assumption of Lemma 11 is satisfied, thanks to (Vaart, 1998, Lemma 19.38). We have proven that all the assumptions of Lemma 11 are satisfied, which yields Theorem 12.  Perhaps surprinsingly, these rates do not depend on the size of H and this theorem shows that any \u03b8\u2217 could be estimated, up to identifiability, at least at the rate n\u22121/4 via the MLE. However, Theorem 12 does not provide uniform bounds. In fact, the OP\u03b8\u2217 signs contain constants that depend on \u03b8\u2217 and may become arbitrarily large. For instance, one expects that the constants hidden in the n1/2 should blow up when two modes \u03b8\u2217 and g\u03b8\u2217, for g /\u2208 H, are distinct but arbitrarily close to each other.  4. Conclusion  In this work, we have exhibited two different pointwise rates for the estimation of the parameter of a mixture of Gaussian distributions with uniform weights, under the invariance of an isometry group action: n\u22121/2 and n\u22121/4. Even in the second regime, we have shown that some components of \u03b8\u2217 could still be estimated at the fast rate n\u22121/2, and we have provided an algebraic description and a geometric interpretation of this fact, in terms of colliding modes of the population log-likelihood. These rates are consistent with the usual pointwise rates obtained known in the literature, even though here, we focused on parameter estimation (as opposed to distribution learning) with respect to the Euclidean loss.  As expected for general mixtures Chen (1995), the slow regime n\u22121/4 occurs when the actual number of components in the mixture is strictly less than the number predicted by the model, here, |G|. In other words, for general mixtures, slower rates occur when the model is overparametrized. However, here, this analogy should be made carefully because the presence of symmetries in \u03b8\u2217 is not necessarily implying an overparametrization.  The projection \u00afH depends on \u03b8\u2217. Therefore, even if Theorem 12 states that some components of \u03b8\u2217 are estimated at the parametric rate n\u22121/2 while the other components are estimated at the slower rate n\u22121/4, the linear subspaces corresponding to these components are unknown. The problem of recovering \u00afH or, more generally, H, is somewhat equivalent to learning the symmetries of \u03b8\u2217. If one assumes that inf g /\u2208H (cid:107)g\u03b8\u2217 \u2212 \u03b8\u2217(cid:107) is bounded away from zero by some known constant, then H can be recovered easily. In general, the estimation of H is a more challenging problem, which we leave for further work.  References  Emmanuel Abbe, Tamir Bendory, William Leeb, Jo\u02dcao M Pereira, Nir Sharon, and Amit Singer. Multireference alignment is easier with an aperiodic translation distribution. IEEE Transactions on Information Theory, 2018a.  Emmanuel Abbe, Jo\u02dcao M Pereira, and Amit Singer. Estimation in the group action channel. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 561-565. IEEE, 2018b.  Carlos Am\u00b4endola, Kristian Ranestad, and Bernd Sturmfels. Algebraic identifiability of gaussian  mixtures. International Mathematics Research Notices, 2016. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Martin Azizyan, Aarti Singh, and Larry Wasserman. Minimax theory for high-dimensional gaussian mixtures with sparse mean separation. In Advances in Neural Information Processing Systems, pages 2139-2147, 2013.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Afonso Bandeira, Philippe Rigollet, and Jonathan Weed. Optimal rates of estimation for multi-  reference alignment. arXiv preprint arXiv:1702.08546, 2017a.  Afonso S Bandeira, Moses Charikar, Amit Singer, and Andy Zhu. Multireference alignment using semidefinite programming. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 459-470. ACM, 2014.  Afonso S Bandeira, Ben Blum-Smith, Joe Kileel, Amelia Perry, Jonathan Weed, and Alexan- der S Wein. Estimation under group actions: recovering orbits from invariants. arXiv preprint arXiv:1712.10163, 2017b.  Jiahua Chen. Optimal rate of convergence for finite mixture models. The Annals of Statistics, pages  221-233, 1995.  Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of computer science, 1999. 40th  annual symposium on, pages 634-644. IEEE, 1999.  Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753-760. ACM, 2015.  Philippe Heinrich and Jonas Kahn. Optimal rates for finite mixture estimation. arXiv preprint  arXiv:1507.04313, 2015.  Nhat Ho and XuanLong Nguyen. Singularity structures and impacts on parameter estimation in  finite mixtures of distributions. arXiv preprint arXiv:1609.02655, 2016.  Nhat Ho, XuanLong Nguyen, et al. Convergence rates of parameter estimation for some weakly  identifiable finite mixtures. The Annals of Statistics, 44(6):2726-2755, 2016.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and In Proceedings of the 4th conference on Innovations in Theoretical  spectral decompositions. Computer Science, pages 11-20. ACM, 2013.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553-562. ACM, 2010.  Michael R. Kosorok. Introduction to empirical processes and semiparametric inference. Springer  Series in Statistics. Springer, New York, 2008. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93- 102. IEEE, 2010.  XuanLong Nguyen et al. Convergence of latent mixing measures in finite and infinite mixture  models. The Annals of Statistics, 41(1):370-400, 2013.  Amelia Perry, Jonathan Weed, Afonso Bandeira, Philippe Rigollet, and Amit Singer. The sample  complexity of multi-reference alignment. arXiv preprint arXiv:1707.00943, 2017.  Arora Sanjeev and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the  thirty-third annual ACM symposium on Theory of computing, pages 247-257. ACM, 2001.  Amit Singer and Yoel Shkolnisky. Three-dimensional structure determination from common lines in cryo-em by eigenvectors and semidefinite programming. SIAM journal on imaging sciences, 4 (2):543-572, 2011.  COS Sorzano, JR Bilbao-Castro, Y Shkolnisky, M Alcorlo, R Melero, G Caffarena-Fern\u00b4andez, M Li, G Xu, R Marabini, and JM Carazo. A clustering approach to multireference alignment of single-particle projections in electron microscopy. Journal of structural biology, 171(2):197-206, 2010.  A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Math-  ematics. Cambridge University Press, 1998.  Alexander Spence Wein. Statistical estimation in the presence of group actions. PhD thesis, Mas-  sachusetts Institute of Technology, 2018.  Yihong Wu and Pengkun Yang. Optimal estimation of gaussian mixtures via denoised method of  moments. Technical report, working paper, 2018. GAUSSIAN MIXTURES WITH ISOMETRY GROUP INVARIANCE  "}, "Near-optimal method for highly smooth convex optimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Near-optimal method for highly smooth convex optimization", "abstract": "We propose a near-optimal method for highly smooth convex optimization. More precisely, in the oracle model where one obtains the $p^{th}$ order Taylor expansion of a function at the query point, we propose a method with rate of convergence $\\tilde{O}(1/k^{\\frac{ 3p +1}{2}})$ after $k$ queries to the oracle for any convex function whose $p^{th}$ order derivative is Lipschitz.", "pdf_url": "http://proceedings.mlr.press/v99/bubeck19a/bubeck19a.pdf", "keywords": ["Convex optimization", "unconstrained minimization", "tensor methods", "oracle complexity", "smoothness"], "reference": "N. Agarwal and E. Hazan. Lower bounds for higher-order convex optimization. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774\u2013792. PMLR, 2018.  Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, 2018.  A. Gasnikov, E. Gorbunov, D. Kovalev, A. Mohhamed, and E. Chernousova. The global rate of convergence for optimal tensor methods in smooth convex optimization. Arxiv preprint arXiv:1809.00382, 2018.  B. Jiang, H. Wang, and S. Zhang. An optimal high-order tensor method for convex optimization.  Arxiv preprint arXiv:1812.06557, 2018.  R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for con- vex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092\u20131125, 2013.  A. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Ser. Tekhnich-  eskaya Kibernetika, 2, 1982.  Interscience, 1983.  A. Nemirovski and D. Yudin. Problem Complexity and Method Ef\ufb01ciency in Optimization. Wiley  Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).  Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.  Y. Nesterov.  Introductory lectures on convex optimization: A basic course. Kluwer Academic  Publishers, 2004.  Nesterov Y. Implementable tensor methods in unconstrained convex optimization. Core discussion papers, 2018. URL https://ideas.repec.org/p/cor/louvco/2018005.html.  Appendix A. Proofs from Section 4  Here we provide proofs of lemmas from Section 4. Proof [Proof of Lemma 13] To compute the derivative of z\u03b8, we note by optimality condition that  Taking derivatives with respect to \u03b8 on both sides gives  \u2207zF (z\u03b8, (cid:101)x\u03b8) = 0.  \u22072  zzF (z\u03b8, (cid:101)x\u03b8) \u00b7  z\u03b8 + \u22072  zxF (z\u03b8, (cid:101)x\u03b8) \u00b7  d d\u03b8  d d\u03b8 (cid:101)x\u03b8 = 0.  Hence, we have  d d\u03b8  z\u03b8 = \u2212 (cid:0)\u22072  zzF (z\u03b8, (cid:101)x\u03b8)(cid:1)\u22121  \u22072  zxF (z\u03b8, (cid:101)x\u03b8) \u00b7 (yk \u2212 xk).  (17)  13   NEAR-OPTIMAL HIGHLY SMOOTH CONVEX OPTIMIZATION  References  N. Agarwal and E. Hazan. Lower bounds for higher-order convex optimization. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774-792. PMLR, 2018.  Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, 2018.  A. Gasnikov, E. Gorbunov, D. Kovalev, A. Mohhamed, and E. Chernousova. The global rate of convergence for optimal tensor methods in smooth convex optimization. Arxiv preprint arXiv:1809.00382, 2018.  B. Jiang, H. Wang, and S. Zhang. An optimal high-order tensor method for convex optimization.  Arxiv preprint arXiv:1812.06557, 2018.  R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for con- vex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2):1092-1125, 2013.  A. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Ser. Tekhnich-  eskaya Kibernetika, 2, 1982.  Interscience, 1983.  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley  Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).  Soviet Mathematics Doklady, 27(2):372-376, 1983.  Y. Nesterov.  Introductory lectures on convex optimization: A basic course. Kluwer Academic  Publishers, 2004.  Nesterov Y. Implementable tensor methods in unconstrained convex optimization. Core discussion papers, 2018. URL https://ideas.repec.org/p/cor/louvco/2018005.html.  "}, "Improved Path-length Regret Bounds for Bandits": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Improved Path-length Regret Bounds for Bandits", "abstract": "We study adaptive regret bounds in terms of the variation of the losses (the so-called path-length bounds) for both multi-armed bandit and more generally linear bandit. We first show that the seemingly suboptimal path-length bound of (Wei and Luo, 2018) is in fact not improvable for adaptive adversary. Despite this negative result, we then develop two new algorithms, one that strictly improves over (Wei and Luo, 2018) with a smaller path-length measure, and the other which improves over (Wei and Luo, 2018) for oblivious adversary when the path-length is large. Our algorithms are based on the well-studied optimistic mirror descent framework, but importantly with several novel techniques, including new optimistic predictions, a slight bias towards recently selected arms, and the use of a hybrid regularizer similar to that of (Bubeck et al., 2018). Furthermore, we extend our results to linear bandit by showing a reduction to obtaining dynamic regret for a full-information problem, followed by a further reduction to convex body chasing. As a consequence we obtain new dynamic regret results as well as the first path-length regret bounds for general linear bandit.", "pdf_url": "http://proceedings.mlr.press/v99/bubeck19b/bubeck19b.pdf", "keywords": ["multi-armed bandit", "linear bandit", "path-length regret bound", "optimistic mirror descent", "dynamic regret", "convex body chasing"], "reference": "Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008.  Zeyuan Allen-Zhu, S\u00b4ebastien Bubeck, and Yuanzhi Li. Make the minority great again: First-order regret bound for contextual bandits. In International Conference on Machine Learning, 2018.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  research, 63(5):1227-1244, 2015.  S\u00b4ebastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self-  concordant barrier. In Conference on Learning Theory, 2015.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online  linear optimization with bandit feedback. In Conference on Learning Theory, 2012.  S\u00b4ebastien Bubeck, Michael B. Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-  armed bandits. In International Conference on Algorithmic Learning Theory, 2018.  Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other adaptive adversaries. In Advances in Neural Information Processing Systems, pages 1160-1168, 2013.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.  Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds.  In Conference on Learning Theory, pages 210-227, 2013.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning  in banach spaces. In Conference on Learning Theory, 2018.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Joel Friedman and Nathan Linial. On convex body chasing. Discrete & Computational Geometry,  9(3):293-321, 1993.  13   IMPROVED PATH-LENGTH REGRET BOUNDS FOR BANDITS  The authors would like to thank all the anonymous reviewers for their valuable comments. HL and CYW are supported by NSF Grant #1755781.  Acknowledgments  References  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008.  Zeyuan Allen-Zhu, S\u00b4ebastien Bubeck, and Yuanzhi Li. Make the minority great again: First-order regret bound for contextual bandits. In International Conference on Machine Learning, 2018.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  research, 63(5):1227-1244, 2015.  S\u00b4ebastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self-  concordant barrier. In Conference on Learning Theory, 2015.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online  linear optimization with bandit feedback. In Conference on Learning Theory, 2012.  S\u00b4ebastien Bubeck, Michael B. Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-  armed bandits. In International Conference on Algorithmic Learning Theory, 2018.  Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other adaptive adversaries. In Advances in Neural Information Processing Systems, pages 1160-1168, 2013.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.  Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds.  In Conference on Learning Theory, pages 210-227, 2013.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning  in banach spaces. In Conference on Learning Theory, 2018.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Joel Friedman and Nathan Linial. On convex body chasing. Discrete & Computational Geometry,  9(3):293-321, 1993. IMPROVED PATH-LENGTH REGRET BOUNDS FOR BANDITS  Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning  Research, 12(Apr):1287-1311, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza- tion: Competing with dynamic comparators. In Artificial Intelligence and Statistics, pages 398- 406, 2015.  Haipeng Luo, Chen-Yu Wei, and Kai Zheng. Efficient online portfolio with logarithmic regret. In  Advances in Neural Information Processing Systems, 2018.  Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization In 55th IEEE  in dynamic environments: Improved regret rates for strongly convex problems. Conference on Decision and Control, pages 7195-7201, 2016.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-  ence on Learning Theory, pages 993-1019, 2013.  Mark Sellke. Chasing convex bodies optimally. arXiv preprint arXiv:1905.11968, 2019.  Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient  algorithm. In International Conference on Machine Learning, pages 1593-1601, 2014.  Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference  on Learning Theory, 2018.  Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic environments. In Advances in Neural Information Processing Systems, pages 3972-3980, 2016.  Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Opti- mal dynamic regret of online learning with true and noisy gradient. In International Conference on Machine Learning, pages 449-457, 2016.  Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret In Advances in Neural Information Processing Systems, pages  for non-degenerate functions. 732-741, 2017.  Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In  Advances in Neural Information Processing Systems, pages 1330-1340, 2018.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  International Conference on Machine Learning, 2003. IMPROVED PATH-LENGTH REGRET BOUNDS FOR BANDITS  "}, "Optimal Learning of Mallows Block Model": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Optimal Learning of Mallows Block Model", "abstract": "The Mallows model, introduced in the seminal paper of Mallows 1957, is one of the most fundamental ranking distribution over the symmetric group $S_m$. To analyze more complex ranking data, several studies considered the Generalized Mallows model defined by Fligner and Verducci 1986. Despite the significant research interest of ranking distributions, the exact sample complexity of estimating the parameters of a Mallows and a Generalized Mallows Model is not well-understood. The main result of the paper is a tight sample complexity bound for learning Mallows and Generalized Mallows Model. We approach the learning problem by analyzing a more general model which interpolates between the single parameter Mallows Model and the $m$ parameter Mallows model. We call our model Mallows Block Model \u2013 referring to the Block Models that are a popular model in theoretical statistics. Our sample complexity analysis gives tight bound for learning the Mallows Block Model for any number of blocks. We provide essentially matching lower bounds for our sample complexity results. As a corollary of our analysis, it turns out that, if the central ranking is known, one single sample from the Mallows Block Model is sufficient to estimate the spread parameters with error that goes to zero as the size of the permutations goes to infinity. In addition, we calculate the exact rate of the parameter estimation error.", "pdf_url": "http://proceedings.mlr.press/v99/busa-fekete19a/busa-fekete19a.pdf", "keywords": ["Ranking distributions", "Mallows model", "Generalized Mallows", "Exponential family"], "reference": "Pranjal Awasthi, Avrim Blum, Or Sheffet, and Aravindan Vijayaraghavan. Learning mixtures of ranking models. In Advances in Neural Information Processing Systems, pages 2609-2617, 2014.  Quentin Berthet, Philippe Rigollet, and Piyush Srivastava. Exact recovery in the ising blockmodel.  arXiv preprint arXiv:1612.03880, 2016.  Ioannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. When do noisy votes reveal the truth?  ACM Transactions on Economics and Computation (TEAC), 4(3):15, 2016.  Harr Chen, S. R. K. Branavan, Regina Barzilay, and David R. Karger. Content modeling using latent  permutations. J. Artif. Intell. Res., 36:129-163, 2009.  Jean-Paul Doignon, Aleksandar Peke\u02c7c, and Michel Regenwetter. The repeated insertion model for rankings: Missing link between two subset choice models. Psychometrika, 69(1):33-54, 2004.  Michael A Fligner and Joseph S Verducci. Distance based ranking models. Journal of the Royal  Statistical Society. Series B (Methodological), pages 359-369, 1986.  Olga Klopp, Alexandre B Tsybakov, Nicolas Verzelen, et al. Oracle inequalities for network models  and sparse graphon estimation. The Annals of Statistics, 45(1):316-354, 2017.  Allen Liu and Ankur Moitra. Efficiently learning mixtures of Mallows models. In FOCS, pages  627-638. IEEE Computer Society, 2018.  Tyler Lu and Craig Boutilier. Learning Mallows models with pairwise preferences. In Proceedings  of the 28th International Conference on Machine Learning,, pages 145-152, 2011.  C. Mallows. Non-null ranking models. Biometrika, 44(1):114-130, 1957.  John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall, 1995.  Marina Meila and Le Bao. An exponential model for infinite rankings. Journal of Machine Learning  Research, 11:3481-3518, 2010.  (2):853-875, 2016.  Sumit Mukherjee. Estimation in exponential families on permutations. The Annals of Statistics, 44  4   Interestingly, our lower bound uses a general way to compute the total variation distance of two distributions that belong to the same exponential family. This theorem states that the total variation of two distributions in the same exponential family is equal to the distance between their parameters times the absolute deviation of a corresponding distribution in the family. This should be compared with the expression of the KL-divergence between two distributions in the same exponential fam- ily. Then our lower bound boils down to showing that for some range of parameters, the absolute deviation is within a constant from the standard deviation. With this proven, we get that the total variation distance is within a constant factor from the square root of the KL-divergence, and Fano\u2019s inequality can be applied.  References  Pranjal Awasthi, Avrim Blum, Or Sheffet, and Aravindan Vijayaraghavan. Learning mixtures of ranking models. In Advances in Neural Information Processing Systems, pages 2609-2617, 2014.  Quentin Berthet, Philippe Rigollet, and Piyush Srivastava. Exact recovery in the ising blockmodel.  arXiv preprint arXiv:1612.03880, 2016.  Ioannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. When do noisy votes reveal the truth?  ACM Transactions on Economics and Computation (TEAC), 4(3):15, 2016.  Harr Chen, S. R. K. Branavan, Regina Barzilay, and David R. Karger. Content modeling using latent  permutations. J. Artif. Intell. Res., 36:129-163, 2009.  Jean-Paul Doignon, Aleksandar Peke\u02c7c, and Michel Regenwetter. The repeated insertion model for rankings: Missing link between two subset choice models. Psychometrika, 69(1):33-54, 2004.  Michael A Fligner and Joseph S Verducci. Distance based ranking models. Journal of the Royal  Statistical Society. Series B (Methodological), pages 359-369, 1986.  Olga Klopp, Alexandre B Tsybakov, Nicolas Verzelen, et al. Oracle inequalities for network models  and sparse graphon estimation. The Annals of Statistics, 45(1):316-354, 2017.  Allen Liu and Ankur Moitra. Efficiently learning mixtures of Mallows models. In FOCS, pages  627-638. IEEE Computer Society, 2018.  Tyler Lu and Craig Boutilier. Learning Mallows models with pairwise preferences. In Proceedings  of the 28th International Conference on Machine Learning,, pages 145-152, 2011.  C. Mallows. Non-null ranking models. Biometrika, 44(1):114-130, 1957.  John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall, 1995.  Marina Meila and Le Bao. An exponential model for infinite rankings. Journal of Machine Learning  Research, 11:3481-3518, 2010.  (2):853-875, 2016.  Sumit Mukherjee. Estimation in exponential families on permutations. The Annals of Statistics, 44 "}, "Gaussian Process Optimization with Adaptive Sketching: Scalable and No Regret": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Gaussian Process Optimization with Adaptive Sketching: Scalable and No Regret", "abstract": "Gaussian processes (GP) are a stochastic processes, used  as Bayesian approach for the optimization of black-box functions. Despite their effectiveness in simple problems, GP-based algorithms hardly scale to high-dimensional functions, as their per-iteration time and space cost is at least \\emph{quadratic} in the number of dimensions $d$ and iterations\u00a0$t$. Given a set of $A$ alternatives to choose from, the overall runtime $O(t^3 A)$ is prohibitive.  In this paper, we introduce BKB (\\textit{budgeted kernelized bandit}), a new approximate GP algorithm for optimization under bandit feedback that achieves near-optimal regret (and hence near-optimal convergence rate) with near-constant per-iteration complexity and remarkably no assumption on the input space or covariance of the GP. We combine a kernelized linear bandit algorithm (GP-UCB)  leverage score sampling as a randomized matrix sketching and  prove that selecting inducing points based on their posterior variance gives an accurate low-rank approximation of the GP, preserving variance estimates and confidence intervals. As a consequence, BKB does not suffer from \\emph{variance starvation}, an important problem faced by many previous sparse GP approximations. Moreover, we show that our procedure selects at most $\\widetilde{O}(d_{eff})$ points, where $d_{eff}$ is the \\emph{effective} dimension of the explored space, which is typically much smaller than both $d$ and $t$. This greatly reduces the dimensionality of the problem, thus leading to a $O(T A d_{eff}^2)$ runtime and $O(A d_{eff})$ space complexity.", "pdf_url": "http://proceedings.mlr.press/v99/calandriello19a/calandriello19a.pdf", "keywords": ["sparse Gaussian process optimization", "kernelized linear bandits", "regret", "sketching", "Bayesian optimization", "black-box optimization", "variance starvation"], "reference": "Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic  bandits. In Neural Information Processing Systems, 2011.  Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical  guarantees. In Neural Information Processing Systems, 2015.  Daniele Calandriello and Lorenzo Rosasco. Statistical and computational trade-offs in kernel k-means.  In Neural Information Processing Systems, 2018.  Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Distributed adaptive sampling for kernel matrix approximation. In International Conference on Artificial Intelligence and Statistics, 2017a.  Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Second-order kernel online convex optimization with adaptive sketching. In International Conference on Machine Learning, 2017b.  Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International  Conference on Machine Learning, 2017.  Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized Markov decision processes.  In International Conference on Artificial Intelligence and Statistics, 2019.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit  feedback. In Conference on Learning Theory, 2008.  Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P. Woodruff. Frequent directions: Simple  and deterministic matrix sketching. The SIAM Journal of Computing, pages 1-28, 2016.  Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Misspecified linear bandits. In AAAI  Conference on Artificial Intelligence, 2017.  Elad Hazan, Adam Tauman Kalai, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms  for online convex optimization. In Conference on Learning Theory, 2006.  Jonathan H. Huggins, Trevor Campbell, Miko\u0142aj Kasprzak, and Tamara Broderick. Scalable Gaussian process inference with finite-data mean and variance guarantees. In International Conference on Artificial Intelligence and Statistics, 2019.  13   BKB  Acknowledgements This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390, and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS- DLV-777826. The research was also supported by European CHIST-ERA project DELTA, French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto- von-Guericke-Universit\u00e4t Magdeburg associated-team north-European project Allocate, and French National Research Agency project BoB (n.ANR-16-CE23-0003). This research has also benefited from the support of the FMJH Program PGMO and from the support to this program from Criteo.  References  Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic  bandits. In Neural Information Processing Systems, 2011.  Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical  guarantees. In Neural Information Processing Systems, 2015.  Daniele Calandriello and Lorenzo Rosasco. Statistical and computational trade-offs in kernel k-means.  In Neural Information Processing Systems, 2018.  Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Distributed adaptive sampling for kernel matrix approximation. In International Conference on Artificial Intelligence and Statistics, 2017a.  Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Second-order kernel online convex optimization with adaptive sketching. In International Conference on Machine Learning, 2017b.  Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International  Conference on Machine Learning, 2017.  Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized Markov decision processes.  In International Conference on Artificial Intelligence and Statistics, 2019.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit  feedback. In Conference on Learning Theory, 2008.  Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P. Woodruff. Frequent directions: Simple  and deterministic matrix sketching. The SIAM Journal of Computing, pages 1-28, 2016.  Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Misspecified linear bandits. In AAAI  Conference on Artificial Intelligence, 2017.  Elad Hazan, Adam Tauman Kalai, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms  for online convex optimization. In Conference on Learning Theory, 2006.  Jonathan H. Huggins, Trevor Campbell, Miko\u0142aj Kasprzak, and Tamara Broderick. Scalable Gaussian process inference with finite-data mean and variance guarantees. In International Conference on Artificial Intelligence and Statistics, 2019. BKB  Ilja Kuzborskij, Leonardo Cella, and Nicol\u00f2 Cesa-Bianchi. Efficient linear bandits through matrix  sketching. In International Conference on Artificial Intelligence and Statistics, 2019.  Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. 2019.  Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. International World Wide Web Conference, 2010.  Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When Gaussian process meets big data:  A Review of scalable GPs. Technical report, 2018.  Jonas Mockus. Global optimization and the Bayesian approach. 1989.  Mojm\u00edr Mutn\u00fd and Andreas Krause. Efficient high-dimensional Bayesian optimization with additivity  and quadrature Fourier features. In Neural Information Processing Systems, 2018.  Martin Pelikan. Hierarchical Bayesian optimization algorithm. In Studies in Fuzziness and Soft  Computing, pages 105-129. 2005.  Joaquin Quinonero-Candela, Carl Edward Rasmussen, and Christopher K. I. Williams. Approxi- mation methods for gaussian process regression. Large-scale kernel machines, pages 203-224, 2007.  Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In Neural Information  Processing Systems, 2007.  MIT Press, 2006.  Carl Edward. Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American  Mathematics Society, 58:527-535, 1952.  Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher. Lower bounds on regret for noisy Gaussian  process bandit optimization. In Conference on Learning Theory, 2017.  Matthias Seeger, Christopher Williams, and Neil Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In International Conference on Artificial Intelligence and Statistics, 2003.  Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine  learning algorithms. In Neural Information Processing Systems, 2012.  Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. International Conference on Machine Learning, 2010.  Joel Aaron Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in  Machine Learning, 8(1-2):1-230, 2015.  Michal Valko, Nathan Korda, R\u00e9mi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis  of kernelised contextual bandits. In Uncertainty in Artificial Intelligence, 2013. Grace Wahba. Spline models for observational data. Society for Industrial and Applied Mathematics,  1990.  Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched Large-scale Bayesian Optimization in High-dimensional Spaces. In International Conference on Artificial Intelligence and Statistics, 2018.  Xiaotian Yu, Michael R. Lyu, and Irwin King. CBRAP: Contextual bandits with random projection.  In AAAI Conference on Artificial Intelligence, 2017.  BKB "}, "Disagreement-Based Combinatorial Pure Exploration: Sample Complexity Bounds and an Efficient Algorithm": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Disagreement-Based Combinatorial Pure Exploration: Sample Complexity Bounds and an Efficient Algorithm", "abstract": "We design new algorithms for the combinatorial pure exploration problem in the multi-arm bandit framework. In this problem, we are given $K$ distributions and a collection of subsets $\\mathcal{V} \\subset 2^{[K]}$ of these distributions, and we would like to find the subset $v \\in \\mathcal{V}$ that has largest mean, while collecting, in a sequential fashion, as few samples from the distributions as possible. In both the fixed budget and fixed confidence settings, our algorithms achieve new sample-complexity bounds that provide polynomial improvements on previous results in some settings. Via an information-theoretic lower bound, we show that no approach based on uniform sampling can improve on ours in any regime, yielding the first interactive algorithms for this problem with this basic property.  Computationally, we show how to efficiently implement our fixed confidence algorithm whenever $\\mathcal{V}$ supports efficient linear optimization.  Our results involve precise concentration-of-measure arguments and a new algorithm for linear programming with exponentially many constraints.", "pdf_url": "http://proceedings.mlr.press/v99/cao19a/cao19a.pdf", "keywords": ["Interactive learning", "Bandits", "Combinatorial optimization"], "reference": "Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 2016.  Louigi Addario-Berry, Nicolas Broutin, Luc Devroye, and G\u00b4abor Lugosi. On combinatorial testing  problems. The Annals of Statistics, 2010.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, 2014.  Andr\u00b4as Antos, Varun Grover, and Csaba Szepesv\u00b4ari. Active learning in heteroscedastic noise. The-  oretical Computer Science, 2010.  Ery Arias-Castro and Emmanuel J. Cand`es. Searching for a trail of evidence in a maze. The Annals  of Statistics, 2008.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  Conference on Learning Theory, 2010.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Advances in Neural Information Processing Systems, 2011.  S\u00b4ebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, 2013.  Cristina Butucea and Yuri I. Ingster. Detection of a sparse submatrix of a high-dimensional noisy  matrix. Bernoulli, 2013.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm  identification bandit problem. In Conference on Learning Theory, 2016.  Rui Castro and Ervin T\u00b4anczos. Adaptive sensing for estimation of structured sparse signals. IEEE  Transactions on Information Theory, 2015.  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid  constraints. In Conference on Learning Theory, 2016.  Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling  algorithms for combinatorial pure exploration. In Conference on Learning Theory, 2017.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- ration of multi-armed bandits. In Advances in Neural Information Processing Systems, 2014.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. Journal of Machine Learning Research, 2016.  David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Ma-  chine Learning, 1994.  13   DISAGREEMENT-BASED COMBINATORIAL EXPLORATION  References  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 2016.  Louigi Addario-Berry, Nicolas Broutin, Luc Devroye, and G\u00b4abor Lugosi. On combinatorial testing  problems. The Annals of Statistics, 2010.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, 2014.  Andr\u00b4as Antos, Varun Grover, and Csaba Szepesv\u00b4ari. Active learning in heteroscedastic noise. The-  oretical Computer Science, 2010.  Ery Arias-Castro and Emmanuel J. Cand`es. Searching for a trail of evidence in a maze. The Annals  of Statistics, 2008.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  Conference on Learning Theory, 2010.  Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise Thresholds for  Spectral Clustering. In Advances in Neural Information Processing Systems, 2011.  S\u00b4ebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, 2013.  Cristina Butucea and Yuri I. Ingster. Detection of a sparse submatrix of a high-dimensional noisy  matrix. Bernoulli, 2013.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm  identification bandit problem. In Conference on Learning Theory, 2016.  Rui Castro and Ervin T\u00b4anczos. Adaptive sensing for estimation of structured sparse signals. IEEE  Transactions on Information Theory, 2015.  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid  constraints. In Conference on Learning Theory, 2016.  Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling  algorithms for combinatorial pure exploration. In Conference on Learning Theory, 2017.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- ration of multi-armed bandits. In Advances in Neural Information Processing Systems, 2014.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. Journal of Machine Learning Research, 2016.  David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Ma-  chine Learning, 1994. CAO KRISHNAMURTHY  Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni. A general agnostic active learning algorithm.  In Advances in Neural Information Processing Systems, 2007.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 2006.  Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett. Improved learning complexity in combinatorial pure exploration bandits. In Artificial Intelligence and Statistics, 2016.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In  Conference on Learning Theory, 2016.  Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in Machine  Daniel Hsu. Algorithms for Active Learning. PhD thesis, University of California at San Diego,  Learning, 2014.  2010.  Tzu-Kuo Huang, Alekh Agarwal, Daniel Hsu, John Langford, and Robert E. Schapire. Efficient and parsimonious agnostic active learning. In Advances in Neural Information Processing Systems, 2015.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 2005.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in  stochastic multi-armed bandits. In International Conference on Machine Learning, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.  In International Conference on Machine Learning, 2013.  Zohar S Karnin. Verification based solution for structured mab problems. In Advances in Neural  Information Processing Systems, 2016.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of a/b testing.  In  Conference on Learning Theory, 2014.  Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, and Aarti Singh. Minimax localization of structural information in large noisy matrices. Advances in Neural Information Processing Systems, 2011.  Akshay Krishnamurthy. Minimax structured normal means inference. IEEE International Sympo-  sium on Information Theory, 2016.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. Journal of Machine Learning Research, 2004. DISAGREEMENT-BASED COMBINATORIAL EXPLORATION  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction and optimal  recovery of block models. In Conference on Learning Theory, 2014.  Serge A. Plotkin, David B. Shmoys, and \u00b4Eva Tardos. Fast approximation algorithms for fractional  packing and covering problems. Mathematics of Operations Research, 1995.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex-  tual bandits. In International Conference on Machine Learning, 2016.  Daniel Russo. Simple bayesian algorithms for best arm identification. In Conference on Learning  Theory, 2016.  Max Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adaptive  sampling in the moderate-confidence regime. In Conference on Learning Theory, 2017.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems, 2016.  Shu Wang, Robin R Gutell, and Daniel P Miranker. Biclustering as a method for rna local multiple  sequence alignment. Bioinformatics, 2007.  "}, "A Rank-1 Sketch for Matrix Multiplicative Weights": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A Rank-1 Sketch for Matrix Multiplicative Weights", "abstract": "We show that a simple randomized sketch of the matrix  multiplicative weight (MMW) update enjoys (in expectation) the same  regret bounds as MMW, up to a small constant factor. Unlike MMW, where  every step requires full matrix exponentiation, our steps require only a  single product of the form $e^A b$, which the Lanczos method  approximates efficiently. Our key technique is to view the sketch as a  \\emph{randomized mirror projection}, and perform mirror descent analysis  on the \\emph{expected projection}. Our sketch solves the online  eigenvector problem, improving the best known complexity bounds by  $\\Omega(\\log^5 n)$. We also apply this sketch to semidefinite  programming in saddle-point form, yielding a simple primal-dual scheme  with guarantees matching the best in the literature.", "pdf_url": "http://proceedings.mlr.press/v99/carmon19a/carmon19a.pdf", "keywords": ["Online learning", "spectrahedron", "matrix exponential", "Lanczos method", "mirror descent"], "reference": "YC was supported by the Stanford Graduate Fellowship. JCD was supported by the NSF CAREER award 1553086, the Sloan Foundation and ONR-YIP N00014-19-1-2288. AS was supported by the NSF CAREER Award CCF-1844855. KT was supported by the NSF Graduate Fellowship DGE- 1656518.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- In Proceedings of the 34th International Conference on Machine  vectors and faster mmwu. Learning, 2017.  Zeyuan Allen-Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive sdp solver. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Math- ematics, 2016.  Horst Alzer. On some inequalities for the gamma and psi functions. Mathematics of Computation  of the American Mathematical Society, 66(217):373-389, 1997.  S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta algorithm and  applications. Theory of Computing, 8(1):121-164, 2012.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the Thirty-Ninth Annual ACM Symposium on the Theory of Computing. ACM, 2007.  K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal,  68:357-367, 1967.  Michel Baes, Michael B\u00fcrgisser, and Arkadi Nemirovski. A randomized mirror-prox method for solving structured large-scale matrix saddle-point problems. SIAM Journal on Optimization, 23 (2):934-962, 2013.  S\u00e9bastien Bubeck and Nicol\u00f3 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning  algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, September 2004.  Michael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub W. Pachocki, and Aaron Sidford. Geometric  median in nearly linear time. arXiv:1606.05225 [cs.DS], 2016.  Alexandre d\u2019Aspremont. Subsampling algorithms for semidefinite programming. Stochastic Sys-  tems, 1(2):209-436, 2011.  Vladimir Druskin and Leonid Knizhnerman. Error bounds in the simple Lanczos procedure for computing functions of symmetric matrices and eigenvalues. U.S.S.R. Computational Mathemat- ics and Mathematical Physics, 31(7):970-983, 1991.  13   A RANK-1 SKETCH FOR MATRIX MULTIPLICATIVE WEIGHTS  Acknowledgments  References  YC was supported by the Stanford Graduate Fellowship. JCD was supported by the NSF CAREER award 1553086, the Sloan Foundation and ONR-YIP N00014-19-1-2288. AS was supported by the NSF CAREER Award CCF-1844855. KT was supported by the NSF Graduate Fellowship DGE- 1656518.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- In Proceedings of the 34th International Conference on Machine  vectors and faster mmwu. Learning, 2017.  Zeyuan Allen-Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive sdp solver. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Math- ematics, 2016.  Horst Alzer. On some inequalities for the gamma and psi functions. Mathematics of Computation  of the American Mathematical Society, 66(217):373-389, 1997.  S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta algorithm and  applications. Theory of Computing, 8(1):121-164, 2012.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the Thirty-Ninth Annual ACM Symposium on the Theory of Computing. ACM, 2007.  K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal,  68:357-367, 1967.  Michel Baes, Michael B\u00fcrgisser, and Arkadi Nemirovski. A randomized mirror-prox method for solving structured large-scale matrix saddle-point problems. SIAM Journal on Optimization, 23 (2):934-962, 2013.  S\u00e9bastien Bubeck and Nicol\u00f3 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning  algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, September 2004.  Michael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub W. Pachocki, and Aaron Sidford. Geometric  median in nearly linear time. arXiv:1606.05225 [cs.DS], 2016.  Alexandre d\u2019Aspremont. Subsampling algorithms for semidefinite programming. Stochastic Sys-  tems, 1(2):209-436, 2011.  Vladimir Druskin and Leonid Knizhnerman. Error bounds in the simple Lanczos procedure for computing functions of symmetric matrices and eigenvalues. U.S.S.R. Computational Mathemat- ics and Mathematical Physics, 31(7):970-983, 1991. A RANK-1 SKETCH FOR MATRIX MULTIPLICATIVE WEIGHTS  Vladimir Druskin and Leonid Knizhnerman. Krylov subspace approximation of eigenpairs and matrix functions in exact and computer arithmetic. Numerical Linear Algebra with Applications, 2(3):205-217, 1995.  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze Gauss: optimal bounds for privacy-preserving principal component analysis. In Proceedings of the Forty-Sixth Annual ACM Symposium on the Theory of Computing. ACM, 2014.  Dan Garber and Elad Hazan. Sublinear time algorithms for approximate semidefinite programming.  Math. Program., 158(1-2):329-361, 2016.  Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings of the  32nd International Conference on Machine Learning, 2015.  Ming Gu and Stanley C. Eisenstat. A divide-and-conquer algorithm for the symmetric tridiagonal  eigenproblem. SIAM Journal on Matrix Analysis and Applications, 16(1):172-191, 1995.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,  2(3-4):157-325, 2016.  New York, 1993.  2010.  J. Hiriart-Urruty and C. Lemar\u00e9chal. Convex Analysis and Minimization Algorithms I & II. Springer,  Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19:209-286,  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58(301):13-30, March 1963.  Arun Jambulapati, Kirankumar Shiragur, and Aaron Sidford. Efficient structured matrix recovery and nearly-linear time algorithms for solving inverse symmetric M-matrices. arXiv:1812.06295 [cs.DS], 2018.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71(3):291-307, 2005.  Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear dif- ferential and integral operators. Journal of Research of the National Bureau of Standards, 45(4), 1950.  Adrian Lewis. Convex analysis on the Hermitian matrices. SIAM Journal on Optimization, 6:  164-177, 1996.  Adrian S. Lewis and Hristo S. Sendov. Twice differentiable spectral functions. SIAM Journal on  Matrix Analysis and Applications, 23(2):368-386, 2001.  G\u00e9rard Meurant. The Lanczos and Conjugate Gradient Algorithms: From Theory to Finite Precision  Computations. Society for Industrial and Applied Mathematics, 2006.  Cleve B. Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a  matrix, twenty-five years later. SIAM Review, 45(1):3-49, 2003. A RANK-1 SKETCH FOR MATRIX MULTIPLICATIVE WEIGHTS  Cameron Musco, Christopher Musco, and Aaron Sidford. Stability of the Lanczos method for  matrix function approximation. arXiv:1708.07788 [cs.DS], 2017.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229-251, 2004.  Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming,  120(1):261-283, 2009.  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Mathemati-  cal Programming, Series A, 110:245-259, 2007.  Jiazhong Nie, Wojciech Kot\u0142owski, and Manfred K Warmuth. Online PCA with optimal regrets. In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, 2013.  Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K. Vishnoi. Approximating the exponential, the lanczos method and an \u00f5(m)-time spectral algorithm for balanced separator. In Proceedings of the Forty-Fourth Annual ACM Symposium on the Theory of Computing, 2012.  Victor Y Pan and Zhao Q Chen. The complexity of the matrix eigenproblem. In Proceedings of the  Thirty-First Annual ACM Symposium on the Theory of Computing. ACM, 1999.  Richard Peng, Kanat Tangwongsan, and Peng Zhang. Faster and simpler width-independent parallel  algorithms for positive semidefinite programming. arXiv:1201.5135v3 [cs.DS], 2016.  Yousef Saad. Analysis of some Krylov subspace approximations to the matrix exponential operator.  SIAM Journal on Numerical Analysis, 29(1):209-228, 1992.  Sushant Sachdeva and Nisheeth K. Vishnoi. Faster algorithms via approximation theory. Founda-  tions and Trends in Theoretical Computer Science, 9(2):125-210, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012.  Koji Tsuda, Gunnar R\u00e4tsch, and Manfred K Warmuth. Matrix exponentiated gradient updates for on-line learning and bregman projection. Journal of Machine Learning Research, 6:995-1018, 2005.  Lieven Vandenberghe, Martin S Andersen, et al. Chordal graphs and semidefinite optimization.  Foundations and Trends\u00ae in Optimization, 1(4):241-433, 2015.  Manfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension. Journal of Machine Learning Research, 9:2287-2320, 2008.  Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. Machine Learning, 87(1):  1-32, 2012. A RANK-1 SKETCH FOR MATRIX MULTIPLICATIVE WEIGHTS  "}, "On the Computational Power of Online Gradient Descent": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "On the Computational Power of Online Gradient Descent", "abstract": "We prove that the evolution of weight vectors in online gradient descent can encode arbitrary polynomial-space computations, even in very simple learning settings. Our results imply that, under weak complexity-theoretic assumptions, it is impossible to reason efficiently about the fine-grained behavior of online gradient descent.", "pdf_url": "http://proceedings.mlr.press/v99/chatziafratis19a/chatziafratis19a.pdf", "keywords": ["Stochastic Gradient Descent", "Complexity Theory", "PSPACE hardness", "SVMs", "reduction"], "reference": "Ilan Adler, Christos Papadimitriou, and Aviad Rubinstein. On simplex pivoting rules and complexity theory. In International Conference on Integer Programming and Combinatorial Optimization, pages 13-24. Springer, 2014.  Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, con- verges to limit cycles for deep networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyWrIgW0W.  Yann Disser and Martin Skutella. The simplex algorithm is np-mighty. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 858-872. Society for Industrial and Applied Mathematics, 2015.  John Fearnley and Rahul Savani. The complexity of the simplex method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 201-208. ACM, 2015.  Paul W Goldberg, Christos H Papadimitriou, and Rahul Savani. The complexity of the homotopy method, equilibrium selection, and lemke-howson solutions. ACM Transactions on Economics and Computation, 1(2):9, 2013.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization, 2(3-4):157-325, 2016. ISSN 2167-3888. doi: 10.1561/2400000013. URL http://dx.doi. org/10.1561/2400000013.  David S Johnson, Christos H Papadimitriou, and Mihalis Yannakakis. How easy is local search?  Journal of computer and system sciences, 37(1):79-100, 1988.  Christos H Papadimitriou and Nisheeth K Vishnoi. On the computational complexity of limit cycles in dynamical systems. In Itcs\" 16: Proceedings Of The 2016 Acm Conference On Innovations In Theoretical Computer Science, pages 403-403. Assoc Computing Machinery, 2016.  Tim Roughgarden and Joshua R Wang. The complexity of the k-means method. In LIPIcs-Leibniz International Proceedings in Informatics, volume 57. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to  algorithms. Cambridge university press, 2014.  Michael Sipser. Introduction to the Theory of Computation, volume 2. Thomson Course Technology,  2006.  1983.  James A Storer. On the complexity of chess. Journal of computer and system sciences, 27(1):77-100,  Kees Van Den Doel and Uri Ascher. The chaotic nature of faster gradient descent methods. Journal  of Scientific Computing, 51(3):560-581, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003.  13   ON THE COMPUTATIONAL POWER OF ONLINE GRADIENT DESCENT  References  Ilan Adler, Christos Papadimitriou, and Aviad Rubinstein. On simplex pivoting rules and complexity theory. In International Conference on Integer Programming and Combinatorial Optimization, pages 13-24. Springer, 2014.  Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, con- verges to limit cycles for deep networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyWrIgW0W.  Yann Disser and Martin Skutella. The simplex algorithm is np-mighty. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 858-872. Society for Industrial and Applied Mathematics, 2015.  John Fearnley and Rahul Savani. The complexity of the simplex method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 201-208. ACM, 2015.  Paul W Goldberg, Christos H Papadimitriou, and Rahul Savani. The complexity of the homotopy method, equilibrium selection, and lemke-howson solutions. ACM Transactions on Economics and Computation, 1(2):9, 2013.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization, 2(3-4):157-325, 2016. ISSN 2167-3888. doi: 10.1561/2400000013. URL http://dx.doi. org/10.1561/2400000013.  David S Johnson, Christos H Papadimitriou, and Mihalis Yannakakis. How easy is local search?  Journal of computer and system sciences, 37(1):79-100, 1988.  Christos H Papadimitriou and Nisheeth K Vishnoi. On the computational complexity of limit cycles in dynamical systems. In Itcs\" 16: Proceedings Of The 2016 Acm Conference On Innovations In Theoretical Computer Science, pages 403-403. Assoc Computing Machinery, 2016.  Tim Roughgarden and Joshua R Wang. The complexity of the k-means method. In LIPIcs-Leibniz International Proceedings in Informatics, volume 57. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to  algorithms. Cambridge university press, 2014.  Michael Sipser. Introduction to the Theory of Computation, volume 2. Thomson Course Technology,  2006.  1983.  James A Storer. On the complexity of chess. Journal of computer and system sciences, 27(1):77-100,  Kees Van Den Doel and Uri Ascher. The chaotic nature of faster gradient descent methods. Journal  of Scientific Computing, 51(3):560-581, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003. ON THE COMPUTATIONAL POWER OF ONLINE GRADIENT DESCENT  "}, "Active Regression via Linear-Sample Sparsification": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Active Regression via Linear-Sample Sparsification", "abstract": "We present an approach that improves the sample complexity for a variety of curve fitting problems, including active learning for linear regression, polynomial regression, and continuous sparse Fourier transforms.  In the active linear regression problem, one would like to estimate the least squares solution $\\beta^*$ minimizing $\\|X\\beta - y\\|_2$ given the entire unlabeled dataset $X \\in \\mathbb{R}^{n \\times d}$ but only observing a small number of labels $y_i$.  We show that $O(d)$ labels suffice to find a constant factor approximation $\\widetilde{\\beta}$: \\[ \\mathbb{E}[\\|{X} \\widetilde{\\beta} - y \\|_2^2] \\leq 2 \\mathbb{E}[\\|X \\beta^* - y\\|_2^2]. \\]{This} improves on the best previous result of $O(d \\log d)$ from leverage score sampling.  We also present results for the \\emph{inductive} setting, showing when $\\widetilde{\\beta}$ will generalize to fresh samples; these apply to continuous settings such as polynomial regression.  Finally, we show how the techniques yield improved results for the non-linear sparse Fourier transform setting.", "pdf_url": "http://proceedings.mlr.press/v99/chen19a/chen19a.pdf", "keywords": ["Linear regression", "active regression", "leversage score", "spectral sparsification", "sparse Fourier transform"], "reference": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.  Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta- tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 253-262, 2017.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE transactions on information theory, 59(10):6880-6892, 2013.  Y. Bresler and A. Macovski. Exact maximum likelihood parameter estimation of superimposed exponential signals in noise. IEEE Transactions on Acoustics, Speech, and Signal Processing, 34 (5):1081-1089, Oct 1986. ISSN 0096-3518. doi: 10.1109/TASSP.1986.1164949.  Kamalika Chaudhuri, Sham M Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of active learning for maximum likelihood estimation. In Advances in Neural Information Processing Systems, pages 1090-1098, 2015.  Xue Chen, Daniel M. Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a  frequency gap. In FOCS 2016, 2016.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, 23:493-507, 1952.  11   ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  coefficients and l = \u02dcO(k2). This upper bounds |f (\u22121)|2 in terms of |f (\u22121 + \u2206)|2 + \u00b7 \u00b7 \u00b7 + |f (\u22121 + l \u00b7 \u2206)|2 and then |f (\u22121)|2/(cid:107)f (cid:107)2 D by integrating \u2206 from 0 to 2/l.  The improvement of Theorem 4 contains two steps. In the first step, we show that f (x) can be expressed as a constant-coefficient linear combination of the elements of an O(k)-length arithmetic sequence on both sides of x, namely, {f (x\u22122k \u00b7\u2206), . . . , f (x+2k \u00b7\u2206)}\\f (x). This is much shorter than the (cid:101)O(k2) elements required by Chen et al. (2016) for the one-sided version, and provides an \u02dcO(k2) factor improvement. Next we find k such linear combinations that are almost orthogonal to each other to remove the extra k factor. These two let us show that  |f (x)|2 (cid:107)f (cid:107)2 D  sup f \u2208F  = O  (cid:19)  (cid:18) k log k 1 \u2212 |x|  for any x \u2208 (\u22121, 1). This leads to \u03ba = O(k log2 k), which appears in Theorem 32 of Section G.  The authors would like to thank Adam Klivans and David Zuckerman for many helpful comments about this work.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. arXiv preprint arXiv:1711.05174, 2017.  Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta- tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 253-262, 2017.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE transactions on information theory, 59(10):6880-6892, 2013.  Y. Bresler and A. Macovski. Exact maximum likelihood parameter estimation of superimposed exponential signals in noise. IEEE Transactions on Acoustics, Speech, and Signal Processing, 34 (5):1081-1089, Oct 1986. ISSN 0096-3518. doi: 10.1109/TASSP.1986.1164949.  Kamalika Chaudhuri, Sham M Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of active learning for maximum likelihood estimation. In Advances in Neural Information Processing Systems, pages 1090-1098, 2015.  Xue Chen, Daniel M. Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a  frequency gap. In FOCS 2016, 2016.  Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. The Annals of Mathematical Statistics, 23:493-507, 1952. ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  Albert Cohen, Mark A Davenport, and Dany Leviatan. On the stability and accuracy of least squares  approximations. Foundations of computational mathematics, 13(5):819-834, 2013.  Michal Derezinski and Manfred K Warmuth. Unbiased estimates for linear regression via volume sampling. In Advances in Neural Information Processing Systems, pages 3087-3096, 2017.  Michal Derezinski, Manfred K Warmuth, and Daniel Hsu. Tail bounds for volume sampled linear  regression. arXiv preprint arXiv:1802.06749, 2018.  Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decomposi-  tions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.  Robert Fano. Transmission of information; a statistical theory of communications. Cambridge,  Massachusetts, M.I.T. Press, 1961.  Ralph Hartley. Transmission of information. Bell System Technical Journal, 1928.  Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The  Journal of Machine Learning Research, 17(1):543-582, 2016.  Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time. In Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), FOCS \u201915, pages 250-269. IEEE Computer Society, 2015.  Malik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutative bernstein  bound. arXiv preprint arXiv:1008.0587, 2010.  Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R(cid:13) in  Machine Learning, 3(2):123-224, 2011.  Ankur Moitra. The threshold for super-resolution via extremal functions. In STOC, 2015.  H.L. Montgomery and R.C. Vaughan. Hilbert\u2019s inequality. Journal of the London Mathematical  Society, s2-8(1):73-82, 1974.  Holger Rauhut and Rachel Ward. Sparse legendre expansions via (cid:96)1-minimization. Journal of  approximation theory, 164(5):517-533, 2012.  Sivan Sabato and Remi Munos. Active regression by stratification. In Advances in Neural Informa-  tion Processing Systems, pages 469-477, 2014.  Claude Shannon. Communication in the presence of noise. Proc. Institute of Radio Engineers, 37  (1):10-21, 1949.  Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2772-2789. SIAM, 2019.  Robert E. Tarjan. Lecture 10: More chernoff bounds, sampling, and the chernoff + union bound.  Princeton Class Notes, Probability and Computing, pages 1-9, 2009. ACTIVE REGRESSION VIA LINEAR-SAMPLE SPARSIFICATION  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12:389-434, 2012.  Rachel Ward. Importance sampling in signal processing applications. In Excursions in Harmonic  Analysis, Volume 4, pages 205-228. Springer, 2015.  David P Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,  2014.  "}, "A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal and Parameter-free": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal and Parameter-free", "abstract": "We propose the first contextual bandit algorithm that is parameter-free, efficient, and optimal in terms of dynamic regret. Specifically, our algorithm achieves $\\mathcal{O}(\\min\\{\\sqrt{KST}, K^{\\frac{1}{3}}\\Delta ^{\\frac{1}{3}}T^{\\frac{2}{3}}\\})$ dynamic regret for a contextual bandit problem with $T$ rounds, $K$ actions, $S$ switches and $\\Delta$ total variation in data distributions. Importantly, our algorithm is adaptive and does not need to know $S$ or $\\Delta$ ahead of time, and can be implemented efficiently assuming access to an ERM oracle. Our results strictly improve the $\\mathcal{O} (\\min \\{S^{\\frac{1}{4}}T^{\\frac{3}{4}}, \\Delta^{\\frac{1}{5}}T^{\\frac{4}{5}}\\})$ bound of (Luo et al., 2018), and greatly generalize and improve the $\\mathcal{O}(\\sqrt{ST})$ result of (Auer et al., 2018) that holds only for the two-armed bandit problem without contextual information. The key novelty of our algorithm is to introduce {\\it replay phases}, in which the algorithm acts according to its previous decisions for a certain amount of time in order to detect non-stationarity while maintaining a good balance between exploration and exploitation.", "pdf_url": "http://proceedings.mlr.press/v99/chen19b/chen19b.pdf", "keywords": ["contextual bandit", "non-stationarity", "optimal dynamic regret", "oracle-efficiency", "parameterfree", "replay"], "reference": "Dmitry Adamskiy, Manfred K Warmuth, and Wouter M Koolen. Putting bayes to sleep. In Advances  in neural information processing systems 25, 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit  problems. In International Conference on Algorithmic Learning Theory, 2011.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016.  13   A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  The authors would like to thank Peter Auer for the discussion about the possibility of getting optimal bounds for our problem, and to thank Peter Auer, Pratik Gajane, Ronald Ortner for kindly sharing their manuscript of (Auer et al., 2018) before it was public. HL and CYW are supported by NSF Grant #1755781.  Acknowledgments  References  Dmitry Adamskiy, Manfred K Warmuth, and Wouter M Koolen. Putting bayes to sleep. In Advances  in neural information processing systems 25, 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number of distribution changes. In 14th European Workshop on Reinforcement Learning, 2018.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non- stationarity. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient In Proceedings of the Conference on Uncertainty in  optimal learning for contextual bandits. Artificial Intelligence, 2011.  Aur\u00b4elien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit  problems. In International Conference on Algorithmic Learning Theory, 2011.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016. A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  Elad Hazan and Comandur Seshadhri. Efficient learning algorithms for changing environments. In Proceedings of the 26th International Conference on Machine Learning, pages 393-400, 2009.  Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151-  178, 1998.  Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza- tion: Competing with dynamic comparators. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, 2015.  Kwang-Sung Jun, Francesco Orabona, Stephen Wright, Rebecca Willett, et al. Online learning for changing environments using coin betting. Electronic Journal of Statistics, 11(2):5282-5310, 2017.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in Neural Information Processing Systems 21, 2008.  Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise- stationary multi-armed bandit problem. In Thirty-Second AAAI Conference on Artificial Intelli- gence, 2018.  Haipeng Luo and Robert E. Schapire. Achieving All with No Parameters: AdaNormalHedge. In  28th Annual Conference on Learning Theory (COLT), 2015.  Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in  non-stationary worlds. In 31st Annual Conference on Learning Theory (COLT), 2018.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, 2016.  Aleksandrs Slivkins and Eli Upfal. Adapting to a changing environment: the brownian restless  bandits. In 21st Annual Conference on Learning Theory (COLT), pages 343-354, 2008.  Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efficient algorithms for ad- versarial contextual learning. In Proceedings of the 33rd International Conference on Machine Learning, 2016a.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems 29, 2016b.  Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic  environments. In Advances in Neural Information Processing Systems 29, 2016.  Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: opti- mal dynamic regret of online learning with true and noisy gradient. In Proceedings of the 33rd International Conference on Machine Learning, pages 449-457, 2016. A NEW ALGORITHM FOR NON-STATIONARY CONTEXTUAL BANDITS  Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In Advances in Neural Information Processing Systems 30, 2017.  Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou. Dynamic regret of strongly adaptive methods. In Proceedings of the 35th International Conference on Machine Learning, 2018.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  Proceedings of the 20th International Conference on Machine Learning, 2003.  "}, "Faster Algorithms for High-Dimensional Robust Covariance Estimation": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Faster Algorithms for High-Dimensional Robust Covariance Estimation", "abstract": "We study the problem of estimating the covariance matrix of a high-dimensional distribution when a small constant fraction of the samples can be arbitrarily corrupted. Recent work gave the first polynomial time algorithms for this problem with near-optimal error guarantees for several natural structured distributions.  Our main contribution is to develop faster algorithms for this problem whose running time nearly matches that of computing the empirical covariance. Given $N = \\tilde{\\Omega}(d^2/\\epsilon^2)$ samples from a $d$-dimensional Gaussian distribution, an $\\epsilon$-fraction of which may be arbitrarily corrupted, our algorithm runs in time $\\tilde{O}(d^{3.26})/\\mathrm{poly}(\\epsilon)$ and approximates the unknown covariance matrix to optimal error up to a logarithmic factor. Previous robust algorithms with comparable error guarantees all have runtimes $\\tilde{\\Omega}(d^{2 \\omega})$ when $\\epsilon = \\Omega(1)$, where $\\omega$ is the exponent of matrix multiplication. We also provide evidence that improving the running time of our algorithm may require new algorithmic techniques.", "pdf_url": "http://proceedings.mlr.press/v99/cheng19a/cheng19a.pdf", "keywords": [], "reference": "Z. Allen-Zhu, Y. Lee, and L. Orecchia. Using optimization to obtain a width-independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th Annual Symposium on Discrete Algorithms (SODA), pages 1824-1831, 2016.  S. Arora and S. Kale. A combinatorial, primal-dual approach to semidefinite programs. Journal of  the ACM, 63(2):12:1-12:35, 2016.  S. Balakrishnan, S. S. Du, J. Li, and A. Singh. Computationally efficient robust sparse estimation in high dimensions. In Proc. 30th Annual Conference on Learning Theory (COLT), pages 169-212, 2017.  P. J. Bickel and E. Levina. Covariance regularization by thresholding. Ann. Statist., 36(6):2577-  2604, 12 2008a.  (1):199-227, 02 2008b.  A, 6(1):1-74, 1957.  P. J. Bickel and E. Levina. Regularized estimation of large covariance matrices. Ann. Statist., 36  J. L. Bordewijk. Inter-reciprocity applied to electrical networks. Applied Scientific Research, Section  M. Braverman and A. Rao.  Information equals amortized communication.  In Proc. 52nd IEEE  Symposium on Foundations of Computer Science (FOCS), pages 748-757, 2011.  M. Braverman, A. Garg, D. Pankratov, and O. Weinstein.  Information lower bounds via self-  reducibility. Theory of Computing Systems, 59(2):377-396, 2016.  T. T. Cai, C.-H. Zhang, and H. H. Zhou. Optimal rates of convergence for covariance matrix esti-  mation. Ann. Statist., 38(4):2118-2144, 08 2010.  A. Chakrabarti and O. Regev. An optimal lower bound on the communication complexity of gap-  hamming-distance. SIAM J. on Comput., 41(5):1299-1317, 2012.  M. Charikar, J. Steinhardt, and G. Valiant. Learning from untrusted data. In Proc. 49th Annual  ACM Symposium on Theory of Computing (STOC), pages 47-60, 2017.  M. Chen, C. Gao, and Z. Ren. Robust covariance and scatter matrix estimation under Huber\u2019s  contamination model. Ann. Statist., 46(5):1932-1960, 10 2018.  Y. Cheng, I. Diakonikolas, D. M. Kane, and A. Stewart. Robust learning of fixed-structure In Proc. 33rd Annual Conference on Neural Information Processing Sys-  Bayesian networks. tems (NeurIPS), pages 10304-10316, 2018.  Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear time. In Proc. 30th Annual Symposium on Discrete Algorithms (SODA), pages 2755-2771, 2019.  J. Demmel, I. Dumitriu, and O. Holtz. Fast linear algebra is stable. Numerische Mathematik, 108  (1):59-91, 2007.  13   ROBUST COVARIANCE ESTIMATION  References  Z. Allen-Zhu, Y. Lee, and L. Orecchia. Using optimization to obtain a width-independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th Annual Symposium on Discrete Algorithms (SODA), pages 1824-1831, 2016.  S. Arora and S. Kale. A combinatorial, primal-dual approach to semidefinite programs. Journal of  the ACM, 63(2):12:1-12:35, 2016.  S. Balakrishnan, S. S. Du, J. Li, and A. Singh. Computationally efficient robust sparse estimation in high dimensions. In Proc. 30th Annual Conference on Learning Theory (COLT), pages 169-212, 2017.  P. J. Bickel and E. Levina. Covariance regularization by thresholding. Ann. Statist., 36(6):2577-  2604, 12 2008a.  (1):199-227, 02 2008b.  A, 6(1):1-74, 1957.  P. J. Bickel and E. Levina. Regularized estimation of large covariance matrices. Ann. Statist., 36  J. L. Bordewijk. Inter-reciprocity applied to electrical networks. Applied Scientific Research, Section  M. Braverman and A. Rao.  Information equals amortized communication.  In Proc. 52nd IEEE  Symposium on Foundations of Computer Science (FOCS), pages 748-757, 2011.  M. Braverman, A. Garg, D. Pankratov, and O. Weinstein.  Information lower bounds via self-  reducibility. Theory of Computing Systems, 59(2):377-396, 2016.  T. T. Cai, C.-H. Zhang, and H. H. Zhou. Optimal rates of convergence for covariance matrix esti-  mation. Ann. Statist., 38(4):2118-2144, 08 2010.  A. Chakrabarti and O. Regev. An optimal lower bound on the communication complexity of gap-  hamming-distance. SIAM J. on Comput., 41(5):1299-1317, 2012.  M. Charikar, J. Steinhardt, and G. Valiant. Learning from untrusted data. In Proc. 49th Annual  ACM Symposium on Theory of Computing (STOC), pages 47-60, 2017.  M. Chen, C. Gao, and Z. Ren. Robust covariance and scatter matrix estimation under Huber\u2019s  contamination model. Ann. Statist., 46(5):1932-1960, 10 2018.  Y. Cheng, I. Diakonikolas, D. M. Kane, and A. Stewart. Robust learning of fixed-structure In Proc. 33rd Annual Conference on Neural Information Processing Sys-  Bayesian networks. tems (NeurIPS), pages 10304-10316, 2018.  Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear time. In Proc. 30th Annual Symposium on Discrete Algorithms (SODA), pages 2755-2771, 2019.  J. Demmel, I. Dumitriu, and O. Holtz. Fast linear algebra is stable. Numerische Mathematik, 108  (1):59-91, 2007. ROBUST COVARIANCE ESTIMATION  I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in In Proc. 57th IEEE Symposium on  high dimensions without the computational intractability. Foundations of Computer Science (FOCS), pages 655-664, 2016.  I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Being robust (in high di- mensions) can be practical. In Proc. 34th International Conference on Machine Learning (ICML), pages 999-1008, 2017a.  I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estima- tion of high-dimensional Gaussians and Gaussian mixtures. In Proc. 58th IEEE Symposium on Foundations of Computer Science (FOCS), pages 73-84, 2017b.  I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robustly learning In Proc. 29th Annual Symposium on Discrete  a Gaussian: Getting optimal error, efficiently. Algorithms (SODA), pages 2683-2702, 2018a.  I. Diakonikolas, D. M. Kane, and A. Stewart. List-decodable robust mean estimation and learning mixtures of spherical Gaussians. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1047-1060, 2018b.  I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nasty noise. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1061-1073, 2018c.  I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust meta-algorithm for stochastic optimization. In Proc. 36th International Conference on Machine Learning (ICML), 2019a.  I. Diakonikolas, W. Kong, and A. Stewart. Efficient algorithms and lower bounds for robust linear regression. In Proc. 30th Annual Symposium on Discrete Algorithms (SODA), pages 2745-2754, 2019b.  C. M. Fiduccia. On the algebraic complexity of matrix multiplication. PhD thesis, Brown University,  1973.  F. L. Gall. Faster algorithms for rectangular matrix multiplication. In Proc. 53rd IEEE Symposium  on Foundations of Computer Science (FOCS), pages 514-523, 2012.  F. L. Gall. Powers of tensors and fast matrix multiplication. In International Symposium on Symbolic  and Algebraic Computation (ISSAC), pages 296-303, 2014.  S. B. Hopkins and J. Li. Mixture models, robustness, and sum of squares proofs. In Proc. 50th  Annual ACM Symposium on Theory of Computing (STOC), pages 1021-1034, 2018.  P. J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73-101, 03 1964.  D. M. Kane.  Robust covariance estimation.  Computational Efficiency and High-Dimensional Robust Statistics, 2018. http://www.iliasdiakonikolas.org/tti-robust/Kane-Covariance.pdf.  Talk given at TTIC Workshop on Available at  A. Klivans, P. Kothari, and R. Meka. Efficient algorithms for outlier-robust regression. In Proc.  31st Annual Conference on Learning Theory (COLT), pages 1420-1430, 2018. ROBUST COVARIANCE ESTIMATION  P. K. Kothari, J. Steinhardt, and D. Steurer. Robust moment estimation and improved clustering via sum of squares. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1035-1046, 2018.  K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In Proc. 57th  IEEE Symposium on Foundations of Computer Science (FOCS), pages 665-674, 2016.  L. Liu, Y. Shen, T. Li, and C. Caramanis. High dimensional robust sparse regression. arXiv preprint  arXiv:1805.11643, 2018.  Comp. Sci., 23:171-185, 1983.  G. Lotti and F. Romani. On the asymptotic complexity of rectangular matrix multiplication. Theor.  J. Novembre, T. Johnson, K. Bryc, Z. Kutalik, A. R. Boyko, A. Auton, A. Indap, K. S. King, S. Bergmann, M. R. Nelson, et al. Genes mirror geography within europe. Nature, 456(7218): 98-101, 2008.  R. Pagh, M. St\u00a8ockel, and D. P. Woodruff. Is min-wise hashing optimal for summarizing set inter- section? In Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS), pages 109-120, 2014.  R. Peng, K. Tangwongsan, and P. Zhang. Faster and simpler width-independent parallel algorithms  for positive semidefinite programming. arXiv preprint arXiv:1201.5135v3, 2016.  A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust gradient  estimation. arXiv preprint arXiv:1802.06485, 2018.  P. Rousseeuw. Multivariate estimation with high breakdown point. Mathematical Statistics and  Applications, pages 283-297, 1985.  J. Steinhardt, M. Charikar, and G. Valiant. Resilience: A criterion for learning in the presence of arbitrary outliers. In Proc. 9th Innovations in Theoretical Computer Science Conference (ITCS), pages 45:1-45:21, 2018.  J. W. Tukey. Mathematics and picturing of data. In Proceedings of ICM, volume 6, pages 523-531,  1975.  D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in  Theoretical Computer Science, 10(1-2):1-157, 2014.  "}, "Testing Symmetric Markov Chains Without Hitting": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Testing Symmetric Markov Chains Without Hitting", "abstract": "We study the problem of identity testing of symmetric markov chains. In this setting, we are given access to a single trajectory from a markov chain with unknown transition matrix $\\bm{Q}$ and the goal is to determine whether $\\bm{Q} = \\bm{P}$ for some known matrix $\\bm{P}$ or $\\text{Dist}(\\bm{P}, \\bm{Q}) \\geq \\epsilon$ where $\\text{Dist}$ is suitably defined. In recent work by Daskalakis et al, 2018, it was shown that it is possible to distinguish between the two cases provided the length of the observed trajectory is at least super-linear in the hitting time of $\\bm{P}$ which may be arbitrarily large. In this paper, we propose an algorithm that avoids this dependence on hitting time thus enabling efficient testing of markov chains even in cases where it is infeasible to observe every state in the chain. Our algorithm is based on combining classical ideas from approximation algorithms with techniques for the spectral analysis of markov chains.", "pdf_url": "http://proceedings.mlr.press/v99/cherapanamjeri19a/cherapanamjeri19a.pdf", "keywords": [], "reference": "Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties of distributions. In Advances in Neural Information Processing Systems, pages 3591-3599, 2015.  Alan Agresti. Categorical Data Analysis. John Wiley & Sons, 2013.  Theodore W Anderson and Leo A Goodman. Statistical inference about Markov chains. The Annals  of Mathematical Statistics, pages 89-110, 1957.  Sanjeev Arora, Satish Rao, and Umesh Vazirani. Expander \ufb02ows, geometric embeddings and graph  partitioning. Journal of the ACM (JACM), 56(2):5, 2009.  Maurice S Bartlett. The frequency goodness of fit test for probability chains.  In Mathematical Proceedings of the Cambridge Philosophical Society, volume 47, pages 86-95. Cambridge Uni- versity Press, 1951.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing that In Foundations of Computer Science, 2000. Proceedings. 41st Annual  distributions are close. Symposium on, pages 259-269. IEEE, 2000.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 442-451. IEEE, 2001.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 381-390. ACM, 2004.  Patrick Billingsley. Statistical methods in Markov chains. The Annals of Mathematical Statistics,  pages 12-40, 1961.  Eric Blais, Cl\u00b4ement L Canonne, and Tom Gur. Distribution testing lower bounds via reductions In LIPIcs-Leibniz International Proceedings in Informatics,  from communication complexity. volume 79. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. Oxford university press, 2013.  Jean Bourgain. On Lipschitz embedding of finite metric spaces in Hilbert space. Israel Journal of  Mathematics, 52(1-2):46-52, 1985.  Clement Canonne, Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Testing Bayesian net-  works. arXiv preprint arXiv:1612.03156, 2016.  Siu-On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing closeness of discrete distributions. In Proceedings of the twenty-fifth annual ACM-SIAM sympo- sium on Discrete algorithms, pages 1193-1203. SIAM, 2014.  Jeff Cheeger. A lower bound for the smallest eigenvalue of the Laplacian. pages 195-199, 1970.  13   TESTING SYMMETRIC MARKOV CHAINS WITHOUT HITTING  References  Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties of distributions. In Advances in Neural Information Processing Systems, pages 3591-3599, 2015.  Alan Agresti. Categorical Data Analysis. John Wiley & Sons, 2013.  Theodore W Anderson and Leo A Goodman. Statistical inference about Markov chains. The Annals  of Mathematical Statistics, pages 89-110, 1957.  Sanjeev Arora, Satish Rao, and Umesh Vazirani. Expander \ufb02ows, geometric embeddings and graph  partitioning. Journal of the ACM (JACM), 56(2):5, 2009.  Maurice S Bartlett. The frequency goodness of fit test for probability chains.  In Mathematical Proceedings of the Cambridge Philosophical Society, volume 47, pages 86-95. Cambridge Uni- versity Press, 1951.  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing that In Foundations of Computer Science, 2000. Proceedings. 41st Annual  distributions are close. Symposium on, pages 259-269. IEEE, 2000.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 442-451. IEEE, 2001.  Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 381-390. ACM, 2004.  Patrick Billingsley. Statistical methods in Markov chains. The Annals of Mathematical Statistics,  pages 12-40, 1961.  Eric Blais, Cl\u00b4ement L Canonne, and Tom Gur. Distribution testing lower bounds via reductions In LIPIcs-Leibniz International Proceedings in Informatics,  from communication complexity. volume 79. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. Oxford university press, 2013.  Jean Bourgain. On Lipschitz embedding of finite metric spaces in Hilbert space. Israel Journal of  Mathematics, 52(1-2):46-52, 1985.  Clement Canonne, Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Testing Bayesian net-  works. arXiv preprint arXiv:1612.03156, 2016.  Siu-On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing closeness of discrete distributions. In Proceedings of the twenty-fifth annual ACM-SIAM sympo- sium on Discrete algorithms, pages 1193-1203. SIAM, 2014.  Jeff Cheeger. A lower bound for the smallest eigenvalue of the Laplacian. pages 195-199, 1970. TESTING SYMMETRIC MARKOV CHAINS WITHOUT HITTING  Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathemati-  cal Soc., 1997.  Harald Cram\u00b4er. On the composition of elementary errors: First paper: Mathematical deductions.  Scandinavian Actuarial Journal, 1928(1):13-74, 1928.  Ralph B D\u2019Agostino. Goodness-of-Fit-Techniques. Routledge, 2017.  Constantinos Daskalakis, Nishanth Dikkala, and Nick Gravin. Testing symmetric Markov chains from a single trajectory. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 385-409. PMLR, 06-09 Jul 2018a.  Constantinos Daskalakis, Gautam Kamath, and John Wright. Which distribution distances are sub- linearly testable? In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2747-2764. SIAM, 2018b.  Ilias Diakonikolas and Daniel M Kane. A new approach for testing properties of discrete distri- butions. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 685-694. IEEE, 2016.  Ilias Diakonikolas, Daniel M Kane, and Vladimir Nikishkin. Testing identity of structured distribu- tions. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 1841-1854. Society for Industrial and Applied Mathematics, 2015.  Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Collision-based testers are  optimal for uniformity and closeness. arXiv preprint arXiv:1611.03579, 2016.  Devdatt P. Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis of  Randomized Algorithms. Cambridge University Press, Cambridge, 2009.  Oded Goldreich and Dana Ron. A sublinear bipartiteness tester for bounded degree graphs. Com-  binatorica, 19(3):335-373, 1999.  Ravi Kannan, L\u00b4aszl\u00b4o Lov\u00b4asz, and Mikl\u00b4os Simonovits. Random walks and an o*(n5) volume algo-  rithm for convex bodies. Random Structures & Algorithms, 11(1):1-50, 1997.  Dimitri Kazakos. The bhattacharyya distance and detection between markov chains. IEEE Trans-  actions on Information Theory, 24(6):747-754, 1978.  Yin Tat Lee and Santosh S Vempala. The Kannan-Lov\\\u2019asz-Simonovits conjecture. arXiv preprint  arXiv:1807.03465, 2018.  Tom Leighton and Satish Rao. Multicommodity max-\ufb02ow min-cut theorems and their use in de-  signing approximation algorithms. Journal of the ACM (JACM), 46(6):787-832, 1999.  Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some of its algo-  rithmic applications. Combinatorica, 15(2):215-245, 1995.  David W Matula and Farhad Shahrokhi. Sparsest cuts and bottlenecks in graphs. Discrete Applied  Mathematics, 27(1-2):113-123, 1990. TESTING SYMMETRIC MARKOV CHAINS WITHOUT HITTING  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data.  IEEE Transactions on Information Theory, 54(10):4750-4755, 2008.  Karl Pearson. X. on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50(302):157-175, 1900.  Ronitt Rubinfeld. Taming big probability distributions. XRDS: Crossroads, The ACM Magazine for  Students, 19(1):24-28, 2012.  Ronitt Rubinfeld and Rocco A Servedio. Testing monotone high-dimensional distributions. Random  Structures & Algorithms, 34(1):24-44, 2009.  David B. Shmoys. Approximation algorithms for np-hard problems. chapter Cut Problems and  Their Application to Divide-and-conquer, pages 192-235. PWS Publishing Co., 1997.  Alistair Sinclair and Mark Jerrum. Approximate counting, uniform generation and rapidly mixing  Markov chains. Inform. and Comput., 82(1):93-133, 1989.  Nikolai V Smirnov. On the estimation of the discrepancy between empirical curves of distribution  for two independent samples. Bull. Math. Univ. Moscou, 2(2):3-14, 1939.  Daniel A Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81-90. ACM, 2004.  Luca Trevisan. Approximation algorithms for unique games. In Foundations of Computer Science,  2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 197-205. IEEE, 2005.  Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity  testing. SIAM Journal on Computing, 46(1):429-455, 2017.  Gregory Valiant, Paul Valiant, Arnab Bhattacharyya, Eldar Fischer, and Ronitt Rubinfeld. Instance- In Electronic Colloquium on Computational Complexity  by-instance optimal identity testing. (ECCC), volume 20, pages 1-1, 2013.  Paul Valiant. Testing symmetric properties of distributions. SIAM Journal on Computing, 40(6):  1927-1968, 2011.  Geoffrey Wolfer and Aryeh Kontorovich. Minimax testing of identity to a reference ergodic markov  chain. arXiv preprint arXiv:1902.00080, 2019. TESTING SYMMETRIC MARKOV CHAINS WITHOUT HITTING  "}, "Fast Mean Estimation with Sub-Gaussian Rates": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Fast Mean Estimation with Sub-Gaussian Rates", "abstract": "We propose an estimator for the mean of a random vector in $\\mathbb{R}^d$ that can be computed in time $O(n^{3.5}+n^2d)$ for $n$ i.i.d.\u00a0samples and that has error bounds matching the sub-Gaussian case. The only assumptions we make about the data distribution are that it has finite mean and covariance; in particular, we make no assumptions about higher-order moments. Like the polynomial time estimator introduced by Hopkins (2018), which is based on the sum-of-squares hierarchy, our estimator achieves optimal statistical efficiency in this challenging setting, but it has a significantly faster runtime and a simpler analysis.", "pdf_url": "http://proceedings.mlr.press/v99/cherapanamjeri19b/cherapanamjeri19b.pdf", "keywords": [], "reference": "10 2011.  F. Alizadeh. Interior point methods in semide\ufb01nite programming with applications to combinatorial  optimization. SIAM journal on Optimization, 5(1):13\u201351, 1995.  N. Alon, Y. Matias, and M. Szegedy. The space complexity of approximating the frequency mo-  ments. Journal of Computer and System Sciences, 58(1):137\u2013147, 1999.  J.-Y. Audibert and O. Catoni. Robust linear least squares regression. Ann. Statist., 39(5):2766\u20132794,  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Independence. Oxford university press, 2013.  C. Brownlees, E. Joly, and G. Lugosi. Empirical risk minimization for heavy-tailed losses. Ann.  Statist., 43(6):2507\u20132536, 12 2015.  O. Catoni. Challenging the empirical mean and empirical variance: a deviation study. Ann. Inst.  Henri Poincar\u00b4e Probab. Stat., 48(4):1148\u20131185, 2012.  12   FAST MEAN ESTIMATION WITH SUB-GAUSSIAN RATES  4.3.3. GRADIENT DESCENT STEP  The following lemma guarantees that Assumption 2 holds with high probability and is used analo- gously to Corollary 5 in the proof of Theorem 2:  Lemma 11 Let Y = (Y1, . . . , Yk) \u2208 Rk\u00d7d be k i.i.d. random vectors with mean \u00b5 and co- variance \u039b and let S denote the set of feasible solutions of M T (\u00b5, r, Y ). Then, we have for r \u2265 300  (cid:16)(cid:112)Tr \u039b/k + (cid:112)(cid:107)\u039b(cid:107) (cid:17)  and k \u2265 3200 log 1/\u03b4:  max X\u2208S  k (cid:88)  i=1  Xbi,bi \u2264  k 20  ,  with probability at least 1 \u2212 \u03b4.  The proof of the lemma is an application of standard empirical process theory and concentration inequalities (Lugosi and Mendelson, 2019; Hopkins, 2018) and is proven in "}, "Vortices Instead of Equilibria in MinMax Optimization: Chaos and Butterfly Effects of Online Learning in Zero-Sum Games": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Vortices Instead of Equilibria in MinMax Optimization: Chaos and Butterfly Effects of Online Learning in Zero-Sum Games", "abstract": "We establish that algorithmic experiments in zero-sum games \u201cfail miserably\u201d to confirm the unique, sharp prediction of maxmin equilibration. Contradicting nearly a century of economic thought that treats zero-sum games nearly axiomatically as the exemplar symbol of economic stability, we prove that no meaningful prediction can be made about the  day-to-day behavior of online learning dynamics in zero-sum games. Concretely, Multiplicative Weights Updates (MWU) with constant step-size is \\emph{Lyapunov chaotic} in the dual (payoff) space. Simply put, let\u2019s assume that an observer asks the agents playing Matching-Pennies whether they prefer Heads or Tails (and by how much in terms of aggregate payoff so far). The range of possible answers consistent with any arbitrary small set of initial conditions blows up exponentially with time everywhere in the payoff space. This result is \\emph{robust} both \\emph{algorithmically} as well as \\emph{game theoretically}. \\textbf{Algorithmic robustness:} Chaos is robust to agents using any of a general sub-family of Follow-the-Regularized-Leader (FTRL) algorithms, the well known regret-minimizing dynamics, even when agents mix-and-match dynamics, use different or slowly decreasing step-sizes. \\textbf{Game theoretic robustness:} Chaos is robust to all affine variants of zero-sum games (strictly competitive games), network variants with arbitrary large number of agents and even to competitive settings beyond these. Our result is in stark contrast with the time-average convergence of online learning to (approximate) Nash equilibrium, a result widely reported as \u201c(weak) convergence to equilibrium\u201d.", "pdf_url": "http://proceedings.mlr.press/v99/cheung19a/cheung19a.pdf", "keywords": ["Lyapunov Chaos", "Zero-Sum Games", "Multiplicative Weights Updates", "Regret Minimization", "Gradient Descent Ascent", "Follow-the-Regularized-Leader Algorithm", "Volume of Flow"], "reference": "Robert John Aumann. Game theory. In The new Palgrave: a dictionary of economics, pages 460-  482. London (UK) Macmillan, 1987.  James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC,  pages 321-338, 2018.  D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel. The Mechanics of  n-Player Differentiable Games. In ICML, 2018.  G.W. Brown. Iterative solutions of games by fictitious play. In Activity Analysis of Production and  Allocation, T.C. Koopmans (Ed.), New York: Wiley., 1951.  Nikolo Cesa-Bianchi and Gabor Lugoisi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum  games. In NeurIPS 2018, pages 3532-3542, 2018.  Thiparat Chotibut, Fryderyk Falniowski, Michal Misiurewicz, and Georgios Piliouras. Family of  chaotic maps from game theory. arXiv preprint arXiv:1807.06831, 2018.  Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9256- 9266, 2018.  Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and  constrained min-max optimization. ITCS, 2019.  Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with  optimism. In ICLR, 2018.  Biology, 18:123-133, 1983.  pages 325-332, 1996.  (4):1232-1236, 2013.  I. Eshel and E. Akin. Coevolutionary instability of mixed nash solutions. Journal of Mathematical  Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting.  In COLT,  Tobias Galla and J. Doyne Farmer. Complex dynamics in learning complicated games. PNAS, 110  Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi Lepriol, Si- mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In AISTATS, 2019.  13   CHAOS AND BUTTERFLY EFFECTS OF ONLINE LEARNING IN ZERO-SUM GAMES  Yun Kuen Cheung and Georgios Piliouras acknowledge SUTD grant SRG ESD 2015 097, MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF- NRFF2018-07.  Acknowledgments  References  Robert John Aumann. Game theory. In The new Palgrave: a dictionary of economics, pages 460-  482. London (UK) Macmillan, 1987.  James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC,  pages 321-338, 2018.  D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel. The Mechanics of  n-Player Differentiable Games. In ICML, 2018.  G.W. Brown. Iterative solutions of games by fictitious play. In Activity Analysis of Production and  Allocation, T.C. Koopmans (Ed.), New York: Wiley., 1951.  Nikolo Cesa-Bianchi and Gabor Lugoisi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum  games. In NeurIPS 2018, pages 3532-3542, 2018.  Thiparat Chotibut, Fryderyk Falniowski, Michal Misiurewicz, and Georgios Piliouras. Family of  chaotic maps from game theory. arXiv preprint arXiv:1807.06831, 2018.  Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9256- 9266, 2018.  Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and  constrained min-max optimization. ITCS, 2019.  Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with  optimism. In ICLR, 2018.  Biology, 18:123-133, 1983.  pages 325-332, 1996.  (4):1232-1236, 2013.  I. Eshel and E. Akin. Coevolutionary instability of mixed nash solutions. Journal of Mathematical  Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting.  In COLT,  Tobias Galla and J. Doyne Farmer. Complex dynamics in learning complicated games. PNAS, 110  Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi Lepriol, Si- mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In AISTATS, 2019. CHAOS AND BUTTERFLY EFFECTS OF ONLINE LEARNING IN ZERO-SUM GAMES  Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, In Proceedings of the Aaron Courville, and Yoshua Bengio. Generative adversarial nets. 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201914, pages 2672-2680, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/ citation.cfm?id=2969033.2969125.  Ralph Howard. The inverse function theorem for lipschitz maps. http://people.math.sc.  edu/howard/Notes/inverse.pdf, 1997.  Tien-Yien Li and James A. Yorke. Period three implies chaos. The American Mathematical Monthly,  82(10):985-992, 1975. doi: 10.1080/00029890.1975.11994008.  P. Mertikopoulos, H. Zenati, B. Lecouat, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Mirror descent in saddle-point problems: Going the extra (gradient) mile. ArXiv e-prints, July 2018.  Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial  regularized learning. In ACM-SIAM Symposium on Discrete Algorithms, 2018.  Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani. Algorithmic Game Theory.  Cambridge University Press, New York, NY, USA, 2007. ISBN 0521872820.  Brynjulf Owren. Volume preserving methods. https://wiki.math.ntnu.no/_media/  aarms2015/lectures/volumepreservation.pdf, 2015.  Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update with constant step-size in congestion games: Convergence, limit cycles and chaos. In NIPS, pages 5874-5884, 2017.  Georgios Piliouras and Leonard J. Schulman. Learning dynamics and the co-evolution of competing  sexual species. In ITCS, 2018.  Georgios Piliouras and Jeff S. Shamma. Optimization despite chaos: Convex relaxations to complex  limit sets via poincar\u00b4e recurrence. In SODA, pages 861-873, 2014.  J. Robinson. An iterative method of solving a game. Annals of Mathematics, 54:296-301, 1951.  Tim Roughgarden. Twenty lectures on algorithmic game theory. Cambridge University Press, 2016.  Yuzuru Sato, Eizo Akiyama, and J. Doyne Farmer. Chaos in learning a simple two-person game.  PNAS, 99(7):4748-4751, 2002.  John von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100:295-300,  1928.  University Press, 1944.  John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton  Y. Yaz\u0131c\u0131, C.-S. Foo, S. Winkler, K.-H. Yap, G. Piliouras, and V. Chandrasekhar. The Unusual  Effectiveness of Averaging in GAN Training. ArXiv e-prints, June 2018.  H Peyton Young. Strategic learning and its limits. Oxford Univ. Press, 2004. CHAOS AND BUTTERFLY EFFECTS OF ONLINE LEARNING IN ZERO-SUM GAMES  "}, "Pure entropic regularization for metrical task systems": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Pure entropic regularization for metrical task systems", "abstract": "We show that on every $n$-point HST metric, there is a randomized online algorithm for metrical task systems (MTS) that is $1$-competitive for service costs and $O(\\log n)$-competitive for movement costs. In general, these refined guarantees are optimal up to the implicit constant. While an $O(\\log n)$-competitive algorithm for MTS on HST metrics was developed by Bubeck et al. (2018), that approach could only establish an $O((\\log n)^2)$-competitive ratio when the service costs are required to be $O(1)$-competitive. Our algorithm is an instantiation of online mirror descent with the regularizer derived from a multiscale conditional entropy. In fact, our algorithm satisfies a set of even more refined guarantees; we are able to exploit this property to combine it with known random embedding theorems and obtain, for {\\em any} $n$-point metric space, a randomized algorithm that is $1$-competitive for service costs and $O((\\log n)^2)$-competitive for movement costs.", "pdf_url": "http://proceedings.mlr.press/v99/coester19a/coester19a.pdf", "keywords": ["Online algorithms", "competitive analysis", "mirror descent", "metrical task systems", "decision making under uncertainty"], "reference": "Jacob Abernethy, Peter Bartlett, Niv Buchbinder, and Isabelle Stanton. A regularization approach  to metrical task systems. In Algorithmic Learning Theory, ALT 2010. Springer, 2010.  Nikhil Bansal, Niv Buchbinder, and Joseph Naor. Metrical task systems and the k-server problem on HSTs. In Proceedings of the 37th International Colloquium Conference on Automata, Languages and Programming, ICALP\u201910, pages 287-298, Berlin, Heidelberg, 2010. Springer-Verlag. ISBN 3-642-14164-1, 978-3-642-14164-5. URL http://dl.acm.org/citation.cfm?id= 1880918.1880950.  Nikhil Bansal, Niv Buchbinder, and Joseph Naor. A primal-dual randomized algorithm for weighted paging. J. ACM, 59(4):Art. 19, 24, 2012. ISSN 0004-5411. doi: 10.1145/2339123.2339126. URL http://dx.doi.org/10.1145/2339123.2339126.  Nikhil Bansal, Niv Buchbinder, Aleksander Madry, and Joseph Naor. A polylogarithmic- competitive algorithm for the k-server problem. J. ACM, 62(5):Art. 40, 49, 2015. ISSN 0004- 5411. doi: 10.1145/2783434. URL http://dx.doi.org/10.1145/2783434.  Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In 37th Annual Symposium on Foundations of Computer Science, FOCS \u201996, Burlington, Vermont, USA, 14-16 October, 1996, pages 184-193, 1996. doi: 10.1109/SFCS.1996.548477. URL https: //doi.org/10.1109/SFCS.1996.548477.  Yair Bartal, Avrim Blum, Carl Burch, and Andrew Tomkins. A polylog(n)-competitive algorithm for metrical task systems. In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing, STOC \u201997, pages 711-719, New York, NY, USA, 1997. ACM. ISBN 0-89791-888-6. doi: 10.1145/258533.258667. URL http://doi.acm.org/10.1145/258533.258667.  Yair Bartal, Nathan Linial, Manor Mendel, and Assaf Naor. On metric Ramsey-type phenomena. Ann. of Math. (2), 162(2):643-709, 2005. ISSN 0003-486X. doi: 10.4007/annals.2005.162.643. URL http://dx.doi.org/10.4007/annals.2005.162.643.  Yair Bartal, B\u00b4ela Bollob\u00b4as, and Manor Mendel. Ramsey-type theorems for metric spaces with applications to online problems. J. Comput. System Sci., 72(5):890-921, 2006. ISSN 0022-0000. doi: 10.1016/j.jcss.2005.05.008. URL http://dx.doi.org/10.1016/j.jcss.2005. 05.008.  Avrim Blum, Howard Karloff, Yuval Rabani, and Michael Saks. A decomposition theorem for task systems and bounds for randomized server problems. SIAM J. Comput., 30(5):1624-1661, 2000. ISSN 0097-5397. URL https://doi-org.offcampus.lib.washington.edu/10. 1137/S0097539799351882.  Allan Borodin, Nathan Linial, and Michael E. Saks. An optimal on-line algorithm for metrical task system. J. ACM, 39(4):745-763, October 1992. ISSN 0004-5411. doi: 10.1145/146585.146588. URL http://doi.acm.org/10.1145/146585.146588.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. doi: 10.1561/2200000024. URL https://doi.org/10.1561/2200000024.  13   PURE ENTROPIC REGULARIZATION FOR METRICAL TASK SYSTEMS  References  Jacob Abernethy, Peter Bartlett, Niv Buchbinder, and Isabelle Stanton. A regularization approach  to metrical task systems. In Algorithmic Learning Theory, ALT 2010. Springer, 2010.  Nikhil Bansal, Niv Buchbinder, and Joseph Naor. Metrical task systems and the k-server problem on HSTs. In Proceedings of the 37th International Colloquium Conference on Automata, Languages and Programming, ICALP\u201910, pages 287-298, Berlin, Heidelberg, 2010. Springer-Verlag. ISBN 3-642-14164-1, 978-3-642-14164-5. URL http://dl.acm.org/citation.cfm?id= 1880918.1880950.  Nikhil Bansal, Niv Buchbinder, and Joseph Naor. A primal-dual randomized algorithm for weighted paging. J. ACM, 59(4):Art. 19, 24, 2012. ISSN 0004-5411. doi: 10.1145/2339123.2339126. URL http://dx.doi.org/10.1145/2339123.2339126.  Nikhil Bansal, Niv Buchbinder, Aleksander Madry, and Joseph Naor. A polylogarithmic- competitive algorithm for the k-server problem. J. ACM, 62(5):Art. 40, 49, 2015. ISSN 0004- 5411. doi: 10.1145/2783434. URL http://dx.doi.org/10.1145/2783434.  Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In 37th Annual Symposium on Foundations of Computer Science, FOCS \u201996, Burlington, Vermont, USA, 14-16 October, 1996, pages 184-193, 1996. doi: 10.1109/SFCS.1996.548477. URL https: //doi.org/10.1109/SFCS.1996.548477.  Yair Bartal, Avrim Blum, Carl Burch, and Andrew Tomkins. A polylog(n)-competitive algorithm for metrical task systems. In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing, STOC \u201997, pages 711-719, New York, NY, USA, 1997. ACM. ISBN 0-89791-888-6. doi: 10.1145/258533.258667. URL http://doi.acm.org/10.1145/258533.258667.  Yair Bartal, Nathan Linial, Manor Mendel, and Assaf Naor. On metric Ramsey-type phenomena. Ann. of Math. (2), 162(2):643-709, 2005. ISSN 0003-486X. doi: 10.4007/annals.2005.162.643. URL http://dx.doi.org/10.4007/annals.2005.162.643.  Yair Bartal, B\u00b4ela Bollob\u00b4as, and Manor Mendel. Ramsey-type theorems for metric spaces with applications to online problems. J. Comput. System Sci., 72(5):890-921, 2006. ISSN 0022-0000. doi: 10.1016/j.jcss.2005.05.008. URL http://dx.doi.org/10.1016/j.jcss.2005. 05.008.  Avrim Blum, Howard Karloff, Yuval Rabani, and Michael Saks. A decomposition theorem for task systems and bounds for randomized server problems. SIAM J. Comput., 30(5):1624-1661, 2000. ISSN 0097-5397. URL https://doi-org.offcampus.lib.washington.edu/10. 1137/S0097539799351882.  Allan Borodin, Nathan Linial, and Michael E. Saks. An optimal on-line algorithm for metrical task system. J. ACM, 39(4):745-763, October 1992. ISSN 0004-5411. doi: 10.1145/146585.146588. URL http://doi.acm.org/10.1145/146585.146588.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. doi: 10.1561/2200000024. URL https://doi.org/10.1561/2200000024. PURE ENTROPIC REGULARIZATION FOR METRICAL TASK SYSTEMS  S\u00b4ebastien Bubeck, Michael B. Cohen, James R. Lee, and Yin-Tat Lee. Metrical task systems on  trees via mirror descent and unfair gluing. 2018a.  S\u00b4ebastien Bubeck, Michael B. Cohen, Yin Tat Lee, James R. Lee, and Aleksander Madry. k-server via multiscale entropic regularization. In Proceedings of the 50th Annual ACM SIGACT Sym- posium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 3-16, 2018b. doi: 10.1145/3188745.3188798. URL http://doi.acm.org/10.1145/ 3188745.3188798.  Niv Buchbinder and Joseph Naor. The design of competitive online algorithms via a primal-dual approach. Found. Trends Theor. Comput. Sci., 3(2-3):front matter, 93-263 (2009), 2007. ISSN 1551-305X. doi: 10.1561/0400000024.  Niv Buchbinder, Shahar Chen, and Joseph (Seffi) Naor. Competitive analysis via regularization. In Proceedings of the Twenty-fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201914, pages 436-444, Philadelphia, PA, USA, 2014. Society for Industrial and Applied Mathemat- ics.  Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary  metrics by tree metrics. J. Comput. Syst. Sci., 69(3):485-497, 2004.  Amos Fiat and Manor Mendel. Better algorithms for unfair metrical task systems and applications. SIAM Journal on Computing, 32(6):1403-1422, 2003. doi: 10.1137/S0097539700376159. URL https://doi.org/10.1137/S0097539700376159.  Steve Seiden. Unfair problems and randomized algorithms for metrical task systems. Inf. Comput., 148(2):219-240, February 1999. ISSN 0890-5401. doi: 10.1006/inco.1998.2744. URL http: //dx.doi.org/10.1006/inco.1998.2744. "}, "A near-optimal algorithm for approximating the John Ellipsoid": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A near-optimal algorithm for approximating the John Ellipsoid", "abstract": "We develop a simple and efficient algorithm for approximating the John Ellipsoid of a symmetric polytope.  Our algorithm is near optimal in the sense that our time complexity matches the current best verification algorithm.  Experimental results suggest that our algorithm significantly outperforms existing algorithms. We also provide the MATLAB code for further research.", "pdf_url": "http://proceedings.mlr.press/v99/cohen19a/cohen19a.pdf", "keywords": ["John Ellipsoid", "fixed point method", "optimal design"], "reference": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments In International Conference on Machine Learning, pages 126-135,  via regret minimization. 2017.  Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding and linear regression with Orlicz norm. In International Conference on Machine Learning, pages 224-233, 2018.  Kurt M Anstreicher. Improved complexity for maximum volume inscribed ellipsoids. SIAM Journal  on Optimization, 13(2):309-320, 2002.  Corwin L Atwood. Optimal and efficient designs of experiments. The Annals of Mathematical  Statistics, pages 1570-1602, 1969.  Christos Boutsidis and David P Woodruff. Optimal CUR matrix decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC), pages 353-362. ACM, https://arxiv.org/pdf/1405.7910, 2014.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, Cam-  bridge, 2004.  S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41-1. Microtome, 2012.  Michal \u02c7Cern`y and Milan Hlad\u00edk. Two complexity results on C-optimality in experimental design.  Computational Optimization and Applications, 51(3):1397-1408, 2012.  Yuansi Chen, Raaz Dwivedi, Martin J Wainwright, and Bin Yu. Fast MCMC sampling algorithms  on polytopes. The Journal of Machine Learning Research, 19(1):2146-2231, 2018.  3. However, the reported results in Table 3 are only on 35 of these 39 models. For two of the instances, our algorithm fails due to numerical issues in the Cholesky factorization; this could likely be fixed by padding the matrix B(cid:48) \u00b7 B with a small multiple of the identity matrix, but we ignored this issue for cleanliness of the code. Two other instances take too long for the other implementations. Thus, all 4 sets of results for Netlib small were run on the same 35 models.  4. However, similar to the Netlib small case, two of the cases fail our method due to numerical issues with the Cholesky  factorization.  11   A NEAR-OPTIMAL ALGORITHM FOR APPROXIMATING THE JOHN ELLIPSOID  with respect to the analytic center in the dual space. 393 of the 109 models in the dataset satisfy these conditions.  7. Netlib large: We consider those Netlib polytopes that satisfy the same conditions as Netlib  small, but have m + n \u2208 [104, 5 \u00b7 104]. 5 Netlib polytopes satisfy these conditions. 4  We thank Zhao Song for his generous help on this paper.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments In International Conference on Machine Learning, pages 126-135,  via regret minimization. 2017.  Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding and linear regression with Orlicz norm. In International Conference on Machine Learning, pages 224-233, 2018.  Kurt M Anstreicher. Improved complexity for maximum volume inscribed ellipsoids. SIAM Journal  on Optimization, 13(2):309-320, 2002.  Corwin L Atwood. Optimal and efficient designs of experiments. The Annals of Mathematical  Statistics, pages 1570-1602, 1969.  Christos Boutsidis and David P Woodruff. Optimal CUR matrix decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC), pages 353-362. ACM, https://arxiv.org/pdf/1405.7910, 2014.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, Cam-  bridge, 2004.  S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41-1. Microtome, 2012.  Michal \u02c7Cern`y and Milan Hlad\u00edk. Two complexity results on C-optimality in experimental design.  Computational Optimization and Applications, 51(3):1397-1408, 2012.  Yuansi Chen, Raaz Dwivedi, Martin J Wainwright, and Bin Yu. Fast MCMC sampling algorithms  on polytopes. The Journal of Machine Learning Research, 19(1):2146-2231, 2018.  3. However, the reported results in Table 3 are only on 35 of these 39 models. For two of the instances, our algorithm fails due to numerical issues in the Cholesky factorization; this could likely be fixed by padding the matrix B(cid:48) \u00b7 B with a small multiple of the identity matrix, but we ignored this issue for cleanliness of the code. Two other instances take too long for the other implementations. Thus, all 4 sets of results for Netlib small were run on the same 35 models.  4. However, similar to the Netlib small case, two of the cases fail our method due to numerical issues with the Cholesky  factorization. A NEAR-OPTIMAL ALGORITHM FOR APPROXIMATING THE JOHN ELLIPSOID  Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input spar- sity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 81-90. ACM, 2013.  Kenneth L Clarkson and David P Woodruff. Sketching for M-estimators: A unified approach to robust regression. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 921-939. Society for Industrial and Applied Mathematics, 2015a.  Kenneth L Clarkson and David P Woodruff. Input sparsity and hardness for robust subspace ap- proximation. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 310-329. IEEE, 2015b.  Michael B Cohen and Richard Peng. (cid:96)p row sampling by Lewis Weights. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 183-192. ACM, 2015.  Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving SDD linear systems in nearly mlog1/2n time. In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, 2014.  Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen- sionality reduction for k-means clustering and low rank approximation. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 163-172. ACM, 2015a.  Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, pages 181-190. ACM, 2015b.  Samuel I Daitch and Daniel A Spielman. Faster approximate lossy generalized \ufb02ow via interior point algorithms. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 451-460. ACM, 2008.  S Damla Ahipasaoglu, Peng Sun, and Michael J Todd. Linear convergence of a modified Frank- Wolfe algorithm for computing minimum-volume enclosing ellipsoids. Optimisation Methods and Software, 23(1):5-19, 2008.  Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regres- sion and p-splines. In International Conference on Artificial Intelligence and Statistics, pages 1299-1308, 2018.  Huaian Diao, Zhao Song, David Woodruff, and Xin Yang. Total least squares regression in input  sparsity time. In Manuscript, 2019.  Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approx- imation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13 (Dec):3475-3506, 2012.  Izrail Solomonovich Gradshteyn and Iosif Moiseevich Ryzhik. Table of integrals, series, and prod-  ucts. Academic press, 2014.  Adam Gustafson and Hariharan Narayanan. John\u2019s walk. arXiv preprint arXiv:1803.02032, 2018. A NEAR-OPTIMAL ALGORITHM FOR APPROXIMATING THE JOHN ELLIPSOID  David H Gutman and Javier F Pe\u00f1a. A unified framework for bregman proximal methods: subgra-  dient, gradient, and accelerated gradient schemes. arXiv preprint arXiv:1812.10198, 2018.  Hulda S. Haraldsd\u00f3ttir, Ben Cousins, Ines Thiele, Ronan M. T. Fleming, and Santosh Vempala. CHRR: coordinate hit-and-run with rounding for uniform sampling of constraint-based models. Bioinformatics, 33(11), 1 2017.  Elad Hazan and Zohar Karnin. Volumetric spanners: an efficient exploration basis for learning. The  Journal of Machine Learning Research, 17(1):4062-4095, 2016.  GJO Jameson. Inequalities for gamma function ratios. The American Mathematical Monthly, 120  (10):936-940, 2013.  Fritz John. Extremum problems with inequalities as subsidiary conditions. In Studies and Essays Presented to R. Courant on his 60th Birthday, January 8, 1948, pages 187-204. Interscience Publishers, Inc., New York, N. Y., 1948.  Ravi Kannan, L\u00e1szl\u00f3 Lov\u00e1sz, and Mikl\u00f3s Simonovits. Random walks and an O\u2217(n5) volume algo-  rithm for convex bodies. Random Structures Algorithms, 11(1):1-50, 1997.  Jonathan A Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combina- torial algorithm for solving SDD systems in nearly-linear time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 911-920. ACM, 2013.  Leonid G Khachiyan. Rounding of polytopes in the real number model of computation. Mathemat-  ics of Operations Research, 21(2):307-320, 1996.  Leonid G Khachiyan and Michael J Todd. On the complexity of approximating the maximal in-  scribed ellipsoid for a polytope. Mathematical Programming, 61(1):137-159, 1993.  Tarasov S. Khachiyan, L. and I. Ehrlich. The method of inscribed ellipsoids. Soviet Math. Doklady,  1988.  Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian Journal  of Mathematics, 12(363-366):234, 1960.  Piyush Kumar and E Alper Yildirim. Minimum-volume enclosing ellipsoids and core sets. Journal  of Optimization Theory and Applications, 126(1):1-21, 2005.  Rasmus Kyng and Sushant Sachdeva. Approximate gaussian elimination for laplacians-fast, sparse, and simple. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 573-582. IEEE, 2016.  Rasmus Kyng, Yin Tat Lee, Richard Peng, Sushant Sachdeva, and Daniel A Spielman. Sparsified In Proceedings of the forty-eighth  cholesky and multigrid solvers for connection laplacians. annual ACM symposium on Theory of Computing, pages 842-850. ACM, 2016.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000. A NEAR-OPTIMAL ALGORITHM FOR APPROXIMATING THE JOHN ELLIPSOID  Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear rank) iterations and faster algorithms for maximum \ufb02ow. In Foundations of programs in O( Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 424-433. IEEE, 2014.  \u221a  Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix  multiplication time. In COLT, 2019.  Xingguo Li, Jarvis Haupt, and David Woodruff. Near optimal sketching of low-rank tensor regres-  sion. In Advances in Neural Information Processing Systems, pages 3466-3476, 2017.  L\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. Hit-and-run from a corner. SIAM J. Comput., 35(4):985-1005,  2006.  Haihao Lu, Robert M Freund, and Yurii Nesterov. Relatively smooth convex optimization by first-  order methods, and applications. SIAM Journal on Optimization, 28(1):333-354, 2018.  Jelani Nelson and Huy L Nguy\u00ean. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 117-126. IEEE, 2013.  Arkadi Nemirovski. On self-concordant convex-concave functions. Optimization Methods and  Software, 11(1-4):303-384, 1999.  ming, volume 13. Siam, 1994.  Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-  Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of differential privacy: the sparse and approximate cases. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 351-360. ACM, 2013.  Eric Price, Zhao Song, and David P. Woodruff. Fast regression with an (cid:96)\u221e guarantee. In Interna-  tional Colloquium on Automata, Languages, and Programming (ICALP), 2017.  Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank approximations with provable guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 250-263. ACM, 2016.  Mohit Singh and Weijun Xie. Approximate positive correlated distributions and approximation al- gorithms for D-optimal design. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Sympo- sium on Discrete Algorithms, pages 2240-2255. Society for Industrial and Applied Mathematics, 2018.  Zhao Song, David P. Woodruff, and Huan Zhang. Sublinear time orthogonal tensor decomposi- tion. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS) 2016, December 5-10, 2016, Barcelona, Spain, pages 793-801, 2016.  Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entrywise (cid:96)1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 688-701. ACM, 2017. A NEAR-OPTIMAL ALGORITHM FOR APPROXIMATING THE JOHN ELLIPSOID  Zhao Song, Lin F Yang, and Peilin Zhong. Sensitivity sampling over dynamic geometric data  streams with applications to k-clustering. arXiv preprint arXiv:1802.00459, 2018.  Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In  SODA. https://arxiv.org/pdf/1704.08246, 2019.  Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM  Journal on Computing, 40(6):1913-1926, 2011.  Peng Sun and Robert M Freund. Computation of minimum-volume covering ellipsoids. Operations  Research, 52(5):690-706, 2004.  optimization. Philadelphia, 2016.  Michael J. Todd. Minimum-volume ellipsoids : theory and algorithms. MOS-SIAM series on  Michael J Todd and E Alper Y\u0131ld\u0131r\u0131m. On khachiyan\u2019s algorithm for the computation of minimum-  volume enclosing ellipsoids. Discrete Applied Mathematics, 155(13):1731-1744, 2007.  Santosh Vempala. Geometric random walks: a survey. In Combinatorial and computational geome- try, volume 52 of Math. Sci. Res. Inst. Publ., pages 577-616. Cambridge Univ. Press, Cambridge, 2005.  Yining Wang, Hsiao-Yu Tung, Alexander J Smola, and Anima Anandkumar. Fast and guaranteed In Advances in Neural Information Processing Systems  tensor decomposition via sketching. (NIPS), pages 991-999. https://arxiv.org/pdf/1506.04448, 2015.  Yining Wang, Adams Wei Yu, and Aarti Singh. On computationally tractable selection of ex- periments in measurement-constrained regression models. The Journal of Machine Learning Research, 18(1):5238-5278, 2017.  David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends\u00ae in  Theoretical Computer Science, 10(1-2):1-157, 2014.  Yin Zhang and Liyan Gao. On numerical solution of the maximum volume ellipsoid problem. SIAM  Journal on Optimization, 14(1):53-76, 2003.  "}, "Artificial Constraints and Hints for Unbounded Online Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Artificial Constraints and Hints for Unbounded Online Learning", "abstract": "We provide algorithms that guarantees regret $R_T(u)\\le \\tilde O(G\\|u\\|^3 +  G(\\|u\\|+1)\\sqrt{T})$ or $R_T(u)\\le \\tilde O(G\\|u\\|^3T^{1/3} + GT^{1/3}+ G\\|u\\|\\sqrt{T})$ for online convex optimization with $G$-Lipschitz losses for any comparison point $u$ without prior knowledge of either $G$ or $\\|u\\|$. Previous algorithms dispense with the $O(\\|u\\|^3)$ term at the expense of knowledge of one or both of these parameters, while a lower bound shows that some additional penalty term over $G\\|u\\|\\sqrt{T}$ is necessary. Previous penalties were \\emph{exponential} while our bounds are polynomial in all quantities. Further, given a known bound $\\|u\\|\\le D$, our same techniques allow us to design algorithms that adapt optimally to the unknown value of $\\|u\\|$ without requiring knowledge of $G$.", "pdf_url": "http://proceedings.mlr.press/v99/cutkosky19a/cutkosky19a.pdf", "keywords": [], "reference": "Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proc. of the nineteenth annual conference on computational learning theory, 2008.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. Information Theory, IEEE Transactions on, 50(9):2050-2057, 2004.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information.  In Satyen Kale and Ohad Shamir, editors, Proc. of the 2017 Conference on Learning Theory, volume 65 of Proc. of Machine Learning Research, pages 643-677, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning  in banach spaces. arXiv preprint arXiv:1802.06293, 2018.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic  optimization. In Conference on Learning Theory (COLT), 2010.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances  in Neural Information Processing Systems 28, pages 3375-3383. 2015.  Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems, pages 6022-6032, 2017.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Alexander Rakhlin, and Peter L Bartlett. Adaptive online gradient descent. In Advances  in Neural Information Processing Systems, pages 65-72, 2008.  Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization. In Advances in neural information processing systems, pages 2402-2410, 2012.  H. Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. arXiv  preprint arXiv:1403.3465, 2014.  H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1222-1230. ACM, 2013.  Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information  Processing Systems, pages 1806-1814, 2013.  13   ARTIFICIAL CONSTRAINTS AND HINTS FOR UNBOUNDED ONLINE LEARNING  References  Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proc. of the nineteenth annual conference on computational learning theory, 2008.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. Information Theory, IEEE Transactions on, 50(9):2050-2057, 2004.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information.  In Satyen Kale and Ohad Shamir, editors, Proc. of the 2017 Conference on Learning Theory, volume 65 of Proc. of Machine Learning Research, pages 643-677, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning  in banach spaces. arXiv preprint arXiv:1802.06293, 2018.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic  optimization. In Conference on Learning Theory (COLT), 2010.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances  in Neural Information Processing Systems 28, pages 3375-3383. 2015.  Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems, pages 6022-6032, 2017.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Alexander Rakhlin, and Peter L Bartlett. Adaptive online gradient descent. In Advances  in Neural Information Processing Systems, pages 65-72, 2008.  Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization. In Advances in neural information processing systems, pages 2402-2410, 2012.  H. Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. arXiv  preprint arXiv:1403.3465, 2014.  H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1222-1230. ACM, 2013.  Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information  Processing Systems, pages 1806-1814, 2013. ARTIFICIAL CONSTRAINTS AND HINTS FOR UNBOUNDED ONLINE LEARNING  Francesco Orabona.  Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems, pages 1116-1124, 2014.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in  Neural Information Processing Systems 29, pages 577-585, 2016.  Francesco Orabona and D\u00b4avid P\u00b4al. Scale-free online learning. Theoretical Computer Science, 716:  50-69, 2018.  Francesco Orabona and Tatiana Tommasi. Backprop without learning rates through coin betting.  CoRR, abs/1705.07795, 2017. URL http://arxiv.org/abs/1705.07795.  Stephane Ross, Paul Mineiro, and John Langford. Normalized online learning. In Proc. of the 29th  Conference on Uncertainty in Artificial Intelligence (UAI), 2013.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In  Advances in Neural Information Processing Systems 23, pages 2199-2207, 2010.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc. of the 20th International Conference on Machine Learning (ICML-03), pages 928-936, 2003.  "}, "Combining Online Learning Guarantees": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Combining Online Learning Guarantees", "abstract": "We show how to take any two parameter-free online learning algorithms with different regret guarantees and obtain a single algorithm whose regret is the minimum of the two base algorithms. Our method is embarrassingly simple: just add the iterates. This trick can generate efficient algorithms that adapt to many norms simultaneously, as well as providing diagonal-style algorithms that still maintain dimension-free guarantees. We then proceed to show how a variant on this idea yields a black-box procedure for generating \\emph{optimistic} online learning algorithms. This yields the first optimistic regret guarantees in the unconstrained setting and generically increases adaptivity. Further, our optimistic algorithms are guaranteed to do no worse than their non-optimistic counterparts regardless of the quality of the optimistic estimates provided to the algorithm.", "pdf_url": "http://proceedings.mlr.press/v99/cutkosky19b/cutkosky19b.pdf", "keywords": [], "reference": "Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning  algorithms. Information Theory, IEEE Transactions on, 50(9):2050-2057, 2004.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, pages 6-1, 2012.  12   COMBINING ONLINE LEARNING GUARANTEES  It may be possible to improve the above technique in the unconstrained setting. Since our unconstrained optimistic guarantee depends on (cid:80)T t=1(cid:104)gt, gt \u2212 2ht(cid:105) rather than (cid:80)T t=1 (cid:107)gt \u2212 ht(cid:107)2, we can set (cid:96)t(h) = (cid:104)gt, gt \u2212 2h(cid:105) to obtain a potentially tighter bound. Notice now that (cid:96)t is no longer strongly-convex, but it is still convex. Thus we can use an adaptive gradient descent algorithm with domain {(cid:107)h(cid:107) \u2264 1} (e.g. Adagrad) (Duchi et al., 2010; McMahan and Streeter, 2010) to obtain:  t=1 (cid:107)gt \u2212 ht(cid:107)2 \u2212 (cid:107)ht(cid:107)2 = (cid:80)T  T (cid:88)  t=1  (cid:96)t(ht) \u2212 (cid:96)t(h) \u2264 O  \uf8eb  \uf8ed  (cid:118) (cid:117) (cid:117) (cid:116)  T (cid:88)  t=1  \uf8f6  (cid:107)gt(cid:107)2  \uf8f8  In this case the optimal value of h is \u2212  , so that we have  T (cid:88)  t=1  (cid:107)gt \u2212 ht(cid:107)2 \u2212 (cid:107)ht(cid:107)2 \u2264  (cid:107)gt \u2212 h(cid:107)2 \u2212 (cid:107)h(cid:107)2 + (cid:96)t(ht) \u2212 (cid:96)t(h)  \u2264  (cid:107)gt(cid:107)2 \u2212 2  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  T (cid:88)  t=1  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  gt  + O  \uf8eb  \uf8ed  (cid:118) (cid:117) (cid:117) (cid:116)  T (cid:88)  t=1  \uf8f6  (cid:107)gt(cid:107)2  \uf8f8  (cid:80)T (cid:107) (cid:80)T  t=1 gt t=1 gt(cid:107)  T (cid:88)  t=1  T (cid:88)  t=1  If we then apply the optimistic bound of Theorem 3, we have  RT (u) \u2264 \u02dcO  (cid:15) + (cid:107)u(cid:107)  (cid:107)gt(cid:107)2 \u2212 2  gt  +  (cid:107)gt(cid:107)2  \uf8eb  \uf8ec \uf8ec \uf8ed  (cid:118) (cid:117) (cid:117) (cid:117) (cid:116)  T (cid:88)  t=1  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  T (cid:88)  t=1  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  (cid:118) (cid:117) (cid:117) (cid:116)  T (cid:88)  t=1  \uf8f6  \uf8f7 \uf8f7 \uf8f8  7. Conclusion  We introduced the simple strategy of adding iterates as a method for obtaining best-of-all-worlds style regret guarantees in parameter-free online learning. Further, a variation on this technique yields optimistic regret bounds. Our optimistic algorithm is a generic reduction that converts any adaptive online learning algorithm into an optimistic algorithm. This extends optimism to unconstrained domains, allows algorithms to use many sequences of hints, and does not degrade performance when the hints are poor. Finally, we provide a simple technique that competes with the best fixed hint, which can be used to provide a simple proof of an empirical Bernstein bound. Intuitively, we achieved optimism by combining an algorithm that had an excellent best-case guarantee but a poor worst-case guarantee with a \u201csafety-net\u201d algorithm that had reasonable worst-case guarantees. It is our hope that similar synergies with other algorithms will yield further increases in adaptivity.  References  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning  algorithms. Information Theory, IEEE Transactions on, 50(9):2050-2057, 2004.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, pages 6-1, 2012. COMBINING ONLINE LEARNING GUARANTEES  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in banach  spaces. arXiv preprint arXiv:1802.06293, 2018.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimiza-  tion. In Conference on Learning Theory (COLT), 2010.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 3375-3383. Curran Associates, Inc., 2015.  Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via  model selection. In Advances in Neural Information Processing Systems, pages 6020-6030, 2017.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Sufficient statistics and the  burkholder method. In Conference on Learning Theory (COLT), 2018.  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs.  Machine learning, 80(2-3):165-188, 2010.  Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penalization.  arXiv preprint arXiv:0907.3740, 2009.  Brendan Mcmahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimization.  In Advances in neural information processing systems, pages 2402-2410, 2012.  H. Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. arXiv preprint  arXiv:1403.3465, 2014.  H Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Min- imax algorithms and normal approximations. In Conference on Learning Theory (COLT), pages 1020- 1039, 2014.  H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization.  In Conference on Learning Theory (COLT), 2010.  Mehryar Mohri and Scott Yang. Accelerating online convex optimization via adaptive prediction. In Artificial  Intelligence and Statistics, pages 848-856, 2016.  Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information Processing  Systems, pages 1806-1814, 2013.  Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochastic learn-  ing. In Advances in Neural Information Processing Systems, pages 1116-1124, 2014.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning.  In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems 29, pages 577-585. Curran Associates, Inc., 2016.  Francesco Orabona and Tatiana Tommasi.  through coin betting. nual Conference on Neural Beach, CA, USA, pages 2157-2167, 6811-training-deep-networks-without-learning-rates-through-coin-betting.  learning rates An- Information Processing Systems 2017, 4-9 December 2017, Long URL http://papers.nips.cc/paper/  Information Processing Systems 30:  Training deep networks without  In Advances  in Neural  2017.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference on  Learning Theory (COLT), pages 993-1019, 2013. COMBINING ONLINE LEARNING GUARANTEES  Alexander Rakhlin and Karthik Sridharan. On equivalence of martingale tail bounds and deterministic regret  inequalities. arXiv preprint arXiv:1510.03925, 2015.  Stephane Ross, Paul Mineiro, and John Langford. Normalized online learning. In Proceedings of the 29th  Conference on Uncertainty in Artificial Intelligence (UAI), 2013.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine  Learning, 4(2):107-194, 2011.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings  of the 20th International Conference on Machine Learning (ICML-03), pages 928-936, 2003.  "}, "Learning from Weakly Dependent Data under Dobrushin\u2019s Condition": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning from Weakly Dependent Data under Dobrushin\u2019s Condition", "abstract": "Statistical learning theory has largely focused on learning and generalization given independent and identically distributed (i.i.d.) samples. Motivated by applications involving time-series data, there has been a growing literature on learning and generalization in settings where data is sampled from an ergodic process. This work has also developed complexity measures, which appropriately extend the notion of Rademacher complexity to bound the generalization error and learning rates of hypothesis classes in this setting. Rather than time-series data, our work is  motivated  by settings where data  is sampled on a network or a spatial domain, and thus do not fit well within the framework of prior work. We provide learning and generalization bounds for data that are complexly dependent, yet their distribution satisfies the standard Dobrushin\u2019s condition. Indeed, we show that the standard complexity measures of Gaussian and Rademacher complexities and VC dimension are sufficient measures of complexity for the purposes of bounding the generalization error and learning rates of hypothesis classes in our setting. Moreover, our generalization bounds only degrade by constant factors compared to their i.i.d.\u00a0analogs, and our learnability bounds degrade by log factors in the size of the training set.", "pdf_url": "http://proceedings.mlr.press/v99/dagan19a/dagan19a.pdf", "keywords": [], "reference": "Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent  data. IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Witold Bednorz and Rafal Latala. On the boundedness of bernoulli processes. Annals of Mathe-  matics, 180(3):1167-1203, 2014.  Patrizia Berti, Irene Crimaldi, Luca Pratelli, Pietro Rigo, et al. Rate of convergence of predictive  distributions for dependent data. Bernoulli, 15(4):1351-1367, 2009.  Marianne Bertrand, Erzo FP Luttmer, and Sendhil Mullainathan. Network effects and welfare cul-  tures. The Quarterly Journal of Economics, 115(3):1019-1055, 2000.  Yann Bramoull\u00e9, Habiba Djebbari, and Bernard Fortin. Identification of peer effects through social  networks. Journal of econometrics, 150(1):41-55, 2009.  Sourav Chatterjee. Concentration Inequalities with Exchangeable Pairs. PhD thesis, Stanford  University, June 2005a.  math/0507526, 2005b.  Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph. d. thesis). arXiv preprint  Nicholas A Christakis and James H Fowler. Social contagion theory: examining dynamic social  networks and human behavior. Statistics in medicine, 32(4):556-577, 2013.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising models.  In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201918, Philadelphia, PA, USA, 2018. SIAM.  Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning via the lens of compression. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 2792-2800. Curran Associates Inc., 2016.  PL Dobrushin. The description of a random field by means of conditional probabilities and condi-  tions of its regularity. Theory of Probability & Its Applications, 13(2):197-224, 1968.  RL Dobrushin and SB Shlosman. Completely analytical interactions: constructive description.  Journal of Statistical Physics, 46(5-6):983-1014, 1987.  Esther Du\ufb02o and Emmanuel Saez. The role of information and social interactions in retirement plan decisions: Evidence from a randomized experiment. The Quarterly journal of economics, 118 (3):815-842, 2003.  Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121  (2):256-285, 1995.  13   LEARNING FROM WEAKLY DEPENDENT DATA  References  Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent  data. IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Witold Bednorz and Rafal Latala. On the boundedness of bernoulli processes. Annals of Mathe-  matics, 180(3):1167-1203, 2014.  Patrizia Berti, Irene Crimaldi, Luca Pratelli, Pietro Rigo, et al. Rate of convergence of predictive  distributions for dependent data. Bernoulli, 15(4):1351-1367, 2009.  Marianne Bertrand, Erzo FP Luttmer, and Sendhil Mullainathan. Network effects and welfare cul-  tures. The Quarterly Journal of Economics, 115(3):1019-1055, 2000.  Yann Bramoull\u00e9, Habiba Djebbari, and Bernard Fortin. Identification of peer effects through social  networks. Journal of econometrics, 150(1):41-55, 2009.  Sourav Chatterjee. Concentration Inequalities with Exchangeable Pairs. PhD thesis, Stanford  University, June 2005a.  math/0507526, 2005b.  Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph. d. thesis). arXiv preprint  Nicholas A Christakis and James H Fowler. Social contagion theory: examining dynamic social  networks and human behavior. Statistics in medicine, 32(4):556-577, 2013.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising models.  In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201918, Philadelphia, PA, USA, 2018. SIAM.  Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning via the lens of compression. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 2792-2800. Curran Associates Inc., 2016.  PL Dobrushin. The description of a random field by means of conditional probabilities and condi-  tions of its regularity. Theory of Probability & Its Applications, 13(2):197-224, 1968.  RL Dobrushin and SB Shlosman. Completely analytical interactions: constructive description.  Journal of Statistical Physics, 46(5-6):983-1014, 1987.  Esther Du\ufb02o and Emmanuel Saez. The role of information and social interactions in retirement plan decisions: Evidence from a randomized experiment. The Quarterly journal of economics, 118 (3):815-842, 2003.  Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121  (2):256-285, 1995. LEARNING FROM WEAKLY DEPENDENT DATA  Reza Gheissari, Eyal Lubetzky, and Yuval Peres. Concentration inequalities for polynomials of  contracting Ising models. arXiv preprint arXiv:1706.00121, 2017.  Edward L Glaeser, Bruce Sacerdote, and Jose A Scheinkman. Crime and social interactions. The  Quarterly Journal of Economics, 111(2):507-548, 1996.  Kathleen Mullan Harris, National Longitudinal Study of Adolescent Health, et al. Waves i & ii, 1994-1996; wave iii, 2001-2002; wave iv, 2007-2009 [machine-readable data file and documen- tation]. Chapel Hill, NC: Carolina Population Center, University of North Carolina at Chapel Hill, 10, 2009.  Christof K\u00fclske. Concentration inequalities for functions of gibbs fields with application to diffrac- tion and random gibbs measures. Communications in mathematical physics, 239(1-2):29-51, 2003.  H K\u00fcnsch. Decay of correlations under dobrushin\u2019s uniqueness condition and its applications.  Communications in Mathematical Physics, 84(2):207-222, 1982.  Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for time series prediction with non- In Peter Auer, Alexander Clark, Thomas Zeugmann, and Sandra Zilles, stationary processes. editors, Algorithmic Learning Theory, pages 260-274, Cham, 2014. Springer International Pub- lishing. ISBN 978-3-319-11662-4.  Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non- stationary time series. In Advances in neural information processing systems, pages 541-549, 2015.  Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for non-stationary mixing processes. Machine Learning, 106(1):93-117, Jan 2017. doi: 10.1007/s10994-016-5588-2. URL https: //doi.org/10.1007/s10994-016-5588-2.  Tianxi Li, Elizaveta Levina, and Ji Zhu. Prediction models for network-linked data. arXiv preprint  arXiv:1602.01192, 2016.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. 1986.  Charles F Manski. Identification of endogenous social effects: The re\ufb02ection problem. The review  of economic studies, 60(3):531-542, 1993.  Katalin Marton et al. Bounding \u00afd-distance by informational divergence: A method to prove measure  concentration. The Annals of Probability, 24(2):857-866, 1996.  Daniel J. McDonald and Cosma Rohilla Shalizi. Rademacher complexity of stationary sequences.  arXiv preprint arXiv:1106.0730, 2017.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes.  In Advances in Neural Information Processing Systems, pages 1097-1104, 2009.  Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary \u03d5-mixing and \u03b2-mixing  processes. Journal of Machine Learning Research, 11(Feb):789-814, 2010. LEARNING FROM WEAKLY DEPENDENT DATA  Andrea Montanari and Amin Saberi. The spread of innovations in social networks. Proceedings of the National Academy of Sciences, 107(47):20196-20201, 2010. ISSN 0027-8424. doi: 10.1073/ pnas.1004098107. URL https://www.pnas.org/content/107/47/20196.  Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes. Journal of the  ACM (JACM), 63(3):21, 2016.  Vladimir Pestov. Predictive pac learnability: A paradigm for learning from exchangeable input data. In Granular Computing (GrC), 2010 IEEE International Conference on, pages 387-391. IEEE, 2010.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Sys- tems, pages 1984-1992, 2010.  Bruce Sacerdote. Peer effects with random assignment: Results for dartmouth roommates. The  Quarterly journal of economics, 116(2):681-704, 2001.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Daniel W Stroock and Boguslaw Zegarlinski. The logarithmic sobolev inequality for discrete spin  systems on a lattice. Communications in Mathematical Physics, 149(1):175-193, 1992.  Michel Talagrand et al. Majorizing measures: the generic chaining. The Annals of Probability, 24  (3):1049-1103, 1996.  Nicole Tomczak-Jaegermann. Banach-Mazur distances and finite-dimensional operator ideals, vol-  ume 38. Longman Sc & Tech, 1989.  Justin G Trogdon, James Nonnemaker, and Joanne Pais. Peer effects in adolescent overweight.  Journal of health economics, 27(5):1388-1399, 2008.  Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of  events to their probabilities. In Measures of complexity, pages 11-30. Springer, 2015.  Dror Weitz. Combinatorial criteria for uniqueness of gibbs measures. Random Structures & Algo-  rithms, 27(4):445-475, 2005.  Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals  of Probability, pages 94-116, 1994. "}, "Space lower bounds for linear prediction in the streaming model": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Space lower bounds for linear prediction in the streaming model", "abstract": "We show that fundamental learning tasks, such as  finding an approximate linear separator or linear regression, require memory at least  \\emph{quadratic} in the dimension, in a natural streaming setting. This  implies that such problems cannot be solved (at least in this setting) by  scalable memory-efficient streaming algorithms. Our  results build  on a memory lower bound for a simple linear-algebraic problem  \u2013 finding approximate null vectors \u2013 and utilize the estimates on the packing of the Grassmannian, the manifold of all linear subspaces of fixed dimension.", "pdf_url": "http://proceedings.mlr.press/v99/dagan19b/dagan19b.pdf", "keywords": ["streaming", "memory", "communication", "linear classification", "linear regression"], "reference": "Shiri Artstein-Avidan, Apostolos Giannopoulos, and Vitali D Milman. Asymptotic geometric anal-  ysis, Part I, volume 202. American Mathematical Soc., 2015.  Maria-Florina Balcan, Yi Li, David P. Woodruff, and Hongyang Zhang. Testing matrix rank, op- timally. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019., pages 727-746, 2019. doi: 10.1137/1.9781611975482.46.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning from small test spaces: Learning low degree polynomial functions. arXiv preprint arXiv:1708.02640, 2017.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning finite functions from random evaluations, with applications to polynomials. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pages 843-856, 2018.  Avrim Blum. Random projection, margins, kernels, and feature-selection.  In Subspace, Latent  Structure and Feature Selection, pages 52-68. Springer, 2006.  11   SPACE LOWER BOUNDS FOR LINEAR PREDICTION IN THE STREAMING MODEL  Reducing Thm. 7 from Lemma 5 We consider a variant of Lemma 5 where the vector w\u2217 or- thogonal to U and V satisfies e(cid:62) 1 w\u2217 \u2265 cf (Lemma 31). We show that if A is a protocol for finding a linear separator, there exists a protocol A(cid:48) for finding an approximate null vector with the same amount of communication.  Here is how A(cid:48) is created, based on A. The first party, given V \u2208 Gr(d/2, d), creates an auxiliary distribution DV over pairs (x, y), with the following property: Any hyperplane w \u2208 Sd\u22121 with low classification error on DV , satisfies (cid:107)ProjV (w)(cid:107)2 \u2248 0. Similarly, the second party will create an auxiliary distribution DU , such that any approximate separator w satisfies (cid:107)ProjU (w)(cid:107)2 \u2248 0. In particular, any hyperplane with low error on the uniform mixture of DV and DU satisfies: (cid:107)ProjV (w)(cid:107)2 + (cid:107)ProjU (w)(cid:107)2 \u2248 0.  Each party draws m = \u2126(d) samples from their corresponding distribution (DV or DU ). Then, they simulate A to find a hyperplane \u02c6w with low classification error on the mixed sample. Since the class of linear separators over Rd is of VC dimension d, \u02c6w has low classification error on the mixture of DV and DU , hence it satisfies (cid:107)ProjV (w)(cid:107)2 + (cid:107)ProjU (w)(cid:107)2 \u2248 0, as required. Lemma 5 states that the communication of A(cid:48) is \u2126(d2), hence the communication of A is \u2126(d2) as well.  Here is how a random pair (x, y) is drawn from DV (DU is analogously defined): First a random point x(cid:48) is drawn uniformly from V \u2229 Sd\u22121. Then, set (x, y) = (x+, 1) with probability 1/2 and (x, y) = (x\u2212, \u22121) with probability 1/2, where x+ = x(cid:48) + \u0398(e1/ d). For any fixed w \u2208 Sd\u22121, if x is drawn uniformly from V \u2229 Sd\u22121 then w(cid:62)x \u223c N (0, (cid:107)ProjV (w)(cid:107)2) (approximately, see Lemma 24). From the definition of DV , any hyperplane w with low clas- sification error on DV satisfies w(cid:62)x \u2248 0 for most x \u2208 V \u2229 Sd\u22121, hence any such w satisfies (cid:107)ProjV (w)(cid:107)2 \u2248 0, as required.  d) and x\u2212 = x(cid:48) \u2212 \u0398(e1/  \u221a  \u221a  This research is supported in part by European Research Council (ERC) grant 754705.  Acknowledgments  References  Shiri Artstein-Avidan, Apostolos Giannopoulos, and Vitali D Milman. Asymptotic geometric anal-  ysis, Part I, volume 202. American Mathematical Soc., 2015.  Maria-Florina Balcan, Yi Li, David P. Woodruff, and Hongyang Zhang. Testing matrix rank, op- timally. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019., pages 727-746, 2019. doi: 10.1137/1.9781611975482.46.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning from small test spaces: Learning low degree polynomial functions. arXiv preprint arXiv:1708.02640, 2017.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning finite functions from random evaluations, with applications to polynomials. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pages 843-856, 2018.  Avrim Blum. Random projection, margins, kernels, and feature-selection.  In Subspace, Latent  Structure and Feature Selection, pages 52-68. Springer, 2006. SPACE LOWER BOUNDS FOR LINEAR PREDICTION IN THE STREAMING MODEL  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruff. Commu- nication lower bounds for statistical estimation problems via a distributed data processing in- In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Com- equality. puting, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 1011-1020, 2016. doi: 10.1145/2897518.2897582.  Vladimir Braverman, Stephen R. Chestnut, Robert Krauthgamer, Yi Li, David P. Woodruff, and Lin F. Yang. Matrix norms in data streams: Faster, multi-pass and row-order. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00a8assan, Stock- holm, Sweden, July 10-15, 2018, pages 648-657, 2018.  Jeff I Chu and Georg Schnitger. The communication complexity of several problems in matrix  computation. Journal of Complexity, 7(4):395-407, 1991.  Jeff I Chu and Georg Schnitger. Communication complexity of matrix computation over finite  fields. Mathematical systems theory, 28(3):215-228, 1995.  Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.  Michael B. Cohen, Jelani Nelson, and David P. Woodruff. Optimal approximate matrix prod- In 43rd International Colloquium on Automata, Languages, and uct in terms of stable rank. Programming, ICALP 2016, July 11-15, 2016, Rome, Italy, pages 11:1-11:14, 2016. doi: 10.4230/LIPIcs.ICALP.2016.11.  Yuval Dagan and Ohad Shamir. Detecting correlations with little memory and communication. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pages 1145- 1198, 2018.  Wei Dai, Brian C Rider, and Youjian Liu. Volume growth and general rate quantization on grass- In Global Telecommunications Conference, 2007. GLOBECOM\u201907. IEEE,  mann manifolds. pages 1441-1445. IEEE, 2007.  Amit Daniely and Vitaly Feldman. Learning without interaction requires separation. CoRR,  abs/1809.09165, 2018.  Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for learning.  arXiv preprint arXiv:1708.02639, 2017.  Sudipto Guha and Andrew McGregor. Tight lower bounds for multi-pass stream computation via In International Colloquium on Automata, Languages, and Programming,  pass elimination. pages 760-772. Springer, 2008.  Daniel M Kane, Roi Livni, Shay Moran, and Amir Yehudayoff. On communication complexity of  classification problems. arXiv preprint arXiv:1711.05893, 2017.  Gillat Kol, Ran Raz, and Avishay Tal. Time-space hardness of learning sparse parities. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1067-1080. ACM, 2017. SPACE LOWER BOUNDS FOR LINEAR PREDICTION IN THE STREAMING MODEL  Roie Levin, Anish Prasad Sevekari, and David P. Woodruff. Robust subspace approximation in a stream. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00b4eal, Canada., pages 10706-10716, 2018.  Yi Li, Xiaoming Sun, Chengu Wang, and David P Woodruff. On the communication complex- ity of linear algebraic problems in the message passing model. In International Symposium on Distributed Computing, pages 499-513. Springer, 2014.  Dana Moshkovitz and Michal Moshkovitz. Mixing implies lower bounds for space bounded learn-  ing. In Conference on Learning Theory, pages 1516-1566, 2017.  Dana Moshkovitz and Michal Moshkovitz. Entropy samplers and strong generic lower bounds for space bounded learning. In 9th Innovations in Theoretical Computer Science Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, pages 28:1-28:20, 2018. doi: 10.4230/LIPIcs. ITCS.2018.28.  Ran Raz. Fast learning requires good memory: A time-space lower bound for parity learning. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 266- 275. IEEE, 2016.  Ran Raz. A time-space lower bound for a large class of learning problems.  In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 732-742. IEEE, 2017.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Vatsal Sharan, Aaron Sidford, and Gregory Valiant. Memory-sample tradeoffs for linear regression  with small error. arXiv preprint arXiv:1904.08544, 2019.  Jacob Steinhardt and John Duchi. Minimax rates for memory-bounded sparse linear regression. In  Conference on Learning Theory, pages 1564-1587, 2015.  Xiaoming Sun and Chengu Wang. Randomized communication complexity for linear algebra prob- lems over finite fields. In STACS\u201912 (29th Symposium on Theoretical Aspects of Computer Sci- ence), volume 14, pages 477-488. LIPIcs, 2012.  Stanislaw J Szarek. Spaces with large distance to (cid:96)n  \u221e and random matrices. American Journal of  Mathematics, 112(6):899-942, 1990.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Feng Wei. Upper bound for intermediate singular values of random matrices. Journal of Mathe-  matical Analysis and Applications, 445(2):1530-1547, 2017.  Ke Ye and Lek-Heng Lim. Schubert varieties and distances between subspaces of different dimen-  sions. SIAM Journal on Matrix Analysis and Applications, 37(3):1176-1197, 2016. SPACE LOWER BOUNDS FOR LINEAR PREDICTION IN THE STREAMING MODEL  Yuchen Zhang, Martin Wainwright, and Michael Jordan. Distributed estimation of generalized matrix rank: Efficient algorithms and lower bounds. In International Conference on Machine Learning, pages 457-465, 2015.  "}, "Computationally and Statistically Efficient Truncated Regression": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Computationally and Statistically Efficient Truncated Regression", "abstract": "We provide a computationally and statistically efficient estimator for the classical problem of truncated linear regression, where the dependent variable $y = \\vec{w}^{\\rm T} \\vec{x}+{\\varepsilon}$ and its corresponding vector of covariates $\\vec{x} \\in \\mathbb{R}^k$ are only revealed if the dependent variable falls in some subset $S \\subseteq \\mathbb{R}$; otherwise the existence of the pair $(\\vec{x},y)$ is hidden. This problem has remained a challenge since the early works of Tobin 1958, Amemiya et al. 1973, HAusman et al 1977, Breen et al. 1996, its applications are abundant, and its history dates back even further to the work of Galton 1897, Pearson 1908, Lee 1915, and Fisher 1931. While consistent estimators of the regression coefficients have been identified, the error rates are not well-understood, especially in high-dimensional settings. Under a \u201cthickness assumption\u201d about the covariance matrix of the covariates in the revealed sample, we provide a computationally efficient estimator for the coefficient vector $\\vec{w}$ from $n$ revealed samples that attains $\\ell_2$ error $\\tilde{O}(\\sqrt{k / n})$, almost recovering the guarantees of least squares in the standard (untruncated) linear regression setting. Our estimator uses Projected Stochastic Gradient Descent (PSGD) on the negative log-likelihood of the truncated sample, and only needs oracle access to the set $S$, which may otherwise be arbitrary, and in particular may be non-convex. PSGD must be restricted to an appropriately defined convex cone to guarantee that the negative log-likelihood is strongly convex, which in turn is established using concentration of matrices on variables with sub-exponential tails. We perform experiments on simulated data to illustrate the accuracy of our estimator. As a corollary of our work, we show that SGD provably learns the parameters of single-layer neural networks with noisy Relu activation functions (see Nair and Hinton 2010, Bengio et al. 2013, Gulcehre et al. 2016), given linearly many, in the number of network parameters, input-output pairs in the realizable setting.", "pdf_url": "http://proceedings.mlr.press/v99/daskalakis19a/daskalakis19a.pdf", "keywords": ["linear regression", "truncated statistics", "truncated regression", "stochastic gradient descent"], "reference": "Takeshi Amemiya. Regression analysis when the dependent variable is truncated normal. Econo-  metrica: Journal of the Econometric Society, pages 997-1016, 1973.  N Balakrishnan and Erhard Cramer. The art of progressive censoring. Springer, 2014.  4   TRUNCATED LINEAR REGRESSION  Nevertheless, we show that, given oracle access to set S, we can get an un-biased sample of the gradient. If (xt, yt) is a (randomly chosen) sample processed by PSGD at step t, and wt the current iterate, we perform rejection sampling to obtain a sample from the Gaussian N (wT t xt, 1) condi- tioned on the truncation set S, in order to compute an unbiased estimate of the gradient. Because we use rejection sampling, it is important to maintain that PSGD remains within a region where the rejection sampling will succeed with constant probability with respect to a random choice of xt, and this is guaranteed by our analysis.  1.2. Further Related Work  We have already surveyed work on truncated and censored linear regression since the 1950s. Early precursors of this literature can be found in the simpler, non-regression version of our problem, where the x(i)\u2019s are single-dimensional and equal, which corresponds to estimating a truncated Normal distribution. This problem goes back to at least Galton (1897), Pearson (1902), Pearson and Lee (1908), and Fisher (1931). Following these early works, there has been a large volume of research devoted to estimating truncated Gaussians or other truncated distributions in one or multiple dimensions; see e.g. Hotelling (1948); Tukey (1949), and Schneider (1986); Cohen (2016); Balakrishnan and Cramer (2014) for an overview of this work. There do exist consistent estimators for estimating the parameters of truncated distributions, but, as in the case of truncated and censored regression, the optimal estimation rates are mostly not well-understood. Only very recentwork of Daskalakis et al. (2018) provides computationally and statistically efficient estimators for the parameters of truncated high-dimensional Gaussians. Similar to the present work, Daskalakis et al. (2018) we use PSGD to optimize the negative log-likelihood of the truncated samples. Showing that the negative log-likelihood is convex in the truncated Gaussian setting follows immediately from the fact that a truncated Gaussian belongs to the exponential family. In our setting it is non- standard; see also discussion in Amemiya (1973). Moreover, identifying the set where the negative log-likelihood is strongly convex and establishing its strong convexity are also simpler tasks in the truncated Gaussian setting compared to the truncated regression setting, due to the shifting of the mean of the samples induced by the different covariate vectors x(i).  Last but not least, our work is related, albeit more loosely, to the literature on robust Statistics, which has recently been revived by a strand of fantastic works Xu et al. (2010); Cand`es et al. (2011); Diakonikolas et al. (2016); Lai et al. (2016); Diakonikolas et al. (2017, 2018); Bhatia et al. (2015); Diakonikolas et al. (2019). For the most part, these works assume that an adversary perturbs a small fraction of the samples arbitrarily. Compared to truncation and censoring, these perturbations are harder to handle. As such only small amounts of perturbation can be accommodated, and the parameters cannot be estimated to arbitrary precision. In contrast, in our setting the truncation set S may very well have an ex ante probability of obliterating most of the observations, say 99% of them, yet the parameters of the model can still be estimated to arbitrary precision.  References  Takeshi Amemiya. Regression analysis when the dependent variable is truncated normal. Econo-  metrica: Journal of the Econometric Society, pages 997-1016, 1973.  N Balakrishnan and Erhard Cramer. The art of progressive censoring. Springer, 2014. TRUNCATED LINEAR REGRESSION  Yoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.  Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Richard Breen et al. Regression models: Censored, sample selected, or truncated data, volume 111.  Sage, 1996.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  A Clifford Cohen. Truncated and censored samples: theory and applications. CRC press, 2016.  Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In the 59th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2018.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In Proceedings of the 34th International Being robust (in high dimensions) can be practical. Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 999-1008, 2017. URL http://proceedings.mlr.press/v70/diakonikolas17a. html.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a gaussian: Getting optimal error, efficiently. In Proceedings of the Twenty- Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2683-2702, 2018. doi: 10.1137/1.9781611975031.171. URL https://doi.org/10.1137/1.9781611975031.171.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for In the 30th Annual ACM-SIAM Symposium on Discrete Algorithms  robust linear regression. (SODA), 2019.  RA Fisher. Properties and applications of Hh functions. Mathematical tables, 1:815-852, 1931.  Francis Galton. An examination into the registered speeds of american trotting horses, with remarks on their value as hereditary data. Proceedings of the Royal Society of London, 62(379-387):310- 315, 1897.  Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.  In International Conference on Machine Learning, pages 3059-3068, 2016. TRUNCATED LINEAR REGRESSION  Jerry A Hausman and David A Wise. Social experimentation, truncated distributions, and efficient  estimation. Econometrica: Journal of the Econometric Society, pages 919-938, 1977.  Harold Hotelling. Fitting generalized truncated normal distributions. In Annals of Mathematical  Statistics, volume 19, pages 596-596, 1948.  Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 665-674, 2016. doi: 10.1109/ FOCS.2016.76. URL https://doi.org/10.1109/FOCS.2016.76.  Alice Lee. Table of the Gaussian \u201cTail\u201d Functions; When the \u201cTail\u201d is Larger than the Body.  Biometrika, 10(2/3):208-214, 1914.  Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807- 814, 2010.  Karl Pearson. On the systematic fitting of frequency curves. Biometrika, 2:2-7, 1902.  Karl Pearson and Alice Lee. On the generalised probable error in multiple normal correlation.  Biometrika, 6(1):59-68, 1908.  Helmut Schneider. Truncated and censored samples from normal populations. Marcel Dekker, Inc.,  1986.  309-311, 1949.  James Tobin. Estimation of relationships for limited dependent variables. Econometrica: journal  of the Econometric Society, pages 24-36, 1958.  John W Tukey. Sufficiency, truncation and selection. The Annals of Mathematical Statistics, pages  Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis with contami-  nated data: The high dimensional case. arXiv preprint arXiv:1002.4658, 2010. "}, "Reconstructing Trees from Traces": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Reconstructing Trees from Traces", "abstract": "We study the problem of learning a node-labeled tree given independent traces from an appropriately defined deletion channel. This problem, tree trace reconstruction, generalizes string trace reconstruction, which corresponds to the tree being a path. For many classes of trees, including complete trees and spiders, we provide algorithms that reconstruct the labels using only a polynomial number of traces. This exhibits a stark contrast to known results on string trace reconstruction, which require exponentially many traces, and where a central open problem is to determine whether a polynomial number of traces suffice. Our techniques combine novel combinatorial and complex analytic methods.", "pdf_url": "http://proceedings.mlr.press/v99/davies19a/davies19a.pdf", "keywords": ["Trace Reconstruction", "Sample Complexity", "Deletion Channel"], "reference": "Tugkan Batu, Sampath Kannan, Sanjeev Khanna, and Andrew McGregor. Reconstructing strings from random traces. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 910-918, 2004. URL http://dl.acm.org/citation.cfm? id=982792.982929.  Philip Bille. A survey on tree edit distance and related problems. Theor. Comput. Sci., 337(1-3): 217-239, 2005. doi: 10.1016/j.tcs.2004.12.030. URL https://doi.org/10.1016/j. tcs.2004.12.030.  George M. Church, Yuan Gao, and Sriram Kosuri. Next-Generation Digital Information Storage in  DNA. Science, 337(6102):1628, 2012.  Anindya De, Ryan O\u2019Donnell, and Rocco A. Servedio. Optimal mean-based algorithms for In Proceedings of the 49th Annual ACM SIGACT Symposium on The- trace reconstruction. ory of Computing (STOC), pages 1047-1056, 2017. doi: 10.1145/3055399.3055450. URL http://doi.acm.org/10.1145/3055399.3055450.  Lisa Hartung, Nina Holden, and Yuval Peres. Trace reconstruction with varying deletion prob- In Proceedings of the Fifteenth Workshop on Analytic Algorithmics and Combina- doi: 10.1137/1.9781611975062.6. URL https:  abilities. torics (ANALCO), pages 54-61, 2018. //doi.org/10.1137/1.9781611975062.6.  Nina Holden and Russell Lyons. Lower bounds for trace reconstruction. Preprint available at  https://arxiv.org/abs/1808.02336, 2018.  Nina Holden, Robin Pemantle, and Yuval Peres. Subpolynomial trace reconstruction for random strings and arbitrary deletion probability. In Proceedings of the 31st Conference On Learning Theory (COLT), pages 1799-1840, 2018. URL http://proceedings.mlr.press/v75/ holden18a.html.  Thomas Holenstein, Michael Mitzenmacher, Rina Panigrahy, and Udi Wieder. Trace reconstruction with constant deletion probability and related results. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 389-398, 2008. URL http: //dl.acm.org/citation.cfm?id=1347082.1347125.  Phillip Karau and Vincent Tabard-Cossa. Capture and translocation characteristics of short doi:  branched dna labels in solid-state nanopores. ACS Sensors, 3(7):1308-1315, 2018. 10.1021/acssensors.8b00165.  Andrew McGregor, Eric Price, and Sofya Vorotnikova. Trace Reconstruction Revisited. In Euro-  pean Symposium on Algorithms (ESA), pages 689-700. Springer, 2014.  Fedor Nazarov and Yuval Peres. Trace reconstruction with exp(O(n1/3)) samples. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1042- 1046, 2017. doi: 10.1145/3055399.3055494. URL http://doi.acm.org/10.1145/ 3055399.3055494.  13   RECONSTRUCTING TREES FROM TRACES  References  Tugkan Batu, Sampath Kannan, Sanjeev Khanna, and Andrew McGregor. Reconstructing strings from random traces. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 910-918, 2004. URL http://dl.acm.org/citation.cfm? id=982792.982929.  Philip Bille. A survey on tree edit distance and related problems. Theor. Comput. Sci., 337(1-3): 217-239, 2005. doi: 10.1016/j.tcs.2004.12.030. URL https://doi.org/10.1016/j. tcs.2004.12.030.  George M. Church, Yuan Gao, and Sriram Kosuri. Next-Generation Digital Information Storage in  DNA. Science, 337(6102):1628, 2012.  Anindya De, Ryan O\u2019Donnell, and Rocco A. Servedio. Optimal mean-based algorithms for In Proceedings of the 49th Annual ACM SIGACT Symposium on The- trace reconstruction. ory of Computing (STOC), pages 1047-1056, 2017. doi: 10.1145/3055399.3055450. URL http://doi.acm.org/10.1145/3055399.3055450.  Lisa Hartung, Nina Holden, and Yuval Peres. Trace reconstruction with varying deletion prob- In Proceedings of the Fifteenth Workshop on Analytic Algorithmics and Combina- doi: 10.1137/1.9781611975062.6. URL https:  abilities. torics (ANALCO), pages 54-61, 2018. //doi.org/10.1137/1.9781611975062.6.  Nina Holden and Russell Lyons. Lower bounds for trace reconstruction. Preprint available at  https://arxiv.org/abs/1808.02336, 2018.  Nina Holden, Robin Pemantle, and Yuval Peres. Subpolynomial trace reconstruction for random strings and arbitrary deletion probability. In Proceedings of the 31st Conference On Learning Theory (COLT), pages 1799-1840, 2018. URL http://proceedings.mlr.press/v75/ holden18a.html.  Thomas Holenstein, Michael Mitzenmacher, Rina Panigrahy, and Udi Wieder. Trace reconstruction with constant deletion probability and related results. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 389-398, 2008. URL http: //dl.acm.org/citation.cfm?id=1347082.1347125.  Phillip Karau and Vincent Tabard-Cossa. Capture and translocation characteristics of short doi:  branched dna labels in solid-state nanopores. ACS Sensors, 3(7):1308-1315, 2018. 10.1021/acssensors.8b00165.  Andrew McGregor, Eric Price, and Sofya Vorotnikova. Trace Reconstruction Revisited. In Euro-  pean Symposium on Algorithms (ESA), pages 689-700. Springer, 2014.  Fedor Nazarov and Yuval Peres. Trace reconstruction with exp(O(n1/3)) samples. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1042- 1046, 2017. doi: 10.1145/3055399.3055494. URL http://doi.acm.org/10.1145/ 3055399.3055494. RECONSTRUCTING TREES FROM TRACES  Lee Organick, Siena Dumas Ang, Yuan-Jyue Chen, Randolph Lopez, Sergey Yekhanin, Konstantin Makarychev, Miklos Z Racz, Govinda Kamath, Parikshit Gopalan, Bichlien Nguyen, Christo- pher N Takahashi, Sharon Newman, Hsing-Yeh Parker, Cyrus Rashtchian, Kendall Stewart, Gagan Gupta, Robert Carlson, John Mulligan, Douglas Carmean, Georg Seelig, Luis Ceze, and Karin Strauss. Random access in large-scale DNA data storage. Nature Biotechnology, 36:242- 248, 2018. URL https://www.nature.com/articles/nbt.4079.  Krishnamurthy Viswanathan and Ram Swaminathan.  Improved String Reconstruction Over Insertion-Deletion Channels. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 399-408, 2008.  Kaizhong Zhang and Dennis E. Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18(6):1245-1262, 1989. doi: 10.1137/0218082. URL https://doi.org/10.1137/0218082. RECONSTRUCTING TREES FROM TRACES  "}, "Is your function low dimensional?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Is your function low dimensional?", "abstract": "We study the problem of testing if a function depends on a small number of linear directions of its input data. We call a function $f$  a \\emph{linear $k$-junta} if it is completely determined by some $k$-dimensional subspace of the input space. In this paper, we study the problem of testing whether a given $n$ variable function $f : \\mathbb{R}^n \\to \\{0,1\\}$, is a linear $k$-junta or $\\epsilon$-far from all linear $k$-juntas, where the closeness is measured with respect to the Gaussian measure on $\\mathbb{R}^n$. Linear $k$-juntas are a common generalization of two fundamental classes from Boolean function analysis (both of which have been studied in property testing) \\textbf{1.} $k$- juntas which are functions on the Boolean cube which depend on at most k of the variables and \\textbf{2.}  intersection of $k$ halfspaces, a fundamental geometric concept class.   We show that the class of linear $k$-juntas is not testable, but adding a surface area constraint makes it testable: we give a $\\mathsf{poly}(k \\cdot s/\\epsilon)$-query non-adaptive tester for linear $k$-juntas with surface area at most $s$. We show that the polynomial dependence on $s$ is necessary. Moreover, we show that if the function is a linear $k$-junta with surface area at most $s$,  we give a $(s \\cdot k)^{O(k)}$-query non-adaptive algorithm to learn the function \\emph{up to a rotation of the basis}.  In particular, this implies that we can test the class of intersections of $k$ halfspaces in $\\mathbb{R}^n$ with query complexity independent of $n$.", "pdf_url": "http://proceedings.mlr.press/v99/de19a/de19a.pdf", "keywords": ["Linear juntas", "Gaussian measure", "Ornstein-Uhlenbeck operator;"], "reference": "J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in Neural Information  Processing Systems, pages 2654-2662, 2014.  M.-F Balcan and P. Long. Active and passive learning of linear separators under log-concave distri-  butions. In Conference on Learning Theory, pages 288-316, 2013.  M.-F Balcan, E. Blais, A. Blum, and L. Yang. Active property testing.  In IEEE 53rd Annual  Symposium onFoundations of Computer Science (FOCS), 2012 , pages 21-30, 2012.  Mihir Bellare, Oded Goldreich, and Madhu Sudan. Free bits, PCPs, and nonapproximability-  towards tight results. SIAM Journal on Computing, 27(3):804-915, 1998.  A. Bhattacharyya, S. Kopparty, G. Schoenebeck, M. Sudan, and D. Zuckerman. Optimal testing In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE  of reed-muller codes. Symposium on, pages 488-497. IEEE, 2010.  A. Bhattacharyya, E. Fischer, H. Hatami, P. Hatami, and S. Lovett. Every locally characterized affine-invariant property is testable. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 429-436. ACM, 2013.  A. Bhattacharyya, E. Grigorescu, and A. Shapira. A unified framework for testing linear-invariant  properties. Random Structures & Algorithms, 46(2):232-260, 2015.  E. Blais. Testing juntas nearly optimally. In Proceedings of the forty-first annual ACM symposium  on Theory of computing, pages 151-158. ACM, 2009.  Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication  complexity. Computational Complexity, 21(2):311-358, 2012.  A. Blum. Relevant examples and relevant features: Thoughts from computational learning theory.  in AAAI Fall Symposium on \u2018Relevance\u2019, 1994.  A. Blum and R. Kannan. Learning an intersection of a constant number of halfspaces under a  uniform distribution. Journal of Computer and System Sciences, 54(2):371-380, 1997.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  M. Blum, M. Luby, and R. Rubinfeld. Self-testing/correcting with applications to numerical prob-  lems. J. Comp. Sys. Sci., 47:549-595, 1993. Earlier version in STOC\u201990.  C. Bucilu\u02d8a, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge discovery and Data mining, pages 535- 541, 2006.  D. Chakrabarty and C. Seshadhri. An o(n) monotonicity tester for boolean functions over the hy-  percube. SIAM Journal on Computing, 45(2):461-472, 2016.  13   IS YOUR FUNCTION LOW-DIMENSIONAL?  References  J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in Neural Information  Processing Systems, pages 2654-2662, 2014.  M.-F Balcan and P. Long. Active and passive learning of linear separators under log-concave distri-  butions. In Conference on Learning Theory, pages 288-316, 2013.  M.-F Balcan, E. Blais, A. Blum, and L. Yang. Active property testing.  In IEEE 53rd Annual  Symposium onFoundations of Computer Science (FOCS), 2012 , pages 21-30, 2012.  Mihir Bellare, Oded Goldreich, and Madhu Sudan. Free bits, PCPs, and nonapproximability-  towards tight results. SIAM Journal on Computing, 27(3):804-915, 1998.  A. Bhattacharyya, S. Kopparty, G. Schoenebeck, M. Sudan, and D. Zuckerman. Optimal testing In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE  of reed-muller codes. Symposium on, pages 488-497. IEEE, 2010.  A. Bhattacharyya, E. Fischer, H. Hatami, P. Hatami, and S. Lovett. Every locally characterized affine-invariant property is testable. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 429-436. ACM, 2013.  A. Bhattacharyya, E. Grigorescu, and A. Shapira. A unified framework for testing linear-invariant  properties. Random Structures & Algorithms, 46(2):232-260, 2015.  E. Blais. Testing juntas nearly optimally. In Proceedings of the forty-first annual ACM symposium  on Theory of computing, pages 151-158. ACM, 2009.  Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication  complexity. Computational Complexity, 21(2):311-358, 2012.  A. Blum. Relevant examples and relevant features: Thoughts from computational learning theory.  in AAAI Fall Symposium on \u2018Relevance\u2019, 1994.  A. Blum and R. Kannan. Learning an intersection of a constant number of halfspaces under a  uniform distribution. Journal of Computer and System Sciences, 54(2):371-380, 1997.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  M. Blum, M. Luby, and R. Rubinfeld. Self-testing/correcting with applications to numerical prob-  lems. J. Comp. Sys. Sci., 47:549-595, 1993. Earlier version in STOC\u201990.  C. Bucilu\u02d8a, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge discovery and Data mining, pages 535- 541, 2006.  D. Chakrabarty and C. Seshadhri. An o(n) monotonicity tester for boolean functions over the hy-  percube. SIAM Journal on Computing, 45(2):461-472, 2016. IS YOUR FUNCTION LOW-DIMENSIONAL?  X. Chen, A. Freilich, R. Servedio, and T. Sun. Sample-based high-dimensional convexity testing. Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 2017a.  X. Chen, Z. Liu, Rocco A. Servedio, Y. Sheng, and J. Xie. Distribution free junta testing.  In  Proceedings of the ACM STOC 2018, 2018.  Xi Chen, Rocco A. Servedio, Li-Yang Tan, Erik Waingarten, and Jinyu Xie. Settling the query complexity of non-adaptive junta testing. In Proceedings of the 32Nd Computational Complexity Conference, pages 26:1-26:19, 2017b.  I. Diakonikolas, D. Kane, and A. Stewart. Learning geometric concepts with nasty noise. In Pro- ceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061- 1073, 2018.  E. Fischer, E. Lehman, I. Newman, S. Raskhodnikova, R. Rubinfeld, and A. Samrodnitsky. Mono- tonicity testing over general poset domains. In Proc. 34th Annual ACM Symposium on the Theory of Computing, pages 474-483, 2002.  E. Fischer, G. Kindler, D. Ron, S. Safra, and A. Samorodnitsky. Testing juntas. J. Computer &  System Sciences, 68(4):753-787, 2004.  Oded Goldreich. Introduction to Property Testing. Cambridge University Press, 2017. doi: 10.  1017/9781108135252.  P. Gopalan, A. Klivans, and R. Meka. Learning functions of halfspaces using prefix covers.  In  Conference on Learning Theory, pages 15-1, 2012.  Parikshit Gopalan, Ryan O\u2019Donnell, Rocco A Servedio, Amir Shpilka, and Karl Wimmer. Testing fourier dimensionality and sparsity. In International Colloquium on Automata, Languages, and Programming, pages 500-512. Springer, 2009.  N. Harms. Testing halfspaces over rotationally invariant distributions.  In Proceedings of SODA  2019, 2019.  (JACM), 59(6):29, 2012.  P. Harsha, A. Klivans, and R. Meka. An invariance principle for polytopes. Journal of the ACM  Charanjit S. Jutla, Anindya C. Patthak, Atri Rudra, and David Zuckerman. Testing low-degree In Proc. 45th IEEE Symposium on Foundations of Computer  polynomials over prime fields. Science (FOCS), pages 423-432. IEEE Computer Society Press, 2004.  T. Kaufman and M. Sudan. Algebraic property testing: the role of invariance. In Proc. 40th Annual  ACM Symposium on Theory of Computing (STOC), pages 403-412, 2008.  S. Khot, D. Minzer, and M. Safra. On monotonicity testing and Boolean isoperimetric type the- orems. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 52-58. IEEE, 2015. IS YOUR FUNCTION LOW-DIMENSIONAL?  A. Klivans, R. O\u2019Donnell, and R. Servedio. Learning intersections and thresholds of halfspaces. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 177- 186, 2002.  A. Klivans, R. O\u2019Donnell, and R. Servedio. Learning geometric concepts via Gaussian surface area. In Proc. 49th IEEE Symposium on Foundations of Computer Science (FOCS), pages 541-550, 2008.  P. Kothari, A. Nayyeri, R. O\u2019Donnell, and C. Wu. Testing surface area.  In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1204-1214. SIAM, 2014.  K. Matulef, R. O\u2019Donnell, R. Rubinfeld, and R. Servedio. Testing halfspaces. In Proc. 20th Annual  Symposium on Discrete Algorithms (SODA), 2009.  K. Matulef, R. O\u2019Donnell, R. Rubinfeld, and R. Servedio. Testing halfspaces. SIAM J. on Comput.,  39(5):2004-2047, 2010.  E. Mossel and J. Neeman. Robust optimality of gaussian noise stability. Journal of the European  Mathematical Society, 17(2):433-482, 2015.  J. Neeman. Testing surface area with arbitrary accuracy. In Proceedings of the forty-sixth annual  ACM symposium on Theory of computing, pages 393-397. ACM, 2014.  Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014.  M. Parnas, D. Ron, and A. Samorodnitsky. Testing basic boolean formulae. SIAM J. Disc. Math.,  16:20-46, 2002. URL citeseer.ifi.unizh.ch/parnas02testing.html.  D. Ron and R. Servedio. Exponentially Improved Algorithms and Lower Bounds for Testing Signed  Majorities. Algorithmica, 72(2):400-429, 2015.  Dana Ron et al. Algorithmic and analysis techniques in property testing. Foundations and Trends R(cid:13)  in Theoretical Computer Science, 5(2):73-205, 2010.  R. Rubinfeld and M. Sudan. Robust characterizations of polynomials with applications to program  testing. SIAM J. on Comput., 25:252-271, 1996.  S. Vempala. Learning convex concepts from gaussian distributions with PCA. In 2010 IEEE 51st  Annual Symposium on Foundations of Computer Science, pages 124-130, 2010a.  S. Vempala. A random-sampling-based algorithm for learning intersections of halfspaces. Journal  of the ACM (JACM), 57(6):32, 2010b.  S. Vempala and Y. Xiao. Complexity of learning subspace juntas and ICA.  In 2013 Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, USA, November 3-6, 2013, pages 320-324, 2013. "}, "Computational Limitations in Robust Classification and Win-Win Results": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Computational Limitations in Robust Classification and Win-Win Results", "abstract": "We continue the study of statistical/computational tradeoffs in learning robust classifiers, following the recent work of Bubeck, Lee, Price and Razenshteyn who showed examples of classification tasks where (a) an efficient robust classifier exists, in the small-perturbation regime; (b) a non-robust classifier can be learned efficiently; but (c) it is computationally hard to learn a robust classifier, assuming the hardness of factoring large numbers. Indeed, the question of whether a robust classifier for their task exists in the large perturbation regime seems related to important open questions in computational number theory. In this work, we extend their work in three directions. First, we demonstrate classification tasks where computationally efficient robust classification is impossible, even when computationally unbounded robust classifiers exist. For this, we rely on the existence of average-case hard functions, requiring no cryptographic assumptions. Second, we show hard-to-robustly-learn classification tasks in the large-perturbation regime. Namely, we show that even though an efficient classifier that is very robust (namely, tolerant to large perturbations) exists, it is computationally hard to learn any non-trivial robust classifier. Our first construction relies on the existence of one-way functions, a minimal assumption in cryptography, and the second on the hardness of the learning parity with noise problem. In the latter setting, not only does a non-robust classifier exist, but also an efficient algorithm that generates fresh new labeled samples given access to polynomially many training examples (termed as generation by Kearns et al. (1994)). Third, we show that any such counterexample implies the existence of cryptographic primitives such as one-way functions or even forms of public-key encryption. This leads us to a win-win scenario: either we can quickly learn an efficient robust classifier, or we can construct new instances of popular and useful cryptographic primitives.", "pdf_url": "http://proceedings.mlr.press/v99/degwekar19a/degwekar19a.pdf", "keywords": [], "reference": "Michael Alekhnovich. More on Average Case vs Approximation Complexity. In Foundations of  Computer Science, pages 298-307. IEEE, 2003.  Sanjeev Arora and Rong Ge. New algorithms for learning in presence of errors. In International  Colloquium on Automata, Languages, and Programming, pages 403-415. Springer, 2011.  Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.  Itay Berman, Akshay Degwekar, Ron D. Rothblum, and Prashant Nalini Vasudevan. Statistical Difference Beyond the Polarizing Regime. Electronic Colloquium on Computational Complexity (ECCC), 26:38, 2019. URL https://eccc.weizmann.ac.il/report/2019/038.  Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine  learning. Pattern Recognition, 84:317-331, 2018.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant Learning, the Parity Problem, and  the Statistical Query Model. J. ACM, 2003.  Lenore Blum, Manuel Blum, and Mike Shub. A Simple Unpredictable Pseudo-Random Number  Generator. SIAM Journal on computing, 1986.  Jehoshua Bruck and Moni Naor. The Hardness of Decoding Linear Codes with Preprocessing. IEEE Trans. Information Theory, 36(2):381-385, 1990. doi: 10.1109/18.52484. URL https: //doi.org/10.1109/18.52484.  S\u00b4ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya P. Razenshteyn. Adversarial examples from cryptographic pseudo-random generators. CoRR, abs/1811.06418, 2018a. URL http: //arxiv.org/abs/1811.06418.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational con- straints. CoRR, abs/1805.10204, 2018b. URL https://arxiv.org/abs/1805.10204.  Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017  IEEE Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017.  Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, Deepak Verma, et al. Adversarial classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 99-108. ACM, 2004.  Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier.  CoRR, abs/1802.08686, 2018. URL http://arxiv.org/abs/1802.08686.  Yoav Freund, Robert E Schapire, et al. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999.  13   COMPUTATIONAL LIMITATIONS IN ROBUST CLASSIFICATION  References  Michael Alekhnovich. More on Average Case vs Approximation Complexity. In Foundations of  Computer Science, pages 298-307. IEEE, 2003.  Sanjeev Arora and Rong Ge. New algorithms for learning in presence of errors. In International  Colloquium on Automata, Languages, and Programming, pages 403-415. Springer, 2011.  Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.  Itay Berman, Akshay Degwekar, Ron D. Rothblum, and Prashant Nalini Vasudevan. Statistical Difference Beyond the Polarizing Regime. Electronic Colloquium on Computational Complexity (ECCC), 26:38, 2019. URL https://eccc.weizmann.ac.il/report/2019/038.  Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine  learning. Pattern Recognition, 84:317-331, 2018.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant Learning, the Parity Problem, and  the Statistical Query Model. J. ACM, 2003.  Lenore Blum, Manuel Blum, and Mike Shub. A Simple Unpredictable Pseudo-Random Number  Generator. SIAM Journal on computing, 1986.  Jehoshua Bruck and Moni Naor. The Hardness of Decoding Linear Codes with Preprocessing. IEEE Trans. Information Theory, 36(2):381-385, 1990. doi: 10.1109/18.52484. URL https: //doi.org/10.1109/18.52484.  S\u00b4ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya P. Razenshteyn. Adversarial examples from cryptographic pseudo-random generators. CoRR, abs/1811.06418, 2018a. URL http: //arxiv.org/abs/1811.06418.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational con- straints. CoRR, abs/1805.10204, 2018b. URL https://arxiv.org/abs/1805.10204.  Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017  IEEE Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017.  Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, Deepak Verma, et al. Adversarial classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 99-108. ACM, 2004.  Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier.  CoRR, abs/1802.08686, 2018. URL http://arxiv.org/abs/1802.08686.  Yoav Freund, Robert E Schapire, et al. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999. COMPUTATIONAL LIMITATIONS IN ROBUST CLASSIFICATION  Craig Gentry, Chris Peikert, and Vinod Vaikuntanathan. Trapdoors for Hard Lattices and New In Symposium on Theory of computing, pages 197-206. ACM,  Cryptographic Constructions. 2008.  Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Watten-  berg, and Ian Goodfellow. Adversarial spheres, 2018.  Oded Goldreich. Foundations of Cryptography: Basic Tools, 2001.  Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to Construct Random Functions. J. ACM, 1986. doi: 10.1145/6490.6503. URL http://doi.acm.org/10.1145/6490. 6503.  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial  examples. corr (2015).  Matthew Green.  A few more notes on NSA random number generators,  2013. https://blog.cryptographyengineering.com/2013/12/28/  URL a-few-more-notes-on-nsa-random-number/.  Venkatesan Guruswami and Piotr Indyk. Expander-based Constructions of Efficiently Decodable  Codes. In IEEE Foundations of Computer Science. IEEE, 2001.  Nadia Heninger. Personal communication, 2019.  Russell Impagliazzo. Hard-core distributions for somewhat hard problems. In Foundations of Com- puter Science, 1995. Proceedings., 36th Annual Symposium on, pages 538-545. IEEE, 1995.  Abhishek Jain, Stephan Krenn, Krzysztof Pietrzak, and Aris Tentes. Commitments and efficient  zero-knowledge proofs from learning parity with noise. In ASIACRYPT, 2012.  Michael Kearns and Leslie Valiant. Cryptographic Limitations on Learning Boolean Formulae and  Finite Automata. J. ACM, 1994.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. On the Learnability of Discrete Distributions. In ACM symposium on Theory of computing, pages 273-282, 1994.  Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641-647. ACM, 2005.  Vadim Lyubashevsky. The parity problem in the presence of noise, decoding random linear codes,  and the subset sum problem. In APPROX\u201905/RANDOM\u201905, 2005.  Saeed Mahloujifar and Mohammad Mahmoody. Can adversarially robust learning leverage compu- tational hardness? CoRR, abs/1810.01407, 2018. URL http://arxiv.org/abs/1810. 01407.  Silvio Micali and Claus-Peter Schnorr. Efficient, Perfect Polynomial Random Number Generators.  Journal of Cryptology, 3(3):157-172, 1991. COMPUTATIONAL LIMITATIONS IN ROBUST CLASSIFICATION  Daniele Micciancio. The hardness of the closest vector problem with preprocessing. IEEE Trans-  actions on Information Theory, 47(3):1212-1215, 2001.  Preetum Nakkiran. Adversarial robustness may be at odds with simplicity.  arXiv preprint  arXiv:1901.00532, 2019.  Moni Naor and Guy N Rothblum. Learning to impersonate. In Proceedings of the 23rd international  conference on Machine learning, pages 649-656. ACM, 2006.  Chris Peikert. A Decade of Lattice Cryptography. Foundations and Trends R(cid:13) in Theoretical Com-  puter Science, 10(4):283-424, 2016.  Oded Regev.  Improved Inapproximability of Lattice and Coding Problems With Preprocessing. IEEE Trans. Information Theory, 50(9):2031-2037, 2004. doi: 10.1109/TIT.2004.833350. URL https://doi.org/10.1109/TIT.2004.833350.  Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad- versarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.  Salil Vadhan and Colin Jia Zheng. A uniform min-max theorem with applications in cryptography.  In Advances in Cryptology-CRYPTO 2013, pages 93-110. Springer, 2013.  "}, "Fast determinantal point processes via distortion-free intermediate sampling": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Fast determinantal point processes via distortion-free intermediate sampling", "abstract": "Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the complexity of sampling from a  distribution over all subsets of rows where the probability of a subset is proportional to the squared volume of the parallelepiped spanned by the rows (a.k.a.\u00a0a determinantal point process). In this task, it is important to minimize the preprocessing cost of the procedure (performed once) as well as the  sampling cost (performed repeatedly). To that end, we propose a new determinantal point process algorithm which has the following two properties, both of which are novel: (1) a preprocessing step which runs in time $O\\big(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log n\\big)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$ time, independent of the number of rows $n$. We achieve this by introducing a new \\textit{regularized} determinantal point process (R-DPP), which serves as an intermediate distribution in the sampling procedure by reducing the number of rows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution does not distort the probabilities of the target sample. Our key novelty in defining the R-DPP  is the use of a Poisson random variable for controlling the probabilities of different subset sizes, leading to new determinantal formulas such as the normalization constant for this distribution. Our algorithm has applications in many diverse areas where determinantal point processes have been used, such as  machine learning, stochastic optimization, data summarization and low-rank matrix reconstruction.", "pdf_url": "http://proceedings.mlr.press/v99/derezinski19a/derezinski19a.pdf", "keywords": ["determinantal point processes", "subset selection", "low-rank approximation"], "reference": "Raja Hafiz Affandi, Alex Kulesza, Emily Fox, and Ben Taskar. Nystrom approximation for large- scale determinantal processes. In Carlos M. Carvalho and Pradeep Ravikumar, editors, Proceed- ings of the Sixteenth International Conference on Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 85-98, Scottsdale, Arizona, USA, 29 Apr- 01 May 2013. PMLR. URL http://proceedings.mlr.press/v31/affandi13a. html.  Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel ridge regression with sta- tistical guarantees. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 775-783, Montreal, Canada, December 2015.  Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. Monte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 103-115, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL http://proceedings.mlr. press/v49/anari16.html.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  R\u00b4emi Bardenet and Adrien Hardy. Monte Carlo with Determinantal Point Processes. work- ing paper or preprint, May 2016. URL https://hal.archives-ouvertes.fr/ hal-01311263.  R\u00b4emi Bardenet, Fr\u00b4ed\u00b4eric Lavancier, Xavier Mary, and Aur\u00b4elien Vasseur. On a few statisti- cal applications of determinantal point processes. ESAIM: Procs, 60:180-202, 2017. doi: 10.1051/proc/201760180. URL https://doi.org/10.1051/proc/201760180.  Christos Boutsidis and David P. Woodruff. Optimal cur matrix decompositions. SIAM Journal on Computing, 46(2):543-589, 2017. doi: 10.1137/140977898. URL https://doi.org/10. 1137/140977898.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based matrix In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,  reconstruction. pages 305-314, Oct 2011. doi: 10.1109/FOCS.2011.21.  Cl\u00b4ement Canonne. A short note on poisson tail bounds. Technical report, Columbia University,  2017.  Kenneth L. Clarkson and David P. Woodruff. Low-rank approximation and regression in input sparsity time. J. ACM, 63(6):54:1-54:45, January 2017. ISSN 0004-5411. doi: 10.1145/3019134. URL http://doi.acm.org/10.1145/3019134.  13   FAST DETERMINANTAL POINT PROCESSES  We thank the NSF for funding via the NSF TRIPODS program. Part of this work was done while MD was visiting the Simons Institute for the Theory of Computing.  Acknowledgments  References  Raja Hafiz Affandi, Alex Kulesza, Emily Fox, and Ben Taskar. Nystrom approximation for large- scale determinantal processes. In Carlos M. Carvalho and Pradeep Ravikumar, editors, Proceed- ings of the Sixteenth International Conference on Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 85-98, Scottsdale, Arizona, USA, 29 Apr- 01 May 2013. PMLR. URL http://proceedings.mlr.press/v31/affandi13a. html.  Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel ridge regression with sta- tistical guarantees. In Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 775-783, Montreal, Canada, December 2015.  Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. Monte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 103-115, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL http://proceedings.mlr. press/v49/anari16.html.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  R\u00b4emi Bardenet and Adrien Hardy. Monte Carlo with Determinantal Point Processes. work- ing paper or preprint, May 2016. URL https://hal.archives-ouvertes.fr/ hal-01311263.  R\u00b4emi Bardenet, Fr\u00b4ed\u00b4eric Lavancier, Xavier Mary, and Aur\u00b4elien Vasseur. On a few statisti- cal applications of determinantal point processes. ESAIM: Procs, 60:180-202, 2017. doi: 10.1051/proc/201760180. URL https://doi.org/10.1051/proc/201760180.  Christos Boutsidis and David P. Woodruff. Optimal cur matrix decompositions. SIAM Journal on Computing, 46(2):543-589, 2017. doi: 10.1137/140977898. URL https://doi.org/10. 1137/140977898.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based matrix In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,  reconstruction. pages 305-314, Oct 2011. doi: 10.1109/FOCS.2011.21.  Cl\u00b4ement Canonne. A short note on poisson tail bounds. Technical report, Columbia University,  2017.  Kenneth L. Clarkson and David P. Woodruff. Low-rank approximation and regression in input sparsity time. J. ACM, 63(6):54:1-54:45, January 2017. ISSN 0004-5411. doi: 10.1145/3019134. URL http://doi.acm.org/10.1145/3019134. FAST DETERMINANTAL POINT PROCESSES  Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen- sionality reduction for k-means clustering and low rank approximation. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing, STOC \u201915, pages 163-172, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3536-2. doi: 10.1145/2746539.2746569. URL http://doi.acm.org/10.1145/2746539.2746569.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Reverse iterative volume sampling for linear regres- sion. Journal of Machine Learning Research, 19(23):1-39, 2018. URL http://jmlr.org/ papers/v19/17-781.html.  Micha\u0142 Derezi\u00b4nski, Manfred K. Warmuth, and Daniel Hsu. Leveraged volume sampling for In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, linear regression. and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2510-2519. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7517-leveraged-volume-sampling-for-linear-regression.pdf.  Micha\u0142 Derezi\u00b4nski, Manfred K. Warmuth, and Daniel Hsu. Correcting the bias in least squares regression with volume-rescaled sampling. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 329-338, Las Vegas, USA, October 2010.  Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projective clustering via volume sampling. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm, pages 1117-1126, Miami, FL, USA, January 2006.  Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approx- imation of matrix coherence and statistical leverage. J. Mach. Learn. Res., 13(1):3475-3506, December 2012.  Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Discovering diverse and salient threads in In Proceedings of the 2012 Joint Conference on Empirical Methods in document collections. Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL \u201912, pages 710-720, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics. URL http://dl.acm.org/citation.cfm?id=2390948.2391026.  Boqing Gong, Wei-Lun Chao, Kristen Grauman, and Fei Sha. Diverse sequential subset selection for supervised video summarization. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2069-2077. Curran Associates, Inc., 2014.  A Guenoche. Random spanning tree. doi:  0196-6774. sciencedirect.com/science/article/pii/0196677483900226.  https://doi.org/10.1016/0196-6774(83)90022-6.  Journal of Algorithms, 4(3):214 - 220, 1983.  ISSN URL http://www.  Venkatesan Guruswami and Ali K. Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1207-1214, Kyoto, Japan, January 2012. FAST DETERMINANTAL POINT PROCESSES  J Ben Hough, Manjunath Krishnapur, Yuval Peres, B\u00b4alint Vir\u00b4ag, et al. Determinantal processes and  independence. Probability surveys, 3:206-229, 2006.  Alex Kulesza and Ben Taskar. Structured determinantal point processes. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1171-1179. Curran Associates, Inc., 2010.  Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In Proceedings of the 28th International Conference on Machine Learning, pages 1193-1200, Bellevue, WA, USA, June 2011.  Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publish-  ers Inc., Hanover, MA, USA, 2012.  Chengtao Li, Stefanie Jegelka, and Suvrit Sra. Efficient sampling for k-determinantal point pro- In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th Interna- cesses. tional Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Ma- chine Learning Research, pages 1328-1337, Cadiz, Spain, 09-11 May 2016. PMLR. URL http://proceedings.mlr.press/v51/li16f.html.  Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In Pro- ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies - Volume 1, HLT \u201911, pages 510-520, Stroudsburg, PA, USA, ISBN 978-1-932432-87-9. URL http: 2011. Association for Computational Linguistics. //dl.acm.org/citation.cfm?id=2002472.2002537.  Odile Macchi. The coincidence approach to stochastic point processes. Advances in Applied Prob- ISSN 00018678. URL http://www.jstor.org/stable/  ability, 7(1):83-122, 1975. 1425855.  Avner Magen and Anastasios Zouzias. Near optimal dimensionality reductions that preserve vol- umes. In Ashish Goel, Klaus Jansen, Jos\u00b4e D. P. Rolim, and Ronitt Rubinfeld, editors, Approxima- tion, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 523- 534, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. ISBN 978-3-540-85363-3.  Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC \u201913, pages 91-100, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-2029-0. doi: 10.1145/2488608.2488621. URL http://doi.acm. org/10.1145/2488608.2488621.  Jelani Nelson and Huy L. Nguy\u02c6en. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS \u201913, pages 117-126, Washington, DC, USA, 2013. IEEE Computer ISBN 978-0-7695-5135-7. doi: 10.1109/FOCS.2013.21. URL http://dx.doi. Society. org/10.1109/FOCS.2013.21.  Robin Pemantle and Yuval Peres. Concentration of lipschitz functionals of determinantal and other strong rayleigh measures. Combinatorics, Probability and Computing, 23(1):140-160, 2014. doi: 10.1017/S0963548313000345. FAST DETERMINANTAL POINT PROCESSES  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, August 2012.  H. Robert van der Vaart. A note on wilks\u2019 internal scatter. Ann. Math. Statist., 36(4):1308-1312, 08 1965. doi: 10.1214/aoms/1177700006. URL https://doi.org/10.1214/aoms/ 1177700006.  Cheng Zhang, Hedvig Kjellstr\u00a8om, and Stephan Mandt. Determinantal point processes for mini- In 33rd Conference on Uncertainty in Artificial Intelligence, UAI 2017,  batch diversification. Sydney, Australia, 11 August 2017 through 15 August 2017. AUAI Press Corvallis, 2017. FAST DETERMINANTAL POINT PROCESSES  "}, "Minimax experimental design: Bridging the gap between statistical and worst-case approaches to least squares regression": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Minimax experimental design: Bridging the gap between statistical and worst-case approaches to least squares regression", "abstract": "In experimental design, we are given a large collection of vectors, each with a hidden response value that we assume derives from an underlying linear model, and we wish to pick a small subset of the vectors such that querying the corresponding responses will lead to a good estimator of the model.   A classical approach in statistics is to assume the responses are linear, plus zero-mean i.i.d. Gaussian noise, in which case the goal is to provide an unbiased estimator with smallest mean squared error (A-optimal design).   A related approach, more common in computer science, is to assume the responses are arbitrary but fixed, in which case the goal is to estimate the least squares solution using few responses, as quickly as possible, for worst-case inputs.    Despite many attempts, characterizing the relationship between these two approaches has proven elusive.   We address this by proposing a framework for experimental  design where the responses are produced by an arbitrary unknown distribution.   We show that there is an efficient randomized experimental design procedure that achieves strong variance bounds for an unbiased estimator using few responses  in this general model. Nearly tight bounds for the classical A-optimality criterion, as well as improved bounds for worst-case responses, emerge as special cases of this result.   In the process, we develop a new algorithm for a joint sampling distribution called volume sampling, and we propose a new i.i.d. importance sampling method: inverse score sampling.   A key novelty of our analysis is in developing new expected error bounds for worst-case regression by controlling the tail behavior of i.i.d. sampling via the jointness of volume sampling.   Our result motivates a new minimax-optimality criterion for experimental design with unbiased estimators, which can be viewed as an extension of both A-optimal design and sampling for worst-case regression.", "pdf_url": "http://proceedings.mlr.press/v99/derezinski19b/derezinski19b.pdf", "keywords": ["A-optimality", "worst-case", "volume sampling", "minimax", "linear regression", "least squares"], "reference": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments via regret minimization. In Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learning Research, pages 126-135, Sydney, Australia, August 2017. URL http://proceedings.mlr.press/v70/allen-zhu17e.html.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE Trans. Information Theory, 59(10):6880-6892, 2013.  Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statist. Sci., 10 (3):273-304, 08 1995. doi: 10.1214/ss/1177009939. URL https://doi.org/10.1214/ ss/1177009939.  Xue Chen and Eric Price. Active regression via linear-sample sparsification. In Proceedings of the  32nd Conference on Learning Theory, 2019.  Micha\u0142 Derezi\u00b4nski. Fast determinantal point processes via distortion-free intermediate sampling. In  Proceedings of the 32nd Conference on Learning Theory, 2019.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Unbiased estimates for linear regression via volume sampling. In Advances in Neural Information Processing Systems 30, pages 3087-3096, Long Beach, CA, USA, December 2017.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Subsampling for ridge regression via regularized volume sampling. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty- First International Conference on Artificial Intelligence and Statistics, pages 716-725, Playa Blanca, Lanzarote, Canary Islands, April 2018.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Reverse iterative volume sampling for linear regres- sion. Journal of Machine Learning Research, 19(23):1-39, 2018. URL http://jmlr.org/ papers/v19/17-781.html.  Micha\u0142 Derezi\u00b4nski, Manfred K. Warmuth, and Daniel Hsu. Leveraged volume sampling for linear regression. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2510-2519. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7517-leveraged-volume-sampling-for-linear-regression.pdf.  13   MINIMAX EXPERIMENTAL DESIGN  MWM would like to acknowledge ARO, DARPA, NSF and ONR for providing partial support of this work. Also, MWM and MD thank the NSF for funding via the NSF TRIPODS program. Part of this work was done while MD, KLC and MWM were visiting the Simons Institute for the Theory of Computing and while MKW was at UC Santa Cruz, supported by NSF grant IIS-1619271.  Acknowledgments  References  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments via regret minimization. In Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learning Research, pages 126-135, Sydney, Australia, August 2017. URL http://proceedings.mlr.press/v70/allen-zhu17e.html.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-  squares regression. IEEE Trans. Information Theory, 59(10):6880-6892, 2013.  Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statist. Sci., 10 (3):273-304, 08 1995. doi: 10.1214/ss/1177009939. URL https://doi.org/10.1214/ ss/1177009939.  Xue Chen and Eric Price. Active regression via linear-sample sparsification. In Proceedings of the  32nd Conference on Learning Theory, 2019.  Micha\u0142 Derezi\u00b4nski. Fast determinantal point processes via distortion-free intermediate sampling. In  Proceedings of the 32nd Conference on Learning Theory, 2019.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Unbiased estimates for linear regression via volume sampling. In Advances in Neural Information Processing Systems 30, pages 3087-3096, Long Beach, CA, USA, December 2017.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Subsampling for ridge regression via regularized volume sampling. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty- First International Conference on Artificial Intelligence and Statistics, pages 716-725, Playa Blanca, Lanzarote, Canary Islands, April 2018.  Micha\u0142 Derezi\u00b4nski and Manfred K. Warmuth. Reverse iterative volume sampling for linear regres- sion. Journal of Machine Learning Research, 19(23):1-39, 2018. URL http://jmlr.org/ papers/v19/17-781.html.  Micha\u0142 Derezi\u00b4nski, Manfred K. Warmuth, and Daniel Hsu. Leveraged volume sampling for linear regression. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2510-2519. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7517-leveraged-volume-sampling-for-linear-regression.pdf. MINIMAX EXPERIMENTAL DESIGN  Micha\u0142 Derezi\u00b4nski, Manfred K. Warmuth, and Daniel Hsu. Correcting the bias in least squares regression with volume-rescaled sampling. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, 2019.  Petros Drineas and Michael W. Mahoney. RandNLA: Randomized numerical linear algebra. Com-  munications of the ACM, 59:80-90, 2016.  Petros Drineas and Michael W. Mahoney. Lectures on randomized numerical linear algebra. Techni- cal report, 2017. Preprint: arXiv:1712.08880; To appear in: Lectures of the 2016 PCMI Summer School on Mathematics of Data.  Petros Drineas, Michael W Mahoney, and S. Muthukrishnan. Sampling algorithms for (cid:96)2 regression and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, pages 1127-1136. Society for Industrial and Applied Mathematics, 2006.  Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approx- imation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13: 3475-3506, 2012.  Valerii V. Fedorov. Theory of optimal experiments. Probability and mathematical statistics. Aca-  demic Press, New York, NY, USA, 1972.  Robert M. Gray and Lee D. Davisson. An Introduction to Statistical Signal Processing. Cambridge University Press, New York, NY, USA, 1st edition, 2010. ISBN 0521131820, 9780521131827.  J. Ben Hough, Manjunath Krishnapur, Yuval Peres, B\u00b4alint Vir\u00b4ag, et al. Determinantal processes and  independence. Probability surveys, 3:206-229, 2006.  Chengtao Li, Stefanie Jegelka, and Suvrit Sra. Efficient sampling for k-determinantal point pro- In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th Interna- cesses. tional Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Ma- chine Learning Research, pages 1328-1337, Cadiz, Spain, 09-11 May 2016. PMLR. URL http://proceedings.mlr.press/v51/li16f.html.  Ping Ma, Michael Mahoney, and Bin Yu. A statistical perspective on algorithmic leveraging. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Ma- chine Learning, volume 32 of Proceedings of Machine Learning Research, pages 91-99, Bejing, China, 22-24 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/ma14. html.  Aleksandar Nikolov, Mohit Singh, and Uthaipon Tao Tantipongpipat. Proportional volume sampling and approximation algorithms for a -optimal design. In Proceedings of the Thirtieth Annual ACM- SIAM Symposium on Discrete Algorithms, pages 1369-1386, January 2019.  Beiyan Ou and Julie Zhou. Minimax robust designs for field experiments. Metrika, 69(1):45-54,  Jan 2009.  Friedrich Pukelsheim. Optimal Design of Experiments (Classics in Applied Mathematics) (Classics in Applied Mathematics, 50). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2006. ISBN 0898716047. MINIMAX EXPERIMENTAL DESIGN  Garvesh Raskutti and Michael Mahoney. Statistical and algorithmic perspectives on randomized sketching for ordinary least-squares. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 617-625, Lille, France, 07-09 Jul 2015. PMLR. URL http:// proceedings.mlr.press/v37/raskutti15.html.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, August 2012.  Yining Wang, Adams W. Yu, and Aarti Singh. On computationally tractable selection of ex- periments in measurement-constrained regression models. J. Mach. Learn. Res., 18(1):5238- 5278, January 2017. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 3122009.3208024.  Douglas P. Wiens and Pengfei Li. V-optimal designs for heteroscedastic regression. Journal of Statistical Planning and Inference, 145:125 - 138, 2014. ISSN 0378-3758. doi: https://doi. org/10.1016/j.jspi.2013.09.007. URL http://www.sciencedirect.com/science/ article/pii/S0378375813002310.  David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends\u00ae in  Theoretical Computer Science, 10(1-2):1-157, 2014. MINIMAX EXPERIMENTAL DESIGN  "}, "Communication and Memory Efficient Testing of Discrete Distributions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Communication and Memory Efficient Testing of Discrete Distributions", "abstract": "We study distribution testing with communication and memory constraints  in the following computational models: (1) The {\\em one-pass streaming model}  where the goal is to minimize the sample complexity of the protocol subject to a memory constraint, and (2) A {\\em distributed model} where the data samples reside at multiple machines and the goal is to minimize the communication cost of the protocol. In both these models, we provide efficient algorithms for uniformity/identity testing (goodness of fit) and closeness testing (two sample testing). Moreover, we show nearly-tight lower bounds on (1) the sample complexity of any one-pass streaming tester for uniformity, subject to the memory constraint,  and (2) the communication cost of any uniformity testing protocol,  in a restricted \u201cone-pass\u201d model of communication.", "pdf_url": "http://proceedings.mlr.press/v99/diakonikolas19a/diakonikolas19a.pdf", "keywords": ["distribution testing", "identity testing", "closeness testing", "communication complexity", "streaming"], "reference": "J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions.  In  Advances in Neural Information Processing Systems (NIPS), pages 3591-3599, 2015.  J. Acharya, C. L. Canonne, and H. Tyagi. Distributed simulation and distributed inference. CoRR,  abs/1804.06952, 2018a. URL http://arxiv.org/abs/1804.06952.  J. Acharya, C. L. Canonne, and H. Tyagi. Inference under information constraints I: lower bounds from chi-square contraction. CoRR, abs/1812.11476, 2018b. URL http://arxiv.org/ abs/1812.11476.  R. Ahlswede and I. Csiszar. Hypothesis testing with communication constraints. IEEE Trans. Inf.  Theor., 32(4):533-542, September 1986.  A. Andoni, T. Malkin, and N. S. Nosatzki. Two party distribution testing: Communication and security. CoRR, abs/1811.04065, 2018. URL http://arxiv.org/abs/1811.04065.  T. Batu and C. L. Canonne. Generalized uniformity testing. In 58th IEEE Annual Symposium on  Foundations of Computer Science, FOCS 2017, pages 880-889, 2017.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000. URL citeseer. ist.psu.edu/batu00testing.html.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  M. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing, STOC 2016, pages 1011-1020, 2016.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016, pages 25:1-25:14, 2016.  C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks.  In  Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 370-448, 2017a.  13   COMMUNICATION AND MEMORY EFFICIENT TESTING OF DISCRETE DISTRIBUTIONS  I.D. is supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. This research was performed while T.G. was a postdoctoral researcher at USC, supported by I.D.\u2019s startup grant. D.K. is supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellow- ship. S.R. would like to thank Dheeraj P.V. for helpful discussions.  Acknowledgments  References  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions.  In  Advances in Neural Information Processing Systems (NIPS), pages 3591-3599, 2015.  J. Acharya, C. L. Canonne, and H. Tyagi. Distributed simulation and distributed inference. CoRR,  abs/1804.06952, 2018a. URL http://arxiv.org/abs/1804.06952.  J. Acharya, C. L. Canonne, and H. Tyagi. Inference under information constraints I: lower bounds from chi-square contraction. CoRR, abs/1812.11476, 2018b. URL http://arxiv.org/ abs/1812.11476.  R. Ahlswede and I. Csiszar. Hypothesis testing with communication constraints. IEEE Trans. Inf.  Theor., 32(4):533-542, September 1986.  A. Andoni, T. Malkin, and N. S. Nosatzki. Two party distribution testing: Communication and security. CoRR, abs/1811.04065, 2018. URL http://arxiv.org/abs/1811.04065.  T. Batu and C. L. Canonne. Generalized uniformity testing. In 58th IEEE Annual Symposium on  Foundations of Computer Science, FOCS 2017, pages 880-889, 2017.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000. URL citeseer. ist.psu.edu/batu00testing.html.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  M. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing, STOC 2016, pages 1011-1020, 2016.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016, pages 25:1-25:14, 2016.  C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks.  In  Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 370-448, 2017a. COMMUNICATION AND MEMORY EFFICIENT TESTING OF DISCRETE DISTRIBUTIONS  C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing conditional independence of discrete distributions. CoRR, abs/1711.11560, 2017b. URL http://arxiv.org/abs/ 1711.11560. In STOC\u201918.  C. L. Canonne, I. Diakonikolas, and A. Stewart. Testing for families of distributions via the fourier transform. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pages 10084-10095, 2018.  S. Chan, I. Diakonikolas, P. Valiant, and G. Valiant. Optimal algorithms for testing closeness of  discrete distributions. In SODA, pages 1193-1203, 2014.  S. Chien, K. Ligett, and A. McGregor. Space-efficient estimation of robust statistics and distribution  testing. In Innovations in Computer Science - ICS 2010, pages 251-265, 2010.  T. M. Cover. Hypothesis testing with finite statistics. Ann. Math. Statist., 40(3):828-835, 06 1969.  C. Daskalakis and Q. Pan. Square Hellinger subadditivity for Bayesian networks and its applications to identity testing. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 697-703, 2017.  C. Daskalakis, I. Diakonikolas, R. Servedio, G. Valiant, and P. Valiant. Testing k-modal distribu-  tions: Optimal algorithms via reductions. In SODA, pages 1833-1852, 2013.  C. Daskalakis, N. Dikkala, and G. Kamath. Testing Ising models. In SODA, 2018.  L. Devroye and L. Gy\u00a8orfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in Statis-  1985.  tics, Springer, 2001.  I. Diakonikolas and D. M. Kane. A new approach for testing properties of discrete distributions. In  FOCS, pages 685-694, 2016. Full version available at abs/1601.05557.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Testing identity of structured distributions.  In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 1841-1854, 2015a.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Optimal algorithms and lower bounds for test- In IEEE 56th Annual Symposium on Foundations of  ing closeness of structured distributions. Computer Science, FOCS 2015, pages 1183-1202, 2015b.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Collision-based testers are optimal for uniformity and closeness. Electronic Colloquium on Computational Complexity (ECCC), 23: 178, 2016.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Sample-optimal identity testing with high  probability. CoRR, abs/1708.02728, 2017a. COMMUNICATION AND MEMORY EFFICIENT TESTING OF DISCRETE DISTRIBUTIONS  I. Diakonikolas, E. Grigorescu, J. Li, A. Natarajan, K. Onak, and L. Schmidt. Communication- efficient distributed learning of discrete distributions. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pages 6394-6404, 2017b.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Near-optimal closeness testing of discrete histogram In 44th International Colloquium on Automata, Languages, and Programming,  distributions. ICALP 2017, pages 8:1-8:15, 2017c.  I. Diakonikolas, D. M. Kane, and A. Stewart. Sharp bounds for generalized uniformity testing. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Informa- tion Processing Systems 2018, NeurIPS 2018, pages 6204-6213, 2018.  O. Fischer, U. Meir, and R. Oshman. Distributed uniformity testing. In Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing, PODC \u201918, pages 455-464, New York, NY, USA, 2018. ACM.  A. Garg, T. Ma, and H. Nguyen. On communication cost of distributed statistical estimation and In Advances in Neural Information Processing Systems (NIPS), pages 2726-  dimensionality. 2734, 2014.  bution. ECCC, 23, 2016.  O. Goldreich. The uniform distribution is complete with respect to testing identity to a fixed distri-  O. Goldreich.  Introduction to Property Testing. Forthcoming, 2017. URL http://www.  wisdom.weizmann.ac.il/\u02dcoded/pt-intro.html.  O. Goldreich and D. Ron. On testing expansion in bounded-degree graphs. Technical Report TR00-  020, Electronic Colloquium on Computational Complexity, 2000.  Y. Han, P. Mukherjee, A. \u00a8Ozg\u00a8ur, and T. Weissman. Distributed statistical estimation of high- In 2018 IEEE International Symposium on In-  dimensional and nonparametric distributions. formation Theory, ISIT 2018, pages 506-510, 2018a.  Y. Han, A. \u00a8Ozg\u00a8ur, and T. Weissman. Geometric lower bounds for distributed parameter estimation under communication constraints. In Conference On Learning Theory, COLT 2018, pages 3163- 3188, 2018b. URL http://proceedings.mlr.press/v75/han18a.html.  M. I. Jordan, J. D. Lee, and Y. Yang. Communication-efficient distributed statistical learning. CoRR,  abs/1605.07689, 2016.  R. Kannan, S. Vempala, and D. Woodruff. Principal component analysis and higher correlations for  distributed data. In Conference on Learning Theory, pages 1040-1057, 2014.  E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Texts in Statistics.  Springer, 2005.  Y. Liang, M. F. Balcan, V. Kanchanapally, and D. Woodruff. Improved distributed principal compo- nent analysis. In Advances in Neural Information Processing Systems (NIPS), pages 3113-3121, 2014. COMMUNICATION AND MEMORY EFFICIENT TESTING OF DISCRETE DISTRIBUTIONS  J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypothe- ses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933. doi: 10.1098/rsta. 1933.0009. URL http://rsta.royalsocietypublishing.org/content/231/ 694-706/289.short.  L. Paninski. A coincidence-based test for uniformity given very sparsely-sampled discrete data.  IEEE Transactions on Information Theory, 54:4750-4755, 2008.  R. Rubinfeld. Taming big probability distributions. XRDS, 19(1):24-28, 2012.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incor-  porated, 2008.  FOCS, 2014.  G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity testing. In  P. Valiant. Testing symmetric properties of distributions. SIAM J. Comput., 40(6):1927-1968, 2011.  Y. Zhang, J. Duchi, M. Jordan, and M. J. Wainwright. Information-theoretic lower bounds for dis- tributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems (NIPS), pages 2328-2336, 2013.  Y. Zhu and J. Lafferty. Distributed nonparametric regression under communication constraints. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 6004-6012, 2018.  APPENDIX  The structure of this "}, "Testing Identity of Multidimensional Histograms": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Testing Identity of Multidimensional Histograms", "abstract": "We investigate the problem of identity testing for multidimensional histogram distributions. A distribution $p: D \\to \\mathbb{R}_+$, where $D \\subseteq \\mathbb{R}^d$, is called a $k$-histogram if there exists a partition of the domain  into $k$ axis-aligned rectangles such that $p$ is constant within each such rectangle. Histograms are one of the most fundamental nonparametric families of distributions and have been extensively studied in computer science and statistics. We give the first identity tester for this problem with {\\em sub-learning} sample complexity in any fixed dimension and a nearly-matching sample complexity lower bound. In more detail, let $q$ be an unknown $d$-dimensional $k$-histogram distribution in fixed dimension $d$,  and $p$ be an explicitly given $d$-dimensional $k$-histogram. We want to correctly distinguish, with probability at least $2/3$, between the case that $p = q$ versus $\\|p-q\\|_1 \\geq \\epsilon$. We design an algorithm for this hypothesis testing problem with sample complexity $O((\\sqrt{k}/\\epsilon^2) 2^{d/2} \\log^{2.5 d}(k/\\epsilon))$ that runs in sample-polynomial time. Our algorithm is robust to model misspecification, i.e., succeeds even if $q$ is only promised  to be {\\em close} to a $k$-histogram. Moreover, for $k = 2^{\\Omega(d)}$, we show a sample complexity lower bound of $(\\sqrt{k}/\\epsilon^2) \\cdot \\Omega(\\log(k)/d)^{d-1}$ when $d\\geq 2$.  That is, for any fixed dimension $d$, our upper and lower bounds are nearly matching. Prior to our work, the sample complexity of the $d=1$ case was well-understood, but no algorithm with sub-learning sample complexity was known, even for $d=2$. Our new upper and lower bounds have interesting conceptual implications regarding the relation between learning and testing in this setting.", "pdf_url": "http://proceedings.mlr.press/v99/diakonikolas19b/diakonikolas19b.pdf", "keywords": ["distribution testing", "hypothesis testing", "goodness of fit", "multivariate histograms"], "reference": "J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions.  In  Advances in Neural Information Processing Systems (NIPS), pages 3591-3599, 2015a.  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and Near-Optimal Algorithms for Approximating Distributions by Histograms. In 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015b.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Fast algorithms for segmented regression. In Pro- ceedings of the 33nd International Conference on Machine Learning, ICML 2016, pages 2878- 2886, 2016.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt.  nearly-linear time. on Discrete Algorithms, SODA 2017, pages 1278-1289, 2017. https://arxiv.org/abs/1506.00671.  Sample-optimal density estimation in In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium Full version available at  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  T. Batu and C. L. Canonne. Generalized uniformity testing. In 58th IEEE Annual Symposium on  Foundations of Computer Science, FOCS 2017, pages 880-889, 2017.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close.  In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  N. Bruno, S. Chaudhuri, and L. Gravano. Stholes: A multidimensional workload-aware histogram. In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, pages 211-222, 2001.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  C. L. Canonne. Are few bins enough: Testing histogram distributions.  In Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2016, pages 455-463, 2016.  13   TESTING IDENTITY OF MULTIDIMENSIONAL HISTOGRAMS  Acknowledgments  We thank Alistair Stewart for his contributions to the early stages of this work. Ilias Diakonikolas was supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. Daniel Kane was supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship. John Peebles was supported by the NSF Graduate Research Fellowship under Grant 1122374, and by NSF Grant 1065125. Some of this research was performed while the third author was visiting USC, supported by a USC startup grant.  References  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions.  In  Advances in Neural Information Processing Systems (NIPS), pages 3591-3599, 2015a.  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and Near-Optimal Algorithms for Approximating Distributions by Histograms. In 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015b.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Fast algorithms for segmented regression. In Pro- ceedings of the 33nd International Conference on Machine Learning, ICML 2016, pages 2878- 2886, 2016.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt.  nearly-linear time. on Discrete Algorithms, SODA 2017, pages 1278-1289, 2017. https://arxiv.org/abs/1506.00671.  Sample-optimal density estimation in In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium Full version available at  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  T. Batu and C. L. Canonne. Generalized uniformity testing. In 58th IEEE Annual Symposium on  Foundations of Computer Science, FOCS 2017, pages 880-889, 2017.  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close.  In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  N. Bruno, S. Chaudhuri, and L. Gravano. Stholes: A multidimensional workload-aware histogram. In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, pages 211-222, 2001.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  C. L. Canonne. Are few bins enough: Testing histogram distributions.  In Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2016, pages 455-463, 2016. TESTING IDENTITY OF MULTIDIMENSIONAL HISTOGRAMS  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016, pages 25:1-25:14, 2016.  C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks.  In  Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 370-448, 2017a.  C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing conditional independence of discrete distributions. CoRR, abs/1711.11560, 2017b. URL http://arxiv.org/abs/ 1711.11560. In STOC\u201918.  C. L. Canonne, I. Diakonikolas, and A. Stewart. Fourier-based testing for families of distributions.  CoRR, abs/1706.05738, 2017c. In NeurIPS 2018.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S. Chan, I. Diakonikolas, P. Valiant, and G. Valiant. Optimal algorithms for testing closeness of  discrete distributions. In SODA, pages 1193-1203, 2014c.  S. Chaudhuri, R. Motwani, and V. R. Narasayya. Random sampling for histogram construction:  How much is enough? In SIGMOD Conference, pages 436-447, 1998.  G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for massive data: Samples, histograms, wavelets, sketches. Found. Trends databases, 4:1-294, 2012. ISSN 1931-7883.  C. Daskalakis and Q. Pan. Square Hellinger subadditivity for Bayesian networks and its applications to identity testing. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 697-703, 2017.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  SODA, pages 1371-1385, 2012.  C. Daskalakis, I. Diakonikolas, R. Servedio, G. Valiant, and P. Valiant. Testing k-modal distribu-  tions: Optimal algorithms via reductions. In SODA, pages 1833-1852, 2013.  C. Daskalakis, N. Dikkala, and G. Kamath. Testing Ising models. In SODA, 2018.  L. Devroye and G. Lugosi. Bin width selection in multivariate histograms by the combinatorial  method. Test, 13(1):129-145, 2004.  I. Diakonikolas and D. M. Kane. A new approach for testing properties of discrete distributions. In  FOCS, pages 685-694, 2016. Full version available at abs/1601.05557. TESTING IDENTITY OF MULTIDIMENSIONAL HISTOGRAMS  I. Diakonikolas, M. Hardt, and L. Schmidt. Differentially private learning of structured discrete  distributions. In NIPS, pages 2566-2574, 2015a.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Testing identity of structured distributions.  In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 1841-1854, 2015b.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Optimal algorithms and lower bounds for test- In IEEE 56th Annual Symposium on Foundations of  ing closeness of structured distributions. Computer Science, FOCS 2015, pages 1183-1202, 2015c.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Collision-based testers are optimal for uniformity and closeness. Electronic Colloquium on Computational Complexity (ECCC), 23: 178, 2016a.  I. Diakonikolas, D. M. Kane, and A. Stewart. Efficient robust proper learning of log-concave distri-  butions. CoRR, abs/1606.03077, 2016b.  I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. CoRR, abs/1611.03473, 2016c. URL http://arxiv.org/abs/1611.03473. In Proceedings of FOCS\u201917.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Sample-optimal identity testing with high  probability. CoRR, abs/1708.02728, 2017a. In ICALP 2018.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Near-optimal closeness testing of discrete histogram In 44th International Colloquium on Automata, Languages, and Programming,  distributions. ICALP 2017, pages 8:1-8:15, 2017b.  I. Diakonikolas, D. M. Kane, and A. Stewart. Sharp bounds for generalized uniformity testing. In Advances in Neural Information Processing Systems 31, NeurIPS 2018, pages 6204-6213, 2018a.  I. Diakonikolas, J. Li, and L. Schmidt. Fast and sample near-optimal algorithms for learning mul- tidimensional histograms. In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 819-842. PMLR, 2018b.  D. Freedman and P. Diaconis. On the histogram as a density estimator:l2 theory. Zeitschrift f\u00a8ur  Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57(4):453-476, 1981.  A. C. Gilbert, S. Guha, P. Indyk, Y. Kotidis, S. Muthukrishnan, and M. Strauss. Fast, small-space  algorithms for approximate histogram maintenance. In STOC, pages 389-398, 2002.  O. Goldreich.  Introduction to Property Testing. Forthcoming, 2017. URL http://www.  wisdom.weizmann.ac.il/\u02dcoded/pt-intro.html.  O. Goldreich, S. Goldwasser, and D. Ron. Property testing and its connection to learning and  approximation. Journal of the ACM, 45:653-750, 1998.  U. Grenander. On the theory of mortality measurement. Skand. Aktuarietidskr., 39:125-153, 1956. TESTING IDENTITY OF MULTIDIMENSIONAL HISTOGRAMS  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  S. Guha, N. Koudas, and K. Shim. Approximation and streaming algorithms for histogram con-  struction problems. ACM Trans. Database Syst., 31(1):396-438, 2006.  D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Approximating multi-dimensional aggregate range queries over real attributes. In Proceedings of the 2000 ACM SIGMOD Interna- tional Conference on Management of Data, pages 463-474, 2000.  P. Indyk, R. Levi, and R. Rubinfeld. Approximating and Testing k-Histogram Distributions in Sub-  linear Time. In PODS, pages 15-22, 2012.  H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel. Optimal  histograms with quality guarantees. In VLDB, pages 275-286, 1998.  J. Klemela. Multivariate histograms with data-dependent partitions. Statistica Sinica, 19(1):159-  176, 2009.  G. Lugosi and A. Nobel. Consistency of data-driven histogram methods for density estimation and  classification. Ann. Statist., 24(2):687-706, 04 1996.  S. Muthukrishnan. Data streams: Algorithms and applications. Found. Trends Theor. Comput. Sci.,  1(2):117-236, 2005.  J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypothe- ses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933. doi: 10.1098/rsta. 1933.0009. URL http://rsta.royalsocietypublishing.org/content/231/ 694-706/289.short.  L. Paninski. A coincidence-based test for uniformity given very sparsely-sampled discrete data.  IEEE Transactions on Information Theory, 54:4750-4755, 2008.  K. Pearson. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine Series 5, 50(302):157-175, 1900.  V. Poosala and Y. E. Ioannidis. Selectivity estimation without the attribute value independence In Proceedings of the 23rd International Conference on Very Large Data Bases, assumption. VLDB \u201997, pages 486-495, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc.  R. Rubinfeld. Taming big probability distributions. XRDS, 19(1):24-28, 2012.  R. Rubinfeld and M. Sudan. Robust characterizations of polynomials with applications to program  testing. SIAM J. on Comput., 25:252-271, 1996.  D. W. Scott. On optimal and data-based histograms. Biometrika, 66(3):605-610, 1979.  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  1992. TESTING IDENTITY OF MULTIDIMENSIONAL HISTOGRAMS  N. Thaper, S. Guha, P. Indyk, and N. Koudas. Dynamic multidimensional histograms. In SIGMOD  Conference, pages 428-439, 2002.  G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity testing. In  FOCS, 2014.  R. Willett and R. D. Nowak. Multiscale poisson intensity and density estimation. IEEE Transactions  on Information Theory, 53(9):3171-3187, 2007.  "}, "Lower Bounds for Parallel and Randomized Convex Optimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Lower Bounds for Parallel and Randomized Convex Optimization", "abstract": "We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds  with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean ($\\ell_2$) and $\\ell_\\infty$ spaces. Our work provides a more general and streamlined approach for proving lower bounds in the setting of parallel convex optimization.", "pdf_url": "http://proceedings.mlr.press/v99/diakonikolas19c/diakonikolas19c.pdf", "keywords": ["Lower bounds", "convex optimization", "parallel algorithms", "randomized algorithms"], "reference": "COLT\u201918, 2018.  Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In Proc.  Eric Balkanski and Yaron Singer. Parallelization does not accelerate convex optimization: Adaptiv- ity lower bounds for non-smooth convex minimization. arXiv preprint arXiv:1808.03880, 2018.  K. Ball, E. Carlen, and E.H. Lieb. Sharp uniform convexity and smoothness inequalities for trace  norms. Inventiones mathematicae, 115(1):463-482, 1994.  G\u00b4abor Braun, Cristobal Guzman, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Trans. Information Theory, 63 (7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017.  A. d\u2019Aspremont, C. Guzm\u00b4an, and M. Jaggi. Optimal affine-invariant smooth minimization algo-  rithms. SIAM Journal on Optimization, 28(3):2384-2405, 2018.  John Duchi, Feng Ruan, and Chulhee Yun. Minimax bounds on stochastic batched convex opti-  mization. In Proc. COLT\u201918, 2018.  M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quar-  terly, 3:95-110, 1956.  Crist\u00b4obal Guzm\u00b4an and Arkadi Nemirovski. On lower complexity bounds for large-scale smooth  convex optimization. Journal of Complexity, 31(1):1 - 14, 2015.  Uffe Haagerup. The best constants in the Khintchine inequality. Studia Mathematica, 70:231-283,  1981.  13   LOWER BOUNDS FOR PARALLEL AND RANDOMIZED CONVEX OPTIMIZATION  Acknowledgments  Part of this work was done while JD was a Microsoft Research Fellow at the Simons Institute for the Theory of Computing, for the program on Foundations of Data Science, and while she was a postdoctoral researcher at Boston University. JD was partially supported by the NSF grant #CCF- 1740855. CG was partially supported by the FONDECYT project 11160939, and the Millennium Science Initiative of the Ministry of Economy, Development, and Tourism, grant \u201cMillennium Nu- cleus Center for the Discovery of Structures in Complex Data.\u201d  We thank Adam Smith for pointing out the importance of lower bounds in parallel convex op- timization from the local differential privacy perspective, for his useful comments and insights that led to this work, and for many useful discussions. We also thank Ilias Diakonikolas for sharing his expertise on anticoncentration, as well as Yossi Arjevani, Nicolas Casabianca, Vitaly Feldman, and Jos\u00b4e Verschae for valuable discussions and comments regarding this work. An earlier version of this paper made an incorrect use of the orthogonal splittings of (cid:96)p spaces, which has since been corrected. We thank Boris Kashin for pointing out this issue. We also thank an anonymous reviewer who pointed out imprecise conditioning in the proof of Lemma 22, which has since been corrected.  References  COLT\u201918, 2018.  Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In Proc.  Eric Balkanski and Yaron Singer. Parallelization does not accelerate convex optimization: Adaptiv- ity lower bounds for non-smooth convex minimization. arXiv preprint arXiv:1808.03880, 2018.  K. Ball, E. Carlen, and E.H. Lieb. Sharp uniform convexity and smoothness inequalities for trace  norms. Inventiones mathematicae, 115(1):463-482, 1994.  G\u00b4abor Braun, Cristobal Guzman, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Trans. Information Theory, 63 (7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017.  A. d\u2019Aspremont, C. Guzm\u00b4an, and M. Jaggi. Optimal affine-invariant smooth minimization algo-  rithms. SIAM Journal on Optimization, 28(3):2384-2405, 2018.  John Duchi, Feng Ruan, and Chulhee Yun. Minimax bounds on stochastic batched convex opti-  mization. In Proc. COLT\u201918, 2018.  M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quar-  terly, 3:95-110, 1956.  Crist\u00b4obal Guzm\u00b4an and Arkadi Nemirovski. On lower complexity bounds for large-scale smooth  convex optimization. Journal of Complexity, 31(1):1 - 14, 2015.  Uffe Haagerup. The best constants in the Khintchine inequality. Studia Mathematica, 70:231-283,  1981. LOWER BOUNDS FOR PARALLEL AND RANDOMIZED CONVEX OPTIMIZATION  Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time al- gorithm for approximate max \ufb02ow in undirected graphs, and its multicommodity generalizations. In Proc. ACM-SIAM SODA\u201914, 2014.  Yin Tat Lee, Satish Rao, and Nikhil Srivastava. A new approach to computing maximum \ufb02ows  using electrical \ufb02ows. In Proc. ACM STOC\u201913, 2013.  A. Nemirovski. On parallel complexity of nonsmooth convex optimization. Journal of Complexity,  10(4):451-463, 1994.  A. Nemirovskii and Y. Nesterov. Optimal methods of smooth convex optimization (in Russian). Zh.  Vychisl. Mat. i Mat. Fiz., 25(3):356-369, 1985.  A.S. Nemirovsky and D.B. Yudin. Problem complexity and method efficiency in optimization.  Wiley-Interscience, 1983. ISBN 0 471 10345 4.  Y. Nesterov and A. Nemirovski. On First-Order Algorithms for (cid:96)1/Nuclear Norm Minimization.  Acta Numerica, 22, 4 2013.  1 edition, 1989.  G. Pisier. The Volume of Convex Bodies and Banach Space Geometry. Cambridge University Press,  Adam Smith, Abhradeep Thakurta, and Jalaj Upadhyay.  Is interaction necessary for distributed  private learning? In Proc. IEEE SP\u201917, 2017.  N. Srebro and K. Sridharan. On convex optimization, fat shattering and learning. Unpublished,  2012. URL http://ttic.uchicago.edu/\u02dckarthik/optfat.pdf.  Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.  Blake Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Proc. NeurIPS\u201918, 2018.  Blake E. Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objec-  tives. In Proc. NIPS\u201916, 2016.  "}, "On the Performance of Thompson Sampling on Logistic Bandits": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "On the Performance of Thompson Sampling on Logistic Bandits", "abstract": "We study the logistic bandit, in which rewards are binary with success probability $\\exp(\\beta a^\\top \\theta) / (1 + \\exp(\\beta a^\\top \\theta))$ and actions $a$ and coefficients $\\theta$ are within the $d$-dimensional unit ball.  While prior regret bounds for algorithms that address the logistic bandit exhibit exponential dependence on the slope parameter $\\beta$, we establish a regret bound for Thompson sampling that is independent of $\\beta$.  Specifically, we establish that, when the set of feasible actions is identical to the set of possible coefficient vectors, the Bayesian regret of Thompson sampling is $\\tilde{O}(d\\sqrt{T})$.  We also establish a $\\tilde{O}(\\sqrt{d\\eta T}/\\Delta)$ bound that applies more broadly, where $\\Delta$ is the worst-case optimal log-odds and $\\eta$ is the \u201cfragility dimension,\u201d a new statistic we define to capture the degree to which an optimal action for one model fails to satisfice for others.  We demonstrate that the fragility dimension plays an essential role by showing that, for any $\\epsilon > 0$, no algorithm can achieve $\\mathrm{poly}(d, 1/\\Delta)\\cdot T^{1-\\epsilon}$ regret.", "pdf_url": "http://proceedings.mlr.press/v99/dong19a/dong19a.pdf", "keywords": ["bandits", "Thompson sampling", "logistic regression", "regret bounds"], "reference": "Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. Electronic Journal of  Statistics, 11(2):5165-5197, 2017.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. In Conference on Learning Theory, pages 583-589, 2016.  Shi Dong and Benjamin Van Roy. An information-theoretic analysis for Thompson sampling with  many actions. In Advances in Neural Information Processing Systems, 2018.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-  tual bandits. In International Conference on Machine Learning, pages 2071-2080, 2017.  Fang Liu, Swapna Buccapatnam, and Ness Shroff.  Information directed sampling for stochastic bandits with graph feedback. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.  Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In  Advances in Neural Information Processing Systems, pages 1583-1591, 2014a.  Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of  Operations Research, 39(4):1221-1243, 2014b.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.  The Journal of Machine Learning Research, 17(1):2442-2471, 2016.  Daniel Russo and Benjamin Van Roy. Satisficing in time-sensitive bandit learning. arXiv preprint  arXiv:1803.02855, 2018.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.  3   ON THE PERFORMANCE OF THOMPSON SAMPLING ON LOGISTIC BANDITS  The assumption that the worst-case optimal log-odds are positive may be restrictive. This is equivalent to assuming that the for each possible model, the optimal action yields more than 50% probability of success. However, this assumption is essential, since it ensures that the fragility di- mension is well-defined. When the worst-case optimal log-odds are negative, the geometry of action and parameter sets plays a less significant role than parameter \u03b2, therefore we conjecture that the exponential dependence on \u03b2 is inevitable. This could be an interesting direction for future research.  References  Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. Electronic Journal of  Statistics, 11(2):5165-5197, 2017.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. In Conference on Learning Theory, pages 583-589, 2016.  Shi Dong and Benjamin Van Roy. An information-theoretic analysis for Thompson sampling with  many actions. In Advances in Neural Information Processing Systems, 2018.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-  tual bandits. In International Conference on Machine Learning, pages 2071-2080, 2017.  Fang Liu, Swapna Buccapatnam, and Ness Shroff.  Information directed sampling for stochastic bandits with graph feedback. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.  Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In  Advances in Neural Information Processing Systems, pages 1583-1591, 2014a.  Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of  Operations Research, 39(4):1221-1243, 2014b.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.  The Journal of Machine Learning Research, 17(1):2442-2471, 2016.  Daniel Russo and Benjamin Van Roy. Satisficing in time-sensitive bandit learning. arXiv preprint  arXiv:1803.02855, 2018.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933. "}, "Lower Bounds for Locally Private Estimation via Communication Complexity": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Lower Bounds for Locally Private Estimation via Communication Complexity", "abstract": "We develop lower bounds for estimation under local privacy constraints\u2014including differential privacy and its relaxations to approximate or R\u00e9nyi differential privacy\u2014by showing an equivalence between private estimation and communication-restricted estimation problems. Our results apply to arbitrarily interactive privacy mechanisms, and they also give sharp lower bounds for all levels of differential privacy protections, that is, privacy mechanisms with privacy levels $\\varepsilon \\in [0, \\infty)$.  As a particular consequence of our results, we show that the minimax mean-squared error for estimating the mean of a bounded or Gaussian random vector in $d$ dimensions scales as $\\frac{d}{n} \\cdot \\frac{d}{ \\min\\{\\varepsilon, \\varepsilon^2\\}}$.", "pdf_url": "http://proceedings.mlr.press/v99/duchi19a/duchi19a.pdf", "keywords": [], "reference": "Apple Differential  Privacy  Team.  2017. https://machinelearning.apple.com/2017/12/06/  Learning with  privacy  scale,  at  Available learning-with-privacy-at-scale.html.  at  P. Assouad. Deux remarques sur l\u2019estimation. Comptes Rendus des S\u00b4eances de l\u2019Acad\u00b4emie des  Sciences, S\u00b4erie I, 296(23):1021-1024, 1983.  A. Beimel, K. Nissim, and E. Omri. Distributed private data analysis: Simultaneously solving how and what. In Advances in Cryptology, volume 5157 of Lecture Notes in Computer Science, pages 451-468. Springer, 2008.  A. Bhowmick, J. Duchi, J. Freudiger, G. Kapoor, and R. Rogers. Protection against reconstruction  and its applications in private federated learning. arXiv:1812.00984 [stat.ML], 2018.  P. Billingsley. Probability and Measure. Wiley, Second edition, 1986.  M. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the Forty-Eighth Annual ACM Symposium on the Theory of Computing, 2016. URL https: //arxiv.org/abs/1506.07216.  M. Bun and T. Steinke. Concentrated differential privacy: Simplifications, extensions, and lower  bounds. In Theory of Cryptography Conference (TCC), pages 635-658, 2016.  T. M. Cover and J. A. Thomas. Elements of Information Theory, Second Edition. Wiley, 2006.  J. C. Duchi and F. Ruan. The right complexity measure in locally private estimation: It is not the  Fisher information. arXiv:1806.05756 [stat.TH], 2018.  J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Minimax optimal procedures for locally private estimation (with discussion). Journal of the American Statistical Association, 113(521):182-215, 2018.  C. Dwork and G. Rothblum. Concentrated differential privacy. arXiv:1603.01887 [cs.DS], 2016.  C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via  distributed noise generation. In Advances in Cryptology (EUROCRYPT 2006), 2006a.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the Third Theory of Cryptography Conference, pages 265-284, 2006b.  C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In 51st Annual  Symposium on Foundations of Computer Science, pages 51-60, 2010.  U. Erlingsson, V. Pihur, and A. Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 21st ACM Conference on Computer and Communications Security (CCS), 2014.  13   LOWER BOUNDS FOR LOCALLY PRIVATE ESTIMATION  References  Apple Differential  Privacy  Team.  2017. https://machinelearning.apple.com/2017/12/06/  Learning with  privacy  scale,  at  Available learning-with-privacy-at-scale.html.  at  P. Assouad. Deux remarques sur l\u2019estimation. Comptes Rendus des S\u00b4eances de l\u2019Acad\u00b4emie des  Sciences, S\u00b4erie I, 296(23):1021-1024, 1983.  A. Beimel, K. Nissim, and E. Omri. Distributed private data analysis: Simultaneously solving how and what. In Advances in Cryptology, volume 5157 of Lecture Notes in Computer Science, pages 451-468. Springer, 2008.  A. Bhowmick, J. Duchi, J. Freudiger, G. Kapoor, and R. Rogers. Protection against reconstruction  and its applications in private federated learning. arXiv:1812.00984 [stat.ML], 2018.  P. Billingsley. Probability and Measure. Wiley, Second edition, 1986.  M. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the Forty-Eighth Annual ACM Symposium on the Theory of Computing, 2016. URL https: //arxiv.org/abs/1506.07216.  M. Bun and T. Steinke. Concentrated differential privacy: Simplifications, extensions, and lower  bounds. In Theory of Cryptography Conference (TCC), pages 635-658, 2016.  T. M. Cover and J. A. Thomas. Elements of Information Theory, Second Edition. Wiley, 2006.  J. C. Duchi and F. Ruan. The right complexity measure in locally private estimation: It is not the  Fisher information. arXiv:1806.05756 [stat.TH], 2018.  J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Minimax optimal procedures for locally private estimation (with discussion). Journal of the American Statistical Association, 113(521):182-215, 2018.  C. Dwork and G. Rothblum. Concentrated differential privacy. arXiv:1603.01887 [cs.DS], 2016.  C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via  distributed noise generation. In Advances in Cryptology (EUROCRYPT 2006), 2006a.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the Third Theory of Cryptography Conference, pages 265-284, 2006b.  C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In 51st Annual  Symposium on Foundations of Computer Science, pages 51-60, 2010.  U. Erlingsson, V. Pihur, and A. Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 21st ACM Conference on Computer and Communications Security (CCS), 2014. LOWER BOUNDS FOR LOCALLY PRIVATE ESTIMATION  U. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and A. Thakurta. Amplification In Proceedings of the  by shuf\ufb02ing: From local to central differential privacy via anonymity. Thirtieth ACM-SIAM Symposium on Discrete Algorithms (SODA), 2019.  V. Feldman and T. Steinke. Calibrating noise to variance in adaptive data analysis. In Proceedings of the Thirty First Annual Conference on Computational Learning Theory, 2018. URL http: //arxiv.org/abs/1712.07196.  M. Gaboardi, R. Rogers, and O. Sheffet. Locally private mean estimation: Z-test and tight confi-  dence intervals. arXiv:1810.08054 [cs.DS], 2018.  A. Garg, T. Ma, and H. L. Nguyen. On communication cost of distributed statistical estimation and  dimensionality. In Advances in Neural Information Processing Systems 28, 2014.  R. M. Gray. Entropy and Information Theory. Springer, 1990.  J. Hiriart-Urruty and C. Lemar\u00b4echal. Convex Analysis and Minimization Algorithms I & II. Springer,  New York, 1993.  I. Johnstone. Gaussian Estimation: Sequence and Wavelet Models. 2013.  M. Joseph, J. Kulkarni, J. Mao, and Z. S. Wu.  Locally private gaussian estimation.  arXiv:1811.08382 [cs.LG], 2018.  arXiv:1904.03564 [cs.LG], 2019.  M. Joseph, J. Mao, S. Neel, and A. Roth. The role of interactivity in local differential privacy.  S. P. Kasiviswanathan and A. Smith. On the \u2019semantics\u2019 of differential privacy: A Bayesian for- mulation. Journal of Privacy and Confidentiality, 6(1), 2014. doi: 10.29012/jpc.v6i1.634. URL http://arxiv.org/abs/0803.3946v3.  S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn  privately? SIAM Journal on Computing, 40(3):793-826, 2011.  L. Le Cam and G. L. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer, 2000.  F. Liese and I. Vajda. On divergences and informations in statistics and information theory. IEEE  Transactions on Information Theory, 52(10):4394-4412, 2006.  A. McGregor, I. Mironov, T. Pitassi, O. Reingold, K. Talwar, and S. Vadhan. The limits of two- party differential privacy. In 51st Annual Symposium on Foundations of Computer Science, pages 81-90. IEEE, 2010.  I. Mironov. R\u00b4enyi differential privacy. In 30th IEEE Computer Security Foundations Symposium  (CSF), pages 263-275, 2017.  R. M. Rogers, A. Roth, A. D. Smith, and O. Thakkar. Max-information, differential privacy, and post-selection hypothesis testing. In 57th Annual Symposium on Foundations of Computer Sci- ence, pages 487-494, 2016. LOWER BOUNDS FOR LOCALLY PRIVATE ESTIMATION  A. Rohde and L. Steinberger. Geometrizing rates of convergence under differential privacy con-  straints. arXiv:1805.01422 [stat.ML], 2018.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.  A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Math-  ematics. Cambridge University Press, 1998.  R. Vershynin.  In Compressed Sensing: Theory and Applications, chapter 5, pages 210-268. Cambridge University Press, 2012.  Introduction to the non-asymptotic analysis of random matrices.  M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Univer-  sity Press, 2019.  S. Warner. Randomized response: a survey technique for eliminating evasive answer bias. Journal  of the American Statistical Association, 60(309):63-69, 1965.  M. Ye and A. Barg. Optimal schemes for discrete distribution estimation under locally differential  privacy. IEEE Transactions on Information Theory, 64(8):5662-5676, 2018.  B. Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer-  Verlag, 1997.  Y. Zhang, J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Information-theoretic lower bounds for distributed estimation with communication constraints. In Advances in Neural Information Processing Systems 27, 2013. LOWER BOUNDS FOR LOCALLY PRIVATE ESTIMATION  7 2  7 2  n (cid:88)  i=1 n (cid:88)  i=1  "}, "Sharp Analysis for Nonconvex SGD Escaping from Saddle Points": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sharp Analysis for Nonconvex SGD Escaping from Saddle Points", "abstract": "In this paper, we give a sharp analysis for  Stochastic Gradient Descent (SGD)  and  prove that SGD is able to efficiently escape from  saddle points and find an $(\\epsilon, O(\\epsilon^{0.5}))$-approximate second-order stationary point in $\\tilde{O}(\\epsilon^{-3.5})$ stochastic gradient computations for generic nonconvex optimization problems, when the objective function satisfies  gradient-Lipschitz, Hessian-Lipschitz, and dispersive noise assumptions. This  result subverts the classical belief that  SGD requires at least $O(\\epsilon^{-4})$ stochastic gradient computations for obtaining an $(\\epsilon,O(\\epsilon^{0.5}))$-approximate second-order stationary point. Such SGD rate matches, up to a polylogarithmic factor of problem-dependent parameters, the rate of most accelerated  nonconvex stochastic optimization algorithms that adopt additional techniques, such as Nesterov\u2019s momentum acceleration, negative curvature search, as well as quadratic and cubic regularization tricks. Our novel analysis gives new insights into nonconvex SGD and can be potentially generalized to a broad class of stochastic optimization algorithms.", "pdf_url": "http://proceedings.mlr.press/v99/fang19a/fang19a.pdf", "keywords": ["Stochastic Gradient Descent", "Non-convex Optimization", "Convergence Rate", "Saddle Escaping"]}, "Achieving the Bayes Error Rate in Stochastic Block Model by SDP, Robustly": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Achieving the Bayes Error Rate in Stochastic Block Model by SDP, Robustly", "abstract": "We study the statistical performance of the semidefinite programming (SDP) relaxation approach for clustering under the binary symmetric Stochastic Block Model (SBM). We show that the SDP achieves an error rate of the form  $\\exp\\left[-(1-o(1))\\frac{n I}{2}\\right]$, where $I$ is an appropriate information-theoretic measure of the signal-to-noise ratio. This bound matches the minimax lower bound on the optimal Bayes error rate for this problem, and improves upon existing results that are sub-optimal by a multiplicative constant in the exponent. As a corollary, our result implies that SDP achieves the optimal exact recovery threshold with the correct leading constant. We further show that this error rate of SDP is robust; that is, it remains unchanged under the so-called semirandom model where the graph is modified by a monotone adversary, as well as under the setting with heterogeneous edge probabilities. Our proof is based on a novel primal-dual analysis of the SDP.", "pdf_url": "http://proceedings.mlr.press/v99/fei19a/fei19a.pdf", "keywords": ["Stochastic Block Model", "semidefinite programming", "minimax rates", "Bayes risk", "robustness"], "reference": "Emmanuel Abbe. Community detection and stochastic block models: Recent developments. Jour-  nal of Machine Learning Research, 18(177):1-86, 2018.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery. In IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), pages 670-688. IEEE, 2015a.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015b.  Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block model without knowing the parameters. In Advances in Neural Information Processing Systems, pages 676-684, 2015c.  Emmanuel Abbe, Afonso S. Bandeira, Annina Bracher, and Amit Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. IEEE Transac- tions on Network Science and Engineering, 1(1):10-22, 2014.  Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector analysis  of random matrices with low expected rank. arXiv preprint arXiv:1709.09565, 2017.  Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral robust-  ness. arXiv preprint arXiv:1809.04818, 2018.  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. In Compressed Sensing and its Applications, pages 125-162. Springer, 2017.  Afonso S. Bandeira. Random laplacian matrices and convex relaxations. Foundations of Computa-  tional Mathematics, 18(2):345-379, 2018.  Debapratim Banerjee. Contiguity and non-reconstruction results for planted partition models: the  dense case. Electronic Journal of Probability, 23, 2018.  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of  Algorithms, 19(2):204-234, 1995.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Nonbacktracking spectrum of random graphs: Community detection and nonregular ramanujan graphs. Annals of Probability, 46(1): 1-71, 2018.  T. Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. Annals of Statistics, 43(3):1027-1059, 2015. URL http: //arxiv.org/abs/1404.6000.  13   ACHIEVING THE BAYES ERROR RATE IN STOCHASTIC BLOCK MODEL BY SDP, ROBUSTLY  References  Emmanuel Abbe. Community detection and stochastic block models: Recent developments. Jour-  nal of Machine Learning Research, 18(177):1-86, 2018.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery. In IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), pages 670-688. IEEE, 2015a.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015b.  Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic block model without knowing the parameters. In Advances in Neural Information Processing Systems, pages 676-684, 2015c.  Emmanuel Abbe, Afonso S. Bandeira, Annina Bracher, and Amit Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. IEEE Transac- tions on Network Science and Engineering, 1(1):10-22, 2014.  Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector analysis  of random matrices with low expected rank. arXiv preprint arXiv:1709.09565, 2017.  Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral robust-  ness. arXiv preprint arXiv:1809.04818, 2018.  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. In Compressed Sensing and its Applications, pages 125-162. Springer, 2017.  Afonso S. Bandeira. Random laplacian matrices and convex relaxations. Foundations of Computa-  tional Mathematics, 18(2):345-379, 2018.  Debapratim Banerjee. Contiguity and non-reconstruction results for planted partition models: the  dense case. Electronic Journal of Probability, 23, 2018.  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of  Algorithms, 19(2):204-234, 1995.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Nonbacktracking spectrum of random graphs: Community detection and nonregular ramanujan graphs. Annals of Probability, 46(1): 1-71, 2018.  T. Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. Annals of Statistics, 43(3):1027-1059, 2015. URL http: //arxiv.org/abs/1404.6000. ACHIEVING THE BAYES ERROR RATE IN STOCHASTIC BLOCK MODEL BY SDP, ROBUSTLY  Francesco Caltagirone, Marc Lelarge, and L\u00b4eo Miolane. Recovering asymmetric communities in the stochastic block model. IEEE Transactions on Network Science and Engineering, 5(3):237-246, 2018.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transactions on  Information Theory, 60(10):6440-6455, 2014.  Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic  thresholds from the cavity method. Advances in Mathematics, 333:694-795, 2018.  Yingjie Fei and Yudong Chen. Hidden integrality of SDP relaxation for sub-gaussian mixture mod-  els. arXiv preprint arXiv:1803.06510, 2018.  Yingjie Fei and Yudong Chen. Achieving the bayes error rate in synchronization and block models  by SDP, robustly. arXiv preprint arXiv:1904.09635, 2019a.  Yingjie Fei and Yudong Chen. Exponential error rates of SDP for block models: Beyond Grothendieck\u2019s inequality. IEEE Transactions on Information Theory, 65(1):551-571, 2019b.  Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer and  System Sciences, 63(4):639-671, 2001.  Chao Gao and Zongming Ma. Minimax rates in network analysis: Graphon estimation, community  detection and hypothesis testing. arXiv preprint arXiv:1811.06055, 2018.  Chao Gao, Zongming Ma, Anderson Y. Zhang, and Harrison H. Zhou. Achieving optimal misclas- sification proportion in stochastic block models. The Journal of Machine Learning Research, 18 (1):1980-2024, 2017.  Chao Gao, Zongming Ma, Anderson Y. Zhang, and Harrison H. Zhou. Community detection in  degree-corrected block models. The Annals of Statistics, 46(5):2153-2185, 2018.  Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed k  means. arXiv preprint arXiv:1807.07547, 2018.  Alexander Grothendieck. R\u00b4esum\u00b4e de la th\u00b4eorie m\u00b4etrique des produits tensoriels topologiques. Re- senhas do Instituto de Matem\u00b4atica e Estatistica da Universidade de S\u02dcao Paulo, 2(4):401-481, 1953.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via Grothendieck\u2019s  inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Exact recovery threshold in the binary censored block  model. In IEEE Information Theory Workshop (ITW), pages 99-103, 2015.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef-  inite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, 2016a.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef- inite programming: Extensions. IEEE Transactions on Information Theory, 62(10):5918-5937, 2016b. ACHIEVING THE BAYES ERROR RATE IN STOCHASTIC BLOCK MODEL BY SDP, ROBUSTLY  Varun Jog and Po-Ling Loh. Information-theoretic bounds for exact recovery in weighted stochastic  block models using the renyi divergence. arXiv preprint arXiv:1509.06418, 2015.  Marc Lelarge, Laurent Massouli\u00b4e, and Jiaming Xu. Reconstruction in the labelled stochastic block  model. IEEE Transactions on Network Science and Engineering, 2(4):152-163, 2015.  Xiaodong Li, Yudong Chen, and Jiaming Xu. Convex relaxation methods for community detection.  arXiv preprint arXiv:1810.00315, 2018.  Joram Lindenstrauss and Aleksander Pe\u0142czy\u00b4nski. Absolutely summing operators in Lp-spaces and  their applications. Studia Mathematica, 3(29):275-326, 1968.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning communities In 29th Annual Conference on Learning Theory, pages 1258-1291,  in the presence of errors. 2016.  Laurent Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property. In Pro- ceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694-703. ACM, 2014.  Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for community detection? In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 828-841. ACM, 2016.  Andrea Montanari and Subhabrata Sen. Semidefinite programs on sparse random graphs and their application to community detection. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 814-827, 2016.  Cristopher Moore. The computer science and physics of community detection: Landscapes, phase transitions, and hardness. Bulletin of European Association for Theoretical Computer Science (EATCS), 1(121), Februrary 2017.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  arXiv preprint arXiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition  model. Probability Theory and Related Fields, 162(3-4):431-461, 2015.  Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for the planted bisection model. Electronic Journal of Probability, 21(21):1-24, 2016. doi: 10.1214/16-EJP4185. URL http://dx.doi.org/10.1214/16-EJP4185.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  Combinatorica, 38(3):665-708, 2018.  William Perry and Alexander S. Wein. A semidefinite program for unbalanced multisection in the  stochastic block model. arXiv preprint arXiv:1507.05605, 2015.  Elizaveta Rebrova and Roman Vershynin. Norms of random matrices: local and global problems.  arXiv preprint arXiv:1608.06953, 2016. ACHIEVING THE BAYES ERROR RATE IN STOCHASTIC BLOCK MODEL BY SDP, ROBUSTLY  Ludovic Stephan and Laurent Massouli\u00b4e. Robustness of spectral methods for community detection.  arXiv preprint arXiv:1811.05808, 2018.  Van Vu. Singular vectors under random perturbation. Random Structures & Algorithms, 39(4):  526-538, 2011.  Min Xu, Varun Jog, and Po-Ling Loh. Optimal rates for community estimation in the weighted  stochastic block model. arXiv preprint arXiv:1706.01175, 2017.  Se-Young Yun and Alexandre Proutiere. Accurate community detection in the stochastic block  model via spectral algorithms. arXiv preprint arXiv:1412.7335, 2014.  Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic block  model. In Advances in Neural Information Processing Systems, pages 965-973, 2016.  Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochastic  block models. The Annals of Statistics, 44(5):2252-2280, 2016.  Anderson Y. Zhang and Harrison H. Zhou. Theoretical and computational guarantees of mean field  variational inference for community detection. arXiv preprint arXiv:1710.11268, 2017.  Zhixin Zhou and Ping Li. Non-asymptotic chernoff lower bound and its application to community  detection in stochastic block model. arXiv preprint arXiv:1812.11269, 2018.  "}, "High probability generalization bounds for uniformly stable algorithms with nearly optimal rate": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "High probability generalization bounds for uniformly stable algorithms with nearly optimal rate", "abstract": "Algorithmic stability is a classical approach to understanding and analysis of the generalization error of learning algorithms. A notable weakness of most stability-based generalization bounds is that they hold only in expectation. Generalization with high probability has been established in a landmark paper of Bousquet and Elisseeff (2001) albeit at the expense of an additional $\\sqrt{n}$ factor in the bound. Specifically, their bound on the estimation error of any $\\gamma$-uniformly stable learning algorithm on $n$ samples and range in $[0,1]$ is $O(\\gamma \\sqrt{n \\log(1/\\delta)} + \\sqrt{\\log(1/\\delta)/n})$ with probability  $\\geq 1-\\delta$. The $\\sqrt{n}$ overhead makes the bound vacuous in the common settings where $\\gamma \\geq 1/\\sqrt{n}$. A stronger bound was recently proved by the authors (Feldman and Vondrak, 2018) that reduces the overhead to at most $O(n^{1/4})$. Still, both of these results give optimal generalization bounds only when $\\gamma = O(1/n)$. We prove a nearly tight bound of $O(\\gamma \\log(n)\\log(n/\\delta) + \\sqrt{\\log(1/\\delta)/n})$ on the estimation error of any $\\gamma$-uniformly stable algorithm. It implies that for algorithms that are uniformly stable with $\\gamma = O(1/\\sqrt{n})$, estimation error is essentially the same as the sampling error. Our result leads to the first high-probability generalization bounds for multi-pass stochastic gradient descent and regularized ERM for stochastic convex problems with nearly optimal rate \u2014 resolving open problems in prior work. Our proof technique is new and we introduce several analysis tools that might find additional applications.", "pdf_url": "http://proceedings.mlr.press/v99/feldman19a/feldman19a.pdf", "keywords": [], "reference": "Karim T. Abou-Moustafa and Csaba Szepesv\u00b4ari. An exponential tail bound for lq stable learning rules. application to k-folds cross-validation. In ISAIM, 2018. URL http://isaim2018. cs.virginia.edu/papers/ISAIM2018_Abou-Moustafa_Szepesvari.pdf.  7   HIGH PROBABILITY GENERALIZATION BOUNDS FOR UNIFORMLY STABLE ALGORITHMS  uniform stability and their applications to generalization properties of strongly convex ERM algo- rithms have been extended and generalized in several directions (e.g. (Zhang, 2003; Wibisono and Poggio, 2009)). Maurer (2017) considers generalization bounds for a special case of linear regres- sion with a strongly convex regularizer and a sufficiently smooth loss function. Their bounds are data-dependent and are potentially stronger for large values of the regularization parameter (and hence stability). However the bound is vacuous when the stability parameter is larger than n\u22121/4 and hence is not directly comparable to ours. Kuzborskij and Lampert (2018) give data-dependent generalization bounds for SGD on smooth convex and non-convex losses based on stability. They use on-average stability that does not imply generalization bounds with high probability. Recent work of Abou-Moustafa and Szepesv\u00b4ari (2018) and Celisse and Guedj (2016) gives high probabi- lity generalization bounds similar to those in (Bousquet and Elisseeff, 2002) but using a bound on a high-order moment of stability instead of the uniform stability. Recent applications of stability to generalization can be found for example in (Liu et al., 2017; Rivasplata et al., 2018; Charles and Pa- pailiopoulos, 2018; Chen et al., 2018). We also remark that all these works are based on techniques different from ours.  Uniform stability has several additional important connections to differential privacy (Dwork et al., 2006). First, differential privacy is itself a type of worst-case stability guarantee that bounds the effect of every data point on the output distribution of the algorithm. Our work is in part inspired by the recent progress showing that differential privacy implies generalization with high probability (Dwork et al., 2014; Bassily et al., 2016). Both the assumptions and guarantees given in this line of work are different from ours and we do not know a way to relate between those. For example, the generalization guarantees obtained in work on differential privacy hold with high probability over the randomness of the algorithm, whereas our results when applied to a differentially private algorithm would only give generalization of the expectation over the algorithm\u2019s randomness. We remark that the techniques developed in this line of work were used to re-derive and extend several standard concentration inequalities (Steinke and Ullman, 2017; Nissim and Stemmer, 2017) and also in (Feldman and Vondr\u00b4ak, 2018) to give an improved generalization bound for uniform stability.  Uniformly stable algorithms also play an important role in privacy-preserving learning since a differentially private learning algorithm can usually be obtained by adding noise to the output of a uniformly stable one (e.g. (Chaudhuri et al., 2011; Wu et al., 2017; Dwork and Feldman, 2018)). Hence understanding the generalization properties of uniformly stable algorithms is likely to play an important role in this line of research.  We thank Nick Harvey, Tomer Koren, Mehryar Mohri, Sasha Rakhlin, Yoram Singer, Karthik Srid- haran, Csaba Szepesvari and Kunal Talwar for thoughtful discussions and insightful comments about this work.  ACKNOWLEDGMENTS  References  Karim T. Abou-Moustafa and Csaba Szepesv\u00b4ari. An exponential tail bound for lq stable learning rules. application to k-folds cross-validation. In ISAIM, 2018. URL http://isaim2018. cs.virginia.edu/papers/ISAIM2018_Abou-Moustafa_Szepesvari.pdf. HIGH PROBABILITY GENERALIZATION BOUNDS FOR UNIFORMLY STABLE ALGORITHMS  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman.  Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016.  Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and pro-  gressive cross-validation. In COLT, pages 203-208, 1999.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. JMLR, 2:499-526, 2002.  Alain Celisse and Benjamin Guedj. Stability revisited: new generalisation bounds for the leave-  one-out. arXiv preprint arXiv:1608.06412, 2016.  Zachary B. Charles and Dimitris S. Papailiopoulos. Stability and generalization of learning algo- rithms that converge to global optima. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018, pages 744-753, 2018. URL http://proceedings.mlr.press/v80/charles18a.html.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. Journal of Machine Learning Research, 12:1069-1109, 2011.  Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization  algorithms. arXiv preprint arXiv:1804.01619, 2018.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer,  1996.  Luc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout error  estimates. IEEE Trans. Information Theory, 25(2):202-207, 1979a.  Luc Devroye and Terry J. Wagner. Distribution-free performance bounds with the resubstitution  error estimate (corresp.). IEEE Trans. Information Theory, 25(2):208-210, 1979b.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. CoRR, abs/1803.10266, 2018.  URL http://arxiv.org/abs/1803.10266. Extended abstract in COLT 2018.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension stri- kes back. CoRR, abs/1608.04414, 2016. URL http://arxiv.org/abs/1608.04414. Extended abstract in NIPS 2016.  Vitaly Feldman and Jan Vondr\u00b4ak. Generalization bounds for uniformly stable algorithms. In Pro- ceedings of NeurIPS, pages 9770-9780, 2018. URL http://papers.nips.cc/paper/ 8182-generalization-bounds-for-uniformly-stable-algorithms. HIGH PROBABILITY GENERALIZATION BOUNDS FOR UNIFORMLY STABLE ALGORITHMS  Vitaly Feldman and Jan Vondr\u00b4ak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. CoRR, abs/1902.10710, 2019. URL http://arxiv. org/abs/1902.10710.  Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of sto- In ICML, pages 1225-1234, 2016. URL http://jmlr.org/  chastic gradient descent. proceedings/papers/v48/hardt16.html.  Satyen Kale, Ravi Kumar, and Sergei Vassilvitskii. Cross-validation and mean-square stability. In Innovations in Computer Science - ICS, pages 487-495, 2011. URL http://conference. itcs.tsinghua.edu.cn/ICS2011/content/papers/31.html.  Michael J. Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out  cross-validation. Neural Computation, 11(6):1427-1453, 1999.  Ravi Kumar, Daniel Lokshtanov, Sergei Vassilvitskii, and Andrea Vattani. Near-optimal bounds for cross-validation via loss stability. In ICML, pages 27-35, 2013. URL http://jmlr.org/ proceedings/papers/v28/kumar13a.html.  Ilja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient des- cent. In ICML, pages 2820-2829, 2018. URL http://proceedings.mlr.press/v80/ kuzborskij18a.html.  Tongliang Liu, G\u00b4abor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In ICML, pages 2159-2167, 2017. URL http://proceedings.mlr.press/ v70/liu17c.html.  G\u00b4abor Lugosi and Miroslaw Pawlak. On the posterior-probability estimate of the error rate of  nonparametric classification rules. IEEE Trans. Information Theory, 40(2):475-481, 1994.  Andreas Maurer. A second-order look at stability and generalization. In COLT, pages 1461-1475,  2017. URL http://proceedings.mlr.press/v65/maurer17a.html.  Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk mini- mization. Advances in Computational Mathematics, 25(1-3):161-193, 2006.  Kobbi Nissim and Uri Stemmer. Concentration bounds for high sensitivity functions through dif- ferential privacy. CoRR, abs/1703.01970, 2017. URL http://arxiv.org/abs/1703. 01970.  Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for pre-  dictivity in learning theory. Nature, 428(6981):419-422, 2004.  Omar Rivasplata, Csaba Szepesvari, John S Shawe-Taylor, Emilio Parrado-Hernandez, and Shiliang In Advances in  Sun. Pac-bayes bounds for stable algorithms with instance-dependent priors. Neural Information Processing Systems, pages 9234-9244, 2018.Ben London. A pac-bayesian analysis of randomized learning with application to stochastic gra- dient descent. In NIPS, pages 2935-2944, 2017. URL http://papers.nips.cc/paper/ 6886-a-pac-bayesian-analysis-of-randomized-learning-with-application-to-stochastic-gradient-descent.   HIGH PROBABILITY GENERALIZATION BOUNDS FOR UNIFORMLY STABLE ALGORITHMS  W. H. Rogers and T. J. Wagner. A finite sample distribution-free performance bound for local dis- crimination rules. The Annals of Statistics, 6(3):506-514, 1978. URL http://www.jstor. org/stable/2958555.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  Thomas Steinke and Jonathan Ullman. Subgaussian tail bounds via stability arguments. arXiv  preprint arXiv:1701.03493, 2017. URL https://arxiv.org/abs/1701.03493.  Nir Weinberger and Alexander Rakhlin. On high probability bounds for uniformly stable learning  algorithms, 2018. Unpublished manuscript.  Rosasco Lorenzo Wibisono, Andre and Tomaso Poggio. Sufficient conditions for uniform stability  of regularization algorithms. Technical Report MIT-CSAIL-TR-2009-060, MIT, 2009.  Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In (SIGMOD), pages 1307-1322, 2017.  Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397-1437,  2003. "}, "Sum-of-squares meets square loss: Fast rates for agnostic tensor completion": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sum-of-squares meets square loss: Fast rates for agnostic tensor completion", "abstract": "We study tensor completion in the agnostic setting. In the classical tensor completion problem, we receive $n$ entries of an unknown rank-$r$ tensor and wish to exactly complete the remaining entries. In agnostic tensor completion, we make \\emph{no assumption} on the rank of the unknown tensor, but attempt to predict unknown entries as well as the best rank-$r$ tensor. For agnostic learning of third-order tensors with the square loss, we give the first polynomial time algorithm that obtains a \u201cfast\u201d (i.e., $O(1/n)$-type) rate improving over the rate obtained by reduction to matrix completion. Our prediction error rate to compete with the best $d\\times{}d\\times{}d$ tensor of rank-$r$ is $\\tilde{O}(r^{2}d^{3/2}/n)$. We also obtain an exact oracle inequality that trades off estimation and approximation error.  Our algorithm is based on the degree-six sum-of-squares relaxation of the tensor nuclear norm. The key feature of our analysis is to show that a certain characterization for the subgradient of the tensor nuclear norm can be encoded in the sum-of-squares proof system. This unlocks the standard toolbox for localization of empirical processes under the square loss, and allows us to establish restricted eigenvalue-type guarantees for various tensor regression models, with tensor completion as a special case. The new analysis of the relaxation complements Barak and Moitra (2016), who gave slow rates for agnostic tensor completion, and Potechin and Steurer (2017), who gave exact recovery guarantees for the noiseless setting. Our techniques are user-friendly, and we anticipate that they will find use elsewhere.", "pdf_url": "http://proceedings.mlr.press/v99/foster19a/foster19a.pdf", "keywords": ["tensor completion", "sum-of-squares", "statistical learning", "agnostic learning", "localization"], "reference": "Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak and David Steurer. Sum-of-squares:proofs, beliefs, and algorithms. 2016. URL https:  //www.sumofsquares.org/public/lec-definitions-general.html.  R\u00b4emi Bardenet, Odalric-Ambrym Maillard, et al. Concentration inequalities for sampling without  replacement. Bernoulli, 21(3):1361-1385, 2015.  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Peter L Bartlett, Shahar Mendelson, and Joseph Neeman. (cid:96)1-regularized linear regression: persistence  and oracle inequalities. Probability theory and related fields, 154(1-2):193-224, 2012.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix  completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 310-321. Springer, 2004.  Amit Daniely. Complexity theoretic limitations on learning halfspaces.  In Proceedings of the  forty-eighth annual ACM symposium on Theory of Computing, pages 105-117. ACM, 2016.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018.  12   FAST RATES FOR AGNOSTIC TENSOR COMPLETION  5. Conclusion  Our results demonstrate the power of the sum-of-squares hierarchy for agnostic statistical learning, and show for the first time that sum-of-squares algorithms can obtain fast rates for prediction. We hope our work will serve as a starting point for applying sum-of-squares to obtain polynomial time algorithms with fast rates in statistical learning for broader classes of models.  A few immediate technical questions emerge. Can the dependence on rank in our results be improved? Can the subgradient results be extended to the general undercomplete or even overcomplete case? Can similar agnostic learning results be obtained with a more practical algorithm that does not rely on solving the full sum-of-squares SDP?  Acknowledgements We thank Sasha Rakhlin and Ankur Moitra for helpful discussions and thank Matthew J. Telgarsky for being a constant source of inspiration.  References  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In  Conference on Learning Theory, pages 417-445, 2016.  Boaz Barak and David Steurer. Sum-of-squares:proofs, beliefs, and algorithms. 2016. URL https:  //www.sumofsquares.org/public/lec-definitions-general.html.  R\u00b4emi Bardenet, Odalric-Ambrym Maillard, et al. Concentration inequalities for sampling without  replacement. Bernoulli, 21(3):1361-1385, 2015.  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Peter L Bartlett, Shahar Mendelson, and Joseph Neeman. (cid:96)1-regularized linear regression: persistence  and oracle inequalities. Probability theory and related fields, 154(1-2):193-224, 2012.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational mathematics, 9(6):717, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix  completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 310-321. Springer, 2004.  Amit Daniely. Complexity theoretic limitations on learning halfspaces.  In Proceedings of the  forty-eighth annual ACM symposium on Theory of Computing, pages 105-117. ACM, 2016.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Shmuel Friedland and Lek-Heng Lim. Nuclear norm of higher-order tensors. Mathematics of  Computation, 87(311):1255-1281, 2018.  St\u00b4ephane Gaiffas and Guillaume Lecu\u00b4e. Sharp oracle inequalities for high-dimensional matrix  prediction. IEEE Transactions on Information Theory, 57(10):6942-6957, 2011.  David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions  on Information Theory, 57(3):1548-1566, 2011.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. The ellipsoid method and its consequences  in combinatorial optimization. Combinatorica, 1(2):169-197, 1981.  David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Information and Computation, 100(1):78-150, 1992.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via  sum-of-square proofs. In Conference on Learning Theory, pages 956-1006, 2015.  Prateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In Advances in  Neural Information Processing Systems, pages 1431-1439, 2014.  Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. Journal of Machine Learning Research, 11(Jul):2057-2078, 2010.  Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression.  Proceedings of The 31st Conference on Learning Theory, 2018.  Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):  455-500, 2009.  Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302-2329, 2011.  Pravesh K Kothari, Ryuhei Mori, Ryan O\u2019Donnell, and David Witmer. Sum of squares lower bounds for refuting any csp. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 132-145. ACM, 2017.  Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal  on Optimization, 11(3):796-817, 2001.  Guillaume Lecu\u00b4e and Shahar Mendelson. Learning subgaussian classes: Upper and minimax bounds.  arXiv preprint arXiv:1305.4825, 2013. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Guillaume Lecu\u00b4e and Shahar Mendelson. Regularization and the small-ball method ii: complexity dependent error rates. The Journal of Machine Learning Research, 18(1):5356-5403, 2017.  Guillaume Lecu\u00b4e and Shahar Mendelson. Regularization and the small-ball method i: sparse recovery.  The Annals of Statistics, 46(2):611-641, 2018.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag, New York,  1991.  Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localization through offset rademacher complexity. In Proceedings of The 28th Conference on Learning Theory, pages 1260-1285, 2015.  Shahar Mendelson. Learning without concentration. In Proceedings of The 27th Conference on  Learning Theory, pages 25-39, 2014.  Vitali D Milman and Gideon Schechtman. Asymptotic theory of finite dimensional normed spaces.  Springer-Verlag New York, Inc., 1986.  Andrea Montanari and Nike Sun. Spectral algorithms for tensor completion. Communications on  Pure and Applied Mathematics, 2016.  Cun Mu, Bo Huang, John Wright, and Donald Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In International Conference on Machine Learning, pages 73-81, 2014.  Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and  high-dimensional scaling. The Annals of Statistics, pages 1069-1097, 2011.  Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May): 1665-1697, 2012.  Sahand Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, 2012.  Ryan O\u2019Donnell and Yuan Zhou. Approximability and proof complexity. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1537-1556. Society for Industrial and Applied Mathematics, 2013.  Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness  and optimization. PhD thesis, California Institute of Technology, 2000.  Aaron Potechin and David Steurer. Exact tensor completion with sum-of-squares. Proceedings of  the 30th Annual Conference on Learning Theory, 2017.  Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random csps below the spectral threshold. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 121-131. ACM, 2017. FAST RATES FOR AGNOSTIC TENSOR COMPLETION  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12(Dec):3413-3430, 2011.  R Tyrrell Rockafellar. Convex Analysis, volume 28. Princeton University Press, 1970.  Bernardino Romera-Paredes and Massimiliano Pontil. A new convex relaxation for tensor completion.  In Advances in Neural Information Processing Systems, pages 2967-2975, 2013.  Naum Zuselevich Shor. An approach to obtaining global extremums in polynomial mathematical  programming problems. Cybernetics, 23(5):695-700, 1987.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference  on Computational Learning Theory, pages 545-560. Springer, 2005.  Ryota Tomioka and Taiji Suzuki. Convex tensor decomposition via structured schatten norm regularization. In Advances in neural information processing systems, pages 1331-1339, 2013.  Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex tensor decomposition. In Advances in Neural Information Processing Systems, pages 972-980, 2011.  Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series  in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.  G Alistair Watson. Characterization of the subdifferential of some matrix norms. Linear algebra and  its applications, 170:33-45, 1992.  Dong Xia, Ming Yuan, and Cun-Hui Zhang. Statistically optimal and computationally efficient low  rank tensor completion from noisy entries. arXiv preprint arXiv:1711.04934, 2017.  Ming Yuan and Cun-Hui Zhang. On tensor completion via nuclear norm minimization. Foundations  of Computational Mathematics, 16(4):1031-1068, 2016.  "}, "The Complexity of Making the Gradient Small in Stochastic Convex Optimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The Complexity of Making the Gradient Small in Stochastic Convex Optimization", "abstract": "We give nearly matching upper and lower bounds on the oracle complexity of finding $\\epsilon$-stationary points $(\\|\\nabla F(x)\\|\\leq\\epsilon$ in stochastic convex optimization. We jointly analyze the oracle complexity in both the local stochastic oracle model and the global oracle (or, statistical learning) model. This allows us to decompose the complexity of finding near-stationary points into optimization complexity and sample complexity, and reveals some surprising differences between the complexity of stochastic optimization versus learning. Notably, we show that in the global oracle/statistical learning model, only logarithmic dependence on smoothness is required to find a near-stationary point, whereas polynomial dependence on smoothness is necessary in the local stochastic oracle model. In other words, the separation in complexity between the two models can be exponential, and the folklore understanding that smoothness is required to find stationary points is only weakly true for statistical learning. Our upper bounds are based on extensions of a recent \u201crecursive regularization\u201d technique proposed by Allen-Zhu (2018). We show how to extend the technique to achieve near-optimal rates, and in particular show how to leverage the extra information available in the global oracle model. Our algorithm for the global model can be implemented efficiently through finite sum methods, and suggests an interesting new computational-statistical tradeoff.", "pdf_url": "http://proceedings.mlr.press/v99/foster19b/foster19b.pdf", "keywords": ["stationary point", "sample complexity", "oracle complexity", "stochastic optimization", "nonconvex optimization"], "reference": "Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1-9, 2009.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205. ACM, 2017.  Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1165-1175. 2018.  G\u00b4abor Braun, Crist\u00b4obal Guzm\u00b4an, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Transactions on Information Theory, 63(7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points i. arXiv preprint arXiv:1710.11606, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017b.  Damek Davis and Dmitriy Drusvyatskiy. Complexity of finding near-stationary points of convex  functions stochastically. arXiv preprint arXiv:1802.08556, 2018.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti- mization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 687-697, 2018.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information.  SIAM Journal on Computing, 23(5):1001-1018, 1994.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-  tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlin- ear and stochastic programming. Math. Program., 156(1-2):59-99, 2016. doi: 10.1007/ s10107-015-0871-8.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724-1732, 2017.  13   THE COMPLEXITY OF MAKING THE GRADIENT SMALL  References  Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1-9, 2009.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205. ACM, 2017.  Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. In Advances in Neural Information Processing Systems, pages 1165-1175. 2018.  G\u00b4abor Braun, Crist\u00b4obal Guzm\u00b4an, and Sebastian Pokutta. Lower bounds on the oracle complexity of nonsmooth convex optimization via information theory. IEEE Transactions on Information Theory, 63(7):4709-4724, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points i. arXiv preprint arXiv:1710.11606, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017b.  Damek Davis and Dmitriy Drusvyatskiy. Complexity of finding near-stationary points of convex  functions stochastically. arXiv preprint arXiv:1802.08556, 2018.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti- mization via stochastic path-integrated differential estimator. In Advances in Neural Information Processing Systems, pages 687-697, 2018.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information.  SIAM Journal on Computing, 23(5):1001-1018, 1994.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-  tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlin- ear and stochastic programming. Math. Program., 156(1-2):59-99, 2016. doi: 10.1007/ s10107-015-0871-8.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pages 1724-1732, 2017. THE COMPLEXITY OF MAKING THE GRADIENT SMALL  Richard M Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 881-890. Society for Industrial and Applied Mathematics, 2007.  Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2348-2358, 2017.  Arkadii Semenovich Nemirovski and David Borisovich Yudin. Problem complexity and method  efficiency in optimization. 1983.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course. 2004.  Yurii Nesterov. How to make the gradients small. Optima, 88:10-11, 2012.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00b4as P\u00b4ocz\u00b4os, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, pages 314-323, 2016.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex  optimization. In Conference on Learning Theory, 2009.  Joseph F Traub, Grzegorz W Wasilkowski, and Henryk Wo\u00b4zniakowski. Information-based complexity.  1988.  Blake Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives.  In Advances in Neural Information Processing Systems 29, pages 3639-3647. 2016.  Blake Woodworth, Jialei Wang, Brendan McMahan, and Nathan Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Advances in Neural Information Processing Systems 31, pages 8505-8515, 2018.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent for nonconvex optimization. In Advances in Neural Information Processing Systems 31, pages 3925-3936. 2018.  "}, "Statistical Learning with a Nuisance Component": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Statistical Learning with a Nuisance Component", "abstract": "We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a \u201cnuisance model\u201d). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates\u2014rates of the same order as if we knew the nuisance model\u2014are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.", "pdf_url": "http://proceedings.mlr.press/v99/foster19c/foster19c.pdf", "keywords": ["statistical learning", "double machine learning", "policy learning", "treatment effects", "neyman orthogonality", "local rademacher complexity"], "reference": "Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1-C68, 2018.  Jerzy Neyman. Optimal asymptotic tests of composite hypotheses. Probability and statsitics, pages  Jerzy Neyman. C (\u03b1) tests and their use. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages  213-234, 1959.  1-21, 1979.  Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax  regret and minimax risk. Bernoulli, 23(2):789-824, 2017.  Vladimir N Vapnik. The nature of statistical learning theory. Springer, 1995.  Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, pages 1564-1599, 1999.  3   STATISTICAL LEARNING WITH A NUISANCE COMPONENT  nuisance estimation error is of the order R2 Gn suffice when the target is parametric.  so, once again, n\u22121/4 RMSE rates for the nuisance  We give conditions on the relative complexity of the target and nuisance classes\u2014quantified via metric entropy\u2014under which the sample splitting meta-algorithm achieves oracle rates (assuming the two black-box estimation algorithms are appropriately instantiated). This allows us to extend several prior works beyond the parametric regime to complex nonparametric target classes. Our technical results extend the work of Yang and Barron (1999); Rakhlin et al. (2017), which provide minimax optimal rates without nuisance components and utilize the technique of aggregation in designing optimal algorithms. We also provide bounds for plug-in empirical risk minimization that extend the local Rademacher complexity analysis of generalization error (Bartlett et al., 2005) to account for the impact of the nuisance error.  References  Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1-C68, 2018.  Jerzy Neyman. Optimal asymptotic tests of composite hypotheses. Probability and statsitics, pages  Jerzy Neyman. C (\u03b1) tests and their use. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages  213-234, 1959.  1-21, 1979.  Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax  regret and minimax risk. Bernoulli, 23(2):789-824, 2017.  Vladimir N Vapnik. The nature of statistical learning theory. Springer, 1995.  Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, pages 1564-1599, 1999. "}, "On the Regret Minimization of Nonconvex Online Gradient Ascent for Online PCA": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "On the Regret Minimization of Nonconvex Online Gradient Ascent for Online PCA", "abstract": "In this paper we focus on the problem of Online Principal Component Analysis in the regret minimization framework. For this problem, all existing regret minimization algorithms for the fully-adversarial setting are based on a positive semidefinite convex relaxation, and hence require quadratic memory and SVD computation (either thin of full) on each iteration, which amounts to at least quadratic runtime per iteration. This is in stark contrast to a corresponding stochastic i.i.d. variant of the problem, which was studied extensively lately, and admits very efficient gradient ascent algorithms that work directly on the natural non-convex formulation of the problem, and hence require only linear memory and linear runtime per iteration. This raises the question: \\textit{can non-convex online gradient ascent algorithms be shown to minimize regret in online adversarial settings?} In this paper we take a step forward towards answering this question. We introduce an \\textit{adversarially-perturbed spiked-covariance model} in which, each data point is assumed to follow a fixed stochastic distribution with a non-zero spectral gap in the covariance matrix,  but is then perturbed with some adversarial vector. This model is a natural extension of a well studied standard \\textit{stochastic} setting that allows for non-stationary (adversarial) patterns to arise in the data  and hence, might serve as a significantly better approximation for real-world data-streams. We show that in an interesting regime of parameters, when the non-convex online gradient ascent algorithm is initialized with a \u201cwarm-start\" vector, it provably minimizes the regret with high probability. We further discuss the possibility of computing such a \u201cwarm-start\" vector, and also the use of regularization to obtain fast regret rates. Our theoretical findings are supported by empirical experiments on both synthetic and real-world data.", "pdf_url": "http://proceedings.mlr.press/v99/garber19a/garber19a.pdf", "keywords": ["online learning", "regret minimization", "online PCA", "online convex optimization"], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1195-1199, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055464. URL http://doi.acm.org/10.1145/3055399.3055464.  Zeyuan Allen-Zhu.  Natasha 2:  In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- Information Processing Systems 31, pages 2680- nett, URL http://papers.nips.cc/paper/ 2691. Curran Associates, 7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf.  Faster non-convex optimization than sgd.  editors, Advances in Neural  Inc., 2018.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap- free, and near-optimal rate. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 487-492. IEEE, 2017a.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- vectors and faster mmwu. In International Conference on Machine Learning, pages 116-125, 2017b.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 3174-3182, 2013.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6- 11 August 2017, pages 654-663, 2017. URL http://proceedings.mlr.press/v70/ carmon17a.html.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332-2341, 2015.  13   NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 1195-1199, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055464. URL http://doi.acm.org/10.1145/3055399.3055464.  Zeyuan Allen-Zhu.  Natasha 2:  In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- Information Processing Systems 31, pages 2680- nett, URL http://papers.nips.cc/paper/ 2691. Curran Associates, 7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf.  Faster non-convex optimization than sgd.  editors, Advances in Neural  Inc., 2018.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap- free, and near-optimal rate. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 487-492. IEEE, 2017a.  Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster online learning of eigen- vectors and faster mmwu. In International Conference on Machine Learning, pages 116-125, 2017b.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pages 584-592, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 3174-3182, 2013.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6- 11 August 2017, pages 654-663, 2017. URL http://proceedings.mlr.press/v70/ carmon17a.html.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332-2341, 2015. NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal bounds for privacy-preserving principal component analysis. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 11-20. ACM, 2014.  Dan Garber and Elad Hazan. Fast and simple pca via convex optimization.  arXiv preprint  arXiv:1509.05647, 2015.  568, 2015.  Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In ICML, pages 560-  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,  2(3-4):157-325, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012.  H. Hotelling. Analysis of a complex of statistical variables into principal components. J. Educ.  Psych., 24, 1933.  Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm. arXiv preprint arXiv:1602.06929, 2016.  Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix completion via non-convex stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4520-4528, 2016.  Ian Jolliffe. Principal component analysis.  In International encyclopedia of statistical science,  pages 1094-1096. Springer, 2011.  Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  Chris Junchi Li, Mengdi Wang, Han Liu, and Tong Zhang. Near-optimal stochastic approximation for online principal component estimation. Mathematical Programming, 167(1):75-97, 2018.  Teodor Vanislavov Marinov, Poorya Mianjy, and Raman Arora. Streaming principal component analysis in noisy setting. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3413-3422, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. NONCONVEX ONLINE GRADIENT ASCENT FOR ONLINE PCA  Poorya Mianjy and Raman Arora. Stochastic PCA with (cid:96)2 and (cid:96)1 regularization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3531-3539, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/ v80/mianjy18a.html.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming pca. In  Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  Jiazhong Nie, Wojciech Kotlowski, and Manfred K. Warmuth. Online PCA with optimal regrets. In  24th International Conference on Algorithmic Learning Theory, ALT, 2013.  K. Pearson. On lines and planes of closest fit to systems of points in space. Philosophical Magazine,  2(6):559-572, 1901.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012.  Ohad Shamir. Convergence of stochastic gradient descent for PCA:. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19- 24, 2016, pages 257-265, 2016.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. In 19th Annual Conference  on Learning Theory, COLT, 2006a.  Manfred K. Warmuth and Dima Kuzmin. Randomized PCA algorithms with regret bounds that are logarithmic in the dimension. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, NIPS, 2006b.  Peng Xu, Bryan He, Christopher De Sa, Ioannis Mitliagkas, and Chris Re. Accelerated stochastic power iteration. In International Conference on Artificial Intelligence and Statistics, pages 58-67, 2018.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003.  "}, "Optimal Tensor Methods in Smooth Convex and Uniformly ConvexOptimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Optimal Tensor Methods in Smooth Convex and Uniformly ConvexOptimization", "abstract": "We consider convex optimization problems with the objective function having Lipshitz-continuous $p$-th order derivative, where $p\\geq 1$. We propose a new tensor method, which closes the gap between the lower  $\\Omega\\left(\\e^{-\\frac{2}{3p+1}} \\right)$ and upper $O\\left(\\e^{-\\frac{1}{p+1}} \\right)$ iteration complexity bounds for this class of optimization problems. We also consider uniformly convex functions, and show how the proposed method can be accelerated under this additional assumption. Moreover, we introduce a $p$-th order condition number which naturally arises in the complexity analysis of tensor methods under this assumption.  Finally, we make a numerical study of the proposed optimal method and show that in practice it is faster than the best known accelerated tensor method. We also compare the performance of tensor methods for $p=2$ and $p=3$ and show that the 3rd-order method is superior to the 2nd-order method in practice.", "pdf_url": "http://proceedings.mlr.press/v99/gasnikov19a/gasnikov19a.pdf", "keywords": ["Convex optimization", "unconstrained minimization", "tensor methods", "worst-case complexity", "global complexity bounds", "condition number"], "reference": "Naman Agarwal and Elad Hazan. Lower bounds for higher-order convex optimization. In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 774\u2013792. PMLR, 06\u201309 Jul 2018. URL http://proceedings.mlr.press/v75/agarwal18a. html.  12  (Nesterov,2018)Algorithm110110210310400.20.40.60.8Iterationsf(xk)Syntheticn=10,d=10010110210310400.20.40.60.8Iterationsf(xk)Syntheticn=100,d=100010110210310400.20.40.60.8Iterationsf(xk)mushroomdataset1011021030.40.60.8Iterationsf(xk)a9adataset OPTIMAL TENSOR METHODS FOR SMOOTH CONVEX OPTIMIZATION  (a)  (b)  (c)  (d)  Figure 2: A performance comparison for the non-regularized logistic regression problem between the accelerated tensor method from Nesterov (2018a) and Algorithm 1. (a) Uses synthetic data with n = 10 and d = 100, (b) uses synthetic data with n = 100 and d = 1000, (c) uses the mushroom dataset (d = 8124 and n = 112) Dheeru and Karra Taniskidou (2017), and (d) uses the a9a dataset (d = 32561 and n = 123) Dheeru and Karra Taniskidou (2017).  d = 112) Dheeru and Karra Taniskidou (2017), and Figure 2(d) uses the a9a dataset (n = 32561 and d = 123) Dheeru and Karra Taniskidou (2017).  For the logistic regression problem, we don\u2019t have access to the optimal value function in gen- eral, thus, we plot only the cost function evaluated at the current iterate. As expected by the theo- retic results, Algorithm 1 requires one order of magnitude less iterations than the accelerated tensor method from Nesterov (2018a) to achieve the same function value.  In "}, "Near Optimal Methods for Minimizing Convex Functions with Lipschitz p-th Derivatives": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Near Optimal Methods for Minimizing Convex Functions with Lipschitz p-th Derivatives", "abstract": "In this merged paper, we consider the problem of minimizing a convex function with Lipschitz-continuous $p$-th order derivatives. Given an oracle which when queried at a point returns the first $p$-derivatives of the function at that point we provide some methods which compute an $\\e$ approximate minimizer in $O\\left(\\e^{-\\frac{2}{3p+1}} \\right)$ iterations. These methods match known lower bounds up to polylogarithmic factors for constant $p$.", "pdf_url": "http://proceedings.mlr.press/v99/gasnikov19b/gasnikov19b.pdf", "keywords": [], "reference": "S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Near-optimal method  for highly smooth convex optimization. arXiv:1812.08026, 2018.  Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Se- likhanovych, and C\u00b4esar A. Uribe. Optimal tensor methods in smooth convex and uniformly convex optimization. arXiv:1809.00382, 2018.  Bo Jiang, Haoyue Wang, and Shuzhong Zhang. An optimal high-order tensor method for convex  optimization. arXiv:1812.06557v2, 2018.  2   NEAR OPTIMAL METHODS FOR MINIMIZING CONVEX FUNCTIONS WITH LIPSCHITZ p-TH DERIVATIVES  Abstract In this merged paper, we consider the problem of minimizing a convex function with Lipschitz- continuous p-th order derivatives. Given an oracle which when queried at a point returns the first p-derivatives of the function at that point we provide some methods which compute an \u03b5 approxi- mate minimizer in O iterations. These methods match known lower bounds up to poly- logarithmic factors for constant p.  \u03b5\u2212 2  3p+1  (cid:16)  (cid:17)  See Gasnikov et al. (2018); Jiang et al. (2018); Bubeck et al. (2018) for details.  1. Results  Acknowledgments  The authors are grateful to Yurii Nesterov for fruitful discussions. The work of A. Gasnikov was supported by RFBR 18-29-03071 mk and was prepared within the framework of the HSE University Basic Research Program and funded by the Russian Academic Excellence Project \u20195-100\u2019. The work of P. Dvurechensky and E. Vorontsova was supported by RFBR 18-31-20005 mol-a-ved. The work of E. Gorbunov was supported by the grant of Russian\u2019s President MD-1320.2018.1. The work of Bo Jiang was supported by NSFC grant 11771269. The work of Yin Tat Lee was supported by NSF Awards CCF-1740551, CCF-1749609, and DMS-1839116. The work of Aaron Sidford was supported by NSF CAREER Award CCF-1844855.  References  S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Near-optimal method  for highly smooth convex optimization. arXiv:1812.08026, 2018.  Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Se- likhanovych, and C\u00b4esar A. Uribe. Optimal tensor methods in smooth convex and uniformly convex optimization. arXiv:1809.00382, 2018.  Bo Jiang, Haoyue Wang, and Shuzhong Zhang. An optimal high-order tensor method for convex  optimization. arXiv:1812.06557v2, 2018. "}, "Stabilized SVRG: Simple Variance Reduction for Nonconvex Optimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Stabilized SVRG: Simple Variance Reduction for Nonconvex Optimization", "abstract": "Variance reduction techniques like SVRG provide simple and fast algorithms for optimizing a convex finite-sum objective. For nonconvex objectives, these techniques can also find a first-order stationary point (with small gradient). However, in nonconvex optimization it is often crucial to find a second-order stationary point (with small gradient and almost PSD hessian). In this paper, we show that Stabilized SVRG (a simple variant of SVRG) can find an $\\epsilon$-second-order stationary point using only $\\widetilde{O}(n^{2/3}/\\epsilon^2+n/\\epsilon^{1.5})$ stochastic gradients. To our best knowledge, this is the first second-order guarantee for a simple variant of SVRG. The running time almost matches the known guarantees for finding $\\epsilon$-first-order stationary points.", "pdf_url": "http://proceedings.mlr.press/v99/ge19a/ge19a.pdf", "keywords": ["nonconvex optimization", "saddle point", "variance reduction"], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arX-  iv:1708.08694, 2017.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zhi-Dong Bai and Yong-Qua Yin. Necessary and sufficient conditions for almost sure convergence of the largest eigenvalue of a wigner matrix. The Annals of Probability, pages 1729-1741, 1988.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57 (4):2342-2359, 2011.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646-1654, 2014.  Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067-1077, 2017.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Infor- mation Processing Systems, pages 687-697, 2018.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a.  13   STABILIZED SVRG  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint arX-  iv:1708.08694, 2017.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  International Conference on Machine Learning, pages 699-707, 2016.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zhi-Dong Bai and Yong-Qua Yin. Necessary and sufficient conditions for almost sure convergence of the largest eigenvalue of a wigner matrix. The Annals of Probability, pages 1729-1741, 1988.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57 (4):2342-2359, 2011.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in neural information processing systems, pages 1646-1654, 2014.  Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067-1077, 2017.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Infor- mation Processing Systems, pages 687-697, 2018.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017a. STABILIZED SVRG  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017b.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017a.  Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle  points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance  reduction. In Advances in neural information processing systems, pages 315-323, 2013.  Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345-2355, 2017.  Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex  optimization. arXiv preprint arXiv:1802.04477, 2018.  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pages 314-323, 2016.  Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an expo- nential convergence rate for finite training sets. In Advances in neural information processing systems, pages 2663-2671, 2012.  Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized  loss minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.  Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.  Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic regularization for fast nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2904-2913, 2018.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Yi Xu, Jing Rong, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. In Advances in Neural Information Processing Systems, pages 5535- 5545, 2018.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Finding local minima via stochastic nested variance  reduction. arXiv preprint arXiv:1806.08782, 2018a.  Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex  optimization. arXiv preprint arXiv:1806.07811, 2018b. STABILIZED SVRG  "}, "Learning Ising Models with Independent Failures": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning Ising Models with Independent Failures", "abstract": "We give the first efficient algorithm for learning the structure of an Ising model that tolerates independent failures; that is, each entry of the observed sample is missing with some unknown probability $p$. Our algorithm matches the essentially optimal runtime and sample complexity bounds of recent work for learning Ising models due to Klivans and Meka (2017). We devise a novel unbiased estimator for the gradient of the Interaction Screening Objective (ISO) due to Vuffray et al. (2016) and apply a stochastic multiplicative gradient descent algorithm to minimize this objective. Solutions to this minimization recover the neighborhood information of the underlying Ising model on a node by node basis.", "pdf_url": "http://proceedings.mlr.press/v99/goel19a/goel19a.pdf", "keywords": ["graphical models", "Ising models", "structure learning", "efficient algorithms", "missing data", "unbiased estimators"], "reference": "Guy Bresler. Efficiently learning ising models on arbitrary graphs. In Proceedings of the forty-  seventh annual ACM symposium on Theory of computing, pages 771-782. ACM, 2015.  Guy Bresler, Elchanan Mossel, and Allan Sly. Reconstruction of markov random fields from sam- ples: Some observations and algorithms. In Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 343-356. Springer, 2008.  Guy Bresler, David Gamarnik, and Devavrat Shah. Hardness of parameter estimation in graphical  models. In Advances in Neural Information Processing Systems, pages 1062-1070, 2014.  Yuxin Chen. Learning sparse ising models with missing data.  Myung Jin Choi, Joseph J Lim, Antonio Torralba, and Alan S Willsky. Exploiting hierarchical  context on a large database of object categories. 2010.  C Chow and Cong Liu. Approximating discrete probability distributions with dependence trees.  IEEE transactions on Information Theory, 14(3):462-467, 1968.  Sanjoy Dasgupta. Learning polytrees. In Proceedings of the Fifteenth conference on Uncertainty in  artificial intelligence, pages 134-141. Morgan Kaufmann Publishers Inc., 1999.  Linus Hamilton, Frederic Koehler, and Ankur Moitra. Information theoretic properties of markov random fields, and their algorithmic applications. In Advances in Neural Information Processing Systems, pages 2463-2472, 2017.  Ariel Jaimovich, Gal Elidan, Hanah Margalit, and Nir Friedman. Towards an integrated protein- protein interaction network: A relational markov network approach. Journal of Computational Biology, 13(2):145-164, 2006.  Sham Kakade, Dean Foster, and Eyal Even-Dar. (exponentiated) stochastic gradient descent for l1  constrained problems, 2008.  Adam Klivans and Raghu Meka. Learning graphical models using multiplicative weights. In Foun- dations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 343-354. IEEE, 2017.  Daphne Koller, Nir Friedman, and Francis Bach. Probabilistic graphical models: principles and  techniques. MIT press, 2009.  Su-In Lee, Varun Ganapathi, and Daphne Koller. Efficient structure learning of markov networks using l 1-regularization. In Advances in neural Information processing systems, pages 817-824, 2007.  Erik M Lindgren, Vatsal Shah, Yanyao Shen, Alexandros G. Dimakis, and Adam Klivans. On robust  learning of ising models. 2018.  13   LEARNING ISING MODELS WITH INDEPENDENT FAILURES  Acknowledgments  We thank Shanshan Wu for useful discussions.  References  Guy Bresler. Efficiently learning ising models on arbitrary graphs. In Proceedings of the forty-  seventh annual ACM symposium on Theory of computing, pages 771-782. ACM, 2015.  Guy Bresler, Elchanan Mossel, and Allan Sly. Reconstruction of markov random fields from sam- ples: Some observations and algorithms. In Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 343-356. Springer, 2008.  Guy Bresler, David Gamarnik, and Devavrat Shah. Hardness of parameter estimation in graphical  models. In Advances in Neural Information Processing Systems, pages 1062-1070, 2014.  Yuxin Chen. Learning sparse ising models with missing data.  Myung Jin Choi, Joseph J Lim, Antonio Torralba, and Alan S Willsky. Exploiting hierarchical  context on a large database of object categories. 2010.  C Chow and Cong Liu. Approximating discrete probability distributions with dependence trees.  IEEE transactions on Information Theory, 14(3):462-467, 1968.  Sanjoy Dasgupta. Learning polytrees. In Proceedings of the Fifteenth conference on Uncertainty in  artificial intelligence, pages 134-141. Morgan Kaufmann Publishers Inc., 1999.  Linus Hamilton, Frederic Koehler, and Ankur Moitra. Information theoretic properties of markov random fields, and their algorithmic applications. In Advances in Neural Information Processing Systems, pages 2463-2472, 2017.  Ariel Jaimovich, Gal Elidan, Hanah Margalit, and Nir Friedman. Towards an integrated protein- protein interaction network: A relational markov network approach. Journal of Computational Biology, 13(2):145-164, 2006.  Sham Kakade, Dean Foster, and Eyal Even-Dar. (exponentiated) stochastic gradient descent for l1  constrained problems, 2008.  Adam Klivans and Raghu Meka. Learning graphical models using multiplicative weights. In Foun- dations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 343-354. IEEE, 2017.  Daphne Koller, Nir Friedman, and Francis Bach. Probabilistic graphical models: principles and  techniques. MIT press, 2009.  Su-In Lee, Varun Ganapathi, and Daphne Koller. Efficient structure learning of markov networks using l 1-regularization. In Advances in neural Information processing systems, pages 817-824, 2007.  Erik M Lindgren, Vatsal Shah, Yanyao Shen, Alexandros G. Dimakis, and Adam Klivans. On robust  learning of ising models. 2018. LEARNING ISING MODELS WITH INDEPENDENT FAILURES  Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pages 2726-2734, 2011.  Daniel Marbach, James C Costello, Robert K\u00a8uffner, Nicole M Vega, Robert J Prill, Diogo M Cama- cho, Kyle R Allison, Andrej Aderhold, Richard Bonneau, Yukun Chen, et al. Wisdom of crowds for robust gene network inference. Nature methods, 9(8):796, 2012.  Pradeep Ravikumar, Martin J Wainwright, John D Lafferty, et al. High-dimensional ising model selection using ?1-regularized logistic regression. The Annals of Statistics, 38(3):1287-1319, 2010.  Bero Roos. Binomial approximation to the poisson binomial distribution: The krawtchouk expan-  sion. Theory of Probability & Its Applications, 45(2):258-272, 2001.  Devavrat Shah and Dogyoon Song. Learning mixture model with missing values and its application  to rankings. arXiv preprint arXiv:1812.11917, 2018.  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities  and the closest pair problem. Journal of the ACM (JACM), 62(2):13, 2015.  Marc Vuffray, Sidhant Misra, Andrey Lokhov, and Michael Chertkov. Interaction screening: Effi- cient and sample-optimal learning of ising models. In Advances in Neural Information Processing Systems, pages 2595-2603, 2016.  Shanshan Wu, Sujay Sanghavi, and Alexandros G Dimakis. Sparse logistic regression learns all  discrete pairwise graphical models. arXiv preprint arXiv:1810.11905, 2018.  Eunho Yang, Genevera Allen, Zhandong Liu, and Pradeep K Ravikumar. Graphical models via generalized linear models. In Advances in Neural Information Processing Systems, pages 1358- 1366, 2012.  Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika,  94(1):19-35, 2007. LEARNING ISING MODELS WITH INDEPENDENT FAILURES  "}, "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time", "abstract": "We give a polynomial-time algorithm for learning neural networks with one layer of sigmoids feeding into any Lipschitz, monotone activation function (e.g., sigmoid or ReLU).  The algorithm succeeds with respect to {\\em any} distribution on the unit ball in $n$ dimensions (hidden weight vectors in the first layer have unit norm).  This is the first efficient algorithm for learning a general class of neural networks with more than one nonlinear layer that makes no restrictions on the VC-dimension of the network. Algorithms for learning relaxations of our model (e.g., allowing larger weight vectors in the first layer) would lead to breakthroughs on notoriously hard problems in Boolean function learning.  Thus, our results are \u201cbest possible\u201d with respect to current techniques. Our algorithm\u2013 {\\em Alphatron}\u2013 is an iterative update rule that combines isotonic regression with kernel methods.  We use this algorithm to give a simple reduction for translating PAC learning algorithms to the more general, real-valued setting of {\\em probabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d. noise-tolerance.  This substantially improves many longstanding results for PAC learning Boolean functions.", "pdf_url": "http://proceedings.mlr.press/v99/goel19b/goel19b.pdf", "keywords": ["neural networks", "efficient algorithm", "isotonic regression", "kernel methods", "probabilistic concepts"], "reference": "Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artifi-  cial Intelligence, 201:81-105, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum and Adam Kalai. A note on learning from multiple-instance examples. Machine  Learning, 30(1):23-29, 1998.  Mathematics, 3(2):168-177, 1990.  Jehoshua Bruck. Harmonic analysis of polynomial threshold functions. SIAM Journal on Discrete  Mahdi Cheraghchi, Adam R. Klivans, Pravesh Kothari, and Homin K. Lee. Submodular functions  are noise stable. In Yuval Rabani, editor, SODA, pages 1586-1592. SIAM, 2012.  Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely. A ptas for agnostically learning halfspaces.  In Conference on Learning Theory,  pages 484-502, 2015.  Amit Daniely. Sgd learns the conjugate kernel class of the network.  In Isabelle Guyon, Ul- rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2419-2427, 2017. URL http://papers.nips.cc/paper/ 6836-sgd-learns-the-conjugate-kernel-class-of-the-network.  Thomas G Dietterich, Richard H Lathrop, and Tom\u00b4as Lozano-P\u00b4erez. Solving the multiple instance  problem with axis-parallel rectangles. Artificial intelligence, 89(1):31-71, 1997.  Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de- scent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.  Vitaly Feldman. Learning dnf expressions from fourier spectrum. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT, volume 23 of JMLR Proceedings, pages 17.1-17.19. JMLR.org, 2012. URL http://jmlr.org/proceedings/papers/v23/.  Vitaly Feldman. Statistical query learning. In Encyclopedia of Algorithms, pages 2090-2095. 2016.  Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one- hidden-layer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  13   LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  References  Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artifi-  cial Intelligence, 201:81-105, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum and Adam Kalai. A note on learning from multiple-instance examples. Machine  Learning, 30(1):23-29, 1998.  Mathematics, 3(2):168-177, 1990.  Jehoshua Bruck. Harmonic analysis of polynomial threshold functions. SIAM Journal on Discrete  Mahdi Cheraghchi, Adam R. Klivans, Pravesh Kothari, and Homin K. Lee. Submodular functions  are noise stable. In Yuval Rabani, editor, SODA, pages 1586-1592. SIAM, 2012.  Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural  information processing systems, pages 342-350, 2009.  Amit Daniely. A ptas for agnostically learning halfspaces.  In Conference on Learning Theory,  pages 484-502, 2015.  Amit Daniely. Sgd learns the conjugate kernel class of the network.  In Isabelle Guyon, Ul- rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2419-2427, 2017. URL http://papers.nips.cc/paper/ 6836-sgd-learns-the-conjugate-kernel-class-of-the-network.  Thomas G Dietterich, Richard H Lathrop, and Tom\u00b4as Lozano-P\u00b4erez. Solving the multiple instance  problem with axis-parallel rectangles. Artificial intelligence, 89(1):31-71, 1997.  Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de- scent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.  Vitaly Feldman. Learning dnf expressions from fourier spectrum. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT, volume 23 of JMLR Proceedings, pages 17.1-17.19. JMLR.org, 2012. URL http://jmlr.org/proceedings/papers/v23/.  Vitaly Feldman. Statistical query learning. In Encyclopedia of Algorithms, pages 2090-2095. 2016.  Weihao Gao, Ashok Vardhan Makkuva, Sewoong Oh, and Pramod Viswanath. Learning one- hidden-layer neural networks under general input distributions. arXiv preprint arXiv:1810.04133, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with  symmetric inputs. arxiv preprint arxiv:1810.06793, 2018.  Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability of neural  networks. In NIPS, 2017.  Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-  mial time. arXiv preprint arXiv:1611.10258, 2016.  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping  patches. arXiv preprint arXiv:1802.02547, 2018.  Parikshit Gopalan, Adam Tauman Kalai, and Adam R Klivans. Agnostically learning decision trees. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 527-536. ACM, 2008.  Francisco Herrera, Sebasti\u00b4an Ventura, Rafael Bello, Chris Cornelis, Amelia Zafra, D\u00b4anel S\u00b4anchez- Tarrag\u00b4o, and Sarah Vluymans. Multiple instance learning. In Multiple Instance Learning, pages 17-33. Springer, 2016.  Jeffrey C. Jackson. An efficient membership-query algorithm for learning dnf with respect to the  uniform distribution. J. Comput. Syst. Sci, 55(3):414-440, 1997.  Jeffrey C. Jackson, Adam R. Klivans, and Rocco A. Servedio. Learnability beyond AC\u02c60. In Pro- ceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC-02), pages 776- 784, New York, May 19-21 2002. ACM Press.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793-800, 2009.  of  Sham M. Kakade, Adam Kalai, Varun Kanade, and Ohad Shamir. and 2011.  ing In NIPS, advances-in-neural-information-processing-systems-24-2011.  learn- index models with regression. URL http://papers.nips.cc/book/  generalized pages  linear 927-935,  Efficient  isotonic  single  Adam Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009. URL http://www.cs.mcgill.ca/\u02dccolt2009/papers/001.pdf# page=1.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically  learning halfspaces. SIAM J. Comput., 37(6):1777-1805, 2008.  Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic con- In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on,  cepts. pages 382-391. IEEE, 1990. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  A. Klivans, R. O\u2019Donnell, and R. Servedio. Learning intersections and thresholds of halfspaces.  JCSS: Journal of Computer and System Sciences, 68, 2004.  Adam R. Klivans and Raghu Meka. Moment-matching polynomials. Electronic Colloquium on Computational Complexity (ECCC), 20:8, 2013. URL http://eccc.hpi-web.de/ report/2013/008.  Adam R Klivans and Rocco A Servedio. Learning intersections of halfspaces with a margin. Journal  of Computer and System Sciences, 74(1):35-48, 2008.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci, 75(1):2-12, 2009. URL http://dx.doi.org/10.1016/ j.jcss.2008.07.008.  Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the fourier spectrum. SIAM  Journal on Computing, 22(6):1331-1348, 1993.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Lee, Bartlett, and Williamson. Lower bounds on the VC-dimension of smoothly parametrized function classes. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 1994.  Linial, Mansour, and Nisan. Constant depth circuits, fourier transform, and learnability. JACM:  Journal of the ACM, 40, 1993.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014.  Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.  In Advances in neural information processing systems, pages 2627-2635, 2014.  Mossel, O\u2019Donnell, and Servedio. Learning juntas.  In STOC: ACM Symposium on Theory of  Computing (STOC), 2003.  Anirbit Mukherjee and Amitabh Basu. Lower bounds over boolean inputs for deep neural networks  with relu gates. arXiv preprint arXiv:1711.03073, 2017.  Ramamohan Paturi. On the degree of polynomials that approximate symmetric Boolean functions In Proceedings of the Twenty-Fourth Annual ACM Symposium on the  (preliminary version). Theory of Computing, pages 468-474, Victoria, British Columbia, Canada, 4-6 May 1992.  Sivan Sabato and Naftali Tishby. Multi-instance learning with any hypothesis class. Journal of  Machine Learning Research, 13(Oct):2999-3039, 2012.  Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non- linear functions. CoRR, abs/1610.09887, 2016. URL http://arxiv.org/abs/1610. 09887. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, and Aurelio Uncini. Kafnets: kernel- based non-parametric activation functions for neural networks. arXiv preprint arXiv:1707.04035, 2017.  Bernhard Sch\u00a8olkopf and Alexander J Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT press, 2002.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with  the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, 2011.  Ohad Shamir. Distribution-specific hardness of learning neural networks.  arXiv preprint  arXiv:1609.01037, 2016.  Alexander A Sherstov. Making polynomials robust to noise.  In Proceedings of the forty-fourth  annual ACM symposium on Theory of computing, pages 747-758. ACM, 2012.  Alex Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00a8olkopf. A hilbert space embedding for dis- tributions. In International Conference on Algorithmic Learning Theory, pages 13-31. Springer, 2007.  Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591,  2017.  Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural net-  works. arXiv preprint arXiv:1707.04615, 2017.  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities  and the closest pair problem. Journal of the ACM (JACM), 62(2):13, 2015.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Wikipedia. Multinomial theorem \u2014 Wikipedia, the free encyclopedia, 2016. [Online; accessed  30-October-2016].  Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva. Electron-proton dynamics in deep learning.  CoRR, abs/1702.00458, 2017. URL http://arxiv.org/abs/1702.00458.  Yuchen Zhang, Jason Lee, and Michael Jordan. \u21131 networks are improperly learnable in polynomial-  time. In ICML, 2016.  Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In ICML, volume 70, pages 4140-4149. JMLR.org, 2017. URL http://jmlr.org/proceedings/papers/v70/. LEARNING NEURAL NETWORKS WITH TWO NONLINEAR LAYERS IN POLYNOMIAL TIME  "}, "When can unlabeled data improve the learning rate?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "When can unlabeled data improve the learning rate?", "abstract": "In semi-supervised classification, one is given access both to labeled and unlabeled data. As unlabeled data is typically cheaper to acquire than labeled data, this setup becomes advantageous as soon as one can exploit the unlabeled data in order to produce a better classifier than with labeled data alone. However, the conditions under which such an improvement is possible are not fully understood yet. Our analysis focuses on improvements in the \\emph{minimax} learning rate in terms of the number of labeled examples (with the number of unlabeled examples being allowed to depend on the number of labeled ones). We argue that for such improvements to be realistic and indisputable, certain specific conditions should be satisfied and previous analyses have failed to meet those conditions. We then demonstrate examples where these conditions can be met, in particular showing rate changes from $1/\\sqrt{\\ell}$ to $e^{-c\\ell}$ and from $1/\\sqrt{\\ell}$ to $1/\\ell$. These results improve our understanding of what is and isn\u2019t possible in semi-supervised learning.", "pdf_url": "http://proceedings.mlr.press/v99/gopfert19a/gopfert19a.pdf", "keywords": ["semi-supervised learning", "learning rates", "minimax analysis", "unlabeled data"], "reference": "Martin Azizyan, Aarti Singh, and Larry Wasserman. Density-sensitive semisupervised in-  ference. The Annals of Statistics, 41(2):751-771, 2013.  Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learn-  ing. Journal of the ACM (JACM), 57(3):19, 2010.  Shai Ben-David, Tyler Lu, and D\u00b4avid P\u00b4al. Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning. In COLT, pages 33-44, 2008.  Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern  Recognition Letters, 16(1):105-111, 1995.  Malte Darnst\u00a8adt. An investigation on the power of unlabeled data. Doctoral thesis, Ruhr-  Universit\u00a8at Bochum, 2015.  Malte Darnst\u00a8adt, Hans Ulrich Simon, and Bal\u00b4azs Sz\u00a8or\u00b4enyi. Unlabeled data does provably In LIPIcs-Leibniz International Proceedings in Informatics, volume 20. Schloss  help. Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.  Amir Globerson, Roi Livni, and Shai Shalev-Shwartz. E\ufb00ective semisupervised learning on  manifolds. In COLT, pages 978-1003, 2017.  Alexander Golovnev, D\u00b4avid P\u00b4al, and Bal\u00b4azs Sz\u00a8or\u00b4enyi. The information-theoretic value of  unlabeled data in semi-supervised learning. arXiv preprint arXiv:1901.05515, 2019.  Matti K\u00a8a\u00a8ari\u00a8ainen. Generalization error bounds using unlabeled data.  In COLT, pages  127-142, 2005.  versity of Waterloo, 2009.  Tyler Tian Lu. Fundamental limitations of semi-supervised learning. Doctoral thesis, Uni-  Partha Niyogi. Manifold regularization and semi-supervised learning: some theoretical  analyses. Journal of Machine Learning Research, 14(1):1229-1250, 2013.  Joel Ratsaby and Santosh S Venkatesh. Learning from a mixture of labeled and unlabeled  examples with parametric side information. In COLT, pages 412-417, 1995.  Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption. Journal of Machine Learning Research, 8(Jul):1369-1392, 2007.  Matthias W. Seeger. Input-dependent regularization of conditional density models. 2000.  Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn\u2019t.  In NIPS, pages 1513-1520, 2009.  Alexandre B. Tsybakov.  Introduction to Nonparametric Estimation. Springer series in  statistics. Springer, 2009.  13   Faster learning with unlabeled data  References  Martin Azizyan, Aarti Singh, and Larry Wasserman. Density-sensitive semisupervised in-  ference. The Annals of Statistics, 41(2):751-771, 2013.  Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learn-  ing. Journal of the ACM (JACM), 57(3):19, 2010.  Shai Ben-David, Tyler Lu, and D\u00b4avid P\u00b4al. Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning. In COLT, pages 33-44, 2008.  Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern  Recognition Letters, 16(1):105-111, 1995.  Malte Darnst\u00a8adt. An investigation on the power of unlabeled data. Doctoral thesis, Ruhr-  Universit\u00a8at Bochum, 2015.  Malte Darnst\u00a8adt, Hans Ulrich Simon, and Bal\u00b4azs Sz\u00a8or\u00b4enyi. Unlabeled data does provably In LIPIcs-Leibniz International Proceedings in Informatics, volume 20. Schloss  help. Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.  Amir Globerson, Roi Livni, and Shai Shalev-Shwartz. E\ufb00ective semisupervised learning on  manifolds. In COLT, pages 978-1003, 2017.  Alexander Golovnev, D\u00b4avid P\u00b4al, and Bal\u00b4azs Sz\u00a8or\u00b4enyi. The information-theoretic value of  unlabeled data in semi-supervised learning. arXiv preprint arXiv:1901.05515, 2019.  Matti K\u00a8a\u00a8ari\u00a8ainen. Generalization error bounds using unlabeled data.  In COLT, pages  127-142, 2005.  versity of Waterloo, 2009.  Tyler Tian Lu. Fundamental limitations of semi-supervised learning. Doctoral thesis, Uni-  Partha Niyogi. Manifold regularization and semi-supervised learning: some theoretical  analyses. Journal of Machine Learning Research, 14(1):1229-1250, 2013.  Joel Ratsaby and Santosh S Venkatesh. Learning from a mixture of labeled and unlabeled  examples with parametric side information. In COLT, pages 412-417, 1995.  Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption. Journal of Machine Learning Research, 8(Jul):1369-1392, 2007.  Matthias W. Seeger. Input-dependent regularization of conditional density models. 2000.  Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn\u2019t.  In NIPS, pages 1513-1520, 2009.  Alexandre B. Tsybakov.  Introduction to Nonparametric Estimation. Springer series in  statistics. Springer, 2009. Faster learning with unlabeled data  Ruth Urner and Shai Ben-David. Probabilistic lipschitzness a niceness assumption for  deterministic labels. In Learning Faster from Easy Data-Workshop@ NIPS, 2013.  Ruth Urner, Shai Shalev-Shwartz, and Shai Ben-David. Access to unlabeled data can speed  up prediction time. In ICML, pages 641-648, 2011.  "}, "Sampling and Optimization on Convex Sets in Riemannian Manifolds of Non-Negative Curvature": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sampling and Optimization on Convex Sets in Riemannian Manifolds of Non-Negative Curvature", "abstract": "The Euclidean space notion of convex sets (and functions) generalizes to Riemannian manifolds in a natural sense and is called geodesic convexity. Extensively studied computational problems such as convex optimization and sampling in convex sets also have meaningful counterparts in the manifold setting. Geodesically convex optimization is a well-studied problem with ongoing research and considerable recent interest in machine learning and theoretical computer science. In this paper, we study sampling and convex optimization problems over manifolds of non-negative curvature proving polynomial running time in the dimension and other relevant parameters. Our algorithms assume a warm start. We first present a random walk based sampling algorithm and then combine it with simulated annealing for solving convex optimization problems. To our knowledge, these are the first algorithms in the general setting of positively curved manifolds with provable polynomial guarantees under reasonable assumptions, and the first study of the connection between sampling and optimization in this setting.", "pdf_url": "http://proceedings.mlr.press/v99/goyal19a/goyal19a.pdf", "keywords": ["Riemannian Manifolds", "Geodesic Convex Optimization", "Simulated Annealing"], "reference": "Jacob Abernethy and Elad Hazan. Faster convex optimization: Simulated annealing with an ef- ficient universal barrier. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pages 2520-2528. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045656.  P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3. doi: 10.1515/9781400830244. URL https://doi.org/10.1515/9781400830244. With a foreword by Paul Van Dooren.  Zeyuan Allen-Zhu, Ankit Garg, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Operator scaling via geodesically convex optimization, invariant theory and polynomial identity testing. In Pro- ceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 172-181. ACM, 2018.  Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Bakry- \u00b4Emery curvature-dimension condition and Riemannian Ricci curvature bounds. Ann. Probab., 43(1):339-404, 2015. ISSN 0091-1798. doi: 10.1214/14-AOP907. URL https://doi.org/10.1214/14-AOP907.  Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffu- sion operators, volume 348 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer, Cham, 2014. ISBN 978-3-319-00226-2; 978- 3-319-00227-9. doi: 10.1007/978-3-319-00227-9. URL https://doi.org/10.1007/ 978-3-319-00227-9.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. Journal of  the ACM (JACM), 51(4):540-556, 2004.  Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint  arXiv:1701.02434, 2017.  Isaac Chavel. Riemannian geometry, volume 98 of Cambridge Studies in Advanced Mathemat- ics. Cambridge University Press, Cambridge, second edition, 2006. ISBN 978-0-521-61954-7; 0-521-61954-8. doi: 10.1017/CBO9780511616822. URL https://doi.org/10.1017/ CBO9780511616822. A modern introduction.  Jeff Cheeger and Detlef Gromoll. On the structure of complete manifolds of nonnegative curvature. Ann. of Math. (2), 96:413-443, 1972. ISSN 0003-486X. doi: 10.2307/1970819. URL https: //doi.org/10.2307/1970819.  Dario Cordero-Erausquin, Robert J. McCann, and Michael Schmuckenschl\u00a8ager. Pr\u00b4ekopa-Leindler type inequalities on Riemannian manifolds, Jacobi fields, and optimal transport. Ann. Fac. Sci. Toulouse Math. (6), 15(4):613-635, 2006. ISSN 0240-2963. URL http://afst.cedram. org/item?id=AFST_2006_6_15_4_613_0.  13   SAMPLING AND OPTIMIZATION ON MANIFOLDS  References  Jacob Abernethy and Elad Hazan. Faster convex optimization: Simulated annealing with an ef- ficient universal barrier. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pages 2520-2528. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045656.  P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3. doi: 10.1515/9781400830244. URL https://doi.org/10.1515/9781400830244. With a foreword by Paul Van Dooren.  Zeyuan Allen-Zhu, Ankit Garg, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Operator scaling via geodesically convex optimization, invariant theory and polynomial identity testing. In Pro- ceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 172-181. ACM, 2018.  Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Bakry- \u00b4Emery curvature-dimension condition and Riemannian Ricci curvature bounds. Ann. Probab., 43(1):339-404, 2015. ISSN 0091-1798. doi: 10.1214/14-AOP907. URL https://doi.org/10.1214/14-AOP907.  Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov diffu- sion operators, volume 348 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer, Cham, 2014. ISBN 978-3-319-00226-2; 978- 3-319-00227-9. doi: 10.1007/978-3-319-00227-9. URL https://doi.org/10.1007/ 978-3-319-00227-9.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. Journal of  the ACM (JACM), 51(4):540-556, 2004.  Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint  arXiv:1701.02434, 2017.  Isaac Chavel. Riemannian geometry, volume 98 of Cambridge Studies in Advanced Mathemat- ics. Cambridge University Press, Cambridge, second edition, 2006. ISBN 978-0-521-61954-7; 0-521-61954-8. doi: 10.1017/CBO9780511616822. URL https://doi.org/10.1017/ CBO9780511616822. A modern introduction.  Jeff Cheeger and Detlef Gromoll. On the structure of complete manifolds of nonnegative curvature. Ann. of Math. (2), 96:413-443, 1972. ISSN 0003-486X. doi: 10.2307/1970819. URL https: //doi.org/10.2307/1970819.  Dario Cordero-Erausquin, Robert J. McCann, and Michael Schmuckenschl\u00a8ager. Pr\u00b4ekopa-Leindler type inequalities on Riemannian manifolds, Jacobi fields, and optimal transport. Ann. Fac. Sci. Toulouse Math. (6), 15(4):613-635, 2006. ISSN 0240-2963. URL http://afst.cedram. org/item?id=AFST_2006_6_15_4_613_0. SAMPLING AND OPTIMIZATION ON MANIFOLDS  Persi Diaconis, Susan Holmes, and Mehrdad Shahshahani. Sampling from a manifold. In Advances in modern statistical theory and applications: a Festschrift in honor of Morris L. Eaton, vol- ume 10 of Inst. Math. Stat. (IMS) Collect., pages 102-125. Inst. Math. Statist., Beachwood, OH, 2013.  Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte Carlo.  Physics letters B, 195(2):216-222, 1987.  Martin Dyer, Alan Frieze, and Ravi Kannan. A random polynomial-time algorithm for approximat- ing the volume of convex bodies. J. ACM, 38(1):1-17, January 1991. ISSN 0004-5411. doi: 10.1145/102782.102783. URL http://doi.acm.org/10.1145/102782.102783.  Matthieu Fradelizi and Olivier Gu\u00b4edon. The extreme points of subsets of s-concave probabil- ities and a geometric localization theorem. Discrete Comput. Geom., 31(2):327-335, 2004. ISSN 0179-5376. doi: 10.1007/s00454-003-2868-y. URL https://doi.org/10.1007/ s00454-003-2868-y.  Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. J. R. Stat. Soc. Ser. B Stat. Methodol., 73(2):123-214, 2011. ISSN 1369-7412. doi: 10.1111/j.1467-9868.2010.00765.x. URL https://doi.org/10.1111/j.1467-9868. 2010.00765.x. With discussion and a reply by the authors.  Adam Tauman Kalai and Santosh Vempala. Simulated annealing for convex optimization. Mathe-  matics of Operations Research, 31(2):253-266, 2006.  Ravi Kannan, L\u00b4aszl\u00b4o Lov\u00b4asz, and Mikl\u00b4os Simonovits. Isoperimetric problems for convex bodies  and a localization lemma. Discrete & Computational Geometry, 13(3-4):541-559, 1995.  Bo\u2019az Klartag. Needle decompositions in Riemannian geometry. Mem. Amer. Math. Soc., 249 (1180):v + 77, 2017. ISSN 0065-9266. doi: 10.1090/memo/1180. URL https://doi.org/ 10.1090/memo/1180.  Gilles Lebeau and Laurent Michel. Semi-classical analysis of a random walk on a manifold. Ann. ISSN 0091-1798. doi: 10.1214/09-AOP483. URL https:  Probab., 38(1):277-315, 2010. //doi.org/10.1214/09-AOP483.  Yin Tat Lee and Santosh S Vempala. Geodesic walks in polytopes.  In Proceedings of the 49th  Annual ACM SIGACT Symposium on Theory of Computing, pages 927-940. ACM, 2017.  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Sym- posium on Theory of Computing, pages 1115-1121. ACM, 2018.  Yin Tat Lee, Zhao Song, and Santosh S Vempala. Algorithmic theory of ODEs and sampling from  well-conditioned logconcave densities. arXiv preprint arXiv:1812.06243, 2018.  L\u00b4aszl\u00b4o Lov\u00b4asz. Hit-and-run mixes fast. Mathematical Programming, 86(3):443-461, 1999. SAMPLING AND OPTIMIZATION ON MANIFOLDS  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. The mixing rate of Markov chains, an isoperimetric inequal- ity, and computing the volume. In 31st Annual Symposium on Foundations of Computer Science, Vol. I, II (St. Louis, MO, 1990), pages 346-354. IEEE Comput. Soc. Press, Los Alamitos, CA, 1990. doi: 10.1109/FSCS.1990.89553. URL https://doi.org/10.1109/FSCS.1990. 89553.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Simulated annealing in convex bodies and an O\u2217(n4) volume algorithm. J. Comput. System Sci., 72(2):392-417, 2006. ISSN 0022-0000. doi: 10.1016/j.jcss. 2005.08.004. URL https://doi.org/10.1016/j.jcss.2005.08.004.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing, 35  (4):985-1005, 2006.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algo-  rithms. Random Structures & Algorithms, 30(3):307-358, 2007.  Oren Mangoubi and Aaron Smith. Rapid mixing of hamiltonian monte carlo on strongly log-  concave distributions. arXiv preprint arXiv:1708.07114, 2017.  Oren Mangoubi and Aaron Smith. Rapid mixing of geodesic walks on manifolds with positive curvature. Ann. Appl. Probab., 28(4):2501-2543, 08 2018. doi: 10.1214/17-AAP1365. URL https://doi.org/10.1214/17-AAP1365.  Oren Mangoubi and Nisheeth Vishnoi. Dimensionally tight bounds for second-order hamilto- In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, nian monte carlo. and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 6030-6040. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7842-dimensionally-tight-bounds-for-second-order-hamiltonian-monte-carlo. pdf.  Emanuel Milman. Isoperimetric bounds on convex manifolds. Concentration, functional inequali-  ties and isoperimetry, 545:195-208, 2011.  Radford M. Neal. MCMC using Hamiltonian dynamics.  In Handbook of Markov chain Monte Carlo, Chapman & Hall/CRC Handb. Mod. Stat. Methods, pages 113-162. CRC Press, Boca Raton, FL, 2011.  Yu. E. Nesterov and M. J. Todd. On the Riemannian geometry defined by self-concordant barriers and interior-point methods. Found. Comput. Math., 2(4):333-361, 2002. ISSN 1615-3375. doi: 10.1007/s102080010032. URL https://doi.org/10.1007/s102080010032.  Liviu I Nicolaescu. Lectures on the Geometry of Manifolds. World Scientific, 2007.  Mark D. Plumbley. Lie group methods for optimization with orthogonality constraints.  In Car- los G. Puntonet and Alberto Prieto, editors, Independent Component Analysis and Blind Signal Separation, pages 1245-1252, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg. ISBN 978- 3-540-30110-3. SAMPLING AND OPTIMIZATION ON MANIFOLDS  Tam\u00b4as Rapcs\u00b4ak. Smooth nonlinear optimization in Rn, volume 19 of Nonconvex Optimization and its Applications. Kluwer Academic Publishers, Dordrecht, 1997. ISBN 0-7923-4680-7. doi: 10. 1007/978-1-4615-6357-0. URL https://doi.org/10.1007/978-1-4615-6357-0.  Alexander Rusciano. A Riemannian corollary of Helly\u2019s theorem. arXiv preprint arXiv:1804.10738,  2018.  Suvrit Sra, Nisheeth K. Vishnoi, and Ozan Yildiz. On Geodesically Convex Formulations for the In Eric Blais, Klaus Jansen, Jos\u00b4e D. P. Rolim, and David Steurer, Brascamp-Lieb Constant. editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech- niques (APPROX/RANDOM 2018), volume 116 of Leibniz International Proceedings in In- formatics (LIPIcs), pages 25:1-25:15, Dagstuhl, Germany, 2018. Schloss Dagstuhl-Leibniz- Zentrum fuer Informatik. ISBN 978-3-95977-085-9. doi: 10.4230/LIPIcs.APPROX-RANDOM. 2018.25. URL http://drops.dagstuhl.de/opus/volltexte/2018/9429.  Constantin Udris\u00b8te. Convex functions and optimization methods on Riemannian manifolds, vol- ume 297 of Mathematics and its Applications. Kluwer Academic Publishers Group, Dordrecht, 1994. ISBN 0-7923-3002-1. doi: 10.1007/978-94-015-8390-9. URL https://doi.org/ 10.1007/978-94-015-8390-9.  Santosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry,  52(573-612):2, 2005.  Santosh S Vempala. Recent progress and open problems in algorithmic convex geometry. In LIPIcs- Leibniz International Proceedings in Informatics, volume 8. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2010.  Nisheeth K Vishnoi. Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics, and  Convexity. arXiv preprint arXiv:1806.06373, 2018.  Shing Tung Yau. Non-existence of continuous convex functions on certain Riemannian manifolds. Math. Ann., 207:269-270, 1974. ISSN 0025-5831. doi: 10.1007/BF01351342. URL https: //doi.org/10.1007/BF01351342.  Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Con-  ference on Learning Theory, pages 1617-1638, 2016.  Hongyi Zhang and Suvrit Sra. Towards Riemannian Accelerated Gradient Methods. arXiv preprint  arXiv:1806.02812, 2018.  Jingzhao Zhang, Hongyi Zhang, and Suvrit Sra. R-SPIDER: A Fast Riemannian Stochastic Opti- mization Algorithm with Curvature Independent Rate. arXiv preprint arXiv:1811.04194, 2018.  Junyu Zhang and Shuzhong Zhang. A cubic regularized Newton\u2019s method over Riemannian mani-  folds. arXiv preprint arXiv:1805.05565, 2018. SAMPLING AND OPTIMIZATION ON MANIFOLDS  "}, "Better Algorithms for Stochastic Bandits with Adversarial Corruptions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Better Algorithms for Stochastic Bandits with Adversarial Corruptions", "abstract": "We study the stochastic multi-armed bandits problem in the presence of adversarial corruption. We present a new algorithm for this problem whose regret is nearly optimal, substantially improving upon previous work. Our algorithm is agnostic to the level of adversarial contamination and can tolerate a significant amount of corruption with virtually no degradation in performance.", "pdf_url": "http://proceedings.mlr.press/v99/gupta19a/gupta19a.pdf", "keywords": [], "reference": "Jason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best arm identification for contami-  nated bandits. arXiv preprint arXiv:1802.09514, 2018.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002b.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15, pages 19-26, 2011.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, pages 42-1, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and G\u00b4abor Lugosi. Bandits with heavy tail. IEEE Trans-  actions on Information Theory, 59(11):7711-7717, 2013.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47-60. ACM, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655-664. IEEE, 2016.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061-1073. ACM, 2018.  Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of ran-  domized algorithms. Cambridge University Press, 2009.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):1079-1105, 2006.  Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73-101, 03  1964.  13   ADVERSARIALLY-CORRUPTED STOCHASTIC BANDITS  AG is supported in part by NSF awards CCF-1536002, CCF-1540541, and CCF-1617790.  Acknowledgments  References  Jason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best arm identification for contami-  nated bandits. arXiv preprint arXiv:1802.09514, 2018.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002b.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15, pages 19-26, 2011.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, pages 42-1, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and G\u00b4abor Lugosi. Bandits with heavy tail. IEEE Trans-  actions on Information Theory, 59(11):7711-7717, 2013.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47-60. ACM, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655-664. IEEE, 2016.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061-1073. ACM, 2018.  Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of ran-  domized algorithms. Cambridge University Press, 2009.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):1079-1105, 2006.  Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73-101, 03  1964. ADVERSARIALLY-CORRUPTED STOCHASTIC BANDITS  Sayash Kapoor, Kumar Kshitij Patel, and Purushottam Kar. Corruption-tolerant bandit learning.  Machine Learning, pages 1-29, 2018.  Michael Kearns and Ming Li. Learning in the presence of malicious errors. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, STOC \u201988, pages 267-280, New York, NY, USA, 1988. ACM.  Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regres-  sion. In COLT, 2018.  Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 665-674. IEEE, 2016.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adver- sarial corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 114-122, New York, NY, USA, 2018. ACM.  Yevgeny Seldin and G\u00b4abor Lugosi. An improved parametrization and analysis of the EXP3++ algorithm for stochastic and adversarial bandits. In Conference on Learning Theory, pages 1743- 1759, 2017.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In ICML, pages 1287-1295, 2014.  Han Shao, Xiaotian Yu, Irwin King, and Michael R Lyu. Almost optimal algorithms for linear stochastic bandits with heavy-tailed payoffs. In Advances in Neural Information Processing Sys- tems, pages 8430-8439, 2018.  L.G. Valiant. Learning disjunctions of conjunctions. In Proceedings of the 9th International Joint  Conference on Artificial Intel ligence, pages 560-566, 1985.  Xiaotian Yu, Han Shao, Michael R Lyu, and Irwin King. Pure exploration of multi-armed bandits In Proceedings of the Thirty-Fourth Conference on Uncertainty in  with heavy-tailed payoffs. Artificial Intelligence, pages 937-946, 2018.  Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits.  arXiv preprint arXiv:1807.07623, 2018.  Julian Zimmert, Haipeng Luo, and Chen-Yu Wei. Beating stochastic and adversarial semi-bandits  optimally and simultaneously. arXiv preprint arXiv:1901.08779, 2019. ADVERSARIALLY-CORRUPTED STOCHASTIC BANDITS  "}, "Tight analyses for non-smooth stochastic gradient descent": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Tight analyses for non-smooth stochastic gradient descent", "abstract": "Consider the problem of minimizing functions that are Lipschitz and strongly convex, but not necessarily differentiable. We prove that after $T$ steps of stochastic gradient descent, the error of the final iterate is $O(\\log(T)/T)$ \\emph{with high probability}. We also construct a function from this class for which the error of the final iterate of \\emph{deterministic} gradient descent is $\\Omega(\\log(T)/T)$. This shows that the upper bound is tight and that, in this setting, the last iterate of stochastic gradient descent has the same general error rate (with high probability) as deterministic gradient descent. This resolves both open questions posed by Shamir (2012). An intermediate step of our analysis proves that the suffix averaging method achieves error $O(1/T)$ \\emph{with high probability}, which is optimal (for any first-order optimization method). This improves results of Rakhlin et al. (2012) and Hazan and Kale (2014), both of which achieved error $O(1/T)$, but only in expectation, and achieved a high probability error bound of $O(\\log \\log(T)/T)$, which is suboptimal.", "pdf_url": "http://proceedings.mlr.press/v99/harvey19a/harvey19a.pdf", "keywords": ["Gradient Descent", "Lipschitz Functions", "Strong Convexity", "Martingales", "High Probability Analysis", "Lower Bounds"]}, "Reasoning in Bayesian Opinion Exchange Networks Is PSPACE-Hard": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Reasoning in Bayesian Opinion Exchange Networks Is PSPACE-Hard", "abstract": "We study the Bayesian model of opinion exchange of fully rational agents arranged on a network. In this model, the agents receive private signals that are indicative of an unknown state of the world.  Then, they repeatedly announce the state of the world they consider most likely to their neighbors, at the same time updating their beliefs based on their neighbors\u2019 announcements. This model is extensively studied in economics since the work of Aumann (1976) and Geanakoplos and Polemarchakis (1982). It is known that the agents eventually agree with high probability on any network. It is often argued that the computations needed by agents in this model are difficult, but prior to our results there was no rigorous work showing this hardness.  We show that it is  $\\mathsf{PSPACE}$-hard for the agents to compute their actions in this model. Furthermore, we show that it is equally difficult even to approximate an agent\u2019s posterior: It is $\\mathsf{PSPACE}$-hard to distinguish between the posterior being almost entirely concentrated on one state of the world or another.", "pdf_url": "http://proceedings.mlr.press/v99/hazla19a/hazla19a.pdf", "keywords": ["Bayesian opinion exchange", "Computational complexity"], "reference": "Scott Aaronson. The complexity of agreement. In ACM Symposium on Theory of Computing, pages  634-643, 2005. doi: 10.1145/1060590.1060686.  Daron Acemoglu and Asuman Ozdaglar. Opinion dynamics and learning in social networks. Dy-  namic Games and Applications, 1(1):3-49, 2011. doi: 10.1007/s13235-010-0004-1.  9   REASONING IN BAYESIAN OPINION EXCHANGE NETWORKS IS PSPACE-HARD  \u2022 Ultimately, the hardness will be shown for the computation of agent OBSK. \u2022 Agent OBSi directly observes variable agents in blocks xK, . . . , xi+1. It will be useful to think of OBSi as \u201ccomputing\u201d a logical formula with i \u2212 1 quantifier switches (since from the perspective of this agent blocks xK, . . . , xi+1 are set to fixed values).  \u2022 For each i, there is a (slightly more complicated) gadget similar to the \u201cEVAL-gadget\u201d em- ployed in the 3-SAT reduction. This gadget involves OBSi\u22121 as well as two new agents Bi and Ci and is observed by OBSi. Its purpose is to \u201c\ufb02ip\u201d relative LLRs of different types of variable assignments to implement a quantifier switch.  4. Conclusion  A natural open question is to make progress on the approximate-case hardness in one of the models. For example, one could try to establish NP-hardness for a worst-case network, but holding for signal configurations arising with non-negligible probability. This might require significant new ideas.  Another interesting problem arises from trying to extend our results to the revealed belief model, as discussed in Section D. Thinking in terms of games, consider a class of \u201cno-mistakes-allowed\u201d games: Games where the player with winning strategy always has exactly one winning move, with all alternative moves in a given position leading to a losing position (and this property holding recursively in all positions reachable from the initial one).  Certainly deciding if a position is winning for the first player in such games is in PSPACE. On the other hand, since such a game with all moves performed by the existential player corresponds to a SAT formula with zero or one satisfying assignments, by the Valiant-Vazirani theorem (Valiant and Vazirani, 1986) it is also (morally) NP-hard. This leaves a large gap between NP and PSPACE. For example, suppose we want to prove \u03a02-hardness in the revealed belief model. Then it is natural to consider formulas of the form \u2200x\u2203y : \u03c6(x, y), and the question becomes: How hard is it to distinguish between the cases:  \u2022 YES: For all x, there exists unique yx such that \u03c6(x, yx) = 1. \u2022 NO: There exists unique x0 such that for all y we have \u03c6(x0, y) = 0. For all other x, there  exists unique yx such that \u03c6(x, yx) = 1.  How hard is this problem? In particular, can it be shown to be harder than NP (in some sense)? Hardness of such games can be thought of as a generalization of the Valiant-Vazirani theorem.  We thank Ehsan Emamjomeh-Zadeh for pointing out the PSPACE upper bound. We are also grateful to referees for helpful corrections. This work was partially supported by awards ONR N00014-16- 1-2227, NSF CCF-1665252 and ARO MURI W911NF-12-1-0509 and by a Vannevar Bush Fellow- ship.  Acknowledgments  References  Scott Aaronson. The complexity of agreement. In ACM Symposium on Theory of Computing, pages  634-643, 2005. doi: 10.1145/1060590.1060686.  Daron Acemoglu and Asuman Ozdaglar. Opinion dynamics and learning in social networks. Dy-  namic Games and Applications, 1(1):3-49, 2011. doi: 10.1007/s13235-010-0004-1. REASONING IN BAYESIAN OPINION EXCHANGE NETWORKS IS PSPACE-HARD  Daron Acemoglu, Asuman Ozdaglar, and Ali ParandehGheibi. Spread of (mis)information in social networks. Games and Economic Behavior, 70(2):194-227, 2010. doi: 10.1016/j.geb.2010.01. 005.  Daron Acemoglu, Munther A. Dahleh, Ilan Lobel, and Asuman Ozdaglar. Bayesian learning in social networks. The Review of Economic Studies, 78(4):1201-1236, 2011. doi: 10.1093/restud/ rdr004.  Itai Arieli, Yakov Babichenko, and Manuel Mueller-Frank. Naive learning through probability  matching. SSRN preprint, doi: 10.2139/ssrn.3338015, 2019.  Robert J. Aumann. Agreeing to disagree. The Annals of Statistics, 4(6):1236-1239, 1976. doi:  10.1214/aos/1176343654.  Venkatesh Bala and Sanjeev Goyal. Learning from neighbours. The Review of Economic Studies,  65(3):595-621, 1998. doi: 10.1111/1467-937X.00059.  Abhijit V. Banerjee. A simple model of herd behavior. The Quarterly Journal of Economics, 107  (3):797-817, 1992. doi: 10.2307/2118364.  Sushil Bikhchandani, David Hirshleifer, and Ivo Welch. A theory of fads, fashion, custom, and cultural change as informational cascades. Journal of Political Economy, 100(5):992-1026, 1992. doi: 10.1086/261849.  Vivek Borkar and Pravin P. Varaiya. Asymptotic agreement in distributed estimation. IEEE Trans-  actions on Automatic Control, 27(3):650-655, 1982. doi: 10.1109/TAC.1982.1102982.  Peter Clifford and Aidan Sudbury. A model for spatial con\ufb02ict. Biometrika, 60(3):581-588, 1973.  doi: 10.1093/biomet/60.3.581.  Anne Condon, Joan Feigenbaum, Carsten Lund, and Peter Shor. Probabilistically checkable proof systems and nonapproximability of PSPACE-hard functions. Chicago Journal of Theoretical Computer Science, 1995(4), 1995.  Anne Condon, Joan Feigenbaum, Carsten Lund, and Peter Shor. Random debaters and the hardness of approximating stochastic functions. SIAM Journal on Computing, 26(2):369-400, 1997. doi: 10.1137/S0097539793260738.  Tyler Cowen and Robin Hanson. Are disagreements honest? Technical report, George Mason  University Economics, 2002.  Morris H. DeGroot. Reaching a consensus. Journal of the American Statistical Association, 69  (345):118-121, 1974. doi: 10.1080/01621459.1974.10480137.  Douglas Gale and Shachar Kariv. Bayesian learning in social networks. Games and Economic  Behavior, 45(2):329-346, 2003. doi: https://doi.org/10.1016/S0899-8256(03)00144-1.  John Geanakoplos. Common knowledge.  In Handbook of Game Theory with Economic Appli- cations, volume 2, chapter 40, pages 1437-1496. Elsevier, 1st edition, 1994. doi: 10.1016/ S1574-0005(05)80072-4. REASONING IN BAYESIAN OPINION EXCHANGE NETWORKS IS PSPACE-HARD  John D. Geanakoplos and Heraklis M. Polemarchakis. We can\u2019t disagree forever. Journal of Eco-  nomic Theory, 28(1):192-200, 1982. doi: 10.1016/0022-0531(82)90099-0.  Benjamin Golub and Matthew O. Jackson. Na\u00efve learning in social networks and the wisdom of crowds. American Economic Journal: Microeconomics, 2(1):112-149, February 2010. doi: 10.1257/mic.2.1.112.  Robin Hanson. For Bayesian wannabes, are disagreements not about information? Theory and  Decision, 54(2):105-123, 2003. doi: 10.1023/A:1026251404228.  Richard A. Holley and Thomas M. Liggett. Ergodic theorems for weakly interacting infinite sys- tems and the voter model. The Annals of Probability, 3(4):643-663, 1975. doi: 10.1214/aop/ 1176996306.  Harry B. Hunt, Madhav V. Marathe, and Richard E. Stearns. Generalized CNF satisfiability prob- lems and non-efficient approximability. In Complexity Theory Conference (CCC), pages 356- 366, 1994.  Jan H \u02dbaz\u0142a, Ali Jadbabaie, Elchanan Mossel, and M. Amin Rahimian. Bayesian decision making in  groups is hard. arXiv:1705.04770v3, 2018.  Peter Jonsson. A nonapproximability result for finite function generation. Information Processing  Letters, 63(3):143-145, 1997. doi: 10.1016/S0020-0190(97)00112-9.  Peter Jonsson. Strong bounds on the approximability of two PSPACE-hard problems in proposi- tional planning. Annals of Mathematics and Artificial Intelligence, 26(1):133-147, 1999. doi: 10.1023/A:1018954827926.  Johan Kwisthout. Approximate inference in Bayesian networks: Parameterized complexity results. International Journal of Approximate Reasoning, 93:119-131, 2018. doi: https://doi.org/10. 1016/j.ijar.2017.10.029.  Madhav V. Marathe, Harry B. Hunt, and S. S. Ravi. The complexity of approximating PSPACE- complete problems for hierarchical specifications. Nordic Journal of Computing, 1(3):275-316, 1994.  Elchanan Mossel and Omer Tamuz. Opinion exchange dynamics. Probability Surveys, 14:155-204,  2017. doi: 10.1214/14-PS230.  Elchanan Mossel, Allan Sly, and Omer Tamuz. Asymptotic learning on Bayesian social networks. Probability Theory and Related Fields, 158(1):127-157, 2014. doi: 10.1007/s00440-013-0479-y.  Elchanan Mossel, Noah Olsman, and Omer Tamuz. Efficient Bayesian learning in social networks with Gaussian estimators. In Allerton Conference on Communication, Control, and Computing (Allerton), pages 425-432, 2016. doi: 10.1109/ALLERTON.2016.7852262.  Christos H. Papadimitriou and John Tsitsiklis. On the complexity of designing distributed protocols.  Information and Control, 53(3):211-218, 1982. doi: 10.1016/S0019-9958(82)91034-8.  Christos H. Papadimitriou and John Tsitsiklis. Intractable problems in control theory. SIAM Journal  on Control and Optimization, 24(4):639-654, 1986. doi: 10.1137/0324038. REASONING IN BAYESIAN OPINION EXCHANGE NETWORKS IS PSPACE-HARD  Christos H. Papadimitriou and John N. Tsitsiklis. The complexity of Markov decision processes.  Mathematics of Operations Research, 12(3):441-450, 1987. doi: 10.1287/moor.12.3.441.  Dinah Rosenberg, Eilon Solan, and Nicolas Vieille. Informational externalities and emergence of consensus. Games and Economic Behavior, 66(2):979-994, 2009. doi: 10.1016/j.geb.2008.09. 027.  Lones Smith and Peter S\u00f8rensen. Pathological outcomes of observational learning. Econometrica,  68(2):371-398, 2000. doi: 10.1111/1468-0262.00113.  John N. Tsitsiklis and Michael Athans. Convergence and asymptotic agreement in distributed deci- sion problems. IEEE Transactions on Automatic Control, 29(1):42-50, 1984. doi: 10.1109/TAC. 1984.1103385.  John N. Tsitsiklis and Michael Athans. On the complexity of decentralized decision making and detection problems. IEEE Transactions on Automatic Control, 30(5):440-446, 1985. doi: 10. 1109/TAC.1985.1103988.  Leslie G. Valiant and Vijay V. Vazirani. NP is as easy as detecting unique solutions. Theoretical  Computer Science, 47:85-93, 1986. doi: 10.1016/0304-3975(86)90135-0. REASONING IN BAYESIAN OPINION EXCHANGE NETWORKS IS PSPACE-HARD  "}, "How Hard is Robust Mean Estimation?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "How Hard is Robust Mean Estimation?", "abstract": "Robust mean estimation is the problem of estimating the mean $\\mu \\in \\mathbb{R}^d$ of a $d$-dimensional distribution $D$ from a list of independent samples, an $\\varepsilon$-fraction of which have been arbitrarily corrupted by a malicious adversary. Recent algorithmic progress has resulted in the first polynomial-time algorithms which achieve \\emph{dimension-independent} rates of error: for instance, if $D$ has covariance $I$, in polynomial-time one may find $\\hat{\\mu}$ with $\\|\\mu - \\hat{\\mu}\\| \\leq O(\\sqrt{\\varepsilon})$. However, error rates achieved by current polynomial-time algorithms, while dimension-independent, are sub-optimal in many natural settings, such as when $D$ is sub-Gaussian, or has bounded $4$-th moments. In this work we give worst-case complexity-theoretic evidence that improving on the error rates of current polynomial-time algorithms for robust mean estimation may be computationally intractable in natural settings. We show that several natural approaches to improving error rates of current polynomial-time robust mean estimation algorithms would imply efficient algorithms for the small-set expansion problem, refuting Raghavendra and Steurer\u2019s small-set expansion hypothesis (so long as $P \\neq NP$). We also give the first direct reduction to the robust mean estimation problem, starting from a plausible but nonstandard variant of the small-set expansion problem.", "pdf_url": "http://proceedings.mlr.press/v99/hopkins19a/hopkins19a.pdf", "keywords": ["robust mean estimation", "small-set expansion", "complexity of learning", "robust statistics", "spectral graph theory"], "reference": "Frank J Anscombe. Rejection of outliers. Technometrics, 2(2):123-146, 1960.  Benny Applebaum, Boaz Barak, and David Xiao. On basing lower-bounds for learning on worst-  case assumptions. In FOCS, pages 211-220. IEEE Computer Society, 2008.  Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient robust sparse estimation in high dimensions. In Conference on Learning Theory, pages 169-212, 2017.  Boaz Barak. Sum of squares upper bounds, lower bounds, and open questions. 2014. URL https:  //www.boazbarak.org/sos/prev/files/all-notes.pdf.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, In STOC,  and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. pages 307-326. ACM, 2012a.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. CoRR, abs/1205.4484, 2012b.  Thorsten Bernholt. Robust estimators are hard to compute. Technical report, Technical Re-  port/Universit\u00a8at Dortmund, SFB 475 Komplexit\u00a8atsreduktion in ?, 2006.  S\u00b4ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from  cryptographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018.  12   HOW HARD IS ROBUST MEAN ESTIMATION?  Hypothesis 17 (\u2264-Small-Set Expansion Hypothesis (SSEH\u2264)) For every constant (cid:15) > 0 there is a small-enough \u03b4 > 0 such that the following problem is NP-hard. Given a graph G, distinguish between \u03a6\u2264  G(\u03b4) \u2265 1 \u2212 (cid:15) and \u03a6\u2264  G(\u03b4) \u2264 (cid:15).  We are not aware of any equivalences or implications between these two (apparently very simi- lar) problems. However, both versions of the problem have been widely used and called the \u201cSmall- Set Expansion Hypothesis\u201d in the literature, see e.g. Barak et al. (2012a).  We remark that while these two problems are very similar, there do appear to be some subtle qualitative differences between them. In particular, in the context of this paper, SSEH= (and vari- ants thereof) implies hardness for problems related to resilience, whereas SSEH\u2264 implies hardness for problems related to bounded moments. At a high level, this is because bounded moments is equivalent to resilience at every scale (see Corollary 32), and thus to control moments, we need to know what occurs at all sets of size at most \u03b4, not just in a neighborhood around \u03b4.  We thank Prasad Raghavendra for numerous helpful discussion regarding this paper, and in partic- ular for the suggestion to use random walks to prove the upper bound in Section 3. We also thank David Steurer for answering emergency questions about small-set expansion.  Acknowledgments  References  Frank J Anscombe. Rejection of outliers. Technometrics, 2(2):123-146, 1960.  Benny Applebaum, Boaz Barak, and David Xiao. On basing lower-bounds for learning on worst-  case assumptions. In FOCS, pages 211-220. IEEE Computer Society, 2008.  Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient robust sparse estimation in high dimensions. In Conference on Learning Theory, pages 169-212, 2017.  Boaz Barak. Sum of squares upper bounds, lower bounds, and open questions. 2014. URL https:  //www.boazbarak.org/sos/prev/files/all-notes.pdf.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, In STOC,  and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. pages 307-326. ACM, 2012a.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. CoRR, abs/1205.4484, 2012b.  Thorsten Bernholt. Robust estimators are hard to compute. Technical report, Technical Re-  port/Universit\u00a8at Dortmund, SFB 475 Komplexit\u00a8atsreduktion in ?, 2006.  S\u00b4ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from  cryptographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018. HOW HARD IS ROBUST MEAN ESTIMATION?  Mark Bun and Mark Zhandry. Order-revealing encryption and the hardness of private learning. In  Theory of Cryptography Conference, pages 176-206. Springer, 2016.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In STOC,  pages 47-60. ACM, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In FOCS, pages 655-664. IEEE Computer Society, 2016.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In ICML, volume 70 of Proceedings of  Being robust (in high dimensions) can be practical. Machine Learning Research, pages 999-1008. PMLR, 2017a.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In International Conference on Machine  Being robust (in high dimensions) can be practical. Learning, pages 999-1008, 2017b.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 73-84. IEEE, 2017c.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1047-1060. ACM, 2018.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2745-2754. SIAM, 2019.  Vitaly Feldman and Varun Kanade. Computational bounds on statistical query learning. In Confer-  ence on Learning Theory, pages 16-1, 2012.  Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. New results for learning noisy parities and halfspaces. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pages 563-574. IEEE, 2006.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. SIAM  J. Comput., 39(2):742-765, 2009.  Moritz Hardt and Ankur Moitra. Algorithms and hardness for robust subspace recovery. In Confer-  ence on Learning Theory, pages 354-375, 2013.  Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Pro- ceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021- 1034. ACM, 2018.  Peter J Huber. Robust estimation of a location parameter. The annals of mathematical statistics, 35  (1):73-101, 1964. HOW HARD IS ROBUST MEAN ESTIMATION?  David S Johnson and Franco P Preparata. The densest hemisphere problem. Theoretical Computer  Science, 6(1):93-107, 1978.  William B Johnson, Gideon Schechtman, and Joel Zinn. Best constants in moment inequalities for linear combinations of independent and exchangeable random variables. The Annals of Proba- bility, pages 234-253, 1985.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. In Proceedings of the twenty-sixth annual ACM  On the learnability of discrete distributions. symposium on Theory of computing, pages 273-282. ACM, 1994.  Adam R. Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In APPROX-RANDOM, volume 28 of LIPIcs, pages 793-809. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2014.  Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035-1046. ACM, 2018.  Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In  FOCS, pages 665-674. IEEE Computer Society, 2016.  Jerry Li. Principled Approaches to Robust Machine Learning and Beyond. 2018.  Prasad Raghavendra and David Steurer. Graph expansion and the unique games conjecture.  In  STOC, pages 755-764. ACM, 2010.  Prasad Raghavendra, David Steurer, and Madhur Tulsiani. Reductions between expansion problems. In IEEE Conference on Computational Complexity, pages 64-73. IEEE Computer Society, 2012.  Oded Regev. On lattices, learning with errors, random linear codes, and cryptography. J. ACM, 56  (6):34:1-34:40, 2009.  Jacob Steinhardt. Talk at stoc 2018 workshop on computational phase transitions. 2018a.  Jacob Steinhardt. Robust Learning: Information Theory and Algorithms. Stanford University,  Thesis, 2018b.  Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the  presence of arbitrary outliers. arXiv preprint arXiv:1703.04940, 2017.  David Steurer. Subexponential algorithms for d-to-1 two-prover games and for certifying almost  perfect expansion. Available at the authors website, pages 2-1, 2010a.  David Steurer. On the complexity of unique games and graph expansion. Citeseer, 2010b.  John W Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress  of Mathematicians, Vancouver, 1975, volume 2, pages 523-531, 1975.  J.W. Tukey. A survey of sampling from contaminated distributions. Contributions to probability  and statistics, 2:448-485, 1960. HOW HARD IS ROBUST MEAN ESTIMATION?  "}, "A Robust Spectral Algorithm for Overcomplete Tensor Decomposition": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A Robust Spectral Algorithm for Overcomplete Tensor Decomposition", "abstract": "We give a spectral algorithm for decomposing overcomplete order-4 tensors, so long as their components satisfy an algebraic non-degeneracy condition that holds for nearly all (all but an algebraic set of measure $0$) tensors over $(\\mathbb{R}^d)^{\\otimes 4}$ with rank $n \\le d^2$. Our algorithm is robust to adversarial perturbations of bounded spectral norm. Our algorithm is inspired by one which uses the sum-of-squares semidefinite programming hierarchy (Ma, Shi, and Steurer STOC\u201916), and we achieve comparable robustness and overcompleteness guarantees under similar algebraic assumptions. However, our algorithm avoids semidefinite programming and may be implemented as a series of basic linear-algebraic operations. We consequently obtain a much faster running time than semidefinite programming methods: our algorithm runs in time $\\tilde O(n^2d^3) \\le \\tilde O(d^7)$, which is subquadratic in the input size $d^4$ (where we have suppressed factors related to the condition number of the input tensor).", "pdf_url": "http://proceedings.mlr.press/v99/hopkins19b/hopkins19b.pdf", "keywords": ["overcomplete tensors", "tensor rank decomposition", "CP decomposition", "spectral algorithms", "algebraic non-degeneracy", "sum-of-squares proofs"], "reference": "Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and B\u00fclent Yener. Mul- tiway analysis of epilepsy tensors. Bioinformatics, 23(13):i10-i18, 2007. doi: 10.1093/ bioinformatics/btm210. URL http://dx.doi.org/10.1093/bioinformatics/btm210.  Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing pain. In Advances in Neural Information Processing Systems, pages 974-982, 2016.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynam- ics: Applications to learning overcomplete latent variable models. CoRR, abs/1411.1488, 2014a.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT, volume 30 of JMLR Workshop and Conference Proceedings, pages 867-881. JMLR.org, 2013.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014b.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor  decomposition via alternating rank-1 updates. CoRR, abs/1402.5180, 2014c.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynamics in overcomplete regime. Journal of Machine Learning Research, 18:22:1-22:40, 2017.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  C.F. Beckmann and S.M. Smith. Tensorial extensions of independent component analysis for multisubject fmri analysis. NeuroImage, 25(1):294 - 311, 2005. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2004.10.043. URL http://www.sciencedirect. com/science/article/pii/S1053811904006378.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed  analysis of tensor decompositions. In STOC, pages 594-603. ACM, 2014.  Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM J. Numer. Anal., 7:1-46, 1970. ISSN 0036-1429. doi: 10.1137/0707001. URL http://dx.doi.org/10.1137/0707001.  13   A Robust Spectral Algorithm for Overcomplete Tensor Decomposition  CFF-1408673, and a Microsoft PhD Fellowship. Tselil Schramm was supported by NSF awards CCF-1565264 and CNS-1618026. Jonathan Shi was supported by David Steurer\u2019s NSF CAREER grant CCF-1350196 and CFF-1408673.  References  Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and B\u00fclent Yener. Mul- tiway analysis of epilepsy tensors. Bioinformatics, 23(13):i10-i18, 2007. doi: 10.1093/ bioinformatics/btm210. URL http://dx.doi.org/10.1093/bioinformatics/btm210.  Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing pain. In Advances in Neural Information Processing Systems, pages 974-982, 2016.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynam- ics: Applications to learning overcomplete latent variable models. CoRR, abs/1411.1488, 2014a.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT, volume 30 of JMLR Workshop and Conference Proceedings, pages 867-881. JMLR.org, 2013.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014b.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor  decomposition via alternating rank-1 updates. CoRR, abs/1402.5180, 2014c.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Analyzing tensor power method dynamics in overcomplete regime. Journal of Machine Learning Research, 18:22:1-22:40, 2017.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  C.F. Beckmann and S.M. Smith. Tensorial extensions of independent component analysis for multisubject fmri analysis. NeuroImage, 25(1):294 - 311, 2005. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2004.10.043. URL http://www.sciencedirect. com/science/article/pii/S1053811904006378.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed  analysis of tensor decompositions. In STOC, pages 594-603. ACM, 2014.  Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM J. Numer. Anal., 7:1-46, 1970. ISSN 0036-1429. doi: 10.1137/0707001. URL http://dx.doi.org/10.1137/0707001. A Robust Spectral Algorithm for Overcomplete Tensor Decomposition  Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. Blind source separation by simultaneous third-order tensor diagonalization. In European Signal Processing Conference, 1996. EUSIPCO 1996. 8th, pages 1-4. IEEE, 1996.  Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-order cumulant- based blind identification of underdetermined mixtures. IEEE Transactions on Signal Processing, 55(6):2965-2973, 2007.  Michael Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. Springer Publishing Company, Incorporated, 1st edition, 2010. ISBN 144197010X, 9781441970107.  Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. In APPROX-RANDOM, volume 40 of LIPIcs, pages 829-849. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2015.  Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In  Advances in Neural Information Processing Systems, pages 3653-3663, 2017.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In Peter Gr\u00fcnwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, volume 40 of JMLR Workshop and Conference Proceedings, pages 797-842. JMLR.org, 2015a. URL http://jmlr.org/proceedings/papers/v40/ Ge15.html.  Rong Ge, Qingqing Huang, and Sham M. Kakade. Learning mixtures of Gaussians in high dimensions [extended abstract]. In STOC\u201915\u2014Proceedings of the 2015 ACM Symposium on Theory of Computing, pages 761-770. ACM, New York, 2015b.  Wu Hai-Long, Shibukawa Masami, and Oguma Koichi. An alternating trilinear decomposition algorithm with application to calibration of HPLC-DAD for simultaneous determination of overlapped chlorinated aromatic hydrocarbons. Journal of Chemometrics, 12(1):1-26. doi: 10.1002/(SICI)1099-128X(199801/02)12:1<1::AID-CEM492>3.0.CO;2-4.  Richard A Harshman. Foundations of the parafac procedure: Models and conditions for an\"  explanatory\" multi-modal factor analysis. 1970.  Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are np-hard. J. ACM, 60(6):  45:1-45:39, 2013.  Samuel B Hopkins and David Steurer. E\ufb03cient bayesian estimation from few samples: community detection and related problems. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 379-390. IEEE, 2017.  Samuel B. Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis In COLT, volume 40 of JMLR Workshop and Conference  via sum-of-square proofs. Proceedings, pages 956-1006. JMLR.org, 2015. A Robust Spectral Algorithm for Overcomplete Tensor Decomposition  Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In STOC, pages 178-191. ACM, 2016.  Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM  Review, 51(3):455-500, 2009.  Lieven De Lathauwer, Jos\u00e9phine Castaing, and Jean-Fran\u00e7ois Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. IEEE Trans. Signal Processing, 55(6-2):2965-2973, 2007.  Michael S. Lewicki and Terrence J. Sejnowski. Learning overcomplete representations. Neural Comput., 12(2):337-365, February 2000. ISSN 0899-7667. doi: 10.1162/089976600300015826. URL http://dx.doi.org/10.1162/089976600300015826.  Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with  sum-of-squares. In FOCS, pages 438-446. IEEE Computer Society, 2016.  Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural networks and tensor decomposition. CoRR, abs/1802.07301, 2018. URL http: //arxiv.org/abs/1802.07301.  Emile Richard and Andrea Montanari. A statistical model for tensor PCA. In NIPS, pages  2897-2905, 2014.  Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to dictionary learning. In COLT, volume 65 of Proceedings of Machine Learning Research, pages 1760-1793. PMLR, 2017.  Vatsal Sharan and Gregory Valiant. Orthogonalized ALS: A theoretically principled tensor decomposition algorithm for practical use. In ICML, volume 70 of Proceedings of Machine Learning Research, pages 3095-3104. PMLR, 2017.  Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller di\ufb00er- entialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathe- matische Annalen, 71(4):441-479, Dec 1912. ISSN 1432-1807. doi: 10.1007/BF01456804. URL https://doi.org/10.1007/BF01456804.  "}, "Sample-Optimal Low-Rank Approximation of Distance Matrices": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sample-Optimal Low-Rank Approximation of Distance Matrices", "abstract": "A distance matrix $A \\in \\mathbb{R}^{n \\times m}$ represents all pairwise distances, $A_{i,j} = d(x_i,y_j)$, between two point sets $x_1,\\dotsc,x_n$ and $y_1,\\dotsc,y_m$ in an arbitrary metric space $(\\mathcal{Z},d)$. Such matrices arise in various computational contexts such as learning image manifolds, handwriting recognition, and multi-dimensional unfolding. In this work we study algorithms for low-rank approximation of distance matrices. Recent work by Bakshi and Woodruff (NeurIPS 2018) showed it is possible to compute a rank-$k$ approximation of a distance matrix in time $O((n+m)^{1+\\gamma}) \\mathrm{poly}(k,1/\\epsilon)$, where $\\epsilon>0$ is an error parameter and $\\gamma>0$ is an arbitrarily small constant. Notably, their bound is sublinear in the matrix size, which is unachieveable for general matrices. We present an algorithm that is both simpler and more efficient. It reads only $O((n+m)k/\\epsilon)$ entries of the input matrix, and has a running time of $O(n+m) \\cdot \\mathrm{poly}(k,1/\\epsilon)$. We complement the sample complexity of our algorithm with a matching lower bound on the number of entries that must be ready by any algorithm. We provide experimental results to validate the approximation quality and running time of our algorithm", "pdf_url": "http://proceedings.mlr.press/v99/indyk19a/indyk19a.pdf", "keywords": ["Low-rank Approximation", "Distance Matrix"], "reference": "Ainesh Bakshi and David Woodruff. Sublinear time low-rank approximation of distance matrices.  In Advances in Neural Information Processing Systems, pages 3786-3796, 2018.  Mark Braverman and Anup Rao. Information equals amortized communication. IEEE Transactions  on Information Theory, 60(10):6058-6069, 2014.  Mark Braverman, Ankit Garg, Denis Pankratov, and Omri Weinstein. Information lower bounds via  self-reducibility. Theory of Computing Systems, 59(2):377-396, 2016.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Ke Chen. On coresets for k-median and k-means clustering in metric and euclidean spaces and their  applications. SIAM Journal on Computing, 39(3):923-947, 2009.  Xue Chen and Eric Price. Condition number-free query and active learning of linear families. arXiv  preprint arXiv:1711.10051, 2017.  Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-  sity time. Journal of the ACM (JACM), 63(6):54, 2017. (first appeared in STOC\u201913).  Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices: essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12-30, 2015.  Ohad N Feldheim and Sasha Sodin. A universality result for the smallest eigenvalues of certain  sample covariance matrices. Geometric And Functional Analysis, 20(1):88-123, 2010.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank  approximations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  Alan J Hoffman and Helmut W Wielandt. The variation of the spectrum of a normal matrix. In Selected Papers Of Alan J Hoffman: With Commentary, pages 118-120. World Scientific, 2003.  Liisa Holm and Chris Sander. Protein structure comparison by alignment of distance matrices.  Journal of molecular biology, 233(1):123-138, 1993.  Piotr Indyk. A sublinear time approximation scheme for clustering in metric spaces. In Foundations  of Computer Science, 1999. 40th Annual Symposium on, pages 154-159. IEEE, 1999.  13   Acknowledgments  P. Indyk, A. Vakilian and T. Wagner were supported by funds from the MIT-IBM Watson AI Lab, NSF, and Simons Foundation. D. Woodruff was supported partly by the National Science Foun- dation under Grant No. CCF-1815840, and this work was done partly while he was visiting the Simons Institute for the Theory of Computing. The authors would also like to thank Ainesh Bakshi for implementing the algorithm in this paper and producing our empirical results.  References  Ainesh Bakshi and David Woodruff. Sublinear time low-rank approximation of distance matrices.  In Advances in Neural Information Processing Systems, pages 3786-3796, 2018.  Mark Braverman and Anup Rao. Information equals amortized communication. IEEE Transactions  on Information Theory, 60(10):6058-6069, 2014.  Mark Braverman, Ankit Garg, Denis Pankratov, and Omri Weinstein. Information lower bounds via  self-reducibility. Theory of Computing Systems, 59(2):377-396, 2016.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Ke Chen. On coresets for k-median and k-means clustering in metric and euclidean spaces and their  applications. SIAM Journal on Computing, 39(3):923-947, 2009.  Xue Chen and Eric Price. Condition number-free query and active learning of linear families. arXiv  preprint arXiv:1711.10051, 2017.  Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-  sity time. Journal of the ACM (JACM), 63(6):54, 2017. (first appeared in STOC\u201913).  Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices: essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12-30, 2015.  Ohad N Feldheim and Sasha Sodin. A universality result for the smallest eigenvalues of certain  sample covariance matrices. Geometric And Functional Analysis, 20(1):88-123, 2010.  Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank  approximations. Journal of the ACM (JACM), 51(6):1025-1041, 2004.  Alan J Hoffman and Helmut W Wielandt. The variation of the spectrum of a normal matrix. In Selected Papers Of Alan J Hoffman: With Commentary, pages 118-120. World Scientific, 2003.  Liisa Holm and Chris Sander. Protein structure comparison by alignment of distance matrices.  Journal of molecular biology, 233(1):123-138, 1993.  Piotr Indyk. A sublinear time approximation scheme for clustering in metric spaces. In Foundations  of Computer Science, 1999. 40th Annual Symposium on, pages 154-159. IEEE, 1999. Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R(cid:13) in  Machine Learning, 3(2):123-224, 2011.  Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of positive In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual  semidefinite matrices. Symposium on, pages 672-683. IEEE, 2017.  Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu- lar values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pages 1576- 1602. World Scientific, 2010.  Terence  Tao.  matrices. 254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices, 2010.  notes hermitian https://terrytao.wordpress.com/2010/01/12/  Eigenvalues  254a,  sums  and  3a:  of  Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for  nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.  RC Thompson. The behavior of eigenvalues and singular values under perturbations of restricted  rank. Linear Algebra and its Applications, 13(1-2):69-78, 1976.  Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semidef-  inite programming. International journal of computer vision, 70(1):77-90, 2006.  David P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends R(cid:13) in  Theoretical Computer Science, 10(1-2):1-157, 2014.  Figure 3: Running time of our algorithm on subsets of MNIST, for k = 40.  "}, "Making the Last Iterate of SGD Information Theoretically Optimal": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Making the Last Iterate of SGD Information Theoretically Optimal", "abstract": "Stochastic gradient descent (SGD) is one of the most widely used algorithms for large scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) \\emph{averages} of iterates and obtains information theoretically optimal bounds on suboptimality, the \\emph{last point} of SGD is, by far, the most preferred choice in practice. The best known results for last point of SGD (Shamir and Zhang, 2013) however, are suboptimal compared to information theoretic lower bounds by a $\\log T$ factor, where $T$ is the number of iterations. Harvey et. al (2018) shows that in fact, this additional $\\log T$ factor is tight for standard step size sequences of $\\OTheta{\\frac{1}{\\sqrt{t}}}$ and $\\OTheta{\\frac{1}{t}}$ for non-strongly convex and strongly convex settings, respectively. Similarly, even for subgradient descent (GD) when applied to non-smooth, convex functions, the best known step-size sequences still lead to $O(\\log T)$-suboptimal convergence rates (on the final iterate). The main contribution of this work is to design new step size sequences that enjoy information theoretically optimal bounds on the suboptimality of \\emph{last point} of SGD as well as GD. We achieve this by designing a modification scheme, that converts one sequence of step sizes to another so that the last point of SGD/GD with modified sequence has the same suboptimality guarantees as the average of SGD/GD with original sequence. We also show that our result holds with high-probability. We validate our results through simulations which demonstrate that the new step size sequence indeed improves the final iterate significantly compared to the standard step size sequences.", "pdf_url": "http://proceedings.mlr.press/v99/jain19a/jain19a.pdf", "keywords": ["Stochastic Gradient Descent", "Machine Learning", "Convex Optimization"], "reference": "Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-50  on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in  Machine Learning, 8(3-4):231-357, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for  non-smooth stochastic gradient descent. arXiv preprint arXiv:1812.05217, 2018.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1): 2489-2512, 2014.  3   LAST ITERATE OF SGD  shown to achieve information theoretically optimal error rates in the convex and strongly convex settings when averaging of iterates is used (Nemirovsky and Yudin (1983),Zinkevich (2003),Cesa- Bianchi et al. (2004), Kakade and Tewari (2009), Epoch GD in Hazan and Kale (2014) , SGD Rakhlin et al. (2012) and Lacoste-Julien et al. (2012)). The question of the last iterate was first considered in Shamir and Zhang (2013) and it gives a bound of O( log T\u221a T ) in expectation T for the general case and strongly convex case respectively. Harvey et al. (2018) show matching high (cid:1) in the general case and O (cid:0) 1 probability bounds and show that for the standard step sizes (O t in the strongly convex case), the logarithmic-suboptimal bounds are tight.  ) and O( log T  (cid:16) 1\u221a  (cid:17)  t  2. Conclusions and Discussion  We studied the fundamental question of sub-optimality of the last point of SGD/GD for general non-smooth convex functions as well as for strongly-convex functions. We proposed a novel step-size sequence that leads to information theoretically optimal rates in both the above mentioned settings. Our result proves a more general result for any \u201cmodified step-size\u201d of a decaying standard step-size, and uses a novel technique of tracking best iterate in each time-interval and ensuring that the later iterates do not significantly deviate from the best iterate in the previous time interval. We also provide a high-probability bound using a super-martingale technique from Harvey et al. (2018). Simulations show that our step-size indeed leads to better last point than the standard step-size sequences.  Our approach fundamentally exploits an assumption that we apriori know the total number of iterations T . Hence, our result does not provide an any-time algorithm. In contrast, existing any-time results have an extra log T multiplicative factor in the sub-optimality. We conjecture that this gap is fundamental and every any-time algorithm would suffer from the extra log T factor.  This research was partially supported by ONR N00014-17-1-2147 and MIT-IBM Watson AI Lab.  Acknowledgments  References  Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-50  on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in  Machine Learning, 8(3-4):231-357, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for  non-smooth stochastic gradient descent. arXiv preprint arXiv:1812.05217, 2018.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1): 2489-2512, 2014. LAST ITERATE OF SGD  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.  Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method  efficiency in optimization. 1983.  Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  Alexander Rakhlin, Ohad Shamir, Karthik Sridharan, et al. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, volume 12, pages 1571-1578. Citeseer, 2012.  Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated  sub-gradient solver for svm. Mathematical programming, 127(1):3-30, 2011.  Ohad Shamir. Open problem: Is averaging needed for strongly convex stochastic gradient descent?  In Conference on Learning Theory, pages 47-1, 2012.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver- gence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71-79, 2013.  Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM, 2004.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928- 936, 2003. "}, "Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation", "abstract": "The analysis of Belief Propagation and other algorithms for the {\\em reconstruction problem} plays a key role in the analysis of community detection in inference on graphs, phylogenetic reconstruction in bioinformatics, and the cavity method in statistical physics.  We prove a conjecture of Evans, Kenyon, Peres, and Schulman (2000) which states that any bounded memory message passing algorithm is statistically much weaker than Belief Propagation for the reconstruction problem. More formally, any recursive algorithm with bounded memory for the  reconstruction problem on the trees with the binary symmetric channel has a phase transition strictly below the Belief Propagation threshold, also known as the Kesten-Stigum bound. The proof combines in novel fashion tools from recursive reconstruction, information theory, and optimal transport, and also establishes an asymptotic normality result for BP and other message-passing algorithms near the critical threshold.", "pdf_url": "http://proceedings.mlr.press/v99/jain19b/jain19b.pdf", "keywords": [], "reference": "Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Inference under information constraints I: Lower bounds from chi-square contraction. arXiv preprint arXiv:1812.11476, 2018.  Rudolf Ahlswede and Imre Csisz\u00e1r. Hypothesis testing with communication constraints.  IEEE Transactions on Information Theory, 32(4), 1986.  Leighton Pate Barnes, Yanjun Han, and Ayfer \u00d6zg\u00fcr. A geometric characterization of fisher information from quantized samples with applications to distributed statistical estimation. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 16-23, 2018.  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57 (2):764-785, 2011.  Noam Berger, Claire Kenyon, Elchanan Mossel, and Yuval Peres. Glauber dynamics on trees and hyperbolic graphs. Probability Theory and Related Fields, 131(3):311-340, 2005.  Quentin Berthet, Philippe Rigollet, et al. Optimal detection of sparse principal components  in high dimension. The Annals of Statistics, 41(4):1780-1815, 2013.  Pavel M. Bleher, Jean Ruiz, and Valentin A. Zagrebnov. On the purity of the limiting Gibbs state for the Ising model on the Bethe lattice. J. Statist. Phys., 79(1-2):473-482, 1995.  Christian Borgs, Jennifer Chayes, Elchanan Mossel, and S\u00e9bastien Roch. The kesten-stigum reconstruction bound is tight for roughly symmetric binary channels. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 518-530. IEEE, 2006.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P Woodru\ufb00. Com- munication lower bounds for statistical estimation problems via a distributed data pro- cessing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011-1020. ACM, 2016.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds for problems with planted sparse structure. In Conference on Learning Theory, pages 48-166, 2018.  Constantinos Daskalakis, Elchanan Mossel, and S\u00e9bastien Roch. Optimal phylogenetic re- construction. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 159-168. ACM, 2006.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Inference and phase transitions in the detection of modules in sparse networks. Physical Review Letters, 107(6):065701, 2011.  13   Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation  References  Jayadev Acharya, Cl\u00e9ment L. Canonne, and Himanshu Tyagi. Inference under information constraints I: Lower bounds from chi-square contraction. arXiv preprint arXiv:1812.11476, 2018.  Rudolf Ahlswede and Imre Csisz\u00e1r. Hypothesis testing with communication constraints.  IEEE Transactions on Information Theory, 32(4), 1986.  Leighton Pate Barnes, Yanjun Han, and Ayfer \u00d6zg\u00fcr. A geometric characterization of fisher information from quantized samples with applications to distributed statistical estimation. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 16-23, 2018.  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57 (2):764-785, 2011.  Noam Berger, Claire Kenyon, Elchanan Mossel, and Yuval Peres. Glauber dynamics on trees and hyperbolic graphs. Probability Theory and Related Fields, 131(3):311-340, 2005.  Quentin Berthet, Philippe Rigollet, et al. Optimal detection of sparse principal components  in high dimension. The Annals of Statistics, 41(4):1780-1815, 2013.  Pavel M. Bleher, Jean Ruiz, and Valentin A. Zagrebnov. On the purity of the limiting Gibbs state for the Ising model on the Bethe lattice. J. Statist. Phys., 79(1-2):473-482, 1995.  Christian Borgs, Jennifer Chayes, Elchanan Mossel, and S\u00e9bastien Roch. The kesten-stigum reconstruction bound is tight for roughly symmetric binary channels. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 518-530. IEEE, 2006.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P Woodru\ufb00. Com- munication lower bounds for statistical estimation problems via a distributed data pro- cessing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011-1020. ACM, 2016.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds for problems with planted sparse structure. In Conference on Learning Theory, pages 48-166, 2018.  Constantinos Daskalakis, Elchanan Mossel, and S\u00e9bastien Roch. Optimal phylogenetic re- construction. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 159-168. ACM, 2006.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Inference and phase transitions in the detection of modules in sparse networks. Physical Review Letters, 107(6):065701, 2011. Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation  William Evans, Claire Kenyon, Yuval Peres, and Leonard J. Schulman. Broadcasting on  trees and the ising model. Annals of Applied Probability, pages 410-433, 2000.  Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of  learning. Computing, pages 990-1002. ACM, 2018.  Uri Hadar, Jingbo Liu, Yury Polyanskiy, and Ofer Shayevitz. Communication complexity of estimating correlations. In Proceedings of the 51st ACM Symp. on Theory of Comp. (STOC), 2019.  Yanjun Han, Ayfer \u00d6zg\u00fcr, and Tsachy Weissman. Geometric lower bounds for distributed parameter estimation under communication constraints. In Conference On Learning The- ory, pages 3163-3188, 2018.  Dmitry Io\ufb00e. Extremality of the disordered state for the Ising model on general trees. In Trees (Versailles, 1995), volume 40 of Progr. Probab., pages 3-14. Birkh\u00e4user, Basel, 1996.  Harry Kesten and Bernt P. Stigum. Additional limit theorems for indecomposable multidi-  mensional Galton-Watson processes. Ann. Math. Statist., 37:1463-1481, 1966.  Harry Kesten and Bernt P. Stigum. Limit theorems for decomposable multi-dimensional  Galton-Watson processes. J. Math. Anal. Appl., 17:309-338, 1967.  Gillat Kol, Ran Raz, and Avishay Tal. Time-space hardness of learning sparse parities. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1067-1080. ACM, 2017.  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zde- borov\u00e1, and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of Sciences, 110(52):20935-20940, 2013.  Jingbo Liu, Paul Cu\ufb00, and Sergio Verd\u00fa. Key capacity with limited one-way communica- tion for product sources. In Proceedings of the 2014 IEEE International Symposium on Information Theory, pages 1146-1150, 2014.  Jingbo Liu, Paul Cu\ufb00, and Sergio Verd\u00fa. Secret key generation with one communicator and a one-shot converse via hypercontractivity. In Proceedings of the 2015 IEEE International Symposium on Information Theory (ISIT), pages 710-714, 2015.  Jingbo Liu, Paul Cu\ufb00, and Sergio Verd\u00fa. Secret key generation with limited interaction.  IEEE Transactions on Information Theory, 63(11):7358-7381, 2017.  Russell Lyons. The Ising model and percolation on trees and tree-like graphs. Communica-  tions in Mathematical Physics, 125(2):337-353, 1989.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection.  The Annals of Statistics, 43(3):1089-1116, 2015.  Laurent Massoulie. Community detection thresholds and the weak ramanujan property.  arXiv preprint arXiv:1311.3085, 2013. Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation  Marc M\u00e9zard and Andrea Montanari. Information, physics, and computation. Oxford Uni-  versity Press, 2009.  Ankur Moitra, Elchanan Mossel, and Colin Sandon. The circuit complexity of inference.  arXiv preprint arXiv:1904.05483, 2019.  Andrea Montanari. Tight bounds for ldpc and ldgm codes under map decoding.  IEEE  Transactions on Information Theory, 51(9):3221-3246, 2005.  Dana Moshkovitz and Michal Moshkovitz. Mixing implies lower bounds for space bounded  learning. In Conference on Learning Theory, pages 1516-1566, 2017.  Elchanan Mossel. Recursive reconstruction on periodic trees. Random Structures & Algo-  rithms, 13(1):81-97, 1998.  Elchanan Mossel. Phase transitions in phylogeny. Transactions of the American Mathemat-  ical Society, 356(6):2379-2404, 2004a.  Elchanan Mossel. Survey-information \ufb02ow on trees. DIMACS series in discrete mathematics  and theoretical computer science, 63:155-170, 2004b.  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction and optimal recovery of block models. In Conference on Learning Theory, pages 356-370, 2014.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted  partition model. Probability Theory and Related Fields, 162(3-4):431-461, 2015.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjec-  ture. Combinatorica, 38(3):665-708, 2018.  Judea Pearl. Probabilistic reasoning in intelligent systems. Morgan Kaufman, San Mateo,  Robin Pemantle, Yuval Peres, et al. The critical ising model on trees, concave recursions  and nonlinear capacity. The Annals of Probability, 38(1):184-206, 2010.  Ran Raz. A time-space lower bound for a large class of learning problems. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 732-742. IEEE, 2017.  Ran Raz. Fast learning requires good memory: A time-space lower bound for parity learning.  Journal of the ACM (JACM), 66(1):3, 2018.  Thomas J. Richardson and R\u00fcdiger L Urbanke. The capacity of low-density parity-check IEEETIT: IEEE Transactions on Information  codes under message-passing decoding. Theory, 47, 2001. URL citeseer.ist.psu.edu/richardson98capacity.html.  1988.  2001.Mike Steel. My Favourite Conjecture. http://www.math.canterbury.ac.nz/\u223cmathmas/conjecture.pdf,   Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation  C\u00e9dric Villani. Topics in optimal transportation, volume 338. Springer Science & Business  Media, 2008.  Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds for distributed function  computation. IEEE Transactions on Information Theory, 63(4):2314-2337, 2017.  Lenka Zdeborov\u00e1 and Florent Krzakala. Statistical physics of inference: Thresholds and  algorithms. Advances in Physics, 65(5):453-552, 2016.  Yuchen Zhang, John Duchi, Michael I. Jordan, and Martin J. Wainwright.  Information- theoretic lower bounds for distributed statistical estimation with communication con- In In Advances in Neural Information Processing Systems, pages 2328-2336, straints. 2013.  Yuchen Zhang, Martin J Wainwright, and Michael I. Jordan. Optimal prediction for sparse linear models? lower bounds for coordinate-separable m-estimators. Electronic Journal of Statistics, 11(1):752-799, 2017. "}, "The implicit bias of gradient descent on nonseparable data": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The implicit bias of gradient descent on nonseparable data", "abstract": "Gradient descent, when applied to the task of logistic regression, outputs iterates which are biased to follow a unique ray defined by the data. The direction of this ray is the maximum margin predictor of a maximal linearly separable subset of the data; the gradient descent iterates converge to this ray in direction at the rate $\\cO(\\nicefrac{\\ln\\ln t }{\\ln t})$. The ray does not pass through the origin in general, and its offset is the bounded global optimum of the risk over the remaining data; gradient descent recovers this offset at a rate $\\cO(\\nicefrac{(\\ln t)^2}{\\sqrt{t}})$.", "pdf_url": "http://proceedings.mlr.press/v99/ji19a/ji19a.pdf", "keywords": ["Implicit bias", "gradient descent", "logistic regression", "maximum margin"], "reference": "Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer Pub-  lishing Company, Incorporated, 2000.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in  Machine Learning, 2015.  Robert M. Freund, Paul Grigas, and Rahul Mazumder. Condition number analysis of logistic re-  gression, and its implications for first-order solution methods. INFORMS, 2017.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.  Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in  terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Conference on Learning Theory, pages 1305- 1320, 2015.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  Publishers, 2004.  Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-  gence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic  Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on  the Mathematical Theory of Automata, 12:615-622, 1962.  Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances in  neural information processing systems, 2016.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.  13   THE IMPLICIT BIAS OF GRADIENT DESCENT ON NONSEPARABLE DATA  References  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Jonathan Borwein and Adrian Lewis. Convex Analysis and Nonlinear Optimization. Springer Pub-  lishing Company, Incorporated, 2000.  S\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in  Machine Learning, 2015.  Robert M. Freund, Paul Grigas, and Rahul Mazumder. Condition number analysis of logistic re-  gression, and its implications for first-order solution methods. INFORMS, 2017.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.  Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in  terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer  Publishing Company, Incorporated, 2001.  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Conference on Learning Theory, pages 1305- 1320, 2015.  Indraneel Mukherjee, Cynthia Rudin, and Robert Schapire. The convergence rate of AdaBoost. In  COLT, 2011.  Publishers, 2004.  Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-  gence of gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic  Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on  the Mathematical Theory of Automata, 12:615-622, 1962.  Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances in  neural information processing systems, 2016.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012. THE IMPLICIT BIAS OF GRADIENT DESCENT ON NONSEPARABLE DATA  Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.  Matthew Streeter and Brendan McMahan. No-regret algorithms for unconstrained online convex  optimization. In Advances in neural information processing systems, 2012.  Matus Telgarsky. A primal-dual convergence analysis of boosting. JMLR, 13:561-606, 2012.  Matus Telgarsky. Margins, shrinkage, and boosting. In ICML, 2013.  "}, "An Optimal High-Order Tensor Method for Convex Optimization": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "An Optimal High-Order Tensor Method for Convex Optimization", "abstract": "This paper is concerned with finding an  optimal algorithm for minimizing a composite convex objective function. The basic setting is that the objective is the sum of two convex functions: the first function is smooth with up to the d-th order derivative information available, and the second function is possibly non-smooth, but its proximal tensor mappings can be computed approximately in an efficient manner. The problem is to find \u2013 in that setting \u2013 the best possible (optimal) iteration complexity for convex optimization. Along that line, for the smooth case (without the second non-smooth part in the objective), Nesterov (1983) proposed an optimal algorithm for the first-order methods (d=1) with iteration complexity O( 1 / k^2 ).  A high-order tensor algorithm with iteration complexity of O( 1 / k^{d+1} ) was proposed by Baes (2009) and Nesterov (2018). In this paper, we propose a new high-order tensor algorithm for the general composite case, with the iteration complexity of O( 1 / k^{(3d+1)/2} ), which matches the lower bound for the d-th order methods as established in Nesterov (2018) and Shamir et al. (2018), and hence is optimal. Our approach is based on the  Accelerated Hybrid Proximal Extragradient (A-HPE) framework proposed in Monteiro and Svaiter (2013), where a bisection procedure is installed for each A-HPE iteration. At each bisection step a proximal tensor subproblem is approximately solved, and the total number of bisection steps per A-HPE iteration is bounded by a logarithmic factor in the precision required.", "pdf_url": "http://proceedings.mlr.press/v99/jiang19a/jiang19a.pdf", "keywords": ["convex optimization", "tensor method", "acceleration", "iteration complexity"], "reference": "Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, published online, 2018.  M. Baes. Estimate sequence methods: extensions and approximations.  Institute for Operations  Research, ETH, Zrich, Switzerland, 2009.  S. Bubeck, Q. Jiang, Y.T. Lee, Y. Li, and A. Sidford. Near-optimal method for highly smooth convex  optimization. ArXiv Preprint: 1812.08026, 2018.  B. Bullins. Fast minimization of structured convex quartics. ArXiv Preprint: 1812.10349, 2018.  A. Gasnikov, P. Dvurechensky, E. Gorbunov, D. Kovalev, A. Mohhamed, E. Chernousova, and C.A. Uribe. The global rate of convergence for optimal tensor methods in smooth convex optimization. arXiv:1809.00382, 2018.  B. Jiang, T. Lin, and S. Zhang. A unified adaptive tensor approximation scheme to accelerate  composite convex optimization. arxiv:1811.02427, 2018.  R.D.C. Monteiro and B.F. Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2): 1092-1125, 2013.  Yu. Nesterov. A method for unconstrained convex minimization problem with the rate of conver-  gence o(1/k2). Doklady AN SSSR, translated as Soviet Math.Docl., 269:543-547, 1983.  Yu. Nesterov. Accelerating the cubic regularization of newton\u2019s method on convex problems. Math-  ematical Programming, 112(1):159-181, 2008.  Yu. Nesterov. Implementable tensor methods in unconstrained convex optimization. CORE Dis- cussion Paper 2018/05, Catholic University of Louvain, Center for Operations Research and Econometrics (CORE), 2018.  3   AN OPTIMAL HIGH-ORDER TENSOR METHOD FOR CONVEX OPTIMIZATION  of bisection steps - each calling to solve a convex tensor subproblem - is upper bounded by a logarithmic factor in the inverse of the required precision. Our bisection procedure is similar to the one proposed in Monteiro and Svaiter (2013) for the case d = 2; however, a key modification is applied which enables the removal of the so-called \u201cbracketing stage\u201d used in Monteiro and Svaiter (2013).  References  Y. Arjevani, O. Shamir, and R. Shiff. Oracle complexity of second-order methods for smooth convex  optimization. Mathematical Programming, published online, 2018.  M. Baes. Estimate sequence methods: extensions and approximations.  Institute for Operations  Research, ETH, Zrich, Switzerland, 2009.  S. Bubeck, Q. Jiang, Y.T. Lee, Y. Li, and A. Sidford. Near-optimal method for highly smooth convex  optimization. ArXiv Preprint: 1812.08026, 2018.  B. Bullins. Fast minimization of structured convex quartics. ArXiv Preprint: 1812.10349, 2018.  A. Gasnikov, P. Dvurechensky, E. Gorbunov, D. Kovalev, A. Mohhamed, E. Chernousova, and C.A. Uribe. The global rate of convergence for optimal tensor methods in smooth convex optimization. arXiv:1809.00382, 2018.  B. Jiang, T. Lin, and S. Zhang. A unified adaptive tensor approximation scheme to accelerate  composite convex optimization. arxiv:1811.02427, 2018.  R.D.C. Monteiro and B.F. Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2): 1092-1125, 2013.  Yu. Nesterov. A method for unconstrained convex minimization problem with the rate of conver-  gence o(1/k2). Doklady AN SSSR, translated as Soviet Math.Docl., 269:543-547, 1983.  Yu. Nesterov. Accelerating the cubic regularization of newton\u2019s method on convex problems. Math-  ematical Programming, 112(1):159-181, 2008.  Yu. Nesterov. Implementable tensor methods in unconstrained convex optimization. CORE Dis- cussion Paper 2018/05, Catholic University of Louvain, Center for Operations Research and Econometrics (CORE), 2018. "}, "Parameter-Free Online Convex Optimization with Sub-Exponential Noise": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Parameter-Free Online Convex Optimization with Sub-Exponential Noise", "abstract": "We consider the problem of unconstrained online convex optimization (OCO) with sub-exponential noise, a strictly more general problem than the standard OCO. In this setting, the learner receives a subgradient of the loss functions corrupted by sub-exponential noise and strives to achieve optimal regret guarantee, without knowledge of the competitor norm, i.e., in a parameter-free way. Recently, Cutkosky and Boahen (COLT 2017) proved that, given unbounded subgradients, it is impossible to guarantee a sublinear regret due to an exponential penalty. This paper shows that it is possible to go around the lower bound by allowing the observed subgradients to be unbounded via stochastic noise. However, the presence of unbounded noise in unconstrained OCO is challenging; existing algorithms do not provide near-optimal regret bounds or fail to have a guarantee. So, we design a novel parameter-free OCO algorithm for Banach space, which we call BANCO, via a reduction to betting on noisy coins. We show that BANCO achieves the optimal regret rate in our problem. Finally, we show the application of our results to obtain a parameter-free locally private stochastic subgradient descent algorithm, and the connection to the law of iterated logarithms.", "pdf_url": "http://proceedings.mlr.press/v99/jun19a/jun19a.pdf", "keywords": ["Parameter-free", "online convex optimization", "unconstrained", "differentially-private stochastic subgradient descent"], "reference": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans. on Information Theory, 58(5):3235, 2012.  A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Proc. of the  Conference on Uncertainty in Artificial Intelligence (UAI), pages 117-125, 2010.  A. Cutkosky and K. Boahen. Online learning without prior information. In Proc. of the Conference  On Learning Theory (COLT), pages 643-677, 2017.  A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach  spaces. In Proc. of the Conference On Learning Theory (COLT), pages 1493-1529, 2018.  J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Privacy aware learning. Journal of the ACM, 61(6):  38, 2014.  R. Durrett. Probability: theory and examples. Cambridge University Press, 2010.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.  D. J. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning. Information Processing Systems (NeurIPS), pages 3375-3383, 2015.  In Advances in Neural  D. J. Foster, S. Kale, M. Mohri, and K. Sridharan. Parameter-free online learning via model selection.  In Advances in Neural Information Processing Systems (NeurIPS), pages 6020-6030, 2017.  D. J. Foster, A. Rakhlin, and K. Sridharan. Online learning: Sufficient statistics and the Burkholder  method. In Proc. of the Conference On Learning Theory (COLT), pages 3028-3064, 2018.  Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian  random vectors. Electronic Communications in Probability, 17, 2012.  S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn  privately? SIAM Journal on Computing, 40(3):793-826, 2011.  W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games.  In Proc. of the Conference On Learning Theory (COLT), pages 1155-1175, 2015.  Wouter M. Koolen. Implementing squint, 2015. URL http://blog.wouterkoolen.info/  Squint_implementation/post.html.  W. Kot\u0142owski. Scale-invariant unconstrained online learning. In Proc. of the International Conference  on Algorithmic Learning Theory (ALT), 2017.  N. Littlestone. From on-line to batch learning. In Proc. of the Conference On Learning Theory  (COLT), pages 269-284, 1989.  5(1):41-65, 1972.  Y. L. Luke. Inequalities for generalized hypergeometric functions. Journal of Approximation Theory,  13   PARAMETER-FREE ONLINE CONVEX OPTIMIZATION WITH SUB-EXPONENTIAL NOISE  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans. on Information Theory, 58(5):3235, 2012.  A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Proc. of the  Conference on Uncertainty in Artificial Intelligence (UAI), pages 117-125, 2010.  A. Cutkosky and K. Boahen. Online learning without prior information. In Proc. of the Conference  On Learning Theory (COLT), pages 643-677, 2017.  A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach  spaces. In Proc. of the Conference On Learning Theory (COLT), pages 1493-1529, 2018.  J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Privacy aware learning. Journal of the ACM, 61(6):  38, 2014.  R. Durrett. Probability: theory and examples. Cambridge University Press, 2010.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.  D. J. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning. Information Processing Systems (NeurIPS), pages 3375-3383, 2015.  In Advances in Neural  D. J. Foster, S. Kale, M. Mohri, and K. Sridharan. Parameter-free online learning via model selection.  In Advances in Neural Information Processing Systems (NeurIPS), pages 6020-6030, 2017.  D. J. Foster, A. Rakhlin, and K. Sridharan. Online learning: Sufficient statistics and the Burkholder  method. In Proc. of the Conference On Learning Theory (COLT), pages 3028-3064, 2018.  Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian  random vectors. Electronic Communications in Probability, 17, 2012.  S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn  privately? SIAM Journal on Computing, 40(3):793-826, 2011.  W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games.  In Proc. of the Conference On Learning Theory (COLT), pages 1155-1175, 2015.  Wouter M. Koolen. Implementing squint, 2015. URL http://blog.wouterkoolen.info/  Squint_implementation/post.html.  W. Kot\u0142owski. Scale-invariant unconstrained online learning. In Proc. of the International Conference  on Algorithmic Learning Theory (ALT), 2017.  N. Littlestone. From on-line to batch learning. In Proc. of the Conference On Learning Theory  (COLT), pages 269-284, 1989.  5(1):41-65, 1972.  Y. L. Luke. Inequalities for generalized hypergeometric functions. Journal of Approximation Theory, PARAMETER-FREE ONLINE CONVEX OPTIMIZATION WITH SUB-EXPONENTIAL NOISE  J. Matou\u0161ek. Lectures on discrete geometry, volume 212. Springer New York, 2002.  H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimization.  In Advances in Neural Information Processing Systems (NeurIPS), pages 2724-2732, 2013.  H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations. In Proc. of the Conference On Learning Theory (COLT), pages 1020-1039, 2014.  F. Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information Processing  Systems (NeurIPS), pages 1806-1814, 2013.  F. Orabona and D. P\u00e1l. Coin betting and parameter-free online learning. In Advances in Neural  Information Processing Systems (NeurIPS), pages 577-585. 2016.  F. Orabona and T. Tommasi. Training deep networks without learning rates through coin betting. In  Advances in Neural Information Processing Systems (NeurIPS), pages 2160-2170, 2017.  I. Pinelis. Rosenthal-type inequalities for martingales in 2-smooth Banach spaces. Theory of  Probability & Its Applications, 59(4):699-706, 2015.  A. Rakhlin and K. Sridharan. On equivalence of martingale tail bounds and deterministic regret inequalities. In Proc. of the Conference On Learning Theory (COLT), pages 1704-1722, 2017.  S. Song, K. Chaudhuri, and A. Sarwate. Learning from data with heterogeneous noise using SGD. In Proc. of International Conference on Artificial Intelligence and Statistics (AISTATS), pages 894-902, 2015.  N. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In Advances in  Neural Information Processing Systems (NeurIPS), pages 2645-2653, 2011.  L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American  Statistical Association, 105(489):375-389, 2010.  X. Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J. Naughton. Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In Proc. of the 2017 ACM International Conference on Management of Data, pages 1307-1322. ACM, 2017. PARAMETER-FREE ONLINE CONVEX OPTIMIZATION WITH SUB-EXPONENTIAL NOISE  "}, "Sample complexity of partition identification using multi-armed bandits": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sample complexity of partition identification using multi-armed bandits", "abstract": "Given a vector of probability distributions, or arms, each of which can be sampled independently, we consider the problem of identifying the partition to which this vector belongs from a finitely partitioned universe of such vector of distributions. We study this as a pure exploration problem in multi-armed bandit settings and develop sample complexity bounds on the total mean number of samples required for identifying the correct partition with high probability.  This framework subsumes  well studied problems such as finding the best arm or the best few arms. We consider distributions belonging to the single parameter exponential family and primarily consider partitions  where the vector of means of arms lie either in a given  set or its complement. The sets considered  correspond to distributions where there exists  a mean above a specified threshold, where the set is a half space and where either the set or its complement  is a polytope, or more generally, a convex set.  In these settings, we characterize the lower bounds on mean number of samples for each arm  highlighting  their dependence on the problem geometry. Further, inspired by the lower bounds, we propose algorithms  that can match these bounds asymptotically with decreasing probability of error.  Applications of this framework may be diverse.  We briefly discuss a few associated with  simulation in finance.", "pdf_url": "http://proceedings.mlr.press/v99/juneja19a/juneja19a.pdf", "keywords": ["multi-armed bandits", "best arm identification", "pure exploration", "partition identification"], "reference": "Albert, A.E., The sequential design of experiments for infinitely many states of nature The Annals  of Mathematical Statistics, pp.774-799. 1961.  Jean-Yves Audibert and S\u00e9bastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Robert Eric Bechhofer, Jack Kiefer, and Milton Sobel. Sequential identification and ranking proce- dures: with special reference to Koopman-Darmois populations, volume 3. University of Chicago Press, 1968.  Mark Broadie, Yiping Du, and Ciamac C Moallemi. Efficient risk estimation via nested sequential  simulation. Management Science, 57(6):1172-1194, 2011.  S\u00e9bastien Bubeck, R\u00e9mi Munos, and Gilles Stoltz.  Pure exploration in finitely-armed and  continuous-armed bandits. Theor. Comput. Sci., 412(19):1832-1852, 2011.  A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for sequential allocation problems.  Advances in Applied Mathematics, 17(2):122-142, 1996.  Olivier Capp\u00e9, Aur\u00e9lien Garivier, Odalric-Ambrym Maillard, R\u00e9mi Munos, Gilles Stoltz, et al. Kullback-leibler upper confidence bounds for optimal sequential allocation. The Annals of Statis- tics, 41(3):1516-1541, 2013.  C.H. Chen, J. Lin, E. Yucesan, and S.E. Chick. Simulation budget allocation for further enhancing the efficiency of ordinal optimization. Discrete Event Dynamic Systems, 10(3):251-270, 2000.  Herman Chernoff. Sequential design of experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  L Dai. Convergence properties of ordinal comparison in the simulation of discrete event dynamic  systems. Journal of Optimization Theory and Applications, 91(2):363-388, 1996.  Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications. 1998. Applications  of Mathematics, 38, 2011.  Darrell Duffie. Dynamic asset pricing theory. Princeton University Press, 2010.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7(Jun):1079-1105, 2006.  Anthony V. Fiacco and Yo Ishizuka. Sensitivity and stability analysis for nonlinear programming.  Annals of Operations Research, 27(1):215-235, Dec 1990.  Aur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In  Conference on Learning Theory, pages 998-1027, 2016.  Peter Glynn and Sandeep Juneja. A large deviations perspective on ordinal optimization. In Pro- ceedings of the 36th conference on Winter simulation, pages 577-585. Winter Simulation Con- ference, 2004.  13   SAMPLE COMPLEXITY OF PARTITION IDENTIFICATION  References  Albert, A.E., The sequential design of experiments for infinitely many states of nature The Annals  of Mathematical Statistics, pp.774-799. 1961.  Jean-Yves Audibert and S\u00e9bastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Robert Eric Bechhofer, Jack Kiefer, and Milton Sobel. Sequential identification and ranking proce- dures: with special reference to Koopman-Darmois populations, volume 3. University of Chicago Press, 1968.  Mark Broadie, Yiping Du, and Ciamac C Moallemi. Efficient risk estimation via nested sequential  simulation. Management Science, 57(6):1172-1194, 2011.  S\u00e9bastien Bubeck, R\u00e9mi Munos, and Gilles Stoltz.  Pure exploration in finitely-armed and  continuous-armed bandits. Theor. Comput. Sci., 412(19):1832-1852, 2011.  A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for sequential allocation problems.  Advances in Applied Mathematics, 17(2):122-142, 1996.  Olivier Capp\u00e9, Aur\u00e9lien Garivier, Odalric-Ambrym Maillard, R\u00e9mi Munos, Gilles Stoltz, et al. Kullback-leibler upper confidence bounds for optimal sequential allocation. The Annals of Statis- tics, 41(3):1516-1541, 2013.  C.H. Chen, J. Lin, E. Yucesan, and S.E. Chick. Simulation budget allocation for further enhancing the efficiency of ordinal optimization. Discrete Event Dynamic Systems, 10(3):251-270, 2000.  Herman Chernoff. Sequential design of experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  L Dai. Convergence properties of ordinal comparison in the simulation of discrete event dynamic  systems. Journal of Optimization Theory and Applications, 91(2):363-388, 1996.  Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications. 1998. Applications  of Mathematics, 38, 2011.  Darrell Duffie. Dynamic asset pricing theory. Princeton University Press, 2010.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7(Jun):1079-1105, 2006.  Anthony V. Fiacco and Yo Ishizuka. Sensitivity and stability analysis for nonlinear programming.  Annals of Operations Research, 27(1):215-235, Dec 1990.  Aur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In  Conference on Learning Theory, pages 998-1027, 2016.  Peter Glynn and Sandeep Juneja. A large deviations perspective on ordinal optimization. In Pro- ceedings of the 36th conference on Winter simulation, pages 577-585. Winter Simulation Con- ference, 2004. SAMPLE COMPLEXITY OF PARTITION IDENTIFICATION  Peter Glynn and Sandeep Juneja. arXiv:1507.04564v3, 1, 2018.  Selecting the best system and multi-armed bandits.  Michael B Gordy and Sandeep Juneja. Nested simulation in portfolio risk measurement. Manage-  ment Science, 56(10):1833-1848, 2010.  Yu-Chi Ho, R_S Sreenivas, and P Vakili. Ordinal optimization of deds. Discrete Event Dynamic  Systems, 2(1):61-88, 1992.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00e9bastien Bubeck.  lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits. In Conference on Learning Theory, pages 423- 439, 2014.  Christopher Jennison, Iain M Johnstone, and Bruce W Turnbull. Asymptotically optimal procedures for sequential adaptive selection of the best of several normal means. In Statistical decision theory and related topics III, pages 55-86. Elsevier, 1982.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection in  stochastic multi-armed bandits. In ICML, volume 12, pages 655-662, 2012.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identifi- cation in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1-42, 2016.  Emilie Kaufmann, W. M. Koolen, and Aur\u00e9lien Garivier. Sequential test for the lowest mean: From Thompson to Murphy sampling. In Advances in Neural Information Processing Systems, pages 6332-6342, 2018.  Seong-Hee Kim and Barry L Nelson. A fully sequential procedure for indifference-zone selection in simulation. ACM Transactions on Modeling and Computer Simulation (TOMACS), 11(3): 251-273, 2001.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in Applied Mathematics, 6(1):4-22, 1985.  Marco L\u00f3pez and Georg Still. Semi-infinite programming. European Journal of Operational Re-  search, 180(2):491-518, 2007.  S. Magureanu, R. Combes, and A. Proutiere. Lipschitz bandits: Regret lower bounds and optimal  algorithms. In 27th Conference on Learning Theory, 2014.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. Journal of Machine Learning Research, 5:623-648, 2004.  Edward Paulson. A sequential procedure for selecting the population with the largest mean from k  normal populations. The Annals of Mathematical Statistics, 35(1):174-180, 1964. SAMPLE COMPLEXITY OF PARTITION IDENTIFICATION  Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third  edition, 1976. International Series in Pure and Applied Mathematics.  Daniel Russo. Simple Bayesian algorithms for best arm identification. In Conference on Learning  Theory, pages 1417-1418, 2016.  Science & Business Media, 2004.  S. E. Shreve. Stochastic calculus for finance II: Continuous-time models, volume 11. Springer  Acknowledgments  We thank the referees for their helpful comments.  "}, "Privately Learning High-Dimensional Distributions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Privately Learning High-Dimensional Distributions", "abstract": "We present novel, computationally efficient, and differentially private algorithms for two fundamental high-dimensional learning problems: learning a multivariate Gaussian and learning a product distribution over the Boolean hypercube in total variation distance.  The sample complexity of our algorithms nearly matches the sample complexity of the optimal non-private learners for these tasks in a wide range of parameters, showing that privacy comes essentially for free for these problems.  In particular, in contrast to previous approaches, our algorithm for learning Gaussians does not require strong a priori bounds on the range of the parameters.  Our algorithms introduce a novel technical approach to reducing the sensitivity of the estimation procedure that we call recursive private preconditioning.", "pdf_url": "http://proceedings.mlr.press/v99/kamath19a/kamath19a.pdf", "keywords": ["Privacy", "learning", "Gaussian", "product distribution1"], "reference": "Jayadev Acharya, Gautam Kamath, Ziteng Sun, and Huanyu Zhang. Inspectre: Privately estimating the unseen. In Proceedings of the 35th International Conference on Machine Learning, ICML \u201918, pages 30-39. JMLR, Inc., 2018a.  Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and closeness of discrete distributions. In Advances in Neural Information Processing Systems 31, NeurIPS \u201918, pages 6878-6891. Curran Associates, Inc., 2018b.  Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity and closeness testing of discrete distributions. In Proceedings of the 35th International Conference on Machine Learning, ICML \u201918, pages 169-178. JMLR, Inc., 2018.  Mitali Bafna and Jonathan Ullman. The price of selection in differential privacy. In Proceedings of  the 30th Annual Conference on Learning Theory, COLT \u201917, pages 151-168, 2017.  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient In Proceedings of the 55th Annual IEEE Symposium on algorithms and tight error bounds. Foundations of Computer Science, FOCS \u201914, pages 464-473, Washington, DC, USA, 2014. IEEE Computer Society.  Amos Beimel, Hai Brenner, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. Machine Learning, 94(3):401-437, 2014.  Andrew C. Berry. The accuracy of the Gaussian approximation to the sum of independent variates.  Transactions of the American Mathematical Society, 49(1):122-136, 1941.  Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Proceedings of the 14th Conference on Theory of Cryptography, TCC \u201916-B, pages 635-658, Berlin, Heidelberg, 2016. Springer.  Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate In Proceedings of the 46th Annual ACM Symposium on the Theory of  differential privacy. Computing, STOC \u201914, pages 1-10, New York, NY, USA, 2014. ACM.  13   PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Acknowledgments  Work done when GK was a graduate student at MIT, supported by NSF Award CCF-1617730, CCF-1650733, CCF-1741137, and ONR N00014-12-1-0999, and when a Microsoft Research Fellow, as part of the Simons-Berkeley Research Fellowship program. Work done when JL was a graduate student at MIT, supported by NSF Award CCF-1453261 (CAREER), CCF-1565235, a Google Faculty Research Award, and an NSF Graduate Research Fellowship, and when a VMware Research Fellow, as part of the Simons-Berkeley Research Fellowship program. Part of this work was also performed while JL was an intern at Google. VS supported by NSF award CCF-1750640. JU supported by NSF awards CCF-1750640 and NSF awards CCF-1718088 and CNS-1816028, and a Google Faculty Research Award.  References  Jayadev Acharya, Gautam Kamath, Ziteng Sun, and Huanyu Zhang. Inspectre: Privately estimating the unseen. In Proceedings of the 35th International Conference on Machine Learning, ICML \u201918, pages 30-39. JMLR, Inc., 2018a.  Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and closeness of discrete distributions. In Advances in Neural Information Processing Systems 31, NeurIPS \u201918, pages 6878-6891. Curran Associates, Inc., 2018b.  Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity and closeness testing of discrete distributions. In Proceedings of the 35th International Conference on Machine Learning, ICML \u201918, pages 169-178. JMLR, Inc., 2018.  Mitali Bafna and Jonathan Ullman. The price of selection in differential privacy. In Proceedings of  the 30th Annual Conference on Learning Theory, COLT \u201917, pages 151-168, 2017.  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient In Proceedings of the 55th Annual IEEE Symposium on algorithms and tight error bounds. Foundations of Computer Science, FOCS \u201914, pages 464-473, Washington, DC, USA, 2014. IEEE Computer Society.  Amos Beimel, Hai Brenner, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. Machine Learning, 94(3):401-437, 2014.  Andrew C. Berry. The accuracy of the Gaussian approximation to the sum of independent variates.  Transactions of the American Mathematical Society, 49(1):122-136, 1941.  Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Proceedings of the 14th Conference on Theory of Cryptography, TCC \u201916-B, pages 635-658, Berlin, Heidelberg, 2016. Springer.  Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate In Proceedings of the 46th Annual ACM Symposium on the Theory of  differential privacy. Computing, STOC \u201914, pages 1-10, New York, NY, USA, 2014. ACM. PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. In Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201915, pages 634-649, Washington, DC, USA, 2015. IEEE Computer Society.  Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in differential privacy. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201917, pages 1306-1325, Philadelphia, PA, USA, 2017. SIAM.  Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Priv\u2019it: Private and sample efficient identity testing. In Proceedings of the 34th International Conference on Machine Learning, ICML \u201917, pages 635-644. JMLR, Inc., 2017.  T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for  parameter estimation with differential privacy. arXiv preprint arXiv:1902.04495, 2019.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceedings of the 49th Annual ACM Symposium on the Theory of Computing, STOC \u201917, pages 47-60, New York, NY, USA, 2017. ACM.  Aref N. Dajani, Amy D. Lauger, Phyllis E. Singer, Daniel Kifer, Jerome P. Reiter, Ashwin Machanava- jjhala, Simson L. Garfinkel, Scot A. Dahl, Matthew Graham, Vishesh Karwa, Hang Kim, Philip Lelerc, Ian M. Schmutte, William N. Sexton, Lars Vilhuber, and John M. Abowd. The moderniza- tion of statistical disclosure limitation at the U.S. census bureau, 2017. Presented at the September 2017 meeting of the Census Scientific Advisory Committee.  Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The minimax learning rate of normal and  Ising undirected graphical models. arXiv preprint arXiv:1806.06887, 2018.  Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured discrete distributions. In Advances in Neural Information Processing Systems 28, NIPS \u201915, pages 2566-2574. Curran Associates, Inc., 2015.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201916, pages 655-664, Washington, DC, USA, 2016. IEEE Computer Society.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. In Proceedings of the 34th International Conference on Machine Learning, ICML \u201917, pages 999-1008. JMLR, Inc., 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a Gaussian: Getting optimal error, efficiently. In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201918, Philadelphia, PA, USA, 2018. SIAM.  Differential  Privacy  privacy https://machinelearning.apple.com/docs/learning-with-privacy-at-scale/appledifferentialprivacysystem.pdf, December 2017.  Learning  Apple.  Team,  scale.  with  at PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Proceedings of the 22nd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS \u201903, pages 202-210, New York, NY, USA, 2003. ACM.  Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the 41st Annual ACM Symposium on the Theory of Computing, STOC \u201909, pages 371-380, New York, NY, USA, 2009. ACM.  Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy.  arXiv preprint  arXiv:1603.01887, 2016.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC \u201906, pages 265-284, Berlin, Heidelberg, 2006. Springer.  Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil Vadhan. On the complexity of differentially private data release: Efficient algorithms and hardness results. In Proceedings of the 41st Annual ACM Symposium on the Theory of Computing, STOC \u201909, pages 381-390, New York, NY, USA, 2009. ACM.  Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy. In Proceed- ings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 51-60, Washington, DC, USA, 2010. IEEE Computer Society.  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis. In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing, STOC \u201914, pages 11-20, New York, NY, USA, 2014. ACM.  Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust trace- ability from trace amounts. In Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201915, pages 650-669, Washington, DC, USA, 2015. IEEE Computer Society.  Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a survey of attacks  on private data. Annual Review of Statistics and Its Application, 4(1):61-84, 2017.  \u00b4Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM Conference on Computer and Communications Security, CCS \u201914, pages 1054-1067, New York, NY, USA, 2014. ACM.  Carl-Gustaf Esseen. On the Liapounoff limit of error in the theory of probability. Arkiv f\u00a8or matematik,  astronomi och fysik, 28A(2):1-19, 1942.  Marco Gaboardi and Ryan Rogers. Local private hypothesis testing: Chi-square tests. In Proceedings of the 35th International Conference on Machine Learning, ICML \u201918, pages 1626-1635. JMLR, Inc., 2018.  Marco Gaboardi, James Honaker, Gary King, Jack Murtagh, Kobbi Nissim, Jonathan Ullman, and Salil Vadhan. Psi (\u03c8): A private data sharing interface. arXiv preprint arXiv:1609.04340, 2016a. PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Marco Gaboardi, Hyun-Woo Lim, Ryan M. Rogers, and Salil P. Vadhan. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In Proceedings of the 33rd International Conference on Machine Learning, ICML \u201916, pages 1395-1403. JMLR, Inc., 2016b.  Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the 42nd Annual ACM Symposium on the Theory of Computing, STOC \u201910, pages 705-714, New York, NY, USA, 2010. ACM.  Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis is hard. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201914, pages 454-463, Washington, DC, USA, 2014. IEEE Computer Society.  Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS Genetics, 4(8):1-9, 2008.  Kazuya Kakizaki, Jun Sakuma, and Kazuto Fukuchi. Differentially private chi-squared test by unit circle mechanism. In Proceedings of the 34th International Conference on Machine Learning, ICML \u201917, pages 1761-1770. JMLR, Inc., 2017.  Gautam Kamath. A lower bound for private covariance estimation. Note, available upon request,  2019.  Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional  distributions. arXiv preprint arXiv:1805.00216, 2018.  Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals.  In Proceedings of the 9th Conference on Innovations in Theoretical Computer Science, ITCS \u201918, pages 44:1-44:9, Dagstuhl, Germany, 2018. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.  Daniel Kifer and Ryan M. Rogers. A new class of private chi-square tests. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS \u201917, pages 991-1000. JMLR, Inc., 2017.  Lucas Kowalczyk, Tal Malkin, Jonathan Ullman, and Mark Zhandry. Strong hardness of privacy from weak traitor tracing. In Proceedings of the 14th Conference on Theory of Cryptography, TCC \u201916-B, pages 659-689, Berlin, Heidelberg, 2016. Springer.  Lucas Kowalczyk, Tal Malkin, Jonathan Ullman, and Daniel Wichs. Hardness of non-interactive differential privacy from one-way functions. In Proceedings of the 38th Annual International Cryptology Conference, CRYPTO \u201918, Berlin, Heidelberg, 2018. Springer.  Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201916, pages 665-674, Washington, DC, USA, 2016. IEEE Computer Society.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model  selection. The Annals of Statistics, 28(5):1302-1338, 2000. PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Prashanth Mohan, Abhradeep Thakurta, Elaine Shi, Dawn Song, and David Culler. GUPT: Privacy preserving data analysis made easy. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201912, pages 349-360, New York, NY, USA, 2012. ACM.  Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the 39th Annual ACM Symposium on the Theory of Computing, STOC \u201907, pages 75-84, New York, NY, USA, 2007. ACM.  Or Sheffet. Differentially private ordinary least squares. In Proceedings of the 34th International  Conference on Machine Learning, ICML \u201917, pages 3105-3114. JMLR, Inc., 2017.  Or Sheffet. Locally private hypothesis testing. In Proceedings of the 35th International Conference  on Machine Learning, ICML \u201918, pages 4605-4614. JMLR, Inc., 2018.  I.G. Shevtsova. An improvement of convergence rate estimates in the Lyapunov theorem. Doklady  Mathematics, 82(3):862-864, 2010.  Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In Proceedings of the 38th IEEE Symposium on Security and Privacy, SP \u201917, pages 3-18, Washington, DC, USA, 2017. IEEE Computer Society.  Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing, STOC \u201911, pages 813-822, New York, NY, USA, 2011. ACM.  Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the presence of arbitrary outliers. In Proceedings of the 9th Conference on Innovations in Theoretical Computer Science, ITCS \u201918, pages 45:1-45:21, Dagstuhl, Germany, 2018. Schloss Dagstuhl- Leibniz-Zentrum fuer Informatik.  Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of preventing false discovery. In Proceedings of the 28th Annual Conference on Learning Theory, COLT \u201915, pages 1588-1628, 2015.  Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. The  Journal of Privacy and Confidentiality, 7(2):3-22, 2017a.  Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. In Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201917, pages 552-563, Washington, DC, USA, 2017b. IEEE Computer Society.  Terence Tao. Topics in random matrix theory, volume 132 of Graduate Studies in Mathematics.  American Mathematical Society, Providence, RI, 2012.  Jonathan Ullman. Answering n2+o(1) counting queries with differential privacy is hard. SIAM  Journal on Computing, 45(2):473-496, 2016.  Jonathan Ullman and Salil Vadhan. PCPs and the hardness of generating private synthetic data. In Proceedings of the 8th Conference on Theory of Cryptography, TCC \u201911, pages 400-416, Berlin, Heidelberg, 2011. Springer. PRIVATELY LEARNING HIGH-DIMENSIONAL DISTRIBUTIONS  Yue Wang, Jaewoo Lee, and Daniel Kifer. Revisiting differentially private hypothesis tests for  categorical data. arXiv preprint arXiv:1511.03376, 2015.  "}, "On Communication Complexity of Classification Problems": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "On Communication Complexity of Classification Problems", "abstract": "This work studies distributed learning in the spirit of Yao\u2019s model of communication complexity: consider a two-party setting, where each of the players gets a list of labelled examples and they communicate in order to jointly perform some learning task. To naturally fit into the framework of learning theory, the players can send each other examples (as well as bits) where each example/bit costs one unit of communication.   This enables a uniform treatment of infinite classes such as half-spaces in $\\R^d$, which are ubiquitous in machine learning.  We study several fundamental questions in this model.  For example, we provide combinatorial characterizations of the classes that can be learned with efficient communication in the proper-case as well as in the improper-case. These findings imply unconditional separations in this context between various  learning tasks, e.g. realizable versus agnostic learning, proper versus improper learning, etcetera. %They also imply lower bounds that match the performance %of algorithm from previous works. The derivation of these results hinges on a type of decision problems we term \u201c{\\it realizability problems}\u201d where the goal is deciding whether a distributed input sample is consistent with an hypothesis from a pre-specified class. From a technical perspective,  the protocols we devise (i.e. the upper bounds) are based on ideas from machine learning and the impossibility results (i.e. the lower bounds) are based on ideas from communication complexity.", "pdf_url": "http://proceedings.mlr.press/v99/kane19a/kane19a.pdf", "keywords": [], "reference": "Harold Abelson. Lower bounds on information transfer in distributed computations. J. ACM, 27 (2):384-392, 1980. doi: 10.1145/322186.322200. URL http://doi.acm.org/10.1145/ 322186.322200.  Alekh Agarwal and John C. Duchi. Distributed delayed stochastic optimization. In Proceedings of the 51th IEEE Conference on Decision and Control, CDC 2012, December 10-13, 2012, Maui, HI, USA, pages 5451-5452, 2012. doi: 10.1109/CDC.2012.6426626.  Alfred V. Aho, Jeffrey D. Ullman, and Mihalis Yannakakis. On notions of information transfer in VLSI circuits. In Proceedings of the 15th Annual ACM Symposium on Theory of Computing, 25-27 April, 1983, Boston, Massachusetts, USA, pages 133-139, 1983. doi: 10.1145/800061. 808742. URL http://doi.acm.org/10.1145/800061.808742.  Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learn- In Advances in Neural Information Processing Systems 28: Annual ing and optimization. Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 1756-1764, 2015. URL http://papers.nips.cc/paper/ 5731-communication-complexity-of-distributed-convex-learning-and-optimization.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Agnostic distribution learning via com- pression. CoRR, abs/1710.05209, 2017. URL http://arxiv.org/abs/1710.05209.  Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, com- In COLT 2012 - The 25th Annual Conference on Learn- munication complexity and privacy. ing Theory, June 25-27, 2012, Edinburgh, Scotland, pages 26.1-26.22, 2012. URL http: //www.jmlr.org/proceedings/papers/v23/balcan12a/balcan12a.pdf.  Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V. Vazirani. Learning economic parameters from revealed preferences. CoRR, abs/1407.7937, 2014. URL http: //arxiv.org/abs/1407.7937.  Shai Ben-David and Ami Litman. Combinatorial variability of vapnik-chervonenkis classes with applications to sample compression schemes. Discrete Applied Mathematics, 86(1):3- 25, 1998. doi: 10.1016/S0166-218X(98)00000-6. URL https://doi.org/10.1016/ S0166-218X(98)00000-6.  Shai Ben-David and Ruth Urner. On version space compression. In Algorithmic Learning Theory - 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings, pages  13   ON COMMUNICATION COMPLEXITY OF CLASSIFICATION PROBLEMS  Acknowledgements. D.K is supported by NSF Award CCF-1553288 (CAREER) and a Sloan Re- search Fellowship. A.Y is supported by ISF grant 1162/15. This work was done while the author was visiting the Simons Institute for the Theory of Computing. S.M. is supported by the Simons Foundation and the NSF; part of this project was carried while the author was at the Institute for Advanced Study and was supported by the National Science Foundation under agreement No. CCF- 1412958.  References  Harold Abelson. Lower bounds on information transfer in distributed computations. J. ACM, 27 (2):384-392, 1980. doi: 10.1145/322186.322200. URL http://doi.acm.org/10.1145/ 322186.322200.  Alekh Agarwal and John C. Duchi. Distributed delayed stochastic optimization. In Proceedings of the 51th IEEE Conference on Decision and Control, CDC 2012, December 10-13, 2012, Maui, HI, USA, pages 5451-5452, 2012. doi: 10.1109/CDC.2012.6426626.  Alfred V. Aho, Jeffrey D. Ullman, and Mihalis Yannakakis. On notions of information transfer in VLSI circuits. In Proceedings of the 15th Annual ACM Symposium on Theory of Computing, 25-27 April, 1983, Boston, Massachusetts, USA, pages 133-139, 1983. doi: 10.1145/800061. 808742. URL http://doi.acm.org/10.1145/800061.808742.  Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learn- In Advances in Neural Information Processing Systems 28: Annual ing and optimization. Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 1756-1764, 2015. URL http://papers.nips.cc/paper/ 5731-communication-complexity-of-distributed-convex-learning-and-optimization.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Agnostic distribution learning via com- pression. CoRR, abs/1710.05209, 2017. URL http://arxiv.org/abs/1710.05209.  Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, com- In COLT 2012 - The 25th Annual Conference on Learn- munication complexity and privacy. ing Theory, June 25-27, 2012, Edinburgh, Scotland, pages 26.1-26.22, 2012. URL http: //www.jmlr.org/proceedings/papers/v23/balcan12a/balcan12a.pdf.  Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V. Vazirani. Learning economic parameters from revealed preferences. CoRR, abs/1407.7937, 2014. URL http: //arxiv.org/abs/1407.7937.  Shai Ben-David and Ami Litman. Combinatorial variability of vapnik-chervonenkis classes with applications to sample compression schemes. Discrete Applied Mathematics, 86(1):3- 25, 1998. doi: 10.1016/S0166-218X(98)00000-6. URL https://doi.org/10.1016/ S0166-218X(98)00000-6.  Shai Ben-David and Ruth Urner. On version space compression. In Algorithmic Learning Theory - 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings, pages ON COMMUNICATION COMPLEXITY OF CLASSIFICATION PROBLEMS  50-64, 2016. doi: 10.1007/978-3-319-46379-7 4. URL https://doi.org/10.1007/ 978-3-319-46379-7_4.  Shai Ben-David, Pavel Hrubes, Shay Moran, Amir Shpilka, and Amir Yehudayoff. A learning problem that is independent of the set theory ZFC axioms. CoRR, abs/1711.05195, 2017. URL http://arxiv.org/abs/1711.05195.  Shang-Tse Chen, Maria-Florina Balcan, and Duen Horng Chau. Communication efficient dis- In Proceedings of the 19th International Conference on Artificial tributed agnostic boosting. Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 1299-1307, 2016. URL http://jmlr.org/proceedings/papers/v51/chen16e.html.  Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning through the lens of compres-  sion. In NIPS, 2016.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13:165-202, 2012. URL http: //dl.acm.org/citation.cfm?id=2188391.  Sally Floyd and Manfred K. Warmuth.  learnability, and the vapnik- chervonenkis dimension. Machine Learning, 21(3):269-304, 1995. doi: 10.1007/BF00993593. URL https://doi.org/10.1007/BF00993593.  Sample compression,  Yoav Freund. Boosting a weak learning algorithm by majority. Inf. Comput., 121(2):256-285, 1995.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997. doi: 10.1006/jcss.1997.1504. URL https://doi.org/10.1006/jcss.1997.1504.  Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compres- In Advances in Neural Information Processing Systems 27: An- sion for nearest neighbors. nual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon- treal, Quebec, Canada, pages 370-378, 2014. URL http://papers.nips.cc/paper/ 5528-near-optimal-sample-compression-for-nearest-neighbors.  David Haussler and Emo Welzl. (cid:15)-nets and simplex range queries. Discrete & Computational Ge- ometry, 2:127-151, 1987. doi: 10.1007/BF02187876. URL https://doi.org/10.1007/ BF02187876.  Zengfeng Huang and Ke Yi. The communication complexity of distributed epsilon-approximations. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadel- phia, PA, USA, October 18-21, 2014, pages 591-600, 2014. doi: 10.1109/FOCS.2014.69. URL https://doi.org/10.1109/FOCS.2014.69.  Hal Daum\u00b4e III, Jeff M. Phillips, Avishek Saha, and Suresh Venkatasubramanian. Efficient protocols In Algorithmic Learning Theory - 23rd Inter- for distributed classification and optimization. national Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings, pages 154- 168, 2012a. doi: 10.1007/978-3-642-34106-9 15. URL https://doi.org/10.1007/ 978-3-642-34106-9_15. ON COMMUNICATION COMPLEXITY OF CLASSIFICATION PROBLEMS  Hal Daum\u00b4e III, Jeff M. Phillips, Avishek Saha, and Suresh Venkatasubramanian. Protocols for learning classifiers on distributed data. In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma, Canary Islands, April 21- 23, 2012, pages 282-290, 2012b. URL http://jmlr.csail.mit.edu/proceedings/ papers/v22/daume12.html.  Bala Kalyanasundaram and Georg Schnitger. The probabilistic communication complexity of set intersection. SIAM J. Discrete Math., 5(4):545-557, 1992. doi: 10.1137/0405044. URL https: //doi.org/10.1137/0405044.  Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression: Effi- ciency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017. URL http://arxiv. org/abs/1705.08184.  Pravesh K. Kothari and Roi Livni. Agnostic learning by refuting. CoRR, abs/1709.03871, 2018.  URL http://arxiv.org/abs/1709.03871. To appear at ITCS\u201918.  Eyal Kushilevitz and Noam Nisan. Communication complexity. Cambridge University Press, 1997.  ISBN 978-0-521-56067-2.  N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished, 1986.  Roi Livni and Pierre Simon. Honest compressions and their application to compression schemes. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton Uni- versity, NJ, USA, pages 77-92, 2013. URL http://jmlr.org/proceedings/papers/ v30/Livni13.html.  Journal of Computer and System Sciences, 47(2):322 - 349, 1993.  L\u00b4aszl\u00b4o Lov\u02d8asz and Michael Saks. Communication complexity and combinatorial lattice the- ory. ISSN 0022-0000. doi: https://doi.org/10.1016/0022-0000(93)90035-U. URL http://www.sciencedirect. com/science/article/pii/002200009390035U.  Ji\u02c7r\u00b4\u0131 Matou\u02c7sek, Emo Welzl, and Lorenz Wernisch. Discrepancy and approximations for bounded vc-dimension. Combinatorica, 13(4):455-466, 1993. doi: 10.1007/BF01303517. URL https: //doi.org/10.1007/BF01303517.  Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. J. ACM, 63 (3):21:1-21:10, 2016. doi: 10.1145/2890490. URL http://doi.acm.org/10.1145/ 2890490.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff. Compressing and teaching for low vc-dimension. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 40-51, 2015. doi: 10.1109/FOCS.2015.12. URL https://doi.org/10.1109/FOCS.2015.12.  Noam Nisan and Avi Wigderson. Rounds in communication complexity revisited. SIAM J. Com- put., 22(1):211-219, 1993. doi: 10.1137/0222016. URL https://doi.org/10.1137/ 0222016. ON COMMUNICATION COMPLEXITY OF CLASSIFICATION PROBLEMS  Christos H. Papadimitriou and Michael Sipser. Communication complexity. J. Comput. Syst. Sci., 28(2):260-269, 1984. doi: 10.1016/0022-0000(84)90069-2. URL https://doi.org/10. 1016/0022-0000(84)90069-2.  Alexander A. Razborov. On the distributional complexity of disjointness. Theor. Comput. Sci., 106 (2):385-390, 1992. doi: 10.1016/0304-3975(92)90260-M. URL https://doi.org/10. 1016/0304-3975(92)90260-M.  Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press, 2012.  Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Proceedings of the Fourteenth Interna- tional Conference on Machine Learning (ICML 1997), Nashville, Tennessee, USA, July 8-12, 1997, pages 322-330, 1997.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to ISBN 1107057132,  Algorithms. Cambridge University Press, New York, NY, USA, 2014. 9781107057135.  Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 52nd An- nual Allerton Conference on Communication, Control, and Computing, Allerton 2014, Aller- ton Park & Retreat Center, Monticello, IL, September 30 - October 3, 2014, pages 850-857, doi: 10.1109/ALLERTON.2014.7028543. URL https://doi.org/10.1109/ 2014. ALLERTON.2014.7028543.  Ohad Shamir, Nathan Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1000-1008, 2014. URL http://jmlr.org/proceedings/papers/v32/shamir14.html.  A. Tychonoff.  \u00a8Uber die topologische erweiterung von r\u00a8aumen. Mathematische Annalen, 102(1): 544-561, Dec 1930. ISSN 1432-1807. doi: 10.1007/BF01782364. URL https://doi.org/ 10.1007/BF01782364.  Salil P. Vadhan. On learning vs. refutation. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages 1835-1848, 2017. URL http://proceedings.mlr.press/v65/vadhan17a.html.  V.N. Vapnik and A.Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory Probab. Appl., 16:264-280, 1971. ISSN 0040-585X; 1095-7219/e. doi: 10.1137/1116025.  Yair Wiener and Ran El-Yaniv. Agnostic pointwise-competitive selective classification. J. Artif. Intell. Res., 52:171-201, 2015. doi: 10.1613/jair.4439. URL https://doi.org/10.1613/ jair.4439.  Andrew Chi-Chih Yao. Some complexity questions related to distributive computing (preliminary  report). In STOC, pages 209-213, 1979. ON COMMUNICATION COMPLEXITY OF CLASSIFICATION PROBLEMS  "}, "Non-asymptotic Analysis of Biased Stochastic Approximation Scheme": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Non-asymptotic Analysis of Biased Stochastic Approximation Scheme", "abstract": "Stochastic approximation (SA) is a key method used in statistical learning. Recently, its non-asymptotic convergence analysis has been considered in many papers. However, most of the prior analyses are made under restrictive assumptions such as unbiased gradient estimates and convex objective function, which significantly limit their applications to sophisticated tasks such as online and reinforcement learning. These restrictions are all essentially relaxed in this work. In particular, we analyze a general SA scheme to minimize a non-convex, smooth objective function. We consider update procedure whose drift term depends on a state-dependent Markov chain and the mean field is not necessarily of gradient type, covering approximate second-order method and allowing asymptotic bias for the one-step updates. We illustrate these settings with the online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning.", "pdf_url": "http://proceedings.mlr.press/v99/karimi19a/karimi19a.pdf", "keywords": ["biased stochastic approximation", "state-dependent Markov chain", "non-convex optimization", "policy gradient", "online expectation-maximization"], "reference": "Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Regret bounds for model-free linear  quadratic control. arXiv preprint arXiv:1804.06021, 2018.  Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent  data. IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the EM algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artifi-  cial Intelligence Research, 15:319-350, 2001.  Albert Benveniste, Pierre Priouret, and Michel M\u00b4etivier. Adaptive Algorithms and Stochastic Ap-  proximation. 01 1990. ISBN 0-387-52894-6. doi: 10.1007/978-3-642-75894-2.  Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691- 1692, 2018.  Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):  291-294, 1997.  2009.  17(9):142, 1998.  Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer,  L\u00b4eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,  L\u00b4eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine  learning. SIAM Review, 60(2):223-311, 2018.  Olivier Capp\u00b4e and Eric Moulines. On-line Expectation Maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593- 613, 2009.  Jianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic Expectation Maximization with variance reduction. In Advances in Neural Information Processing Systems, pages 7978-7988, 2018.  Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two- timescale stochastic approximation with applications to reinforcement learning. In Conference On Learning Theory, 2018a.  13   NON-ASYMPTOTIC ANALYSIS OF BIASED STOCHASTIC APPROXIMATION SCHEME  HTW\u2019s work is supported by the CUHK Direct Grant #4055113. The authors would like to thank the anonymous reviewers for valuable feedback.  Acknowledgement  References  Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Regret bounds for model-free linear  quadratic control. arXiv preprint arXiv:1804.06021, 2018.  Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent  data. IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the EM algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artifi-  cial Intelligence Research, 15:319-350, 2001.  Albert Benveniste, Pierre Priouret, and Michel M\u00b4etivier. Adaptive Algorithms and Stochastic Ap-  proximation. 01 1990. ISBN 0-387-52894-6. doi: 10.1007/978-3-642-75894-2.  Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691- 1692, 2018.  Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):  291-294, 1997.  2009.  17(9):142, 1998.  Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer,  L\u00b4eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,  L\u00b4eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine  learning. SIAM Review, 60(2):223-311, 2018.  Olivier Capp\u00b4e and Eric Moulines. On-line Expectation Maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593- 613, 2009.  Jianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic Expectation Maximization with variance reduction. In Advances in Neural Information Processing Systems, pages 7978-7988, 2018.  Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two- timescale stochastic approximation with applications to reinforcement learning. In Conference On Learning Theory, 2018a. NON-ASYMPTOTIC ANALYSIS OF BIASED STOCHASTIC APPROXIMATION SCHEME  Gal Dalal, Bal\u00b4azs Sz\u00a8or\u00b4enyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with function approximation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018b.  Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint  arXiv:1205.4839, 2012.  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (methodological), pages 1-38, 1977.  Randal Douc, Eric Moulines, and David Stoffer. Nonlinear Time Series: Theory, Methods and  Applications with R examples. Chapman and Hall/CRC, 2014.  John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent.  SIAM Journal on Optimization, 22(4):1549-1578, 2012.  Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In Advances in Neural Infor- mation Processing Systems, pages 687-697, 2018.  Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradi- ent methods for the linear quadratic regulator. In International Conference on Machine Learning, pages 1466-1475, 2018.  Gersende Fort, Eric Moulines, and Pierre Priouret. Convergence of adaptive and interacting Markov  chain monte carlo algorithms. The Annals of Statistics, 39(6):3262-3289, 2011.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex  stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dy- namic programming algorithms. In Advances in Neural Information Processing Systems, pages 703-710, 1994.  Vijay R Konda and John N Tsitsiklis. On actor-critic algorithms. SIAM journal on Control and  Optimization, 42(4):1143-1166, 2003.  Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and appli-  cations, volume 35. Springer Science & Business Media, 2003.  Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics, pages 1347-1355, 2018.  Julien Mairal. Incremental majorization-minimization optimization with application to large-scale  machine learning. SIAM Journal on Optimization, 25(2):829-855, 2015.  Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451-459, 2011. NON-ASYMPTOTIC ANALYSIS OF BIASED STOCHASTIC APPROXIMATION SCHEME  Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. 80:4026-4035, 10-15 Jul 2018. URL http: //proceedings.mlr.press/v80/papini18a.html.  Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, 22(3):400-407, 1951.  Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 31, pages 9918-9927. Curran Associates, Inc., 2018. URL http:// papers.nips.cc/paper/8195-on-markov-chain-gradient-descent.pdf.  Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction, 2nd Edition. MIT  Press, 2018.  Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Infor- mation Processing Systems, pages 1057-1063, 2000.  Vladislav B Tadi\u00b4c and Arnaud Doucet. Asymptotic bias of stochastic gradient search. The Annals  of Applied Probability, 27(6):3255-3304, 2017.  Zhaoran Wang, Quanquan Gu, Yang Ning, and Han Liu. High dimensional em algorithm: Statistical optimization and asymptotic normality. In Advances in neural information processing systems, pages 2521-2529, 2015.  Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine Learning, 8(3-4):229-256, 1992.  CF Jeff Wu. On the convergence properties of the EM algorithm. The Annals of Statistics, pages  Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of Expectation Maximization for mixtures In Advances in Neural Information Processing Systems, pages 2676-2684,  95-103, 1983.  of two gaussians. 2016. NON-ASYMPTOTIC ANALYSIS OF BIASED STOCHASTIC APPROXIMATION SCHEME  "}, "Discrepancy, Coresets, and Sketches in Machine Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Discrepancy, Coresets, and Sketches in Machine Learning", "abstract": "This paper defines the notion of class discrepancy for families of functions. It shows that low discrepancy classes admit small offline and streaming coresets. We provide general techniques for bounding the class discrepancy of machine learning problems. As corollaries of the general technique we bound the discrepancy of logistic regression, sigmoid activation loss, matrix covariance, kernel density and any analytic function of the dot product or the squared distance. Our result resolves a long-standing open problem regarding the coreset complexity of Gaussian kernel density estimation. We provide two more related but independent results. First, an exponential improvement of the widely used merge-and-reduce trick which gives improved streaming sketches for any low discrepancy problem. Second, an extremely simple deterministic algorithm for finding low discrepancy sequences (and therefore coresets) for any positive semi-definite kernel. This paper establishes some explicit connections between class discrepancy, coreset complexity, learnability, and streaming algorithms.", "pdf_url": "http://proceedings.mlr.press/v99/karnin19a/karnin19a.pdf", "keywords": [], "reference": "Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via  coresets. Combinatorial and computational geometry, 52:1-30, 2005.  \u221a  11   DISCREPANCY, CORESETS, AND SKETCHES IN MACHINE LEARNING  m (cid:88)  i=1  (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  Proof For any positive semi-definite kernel K there exist a mapping \u03c6 : Rd \u2192 V to an inner product space V such that K(x, q) = (cid:104)\u03c6(x), \u03c6(q)(cid:105). Using this function \u03c6 our objective function becomes  m (cid:88)  |  i=1  \u03c3iK(xi, q)| = |  \u03c3i (cid:104)\u03c6(xi), \u03c6(q)(cid:105) | =  \u03c3i\u03c6(xi), \u03c6(q)  \u2264 (cid:107)\u03c6(q)(cid:107) \u00b7  \u03c3i\u03c6(xi)  (cid:12) (cid:42) m (cid:12) (cid:88) (cid:12) (cid:12) (cid:12)  i=1  (cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  m (cid:88)  i=1  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  Since (cid:107)\u03c6(q)(cid:107) \u2264 1 we reduced the problem to bounding the norm of (cid:80)m (cid:13) 2 (cid:13) j=1 \u03c3j\u03c6(xj) (cid:13)  i=1 \u03c3i\u03c6(xi). We show by j=1 (cid:107)\u03c6(xj)(cid:107)2 \u2264 i. This is trivially true for i = 1 since  induction on i that (cid:107)\u03c6(x)(cid:107) \u2264 1. Using our induction assumption we get  \u2264 (cid:80)i  (cid:80)i  (cid:13) (cid:13) (cid:13)  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  i (cid:88)  j=1  \u03c3j\u03c6(xj)  \u03c3j\u03c6(xj)  + (cid:107)\u03c6(xi)(cid:107)2 + 2  \u03c3j\u03c6(xj), \u03c3i\u03c6(xi)  (cid:43)  =  \u2264  =  i\u22121 (cid:88)  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) i\u22121 (cid:88)  j=1  j=1  i (cid:88)  j=1  (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  (cid:42) i\u22121 (cid:88)  j=1  i\u22121 (cid:88)  (cid:107)\u03c6(xj)(cid:107)2 + (cid:107)\u03c6(xi)(cid:107)2 + 2\u03c3i  \u03c3jK(xj, xi)  (cid:107)\u03c6(xj)(cid:107)2 \u2212 2  (cid:107)\u03c6(xj)(cid:107)2  j=1 (cid:12) (cid:12) (cid:12) \u03c3jK(xj, xi) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  i\u22121 (cid:88)  j=1  \u2264  i (cid:88)  j=1  The first equality simply unpacks the squared vector norm, the second transition is due to the induc- tion assumption and the last substitutes our choice of \u03c3 (and sign(z) \u00b7 z = |z|). This completes the proof that | (cid:80)m  m for all q.  \u221a  i=1 \u03c3iK(xi, q)| \u2264  Using the framework above provides a deterministic coreset construction for kernel density estimation of size O(1/(cid:15)2) such that \u2200 q | \u02dcF (q) \u2212 F (q)| \u2264 (cid:15)n. This matches and simplifies the results achieved by Phillips and Tai (2018a) and Phillips and Tai (2018b). Theorem 12 leads to a deterministic streaming algorithm with a memory complexity of O(log3((cid:15)2n)/(cid:15)2). For L-Lipchitz kernels, meaning K such that |K(x, q + h) \u2212 K(x, q)|/(cid:107)h(cid:107) \u2264 L for all h (cid:54)= 0, Theorem 14 leads to a randomized streaming algorithm with a memory complexity of O (cid:0)log3 (d log (RLn/\u03b4(cid:15))) /(cid:15)2(cid:1) that succeeds in finding a coreset with probability 1 \u2212 \u03b4. The parameter R is the maximum norm of a query. The argument goes through a union bound over an (cid:15)/L-net over vectors of norm at most R, the size of which is (RL/(cid:15))O(d).  m for the sign discrepancy. This upper bound is Note Theorem 24 provides an upper bound of tight since there exist sets of vectors in high dimensions that require it. For data that lends itself to density estimation, however, one should expect input vectors to be clustered together. In such cases, the algorithm above performs much better than the worst-case bound predicts. We leave it to future work to define properties of the data that ensure better guarantees for Algorithm 1.  References  Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via  coresets. Combinatorial and computational geometry, 52:1-30, 2005.  \u221a DISCREPANCY, CORESETS, AND SKETCHES IN MACHINE LEARNING  Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine  learning. arXiv preprint arXiv:1703.06476, 2017.  Wojciech Banaszczyk. Balancing vectors and gaussian measures of n-dimensional convex bod- ies. Random Struct. Algorithms, 12(4):351-360, July 1998. ISSN 1042-9832. doi: 10.1002/ (SICI)1098-2418(199807)12:4(cid:104)351::AID-RSA3(cid:105)3.0.CO;2-S. URL http://dx.doi.org/ 10.1002/(SICI)1098-2418(199807)12:4<351::AID-RSA3>3.0.CO;2-S.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463-482, March 2003. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=944919.944944.  Jon Louis Bentley and James B Saxe. Decomposable searching problems i. static-to-dynamic trans- formation. Journal of Algorithms, 1(4):301 - 358, 1980. ISSN 0196-6774. doi: https://doi.org/ 10.1016/0196-6774(80)90015-2. URL http://www.sciencedirect.com/science/ article/pii/0196677480900152.  Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for of\ufb02ine and streaming  coreset constructions. arXiv preprint arXiv:1612.00889, 2016.  Nader H. Bshouty, Yi Li, and Philip M. Long. Using the doubling dimension to analyze the generalization of learning algorithms. Journal of Computer and System Sciences, 75(6):323 - ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2009.01.003. URL http: 335, 2009. //www.sciencedirect.com/science/article/pii/S0022000009000130.  Jianqing Fan. Local polynomial modelling and its applications: monographs on statistics and  applied probability 66. Routledge, 2018.  Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In Lance Fortnow and Salil P. Vadhan, editors, Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 569-578. ACM, 2011. ISBN 978-1-4503-0691-1. doi: 10.1145/1993636.1993712. URL https://doi.org/ 10.1145/1993636.1993712.  Dan Feldman, Morteza Monemizadeh, and Christian Sohler. A ptas for k-means clustering based In Proceedings of the Twenty-third Annual Symposium on Computational on weak coresets. Geometry, SCG \u201907, pages 11-18, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-705- 6. doi: 10.1145/1247069.1247072. URL http://doi.acm.org/10.1145/1247069. 1247072.  Sariel Har-Peled and Akash Kushal. Smaller coresets for k-median and k-means clustering.  In Joseph S. B. Mitchell and G\u00a8unter Rote, editors, Proceedings of the 21st ACM Symposium on Com- putational Geometry, Pisa, Italy, June 6-8, 2005, pages 126-134. ACM, 2005. ISBN 1-58113- 991-8. doi: 10.1145/1064092.1064114. URL https://doi.org/10.1145/1064092. 1064114.  Sariel Har-Peled, Dan Roth, and Dav Zimak. Maximum margin coresets for active and noise tolerant learning. In Manuela M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint DISCREPANCY, CORESETS, AND SKETCHES IN MACHINE LEARNING  Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 836-841, 2007. URL http://ijcai.org/Proceedings/07/Papers/134.pdf.  Zohar S. Karnin, Kevin J. Lang, and Edo Liberty. Optimal quantile approximation in streams. In Irit Dinur, editor, IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 71-78. IEEE Computer Society, 2016. ISBN 978-1-5090-3933-3. doi: 10.1109/FOCS.2016.17. URL https: //doi.org/10.1109/FOCS.2016.17.  Michael Langberg and Leonard J Schulman. Universal \u03b5-approximators for integrals. In Proceed- ings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 598-607. SIAM, 2010.  David Lopez-Paz, Krikamol Muandet, Bernhard Sch\u00a8olkopf, and Iliya Tolstikhin. Towards a learning theory of cause-effect inference. In International Conference on Machine Learning, pages 1452- 1461, 2015.  Gurmeet Singh Manku, Sridhar Rajagopalan, and Bruce G. Lindsay. Random sampling techniques for space efficient online computation of order statistics of large datasets. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201999, pages 251-262, New York, NY, USA, 1999. ACM. ISBN 1-58113-084-8. doi: 10.1145/304182.304204. URL http://doi.acm.org/10.1145/304182.304204.  Jiri Matousek. Approximations and optimal geometric divide-and-conquer. Journal of Computer  and System Sciences, 50(2):203-208, 1995.  Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David P. Woodruff. On coresets for logistic regression. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00b4eal, Canada., pages 6562-6571, 2018a. URL http://papers. nips.cc/paper/7891-on-coresets-for-logistic-regression.  Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David P. Woodruff. On coresets for logistic regression. CoRR, abs/1805.08571, 2018b. URL http://arxiv.org/abs/ 1805.08571.  Nabil H. Mustafa and Kasturi R. Varadarajan. Epsilon-approximations and epsilon-nets. CoRR,  abs/1702.03676, 2017. URL http://arxiv.org/abs/1702.03676.  Jeff M Phillips. Small and stable descriptors of distributions for geometric statistical problems.  PhD thesis, 2009.  Jeff M. Phillips and Wai Ming Tai.  In Ar- tur Czumaj, editor, Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Dis- crete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2718- ISBN 978-1-61197-503-1. doi: 10.1137/1.9781611975031.173. URL 2727. SIAM, 2018a. https://doi.org/10.1137/1.9781611975031.173.  Improved coresets for kernel density estimates. DISCREPANCY, CORESETS, AND SKETCHES IN MACHINE LEARNING  Jeff M. Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. CoRR,  abs/1802.01751, 2018b. URL http://arxiv.org/abs/1802.01751.  Alessandro Rinaldo, Larry Wasserman, et al. Generalized density clustering. The Annals of Statis-  tics, 38(5):2678-2722, 2010.  Erich Schubert, Arthur Zimek, and Hans-Peter Kriegel. Generalized outlier detection with \ufb02exible kernel density estimates. In Proceedings of the 2014 SIAM International Conference on Data Mining, pages 542-550. SIAM, 2014.  Bernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.  Michel Talagrand. Sharper bounds for gaussian and empirical processes. The Annals of Probability,  pages 28-76, 1994.  Elad Tolochinsky and Dan Feldman. Coresets for monotonic functions with applications to deep  learning. arXiv preprint arXiv:1802.07382, 2018.  Ryota Tomioka and Taiji Suzuki. Spectral norm of random tensors. arXiv preprint arXiv:1407.1870,  2014.  Tim Van Erven, Peter D Gr\u00a8unwald, Nishant A Mehta, Mark D Reid, and Robert C Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793-1861, 2015.  "}, "Bandit Principal Component Analysis": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Bandit Principal Component Analysis", "abstract": "We consider a partial-feedback variant of the well-studied online PCA problem where a learner attempts to predict a sequence of $d$-dimensional vectors in terms of a quadratic loss, while only having limited feedback about the environment\u2019s choices. We focus on a natural notion of bandit feedback where the learner only observes the loss associated with its own prediction. Based on the classical observation that this decision-making problem can be lifted to the space of density matrices, we propose an algorithm that is shown to achieve a regret of $O(d^{3/2}\\sqrt{T})$ after $T$ rounds in the worst case. We also prove data-dependent bounds that improve on the basic result when the loss matrices of the environment have bounded rank or the loss of the best action is bounded. One version of our algorithm runs in $O(d)$ time per trial which massively improves over every previously known online PCA method. We complement these results by a lower bound of $\\Omega(d\\sqrt{T})$.", "pdf_url": "http://proceedings.mlr.press/v99/kotlowski19a/kotlowski19a.pdf", "keywords": ["online PCA", "bandit PCA", "online linear optimization", "phase retrieval"], "reference": "Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak. Online learning of quantum states. In Advances in Neural Information Processing Systems 31 (NIPS), pages 8976\u2013 8986. 2018.  Jacob D. Abernethy and Alexander Rakhlin. Beating the adaptive bandit with high probability. In  Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.  Jacob D. Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smooth- ness. In Advances in Neural Information Processing Systems 28 (NIPS), pages 2197\u20132205. 2015.  Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 28\u201340, 2010.  12   BANDIT PCA  Unfortunately, our attempts to prove such bounds were unsuccessful due to a limitation common to all known techniques for proving high-probability bounds. Brie\ufb02y put, all known approaches (Auer et al., 2002; Bartlett et al., 2008; Audibert and Bubeck, 2010; Beygelzimer et al., 2011; Neu, 2015) are based on adjusting the unbiased loss estimates so that the loss of every action v is (cid:11)(cid:3) for some small \u03b2 of order T \u22121/2 (see, e.g., (cid:2)(cid:10)vvT, (cid:101)L slightly underestimated by a margin of \u03b2Et Abernethy and Rakhlin, 2009 for a general discussion). While it is straightforward to bias our own (cid:11)(cid:3) in the bound, estimates in the same way, this eventually leads to extra terms of order \u03b2Et which are impossible to control by a small enough upper bound, as shown in "}, "Contextual bandits with continuous actions: Smoothing, zooming, and adapting": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Contextual bandits with continuous actions: Smoothing, zooming, and adapting", "abstract": "We study contextual bandit learning for any competitor policy class and continuous action space.  We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent \u201czooming\" behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.", "pdf_url": "http://proceedings.mlr.press/v99/krishnamurthy19a/krishnamurthy19a.pdf", "keywords": ["Contextual bandits", "Lipschitz bandits", "Nonparametric learning"], "reference": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, 2014.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making contextual decisions with low technical debt. arxiv:1606.03966, 2017.  Rajeev Agrawal. The continuum-armed bandit problem. SIAM Journal on Control and Optimization,  1995.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed  bandit problem. SIAM Journal on Computing, 2002.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesv\u00e1ri. X-armed bandits. Journal of  Machine Learning Research, 2011.  Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in  Neural Information Processing Systems, 2004.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Journal of the ACM, 2019. To appear. Merged and revised version of conference papers in ACM STOC 2008 and ACM-SIAM SODA 2010. Also available at http://arxiv.org/abs/1312.1277.  John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In  Advances in Neural Information Processing Systems, 2007.  Aleksandrs Slivkins. Contextual bandits with similarity information. The Journal of Machine  Learning Research, 2014.  3   CONTEXTUAL BANDITS WITH CONTINUOUS ACTIONS  References  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, 2014.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making contextual decisions with low technical debt. arxiv:1606.03966, 2017.  Rajeev Agrawal. The continuum-armed bandit problem. SIAM Journal on Control and Optimization,  1995.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed  bandit problem. SIAM Journal on Computing, 2002.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesv\u00e1ri. X-armed bandits. Journal of  Machine Learning Research, 2011.  Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in  Neural Information Processing Systems, 2004.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Journal of the ACM, 2019. To appear. Merged and revised version of conference papers in ACM STOC 2008 and ACM-SIAM SODA 2010. Also available at http://arxiv.org/abs/1312.1277.  John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In  Advances in Neural Information Processing Systems, 2007.  Aleksandrs Slivkins. Contextual bandits with similarity information. The Journal of Machine  Learning Research, 2014. "}, "Distribution-Dependent Analysis of Gibbs-ERM Principle": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Distribution-Dependent Analysis of Gibbs-ERM Principle", "abstract": "Gibbs-ERM learning is a natural idealized model of learning with stochastic optimization algorithms (such as SGLD and \u2014to some extent\u2014 SGD), while it also arises in other contexts, including PAC-Bayesian theory, and sampling mechanisms. In this work we study the excess risk suffered by a Gibbs-ERM learner that uses non-convex, regularized empirical risk with the goal to understand the interplay between the data-generating distribution and learning in large hypothesis spaces. Our main results are \\emph{distribution-dependent} upper bounds on several notions of excess risk. We show that, in all cases, the distribution-dependent excess risk is essentially controlled by the \\emph{effective dimension} $\\text{tr}\\left(\\boldsymbol{H}^{\\star} (\\boldsymbol{H}^{\\star} + \\lambda \\boldsymbol{I})^{-1}\\right)$ of the problem, where $\\boldsymbol{H}^{\\star}$ is the Hessian matrix of the risk at a local minimum. This is a well-established notion of effective dimension appearing in several previous works, including the analyses of SGD and ridge regression, but ours is the first work that brings this dimension to the analysis of learning using Gibbs densities. The distribution-dependent view we advocate here improves upon earlier results of Raginsky et al. 2017, and can yield much tighter bounds depending on the interplay between the data-generating distribution and the loss function. The first part of our analysis focuses on the \\emph{localized} excess risk in the vicinity of a fixed local minimizer. This result is then extended to bounds on the \\emph{global} excess risk, by characterizing probabilities of local minima (and their complement) under Gibbs densities, a results which might be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v99/kuzborskij19a/kuzborskij19a.pdf", "keywords": [], "reference": "P. Alquier, J. Ridgway, and N. Chopin. On the properties of variational approximations of Gibbs  posteriors. Journal of Machine Learning Research, 17(1):8374-8414, 2016.  C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine  learning. Machine Learning, 50(1-2):5-43, 2003.  J.-Y. Audibert and O. Catoni. Robust linear least squares regression. The Annals of Statistics, 39(5):  2766-2794, 2011.  2:499-526, 2002.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Oxford University Press, 2013.  O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In International Conference on Learning Representations (ICLR), 2018.  X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I Jordan. Sharp convergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648, 2018a.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. In Conference on Computational Learning Theory (COLT), 2018b.  G. K. Dziugaite and D. M. Roy. Data-dependent PAC-Bayes priors via differential privacy. In  International Conference on Machine Learing (ICML), 2018.  R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points-online stochastic gradient for tensor decomposition. In Conference on Computational Learning Theory (COLT), pages 797-842, 2015.  P. Germain, A. Lacasse, F. Laviolette, M. Marchand, and J.-F. Roy. Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning algorithm. Journal of Machine Learning Research, 16(1):787-860, 2015.  A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems.  In Conference on Computational Learning Theory (COLT), 2017.  P. D. Gr\u00fcnwald and N. A. Mehta. A tight excess risk bound via a unified PAC-Bayesian-Rademacher-  Shtarkov-MDL complexity. arXiv preprint arXiv:1710.07732, 2017.  T. Jaakkola, M. Meila, and T. Jebara. Maximum entropy discrimination. In Conference on Neural  Information Processing Systems (NIPS), 1999.  13   DISTRIBUTION-DEPENDENT ANALYSIS OF GIBBS-ERM PRINCIPLE  The authors would like to thank Olivier Bousquet, S\u00e9bastien Gerchinovitz, Abbas Mehrabian, and Massimiliano Pontil for stimulating discussions on this work.  ACKNOWLEDGEMENTS  References  P. Alquier, J. Ridgway, and N. Chopin. On the properties of variational approximations of Gibbs  posteriors. Journal of Machine Learning Research, 17(1):8374-8414, 2016.  C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine  learning. Machine Learning, 50(1-2):5-43, 2003.  J.-Y. Audibert and O. Catoni. Robust linear least squares regression. The Annals of Statistics, 39(5):  2766-2794, 2011.  2:499-526, 2002.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Oxford University Press, 2013.  O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In International Conference on Learning Representations (ICLR), 2018.  X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I Jordan. Sharp convergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648, 2018a.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. In Conference on Computational Learning Theory (COLT), 2018b.  G. K. Dziugaite and D. M. Roy. Data-dependent PAC-Bayes priors via differential privacy. In  International Conference on Machine Learing (ICML), 2018.  R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points-online stochastic gradient for tensor decomposition. In Conference on Computational Learning Theory (COLT), pages 797-842, 2015.  P. Germain, A. Lacasse, F. Laviolette, M. Marchand, and J.-F. Roy. Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning algorithm. Journal of Machine Learning Research, 16(1):787-860, 2015.  A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems.  In Conference on Computational Learning Theory (COLT), 2017.  P. D. Gr\u00fcnwald and N. A. Mehta. A tight excess risk bound via a unified PAC-Bayesian-Rademacher-  Shtarkov-MDL complexity. arXiv preprint arXiv:1710.07732, 2017.  T. Jaakkola, M. Meila, and T. Jebara. Maximum entropy discrimination. In Conference on Neural  Information Processing Systems (NIPS), 1999. DISTRIBUTION-DEPENDENT ANALYSIS OF GIBBS-ERM PRINCIPLE  N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations (ICLR), 2017.  J. Langford and J. Shawe-Taylor. PAC-Bayes & margins. In Conference on Neural Information  Processing Systems (NIPS), 2003.  G. Lever, F. Laviolette, and J. Shawe-Taylor. Tighter PAC-Bayes bounds through distribution-  dependent priors. Theoretical Computer Science, 473:4-28, 2013.  T. Liang, T. Poggio, A. Rakhlin, and J. Stokes. Fisher-Rao Metric, Geometry, and Complexity of Neural Networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.  Y.-A. Ma, Y. Chen, C. Jin, N. Flammarion, and M. I. Jordan. Sampling can be faster than optimization.  arXiv preprint arXiv:1811.08413, 2018.  S. Mandt, M. Hoffman, and D. Blei. A variational analysis of stochastic gradient algorithms. In  International Conference on Machine Learing (ICML), pages 354-363, 2016.  D. A. McAllester. Some PAC-Bayesian theorems. In Conference on Computational Learning Theory  (COLT), 1998.  T. Milne. Piecewise strong convexity of neural networks. arXiv preprint arXiv:1810.12805, 2018.  W. Mou, L. Wang, X. Zhai, and K. Zheng. Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints. In Conference on Computational Learning Theory (COLT), 2018.  G. Neu and L. Rosasco. Iterate averaging as regularization for stochastic gradient descent. In  Conference on Computational Learning Theory (COLT), 2018.  NIST. Incomplete Gamma functions. https://dlmf.nist.gov/8.10#Px1.p3, 2018. Ac-  cessed: 2018-12-12.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis. Conference on Computational Learning Theory (COLT), 2017.  M. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal of  Machine Learning Research, 3(Oct):233-269, 2002.  S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.  Cambridge University Press, 2014.  R. Sheth and R. Khardon. Excess risk bounds for the Bayes risk using variational inference in latent  Gaussian models. In Conference on Neural Information Processing Systems (NIPS), 2017.  G.M. Tallis. Elliptical and radial truncation in normal populations. The Annals of Mathematical  Statistics, 34(3):940-944, 1963. DISTRIBUTION-DEPENDENT ANALYSIS OF GIBBS-ERM PRINCIPLE  B. Tzen, T. Liang, and M. Raginsky. Local optimality and generalization guarantees for the Langevin  algorithm via empirical metastability. arXiv preprint arXiv:1802.06439, 2018.  Y. Wang and D. M. Blei. Frequentist consistency of variational Bayes. Journal of the American  Statistical Association, pages 1-85, 2018.  M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics.  In  International Conference on Machine Learing (ICML), 2011.  A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Conference on Neural Information Processing Systems (NIPS), pages 2524-2533, 2017.  P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. In Conference on Neural Information Processing Systems (NeurIPS), 2018.  T. Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE Transactions  on Information Theory, 52(4):1307-1321, 2006.  "}, "Global Convergence of the EM Algorithm for Mixtures of Two Component Linear Regression": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Global Convergence of the EM Algorithm for Mixtures of Two Component Linear Regression", "abstract": "The Expectation-Maximization algorithm is perhaps the most broadly used algorithm for inference of latent variable problems. A theoretical understanding of its performance, however, largely remains lacking. Recent results established that EM enjoys global convergence for Gaussian Mixture Models. For Mixed Linear Regression, however, only local convergence results have been established, and those only for the high SNR regime. We show here that EM converges for mixed linear regression with two components (it is known that it may fail to converge for three or more), and moreover that this convergence holds for random initialization. Our analysis reveals that EM exhibits very different behavior in Mixed Linear Regression from its behavior in Gaussian Mixture Models, and hence our proofs require the development of several new ideas.", "pdf_url": "http://proceedings.mlr.press/v99/kwon19a/kwon19a.pdf", "keywords": [], "reference": "Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, February 2017. ISSN 0090-5364. doi: 10.1214/16-AOS1435. URL http://projecteuclid. org/euclid.aos/1487667618.  Arun Tejasvi Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions.  In International Conference on Machine Learning, pages 1040-1048, 2013.  Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory, pages 560-604, 2014.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for  mixtures of two gaussians. In Conference on Learning Theory, pages 704-710, 2017.  Richard D. De Veaux. Mixtures of linear regressions. Computational Statistics & Data Analysis, 8  (3):227-245, 1989.  Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1-38, 1977.  Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J Wainwright, and Michael I Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances in Neural Information Processing Systems, pages 4116-4124, 2016.  Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.  Neural computation, 6(2):181-214, 1994.  Jason M Klusowski, Dana Yang, and WD Brinda. Estimating the coefficients of a mixture of two linear regressions by expectation maximization. IEEE Transactions on Information Theory, 2019.  Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal complexity.  In Conference On Learning Theory, pages 1125-1144, 2018.  Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223-1231, 2016.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. preparation.  University of California, Berkeley, 2015.  CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages  95-103, 1983.  13   GLOBAL CONVERGENCE OF THE EM ALGORITHM FOR MIXTURES OF TWO COMPONENT LINEAR REGRESSION  References  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, February 2017. ISSN 0090-5364. doi: 10.1214/16-AOS1435. URL http://projecteuclid. org/euclid.aos/1487667618.  Arun Tejasvi Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions.  In International Conference on Machine Learning, pages 1040-1048, 2013.  Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory, pages 560-604, 2014.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for  mixtures of two gaussians. In Conference on Learning Theory, pages 704-710, 2017.  Richard D. De Veaux. Mixtures of linear regressions. Computational Statistics & Data Analysis, 8  (3):227-245, 1989.  Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1-38, 1977.  Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J Wainwright, and Michael I Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances in Neural Information Processing Systems, pages 4116-4124, 2016.  Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.  Neural computation, 6(2):181-214, 1994.  Jason M Klusowski, Dana Yang, and WD Brinda. Estimating the coefficients of a mixture of two linear regressions by expectation maximization. IEEE Transactions on Information Theory, 2019.  Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal complexity.  In Conference On Learning Theory, pages 1125-1144, 2018.  Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223-1231, 2016.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. preparation.  University of California, Berkeley, 2015.  CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages  95-103, 1983. GLOBAL CONVERGENCE OF THE EM ALGORITHM FOR MIXTURES OF TWO COMPONENT LINEAR REGRESSION  Chong Wu, Can Yang, Hongyu Zhao, and Ji Zhu. On the convergence of the em algorithm: A  data-adaptive analysis. arXiv preprint arXiv:1611.00519, 2016.  Ji Xu, Daniel J. Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. In Advances in Neural Information Processing Systems, pages 2676-2684, 2016.  Xinyang Yi and Constantine Caramanis. Regularized em algorithms: A unified framework and statistical guarantees. In Proc. Advances in Neural Information Processing Systems, pages 1567- 1575, 2015.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear  regression. In International Conference on Machine Learning, pages 613-621, 2014.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Solving a mixture of many random linear equations by tensor decomposition and alternating minimization. arXiv preprint arXiv:1608.05749, 2016.  Kai Zhong, Prateek Jain, and Inderjit S. Dhillon. Mixed linear regression with multiple components.  In Advances in neural information processing systems, pages 2190-2198, 2016. GLOBAL CONVERGENCE OF THE EM ALGORITHM FOR MIXTURES OF TWO COMPONENT LINEAR REGRESSION  1. Proofs for Population EM Update  1.1. Proof of Lemma 1  Lemma 1 Define \u03c32  2 := \u03c32 + b\u2217 2  2. We can write \u03b2(cid:48) = b(cid:48)  1v1 + b(cid:48)  2v2, where b(cid:48)  1 and b(cid:48)  2 satisfy  1 = b\u2217 b(cid:48)  1S + R,  and b(cid:48)  2 = b\u2217  2S,  where S \u2265 0 and R > 0 are given explicitly in (19) in "}, "An Information-Theoretic Approach to Minimax Regret in Partial Monitoring": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "An Information-Theoretic Approach to Minimax Regret in Partial Monitoring", "abstract": "We prove a new minimax theorem connecting the worst-case Bayesian regret and minimax regret under finite-action partial monitoring with no assumptions on the space of signals or decisions of the adversary. We then generalise the information-theoretic tools of Russo and Van Roy (2016) for proving Bayesian regret bounds and combine them with the minimax theorem to derive minimax regret bounds for various partial monitoring settings. The highlight is a clean analysis of \u2018easy\u2019 and \u2018hard\u2019 finite partial monitoring, with new regret bounds that are independent of arbitrarily large game-dependent constants and eliminate the logarithmic dependence on the horizon for easy games that appeared in earlier work. The power of the generalised machinery is further demonstrated by proving that the minimax regret for $k$-armed adversarial bandits is at most $\\sqrt{2kn}$, improving on existing results by a factor of 2. Finally, we provide a simple analysis of the cops and robbers game, also improving best known constants.", "pdf_url": "http://proceedings.mlr.press/v99/lattimore19a/lattimore19a.pdf", "keywords": ["Online learning", "partial monitoring", "minimax theorems", "bandits"], "reference": "J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  J. D. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M. F. Balcan, V. Feldman, and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of Proceedings of Machine Learning Research, pages 807-823, Barcelona, Spain, 13-15 Jun 2014. PMLR.  S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In C. M. Carvalho and P. Ravikumar, editors, Proceedings of the 16th International Conference on Artificial Intel- ligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 99-107, Scottsdale, Arizona, USA, 29 Apr-01 May 2013. PMLR.  N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs: Beyond In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th bandits. Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 23-35, Paris, France, 03-06 Jul 2015. PMLR.  A. Antos, G. Bart\u00b4ok, D. P\u00b4al, and Cs. Szepesv\u00b4ari. Toward a classification of finite partial-monitoring  games. Theoretical Computer Science, 473:77-99, 2013.  G. Bart\u00b4ok, D. P\u00b4al, and Cs. Szepesv\u00b4ari. Minimax regret of finite partial-monitoring games in stochas- In Proceedings of the 24th Annual Conference on Learning Theory, pages  tic environments. 133-154, 2011.  G. Bart\u00b4ok, N. Zolghadr, and Cs. Szepesv\u00b4ari. An adaptive algorithm for finite stochastic partial monitoring. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML, pages 1779-1786, USA, 2012. Omnipress.  G. Bart\u00b4ok, D. P. Foster, D. P\u00b4al, A. Rakhlin, and Cs. Szepesv\u00b4ari. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967-997, 2014.  P. Bernhard. Information and strategies in dynamic games. SIAM Journal on Control and Optimiza-  tion, 30(1):212-228, 1992.  V. I. Bogachev. Measure theory, volume 2. Springer Science & Business Media, 2007.  S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza- tion with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41-1. Microtome, 2012.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:  T regret in one dimension. In P. Gr\u00a8unwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 266-278, Paris, France, 03-06 Jul 2015. PMLR.  \u221a  13   AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING  References  J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  J. D. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In M. F. Balcan, V. Feldman, and Cs. Szepesv\u00b4ari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of Proceedings of Machine Learning Research, pages 807-823, Barcelona, Spain, 13-15 Jun 2014. PMLR.  S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In C. M. Carvalho and P. Ravikumar, editors, Proceedings of the 16th International Conference on Artificial Intel- ligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 99-107, Scottsdale, Arizona, USA, 29 Apr-01 May 2013. PMLR.  N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs: Beyond In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th bandits. Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 23-35, Paris, France, 03-06 Jul 2015. PMLR.  A. Antos, G. Bart\u00b4ok, D. P\u00b4al, and Cs. Szepesv\u00b4ari. Toward a classification of finite partial-monitoring  games. Theoretical Computer Science, 473:77-99, 2013.  G. Bart\u00b4ok, D. P\u00b4al, and Cs. Szepesv\u00b4ari. Minimax regret of finite partial-monitoring games in stochas- In Proceedings of the 24th Annual Conference on Learning Theory, pages  tic environments. 133-154, 2011.  G. Bart\u00b4ok, N. Zolghadr, and Cs. Szepesv\u00b4ari. An adaptive algorithm for finite stochastic partial monitoring. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML, pages 1779-1786, USA, 2012. Omnipress.  G. Bart\u00b4ok, D. P. Foster, D. P\u00b4al, A. Rakhlin, and Cs. Szepesv\u00b4ari. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967-997, 2014.  P. Bernhard. Information and strategies in dynamic games. SIAM Journal on Control and Optimiza-  tion, 30(1):212-228, 1992.  V. I. Bogachev. Measure theory, volume 2. Springer Science & Business Media, 2007.  S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012.  S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza- tion with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41-1. Microtome, 2012.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:  T regret in one dimension. In P. Gr\u00a8unwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 266-278, Paris, France, 03-06 Jul 2015. PMLR.  \u221a AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING  S. Bubeck, M. Cohen, and Y. Li. Sparsity, variance and curvature in multi-armed bandits.  In F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of Algorithmic Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 111-127. PMLR, 07-09 Apr 2018.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press,  2006.  N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Regret minimization under partial monitoring. Mathe-  matics of Operations Research, 31:562-580, 2006.  D. Foster and A. Rakhlin. No internal regret via neighborhood watch.  In N. D. Lawrence and M. Girolami, editors, Proceedings of the 15th International Conference on Artificial Intelligence and Statistics, volume 22 of Proceedings of Machine Learning Research, pages 382-390, La Palma, Canary Islands, 21-23 Apr 2012. PMLR.  S. Ghosal and A. van der Vaart. Fundamentals of nonparametric Bayesian inference, volume 44.  Cambridge University Press, 2017.  M. K. Ghosh, D. McDonald, and S. Sinha. Zero-sum stochastic games with partial information.  Journal of optimization theory and applications, 121(1):99-118, 2004.  N. Gravin, Y. Peres, and B. Sivan. Towards optimal algorithms for prediction with expert advice. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms, pages 528-547. SIAM, 2016.  O. Kallenberg. Foundations of modern probability. Springer-Verlag, 2002.  E. Kaufmann, N. Korda, and R. Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In NaderH. Bshouty, Gilles Stoltz, Nicolas Vayatis, and Thomas Zeugmann, editors, Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 199- 213. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-34105-2.  J. Komiyama, J. Honda, and H. Nakagawa. Regret lower bound and optimal algorithm in finite stochastic partial monitoring. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, NIPS, pages 1792- 1800. Curran Associates, Inc., 2015.  H.W. Kuhn. Extensive games and the problem of information, contributions to the theory of games  II. Annals of Mathematics Studies, 28:193-216, 1953.  T. Lattimore and Cs. Szepesv\u00b4ari. Cleaning up the neighbourhood: A full classification for adversar-  ial partial monitoring. In International Conference on Algorithmic Learning Theory, 2019.  T. Lattimore and Cs. Szepesv\u00b4ari. Bandit Algorithms. Cambridge University Press (preprint), 2019.  D. Leao, J. B. R. do Val, and M. D. Fragoso. Nonstationary zero sum stochastic games with in- In 39th IEEE Conference on Decision and Control, pages 2278-2283,  complete observation. 2000. AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING  S. Mannor and N. Shimkin. On-line learning with imperfect monitoring. In Learning Theory and  Kernel Machines, pages 552-566. Springer, 2003.  S. Mannor, V. Perchet, and G. Stoltz. Set-valued approachability and online learning with partial  monitoring. The Journal of Machine Learning Research, 15(1):3247-3295, 2014.  V. Perchet. Approachability of convex sets in games with partial monitoring. Journal of Optimiza-  tion Theory and Applications, 149(3):665-677, 2011.  D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Z. Ghahra- mani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, NIPS, pages 1583-1591. Curran Associates, Inc., 2014.  D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal of  Machine Learning Research, 17(1):2442-2471, 2016. ISSN 1532-4435.  D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. Operations  Research, 66(1):230-252, 2017.  A. Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1):224-  M. Sion. On general minimax theorems. Pacific Journal of mathematics, 8(1):171-176, 1958.  A. B. Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media,  243, 1999.  2008.  H. P. Vanchinathan, G. Bart\u00b4ok, and A. Krause. Efficient partial monitoring with prior information. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, NIPS, pages 1691-1699. Curran Associates, Inc., 2014.  C-Y. Wei and H. Luo. More adaptive algorithms for adversarial bandits.  In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1263-1291. PMLR, 06-09 Jul 2018.  "}, "Solving Empirical Risk Minimization in the Current Matrix Multiplication Time": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Solving Empirical Risk Minimization in the Current Matrix Multiplication Time", "abstract": "Many convex problems in machine learning and computer science share the same form:  \\begin{align*} \\min_{x} \\sum_{i} f_i( A_i x + b_i), \\end{align*} where $f_i$ are convex functions on $\\R^{n_i}$ with constant $n_i$, $A_i \\in \\R^{n_i \\times d}$, $b_i \\in \\R^{n_i}$ and $\\sum_i n_i = n$. This problem generalizes linear programming and includes many problems in empirical risk minimization. In this paper, we give an algorithm that runs in time \\begin{align*} O^* (  ( n^{\\omega} + n^{2.5 - \\alpha/2} + n^{2+ 1/6} ) \\log (n / \\delta)  ) \\end{align*} where $\\omega$ is the exponent of matrix multiplication, $\\alpha$ is the dual exponent of matrix multiplication, and $\\delta$ is the relative accuracy.  Note that the runtime has only a log dependence on the condition numbers or other data dependent parameters and these are captured in $\\delta$.  For the current bound $\\omega \\sim 2.38$ [Vassilevska Williams\u201912, Le Gall\u201914] and $\\alpha \\sim 0.31$ [Le Gall, Urrutia\u201918], our runtime $O^* ( n^{\\omega} \\log (n / \\delta))$ matches the current best for solving a dense least squares regression problem, a special case of the problem we consider. Very recently, [Alman\u201918] proved that all the current known techniques can not give a better $\\omega$ below $2.168$ which is larger than our $2+1/6$. Our result generalizes the very recent result of solving linear programs in the  current matrix multiplication time [Cohen, Lee, Song\u201919] to a more broad class of problems. Our algorithm proposes two concepts which are different from [Cohen, Lee, Song\u201919] :\\ $\\bullet$ We give a robust deterministic central path method, whereas the previous one is a stochastic central path which updates weights by a random sparse vector. \\ $\\bullet$ We propose an efficient data-structure to maintain the central path of interior point methods even when the weights update vector is dense.", "pdf_url": "http://proceedings.mlr.press/v99/lee19a/lee19a.pdf", "keywords": []}, "On Mean Estimation for General Norms with Statistical Queries": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "On Mean Estimation for General Norms with Statistical Queries", "abstract": "We study the problem of mean estimation for high-dimensional distributions given access to a statistical query oracle. For a normed space $X = (\\mathbb{R}^d, \\|\\cdot\\|_X)$ and a distribution supported on vectors $x \\in \\mathbb{R}^d$ with $\\|x\\|_{X} \\leq 1$, the task is to output an estimate $\\hat{\\mu} \\in \\mathbb{R}^d$ which is $\\varepsilon$-close in the distance induced by $\\|\\cdot\\|_X$ to the true mean of the distribution. We obtain sharp upper and lower bounds for the statistical query complexity of this problem when the the underlying norm is \\emph{symmetric} as well as for Schatten-$p$ norms, answering two questions raised by Feldman, Guzm\u00e1n, and Vempala (SODA 2017).", "pdf_url": "http://proceedings.mlr.press/v99/li19a/li19a.pdf", "keywords": ["statistical queries", "mean estimation", "normed spaces"], "reference": "Alexandr Andoni, Huy L Nguyen, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. Approximate near neighbors for general symmetric norms. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 902-913. ACM, 2017.  Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding  and linear regression with orlicz norm. arXiv preprint arXiv:1806.06430, 2018.  Keith Ball, Eric A. Carlen, and Elliott H. Lieb. Sharp uniform convexity and smoothness in- doi:  equalities for trace norms. 10.1007/BF01231769. URL https://doi.org/10.1007/BF01231769.  Invent. Math., 115(3):463-482, 1994.  ISSN 0020-9910.  Jaros\u0142aw B\u0142asiok, Vladimir Braverman, Stephen R Chestnut, Robert Krauthgamer, and Lin F Yang. Streaming symmetric norms via measure concentration. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 716-729. ACM, 2017.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 73-84. IEEE, 2017.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2745-2754. SIAM, 2019.  Vitaly Feldman. A general characterization of the statistical query complexity. arXiv preprint  arXiv:1608.02198, 2016a.  Vitaly Feldman. Statistical query learning.  In Encyclopedia of Algorithms, pages 2090-2095. doi: 10.1007/978-1-4939-2864-4\\ 401. URL https://doi.org/10.1007/  2016b. 978-1-4939-2864-4_401.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical al- In Proceedings of the 45th ACM  gorithms and a lower bound for detecting planted cliques. Symposium on the Theory of Computing (STOC \u20192013), 2013a.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for detecting planted cliques. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 655-664. ACM, 2013b.  Vitaly Feldman, Crist\u00b4obal Guzm\u00b4an, and Santosh Vempala. Statistical query algorithms for mean In Proceedings of the 28th ACM-SIAM  vector estimation and stochastic convex optimization. Symposium on Discrete Algorithms (SODA \u20192017), 2017.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018.  13   ON MEAN ESTIMATION FOR GENERAL NORMS WITH STATISTICAL QUERIES  References  Alexandr Andoni, Huy L Nguyen, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. Approximate near neighbors for general symmetric norms. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 902-913. ACM, 2017.  Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding  and linear regression with orlicz norm. arXiv preprint arXiv:1806.06430, 2018.  Keith Ball, Eric A. Carlen, and Elliott H. Lieb. Sharp uniform convexity and smoothness in- doi:  equalities for trace norms. 10.1007/BF01231769. URL https://doi.org/10.1007/BF01231769.  Invent. Math., 115(3):463-482, 1994.  ISSN 0020-9910.  Jaros\u0142aw B\u0142asiok, Vladimir Braverman, Stephen R Chestnut, Robert Krauthgamer, and Lin F Yang. Streaming symmetric norms via measure concentration. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 716-729. ACM, 2017.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 73-84. IEEE, 2017.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2745-2754. SIAM, 2019.  Vitaly Feldman. A general characterization of the statistical query complexity. arXiv preprint  arXiv:1608.02198, 2016a.  Vitaly Feldman. Statistical query learning.  In Encyclopedia of Algorithms, pages 2090-2095. doi: 10.1007/978-1-4939-2864-4\\ 401. URL https://doi.org/10.1007/  2016b. 978-1-4939-2864-4_401.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical al- In Proceedings of the 45th ACM  gorithms and a lower bound for detecting planted cliques. Symposium on the Theory of Computing (STOC \u20192013), 2013a.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for detecting planted cliques. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 655-664. ACM, 2013b.  Vitaly Feldman, Crist\u00b4obal Guzm\u00b4an, and Santosh Vempala. Statistical query algorithms for mean In Proceedings of the 28th ACM-SIAM  vector estimation and stochastic convex optimization. Symposium on Discrete Algorithms (SODA \u20192017), 2017.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. SIAM Journal on Computing, 47(4):1294-1338, 2018. ON MEAN ESTIMATION FOR GENERAL NORMS WITH STATISTICAL QUERIES  Fritz John. Extremum problems with inequalities as subsidiary conditions. In Studies and Essays Presented to R. Courant on his 60th Birthday, January 8, 1948, pages 187-204. Interscience Publishers, Inc., New York, N. Y., 1948.  Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011.  Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM  (JACM), 45(6):983-1006, 1998.  Adam R Klivans and Alexander A Sherstov. Unconditional lower bounds for learning intersections  of halfspaces. Machine Learning, 69(2-3):97-114, 2007.  Michel Ledoux and Michel Talagrand. Probability in Banach spaces. Classics in Mathematics. Springer-Verlag, Berlin, 2011. ISBN 978-3-642-20211-7. Isoperimetry and processes, Reprint of the 1991 edition.  Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014.  Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural net-  works. In Advances in Neural Information Processing Systems, pages 5514-5522, 2017.  Zhao Song, David P Woodruff, and Peilin Zhong. Towards a zero-one law for entrywise low rank  approximation. arXiv preprint arXiv:1811.01442, 2018.  Jacob Steinhardt, Gregory Valiant, and Stefan Wager. Memory, communication, and statistical  queries. In Conference on Learning Theory, pages 1490-1516, 2016.  Przemyslaw Wojtaszczyk. Banach spaces for analysts, volume 25. Cambridge University Press,  1996.  "}, "Nearly Minimax-Optimal Regret for Linearly Parameterized Bandits": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Nearly Minimax-Optimal Regret for Linearly Parameterized Bandits", "abstract": "We study the linear contextual bandit problem with finite action sets. When the problem dimension is $d$,  the time horizon is $T$, and there are $n \\leq 2^{d/2}$ candidate actions per time period, we  (1) show that the minimax expected regret is $\\Omega(\\sqrt{dT \\log T \\log n})$ for every algorithm, and  (2) introduce a Variable-Confidence-Level (VCL) SupLinUCB algorithm whose regret matches the lower bound up to iterated logarithmic factors.  Our algorithmic result saves two $\\sqrt{\\log T}$ factors from previous analysis, and our information-theoretical lower bound also improves previous results by one $\\sqrt{\\log T}$ factor, revealing a regret scaling quite different from classical multi-armed bandits in which no logarithmic $T$ term is present in minimax regret. Our proof techniques include variable confidence levels and a careful analysis of layer sizes of SupLinUCB on the upper bound side,  and delicately constructed adversarial sequences showing the tightness of elliptical potential lemmas on the lower bound side.", "pdf_url": "http://proceedings.mlr.press/v99/li19b/li19b.pdf", "keywords": [], "reference": "Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2011.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.  In Proceedings of the Conference on Learning Theory (COLT), 2009.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3(Nov):397-422, 2002.  Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit  feedback. In Proceedings of the Conference on Learning Theory (COLT), 2008.  2   MINIMAX REGRET FOR LINEAR BANDIT  Table 1: Previous results and our results on upper and lower bounds of R(T ; n, d). Lower bound \u221a  Upper bound \u221a dT log3/2(nT ))  O(  n < \u221e  n = \u221e  Previous result  Our result O(  Previous result Our result  \u221a  (Auer, 2002; Chu et al., 2011) dT log T log n) \u00b7 poly(log log(nT )) \u2126((cid:112)dT log n log(T /d))  (Chu et al., 2011)  \u221a  O(d  T log T )  (Abbasi-Yadkori et al., 2011) N/A  (Dani et al., 2008)  \u221a  \u2126(d  T log T )  \u2126(  dT )  \u221a  \u2126(d  T )  the following defined minimax regret  R(T ; n, d) := inf \u03c0  sup \u03b8\u2208Rd,|At|\u2264n  E[RT ].  (1)  Note that for n = \u221e, the supremum is taken over all closed At \u2286 {x \u2208 Rd : (cid:107)x(cid:107)2 \u2264 1} for all t.  1.1. Our results  The main results of this paper are the following two theorems that upper and lower bound the minimax regret R(T ; n, d) for various problem parameter values.  Theorem 1 (Upper bound) For any n < \u221e, the minimax regret R(T ; n, d) can be asymptotically upper bounded by poly(log log(nT )) \u00b7 O(  dT log T log n).  \u221a  Theorem 2 (Lower bound) For any small constant (cid:15) > 0, and any n, d, such that n \u2264 2d/2 and T \u2265 d(log2 n)1+(cid:15), the minimax regret R(T ; n, d) can be asymptotically lower bounded by \u2126(1) \u00b7 (cid:112)dT log n log(T /d).  Comparing Theorems 1 and 2, we see that the upper and lower bounds nearly match each other  up to iterated logarithmic terms when n (the number of actions per time period) is not too large.  References  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2011.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.  In Proceedings of the Conference on Learning Theory (COLT), 2009.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3(Nov):397-422, 2002.  Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit  feedback. In Proceedings of the Conference on Learning Theory (COLT), 2008. "}, "Sharp Theoretical Analysis for Nonparametric Testing under Random Projection": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Sharp Theoretical Analysis for Nonparametric Testing under Random Projection", "abstract": "A common challenge in nonparametric inference is its high computational complexity when data volume is large. In this paper, we develop computationally efficient nonparametric testing by employing a random projection strategy. In the specific kernel ridge regression setup, a simple distance-based test statistic is proposed. Notably, we derive the minimum number of random projections that is sufficient for achieving testing optimality in terms of the minimax rate. As a by-product, the lower bound of projection dimension for minimax optimal estimation derived in Yang (2017) is proven to be sharp. One technical contribution is to establish upper bounds for a range of tail sums of empirical kernel eigenvalues.", "pdf_url": "http://proceedings.mlr.press/v99/liu19a/liu19a.pdf", "keywords": ["Computational limit", "kernel ridge regression", "minimax optimality", "nonparametric testing", "random projection"], "reference": "[1] Nir Ailon and Bernard Chazelle. The fast johnson\u2013lindenstrauss transform and approximate  nearest neighbors. SIAM Journal on computing, 39(1):302\u2013322, 2009.  [2] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statis- tical guarantees. Advances in Neural Information Processing Systems, pages 775\u2013783, 2015.  12   SHARP NONPARAMETRIC TESTING UNDER RANDOM PROJECTION  The last inequality holds with probability greater than 1 \u2212 e\u2212n\u03b4n by Lemma B.2. Furthermore, it can be shown that T1 = OP (\u03bb), T2 = op(T1), and T3 = oP (T1) . Then with probability at least 1 \u2212 e\u2212n\u03b4n,  sup f0\u2208B  (cid:107) (cid:98)fR \u2212 f0(cid:107)2 n  (cid:38) sup f0\u2208B  (cid:107) E(cid:15) (cid:98)fR \u2212 f0(cid:107)2  n (cid:29) C\u03bb\u2020.  Our second result is about the sharpness of s\u2217. Theorem 4.8 shows that when s (cid:28) s\u2217, there exists a local alternative f that is not detectable by Tn,\u03bb even when it is separated from zero by d\u2217 n. In this case, the asymptotic testing power is actually smaller than the nominal level \u03b1.  Theorem 4.8 Suppose s = o(s\u2217). Then there exists an s \u00d7 n projection matrix S satisfying Assumption A3 and a positive nonrandom sequence \u03b2n,\u03bb satisfying limn\u2192\u221e \u03b2n,\u03bb = \u221e such that, with probability at least 1 \u2212 e\u2212cn\u03b4n \u2212 e\u2212c1s \u2212 e\u2212c2s\u03bb,  lim sup n\u2192\u221e  inf f \u2208B (cid:107)f (cid:107)n\u2265\u03b2n,\u03bbd\u2217 n  Pf (\u03c6n,\u03bb = 1|x, S) \u2264 \u03b1,  where c is a constant independent of n, and c1, c2 \u2208 (0, \u221e] are given in Assumption A3 (b). Recall 1 \u2212 \u03b1 is the significance level.  The proof of Theorem 4.8 is similar as that of Theorem 4.7, except that a different true function i=1 K(xi, \u00b7)wi with w = (w1, \u00b7 \u00b7 \u00b7 , wn)(cid:62) = U \u03b1, where \u03b1 \u2208 Rn satisfies  is constructed as f (\u00b7) = (cid:80)n  \u03b12  i =  (cid:40) 1 n 0  C  s\u22121 (cid:98)\u00b5\u22121  gs+k  for i = (gs + k) k = 1, 2, \u00b7 \u00b7 \u00b7 , s \u2212 1; otherwise,  (4.5)  with g \u2265 1 as an integer satisfying (g + 1)s (cid:28) s\u2217; see "}, "Combinatorial Algorithms for Optimal Design": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Combinatorial Algorithms for Optimal Design", "abstract": "In an optimal design problem, we are given a set of linear experiments $v_1,\u2026,v_n\\in \\mathbb{R}^d$ and $k \\geq d$, and our goal is to select a set or a multiset $S \\subseteq [n]$ of size $k$ such that $\\Phi((\\sum_{i \\in S} v_i v_i^\\top )^{-1})$ is minimized. When $\\Phi(M) = Determinant(M)^{1/d}$, the problem is known as the D-optimal design problem, and when $\\Phi(M) = Trace(M)$, it is known as the A-optimal design problem. One of the most common heuristics used in practice to solve these problems is the local search heuristic, also known as the Fedorov\u2019s exchange method (Fedorov, 1972). This is due to its simplicity and its empirical performance (Cook and Nachtrheim, 1980; Miller and Nguyen, 1994; Atkinson et al., 2007).  However, despite its wide usage no theoretical bound has been proven for this algorithm. In this paper, we bridge this gap and prove approximation guarantees for the local search algorithms for D-optimal design and A-optimal design problems. We show that the local search algorithms are asymptotically optimal when $\\frac{k}{d}$ is large. In addition to this, we also prove similar approximation guarantees for the greedy algorithms for D-optimal design and A-optimal design problems when $\\frac{k}{d}$ is large.", "pdf_url": "http://proceedings.mlr.press/v99/madan19a/madan19a.pdf", "keywords": ["Optimal Design", "Experimental Design", "D-optimal design", "A-optimal design", "Fedorov Exchange", "Local Search", "Greedy Algorithm"], "reference": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments via regret minimization. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th In- ternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 126-135. PMLR, 2017.  Anthony Atkinson, Alexander Donev, and Randall Tobias. Optimum experimental designs, with  SAS, volume 34. Oxford University Press, 2007.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  Dennis S Bernstein. Matrix mathematics: Theory, facts, and formulas with application to linear  systems theory, volume 41. Princeton university press Princeton, 2005.  Mustapha Bouhtou, Stephane Gaubert, and Guillaume Sagnol. Submodularity and randomized rounding techniques for optimal experimental design. Electronic Notes in Discrete Mathematics, 36:679-686, 2010.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  Ali C\u00b8 ivril and Malik Magdon-Ismail. On selecting a maximum volume sub-matrix of a matrix and  related problems. Theoretical Computer Science, 410(47-49):4801-4811, 2009.  R Dennis Cook and Christopher J Nachtrheim. A comparison of algorithms for constructing exact  D-optimal designs. Technometrics, 22(3):315-324, 1980.  Micha\u0142 Derezi\u00b4nski and Manfred K Warmuth. Subsampling for ridge regression via regularized  volume sampling. arXiv preprint arXiv:1710.05110, 2017.  Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approx- imation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13 (Dec):3475-3506, 2012.  Walter Theodore Federer et al. Experimental design. Macmillan Co., New York and London, 1955.  Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 1972.  David A Harville. Matrix algebra from a statistician\u2019s perspective, volume 1. Springer, 1997.  Carl D Meyer, Jr. Generalized inversion of modified matrices.  SIAM Journal on Applied Mathe-  matics, 24(3):315-323, 1973.  Alan J Miller and Nam-Ky Nguyen. Algorithm as 295: A fedorov exchange algorithm for D-optimal design. Journal of the royal statistical society. series c (applied statistics), 43(4):669-677, 1994.  Toby J Mitchell and FL Miller Jr. Use of design repair to construct designs for special linear models.  Math. Div. Ann. Progr. Rept.(ORNL-4661), 13, 1970.  Nam-Ky Nguyen and Alan J Miller. A review of some exchange algorithms for constructing discrete  D-optimal designs. Computational Statistics & Data Analysis, 14(4):489-498, 1992.  13   COMBINATORIAL ALGORITHMS FOR OPTIMAL DESIGN  References  Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design of experiments via regret minimization. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th In- ternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 126-135. PMLR, 2017.  Anthony Atkinson, Alexander Donev, and Randall Tobias. Optimum experimental designs, with  SAS, volume 34. Oxford University Press, 2007.  Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM  Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013.  Dennis S Bernstein. Matrix mathematics: Theory, facts, and formulas with application to linear  systems theory, volume 41. Princeton university press Princeton, 2005.  Mustapha Bouhtou, Stephane Gaubert, and Guillaume Sagnol. Submodularity and randomized rounding techniques for optimal experimental design. Electronic Notes in Discrete Mathematics, 36:679-686, 2010.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  Ali C\u00b8 ivril and Malik Magdon-Ismail. On selecting a maximum volume sub-matrix of a matrix and  related problems. Theoretical Computer Science, 410(47-49):4801-4811, 2009.  R Dennis Cook and Christopher J Nachtrheim. A comparison of algorithms for constructing exact  D-optimal designs. Technometrics, 22(3):315-324, 1980.  Micha\u0142 Derezi\u00b4nski and Manfred K Warmuth. Subsampling for ridge regression via regularized  volume sampling. arXiv preprint arXiv:1710.05110, 2017.  Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approx- imation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13 (Dec):3475-3506, 2012.  Walter Theodore Federer et al. Experimental design. Macmillan Co., New York and London, 1955.  Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 1972.  David A Harville. Matrix algebra from a statistician\u2019s perspective, volume 1. Springer, 1997.  Carl D Meyer, Jr. Generalized inversion of modified matrices.  SIAM Journal on Applied Mathe-  matics, 24(3):315-323, 1973.  Alan J Miller and Nam-Ky Nguyen. Algorithm as 295: A fedorov exchange algorithm for D-optimal design. Journal of the royal statistical society. series c (applied statistics), 43(4):669-677, 1994.  Toby J Mitchell and FL Miller Jr. Use of design repair to construct designs for special linear models.  Math. Div. Ann. Progr. Rept.(ORNL-4661), 13, 1970.  Nam-Ky Nguyen and Alan J Miller. A review of some exchange algorithms for constructing discrete  D-optimal designs. Computational Statistics & Data Analysis, 14(4):489-498, 1992. COMBINATORIAL ALGORITHMS FOR OPTIMAL DESIGN  Aleksandar Nikolov, Mohit Singh, and Uthaipon Tao Tantipongpipat. Proportional volume sampling In Proceedings of the Thirtieth Annual  and approximation algorithms for A-optimal design. ACM-SIAM Symposium on Discrete Algorithms, pages 1369-1386. SIAM, 2019.  Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.  Guillaume Sagnol and Radoslav Harman. Computing exact d-optimal designs by mixed integer  second-order cone programming. The Annals of Statistics, 43(5):2198-2224, 2015.  Alexander Schrijver. Theory of linear and integer programming. John Wiley & Sons, 1998.  Mohit Singh and Weijun Xie. Approximate positive correlated distributions and approximation  algorithms for D-optimal design. To appear in SODA, 2018.  Yining Wang, Adams Wei Yu, and Aarti Singh. On computationally tractable selection of experi-  ments in regression models. arXiv preprint arXiv:1601.02068, 2016.  William J Welch. Algorithmic complexity:  three np-hard problems in computational statistics.  Journal of Statistical Computation and Simulation, 15(1):17-25, 1982.  "}, "Nonconvex sampling with the Metropolis-adjusted Langevin algorithm": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Nonconvex sampling with the Metropolis-adjusted Langevin algorithm", "abstract": "The Langevin Markov chain algorithms are widely deployed methods to sample from distributions in challenging high-dimensional and non-convex statistics and machine learning applications. Despite this, current bounds for the Langevin algorithms are worse than those of competing algorithms in many important situations, for instance when sampling from weakly log-concave distributions, or when sampling or optimizing non-convex log-densities. We obtain improved bounds in many of these situations, showing that the Metropolis-adjusted Langevin algorithm (MALA) is faster than the best bounds for its competitor algorithms when the target distribution satisfies weak third- and fourth- order regularity properties associated with the input data. In many settings, our regularity conditions are weaker than the usual Euclidean operator norm regularity properties, allowing us to show faster bounds for a much larger class of distributions than would be possible with the usual Euclidean operator norm approach,  including in statistics and machine learning applications where the data satisfy a certain incoherence condition. In particular, we show that using our regularity conditions one can obtain faster bounds for applications which include sampling problems in Bayesian logistic regression with weakly convex priors, and the nonconvex optimization problem of learning linear classifiers with zero-one loss functions. Our main technical contribution is an analysis of the Metropolis acceptance probability of MALA in terms of its \u201cenergy-conservation error,\" and a bound for this error in terms of third- and fourth- order regularity conditions. The combination of this higher-order analysis of the energy conservation error with the conductance method is key to obtaining bounds which have a sub-linear dependence on the dimension $d$ in the non-strongly logconcave setting.", "pdf_url": "http://proceedings.mlr.press/v99/mangoubi19a/mangoubi19a.pdf", "keywords": [], "reference": "David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear  separators under bounded noise. In Conference on Learning Theory, pages 167-190, 2015.  Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the MALA algorithm. IMA Journal  of Numerical Analysis, 33(1):80-110, 2013.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceed-  ings of Algorithmic Learning Theory, pages 186-211, 2018.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-  dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.  Alain Durmus, Eric Moulines, et al. Nonasymptotic convergence analysis for the unadjusted  Langevin algorithm. The Annals of Applied Probability, 27(3):1551-1587, 2017.  Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: Metropolis-  Hastings algorithms are fast! In Conference on Learning Theory, pages 793-797, 2018.  Andreas Eberle et al. Error bounds for Metropolis-Hastings algorithms applied to perturbations of Gaussian measures in high dimensions. The Annals of Applied Probability, 24(1):337-377, 2014.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Michel Ledoux. The geometry of Markov diffusion generators.  In Annales de la Facult\u00b4e des sciences de Toulouse: Math\u00b4ematiques, volume 9, pages 305-366. Universit\u00b4e Paul Sabatier, 2000.  Holden Lee, Oren Mangoubi, and Nisheeth K Vishnoi. Online sampling from log-concave distribu-  tions. arXiv preprint arXiv:1902.08179, 2019.  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Sym- posium on Theory of Computing, pages 1115-1121. ACM, 2018.  Yin Tat Lee and Santosh Srinivas Vempala. Eldan\u2019s stochastic localization and the KLS hyper- plane conjecture: An improved lower bound for expansion. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 998-1007. IEEE, 2017.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993.  13   NONCONVEX MALA  References  David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear  separators under bounded noise. In Conference on Learning Theory, pages 167-190, 2015.  Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the MALA algorithm. IMA Journal  of Numerical Analysis, 33(1):80-110, 2013.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceed-  ings of Algorithmic Learning Theory, pages 186-211, 2018.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-  dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.  Alain Durmus, Eric Moulines, et al. Nonasymptotic convergence analysis for the unadjusted  Langevin algorithm. The Annals of Applied Probability, 27(3):1551-1587, 2017.  Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: Metropolis-  Hastings algorithms are fast! In Conference on Learning Theory, pages 793-797, 2018.  Andreas Eberle et al. Error bounds for Metropolis-Hastings algorithms applied to perturbations of Gaussian measures in high dimensions. The Annals of Applied Probability, 24(1):337-377, 2014.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Michel Ledoux. The geometry of Markov diffusion generators.  In Annales de la Facult\u00b4e des sciences de Toulouse: Math\u00b4ematiques, volume 9, pages 305-366. Universit\u00b4e Paul Sabatier, 2000.  Holden Lee, Oren Mangoubi, and Nisheeth K Vishnoi. Online sampling from log-concave distribu-  tions. arXiv preprint arXiv:1902.08179, 2019.  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Sym- posium on Theory of Computing, pages 1115-1121. ACM, 2018.  Yin Tat Lee and Santosh Srinivas Vempala. Eldan\u2019s stochastic localization and the KLS hyper- plane conjecture: An improved lower bound for expansion. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 998-1007. IEEE, 2017.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993. NONCONVEX MALA  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Hit-and-run is fast and fun. preprint, Microsoft Research,  2003.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Fast algorithms for logconcave functions: Sampling, round- ing, integration and optimization. In Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 57-68. IEEE, 2006a.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing, 35  (4):985-1005, 2006b.  Oren Mangoubi and Nisheeth K Vishnoi. Convex optimization with unbounded nonconvex oracles  using simulated annealing. In Conference On Learning Theory, pages 1086-1124, 2018a.  Oren Mangoubi and Nisheeth K Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte Carlo. In Advances in neural information processing systems, pages 6027-6037, 2018b.  Oren Mangoubi and Nisheeth K Vishnoi. Dimensionally tight bounds for second-order Hamiltonian  monte carlo. arXiv preprint arXiv:1802.08898, 2018c.  Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,  2:113-162, 2011.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic In Conference on Learning Theory,  gradient Langevin dynamics: a nonasymptotic analysis. pages 1674-1703, 2017.  Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling of discrete approximations to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1): 255-268, 1998.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Santosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry,  52(573-612):2, 2005.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient  Langevin dynamics. In Conference on Learning Theory, pages 1980-2022, 2017.  "}, "Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance", "abstract": "We consider learning methods based on the regularization of a convex empirical risk by a squared Hilbertian norm, a setting that includes linear predictors and non-linear predictors through positive-definite kernels. In order to go beyond the generic analysis leading to convergence rates of the excess risk as $O(1/\\sqrt{n})$ from $n$ observations, we assume that the individual losses are self-concordant, that is, their third-order derivatives are bounded by their second-order derivatives. This setting includes least-squares, as well as all generalized linear models such as logistic and softmax regression. For this class of losses, we provide a bias-variance decomposition and show that the assumptions commonly made in least-squares regression, such as the source and capacity conditions, can be adapted to obtain fast non-asymptotic rates of convergence by improving the bias terms, the variance terms or both.", "pdf_url": "http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf", "keywords": ["Self-concordance", "regularization", "logistic regression", "non-parametric estimation"], "reference": "society, 68(3):337-404, 1950.  384-414, 2010.  Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  M. S. Bartlett. Approximate confidence intervals. Biometrika, 40(1/2):12-19, 1953.  Gilles Blanchard and Nicole M\u00fccke. Optimal rates for regularization of statistical inverse learning  problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.  L\u00e9on Bottou and Olivier Bousquet. The trade-offs of large scale learning. In Advances in Neural  Information Processing systems, pages 161-168, 2008.  St\u00e9phane Boucheron and Pascal Massart. A high-dimensional Wilks phenomenon. Probability  Theory and Related Fields, 150(3-4):405-433, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Found.  Comput. Math., 7(3):331-368, July 2007.  Nicolo Cesa-Bianchi, Yishay Mansour, and Ohad Shamir. On the complexity of learning with  kernels. In Conference on Learning Theory, pages 297-325, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-  rithm. arXiv preprint arXiv:1702.07254, 2017.  Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic re-  gression: The importance of being improper. Proceedings of COLT, 2018.  Stuart Geman, Elie Bienenstock, and Ren\u00e9 Doursat. Neural networks and the bias/variance  dilemma. Neural Computation, 4(1):1-58, 1992.  L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E. De Vito, and Alessandro Verri. Spectral  algorithms for supervised learning. Neural Computation, 20(7):1873-1897, 2008.  Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statis-  tics: the approach based on in\ufb02uence functions, volume 196. John Wiley & Sons, 2011.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: tight bounds for stochastic and online optimization. In Proceedings of The 27th Conference on Learning Theory, volume 35, pages 197-209, 2014.  Arthur E. Hoerl and Robert W. Kennard. Ridge regression iterative estimation of the biasing param-  eter. Communications in Statistics-Theory and Methods, 5(1):77-88, 1976.  13   FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  References  society, 68(3):337-404, 1950.  384-414, 2010.  Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  M. S. Bartlett. Approximate confidence intervals. Biometrika, 40(1/2):12-19, 1953.  Gilles Blanchard and Nicole M\u00fccke. Optimal rates for regularization of statistical inverse learning  problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.  L\u00e9on Bottou and Olivier Bousquet. The trade-offs of large scale learning. In Advances in Neural  Information Processing systems, pages 161-168, 2008.  St\u00e9phane Boucheron and Pascal Massart. A high-dimensional Wilks phenomenon. Probability  Theory and Related Fields, 150(3-4):405-433, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Found.  Comput. Math., 7(3):331-368, July 2007.  Nicolo Cesa-Bianchi, Yishay Mansour, and Ohad Shamir. On the complexity of learning with  kernels. In Conference on Learning Theory, pages 297-325, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363-1399, 2016.  Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-  rithm. arXiv preprint arXiv:1702.07254, 2017.  Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic re-  gression: The importance of being improper. Proceedings of COLT, 2018.  Stuart Geman, Elie Bienenstock, and Ren\u00e9 Doursat. Neural networks and the bias/variance  dilemma. Neural Computation, 4(1):1-58, 1992.  L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E. De Vito, and Alessandro Verri. Spectral  algorithms for supervised learning. Neural Computation, 20(7):1873-1897, 2008.  Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statis-  tics: the approach based on in\ufb02uence functions, volume 196. John Wiley & Sons, 2011.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: tight bounds for stochastic and online optimization. In Proceedings of The 27th Conference on Learning Theory, volume 35, pages 197-209, 2014.  Arthur E. Hoerl and Robert W. Kennard. Ridge regression iterative estimation of the biasing param-  eter. Communications in Statistics-Theory and Methods, 5(1):77-88, 1976. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  S. Sathiya Keerthi, K. B. Duan, Shirish Krishnaj Shevade, and Aun Neow Poo. A fast dual algorithm  for kernel logistic regression. Machine learning, 61(1-3):151-165, 2005.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems, pages 1477-1485, 2015.  John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Confer- ence on Machine Learning (ICML), 2001.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Science & Business  Media, 2006.  P. McCullagh and J.A. Nelder. Generalized Linear Models. Chapman and Hall, 1989.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. Technical  Report 1605.01288, ArXiv, 2016.  ming, volume 13. SIAM, 1994.  Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-  Dmitrii Ostrovskii and Francis Bach.  Finite-sample analysis of M-estimators using self-  concordance. Technical Report 1810.06838, arXiv, 2018.  Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Advances in Neural In- formation Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada., pages 8125-8135, 2018.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss func-  tions. arXiv preprint arXiv:1501.06598, 2015.  Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems 30, pages 3215-3225. Curran Associates, Inc., 2017.  Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel  method. In Advances in Neural Information Processing Systems, pages 3888-3898, 2017.  Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated  sub-gradient solver for SVM. Mathematical programming, 127(1):3-30, 2011.  John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-  sity Press, 2004.  Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their ap-  proximations. Constructive Approximation, 26(2):153-172, 2007.  Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro. Fast rates for regularized objectives.  In Advances in Neural Information Processing Systems 21, pages 1545-1552. 2009. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  Ingo Steinwart and Clint Scovel. Fast rates for support vector machines using gaussian kernels. The  Annals of Statistics, 35(2):575-607, 2007.  Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regres-  sion. In Proc. COLT, 2009.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht. Large scale kernel  learning using block coordinate descent. arXiv preprint arXiv:1602.05310, 2016.  Sara A. Van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of  Statistics, 36(2):614-645, 2008.  Aad W. Van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.  Tim Van Erven, Peter D. Gr\u00fcnwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793- 1861, 2015.  Grace Wahba. Spline Models for Observational Data, volume 59. SIAM, 1990.  Vadim Yurinsky. Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics.  Springer-Verlag, Berlin, 1995. FAST RATES FOR REGULARIZED EMPIRICAL RISK MINIMIZATION THROUGH SELF-CONCORDANCE  Organization of the "}, "Planting trees in graphs, and finding them back": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Planting trees in graphs, and finding them back", "abstract": "In this paper we study the two inference problems of detection and reconstruction in the context of planted structures in sparse Erd\u0151s-R\u00e9nyi random graphs $\\mathcal G(n,\\lambda/n)$ with fixed average degree $\\lambda>0$. Motivated by a problem of communication security, we focus on the case where the planted structure consists in the addition of a tree graph.  In the case of planted line graphs, we establish the following phase diagram for detection and reconstruction. In a low density region where the average degree $\\lambda$ of the original graph is below some critical value $\\lambda_c=1$, both detection and reconstruction go from  impossible to easy as the line length $K$ crosses some critical value $K^*=\\ln(n)/\\ln(1/\\lambda)$, where $n$ is the number of nodes in the graph. In a high density region where $\\lambda>\\lambda_c$, detection goes from impossible to easy as $K$ goes from $o(\\sqrt{n})$ to $\\omega(\\sqrt{n})$. In contrast, reconstruction remains impossible so long as $K=o(n)$.  We then consider planted $D$-ary trees of varying depth $h$ and $2\\le D\\le O(1)$. For these we identify a low-density region $\\lambda\\lambda_D$, but confirm only the following part of this picture: Detection is easy for $D$-ary trees of size $\\omega(\\sqrt{n})$, while at best only partial reconstruction is feasible for $D$-ary trees of any size $o(n)$. These results provide a clear contrast with the corresponding picture for detection and reconstruction of {\\em low rank} planted structures, such as dense subgraphs and block communities.  In the examples we study, there is i) an absence of hard phases for both detection and reconstruction, and ii) a discrepancy between detection and reconstruction, the latter being impossible for a wide range of parameters where detection is easy. The latter property does not hold for previously studied low rank planted structures.", "pdf_url": "http://proceedings.mlr.press/v99/massoulie19a/massoulie19a.pdf", "keywords": [], "reference": "E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. In NIPS\u201916, 2016.  Vivek Kumar Bagaria, Jian Ding, David Tse, Yihong Wu, and Jiaming Xu. Hidden hamiltonian cycle recovery via linear programming. CoRR, abs/1804.05436, 2018. URL http://arxiv. org/abs/1804.05436.  Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization. IEEE Trans. Information Theory, 64(7):4872-4894, 2018. doi: 10.1109/TIT.2018.2810020. URL https://doi.org/10.1109/TIT.2018.2810020.  Boaz Barak, Samuel B. Hopkins, Jonathan Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. Foun- dations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, 2016.  A. D. Barbour and Louis H. Y. Chen, editors. An introduction to Stein\u2019s method, volume 4 of Lecture Notes Series. Institute for Mathematical Sciences. National University of Singapore. Singapore University Press, Singapore, 2005.  Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse PCA. CoRR,  abs/1304.0828, 2013. URL http://arxiv.org/abs/1304.0828.  Rajendra Bhatia. Matrix analysis, volume 169 of Graduate Texts in Mathematics. Springer-Verlag, New York, 1997. ISBN 0-387-94846-5. doi: 10.1007/978-1-4612-0653-8. URL http://dx. doi.org/10.1007/978-1-4612-0653-8.  B. Bollob\u00e1s. Random Graphs. Cambridge Studies in Advanced Mathematics. Cambridge Univer- sity Press, 2001. ISBN 9780521797221. URL https://books.google.fr/books?id= o9WecWgilzYC.  10   PLANTING TREES IN GRAPHS, AND FINDING THEM BACK  6.4. Reconstruction for large h When \u03bb < \u03bbD and h \u2265 h, we have shown that under P0 there is w.h.p no copy of \u0393 in G. One could therefore expect to be able to reconstruct \u0393 with overlap 1 \u2212 o(1) ; however, this is not the case :  Theorem 24 Given \u03bb > 0, h \u2265 h such that K = o(n), and a realization G of the graph under P1, the overlap achieved by any estimator \u02c6K of the attack is bounded above, i.e ov( \u02c6K) \u2264 (1 \u2212 \u03b4)K for some \u03b4 > 0.  The proof is based on the fact that when D > 1, the leaves make up a positive proportion of \u0393, and they are hard to reconstruct with high precision. On the other hand, since there is no copy of \u0393 in G w.h.p, one can still reasonably expect to achieve a partial reconstuction. This is the heuristic behind our second conjecture :  Conjecture 25 For all h \u2265 h, there exists a \u03b4 > 0 and an estimator (possibly random) \u02c6K such that w.h.p ov( \u02c6K) \u2265 \u03b4K.  References  E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. In NIPS\u201916, 2016.  Vivek Kumar Bagaria, Jian Ding, David Tse, Yihong Wu, and Jiaming Xu. Hidden hamiltonian cycle recovery via linear programming. CoRR, abs/1804.05436, 2018. URL http://arxiv. org/abs/1804.05436.  Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization. IEEE Trans. Information Theory, 64(7):4872-4894, 2018. doi: 10.1109/TIT.2018.2810020. URL https://doi.org/10.1109/TIT.2018.2810020.  Boaz Barak, Samuel B. Hopkins, Jonathan Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. Foun- dations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, 2016.  A. D. Barbour and Louis H. Y. Chen, editors. An introduction to Stein\u2019s method, volume 4 of Lecture Notes Series. Institute for Mathematical Sciences. National University of Singapore. Singapore University Press, Singapore, 2005.  Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse PCA. CoRR,  abs/1304.0828, 2013. URL http://arxiv.org/abs/1304.0828.  Rajendra Bhatia. Matrix analysis, volume 169 of Graduate Texts in Mathematics. Springer-Verlag, New York, 1997. ISBN 0-387-94846-5. doi: 10.1007/978-1-4612-0653-8. URL http://dx. doi.org/10.1007/978-1-4612-0653-8.  B. Bollob\u00e1s. Random Graphs. Cambridge Studies in Advanced Mathematics. Cambridge Univer- sity Press, 2001. ISBN 9780521797221. URL https://books.google.fr/books?id= o9WecWgilzYC. PLANTING TREES IN GRAPHS, AND FINDING THEM BACK  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00e9. Non-backtracking spectrum of ran- In IEEE 56th An- dom graphs: Community detection and non-regular ramanujan graphs. nual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17- doi: 10.1109/FOCS.2015.86. URL https: 20 October, 2015, pages 1347-1357, 2015. //doi.org/10.1109/FOCS.2015.86.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. OUP Oxford, 2013. ISBN 9780199535255. URL https://books.google. fr/books?id=koNqWRluhP0C.  Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computational lower bounds for problems with planted sparse structure. In S\u00e9bastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 48-166. PMLR, 06-09 Jul 2018. URL http://proceedings.mlr.press/v75/brennan18a.html.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106 (1-19), Dec 2011. doi: 10.1103/PhysRevE.84.066106.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high probability. Combinatorics, Probability and Computing, 23(1):29-49, 2014. doi: 10.1017/ S096354831300045X.  Y. Deshpande and A. Montanari. Finding hidden cliques of size (cid:112)N/e in nearly linear time. Foun-  dations of Computational Mathematics, 15(4):1069-1128, August 2015.  B. Hajek, Y. Wu, and J. Xu. Computational lower bounds for community detection on random  graphs. In Proceedings COLT 2015, pages 899--928, June 2015.  S. Janson, T. Luczak, and A. Rucinski. Random Graphs. Wiley Series in Discrete Mathematics and  Optimization. Wiley, 2011.  Tosio Kato. Perturbation Theory for Linear Operators. Springer, 1966.  L. Massouli\u00e9. Community detection thresholds and the weak ramanujan property. arXiv:1109.3318. The conference version appeared in Proceedings of the 46th Annual ACM Symposium on Theory of Computing, 2013.  Cristopher Moore and Stephan Mertens. The Nature of Computation. Oxford University Press, Inc.,  New York, NY, USA, 2011. ISBN 0199233217, 9780199233212.  E. Mossel, J. Neeman, and A. Sly.  A proof of the block model  threshold conjecture.  arxiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition  model. Probability Theory and Related Fields, 162(3-4):431-461, 2015. ISSN 0178-8051.  Nicolas Verzelen and Ery Arias-Castro. Community detection in sparse random networks. Ann. Appl. Probab., 25(6):3465-3510, 12 2015. doi: 10.1214/14-AAP1080. URL https://doi. org/10.1214/14-AAP1080. PLANTING TREES IN GRAPHS, AND FINDING THEM BACK  "}, "Uniform concentration and symmetrization for weak interactions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Uniform concentration and symmetrization for weak interactions", "abstract": "The method to derive uniform bounds with Gaussian and Rademacher complexities is extended to the case where the sample average is replaced by a nonlinear statistic. Tight bounds are obtained for U-statistics, smoothened L-statistics and error functionals of l2-regularized algorithms.", "pdf_url": "http://proceedings.mlr.press/v99/maurer19a/maurer19a.pdf", "keywords": [], "reference": "S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, S., and D. Roth, D. Generalization bounds for  the area under the ROC curve. Journal of Machine Learning Research, 6:393-425, 2005.  A. Ambroladze, E. Parrado-Hern\u00b4andez, and J. Shawe-Taylor. Complexity of pattern classes and the  Lipschitz property. Theoretical Computer Science, 382(3):232-246, 2007.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural  results. Journal of Machine Learning Research, 3: 463-482, 2002.  12   UNIFORM CONCENTRATION AND SYMMETRIZATION FOR WEAK INTERACTIONS  then gives  f (h (X)) \u2212 f (cid:0)h (cid:0)X(cid:48)(cid:1)(cid:1) \u2264  E sup h  1 2  E sup h  +  1 2  E sup g  (cid:112)\u03c0/2 (cid:10)\u03b3m, vm  (cid:0)h (X) , h (cid:0)X(cid:48)(cid:1)(cid:1)(cid:11) + Rh  (cid:112)\u03c0/2 (cid:10)\u2212\u03b3m, vm  (cid:0)g (X) , g (cid:0)X(cid:48)(cid:1)(cid:1)(cid:11) + Rg  = E sup h  (cid:112)\u03c0/2 (cid:10)\u03b3m, vm  (cid:0)h (X) , h (cid:0)X(cid:48)(cid:1)(cid:1)(cid:11) + Rh.  By definition of Rh this completes the induction and proves the claim (12).  Let (\u03b71, ..., \u03b7n, \u03b7(cid:48) random variable (cid:80)n  1, ..., \u03b7(cid:48) k=1 \u03b3kiVki is identically distributed to  n) be isonormal and independent of all the \u03b3ki. By Lemma 7 (i) the  (cid:33)1/2  (cid:32) n (cid:88)  k=1  V 2 ki  (cid:112)  \u03b7i =  4M 2 + J 2 \u03b7i.  Unraveling the definition of vk it follows for every h \u2208 H that  n (cid:88)  k=1  (cid:10)\u03b3k, vk  (cid:0)h (X) , h (cid:0)X(cid:48)(cid:1)(cid:1)(cid:11) =  n (cid:88)  (cid:34)(cid:32) n (cid:88)  i=1  k=1  (cid:33)  \u03b3kiVki  h (Xi) +  (cid:33)  (cid:35)  \u03b3(cid:48) kiVki  h (cid:0)X (cid:48) i  (cid:1)  (cid:32) n (cid:88)  k=1  is identically distributed to  (cid:112)  4M 2 + J 2  (cid:0)\u03b7ih (Xi) + \u03b7(cid:48)  ih (cid:0)X (cid:48)  i  (cid:1)(cid:1) .  n (cid:88)  i=1  Combined with (12) this gives  f (h (X)) \u2212 f (cid:0)h (cid:0)X(cid:48)(cid:1)(cid:1) \u2264 (cid:112)\u03c0/2E sup  (cid:10)\u03b3k, vk  (cid:0)h (X) , h (cid:0)X(cid:48)(cid:1)(cid:1)(cid:11)  E sup h  n (cid:88)  h  k=1  (cid:112)  = (cid:112)\u03c0/2 \u221a  \u2264  4M 2 + J 2 E sup h  i=1 2\u03c0 (2M + J) E G (H (X)) .  n (cid:88)  \u03b7ih (Xi) + \u03b7(cid:48)  ih (cid:0)X (cid:48)  i  (cid:1)  References  S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, S., and D. Roth, D. Generalization bounds for  the area under the ROC curve. Journal of Machine Learning Research, 6:393-425, 2005.  A. Ambroladze, E. Parrado-Hern\u00b4andez, and J. Shawe-Taylor. Complexity of pattern classes and the  Lipschitz property. Theoretical Computer Science, 382(3):232-246, 2007.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural  results. Journal of Machine Learning Research, 3: 463-482, 2002. UNIFORM CONCENTRATION AND SYMMETRIZATION FOR WEAK INTERACTIONS  G. Biau, L. Devroye, and G. Lugosi. On the performance of clustering in Hilbert spaces. IEEE  Transactions on Information Theory, 54(2):781-790, 2008.  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities, Oxford University Press, 2013.  Q. Cao, Z. C. Guo, and Y. Ying. Generalization bounds for metric and similarity learning. Machine  Learning, 102(1):115-132, 2016.  S. Cl\u00b4emenc\u00b8on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. The  Annals of Statistics, 36(2):844-874, 2008.  J. A. Cuesta-Albertos, A. Gordaliza, and C. Matr\u00b4an, C. Trimmed k-means: An attempt to robustify  quantizers. The Annals of Statistics, 25(2):553-576, 1997.  L. A. Garc\u00b4\u0131a-Escudero, A. Gordaliza, C. Matr\u00b4an, and A. Mayo-Iscar. A review of robust clustering  methods. Advances in Data Analysis and Classification, 4(2-3):89-109, 2010.  S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems, pp. 793-800, 2009.  S. M. Kakade, S. Shalev-Shwartz, and A. Tewari. Regularization Techniques for Learning with  Matrices. Journal of Machine Learning Research 13:1865-1890, 2012.  V. Koltchinskii and D. Panchenko, Empirical margin distributions and bounding the generalization  error of combined classifiers, The Annals of Statistics, 30(1):1-50, 2002.  M. Ledoux and M. Talagrand. Probability in Banach Spaces, Springer, 1991.  A. Maurer and M. Pontil. Empirical bounds for functions with weak interactions. Proceedings of  the 31st Annual Conference on Learning Theory, PMLR, 75:987-1010, 2018.  A. Maurer. A Second-order look at stability and generalization. Proceedings of the 30th Annual  Conference on Learning Theory, PMLR, 65:1461-1475, 2017.  A. Maurer. A Bernstein-type inequality for functions of bounded interaction. Bernoulli (Forthcom-  ing), (see also arXiv preprint arXiv:1701.06191).  C. McDiarmid. Concentration. In Probabilistic Methods of Algorithmic Discrete Mathematics, pp.  195-248. Springer, Berlin, 1998.  R. Meir and T. Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of  Machine Learning Research, 4:839-860, 2003.  Y. Ying, L. Wen, and S. Lyu. Stochastic online AUC maximization. In Advances in neural informa-  tion processing systems, pp. 451-459, 2016. UNIFORM CONCENTRATION AND SYMMETRIZATION FOR WEAK INTERACTIONS  "}, "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "abstract": "We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolution in the space of probability distributions in $\\mathbb{R}^D$ (where $D$ is the number of parameters associated to each neuron). This evolution can be defined through a partial differential equation or, equivalently, as the gradient flow in the Wasserstein space of probability distributions. Earlier work shows that (under some regularity assumptions), the mean field description is accurate as soon as the number of hidden units is much larger than the dimension $D$. In this paper we establish stronger and more general approximation guarantees. First of all, we show that the number of hidden units only needs to be larger than a quantity dependent on the regularity properties of the data, and independent of the dimensions. Next, we generalize this analysis to the case of unbounded activation functions, which was not covered by earlier bounds.  We extend our results to noisy stochastic gradient descent. Finally, we show that kernel ridge regression can be recovered as a special limit of  the mean field analysis.", "pdf_url": "http://proceedings.mlr.press/v99/mei19a/mei19a.pdf", "keywords": ["Mean-field", "neural networks", "kernel limit", "distributional dynamics", "residual dynamics"], "reference": "A Notations  3 Dimension-free mean field approximation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 General results 3.2 Example: Centered anisotropic Gaussians . . . . . . . . . . . . . . . . . . . . . .  .  .  .  .  .  4 Connection with kernel methods  4.1 A coupled dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Kernel limit of residual dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  B Proof of Theorem 1 part (A) B.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . B.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . B.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . B.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  C Proof of Theorem 1 part (B) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Technical lemmas . C.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . C.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . C.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . C.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  D Proof of Theorem 4 part (A) .  .  .  D.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . D.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . D.4 Bound between particle dynamic and GD . . . . . . . . . . . . . . . . . . . . . . D.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E Proof of Theorem 4 part (B) E.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . E.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . E.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . . E.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  15  1  4  5 5 8  9 9 10  14  16  17 19 20 21 23 23  24 26 29 30 32 33  36 37 40 41 42 43  43 45 49 52 56 58   MEAN-FIELD THEORY OF TWO-LAYERS NEURAL NETWORKS  ContentsIntroduction  2 Related work  References  A Notations  3 Dimension-free mean field approximation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 General results 3.2 Example: Centered anisotropic Gaussians . . . . . . . . . . . . . . . . . . . . . .  .  .  .  .  .  4 Connection with kernel methods  4.1 A coupled dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Kernel limit of residual dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  B Proof of Theorem 1 part (A) B.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . B.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . B.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . B.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  C Proof of Theorem 1 part (B) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Technical lemmas . C.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . C.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . C.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . C.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  D Proof of Theorem 4 part (A) .  .  .  D.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . D.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . D.4 Bound between particle dynamic and GD . . . . . . . . . . . . . . . . . . . . . . D.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E Proof of Theorem 4 part (B) E.1 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Bound between PDE and nonlinear dynamics . . . . . . . . . . . . . . . . . . . . E.3 Bound between nonlinear dynamics and particle dynamics . . . . . . . . . . . . . E.4 Bound between particle dynamics and GD . . . . . . . . . . . . . . . . . . . . . . E.5 Bound between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .15 5 8  9 9 1016  17 19 20 21 23 23  24 26 29 30 32 33  36 37 40 41 42 43  43 45 49 52 56 58   MEAN-FIELD THEORY OF TWO-LAYERS NEURAL NETWORKS  F Existence and uniqueness of PDEs solutions  F.1 Equation (DD) (noiseless SGD) F.2 Equation (diffusion-DD) (noisy SGD) F.3 The noisy PDE as a gradient \ufb02ow in the space of probability distributions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  G Proof of Theorem 8  H The mean field limit and kernel limit  H.1 Two layers neural networks with a scale parameter \u03b1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 The residual dynamics in the pre-limit . . . . . . . . . . . . . . . . . . . . . H.3 The distributional dynamics in the pre-limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.4 The coupled dynamics . . H.5 The mean field limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.6 The kernel limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.7 Kernel limit as kernel ridge regression . . . . . . . . . . . . . . . . . . . . . . . .  . . .  . .  .  I Technical lemmas  "}, "Batch-Size Independent Regret Bounds for the Combinatorial Multi-Armed Bandit Problem": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Batch-Size Independent Regret Bounds for the Combinatorial Multi-Armed Bandit Problem", "abstract": "We consider the combinatorial multi-armed bandit (CMAB) problem, where the reward function is nonlinear. In this setting, the agent chooses a batch of arms on each round and receives feedback from each arm of the batch. The reward that the agent aims to maximize is a function of the selected arms and their expectations. In many applications, the reward function is highly nonlinear, and the performance of existing algorithms relies on a global Lipschitz constant to encapsulate the function\u2019s nonlinearity. This may lead to loose regret bounds, since by itself, a large gradient does not necessarily cause a large regret, but only in regions where the uncertainty in the reward\u2019s parameters is high.  To overcome this problem, we introduce a new smoothness criterion, which we term \\emph{Gini-weighted smoothness}, that takes into account both the nonlinearity of the reward and concentration properties of the arms. We show that a linear dependence of the regret in the batch size in existing algorithms can be replaced by this smoothness parameter. This, in turn, leads to much tighter regret bounds when the smoothness parameter is batch-size independent. For example, in the probabilistic maximum coverage (PMC) problem, that has many applications, including influence maximization, diverse recommendations and more, we achieve dramatic improvements in the upper bounds. We also prove matching lower bounds for the PMC problem and show that our algorithm is tight, up to a logarithmic factor in the problem\u2019s parameters.", "pdf_url": "http://proceedings.mlr.press/v99/merlis19a/merlis19a.pdf", "keywords": ["Multi-Armed Bandits", "Combinatorial Bandits", "Probabilistic Maximum Coverage", "GiniWeighted Smoothness", "Empirical Bernstein"], "reference": "Pallavi Arora, Csaba Szepesv\u00e1ri, and Rong Zheng. Sequential learning for optimal monitoring of  multi-channel wireless networks. IEEE, 2011.  Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902, 2009.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed  bandit problem. SIAM journal on computing, 32(1):48-77, 2002b.  Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.  S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Felipe Caro and J\u00e9r\u00e9mie Gallien. Dynamic assortment with demand learning for seasonal consumer  goods. Management Science, 53(2):276-292, 2007.  Alexandra Carpentier and Michal Valko. Revealing graph bandits for maximizing local in\ufb02uence. In  International Conference on Artificial Intelligence and Statistics, 2016.  Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Combinatorial bandits. Journal of Computer and System  Sciences, 78(5):1404-1422, 2012.  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and  applications. In International Conference on Machine Learning, pages 151-159, 2013.  Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. The Journal of Machine Learning Research, 17(1): 1746-1778, 2016a.  Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial multi-armed bandit with general reward functions. In Advances in Neural Information Processing Systems, pages 1659-1667, 2016b.  Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille Laroche. Learning to rank: Regret lower bounds and efficient algorithms. ACM SIGMETRICS Performance Evaluation Review, 43(1):231-244, 2015a.  13   BATCH-SIZE INDEPENDENT REGRET BOUNDS FOR THE CMAB PROBLEM  The authors thank Asaf Cassel and Esther Derman for their helpful comments on the manuscript.  Acknowledgments  References  Pallavi Arora, Csaba Szepesv\u00e1ri, and Rong Zheng. Sequential learning for optimal monitoring of  multi-channel wireless networks. IEEE, 2011.  Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902, 2009.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed  bandit problem. SIAM journal on computing, 32(1):48-77, 2002b.  Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.  S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Felipe Caro and J\u00e9r\u00e9mie Gallien. Dynamic assortment with demand learning for seasonal consumer  goods. Management Science, 53(2):276-292, 2007.  Alexandra Carpentier and Michal Valko. Revealing graph bandits for maximizing local in\ufb02uence. In  International Conference on Artificial Intelligence and Statistics, 2016.  Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Combinatorial bandits. Journal of Computer and System  Sciences, 78(5):1404-1422, 2012.  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and  applications. In International Conference on Machine Learning, pages 151-159, 2013.  Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. The Journal of Machine Learning Research, 17(1): 1746-1778, 2016a.  Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial multi-armed bandit with general reward functions. In Advances in Neural Information Processing Systems, pages 1659-1667, 2016b.  Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille Laroche. Learning to rank: Regret lower bounds and efficient algorithms. ACM SIGMETRICS Performance Evaluation Review, 43(1):231-244, 2015a. BATCH-SIZE INDEPENDENT REGRET BOUNDS FOR THE CMAB PROBLEM  Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre Proutiere, et al. Combinato- rial bandits revisited. In Advances in Neural Information Processing Systems, pages 2116-2124, 2015b.  Imre Csisz\u00e1r and Zsolt Talata. Context tree estimation for not necessarily finite memory processes,  via bic and mdl. IEEE Transactions on Information theory, 52(3):1007-1016, 2006.  R\u00e9my Degenne and Vianney Perchet. Combinatorial semi-bandit with known covariance.  In  Advances in Neural Information Processing Systems, pages 2972-2980, 2016.  Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation. In New Frontiers in Dynamic Spectrum, 2010 IEEE Symposium on, pages 1-9. IEEE, 2010.  Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown IEEE/ACM  variables: Multi-armed bandits with linear rewards and individual observations. Transactions on Networking (TON), 20(5):1466-1478, 2012.  Aur\u00e9lien Garivier and Olivier Capp\u00e9. The kl-ucb algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual Conference On Learning Theory, pages 359-376, 2011.  Thibault Gisselbrecht, Ludovic Denoyer, Patrick Gallinari, and Sylvain Lamprier. Whichstreams: A dynamic approach for focused data capture from large social media. In Ninth International AAAI Conference on Web and Social Media, 2015.  Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized markov decision  processes. In Conference on Learning Theory, pages 861-898, 2015.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online  problems. In International Conference on Machine Learning, pages 100-108, 2014.  Dorit S Hochbaum. Approximation algorithms for NP-hard problems. PWS Publishing Co., 1996.  Alihan H\u00fcy\u00fck and Cem Tekin. Thompson sampling for combinatorial multi-armed bandit with  probabilistically triggered arms. arXiv preprint arXiv:1809.02707, 2018.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement  learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.  Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: fast combinatorial optimization with learning. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pages 420-429. AUAI Press, 2014.  Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In International Conference on Machine Learning, pages 767-776, 2015a.  Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Combinatorial cascading  bandits. In Advances in Neural Information Processing Systems, pages 1450-1458, 2015b. BATCH-SIZE INDEPENDENT REGRET BOUNDS FOR THE CMAB PROBLEM  Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Artificial Intelligence and Statistics, pages 535-543, 2015c.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in  applied mathematics, 6(1):4-22, 1985.  Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. preprint, 2018.  Tor Lattimore, Branislav Kveton, Shuai Li, and Csaba Szepesvari. Toprank: A practical algorithm for online stochastic ranking. In Advances in Neural Information Processing Systems, pages 3949-3958, 2018.  Keqin Liu and Qing Zhao. Adaptive shortest-path routing under unknown and stochastically varying link states. In Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (WiOpt), 2012 10th International Symposium on, pages 232-237. IEEE, 2012.  Subhojyoti Mukherjee, KP Naveen, Nandan Sudarsanam, and Balaraman Ravindran. Efficient-ucbv: An almost optimal algorithm using variance estimates. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions\u2014i. Mathematical programming, 14(1):265-294, 1978.  Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, pages 3003-3011, 2013.  Pierre Perrault, Vianney Perchet, and Michal Valko. Finding the bandit in a graph: Sequential  search-and-stop. arXiv preprint arXiv:1806.02282, 2018.  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American  Mathematical Society, 58(5):527-535, 1952.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.  Sharan Vaswani, Laks Lakshmanan, Mark Schmidt, et al. In\ufb02uence maximization with bandits.  arXiv preprint arXiv:1503.00024, 2015.  Qinshi Wang and Wei Chen. Improving regret bounds for combinatorial semi-bandits with proba- bilistically triggered arms and its applications. In Advances in Neural Information Processing Systems, pages 1161-1171, 2017.  Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In International  Conference on Machine Learning, pages 5101-5109, 2018.  Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online in\ufb02uence maximiza- tion under independent cascade model with semi-bandit feedback. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 3026-3036. Curran Associates Inc., 2017. BATCH-SIZE INDEPENDENT REGRET BOUNDS FOR THE CMAB PROBLEM  (9)  (20)  (21)  "}, "Lipschitz Adaptivity with Multiple Learning Rates in Online Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Lipschitz Adaptivity with Multiple Learning Rates in Online Learning", "abstract": "We aim to design adaptive online learning algorithms that take advantage of any special structure that might be present in the learning task at hand, with as little manual tuning by the user as possible. A fundamental obstacle that comes up in the design of such adaptive algorithms is to calibrate a so-called step-size or learning rate hyperparameter depending on variance, gradient norms, etc. A recent technique promises to overcome this difficulty by maintaining multiple learning rates in parallel. This technique has been applied in the MetaGrad algorithm for online convex optimization and the Squint algorithm for prediction with expert advice. However, in both cases the user still has to provide in advance a Lipschitz hyperparameter that bounds the norm of the gradients. Although this hyperparameter is typically not available in advance, tuning it correctly is crucial: if it is set too small, the methods may fail completely; but if it is taken too large, performance deteriorates significantly. In the present work we remove this Lipschitz hyperparameter by designing new versions of MetaGrad and Squint that adapt to its optimal value automatically. We achieve this by dynamically updating the set of active learning rates. For MetaGrad, we further improve the computational efficiency of handling constraints on the domain of prediction, and we remove the need to specify the number of rounds in advance.", "pdf_url": "http://proceedings.mlr.press/v99/mhammedi19a/mhammedi19a.pdf", "keywords": [], "reference": "Peter L. Bartlett, Elad Hazan, and Alexander Rakhlin. Adaptive online gradient descent. In Advances  in Neural Information Processing Systems 20 (NIPS), pages 65-72, 2007.  James R. Bunch, Christopher P. Nielsen, and Danny C. Sorensen. Rank-one modification of the  symmetric eigenproblem. Numerische Mathematik, 31(1):31-48, 1978.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and  Manfred K. Warmuth. How to use expert advice. J. ACM, 44(3):427-485, 1997.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.  Improved second-order bounds for  prediction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. Advances in Neural Information Processing Systems 22 (NIPS 2009), pages 297-305, 2009.  In  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pages 117-125, 2010.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Le, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proc. of the 25th Annual Confer- ence on Learning Theory (COLT), pages 6.1-6.20, 2012.  Ashok Cutkosky. Artificial constraints and lipschitz hints for unconstrained online learning. arXiv  preprint arXiv:1902.09013, 2019.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In Proceedings  of the 30th Annual Conference on Learning Theory (COLT), pages 643-677, 2017a.  Ashok Cutkosky and Kwabena A. Boahen. Stochastic and adversarial online learning without hyperparameters. In Advances in Neural Information Processing Systems, pages 5059-5067, 2017b.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pages 1493-1529, 2018.  Chuong B. Do, Quoc V. Le, and Chuan-Sheng Foo. Proximal regularization for online and batch learning. In Proc. of the 26th Annual International Conference on Machine Learning (ICML), pages 257-264, 2009.  13   LIPSCHITZ ADAPTIVITY  We thank the anonymous reviewers for feedback that improved the presentation. Part of this work was performed while Zakaria Mhammedi was conducting an internship at the Centrum Wiskunde & Informatica (CWI). This work was also supported by the Australian Research Council and Data61.  Acknowledgments  References  Peter L. Bartlett, Elad Hazan, and Alexander Rakhlin. Adaptive online gradient descent. In Advances  in Neural Information Processing Systems 20 (NIPS), pages 65-72, 2007.  James R. Bunch, Christopher P. Nielsen, and Danny C. Sorensen. Rank-one modification of the  symmetric eigenproblem. Numerische Mathematik, 31(1):31-48, 1978.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and  Manfred K. Warmuth. How to use expert advice. J. ACM, 44(3):427-485, 1997.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.  Improved second-order bounds for  prediction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. A parameter-free hedging algorithm. Advances in Neural Information Processing Systems 22 (NIPS 2009), pages 297-305, 2009.  In  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pages 117-125, 2010.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Le, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proc. of the 25th Annual Confer- ence on Learning Theory (COLT), pages 6.1-6.20, 2012.  Ashok Cutkosky. Artificial constraints and lipschitz hints for unconstrained online learning. arXiv  preprint arXiv:1902.09013, 2019.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In Proceedings  of the 30th Annual Conference on Learning Theory (COLT), pages 643-677, 2017a.  Ashok Cutkosky and Kwabena A. Boahen. Stochastic and adversarial online learning without hyperparameters. In Advances in Neural Information Processing Systems, pages 5059-5067, 2017b.  Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018., pages 1493-1529, 2018.  Chuong B. Do, Quoc V. Le, and Chuan-Sheng Foo. Proximal regularization for online and batch learning. In Proc. of the 26th Annual International Conference on Machine Learning (ICML), pages 257-264, 2009. LIPSCHITZ ADAPTIVITY  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.  Tim van Erven and Wouter M. Koolen. Metagrad: Multiple learning rates in online learning. In  Advances in Neural Information Processing Systems 29 (NIPS), pages 3666-3674, 2016.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances in  Neural Information Processing Systems 28 (NIPS), pages 3375-3383, 2015.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Zigzag: A new approach to adaptive online learning. In Proc. of the 2017 Annual Conference on Learning Theory (COLT), pages 876-924, 2017.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In Proc.  of the 27th Annual Conference on Learning Theory (COLT), pages 176-196, 2014.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2  (3-4):157-325, 2016.  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in  costs. Machine learning, 80(2-3):165-188, 2010.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Steve Howard, Aaditya Ramdas, John McAuliffe, and Jasjeet Sekhon. Exponential line-crossing  inequalities. ArXiv e-prints, August 2018.  Wouter M. Koolen. The relative entropy bound for Squint, August 2015. Adversarial Intelligence  blog.  Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinatorial  games. In Conference on Learning Theory, pages 1155-1175, 2015.  Wouter M. Koolen, Peter D. Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and stochastic fast rates in online learning. In Advances in Neural Information Processing Systems, pages 4457-4465, 2016.  Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive NormalHedge. In Proc. of The 28th Annual Conference on Learning Theory (COLT), pages 1286-1304, 2015.  Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In Advances in Neural Information Processing Systems 26 (NIPS), pages 2724-2732, 2013.  H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 244-256, 2010. LIPSCHITZ ADAPTIVITY  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proc. of  the 26th Annual Conference on Learning Theory (COLT), pages 993-1019, 2013.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15(1):1281-1316, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Rachel Ward, Xiaoxia Wu, and L\u00b4eon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex  landscapes, from any initialization. ArXiv:1806.01811 preprint, 2018.  Olivier Wintenberger. Optimal learning with Bernstein online aggregation. ArXiv:1404.1356v2  Olivier Wintenberger. Optimal learning with Bernstein online aggregation. Machine Learning, 106  preprint, 2014.  (1):119-141, 2017.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th Annual International Conference on Machine Learning (ICML), pages 928-936, 2003.  "}, "VC Classes are Adversarially Robustly Learnable, but Only Improperly": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "VC Classes are Adversarially Robustly Learnable, but Only Improperly", "abstract": "We study the question of learning an adversarially robust predictor. We show that any hypothesis class $\\mathcal{H}$ with finite VC dimension is robustly PAC learnable with an \\emph{improper} learning rule. The requirement of being improper is necessary as we exhibit examples of hypothesis classes $\\mathcal{H}$ with finite VC dimension that are \\emph{not} robustly PAC learnable with any \\emph{proper} learning rule.", "pdf_url": "http://proceedings.mlr.press/v99/montasser19a/montasser19a.pdf", "keywords": ["adversarial robustness", "PAC learning", "sample complexity", "improper learning"], "reference": "University Press, 1999.  M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  P. Assouad. Densit\u00b4e et dimension. Annales de l\u2019Institut Fourier (Grenoble), 33(3):233-282, 1983.  Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust  learning. arXiv preprint arXiv:1810.02180, 2018.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50:74-86, 1995.  Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Gior- In Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pages 387- 402. Springer, 2013.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik- Chervonenkis dimension. Journal of the Association for Computing Machinery, 36(4):929-965, 1989.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of evasion  adversaries. arXiv preprint arXiv:1806.01471, 2018.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. Journal of Machine Learning Research, 16:2377-2404, 2015.  O. David, S. Moran, and A. Yehudayoff. Supervised learning through the lens of compression. In  Advances in Neural Information Processing Systems 29, pages 2784-2792, 2016.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimen-  sion. Machine Learning, 21(3):269-304, 1995.  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial  examples. arXiv preprint arXiv:1412.6572, 2014.  T. Graepel, R. Herbrich, and J. Shawe-Taylor. PAC-Bayesian compression bounds on the prediction  error of learning algorithms for classification. Machine Learning, 59(1-2):55-76, 2005.  S. Hanneke, A. Kontorovich, and M. Sadigurschi. Sample compression for real-valued learners. In  Proceedings of the 30th International Conference on Algorithmic Learning Theory, 2019.  12   ADVERSARIALLY ROBUST LEARNABILITY  References  University Press, 1999.  M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  P. Assouad. Densit\u00b4e et dimension. Annales de l\u2019Institut Fourier (Grenoble), 33(3):233-282, 1983.  Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust  learning. arXiv preprint arXiv:1810.02180, 2018.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50:74-86, 1995.  Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Gior- In Joint gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. European conference on machine learning and knowledge discovery in databases, pages 387- 402. Springer, 2013.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik- Chervonenkis dimension. Journal of the Association for Computing Machinery, 36(4):929-965, 1989.  S\u00b4ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational  constraints. arXiv preprint arXiv:1805.10204, 2018.  Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of evasion  adversaries. arXiv preprint arXiv:1806.01471, 2018.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. Journal of Machine Learning Research, 16:2377-2404, 2015.  O. David, S. Moran, and A. Yehudayoff. Supervised learning through the lens of compression. In  Advances in Neural Information Processing Systems 29, pages 2784-2792, 2016.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimen-  sion. Machine Learning, 21(3):269-304, 1995.  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial  examples. arXiv preprint arXiv:1412.6572, 2014.  T. Graepel, R. Herbrich, and J. Shawe-Taylor. PAC-Bayesian compression bounds on the prediction  error of learning algorithms for classification. Machine Learning, 59(1-2):55-76, 2005.  S. Hanneke, A. Kontorovich, and M. Sadigurschi. Sample compression for real-valued learners. In  Proceedings of the 30th International Conference on Algorithmic Learning Theory, 2019. ADVERSARIALLY ROBUST LEARNABILITY  Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classification via function trans-  formation. arXiv preprint arXiv:1810.09519, 2018.  N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished  manuscript, 1986.  Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.  MIT press, 2018.  63(3):21:1-21:10, 2016.  S. Moran and A. Yehudayoff. Sample compression schemes for VC classes. Journal of the ACM,  B. K. Natarajan. On learning sets and functions. Machine Learning, 4:67-97, 1989.  Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial ex-  amples. arXiv preprint arXiv:1801.09344, 2018a.  Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-  bustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018b.  N. Sauer. On the density of families of sets. Journal of Combinatorial Theory (A), 13(1):145-147,  1972.  Cambridge, MA, 2012.  R. E. Schapire and Y. Freund. Boosting. Adaptive Computation and Machine Learning. MIT Press,  Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad- versarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge university press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In COLT, 2009.  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.  A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer, 1996.  V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag, New York,  1982.  V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their probabilities. Theory of Probability and its Applications, 16(2):264-280, 1971.  V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974. ADVERSARIALLY ROBUST LEARNABILITY  M. Warmuth. Compressing to VC dimension many points. In Proceedings of the 16th Conference  on Learning Theory, 2003.  Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5283-5292, 2018.  Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,  2012.  Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adversarially  robust generalization. arXiv preprint arXiv:1810.11914, 2018.  "}, "Affine Invariant Covariance Estimation for Heavy-Tailed Distributions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Affine Invariant Covariance Estimation for Heavy-Tailed Distributions", "abstract": "In this work we provide an estimator for the covariance matrix of a heavy-tailed multivariate distribution. We prove that the proposed estimator $\\widehat{\\mathbf{S}}$ admits an \\textit{affine-invariant} bound of the form \\[ (1-\\varepsilon) \\mathbf{S} \\preccurlyeq \\widehat{\\mathbf{S}} \\preccurlyeq (1+\\varepsilon) \\mathbf{S} \\]{in} high probability, where $\\mathbf{S}$ is the unknown covariance matrix, and $\\preccurlyeq$ is the positive semidefinite order on symmetric matrices. The result only requires the existence of fourth-order moments, and allows for $\\varepsilon = O(\\sqrt{\\kappa^4 d\\log(d/\\delta)/n})$ where $\\kappa^4$ is a measure of kurtosis of the distribution, $d$ is the dimensionality of the space, $n$ is the sample size, and $1-\\delta$ is the desired confidence level. More generally, we can allow for regularization with level $\\lambda$, then $d$ gets replaced with the degrees of freedom number. Denoting $\\text{cond}(\\mathbf{S})$ the condition number of $\\mathbf{S}$, the computational cost of the novel estimator is $O(d^2 n + d^3\\log(\\text{cond}(\\mathbf{S})))$, which is comparable to the cost of the sample covariance estimator in the statistically interesing regime $n \\ge d$. We consider applications of our estimator to eigenvalue estimation with relative error, and to ridge regression with heavy-tailed random design.", "pdf_url": "http://proceedings.mlr.press/v99/ostrovskii19a/ostrovskii19a.pdf", "keywords": ["Covariance estimation", "heavy-tailed distributions", "random design linear regression"], "reference": "Rudolf Ahlswede and Andreas Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Maria-Florina Balcan, Simon S. Du, Yining Wang, and Adams W. Yu. An improved gap-dependency analysis of the noisy power method. In Conference on Learning Theory, pages 284-309, 2016.  Peter J. Bickel and Elizaveta Levina. Regularized estimation of large covariance matrices. The  Annals of Statistics, 36(1):199-227, 2008.  Brendan O. Bradley and Murad S. Taqqu. Financial risk and heavy tails. Handbook of Heavy-Tailed  Distributions in Finance, ST Rachev, ed. Elsevier, Amsterdam, pages 35-103, 2003.  T. Tony Cai, Cun-Hui Zhang, and Harrison H. Zhou. Optimal rates of convergence for covariance  matrix estimation. Ann. Statist., 38(4):2118-2144, 2010. doi: 10.1214/09-AOS752.  Norm A. Campbell. Robust procedures in multivariate analysis I: Robust covariance estimation.  Applied statistics, pages 231-237, 1980.  Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.  Foundations of Computational Mathematics, 7(3):331-368, 2007.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1.  Springer series in statistics New York, NY, USA:, 2001.  Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2): 217-288, 2011.  Lars P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:  Journal of the Econometric Society, pages 1029-1054, 1982.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In  Advances in Neural Information Processing Systems, pages 2861-2869, 2014.  Daniel Hsu, Sham M. Kakade, and Tong Zhang. Random design analysis of ridge regression. The  Journal of Machine Learning Research, 23(9):1-24, 2012.  Ian Jolliffe. Principal component analysis. In International encyclopedia of statistical science, pages  1094-1096. Springer, 2002.  2841-2871, 2014.  Bryan Kelly and Hao Jiang. Tail risk and asset prices. The Review of Financial Studies, 27(10):  Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample  covariance operators. Bernoulli, 23(1):110-133, 02 2017.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.  Springer Science & Business Media, 2013.  13   AFFINE INVARIANT COVARIANCE ESTIMATION FOR HEAVY-TAILED DISTRIBUTIONS  References  Rudolf Ahlswede and Andreas Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Maria-Florina Balcan, Simon S. Du, Yining Wang, and Adams W. Yu. An improved gap-dependency analysis of the noisy power method. In Conference on Learning Theory, pages 284-309, 2016.  Peter J. Bickel and Elizaveta Levina. Regularized estimation of large covariance matrices. The  Annals of Statistics, 36(1):199-227, 2008.  Brendan O. Bradley and Murad S. Taqqu. Financial risk and heavy tails. Handbook of Heavy-Tailed  Distributions in Finance, ST Rachev, ed. Elsevier, Amsterdam, pages 35-103, 2003.  T. Tony Cai, Cun-Hui Zhang, and Harrison H. Zhou. Optimal rates of convergence for covariance  matrix estimation. Ann. Statist., 38(4):2118-2144, 2010. doi: 10.1214/09-AOS752.  Norm A. Campbell. Robust procedures in multivariate analysis I: Robust covariance estimation.  Applied statistics, pages 231-237, 1980.  Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.  Foundations of Computational Mathematics, 7(3):331-368, 2007.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1.  Springer series in statistics New York, NY, USA:, 2001.  Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2): 217-288, 2011.  Lars P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:  Journal of the Econometric Society, pages 1029-1054, 1982.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In  Advances in Neural Information Processing Systems, pages 2861-2869, 2014.  Daniel Hsu, Sham M. Kakade, and Tong Zhang. Random design analysis of ridge regression. The  Journal of Machine Learning Research, 23(9):1-24, 2012.  Ian Jolliffe. Principal component analysis. In International encyclopedia of statistical science, pages  1094-1096. Springer, 2002.  2841-2871, 2014.  Bryan Kelly and Hao Jiang. Tail risk and asset prices. The Review of Financial Studies, 27(10):  Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample  covariance operators. Bernoulli, 23(1):110-133, 02 2017.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.  Springer Science & Business Media, 2013. AFFINE INVARIANT COVARIANCE ESTIMATION FOR HEAVY-TAILED DISTRIBUTIONS  Oleg V. Lepskii. On a problem of adaptive estimation in Gaussian white noise. Theory of Probability  & Its Applications, 35(3):454-466, 1991.  Hendrik P. Lopuhaa and Peter J. Rousseeuw. Breakdown points of affine equivariant estimators of multivariate location and covariance matrices. The Annals of Statistics, 19(1):229-248, 1991.  Karim Lounici. High-dimensional covariance matrix estimation with missing observations. Bernoulli,  20(3):1029-1058, 08 2014.  Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77-91, 1952.  Shahar Mendelson and Nikita Zhivotovskiy. Robust covariance estimation under L4 \u2212 L2 norm  equivalence. arXiv:1809.10462, 2018.  Stanislav Minsker. Sub-gaussian estimators of the mean of a random matrix with heavy-tailed entries.  The Annals of Statistics, 46(6A):2871-2903, 2018.  Stanislav Minsker and Xiaohan Wei. Estimation of the covariance structure of heavy-tailed distribu-  tions. arXiv:1708.00502, 2017.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming pca. In  Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  Roberto I. Oliveira. Sums of random Hermitian matrices and an inequality by Rudelson. Electron.  Commun. Probab., 15(26):203-212, 2010.  Roberto I. Oliveira. The lower tail of random quadratic forms with applications to ordinary least  squares. Probability Theory and Related Fields, 166(3-4):1175-1194, 2016.  Peter J. Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance  determinant estimator. Technometrics, 41(3):212-223, 1999.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in  Machine Learning, 8(1-2):1-230, 2015.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed  Sensing: Theory and Applications, pages 210-268. Cambridge University Press, 2012.  Xiaohan Wei and Stanislav Minsker. Estimation of the covariance structure of heavy-tailed distribu-  tions. In Advances in Neural Information Processing Systems, pages 2859-2868, 2017. AFFINE INVARIANT COVARIANCE ESTIMATION FOR HEAVY-TAILED DISTRIBUTIONS  "}, "Stochastic Gradient Descent Learns State Equations with Nonlinear Activations": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Stochastic Gradient Descent Learns State Equations with Nonlinear Activations", "abstract": "We study discrete time dynamical systems governed by the state equation $h_{t+1}=\\phi(Ah_t+Bu_t)$. Here $A,B$ are weight matrices, $\\phi$ is an activation function, and $u_t$ is the input data. This relation is the backbone of recurrent neural networks (e.g.\u00a0LSTMs) which have broad applications in sequential learning tasks. We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $\\{u_t,h_t\\}_{t=0}^N$. We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size. Our results apply to increasing activations whose derivatives are bounded away from zero. The analysis is based on i) a novel SGD convergence result with nonlinear activations and ii) careful statistical characterization of the state vector. Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.", "pdf_url": "http://proceedings.mlr.press/v99/oymak19a/oymak19a.pdf", "keywords": ["state equation", "dynamical systems", "sample complexity", "stochastic gradient descent"], "reference": "Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Karl Johan \u00c5str\u00f6m and Peter Eykhoff. System identification\u2014a survey. Automatica, 7(2):123-162, 1971.  Karl Johan \u00c5str\u00f6m and Tore H\u00e4gglund. PID controllers: theory, design, and tuning, volume 2. Instrument  society of America Research Triangle Park, NC, 1995.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to  align and translate. arXiv preprint arXiv:1409.0473, 2014.  Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks.  arXiv preprint  arXiv:1901.00434, 2019.  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural  networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.  SIAM journal on imaging sciences, 2(1):183-202, 2009.  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the  bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.  Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering,  volume 3. Wiley New York, 1992.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.  arXiv preprint arXiv:1702.07966, 2017.  Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix  completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models  using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.  Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint  arXiv:1709.06129, 2017.  Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in  unstable linear systems. Automatica, 96:342-353, 2018.  Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex  learning and optimization. NIPS, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.  arXiv preprint arXiv:1711.00501, 2017.  13   STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  References  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-  parameterization. arXiv preprint arXiv:1811.03962, 2018.  Karl Johan \u00c5str\u00f6m and Peter Eykhoff. System identification\u2014a survey. Automatica, 7(2):123-162, 1971.  Karl Johan \u00c5str\u00f6m and Tore H\u00e4gglund. PID controllers: theory, design, and tuning, volume 2. Instrument  society of America Research Triangle Park, NC, 1995.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to  align and translate. arXiv preprint arXiv:1409.0473, 2014.  Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks.  arXiv preprint  arXiv:1901.00434, 2019.  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural  networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.  SIAM journal on imaging sciences, 2(1):183-202, 2009.  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the  bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.  Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering,  volume 3. Wiley New York, 1992.  Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.  arXiv preprint arXiv:1702.07966, 2017.  Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix  completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models  using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.  Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint  arXiv:1709.06129, 2017.  Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in  unstable linear systems. Automatica, 96:342-353, 2018.  Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex  learning and optimization. NIPS, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.  arXiv preprint arXiv:1711.00501, 2017. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches.  arXiv preprint arXiv:1802.02547, 2018.  Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645-6649. IEEE, 2013.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv  preprint arXiv:1609.05191, 2016.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human- level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015.  BL Ho and Rudolph E Kalman. Effective construction of linear state-variable models from input/output  functions. at-Automatisierungstechnik, 14(1-12):545-548, 1966.  Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,  1997.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed  training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv preprint  arXiv:1810.02032, 2018.  Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927-935, 2011.  Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks.  arXiv preprint arXiv:1711.00811, 2017.  Michel Ledoux. The concentration of measure phenomenon. American Mathematical Soc., 2001.  Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680, 2019.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In  Advances in Neural Information Processing Systems, pages 597-607, 2017.  Lennart Ljung. System identification: theory for the user. Prentice-hall, 1987.  Lennart Ljung. System identification. In Signal analysis and prediction, pages 163-173. Springer, 1998.  Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses. The Annals  of Statistics, 46(6A):2747-2774, 2018a.  Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers  neural networks. arXiv preprint arXiv:1804.06561, 2018b.  John Miller and Moritz Hardt. When recurrent models don\u2019t need to be recurrent.  arXiv preprint  arXiv:1805.10369, 2018.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to  spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understand- ing the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018.  Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. arXiv  preprint arXiv:1806.05722, 2018.  Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the  shortest path? International Conference on Machine Learning, 2019.  Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time-data tradeoffs for linear inverse  problems. IEEE Transactions on Information Theory, 64(6):4129-4158, 2018.  Jos\u00e9 Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic differential equations.  In Advances in Neural Information Processing Systems, pages 172-180, 2010.  Borhan M Sanandaji, Tyrone L Vincent, and Michael B Wakin. Exact topology identification of large-scale interconnected dynamical systems from compressive observations. In American Control Conference (ACC), 2011, pages 649-656. IEEE, 2011a.  Borhan M Sanandaji, Tyrone L Vincent, Michael B Wakin, Roland T\u00f3th, and Kameshwar Poolla. Compressive system identification of lti and ltv arx models. In Decision and Control and European Control Conference (CDC-ECC), 2011 50th IEEE Conference on, pages 791-798. IEEE, 2011b.  Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral  methods. arXiv preprint arXiv:1603.00954, 2016.  Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing:  Towards a sharp analysis of linear system identification. arXiv preprint arXiv:1802.08334, 2018.  Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.  Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape  of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.  Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic  Processes, pages 13-73. Springer, 2014.  Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust control  from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.  Stephen Tu, Ross Boczar, and Benjamin Recht. On the approximation of toeplitz operators for nonparametric (cid:104)\u221e-norm estimation. In 2018 Annual American Control Conference (ACC), pages 1867-1872. IEEE, 2018.  Roman Vershynin.  Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with  multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.  Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for  one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b. STOCHASTIC GRADIENT DESCENT LEARNS NONLINEAR STATE EQUATIONS  "}, "A Theory of Selective Prediction": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "A Theory of Selective Prediction", "abstract": "We consider a model of selective prediction, where the prediction algorithm is given a data sequence in an online fashion and asked to predict a pre-specified statistic of the upcoming data points. The algorithm is allowed to choose when to make the prediction as well as the length of the prediction window, possibly depending on the observations so far. We prove that, even without any distributional assumption on the input data stream, a large family of statistics can be estimated to non-trivial accuracy. To give one concrete example, suppose that we are given access to an arbitrary binary sequence $x_1, \\ldots, x_n$ of length $n$.  Our goal is to accurately predict the average observation, and we are allowed to choose the window over which the prediction is made: for some $t < n$ and $m \\le n - t$, after seeing $t$ observations we predict the average of $x_{t+1}, \\ldots, x_{t+m}$. This particular problem was first studied in Drucker (2013) and referred to as the \u201cdensity prediction game\u201d. We show that the expected squared error of our prediction can be bounded by $O(\\frac{1}{\\log n})$ and prove a matching lower bound, which resolves an open question raised in Drucker (2013). This result holds for any sequence (that is not adaptive to when the prediction is made, or the predicted value), and the expectation of the error is with respect to the randomness of the prediction algorithm. Our results apply to more general statistics of a sequence of observations, and we highlight several open directions for future work.", "pdf_url": "http://proceedings.mlr.press/v99/qiao19a/qiao19a.pdf", "keywords": ["Online Learning", "Prediction"], "reference": "press, 2006.  Avrim Blum and Adam Kalai. Universal portfolios with and without transaction costs. Machine  Learning, 35(3):193\u2013205, 1999.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427\u2013485, 1997.  Thomas M Cover. Universal portfolios. Mathematical Finance, 1(1):1\u201329, 1991.  Thomas M Cover and Erik Ordentlich. Universal portfolios with side information. Transactions on  Information Theory (TIT), 42(2):348\u2013363, 1996.  Lee H Dicker. Variance estimation in high-dimensional linear models. Biometrika, 101(2):269\u2013284,  2014.  Andrew Drucker. High-con\ufb01dence predictions under adversarial uncertainty. Transactions on Com-  putation Theory (TOCT), 5(3):12, 2013.  Meir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences.  Transactions on Information Theory (TIT), 38(4):1258\u20131270, 1992.  Uriel Feige, Tomer Koren, and Moshe Tennenholtz. Chasing ghosts: competing with stateful poli-  cies. SIAM Journal on Computing, 46(1):190\u2013223, 2017.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,  3:97\u2013139, 1957.  Weihao Kong and Gregory Valiant. Estimating learnability in the sublinear data regime. In Advances  in Neural Information Processing Systems (NeurIPS), pages 5460\u20135469, 2018.  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm.  Information and  Computation, 108(2):212\u2013261, 1994.  Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. Prediction with a short memory.  In Symposium on Theory of Computing (STOC), pages 1074\u20131087, 2018.  Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Conformal prediction. Springer, 2005.  Appendix A. Missing Proofs  A.1. Proof of Proposition 9  Proof In the \ufb01rst case that t is known to the adversary, we simply construct a binary sequence such that xt+1 = \u00b7 \u00b7 \u00b7 = xn, and xt+1 is randomly drawn from {0, 1} with equal probability. When A  13   A THEORY OF SELECTIVE PREDICTION  References  press, 2006.  Avrim Blum and Adam Kalai. Universal portfolios with and without transaction costs. Machine  Learning, 35(3):193-205, 1999.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427-485, 1997.  Thomas M Cover. Universal portfolios. Mathematical Finance, 1(1):1-29, 1991.  Thomas M Cover and Erik Ordentlich. Universal portfolios with side information. Transactions on  Information Theory (TIT), 42(2):348-363, 1996.  Lee H Dicker. Variance estimation in high-dimensional linear models. Biometrika, 101(2):269-284,  2014.  Andrew Drucker. High-confidence predictions under adversarial uncertainty. Transactions on Com-  putation Theory (TOCT), 5(3):12, 2013.  Meir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences.  Transactions on Information Theory (TIT), 38(4):1258-1270, 1992.  Uriel Feige, Tomer Koren, and Moshe Tennenholtz. Chasing ghosts: competing with stateful poli-  cies. SIAM Journal on Computing, 46(1):190-223, 2017.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,  3:97-139, 1957.  Weihao Kong and Gregory Valiant. Estimating learnability in the sublinear data regime. In Advances  in Neural Information Processing Systems (NeurIPS), pages 5460-5469, 2018.  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm.  Information and  Computation, 108(2):212-261, 1994.  Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. Prediction with a short memory.  In Symposium on Theory of Computing (STOC), pages 1074-1087, 2018.  Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Conformal prediction. Springer, 2005.  "}, "Consistency of Interpolation with Laplace Kernels is a High-Dimensional Phenomenon": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Consistency of Interpolation with Laplace Kernels is a High-Dimensional Phenomenon", "abstract": "We show that minimum-norm interpolation in the Reproducing Kernel Hilbert Space corresponding to the Laplace kernel is not consistent if input dimension is constant. The lower bound holds for any choice of kernel bandwidth, even if selected based on data. The result supports the empirical observation that minimum-norm interpolation (that is, exact fit to training data) in RKHS generalizes well for some high-dimensional datasets, but not for low-dimensional ones.", "pdf_url": "http://proceedings.mlr.press/v99/rakhlin19a/rakhlin19a.pdf", "keywords": ["List of keywords"], "reference": "Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classi-  fication and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018a.  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-  stand kernel learning. arXiv preprint arXiv:1802.01396, 2018b.  Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict  statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-  parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-  mation Processing Systems, pages 2422-2430, 2017.  Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes  over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.  Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):  1-50, 2010.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of  nonparametric regression. Springer Science & Business Media, 2006.  Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler. Neural tangent kernel: Convergence and gen-  eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.  10   INTERPOLATION LOWER BOUNDS  allows, under a number of additional assumptions, the estimation error to be small. The interaction of dimensionality, sample size, and eigenvalue decays for the population and sample covariance matrices is complex, and identifying all the regimes when interpolation succeeds is still a largely unexplored area. In particular, our lower bound becomes vacuous as soon as d starts to scale with n. It would be interesting to understand the minimal scaling of d along with assumptions on the underlying distribution that allow minimum-norm interpolation to succeed.  Partial motivation for the study of interpolation methods comes from the recent successes of neural networks. These overparametrized models are typically trained to achieve zero error on the training data (Zhang et al., 2016; Belkin et al., 2018b), yet perform well out-of-sample. Recent work connecting sufficiently wide neural networks and the effective kernel (Mei et al., 2018; Chizat and Bach, 2018; Daniely, 2017; Jacot et al., 2018; Du et al., 2018) suggests that interpolating neural networks can be studied through the lens of kernel methods. In particular, it can be shown that the limiting solutions in such cases are, in fact, minimum-norm interpolants with respect to the corre- sponding kernel. Hence, further study of strengths and limitations of minimum-norm interpolation can shed light on performance of neural networks.  This work was partly supported by the DARPA Lagrange program and the MIT-Sensetime Alliance on Artificial Intelligence.  Acknowledgments  References  Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classi-  fication and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018a.  Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-  stand kernel learning. arXiv preprint arXiv:1802.01396, 2018b.  Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict  statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-  parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.  Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Infor-  mation Processing Systems, pages 2422-2430, 2017.  Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes  over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.  Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):  1-50, 2010.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of  nonparametric regression. Springer Science & Business Media, 2006.  Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler. Neural tangent kernel: Convergence and gen-  eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018. INTERPOLATION LOWER BOUNDS  Giovanni Leoni. A First Course in Sobolev Spaces, Second Edition. American Mathematical Soci-  ety, 2017.  Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel\u201d ridgeless\u201d regression can gener-  alize. arXiv preprint arXiv:1808.00387, 2018.  Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of  two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.  Elias M Stein and Guido Weiss. Introduction to Fourier analysis on Euclidean spaces (PMS-32).  Princeton University Press, 1971.  Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of adaboost and random forests as interpolating classifiers. The Journal of Machine Learning Re- search, 18(1):1558-1590, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.  "}, "Classification with unknown class-conditional label noise on non-compact feature spaces": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Classification with unknown class-conditional label noise on non-compact feature spaces", "abstract": "We investigate the problem of classification in the presence of unknown class-conditional label noise in which the labels observed by the learner have been corrupted with some unknown class dependent probability. In order to obtain finite sample rates, previous approaches to classification with unknown class-conditional label noise have required that the regression function is close to its extrema on sets of large measure. We shall consider this problem in the setting of non-compact metric spaces, where the regression function need not attain its extrema. In this setting we determine the minimax optimal learning rates (up to logarithmic factors). The rate displays interesting threshold behaviour: When the regression function approaches its extrema at a sufficient rate, the optimal learning rates are of the same order as those obtained in the label-noise free setting. If the regression function approaches its extrema more gradually then classification performance necessarily degrades. In addition, we present an adaptive algorithm which attains these rates without prior knowledge of either the distributional parameters or the local density. This identifies for the first time a scenario in which finite sample rates are achievable in the label noise setting, but they differ from the optimal rates without label noise.", "pdf_url": "http://proceedings.mlr.press/v99/reeve19a/reeve19a.pdf", "keywords": ["Label noise", "minimax rates", "non-parametric classification", "metric spaces"], "reference": "J-Y Audibert. Classification under polynomial entropy and margin assumptions and randomized  estimators. www.proba.jussieu.fr/mathdoc/textes/PMA-908.pdf., 2004.  Jean-Yves Audibert, Alexandre B Tsybakov, et al. Fast learning rates for plug-in classifiers. The  Annals of statistics, 35(2):608-633, 2007.  L Birg\u00b4e. A new look at an old result: Fano\u2019s lemma. Technical Report, Universite Paris VI., 2001. Gilles Blanchard, Gyemin Lee, and Clayton Scott. Semi-supervised novelty detection. Journal of  Machine Learning Research, 11(Nov):2973-3009, 2010.  Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, and Clayton Scott. Classification with asymmetric label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10 (2):2780-2824, 2016.  Jakramate Bootkrajang and Ata Kab\u00b4an. Learning kernel logistic regression in the presence of class  label noise. Pattern Recognition, 47(11):3641-3655, 2014.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, Gilles Stoltz, and Csaba Szepesv\u00b4ari. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011.  T. I. Cannings, Y. Fan, and R. J. Samworth. Classification with imperfect training labels. ArXiv  e-prints, May 2018.  Timothy I. Cannings, Thomas B. Berrett, and Richard J. Samworth. Local nearest neighbour clas-  sification with applications to semi-supervised learning. CoRR, abs/1704.00642, 2017.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Imre Csisz\u00b4ar and Zsolt Talata. Context tree estimation for not necessarily finite memory processes,  via bic and mdl. IEEE Transactions on Information theory, 52(3):1007-1016, 2006.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-nn density and mode estimation. In  Advances in Neural Information Processing Systems, pages 2555-2563, 2014.  Maik D\u00a8oring, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Harro Walk. Rate of convergence of k-nearest-neighbor classifi-  cation rule. Journal of Machine Learning Research, 18:227:1-227:16, 2017.  Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data.  In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, New York, NY, USA, 2008. ACM.  Beno\u02c6\u0131t Fr\u00b4enay and Michel Verleysen. Classification in the presence of label noise: A survey. IEEE  Trans. Neural Netw. Learning Syst., 25(5):845-869, 2014.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification in general finite dimensional spaces with the k nearest neighbour rule. The Annals of Statistics, 44(3):982-1009, 06 2016. Wei Gao, Xin-Yi Niu, and Zhi-Hua Zhou. On the consistency of exact and approximate nearest  neighbor with noisy data. Arxiv, abs/1607.07526, 2018.  Heinrich Jiang. Non-asymptotic uniform rates of consistency for k-nn regression. In Proceedings  of the 33rd AAAI Conference on Artificial Intelligence. AAAI, 2019.  13   CLASSIFICATION WITH LABEL NOISE ON NON-COMPACT FEATURE SPACES  This work is funded by EPSRC under Fellowship grant EP/P004245/1. The authors would like to thank Nikolaos Nikolaou and Timothy I. Cannings for useful discussions. We would also like to thank the anonymous reviewers for their careful feedback which led to several improvements in the presentation.  Acknowledgments  References  J-Y Audibert. Classification under polynomial entropy and margin assumptions and randomized  estimators. www.proba.jussieu.fr/mathdoc/textes/PMA-908.pdf., 2004.  Jean-Yves Audibert, Alexandre B Tsybakov, et al. Fast learning rates for plug-in classifiers. The  Annals of statistics, 35(2):608-633, 2007.  L Birg\u00b4e. A new look at an old result: Fano\u2019s lemma. Technical Report, Universite Paris VI., 2001. Gilles Blanchard, Gyemin Lee, and Clayton Scott. Semi-supervised novelty detection. Journal of  Machine Learning Research, 11(Nov):2973-3009, 2010.  Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, and Clayton Scott. Classification with asymmetric label noise: Consistency and maximal denoising. Electronic Journal of Statistics, 10 (2):2780-2824, 2016.  Jakramate Bootkrajang and Ata Kab\u00b4an. Learning kernel logistic regression in the presence of class  label noise. Pattern Recognition, 47(11):3641-3655, 2014.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, Gilles Stoltz, and Csaba Szepesv\u00b4ari. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011.  T. I. Cannings, Y. Fan, and R. J. Samworth. Classification with imperfect training labels. ArXiv  e-prints, May 2018.  Timothy I. Cannings, Thomas B. Berrett, and Richard J. Samworth. Local nearest neighbour clas-  sification with applications to semi-supervised learning. CoRR, abs/1704.00642, 2017.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Imre Csisz\u00b4ar and Zsolt Talata. Context tree estimation for not necessarily finite memory processes,  via bic and mdl. IEEE Transactions on Information theory, 52(3):1007-1016, 2006.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-nn density and mode estimation. In  Advances in Neural Information Processing Systems, pages 2555-2563, 2014.  Maik D\u00a8oring, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Harro Walk. Rate of convergence of k-nearest-neighbor classifi-  cation rule. Journal of Machine Learning Research, 18:227:1-227:16, 2017.  Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data.  In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, New York, NY, USA, 2008. ACM.  Beno\u02c6\u0131t Fr\u00b4enay and Michel Verleysen. Classification in the presence of label noise: A survey. IEEE  Trans. Neural Netw. Learning Syst., 25(5):845-869, 2014.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification in general finite dimensional spaces with the k nearest neighbour rule. The Annals of Statistics, 44(3):982-1009, 06 2016. Wei Gao, Xin-Yi Niu, and Zhi-Hua Zhou. On the consistency of exact and approximate nearest  neighbor with noisy data. Arxiv, abs/1607.07526, 2018.  Heinrich Jiang. Non-asymptotic uniform rates of consistency for k-nn regression. In Proceedings  of the 33rd AAAI Conference on Artificial Intelligence. AAAI, 2019. CLASSIFICATION WITH LABEL NOISE ON NON-COMPACT FEATURE SPACES  Samory Kpotufe. k-nn regression adapts to local intrinsic dimension. In Advances in Neural Infor-  mation Processing Systems, pages 729-737, 2011.  Samory Kpotufe and Vikas Garg. Adaptivity to local smoothness and dimension in kernel regres-  sion. In Advances in neural information processing systems, pages 3075-3083, 2013.  Oleg V Lepski and Vladimir G Spokoiny. Optimal pointwise adaptive methods in nonparametric  estimation. The Annals of Statistics, pages 2512-2546, 1997.  Fuyi Li, Yang Zhang, Anthony W Purcell, Geoffrey I Webb, Kuo-Chen Chou, Trevor Lithgow, Chen Li, and Jiangning Song. Positive-unlabelled learning of glycosylation sites in the human proteome. BMC bioinformatics, 20(1):112, 2019.  Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits. In Con-  ference on Learning Theory, pages 1463-1492, 2018.  Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Ann. Statist., 27(6):  1808-1829, 12 1999. doi: 10.1214/aos/1017939240.  Aditya Menon, Brendan van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from cor- rupted binary labels via class-probability estimation. In International Conference on Machine Learning, pages 125-134, 2015.  Aditya Krishna Menon, Brendan van Rooyen, and Nagarajan Natarajan. Learning from binary  labels with instance-dependent noise. Machine Learning, 107(8):1561-1595, Sep 2018.  Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in neural information processing systems, pages 1196-1204, 2013. Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Cost-sensitive  learning with noisy labels. Journal of Machine Learning Research, 18(155):1-33, 2018.  Harish Ramaswamy, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel In International Conference on Machine Learning, pages 2052-  embeddings of distributions. 2060, 2016.  Henry W. J. Reeve and Ata Kab\u00b4an. Fast rates for a knn classifier robust to unknown asymmetric label noise. In Proceedings of the 36th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, Long Beach, California, 10-15 Jul 2019. PMLR. Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning  from noisy labels. In Artificial Intelligence and Statistics, pages 838-846, 2015.  Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In Conference On Learning Theory, pages 489-511, 2013. Brendan van Rooyen and Robert C Williamson. A theory of learning with corrupted labels. Journal  of Machine Learning Research, 18(228):1-50, 2018. CLASSIFICATION WITH LABEL NOISE ON NON-COMPACT FEATURE SPACES  "}, "The All-or-Nothing Phenomenon in Sparse Linear Regression": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The All-or-Nothing Phenomenon in Sparse Linear Regression", "abstract": "We study the problem of recovering a hidden binary $k$-sparse $p$-dimensional vector $\\beta$ from $n$ noisy linear observations $Y=X\\beta+W$ where $X_{ij}$ are i.i.d.  $\\mathcal{N}(0,1)$ and $W_i$ are i.i.d.  $\\mathcal{N}(0,\\sigma^2)$. A closely related  hypothesis testing problem is to distinguish the pair $(X,Y)$ generated from this structured model from a corresponding null model where $(X,Y)$ consist of purely independent Gaussian entries. In the low sparsity $k=o(p)$ and high signal to noise ratio $k/\\sigma^2=\\Omega\\left(1\\right)$ regime, we establish an \u201cAll-or-Nothing\u201d information-theoretic phase transition at a critical sample size $n^*=2 k\\log \\left(p/k\\right) /\\log \\left(1+k/\\sigma^2\\right)$, resolving a conjecture of [GamarnikZadik17]. Specifically, we show that if $\\liminf_{p\\rightarrow \\infty} n/n^*>1$, then the maximum likelihood estimator almost perfectly recovers the hidden vector with high probability and moreover the true hypothesis can be detected with a vanishing error probability. Conversely, if $\\limsup_{p\\rightarrow \\infty} n/n^*<1$, then it becomes information-theoretically impossible even to  recover an arbitrarily small but fixed fraction of the hidden vector support, or to test hypotheses strictly better than random guess. Our proof of the impossibility result builds upon two key techniques, which could be of independent interest. First, we use a conditional second moment method to upper bound the Kullback-Leibler (KL) divergence between the structured and the null model. Second, inspired by the celebrated area theorem, we establish a lower bound to the minimum mean squared estimation error of the hidden vector in terms of the KL divergence between the two models.", "pdf_url": "http://proceedings.mlr.press/v99/reeves19a/reeves19a.pdf", "keywords": ["Sparse linear regression", "conditional second moment method", "area theorem"], "reference": "Shuchin Aeron, Venkatesh Saligrama, and Manqi Zhao.  Information theoretic bounds for com- pressed sensing. IEEE Transactions on Information Theory, 56(10):5111-5130, October 2010. doi: 10.1109/TIT.2010.2059891.  Mehmet Akcakaya and Vahid Tarokh. Shannon-theoretic limits on noisy compressive sampling. IEEE Transactions on Information Theory, 56(1):492-504, December 2010. doi: 10.1109/TIT. 2009.2034796.  Ahmed El Alaoui, Florent Krzakala, and Michael I Jordan. Finite size corrections and likelihood  ratio \ufb02uctuations in the spiked Wigner model. arXiv preprint arXiv:1710.02903, 2017.  J. Barbier and F. Krzakala. Replica analysis and approximate message passing decoder for superpo- sition codes. In 2014 IEEE International Symposium on Information Theory, pages 1494-1498, June 2014. doi: 10.1109/ISIT.2014.6875082.  J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, Aug 2017. ISSN 0018-9448. doi: 10.1109/TIT.2017.2713833.  Jean Barbier, Mohamad Dia, Nicolas Macris, and Florent Krzakala. The mutual information in random linear estimation. In Proceedings of the Allerton Conference on Communication, Control, and Computing, Monticello, IL, 2016.  A. R. Barron and S. Cho. High-rate sparse superposition codes with iteratively optimal estimates.  Proc. IEEE Int. Symp. Inf. Theory, 2012.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Alan Miller. Chapman and Hall. Subset selection in regression. Chapman and Hall, 1990.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  S. Cho. High-dimensional regression with random design, including sparse superposition codes.  Ph.D. dissertation, Dept. Statist., Yale Univ., New Haven, CT, USA, 2014.  David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-  1306, 2006.  Alyson K. Fletcher, Sundeep Rangan, and Vivek K Goyal. Necessary and sufficient conditions IEEE Transactions on Information Theory, 55(12):5758-5772,  for sparsity pattern recovery. November 2009. doi: 10.1109/TIT.2009.2032726.  10   THE ALL-OR-NOTHING PHENOMENON IN SPARSE LINEAR REGRESSION  G. Reeves is supported by the NSF Grants CCF-1718494 and CCF-1750362. J. Xu is supported by the NSF Grants CCF-1850743, IIS-1838124, and CCF-1856424.  Acknowledgment  References  Shuchin Aeron, Venkatesh Saligrama, and Manqi Zhao.  Information theoretic bounds for com- pressed sensing. IEEE Transactions on Information Theory, 56(10):5111-5130, October 2010. doi: 10.1109/TIT.2010.2059891.  Mehmet Akcakaya and Vahid Tarokh. Shannon-theoretic limits on noisy compressive sampling. IEEE Transactions on Information Theory, 56(1):492-504, December 2010. doi: 10.1109/TIT. 2009.2034796.  Ahmed El Alaoui, Florent Krzakala, and Michael I Jordan. Finite size corrections and likelihood  ratio \ufb02uctuations in the spiked Wigner model. arXiv preprint arXiv:1710.02903, 2017.  J. Barbier and F. Krzakala. Replica analysis and approximate message passing decoder for superpo- sition codes. In 2014 IEEE International Symposium on Information Theory, pages 1494-1498, June 2014. doi: 10.1109/ISIT.2014.6875082.  J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, Aug 2017. ISSN 0018-9448. doi: 10.1109/TIT.2017.2713833.  Jean Barbier, Mohamad Dia, Nicolas Macris, and Florent Krzakala. The mutual information in random linear estimation. In Proceedings of the Allerton Conference on Communication, Control, and Computing, Monticello, IL, 2016.  A. R. Barron and S. Cho. High-rate sparse superposition codes with iteratively optimal estimates.  Proc. IEEE Int. Symp. Inf. Theory, 2012.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Alan Miller. Chapman and Hall. Subset selection in regression. Chapman and Hall, 1990.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  S. Cho. High-dimensional regression with random design, including sparse superposition codes.  Ph.D. dissertation, Dept. Statist., Yale Univ., New Haven, CT, USA, 2014.  David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-  1306, 2006.  Alyson K. Fletcher, Sundeep Rangan, and Vivek K Goyal. Necessary and sufficient conditions IEEE Transactions on Information Theory, 55(12):5758-5772,  for sparsity pattern recovery. November 2009. doi: 10.1109/TIT.2009.2032726. THE ALL-OR-NOTHING PHENOMENON IN SPARSE LINEAR REGRESSION  David Gamarnik and Ilias Zadik. High dimensional linear regression with binary coefficients: Mean squared error and a phase transition. Conference on Learning Theory (COLT), 2017a. URL https://arxiv.org/abs/1701.04455.  David Gamarnik and Ilias Zadik. Sparse high dimensional linear regression: Algorithmic barrier and a local search algorithm. arXiv Preprint, 2017b. URL https://arxiv.org/abs/ 1711.04952.  David Gamarnik and Ilias Zadik. High dimensional linear regression using lattice basis reduction.  In Advances in Neural Information Processing Systems (NIPS), 2018.  Dongning Guo and Sergio Verd\u00b4u. Randomly spread CDMA: Asymptotics via statistical physics.  IEEE Transactions on Information Theory, 51(6):1983-2010, June 2005.  Yuzhe Jin, Young-Han Kim, and Bhaskar D Rao. Limits on support recovery of sparse signals via multiple-access communication techniques. IEEE Transactions on Information Theory, 57(12): 7877-7892, 2011.  A. Joseph and A. R. Barron. Fast sparse superposition codes have near exponential error probability  for r \u00a1 c,. IEEE Trans. Inf. Theory, vol. 60, no. 2, pp. 919-942, 2014.  Antony Joseph and Andrew R. Barron. Least sqaures superposition codes of moderate dictionary-  size are reliable at rates up to capacity. IEEE Transactions on Information Theory, 2012.  Shrinivas Kudekar, Santhosh Kumar, Marco Mondelli, Henry D Pfister, Eren S\u00b8 as\u00b8o\u02c7glu, and IEEE Trans-  R\u00a8udiger L Urbanke. Reed-muller codes achieve capacity on erasure channels. actions on Information Theory, 63(7):4298-4316, 2017.  Cyril M\u00b4easson, Andrea Montanari, and R\u00a8udiger Urbanke. Maxwell construction: The hidden bridge between iterative and maximum a posteriori decoding. IEEE Transactions on Information Theory, 54(12):5277-5307, 2008.  Mohamed Ndaoud and Alexandre B Tsybakov. Optimal variable selection and adaptive noisy com-  pressed sensing. arXiv preprint arXiv:1809.03145, 2018.  Amelia Perry, Alexander S. Wein, and Afonso S. Bandeira. Statistical limits of spiked tensor mod-  els. arXiv:1612.07728, Dec. 2016.  K. Rahnama Rad. Nearly sharp sufficient conditions on exact sparsity pattern recovery.  IEEE Transactions on Information Theory, 57(7):4672-4679, July 2011. ISSN 0018-9448. doi: 10. 1109/TIT.2011.2145670.  Galen Reeves. Conditional central limit theorems for Gaussian projections.  In Proceedings of the IEEE International Symposium on Information Theory (ISIT), pages 3055-3059, Aachen, Germany, June 2017.  Galen Reeves and Michael Gastpar. The sampling rate-distortion tradeoff for sparsity pattern recov- ery in compressed sensing. IEEE Transactions on Information Theory, 58(5):3065-3092, May 2012. doi: 10.1109/TIT.2012.2184848. THE ALL-OR-NOTHING PHENOMENON IN SPARSE LINEAR REGRESSION  Galen Reeves and Michael Gastpar. Approximate sparsity pattern recovery: Information-theoretic IEEE Transactions on Information Theory, 59(6):3451-3465, June 2013. doi:  lower bounds. 10.1109/TIT.2013.2253852.  Galen Reeves and Henry D. Pfister. The replica-symmetric prediction for compressed sensing with Gaussian matrices is exact. In Proceedings of the IEEE International Symposium on Information Theory (ISIT), pages 665 - 669, Barcelona, Spain, July 2016. doi: 10.1109/ISIT.2016.7541382. arXiv. Available: https://arxiv.org/abs/1607.02524.  Galen Reeves, Jiaming Xu, and Ilias Zadik. The all-or-nothing phenomenon in sparse linear regres-  sion. arXiv Preprint arXiv:1903.05046, 2019.  C. Rush, A. Greig, and R. Venkataramanan. Capacity-achieving sparse superposition codes via approximate message passing decoding. IEEE Trans. Inf. Theory, vol. 63, pp. 1476-1500, 2017.  Jonathan Scarlett and Volkan Cevher. Limits on support recovery with probabilistic models: An IEEE Transactions on Information Theory, 63(1):593-620,  information-theoretic framework. September 2017. doi: 10.1109/TIT.2016.2606605.  T. Tanaka. A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors.  IEEE Transactions on Information Theory, 48(11):2888-2910, November 2002.  Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using IEEE transactions on information theory, 55(5):  constrained quadratic programming (lasso). 2183-2202, 2009a.  Martin J. Wainwright. Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting. IEEE Transactions on Information Theory, 55(12):5728-5741, December 2009b.  Wei Wang, Martin J Wainwright, and Kannan Ramchandran. Information-theoretic limits on sparse signal recovery: Dense versus sparse measurement matrices. Information Theory, IEEE Trans- actions on, 56(6):2967-2979, 2010.  Yihong Wu and Jiaming Xu. Statistical problems with planted structures: Information-theoretical  and computational limits. arXiv preprint arXiv:1806.00118, 2018. "}, "Depth Separations in Neural Networks: What is Actually Being Separated?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Depth Separations in Neural Networks: What is Actually Being Separated?", "abstract": "Existing depth separation results for constant-depth networks essentially show that certain radial functions in $\\mathbb{R}^d$, which can be easily approximated with depth $3$ networks, cannot be approximated by depth $2$ networks, even up to constant accuracy, unless their size is exponential in $d$. However, the functions used to demonstrate this are rapidly oscillating, with a Lipschitz parameter scaling polynomially with the dimension $d$ (or equivalently, by scaling the function, the hardness result applies to $\\mathcal{O}(1)$-Lipschitz functions only when the target accuracy $\\epsilon$ is at most $\\text{poly}(1/d)$). In this paper, we study whether such depth separations might still hold in the natural setting of $\\mathcal{O}(1)$-Lipschitz radial functions, when $\\epsilon$ does not scale with $d$. Perhaps surprisingly, we show that the answer is negative: In contrast to the intuition suggested by previous work, it \\emph{is} possible to approximate $\\mathcal{O}(1)$-Lipschitz radial functions with depth $2$, size $\\text{poly}(d)$ networks, for every constant $\\epsilon$. We complement it by showing that approximating such functions is also possible with depth $2$, size $\\text{poly}(1/\\epsilon)$ networks, for every constant $d$. Finally, we show that it is not possible to have polynomial dependence in both $d,1/\\epsilon$ simultaneously. Overall, our results indicate that in order to show depth separations for expressing $\\mathcal{O}(1)$-Lipschitz functions with constant accuracy \u2013 if at all possible \u2013 one would need fundamentally different techniques than existing ones in the literature.", "pdf_url": "http://proceedings.mlr.press/v99/safran19a/safran19a.pdf", "keywords": [], "reference": "2017.  3   Depth Separations in Neural Networks:What is Actually Being Separated?  Theorem 3 Suppose \u03c3 : R \u2192 R satisfies Assumption 1. Then for any (cid:15) > 0 and any natural k \u2265 1, there exists a depth 2 neural network N with \u03c3 activations of width exp (cid:0)O (cid:0)k2 log (d/(cid:15))(cid:1)(cid:1) satisfying supx\u2208Bd (cid:12) (cid:12) \u2264 (cid:15), where the big O notation hides a constant that depends solely on \u03c3.  (cid:12)N (x) \u2212 ||x||2k(cid:12)  (cid:12) (cid:12)  Finally, we formally prove (using a reduction from Eldan and Shamir (2016); Daniely (2017), and using their assumptions) that it is impossible to obtain a general polynomial dependence on both d and 1/(cid:15) in our setting. More formally, we have the following two theorems:  Theorem 4 The following holds for some positive universal constants c1, c2, and any depth 2 network employing a ReLU activation function. Consider the 1-Lipschitz function f (x) = 1 on Bd. Suppose N is a depth 2 network of width w(d, 1/(cid:15)), with weights bounded by 2d+1 any d \u2265 2. Then for any d > c1,  2\u03c0d3 ||x||2 2 2\u03c0d3 , and satisfying supx\u2208Bd  |N (x) \u2212 f (x)| \u2264 (cid:15) for any (cid:15) > 0 and  2\u03c0d3 sin  (cid:17)  (cid:16)  w(d, 101 exp(2)\u03c03d3) \u2265 2c2d log d.  In particular, depth 2 networks of width poly(d, 1/(cid:15)) cannot approximate f to accuracy (cid:15).  Theorem 5 The following holds for some positive universal constants c1, c2, c3, c4, and any network employing an activation function satisfying Assumptions 1 and 2 in Eldan and Shamir (2016). Let f (x) = max {0, \u2212 ||x|| + 1}. For any d > c1, there exists a continuous probability distribution \u03b3 on Rd, such that for any (cid:15) > 0, and any depth 2 neural network N satisfying ||N (x) \u2212 f (x)||L2(\u03b3) \u2264 (cid:15) and having width w(d, 1/(cid:15)), it must hold that  w(d, c2d6) \u2265 c3 exp(c4d).  In particular, depth 2 networks of width poly(d, 1/(cid:15)) cannot approximate f to accuracy (cid:15).  Overall, these results show that to approximate radial functions with depth 2 networks, their width can be polynomial in either d or 1/(cid:15), but generally not in both.  This research is supported in part by European Research Council (ERC) grant 754705.  Amit Daniely. Depth separation for neural networks. arXiv preprint arXiv:1702.08489,  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In  29th Annual Conference on Learning Theory, pages 907-940, 2016.  Acknowledgements  References  2017. "}, "How do infinite width bounded norm networks look in function space?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "How do infinite width bounded norm networks look in function space?", "abstract": "We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions $f:\\mathbb R \\rightarrow\\mathbb R$ and a single hidden layer, we show that the minimal network norm for representing $f$ is $\\max(\\int \\lvert f\u201d(x) \\rvert \\mathrm{d} x, \\lvert  f\u2019(-\\infty) + f\u2019(+\\infty) \\rvert)$, and hence the minimal norm fit for a sample is given by a linear spline interpolation.", "pdf_url": "http://proceedings.mlr.press/v99/savarese19a/savarese19a.pdf", "keywords": [], "reference": "Learning Research, 2017.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine  Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE  Transactions on Information theory, 39(3):930\u2013945, 1993.  12   INFINITE WIDTH BOUNDED NORM NETWORKS  Much in the same as linear convolutional neural networks can represent any linear function, and the architecture\u2019s only role is to induce complexity control in that space, infinite width ReLU networks can represent any continuous function, and the role of the architecture, in our view, is to induce complexity control over function space. We see that indeed, even for univariate functions, the architecture already induces a natural complexity control that is not obvious nor explicit. Furthermore, we are particularly excited that this complexity control exactly matches the learning rules studied in recent work on interpolation learning (Belkin et al., 2018), and provides a concrete connection of that work to deep learning. We are eager to find out whether this connection carries also to the multivariate case.  We also hope that our study will reinvigorate approximation theory work on neural networks,  studying approximation by networks of bounded norm rather than a bounded number of units.  Similarly, we would argue that the study of the importance of depth in neural networks should focus not on gaps in the size (number of units) required to fit a function, but whether deeper networks allow lower norm representation, and how depth changes the inductive bias induced by norm control over the weights. Gunasekar et al. (2018) showed that for linear convolutional networks, the depth meaningfully changes the induced inductive bias, with depth L networks corresponding to an (cid:96)2/L bridge penalty, but in fully connected linear network the depth has no effect. In "}, "Exponential Convergence Time of Gradient Descent for    One-Dimensional Deep Linear Neural Networks": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Exponential Convergence Time of Gradient Descent for    One-Dimensional Deep Linear Neural Networks", "abstract": "We study the dynamics of gradient descent on objective  functions of the form $f(\\prod_{i=1}^{k} w_i)$ (with respect to scalar  parameters $w_1,\\ldots,w_k$), which arise in the context of  training depth-$k$ linear neural networks. We prove that for standard  random initializations, and under mild assumptions on $f$, the number of  iterations required for convergence scales exponentially with the depth  $k$. We also show empirically that this phenomenon can occur in higher  dimensions, where each $w_i$ is a matrix. This highlights a potential  obstacle in understanding the convergence of gradient-based methods for  deep linear neural networks, where $k$ is large.", "pdf_url": "http://proceedings.mlr.press/v99/shamir19a/shamir19a.pdf", "keywords": ["Deep Learning", "linear neural networks", "gradient descent"], "reference": "Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient  descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.  Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit  acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.  Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization effi- ciently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520-529, 2018.  Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous  models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,  2016.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv  preprint arXiv:1810.02032, 2018.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information  Processing Systems, pages 586-594, 2016.  Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are  global. In International Conference on Machine Learning, pages 2908-2913, 2018.  Haihao Lu and Kenji Kawaguchi.  Depth creates no bad local minima.  arXiv preprint  arXiv:1702.08580, 2017.  Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4785-4795, 2017.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-  ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.  12   EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS  This research was partially supported by an ERC grant 754705.  Acknowledgments  References  Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient  descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.  Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit  acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.  Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization effi- ciently learns positive definite linear transformations. In International Conference on Machine Learning, pages 520-529, 2018.  Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous  models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,  2016.  Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv  preprint arXiv:1810.02032, 2018.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information  Processing Systems, pages 586-594, 2016.  Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are  global. In International Conference on Machine Learning, pages 2908-2913, 2018.  Haihao Lu and Kenji Kawaguchi.  Depth creates no bad local minima.  arXiv preprint  arXiv:1702.08580, 2017.  Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4785-4795, 2017.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-  ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS  J Michael Steele. The Cauchy-Schwarz master class: an introduction to the art of mathematical  inequalities. Cambridge University Press, 2004. EXPONENTIAL CONVERGENCE TIME OF GRADIENT DESCENT FOR ONE-DIMENSIONAL LINEAR NETWORKS  "}, "Learning Linear Dynamical Systems with Semi-Parametric Least Squares": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Learning Linear Dynamical Systems with Semi-Parametric Least Squares", "abstract": "We analyze a simple prefiltered variation of the least squares estimator for the problem of estimation with biased, \\emph{semi-parametric} noise, an error model studied more broadly in causal statistics and active learning. We prove an oracle inequality which demonstrates that this procedure provably mitigates the variance introduced by long-term dependencies.  % We then demonstrate that prefiltered least squares yields, to our knowledge, the first algorithm that provably estimates the parameters of partially-observed linear systems that attains rates which do not not incur a worst-case dependence on the rate at which these dependencies decay.  % The algorithm is provably consistent even for systems which satisfy the weaker \\emph{marginal stability} condition obeyed by many classical models based on Newtonian mechanics. In this context, our semi-parametric framework yields guarantees for both stochastic and worst-case noise.", "pdf_url": "http://proceedings.mlr.press/v99/simchowitz19a/simchowitz19a.pdf", "keywords": ["Linear Dynamical Systems", "Statistical Learning Theory", "Variance Reduction", "Time Series", "Semi-Parametric Statistics"], "reference": "Yasin Abbasi-Adkori. Online Least Squares Estimation with Self-Normalized Processes: An Ap-  plication to Bandit Problems. 2011.  Joshua D. Angrist, Guido W. Imbens, and Donald B. Rubin. Identification of causal effects using instrumental variables. Journal of the American Statistical Association, 91(434):444-455, 1996.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, and Whit- ney Newey. Double/debiased/neyman machine learning of treatment effects. American Economic Review, 107(5):261-65, 2017.  Feng Ding. Two-stage least squares based iterative estimation algorithm for CARARMA sys- tem modeling. ISSN 0307904X. doi: 10.1016/j.apm.2012.10.014. URL https://linkinghub.elsevier. com/retrieve/pii/S0307904X12006191.  Applied Mathematical Modelling, 37(7):4798-4808, April 2013.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identi-  fication in unstable linear systems. Automatica, 96:342-353, 2018.  Miguel Galrinho.  Least squares methods for system identification of structured mod- 2016. URL http://www.diva-portal.org/smash/get/diva2:953835/  els. FULLTEXT01.pdf.  Miguel Galrinho, Cristian Rojas, and H\u00b4akan Hjalmarsson. A weighted least-squares method for parameter estimation in structured models. In 53rd IEEE Conference on Decision and Control, December 2014. doi: 10.1109/CDC.2014.7039903.  Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530, 2004.  Lei Guo and Dawei Huang. Least-squares identification for ARMAX models without the positive real condition. IEEE Transactions on Automatic Control, 34(10):1094-1098, October 1989. ISSN 0018-9286. doi: 10.1109/9.35285.  Lars Peter Hansen and Kenneth J. Singleton. Generalized instrumental variables estimation of nonlinear rational expectations models. Econometrica: Journal of the Econometric Society, pages 1269-1286, 1982.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient Descent Learns Linear Dynamical Sys-  tems. arXiv:1609.05191, 2016.  M.L.J. Hautus. Strong detectability and observers. Linear Algebra and its applications, 50:353-  368, 1983.  Elad Hazan, Karan Singh, and Cyril Zhang. Learning Linear Dynamical Systems via Spectral  Filtering. In Neural Information Processing Systems, 2017.  13   LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  References  Yasin Abbasi-Adkori. Online Least Squares Estimation with Self-Normalized Processes: An Ap-  plication to Bandit Problems. 2011.  Joshua D. Angrist, Guido W. Imbens, and Donald B. Rubin. Identification of causal effects using instrumental variables. Journal of the American Statistical Association, 91(434):444-455, 1996.  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Du\ufb02o, Christian Hansen, and Whit- ney Newey. Double/debiased/neyman machine learning of treatment effects. American Economic Review, 107(5):261-65, 2017.  Feng Ding. Two-stage least squares based iterative estimation algorithm for CARARMA sys- tem modeling. ISSN 0307904X. doi: 10.1016/j.apm.2012.10.014. URL https://linkinghub.elsevier. com/retrieve/pii/S0307904X12006191.  Applied Mathematical Modelling, 37(7):4798-4808, April 2013.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identi-  fication in unstable linear systems. Automatica, 96:342-353, 2018.  Miguel Galrinho.  Least squares methods for system identification of structured mod- 2016. URL http://www.diva-portal.org/smash/get/diva2:953835/  els. FULLTEXT01.pdf.  Miguel Galrinho, Cristian Rojas, and H\u00b4akan Hjalmarsson. A weighted least-squares method for parameter estimation in structured models. In 53rd IEEE Conference on Decision and Control, December 2014. doi: 10.1109/CDC.2014.7039903.  Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530, 2004.  Lei Guo and Dawei Huang. Least-squares identification for ARMAX models without the positive real condition. IEEE Transactions on Automatic Control, 34(10):1094-1098, October 1989. ISSN 0018-9286. doi: 10.1109/9.35285.  Lars Peter Hansen and Kenneth J. Singleton. Generalized instrumental variables estimation of nonlinear rational expectations models. Econometrica: Journal of the Econometric Society, pages 1269-1286, 1982.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient Descent Learns Linear Dynamical Sys-  tems. arXiv:1609.05191, 2016.  M.L.J. Hautus. Strong detectability and observers. Linear Algebra and its applications, 50:353-  368, 1983.  Elad Hazan, Karan Singh, and Cyril Zhang. Learning Linear Dynamical Systems via Spectral  Filtering. In Neural Information Processing Systems, 2017. LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral Filtering for General  Linear Dynamical Systems. 2018.  B. L. Ho and R. E. Kalman. Effective construction of linear state-variable models from input/output  functions. Automatisierungs-Technik, 14(1-12):545-548, 1966.  Sham Kakade, Mengdi Wang, and Lin F Yang. Variance reduction methods for sublinear reinforce-  ment learning. arXiv preprint arXiv:1802.09184, 2018.  Felix Krahmer, Shahar Mendelson, and Holger Rauhut. Suprema of chaos processes and the re- stricted isometry property. Communications on Pure and Applied Mathematics, 67(11):1877- 1904, 2014.  Akshay Krishnamurthy, Zhiwei Steven Wu, and Vasilis Syrgkanis. Semiparametric contextual ban-  dits. arXiv preprint arXiv:1803.04204, 2018.  Sun-Yuan Kung. A new identification and model reduction algorithm via singular value decom- position. In 12th Asilomar Conference on Circuits, Systems and Computers, Pacific Grove, CA, November, 1978, 1978.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000.  Lennart Ljung. System Identification: Theory for the User. 1999.  Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. arXiv  preprint arXiv:1809.03019, 2018.  Samet Oymak and Necimye Ozay. Non-asymptotic Identification of LTI Systems from a Single  Trajectory. 2018.  S. Joe Qin. An overview of subspace identification. Computers & Chemical Engineering, 30(10-12): 1502-1513, September 2006. ISSN 00981354. doi: 10.1016/j.compchemeng.2006.05.045. URL https://linkinghub.elsevier.com/retrieve/pii/S009813540600158X.  Tuhin Sarkar and Alexander Rakhlin. How fast can linear dynamical systems be learned? arXiv  preprint arXiv:1812.01251, 2018.  Tuhin Sarkar, Alexander Rakhlin, and Munther A Dahleh. Finite-time system identification for  partially observed lti systems of unknown order. arXiv preprint arXiv:1902.01848, 2019.  Parikshit Shah, Badri Narayan Bhaskar, Gongguo Tang, and Benjamin Recht. Linear System Iden-  tification via Atomic Norm Regularization. In Conference on Decision and Control, 2012.  John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. IEEE transactions on Information Theory, 44(5): 1926-1940, 1998. LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 770-787. Society for Industrial and Ap- plied Mathematics, 2018.  Max Simchowitz, Horia Mania, Stephen Tu, Michael I. Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 439-473. PMLR, 06-09 Jul 2018. URL http://proceedings.mlr.press/v75/simchowitz18a.html.  William Spinelli, Luigi Piroddi, and Marco Lovera. On the role of prefiltering in nonlinear sys- tem identification. IEEE Transactions on Automatic Control, 50(10):1597-1602, October 2005. ISSN 0018-9286. doi: 10.1109/TAC.2005.856655. URL http://ieeexplore.ieee. org/document/1516260/.  Elias M. Stein and Rami Shakarchi. Princeton lecture in analysis ii. complex analysis, 2003.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning. 1998.  Michel Talagrand. Upper and Lower Bounds for Stochastic Processes: Modern Methods and Clas-  sical Problems, volume 60. Springer Science & Business Media, 2014.  Paolo Tilli. Singular values and eigenvalues of non-hermitian block toeplitz matrices. Linear Alge-  bra and its Applications, 272(1-3):59-89, 1998.  George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in Neural Information Processing Systems, pages 2627-2636, 2017.  Michel Verhaegen. Subspace model identification part 3. analysis of the ordinary output-error state- space model identification algorithm. International Journal of Control, 58(3):555-586, 1993.  Roman Vershynin. High-dimensional Probability: An introduction with Applications in Data Sci-  ence, volume 47. Cambridge University Press, 2018.  Mats Viberg, Bo Wahlberg, and Bj\u00a8orn Ottersten. Analysis of state space system identification methods based on instrumental variables and subspace fitting. Automatica, 33(9):1603-1616, 1997.  D. Q. Wang. Least squares-based recursive and iterative estimation for output error moving average IET Control Theory Applications, 5(14):1648-1657, September  systems using data filtering. 2011. ISSN 1751-8644. doi: 10.1049/iet-cta.2010.0416.  Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial iItelligence, pages 538-545. Morgan Kaufmann Publishers Inc., 2001. LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  Yong Zhang. Unbiased identification of a class of multi-input single-output systems with correlated disturbances using bias compensation methods. Mathematical and Computer Modelling, 53(9- 10):1810-1819, May 2011. ISSN 08957177. doi: 10.1016/j.mcm.2010.12.059. URL https: //linkinghub.elsevier.com/retrieve/pii/S0895717711000045.  Wei Xing Zheng. A revisit to least-squares parameter estimation of ARMAX systems. In 2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601), volume 4, pages 3587-3592 Vol.4, December 2004. doi: 10.1109/CDC.2004.1429269.  Kemin Zhou, John C. Doyle, and Keith Glover. Robust and Optimal Control, volume 40. Prentice  Hall, 1996. LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  ContentsIntroduction 1.1 Problem Statement 1.2 Prefiltered Least Squares (PF-LS) 1.3 Organization .  .  .  .  .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 Rates for Learning LTI Systems  2.1 Learning without the stability gap . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Recovering the System Parameters . . . . . . . . . . . . . . . . . . . . . . . . . .  3 Oracle Inequality for Prefiltered Least Squares  3.1 Statement of the Oracle Inequality . . . . . . . . . . . . . . . . . . . . . . . . . .  4 Proof Sketch for Bounding Opt\u00b5  5 Related Work  Preface  Notation  I Proof of Secondary Results  A Examples of Phase Rank  B Proof of Corollary B.1  B.1 Dependence on and Selection of T and L . . . . . . . . . . . . . . . . . . . . . . B.2 Proof of Corollary B.1 (a) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Proof of Corollary B.1 (b) . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Proof of Corollary B.1 (c) and (d)  C Lower Bound for OLS for Marginally Stable Systems  C.1 Proof of Proposition C.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1 Proof of Lemma C.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  C.2 Proof of Lemma C.3 .  .  II General Bound for Prefiltered Least Squares  D General Statement and Analysis PF-LS  D.1 Proof of Uniform Bound: Theorem D.1, Part (a) . . . . . . . . . . . . . . . . . . . D.2 Proof of (cid:103)Opt\u00b5 bound, Theorem D.1, Part (b) . . . . . . . . . . . . . . . . . . . . . D.2.1 Proof of Lemma D.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.2 Proof of Lemma D.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 2 3 4  5 6 7  7 9122122  25 26 27 27 27  28 29 30 3133 34 36 37 38   LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  D.2.3 Proof of Lemma D.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E Semi-Parametric Regression  E.1 Proof of Theorem E.1, Part (a) . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Proof of Theorem E.1, Part (b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Proof of Lemma E.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Proof of Lemma E.2 .  . .  F Chaining for Self-Normalized Tail Inequalities  F.1 Proof of Proposition E.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Lemma F.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  F.2 Proof of Theorem F.2 .  F.1.1  .  III Prefiltered Least Squares for Linear Dynamical Systems  G Bounds on (cid:107)\u2206\u03c6(cid:107)op  . .  G.1 Outline of the Proofs G.2 Bounding G(1) . G.3 Bounding G(2) . . G.4 Additional terms  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.1 Process noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.2 Output noise . G.4.3 Contribution of the initial state . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . .  . . .  H Definition of M , M adv, and Proof of Proposition 2.1 .  .  .  .  .  .  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Proof of Proposition 2.1 . H.3 Proof of Lemma H.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.4 Selecting the parameter L . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.4.1 Proof Sketch of Proposition H.4 . . . . . . . . . . . . . . . . . . . . . . .  I Polynomial Approximations and Phase Rank .  I.1 Main Results  .  .  .  .  .  Results for Adversarial Noise Bounds for Disentangling Filters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.1.1 K1 and K2: Controlling Poles and Markov Operator Norms . . . . . . . . I.1.2 Main Results: Stochastic Noise with Block-Scalar Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.1.3 . . . . . . . . . . . . . . . . . . . . . . I.1.4 Polynomial Approximations for Linear Dynamical Systems . . . . . . . . . . . . I.2.1 Approximations Using Block-Scalar Filters: Proof of Proposition I.1 . . . . . . . . . . . . . . . . . . . I.2.2 Approximations using Disentangling Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem I.4 .  I.2  I.339 40 43 44 45  45 47 48 4951 51 53 54 56 56 56 56  57 57 57 60 61 63  64 65 65 67 67 68 69 71 72 72 LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  J Supporting Proofs  .  .  J.1  Proofs for Section I.2 . J.1.1 J.1.2 J.1.3 J.1.4  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem I.12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Proposition I.13 . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem I.14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Proposition I.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  J.2 Bounds on Finite System Norms: Proof of Proposition I.2  K Bounds under Strong Observability  K.1 Granular Bounds for Strong Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K.2 Proof of Theorem K.2 . K.3 Proof of Proposition K.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K.3.1 Proof of Lemma K.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K.3.2 Proof of Lemma K.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  73 73 73 74 76 78 82  83 85 85 87 88 88 LEARNING LINEAR DYNAMICAL SYSTEMS WITH SEMI-PARAMETRIC LEAST SQUARES  Preface  The "}, "Finite-Time Error Bounds For Linear Stochastic Approximation andTD Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Finite-Time Error Bounds For Linear Stochastic Approximation andTD Learning", "abstract": "We consider the dynamics of a linear stochastic approximation algorithm driven by Markovian noise, and derive finite-time bounds on the moments of the error, i.e., deviation of the output of the algorithm from the equilibrium point of an associated ordinary differential equation (ODE). We obtain finite-time bounds on the mean-square error in the case of constant step-size algorithms by considering the drift of an appropriately chosen Lyapunov function. The Lyapunov function can be interpreted either in terms of Stein\u2019s method to obtain bounds on steady-state performance or in terms of Lyapunov stability theory for linear ODEs. We also provide a comprehensive treatment of the moments of the square of the 2-norm of the approximation error. Our analysis yields the following results: (i) for a given step-size, we show that the lower-order moments can be made small as a function of the step-size and can be upper-bounded by the moments of a Gaussian random variable; (ii) we show that the higher-order moments beyond a threshold may be infinite in steady-state; and (iii) we characterize the number of samples needed for the finite-time bounds to be of the same order as the steady-state bounds. As a by-product of our analysis, we also solve the open problem of obtaining finite-time bounds for the performance of temporal difference learning algorithms with linear function approximation and a constant step-size, without requiring a projection step or an i.i.d. noise assumption.", "pdf_url": "http://proceedings.mlr.press/v99/srikant19a/srikant19a.pdf", "keywords": [], "reference": "C. L. Beck and R Srikant. Error bounds for constant step-size Q-learning. Systems & Control  Letters, 61(12):1203-1208, 2012.  A. Benveniste, M. M\u00b4etivier, and P. Priouret. Adaptive algorithms and stochastic approximations,  volume 22. Springer Science & Business Media, 2012.  D. P. Bertsekas. Dynamic programming and optimal control 3rd edition, volume II. Belmont, MA:  Athena Scientific, 2011.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena, 1996.  J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018. Also appeared in COLT 2018.  S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic recursive algorithms for optimization:  simultaneous perturbation methods, volume 434. Springer, 2012.  V. S. Borkar. Stochastic approximation: a dynamical systems viewpoint. Springer, 2009.  V. S. Borkar and S. P. Meyn. The ODE method for convergence of stochastic approximation and  reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.  P. Br\u00b4emaud. Markov chains: Gibbs fields, Monte Carlo Simulation, and Queues, volume 31.  Springer Science & Business Media, 2013.  C. T. Chen. Linear system theory and design. Oxford University Press, Inc., 1998.  G. Dalal, B. Sz\u00a8or\u00b4enyi, G. Thoppe, and S. Mannor. Finite sample analyses for TD(0) with function  approximation. arXiv preprint arXiv:1704.01161, 2017. Also appeared in AAAI 2018.  A. Eryilmaz and R. Srikant. Asymptotically tight steady-state queue length bounds implied by drift  conditions. Queueing Systems, 72(3-4):311-359, December 2012.  B. Hajek. Stochastic approximation methods for decentralized control of multiaccess communica-  tions. IEEE Transactions on Information Theory, 31(2):176-184, 1985.  H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications,  volume 35. Springer Science & Business Media, 2003.  C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant In International Conference on Artificial Intelligence and  step-size and iterate averaging go? Statistics, pages 1347-1355, 2018.  S. M. Meerkov. Simplified description of slow Markov walks. part i. Automation and Remote  S. M. Meerkov. Simplified description of slow Markov walks. part ii. Automation and Remote  Control, March 1972a.  Control, March 1972b.  13   FINITE-TIME ERROR BOUNDS FOR LINEAR STOCHASTIC APPROXIMATION AND TD LEARNING  References  C. L. Beck and R Srikant. Error bounds for constant step-size Q-learning. Systems & Control  Letters, 61(12):1203-1208, 2012.  A. Benveniste, M. M\u00b4etivier, and P. Priouret. Adaptive algorithms and stochastic approximations,  volume 22. Springer Science & Business Media, 2012.  D. P. Bertsekas. Dynamic programming and optimal control 3rd edition, volume II. Belmont, MA:  Athena Scientific, 2011.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena, 1996.  J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018. Also appeared in COLT 2018.  S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic recursive algorithms for optimization:  simultaneous perturbation methods, volume 434. Springer, 2012.  V. S. Borkar. Stochastic approximation: a dynamical systems viewpoint. Springer, 2009.  V. S. Borkar and S. P. Meyn. The ODE method for convergence of stochastic approximation and  reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.  P. Br\u00b4emaud. Markov chains: Gibbs fields, Monte Carlo Simulation, and Queues, volume 31.  Springer Science & Business Media, 2013.  C. T. Chen. Linear system theory and design. Oxford University Press, Inc., 1998.  G. Dalal, B. Sz\u00a8or\u00b4enyi, G. Thoppe, and S. Mannor. Finite sample analyses for TD(0) with function  approximation. arXiv preprint arXiv:1704.01161, 2017. Also appeared in AAAI 2018.  A. Eryilmaz and R. Srikant. Asymptotically tight steady-state queue length bounds implied by drift  conditions. Queueing Systems, 72(3-4):311-359, December 2012.  B. Hajek. Stochastic approximation methods for decentralized control of multiaccess communica-  tions. IEEE Transactions on Information Theory, 31(2):176-184, 1985.  H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications,  volume 35. Springer Science & Business Media, 2003.  C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant In International Conference on Artificial Intelligence and  step-size and iterate averaging go? Statistics, pages 1347-1355, 2018.  S. M. Meerkov. Simplified description of slow Markov walks. part i. Automation and Remote  S. M. Meerkov. Simplified description of slow Markov walks. part ii. Automation and Remote  Control, March 1972a.  Control, March 1972b. FINITE-TIME ERROR BOUNDS FOR LINEAR STOCHASTIC APPROXIMATION AND TD LEARNING  R. Srikant and L. Ying. Communication networks: an optimization, control, and stochastic networks  perspective. Cambridge University Press, 2013.  R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):  9-44, 1988.  R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.  C. Szepesv\u00b4ari. Algorithms for reinforcement learning. Synthesis lectures on Artificial Intelligence  and Machine Learning, 4(1):1-103, 2010.  J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approxi-  mation. IEEE Transactions on Automatic Control, 42(5), 1997.  L. Ying. On the approximation error of mean-field models. In ACM SIGMETRICS Performance  Evaluation Review, pages 285-297. ACM, 2016.  "}, "Robustness of Spectral Methods for Community Detection": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Robustness of Spectral Methods for Community Detection", "abstract": "The present work is concerned with community detection. Specifically, we consider a random graph drawn according to the stochastic block model: its vertex set is partitioned into blocks, or communities, and edges are placed randomly and independently of each other with probability depending only on the communities of their two endpoints. In this context, our aim is to recover the community labels better than by random guess, based only on the observation of the graph. In the sparse case, where edge probabilities are in $O(1/n)$, we introduce a new spectral method based on the distance matrix $D^{(\\ell)}$, where $D^{(\\ell)}_{ij} = 1$ iff the graph distance between $i$ and $j$, noted $d(i, j)$ is equal to $\\ell$. We show that when $\\ell \\sim c\\log(n)$ for carefully chosen $c$, the eigenvectors associated to the largest eigenvalues of $D^{(\\ell)}$ provide enough information to perform non-trivial community recovery with high probability, provided we are above the so-called Kesten-Stigum threshold. This yields an efficient algorithm for community detection, since computation of the matrix $D^{(\\ell)}$ can be done in $O(n^{1+\\kappa})$ operations for a small constant $\\kappa$.  We then study the sensitivity of the eigendecomposition of $D^{(\\ell)}$ when we allow an adversarial perturbation of the edges of $G$. We show that when the considered perturbation does not affect more than $O(n^\\varepsilon)$ vertices for some small $\\varepsilon > 0$, the highest eigenvalues and their corresponding eigenvectors incur negligible perturbations, which allows us to still perform efficient recovery. Our proposed spectral method therefore: i) is robust to larger perturbations than prior spectral methods,  while semi-definite programming (or SDP) methods can tolerate yet larger perturbations; ii) achieves  non-trivial detection down to the KS threshold, which is conjectured to be optimal and is beyond reach of  existing SDP approaches; iii) is faster than SDP approaches.", "pdf_url": "http://proceedings.mlr.press/v99/stephan19a/stephan19a.pdf", "keywords": [], "reference": "E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. In NIPS\u201916, 2016.  Emmanuel Abbe. Community detection and stochastic block models: recent developments. CoRR,  abs/1703.10146, 2017. URL http://arxiv.org/abs/1703.10146.  Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral robust-  ness. CoRR, abs/1809.04818, 2018. URL http://arxiv.org/abs/1809.04818.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00e9. Non-backtracking spectrum of ran- In IEEE 56th An- dom graphs: Community detection and non-regular ramanujan graphs. nual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17- doi: 10.1109/FOCS.2015.86. URL https: 20 October, 2015, pages 1347-1357, 2015. //doi.org/10.1109/FOCS.2015.86.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106 (1-19), Dec 2011. doi: 10.1103/PhysRevE.84.066106.  Santo Fortunato. Community detection in graphs. Physics Reports, 486(3):75 - 174, 2010. ISSN 0370-1573. doi: https://doi.org/10.1016/j.physrep.2009.11.002. URL http://www. sciencedirect.com/science/article/pii/S0370157309002841.  P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks,  5(2):109-137, 1983.  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov\u00e1, and Pan Zhang. Spectral redemption: clustering sparse networks. CoRR, abs/1306.5550, 2013. URL http://arxiv.org/abs/1306.5550.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning communities In Proceedings of the 29th Conference on Learning Theory, COLT in the presence of errors. 2016, New York, USA, June 23-26, 2016, pages 1258-1291, 2016. URL http://jmlr.org/ proceedings/papers/v49/makarychev16.html.  Laurent Massouli\u00e9. Community detection thresholds and the weak ramanujan property. In Pro- ceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 694-703, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796. 2591857. URL http://doi.acm.org/10.1145/2591796.2591857.  Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for community detection? In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 828-841, 2016. doi: 10.1145/2897518.2897573. URL https://doi.org/10.1145/2897518.2897573.  A. Montanari and S. Sen. Semidefinite programs on sparse random graphs and their application to community detection. In STOC\u201916 Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, 2016.  13   ROBUSTNESS OF SPECTRAL METHODS FOR COMMUNITY DETECTION  References  E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. In NIPS\u201916, 2016.  Emmanuel Abbe. Community detection and stochastic block models: recent developments. CoRR,  abs/1703.10146, 2017. URL http://arxiv.org/abs/1703.10146.  Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral robust-  ness. CoRR, abs/1809.04818, 2018. URL http://arxiv.org/abs/1809.04818.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00e9. Non-backtracking spectrum of ran- In IEEE 56th An- dom graphs: Community detection and non-regular ramanujan graphs. nual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17- doi: 10.1109/FOCS.2015.86. URL https: 20 October, 2015, pages 1347-1357, 2015. //doi.org/10.1109/FOCS.2015.86.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106 (1-19), Dec 2011. doi: 10.1103/PhysRevE.84.066106.  Santo Fortunato. Community detection in graphs. Physics Reports, 486(3):75 - 174, 2010. ISSN 0370-1573. doi: https://doi.org/10.1016/j.physrep.2009.11.002. URL http://www. sciencedirect.com/science/article/pii/S0370157309002841.  P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks,  5(2):109-137, 1983.  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov\u00e1, and Pan Zhang. Spectral redemption: clustering sparse networks. CoRR, abs/1306.5550, 2013. URL http://arxiv.org/abs/1306.5550.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning communities In Proceedings of the 29th Conference on Learning Theory, COLT in the presence of errors. 2016, New York, USA, June 23-26, 2016, pages 1258-1291, 2016. URL http://jmlr.org/ proceedings/papers/v49/makarychev16.html.  Laurent Massouli\u00e9. Community detection thresholds and the weak ramanujan property. In Pro- ceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 694-703, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796. 2591857. URL http://doi.acm.org/10.1145/2591796.2591857.  Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for community detection? In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 828-841, 2016. doi: 10.1145/2897518.2897573. URL https://doi.org/10.1145/2897518.2897573.  A. Montanari and S. Sen. Semidefinite programs on sparse random graphs and their application to community detection. In STOC\u201916 Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, 2016. ROBUSTNESS OF SPECTRAL METHODS FOR COMMUNITY DETECTION  E. Mossel, J. Neeman, and A. Sly.  A proof of the block model  threshold conjecture.  arxiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition  model. Probability Theory and Related Fields, 162(3-4):431-461, 2015. ISSN 0178-8051.  Hermann Weyl. Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Differen- tialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung). Math. Ann., 71 (4):441-479, 1912. ISSN 0025-5831. doi: 10.1007/BF01456804. URL https://doi.org/ 10.1007/BF01456804.  Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis-Kahan theorem for statisti- cians. Biometrika, 102(2):315-323, 2015. ISSN 0006-3444. doi: 10.1093/biomet/asv008. URL https://doi.org/10.1093/biomet/asv008.  P. Zhang. Robust spectral detection of global structures in the data by learning a regularization. In  NIPS\u201916, 2016. ROBUSTNESS OF SPECTRAL METHODS FOR COMMUNITY DETECTION  "}, "Maximum Entropy Distributions: Bit Complexity and Stability": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Maximum Entropy Distributions: Bit Complexity and Stability", "abstract": "Maximum entropy distributions with discrete support in $m$ dimensions  arise in machine learning, statistics, information theory, and theoretical computer science. While structural and computational properties of max-entropy distributions have been extensively studied,  basic questions such as: Do max-entropy distributions over a large support  (e.g., $2^m$) with a specified marginal vector have succinct descriptions (polynomial-size in the input description)? and: Are entropy maximizing distributions \u201cstable\u201d under the perturbation of the marginal vector? have resisted a rigorous resolution. Here we show that these questions are related and resolve both of them. Our main result shows  a ${\\rm poly}(m, \\log 1/\\varepsilon)$ bound on the bit complexity of $\\varepsilon$-optimal dual solutions to the maximum entropy convex program \u2013 for very general support sets and with no restriction on the marginal vector. Applications of this result include polynomial time algorithms to compute  max-entropy distributions over several new and old polytopes for any marginal vector in a unified manner, a  polynomial time algorithm to compute the Brascamp-Lieb constant in the rank-1 case. The proof of this result allows us to show that changing the marginal vector by $\\delta$ changes the max-entropy distribution in the total variation distance roughly by a factor of ${\\rm poly}(m, \\log 1/\\delta)\\sqrt{\\delta}$ \u2013 even when the size of the support set is exponential. Together, our results put max-entropy distributions on a mathematically sound footing \u2013  these distributions are  robust and computationally feasible models for data.", "pdf_url": "http://proceedings.mlr.press/v99/straszak19a/straszak19a.pdf", "keywords": ["Maximum entropy distributions", "Bit complexity", "Stability", "Convex optimization"], "reference": "Zeyuan Allen Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms for matrix scaling. In FOCS\u201917: Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science, 2017.  Noga Alon and Van H. Vu. Anti-Hadamard matrices, coin weighing, threshold gates, and indecom- posable hypergraphs. J. Comb. Theory, Ser. A, 79(1):133-160, 1997. doi: 10.1006/jcta.1997. 2780. URL https://doi.org/10.1006/jcta.1997.2780.  Nima Anari and Shayan Oveis Gharan. A generalization of permanent inequalities and applications in counting and optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 384-396, 2017. ISBN 978-1-4503-4528-6. doi: 10. 1145/3055399.3055469. URL http://doi.acm.org/10.1145/3055399.3055469.  Arash Asadpour, Michel X. Goemans, Aleksander Madry, Shayan Oveis Gharan, and Amin Saberi. An O(log n/ log log n)-approximation algorithm for the asymmetric traveling salesman problem. In Proceedings of the Twenty-first Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201910, pages 379-389, 2010.  Franck Barthe. On a reverse form of the brascamp-lieb inequality. Inventiones mathematicae, 134  (2):335-361, Oct 1998.  Aharon Ben-Tal and Arkadi Nemirovski. Optimization III: Convex analysis, nonlinear program-  ming theory, nonlinear programming algorithms. Lecture Notes, 2012.  Andr\u00b4e Bouchet and William H. Cunningham. Delta-matroids, jump systems, and bisubmodular polyhedra. SIAM J. Discrete Math., 8(1):17-32, 1995. doi: 10.1137/S0895480191222926. URL http://dx.doi.org/10.1137/S0895480191222926.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of machine learning  research, 2(Mar):499-526, 2002.  matics, 216(1):302 - 320, 2007.  Petter Branden. Polynomials with the half-plane property and matroid theory. Advances in Mathe-  Herm Jan Brascamp and Elliott H. Lieb. Best constants in Young\u2019s inequality, its converse, and its generalization to more than three functions, pages 417-439. Springer Berlin Heidelberg, Berlin, Heidelberg, 2002.  L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth K. Vish- In Approxima- noi. On the complexity of constrained determinantal point processes. tion, Randomization, and Combinatorial Optimization. Algorithms and Techniques, AP- PROX/RANDOM 2017, August 16-18, 2017, Berkeley, CA, USA, pages 36:1-36:22, 2017. doi: 10.4230/LIPIcs.APPROX-RANDOM.2017.36. URL https://doi.org/10.4230/ LIPIcs.APPROX-RANDOM.2017.36.  L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth K. Vishnoi. Fair and Diverse DPP-Based Data Summarization. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 715-724. PMLR, 2018.  13   MAXIMUM ENTROPY DISTRIBUTIONS  References  Zeyuan Allen Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms for matrix scaling. In FOCS\u201917: Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science, 2017.  Noga Alon and Van H. Vu. Anti-Hadamard matrices, coin weighing, threshold gates, and indecom- posable hypergraphs. J. Comb. Theory, Ser. A, 79(1):133-160, 1997. doi: 10.1006/jcta.1997. 2780. URL https://doi.org/10.1006/jcta.1997.2780.  Nima Anari and Shayan Oveis Gharan. A generalization of permanent inequalities and applications in counting and optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 384-396, 2017. ISBN 978-1-4503-4528-6. doi: 10. 1145/3055399.3055469. URL http://doi.acm.org/10.1145/3055399.3055469.  Arash Asadpour, Michel X. Goemans, Aleksander Madry, Shayan Oveis Gharan, and Amin Saberi. An O(log n/ log log n)-approximation algorithm for the asymmetric traveling salesman problem. In Proceedings of the Twenty-first Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201910, pages 379-389, 2010.  Franck Barthe. On a reverse form of the brascamp-lieb inequality. Inventiones mathematicae, 134  (2):335-361, Oct 1998.  Aharon Ben-Tal and Arkadi Nemirovski. Optimization III: Convex analysis, nonlinear program-  ming theory, nonlinear programming algorithms. Lecture Notes, 2012.  Andr\u00b4e Bouchet and William H. Cunningham. Delta-matroids, jump systems, and bisubmodular polyhedra. SIAM J. Discrete Math., 8(1):17-32, 1995. doi: 10.1137/S0895480191222926. URL http://dx.doi.org/10.1137/S0895480191222926.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of machine learning  research, 2(Mar):499-526, 2002.  matics, 216(1):302 - 320, 2007.  Petter Branden. Polynomials with the half-plane property and matroid theory. Advances in Mathe-  Herm Jan Brascamp and Elliott H. Lieb. Best constants in Young\u2019s inequality, its converse, and its generalization to more than three functions, pages 417-439. Springer Berlin Heidelberg, Berlin, Heidelberg, 2002.  L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth K. Vish- In Approxima- noi. On the complexity of constrained determinantal point processes. tion, Randomization, and Combinatorial Optimization. Algorithms and Techniques, AP- PROX/RANDOM 2017, August 16-18, 2017, Berkeley, CA, USA, pages 36:1-36:22, 2017. doi: 10.4230/LIPIcs.APPROX-RANDOM.2017.36. URL https://doi.org/10.4230/ LIPIcs.APPROX-RANDOM.2017.36.  L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth K. Vishnoi. Fair and Diverse DPP-Based Data Summarization. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 715-724. PMLR, 2018. MAXIMUM ENTROPY DISTRIBUTIONS  Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and balancing via box constrained Newton\u2019s method and interior point methods. In FOCS\u201917: Pro- ceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science, 2017.  Miroslav Dud\u00b4\u0131k, Steven J Phillips, and Robert E Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. Journal of Machine Learning Research, 8(Jun):1217-1260, 2007.  Ankit Garg, Leonid Gurvits, Rafael Oliveira, and Avi Wigderson. Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via operator scaling. In Proceedings of the 49th Annual ACM Symposium on Theory of Computing, Montreal, QC, Canada, June 19-23, 2017, pages 397-409, 2017. doi: 10.1145/3055399.3055458. URL http://doi.acm.org/10.1145/ 3055399.3055458.  Martin Gr\u00a8otschel, L\u00b4aszlo Lov\u00b4asz, and Alexander Schrijver. Geometric algorithms and combina- torial optimization, volume 2 of Algorithms and Combinatorics. Springer, 1988. ISBN 3-540- 13624-X, 0-387-13624-X (U.S.).  Osman G\u00a8uler. Hyperbolic polynomials and interior point methods for convex programming. Math. Oper. Res., 22(2):350-377, 1997. doi: 10.1287/moor.22.2.350. URL https://doi.org/ 10.1287/moor.22.2.350.  Leonid Gurvits. Hyperbolic polynomials approach to Van der Waerden/Schrijver-Valiant like con- In Proceedings of the  jectures: sharper bounds, simpler proofs and algorithmic applications. thirty-eighth annual ACM symposium on Theory of computing, pages 417-426. ACM, 2006.  Leonid Gurvits and Alex Samorodnitsky. A deterministic polynomial-time algorithm for approxi- mating mixed discriminant and mixed volume, and a combinatorial corollary. Discrete & Com- putational Geometry, 27:531-550, 2002.  Leonid Gurvits and Alex Samorodnitsky. Bounds on the permanent and some applications. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 90-99, 2014. doi: 10.1109/FOCS.2014.18. URL https: //doi.org/10.1109/FOCS.2014.18.  Lingxiao Huang and Nisheeth K. Vishnoi. Stable and Fair Classification. In ICML, 2019.  Edwin T. Jaynes.  Information theory and statistical mechanics. Physical Review, 106:620-630,  May 1957a. doi: 10.1103/PhysRev.106.620.  Edwin T. Jaynes. Information theory and statistical mechanics. II. Physical Review, 108:171-190,  October 1957b. doi: 10.1103/PhysRev.108.171.  Bahman Kalantari, Isabella Lari, Federica Ricca, and Bruno Simeone. On the complexity of gen- eral matrix scaling and entropy minimization via the RAS algorithm. Math. Program., 112(2): 371-401, 2008. doi: 10.1007/s10107-006-0021-4. URL https://doi.org/10.1007/ s10107-006-0021-4.  Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Now Publishers  Inc., Hanover, MA, USA, 2012. ISBN 1601986289, 9781601986283. MAXIMUM ENTROPY DISTRIBUTIONS  Elliott H. Lieb. Gaussian kernels have only gaussian maximizers. Inventiones mathematicae, 102  (1):179-208, Dec 1990.  Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer Publishing  Company, Incorporated, 1 edition, 2014. ISBN 1461346916, 9781461346913.  Kamal Nigam. Using maximum entropy for text classification. In In IJCAI-99 Workshop on Ma-  chine Learning for Information Filtering, pages 61-67, 1999.  Aleksandar Nikolov and Mohit Singh. Maximizing determinants under partition constraints.  In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 192- 201, 2016.  Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. Inducing features of random fields. IEEE Trans. Pattern Anal. Mach. Intell., 19(4):380-393, 1997. doi: 10.1109/34.588021. URL https://doi.org/10.1109/34.588021.  M. C. Shewry and H. P. Wynn. Maximum entropy sampling. Journal of Applied Statistics, 14(2):  165-170, 1987. doi: 10.1080/02664768700000020.  Mohit Singh and Nisheeth K Vishnoi. Entropy, optimization and counting. In Proceedings of the  46th Annual ACM Symposium on Theory of Computing, pages 50-59. ACM, 2014.  Ehsan S Soofi. Principal information theoretic approaches. Journal of the American Statistical  Association, 95(452):1349-1353, 2000.  Damian Straszak and Nisheeth K. Vishnoi. Real stable polynomials and matroids: Optimization and counting. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 370-383, 2017. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.3055457. URL http://doi.acm.org/10.1145/3055399.3055457.  David Wagner. Multivariate stable polynomials: theory and applications. Bulletin of the American  Mathematical Society, 48(1):53-84, 2011.  Bin Yu. Stability. Bernoulli, 19(4):1484-1500, 09 2013. doi: 10.3150/13-BEJSP14. URL https:  //doi.org/10.3150/13-BEJSP14.  "}, "Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression", "abstract": "We study the problem of robust linear regression with response variable corruptions. We consider the oblivious adversary model, where the adversary corrupts a fraction of the responses in complete ignorance of the data. We provide a nearly linear time estimator which consistently estimates the true regression vector, even with $1-o(1)$ fraction of corruptions. Existing results in this setting either don\u2019t guarantee consistent estimates or can only handle a small fraction of corruptions. We also extend our estimator to robust sparse linear regression and show that similar guarantees hold in this setting. Finally, we apply our estimator to the problem of linear regression with heavy-tailed noise and show that our estimator consistently estimates the regression vector even when the noise has unbounded variance (e.g., Cauchy distribution), for which most existing results don\u2019t even apply. Our estimator is based on a novel variant of outlier removal via hard thresholding in which the threshold is chosen adaptively and crucially relies on randomness to escape bad fixed points of the non-convex hard thresholding operation.", "pdf_url": "http://proceedings.mlr.press/v99/suggala19a/suggala19a.pdf", "keywords": ["Robust regression", "heavy tails", "hard thresholding", "outlier removal"], "reference": "Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar. Consistent robust regression. In Advances in Neural Information Processing Systems, pages 2110-2119, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018.  Frank R Hampel. A general qualitative definition of robustness. The Annals of Mathematical  Statistics, pages 1887-1896, 1971.  Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The  Journal of Machine Learning Research, 17(1):543-582, 2016.  Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. Univariate Discrete Distributions, volume  444. John Wiley & Sons, 2005.  Guillaume Lecu\u00e9 and Matthieu Lerasle. Robust machine learning by median-of-means: theory and  practice. arXiv preprint arXiv:1711.10306, 2017.  Nasser M Nasrabadi, Trac D Tran, and Nam Nguyen. Robust lasso with missing and grossly corrupted observations. In Advances in Neural Information Processing Systems, pages 1881-1889, 2011.  Nam H Nguyen and Trac D Tran. Exact recoverability from dense corrupted observations via  (cid:96)1-minimization. IEEE transactions on information theory, 59(4):2017-2035, 2013.  Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust  estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.  Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive Huber regression. Journal of the American  Statistical Association, (just-accepted):1-35, 2018.  Efthymios Tsakonas, Joakim Jald\u00e9n, Nicholas D Sidiropoulos, and Bj\u00f6rn Ottersten. Convergence of the huber regression m-estimate in the presence of dense outliers. IEEE Signal Processing Letters, 21(10):1211-1214, 2014.  Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained quadratic programming (lasso). IEEE transactions on information theory, 55(5): 2183-2202, 2009.  John Wright and Yi Ma. Dense error correction via (cid:96)1-minimization.  IEEE Transactions on  Information Theory, 56(7):3540-3560, 2010.  6   NEAR-OPTIMAL CONSISTENT ROBUST REGRESSION  References  Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar. Consistent robust regression. In Advances in Neural Information Processing Systems, pages 2110-2119, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018.  Frank R Hampel. A general qualitative definition of robustness. The Annals of Mathematical  Statistics, pages 1887-1896, 1971.  Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The  Journal of Machine Learning Research, 17(1):543-582, 2016.  Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. Univariate Discrete Distributions, volume  444. John Wiley & Sons, 2005.  Guillaume Lecu\u00e9 and Matthieu Lerasle. Robust machine learning by median-of-means: theory and  practice. arXiv preprint arXiv:1711.10306, 2017.  Nasser M Nasrabadi, Trac D Tran, and Nam Nguyen. Robust lasso with missing and grossly corrupted observations. In Advances in Neural Information Processing Systems, pages 1881-1889, 2011.  Nam H Nguyen and Trac D Tran. Exact recoverability from dense corrupted observations via  (cid:96)1-minimization. IEEE transactions on information theory, 59(4):2017-2035, 2013.  Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust  estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.  Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive Huber regression. Journal of the American  Statistical Association, (just-accepted):1-35, 2018.  Efthymios Tsakonas, Joakim Jald\u00e9n, Nicholas D Sidiropoulos, and Bj\u00f6rn Ottersten. Convergence of the huber regression m-estimate in the presence of dense outliers. IEEE Signal Processing Letters, 21(10):1211-1214, 2014.  Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained quadratic programming (lasso). IEEE transactions on information theory, 55(5): 2183-2202, 2009.  John Wright and Yi Ma. Dense error correction via (cid:96)1-minimization.  IEEE Transactions on  Information Theory, 56(7):3540-3560, 2010. "}, "Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches", "abstract": "We study the sample complexity of model-based reinforcement learning (henceforth RL) in general contextual decision processes that require strategic exploration to find a near-optimal policy. We design new algorithms for RL with a generic model class and analyze their statistical properties. Our algorithms have sample complexity governed by a new structural parameter called the witness rank, which we show to be small in several settings of interest, including factored MDPs. We also show that the witness rank is never larger than the recently proposed Bellman rank parameter governing the sample complexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only other provably sample-efficient algorithm for global exploration at this level of generality. Focusing on the special case of factored MDPs, we prove an exponential lower bound for a general class of model-free approaches, including OLIVE, which, when combined with our algorithmic results, demonstrates exponential separation between model-based and model-free RL in some rich-observation settings.", "pdf_url": "http://proceedings.mlr.press/v99/sun19a/sun19a.pdf", "keywords": ["Reinforcement Learning", "exploration"], "reference": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear  quadratic systems. In Conference on Learning Theory, 2011.  Eric W Aboaf, Steven M Drucker, and Christopher G Atkeson. Task-level robot learning: Juggling a tennis ball more accurately. In IEEE International Conference on Robotics and Automation, 1989.  Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, and Alexander Russell. A note on the representational incompatibility of function approximation and factored dynamics. In Advances in Neural Information Processing Systems, 2003.  Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforce-  ment learning. arXiv:1703.05449, 2017.  Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear \u03c0 learning using state and action  features. In International Conference on Machine Learning, pages 833-842, 2018.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement  learning. In Advances in Neural Information Processing Systems, pages 2818-2826, 2015.  Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing Systems, 2018.  Marc P Deisenroth, Carl E Rasmussen, and Dieter Fox. Learning to control a low-cost manipulator  using data-efficient reinforcement learning. In Robotics: Science and Systems, 2011.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer Science &  Business Media, 2012.  Carlos Diuk, Lihong Li, and Bethany R Lef\ufb02er. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 249-256. ACM, 2009.  Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for  model-based reinforcement learning. In Artificial Intelligence and Statistics, 2017.  Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu. Robust estimation and generative adversarial nets.  arXiv:1810.02030, 2018.  Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A  kernel two-sample test. Journal of Machine Learning Research, 2012.  13   MODEL-BASED RL IN CDPS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear  quadratic systems. In Conference on Learning Theory, 2011.  Eric W Aboaf, Steven M Drucker, and Christopher G Atkeson. Task-level robot learning: Juggling a tennis ball more accurately. In IEEE International Conference on Robotics and Automation, 1989.  Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, and Alexander Russell. A note on the representational incompatibility of function approximation and factored dynamics. In Advances in Neural Information Processing Systems, 2003.  Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforce-  ment learning. arXiv:1703.05449, 2017.  Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear \u03c0 learning using state and action  features. In International Conference on Machine Learning, pages 833-842, 2018.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement  learning. In Advances in Neural Information Processing Systems, pages 2818-2826, 2015.  Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing Systems, 2018.  Marc P Deisenroth, Carl E Rasmussen, and Dieter Fox. Learning to control a low-cost manipulator  using data-efficient reinforcement learning. In Robotics: Science and Systems, 2011.  Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer Science &  Business Media, 2012.  Carlos Diuk, Lihong Li, and Bethany R Lef\ufb02er. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 249-256. ACM, 2009.  Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for  model-based reinforcement learning. In Artificial Intelligence and Statistics, 2017.  Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu. Robust estimation and generative adversarial nets.  arXiv:1810.02030, 2018.  Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A  kernel two-sample test. Journal of Machine Learning Research, 2012. MODEL-BASED RL IN CDPS  Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algo-  rithms for factored MDPs. Journal of Aprtificial Intelligence Research, 2003.  Zhaohan Daniel Guo and Emma Brunskill. Sample efficient feature selection for factored MDPs.  arXiv preprint arXiv:1703.03454, 2017.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement  learning. Journal of Machine Learning Research, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex- tual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?  In Advances in Neural Information Processing Systems, 2018.  Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces.  In  International Conference on Machine Learning, 2003.  Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored MDPs. In Interna-  tional Joint Conference on Artificial Intelligence, 1999.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.  Machine learning, 2002.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich  observations. In Advances in Neural Information Processing Systems, 2016.  Kailasam Lakshmanan, Ronald Ortner, and Daniil Ryabko. Improved regret bounds for undiscounted continuous reinforcement learning. In International Conference on Machine Learning, 2015.  Tor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general reinforcement  learning. In International Conference on Machine Learning, 2013.  Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite- horizon off-policy estimation. In Advances in Neural Information Processing Systems, pages 5356-5366, 2018.  Alfred M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in  Applied Probability, 1997.  Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity of finite-  horizon Markov decision process problems. Journal of the ACM, 2000.  Trung Nguyen, Zhuoru Li, Tomi Silander, and Tze Yun Leong. Online feature selection for model- based reinforcement learning. In International Conference on Machine Learning, pages 498-506, 2013.  Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement learning. In Advances in Neural Information Processing Systems 31, pages 8888-8896, 2018. MODEL-BASED RL IN CDPS  Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement  learning. In Advances in Neural Information Processing Systems, 2012.  Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.  In Advances in Neural Information Processing Systems, 2014a.  Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored MDPs. In  Advances in Neural Information Processing Systems, pages 604-612, 2014b.  Jason Pazis and Ronald Parr. PAC Optimal Exploration in Continuous Space Markov Decision  Processes. In AAAI, 2013.  Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-free reinforcement learning. In International Conference on Machine Learning, 2006.  Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.  Istv\u00e1n Szita and Csaba Szepesv\u00e1ri. Model-based reinforcement learning with nearly tight exploration  complexity bounds. In International Conference on Machine Learning, 2010.  Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018.  Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 1992.  Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in  deterministic systems. In Advances in Neural Information Processing Systems, 2013.  Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine learning, 8(3-4):229-256, 1992.  Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based reinforcement learning with theoretical guarantees. arXiv:1807.03858, 2018.  "}, "Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions", "abstract": "We provide a novel computer-assisted technique for systematically analyzing first-order methods for optimization. In contrast with previous works, the approach is particularly suited for handling sublinear convergence rates and stochastic oracles. The technique relies on semidefinite programming and potential functions. It allows simultaneously obtaining worst-case guarantees on the behavior of those algorithms, and assisting in choosing appropriate parameters for tuning their worst-case performances. The technique also benefits from comfortable tightness guarantees, meaning that unsatisfactory results can be improved only by changing the setting. We use the approach for analyzing deterministic and stochastic first-order methods under different assumptions on the nature of the stochastic noise. Among others, we treat unstructured noise with bounded variance, different noise models arising in over-parametrized expectation minimization problems, and randomized block-coordinate descent schemes.", "pdf_url": "http://proceedings.mlr.press/v99/taylor19a/taylor19a.pdf", "keywords": [], "reference": "Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205, 2017.  Daniel Azagra and Carlos Mudarra. An extension theorem for convex functions of class C1,1 on Hilbert spaces. Journal of Mathematical Analysis and Applications, 446(2):1167-1182, 2017.  Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms In Advances in Neural Information Processing Systems (NIPS), pages  for machine learning. 451-459, 2011.  vergence rate O(1/n). 773-781, 2013.  arXiv:1712.04581, 2017.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- In Advances in Neural Information Processing Systems (NIPS), pages  Nikhil Bansal and Anupam Gupta. Potential-function proofs for first-order methods. preprint  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.  L\u00b4eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine  learning. SIAM Review, 60(2):223-311, 2018.  Michael B. Cohen, Jelena Diakonikolas, and Lorenzo Orecchia. On acceleration with noise- corrupted gradients. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 1018-1027, 2018.  Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference (ACC), pages 1376-1381, 2018.  Aris Daniilidis, Mounir Haddou, Erwan Le Gruyer, and Olivier Ley. Explicit formulas for C{1,1} Glaeser-Whitney extensions of 1-Taylor fields in Hilbert spaces. Proceedings of the American Mathematical Society, 146(10):4487-4495, 2018.  13   STOCHASTICITY USING POTENTIAL FUNCTIONS  The codes used to generate and validate the results are available at github.com/AdrienTaylor/Potential-functions-for-first-order-methods.  Codes  Acknowledgments  The authors would like to thank Nicolas Flammarion, Franc\u00b8ois Glineur and Yurii Nesterov for in- sightful discussions related on the one hand to stochastic gradient methods and on the other one to potential functions and estimate sequences. The authors also thank the three anonymous refer- ees for their constructive remarks which helped improving the quality of this paper. The authors acknowledge support from the European Research Council (grant SEQUOIA 724063).  References  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200- 1205, 2017.  Daniel Azagra and Carlos Mudarra. An extension theorem for convex functions of class C1,1 on Hilbert spaces. Journal of Mathematical Analysis and Applications, 446(2):1167-1182, 2017.  Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms In Advances in Neural Information Processing Systems (NIPS), pages  for machine learning. 451-459, 2011.  vergence rate O(1/n). 773-781, 2013.  arXiv:1712.04581, 2017.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- In Advances in Neural Information Processing Systems (NIPS), pages  Nikhil Bansal and Anupam Gupta. Potential-function proofs for first-order methods. preprint  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.  L\u00b4eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine  learning. SIAM Review, 60(2):223-311, 2018.  Michael B. Cohen, Jelena Diakonikolas, and Lorenzo Orecchia. On acceleration with noise- corrupted gradients. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 1018-1027, 2018.  Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference (ACC), pages 1376-1381, 2018.  Aris Daniilidis, Mounir Haddou, Erwan Le Gruyer, and Olivier Ley. Explicit formulas for C{1,1} Glaeser-Whitney extensions of 1-Taylor fields in Hilbert spaces. Proceedings of the American Mathematical Society, 146(10):4487-4495, 2018. STOCHASTICITY USING POTENTIAL FUNCTIONS  Etienne de Klerk, Franc\u00b8ois Glineur, and Adrien B. Taylor. On the worst-case complexity of the gra- dient method with exact line search for smooth strongly convex functions. Optimization Letters, 11(7):1185-1199, 2017a.  Etienne de Klerk, Francois Glineur, and Adrien B. Taylor. Worst-case convergence analysis of gra- dient and newton methods through semidefinite programming performance estimation. preprint arXiv:1709.05191, 2017b.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Infor- mation Processing Systems (NIPS), pages 1646-1654, 2014.  Olivier Devolder. Stochastic first order methods in smooth convex optimization. Technical report, Universit\u00b4e catholique de Louvain, Center for Operations Research and Econometrics, 2011.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger con- vergence rates for least-squares regression. Journal of Machine Learning Research, 18(1):3520- 3570, 2017.  Yoel Drori. Contributions to the Complexity Analysis of Optimization Algorithms. PhD thesis,  Tel-Aviv University, 2014.  Complexity, 39:1-16, 2017.  Yoel Drori. The exact information-based complexity of smooth convex minimization. Journal of  Yoel Drori. On the properties of convex functions over open sets. preprint arXiv:1812.02419, 2018.  Yoel Drori and Adrien B. Taylor. Efficient first-order methods for convex minimization: a construc-  tive approach. preprint arXiv:1803.05676, 2018.  Yoel Drori and Marc Teboulle. Performance of first-order methods for smooth convex minimization:  a novel approach. Mathematical Programming, 145(1-2):451-482, 2014.  Yoel Drori and Marc Teboulle. An optimal variant of Kelley\u2019s cutting-plane method. Mathematical  Programming, 160(1-2):321-351, 2016.  Marie Du\ufb02o. Random Iterative Models. Springer-Verlag, Berlin, Heidelberg, 1997.  Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari, and Victor M. Preciado. Analysis of op- timization algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM Journal on Optimization, 28(3):2654-2689, 2018.  Olivier Fercoq and Peter Richt\u00b4arik. Accelerated, parallel, and proximal coordinate descent. SIAM  Journal on Optimization, 25(4):1997-2023, 2015.  Bin Hu and Laurent Lessard. Dissipativity theory for nesterov\u2019s accelerated method. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1549-1557, 2017.  Bin Hu, Peter Seiler, and Laurent Lessard. Analysis of approximate stochastic gradient using quadratic constraints and sequential semidefinite programs. preprint arXiv:1711.00987, 2017a. STOCHASTICITY USING POTENTIAL FUNCTIONS  Bin Hu, Peter Seiler, and Anders Rantzer. A unified analysis of stochastic optimization methods using jump system theory and quadratic constraints. In Conference on Learning Theory (COLT), pages 1157-1189, 2017b.  Chonghai Hu, Weike Pan, and James T. Kwok. Accelerated gradient methods for stochastic op- timization and online learning. In Advances in Neural Information Processing Systems (NIPS), pages 781-789, 2009.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Acceler- ating stochastic gradient descent for least squares regression. In Conference on Learning Theory (COLT), pages 545-604, 2018a.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paralleliz- ing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. Journal of Machine Learning Research, 18(223):1-42, 2018b.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems (NIPS), pages 315-323, 2013.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- gradient methods under the Polyak-\u0142ojasiewicz condition. In Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases, pages 795-811, 2016.  Donghwan Kim and Jeffrey A. Fessler. Optimized first-order methods for smooth convex minimiza-  tion. Mathematical Programming, 159(1-2):81-107, 2016.  Donghwan Kim and Jeffrey A Fessler. Another look at the fast iterative shrinkage/thresholding  algorithm (fista). SIAM Journal on Optimization, 28(1):223-250, 2018a.  Donghwan Kim and Jeffrey A Fessler. Generalizing the optimized gradient method for smooth  convex minimization. SIAM Journal on Optimization, 28(2):1920-1950, 2018b.  Donghwan Kim and Jeffrey A. Fessler. Optimizing the efficiency of first-order methods for decreas-  ing the gradient of smooth convex functions. preprint arXiv:1803.06600, 2018c.  Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Program-  ming, 133(1-2):365-397, 2012.  Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo- nential convergence rate for finite training sets. In Advances in Neural Information Processing Systems (NIPS), pages 2663-2671, 2012.  Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algo- rithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.  Johan L\u00a8ofberg. YALMIP : A toolbox for modeling and optimization in MATLAB. In Proceedings  of the CACSD Conference, 2004.  Aleksandr M. Lyapunov and Anthony T. Fuller. General Problem of the Stability Of Motion. Control  Theory and Applications Series. Taylor & Francis, 1992. Original text in Russian, 1892. STOCHASTICITY USING POTENTIAL FUNCTIONS  Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec- tiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 3325-3334, 2018.  APS Mosek. The MOSEK optimization software. Online at http://www.mosek.com, 54, 2010.  Ion Necoara, Yurii Nesterov, and Francois Glineur. Linear convergence of first order methods for  non-strongly convex optimization. Mathematical Programming, pages 1-39, 2018.  Arkadi S. Nemirovski. Orth-method for smooth convex optimization. Izvestia AN SSSR, Tekhnich-  eskaya Kibernetika, 2:937-947, 1982.  Arkadi S. Nemirovski and David B. Yudin. Problem complexity and method efficiency in optimiza-  tion. Willey-Interscience, New York, 1983.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate  O(1/k2). Soviet Mathematics Doklady, 27:372-376, 1983.  Yurii Nesterov. Introductory Lectures on Convex Optimization : a Basic Course. Applied optimiza-  tion. Kluwer Academic Publishing, 2004.  Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.  SIAM Journal on Optimization, 22(2):341-362, 2012a.  Yurii Nesterov. How to make the gradients small. Optima, 88:10-11, 2012b.  Yurii Nesterov. Lectures on Convex Optimization. Springer Optimization and Its Applications.  Springer International Publishing, 2018.  Lam M. Nguyen, Phuong Ha Nguyen, Peter Richt\u00b4arik, Katya Scheinberg, Martin Tak\u00b4a\u02c7c, and Marten van Dijk. New convergence aspects of stochastic gradient algorithms. preprint arXiv:1811.12403, 2018.  Boris T. Polyak. Introduction to Optimization. Optimization Software New York, 1987.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  Peter Richt\u00b4arik and Martin Tak\u00b4a\u02c7c.  Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):1-38, 2014.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, 22(3):400-407, 1951.  Herbert Robbins and David Siegmund. A convergence theorem for non negative almost super- martingales and some applications. In Optimizing methods in statistics, pages 233-257. Elsevier, 1971.  David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical  report, Cornell University Operations Research and Industrial Engineering, 1988. STOCHASTICITY USING POTENTIAL FUNCTIONS  Ernest K. Ryu, Adrien B. Taylor, Carolina Bergeling, and Pontus Giselsson. Operator splitting performance estimation: Tight contraction factors and optimal parameter selection. preprint arXiv:1812.00146, 2018.  Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong  growth condition. preprint arXiv:1308.6370, 2013.  Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic  average gradient. Mathematical Programming, 162(1-2):83-112, 2017.  Ziqiang Shi and Rujie Liu. Better worst-case complexity analysis of the block coordinate descent method for large scale machine learning. In 16th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 889-892, 2017.  Jos F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones.  Optimization Methods and Software, 11-12:625-653, 1999.  Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. Primal averaging: A new gradient evaluation step to attain the optimal individual convergence. IEEE Transactions on Cybernetics, 2018.  Adrien Taylor, Bryan Van Scoy, and Laurent Lessard. Lyapunov functions for first-order methods: Tight automated convergence guarantees. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 4897-4906, 2018a.  Adrien B. Taylor, Julien M. Hendrickx, and Franc\u00b8ois Glineur. Exact worst-case performance of first-order methods for composite convex optimization. SIAM Journal on Optimization, 27(3): 1283-1313, 2017a.  Adrien B. Taylor, Julien M. Hendrickx, and Franc\u00b8ois Glineur. Performance Estimation Toolbox (PESTO): automated worst-case analysis of first-order optimization methods. In IEEE 56th An- nual Conference on Decision and Control (CDC), pages 1278-1283, 2017b.  Adrien B. Taylor, Julien M. Hendrickx, and Franc\u00b8ois Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. Mathematical Programming, 161(1-2): 307-345, 2017c.  Adrien B. Taylor, Julien M Hendrickx, and Franc\u00b8ois Glineur. Exact worst-case convergence rates of the proximal gradient method for composite convex minimization. Journal of Optimization Theory and Applications, 178(2):455-476, 2018b.  Onur Toker and Hitay Ozbay. On the np-hardness of solving bilinear matrix inequalities and simul- taneous stabilization with static output feedback. In 1995 Annual American Control Conference (ACC), volume 4, pages 2525-2526, 1995.  Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted  to SIAM Journal on Optimization, 2008.  Bryan Van Scoy, Randy A. Freeman, and Kevin M. Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2 (1):49-54, 2018. STOCHASTICITY USING POTENTIAL FUNCTIONS  Lieven Vandenberghe and Stephen Boyd. Semidefinite programming. SIAM review, 38(1):49-95,  1996.  Lieven Vandenberghe, V. Ragu Balakrishnan, Ragnar Wallin, Anders Hansson, and Tae Roh. Interior-point algorithms for semidefinite programming problems derived from the kyp lemma. Positive Polynomials in Control, pages 579-579, 2005.  Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over- parameterized models and an accelerated perceptron. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1195-1204, 2019.  Ashia C. Wilson, Benjamin Recht, and Michael I. Jordan. A Lyapunov analysis of momentum  methods in optimization. preprint arXiv:1611.02635, 2016.  Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.  Journal of Machine Learning Research, 11:2543-2596, 2010.  Kaiwen Zhou, Qinghua Ding, Fanhua Shang, James Cheng, Danli Li, and Zhi-Quan Luo. Direct acceleration of saga using sampled negative momentum. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1602-1610, 2019. STOCHASTICITY USING POTENTIAL FUNCTIONS  "}, "The Relative Complexity of Maximum Likelihood Estimation, MAP Estimation, and Sampling": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The Relative Complexity of Maximum Likelihood Estimation, MAP Estimation, and Sampling", "abstract": "We prove that, for a broad range of problems, maximum-a-posteriori (MAP) estimation and approximate sampling of the posterior are at least as computationally difficult as maximum-likelihood (ML) estimation. By way of illustration, we show how hardness results for ML estimation of mixtures of Gaussians and topic models carry over to MAP estimation and approximate sampling under commonly used priors.", "pdf_url": "http://proceedings.mlr.press/v99/tosh19a/tosh19a.pdf", "keywords": ["Hardness", "reductions", "sampling"], "reference": "D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clus-  tering. Machine Learning, 75(2):245-248, 2009.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond SVD. In Proceedings of the  53rd IEEE Annual Symposium on Foundations of Computer Science, pages 1-10, 2012.  A. Baker. Transcendental Number Theory. Cambridge University Press, 1975.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A  non-asymptotic analysis. In Conference on Learning Theory, pages 300-323, 2018.  B. Chor and T. Tuller. Finding a maximum likelihood tree is hard. Journal of the ACM, 53(5):  722-744, 2006.  V. Guruswami and A. Vardy. Maximum-likelihood decoding of Reed-Solomon codes is NP-hard. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 470-478, 2005.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Opera-  tions Research, 31(2):253-266, 2006.  L. M. Le Cam. On some asymptotic properties of maximum likelihood estimates and related Bayes  estimates. University of California Publications in Statistics, 1:277-330, 1953.  K. W. Ng, G.-L. Tian, and M.-L. Tang. Dirichlet and Related Distributions: Theory, Methods and  Applications. Wiley, 2011.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin In Conference on Learning Theory, pages 1674-1703,  dynamics: a nonasymptotic analysis. 2017.  D. Sontag and D. Roy. Complexity of inference in latent Dirichlet allocation. In Advances in Neural  Information Processing Systems, pages 1008-1016, 2011.  F. Tops\u00f8e. Some bounds for the logarithmic function. Inequal. Theory Appl., pages 137-151, 2004.  C. Tosh and S. Dasgupta. Maximum likelihood estimation for mixtures of spherical Gaussians is  NP-hard. JMLR, 18(1):1-11, 2018.  13   RELATING ML ESTIMATION, MAP ESTIMATION, SAMPLING  References  D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clus-  tering. Machine Learning, 75(2):245-248, 2009.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond SVD. In Proceedings of the  53rd IEEE Annual Symposium on Foundations of Computer Science, pages 1-10, 2012.  A. Baker. Transcendental Number Theory. Cambridge University Press, 1975.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.  X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan. Underdamped Langevin MCMC: A  non-asymptotic analysis. In Conference on Learning Theory, pages 300-323, 2018.  B. Chor and T. Tuller. Finding a maximum likelihood tree is hard. Journal of the ACM, 53(5):  722-744, 2006.  V. Guruswami and A. Vardy. Maximum-likelihood decoding of Reed-Solomon codes is NP-hard. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 470-478, 2005.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Opera-  tions Research, 31(2):253-266, 2006.  L. M. Le Cam. On some asymptotic properties of maximum likelihood estimates and related Bayes  estimates. University of California Publications in Statistics, 1:277-330, 1953.  K. W. Ng, G.-L. Tian, and M.-L. Tang. Dirichlet and Related Distributions: Theory, Methods and  Applications. Wiley, 2011.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin In Conference on Learning Theory, pages 1674-1703,  dynamics: a nonasymptotic analysis. 2017.  D. Sontag and D. Roy. Complexity of inference in latent Dirichlet allocation. In Advances in Neural  Information Processing Systems, pages 1008-1016, 2011.  F. Tops\u00f8e. Some bounds for the logarithmic function. Inequal. Theory Appl., pages 137-151, 2004.  C. Tosh and S. Dasgupta. Maximum likelihood estimation for mixtures of spherical Gaussians is  NP-hard. JMLR, 18(1):1-11, 2018. RELATING ML ESTIMATION, MAP ESTIMATION, SAMPLING  "}, "The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint", "abstract": "The effectiveness of model-based versus model-free methods is a long-standing question in reinforcement learning (RL).  Motivated by recent empirical success of RL on continuous control tasks, we study the sample complexity of popular model-based and model-free algorithms on the Linear Quadratic Regulator (LQR).  We show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension.  For policy optimization, we study a simple family of problem instances and show that nominal (certainty equivalence principle) control also requires several factors of state and input dimension fewer samples than the policy gradient method to reach the same level of control performance on these instances.  Furthermore, the gap persists even when employing baselines commonly used in practice.  To the best of our knowledge, this is the first theoretical result which demonstrates a separation in the sample complexity between model-based and model-free methods on a continuous control task.", "pdf_url": "http://proceedings.mlr.press/v99/tu19a/tu19a.pdf", "keywords": ["Linear dynamical systems", "linear quadratic regulator", "least-squares temporal difference learning", "policy gradients"], "reference": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret Bounds for the Adaptive Control of Linear  Quadratic Systems. In Conference on Learning Theory, 2011.  Yasin Abbasi-Yadkori, Nevena Lazi\u00b4c, and Csaba Szepesv\u00e1ri. Model-Free Linear Quadratic Control  via Reduction to Expert Prediction. In AISTATS, 2019.  Marc Abeille and Alessandro Lazaric. Thompson Sampling for Linear-Quadratic Control Problems.  In AISTATS, 2017.  Marc Abeille and Alessandro Lazaric. Improved Regret Bounds for Thompson Sampling in Linear  Quadratic Control Problems. In International Conference on Machine Learning, 2018.  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-  case regret bounds. In Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3): 325-349, 2013.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax Regret Bounds for Rein-  forcement Learning. In International Conference on Machine Learning, 2017.  Patrick Billingsley. Probability and Measure. 1995.  Justin Boyan. Least-Squares Temporal Difference Learning. In International Conference on Ma-  chine Learning, 1999.  Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares Algorithms for Temporal Difference  Learning. Machine Learning, 22(1-3):33-57, 1996.  Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization. In Conference on Robot Learning, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017.  13   THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Acknowledgments  We thank John Duchi and Daniel Russo, who both independently suggested studying LQR using asymptotic analysis; this paper is a direct result of their feedback. We also thank Horia Mania for many helpful discussions regarding policy gradient methods. Finally, we thank Nicolas Flammarion and Nilesh Tripuraneni for pointers regarding asymptotic analysis of stochastic gradient methods. ST is supported by a Google PhD fellowship. BR is generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, and an Amazon AWS AI Re- search Award.  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret Bounds for the Adaptive Control of Linear  Quadratic Systems. In Conference on Learning Theory, 2011.  Yasin Abbasi-Yadkori, Nevena Lazi\u00b4c, and Csaba Szepesv\u00e1ri. Model-Free Linear Quadratic Control  via Reduction to Expert Prediction. In AISTATS, 2019.  Marc Abeille and Alessandro Lazaric. Thompson Sampling for Linear-Quadratic Control Problems.  In AISTATS, 2017.  Marc Abeille and Alessandro Lazaric. Improved Regret Bounds for Thompson Sampling in Linear  Quadratic Control Problems. In International Conference on Machine Learning, 2018.  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-  case regret bounds. In Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3): 325-349, 2013.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax Regret Bounds for Rein-  forcement Learning. In International Conference on Machine Learning, 2017.  Patrick Billingsley. Probability and Measure. 1995.  Justin Boyan. Least-Squares Temporal Difference Learning. In International Conference on Ma-  chine Learning, 1999.  Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares Algorithms for Temporal Difference  Learning. Machine Learning, 22(1-3):33-57, 1996.  Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization. In Conference on Robot Learning, 2018.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator. In Neural Information Processing Systems, 2018.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Optimism-Based  Adaptive Regulation of Linear-Quadratic Systems. arXiv:1711.07230, 2017.  Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator. In International Conference on Machine Learning, 2018.  Claude-Nicolas Fiechter. PAC Adaptive Control of Linear Systems. In Conference on Learning  Theory, 1997.  Morteza Ibrahimi, Adel Javanmard, and Benjamin Van Roy. Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems. In Neural Information Processing Systems, 2012.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement  Learning. Journal of Machine Learning Research, 11:1563-1600, 2010.  Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query Complexity of Derivative-Free  Optimization. In Neural Information Processing Systems, 2012.  Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan.  Is Q-learning Provably  Efficient? In Neural Information Processing Systems, 2018.  Galin L. Jones. On the Markov chain central limit theorem. Probability Surveys, 1:299-320, 2004.  Harold Kushner and George Yin. Stochastic Approximation and Recursive Algorithms and Appli-  cations. 2003.  Jan R. Magnus. The expectation of products of quadratic forms in normal variables: the practice.  Statistica Neerlandica, 33(3):131-136, 1979.  Dhruv Malik, Kush Bhatia, Koulik Khamaru, Peter L. Bartlett,  , and Martin J. Wainwright. Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. In AISTATS, 2019.  Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive  approach to reinforcement learning. In Neural Information Processing Systems, 2018.  Henry B. Mann and Abraham Wald. On the Statistical Treatment of Linear Stochastic Difference  Equations. Econometrica, 11(3-4):173-220, 1943.  Abdelkader Mokkadem. Mixing properties of ARMA processes. Stochastic Processes and their  Applications, 29(2):309-315, 1988.  B. Molinari. The Stabilizing Solution of the Discrete Algebraic Riccati Equation. IEEE Transac-  tions on Automatic Control, 20(3):396-399, 1975. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural Network Dy- namics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. In Inter- national Conference on Robotics and Automation, 2018.  Yurii Nesterov and Vladimir Spokoiny. Random Gradient-Free Minimization of Convex Functions.  Foundations of Computational Mathematics, 17(2):527-566, 2017.  Yi Ouyang, Mukul Gagrani, and Rahul Jain. Control of unknown linear systems with Thompson In 55th Annual Allerton Conference on Communication, Control, and Computing,  Alain Pajor. Metric Entropy of the Grassmann Manifold. Convex Geometric Analysis, 34:181-188,  sampling. 2017.  1998.  Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural  Networks, 21(4):682-697, 2008.  Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal Difference Models: Model-Free Deep RL for Model-Based Control. In International Conference on Learning Rep- resentations, 2018.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization. In International Conference on Machine Learning, 2012.  Benjamin Recht. A Tour of Reinforcement Learning: The View from Continuous Control.  arXiv:1806.09460, 2018.  Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as a  Scalable Alternative to Reinforcement Learning. arXiv:1703.03864, 2017.  Kathrin Sch\u00e4cke. On the Kronecker Product. Master\u2019s thesis, University of Waterloo, 2004.  John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High- In International  Dimensional Continuous Control Using Generalized Advantage Estimation. Conference on Learning Representations, 2016.  Aaron Sidford, Mengdi Wang, Xian Wu, Lin F. Yang, and Yinyu Ye. Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model. In Neural Information Processes Systems, 2018.  Max Simchowitz, Horia Mania, Stephen Tu, Michael I. Jordan, and Benjamin Recht. Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification. In Conference on Learning Theory, 2018.  Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC Model-Free Reinforcement Learning. In International Conference on Machine Learning, 2006.  Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-Based Reinforcement Learning in Contextual Decision Processes. In Conference on Learning Theory, 2019. THE GAP BETWEEN MODEL-BASED AND MODEL-FREE METHODS ON LQR  Panos Toulis and Edoardo M. Airoldi. Asymptotic and finite-sample properties of estimators based  on stochastic gradients. The Annals of Statistics, 45(4):1694-1727, 2017.  Stephen Tu and Benjamin Recht. Least-Squares Temporal Difference Learning for the Linear  Quadratic Regulator. In International Conference on Machine Learning, 2018.  Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-  ment learning. Machine Learning, 8(3-4):229-246, 1992.  Fuzhen Zhang. The Schur Complement and its Applications, volume 4 of Numerical Methods and  Algorithms. 2005.  "}, "Theoretical guarantees for sampling and inference in generative models with latent diffusions": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Theoretical guarantees for sampling and inference in generative models with latent diffusions", "abstract": "We introduce and study a class of probabilistic generative models, where the latent object is a finite-dimensional diffusion process on a finite time interval and the observed variable is drawn conditionally on the terminal point of the diffusion. We make the following contributions: We provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. We quantify the expressiveness of diffusion-based generative models. Specifically, we show that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback\u2013Leibler divergence to the target distribution. Finally, we present and analyze a scheme for unbiased simulation of generative models with latent diffusions and provide bounds on the variance of the resulting estimators. This scheme can be implemented as a deep generative model with a random number of layers.", "pdf_url": "http://proceedings.mlr.press/v99/tzen19a/tzen19a.pdf", "keywords": [], "reference": "Rados\u0142aw Adamczak. A tail inequality for suprema of unbounded empirical processes with appli-  cations to Markov chains. Electronic Journal of Probability, 13:1000-1034, 2008.  Patrik Andersson and Arturo Kohatsu-Higa. Unbiased simulation of stochastic differential equations  using parametrix expansions. Bernoulli, 23(3):2028-2057, 2017.  Vlad Bally and Arturo Kohatsu-Higa. A probabilistic interpretation of the parametrix method.  Annals of Applied Probability, 25(6):3095-3138, 2015.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. Oxford University Press, 2013.  Michelle Bou\u00b4e and Paul Dupuis. A variational representation for certain functionals of Brownian  motion. Annals of Probability, 26(4):1641-1659, 1998.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected Langevin Monte Carlo. Discrete and Computational Geometry, 57:757-783, 2018.  Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary differ-  ential equations. In Neural Information Processing Systems, 2018.  Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied Mathematics  and Optimization, 23:313-329, 1991.  Ronen Eldan and James R. Lee. Regularization under diffusion and anticoncentration of the infor-  mation content. Duke Mathematical Journal, 167(5):969-993, 2018.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Proceed-  ings of the 29th Annual Conference on Learning Theory, pages 907-940, 2016.  Arthur Erd\u00b4elyi, Wilhelm Magnus, Fritz Oberhettinger, and Francesco G. Tricomi. Higher Transcen-  dental Functions, volume III. McGraw-Hill, New York, 1955.  28   SAMPLING AND INFERENCE IN GENERATIVE MODELS WITH LATENT DIFFUSIONS  Using Markov\u2019s inequality and the fact that the \u03c4i\u2019s are i.i.d., we can further estimate  P[Sk < k\u03b2\u22121] = P[k \u2212 \u03b2Sk > 0] \u2264 ekE  (cid:104) e\u2212\u03b2Sk  (cid:105)  =  (cid:16)  eM\u03c4 (\u2212\u03b2)  (cid:17)k  .  Substituting these estimates into (F.1) and optimizing over \u03b2, we get (4.6).  Acknowledgments  The authors would like to thank Matus Telgarsky for many enlightening discussions. This work was supported in part by the NSF CAREER award CCF-1254041, in part by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370, in part by the Center for Advanced Electronics through Machine Learning (CAEML) I/UCRC award no. CNS-16-24811, and in part by the Office of Naval Research under grant no. N00014-12-1-0998.  References  Rados\u0142aw Adamczak. A tail inequality for suprema of unbounded empirical processes with appli-  cations to Markov chains. Electronic Journal of Probability, 13:1000-1034, 2008.  Patrik Andersson and Arturo Kohatsu-Higa. Unbiased simulation of stochastic differential equations  using parametrix expansions. Bernoulli, 23(3):2028-2057, 2017.  Vlad Bally and Arturo Kohatsu-Higa. A probabilistic interpretation of the parametrix method.  Annals of Applied Probability, 25(6):3095-3138, 2015.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. Oxford University Press, 2013.  Michelle Bou\u00b4e and Paul Dupuis. A variational representation for certain functionals of Brownian  motion. Annals of Probability, 26(4):1641-1659, 1998.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected Langevin Monte Carlo. Discrete and Computational Geometry, 57:757-783, 2018.  Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary differ-  ential equations. In Neural Information Processing Systems, 2018.  Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied Mathematics  and Optimization, 23:313-329, 1991.  Ronen Eldan and James R. Lee. Regularization under diffusion and anticoncentration of the infor-  mation content. Duke Mathematical Journal, 167(5):969-993, 2018.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Proceed-  ings of the 29th Annual Conference on Learning Theory, pages 907-940, 2016.  Arthur Erd\u00b4elyi, Wilhelm Magnus, Fritz Oberhettinger, and Francesco G. Tricomi. Higher Transcen-  dental Functions, volume III. McGraw-Hill, New York, 1955. SAMPLING AND INFERENCE IN GENERATIVE MODELS WITH LATENT DIFFUSIONS  Wendell H. Fleming. Exit probabilities and optimal stochastic control. Applied Mathematics and  Optimization, 4:329-346, 1978.  Springer, 1975.  Wendell H. Fleming and Raymond W. Rishel. Deterministic and Stochastic Optimal Control.  Wendell H. Fleming and Sheunn-Jyi Sheu. Stochastic variational formula for fundamental solutions  of parabolic PDE. Applied Mathematics and Optimization, 13:193-204, 1985.  Hans F\u00a8ollmer. An entropy approach to time reversal of diffusion processes. In Stochastic Differen- tial Systems (Marseille-Luminy, 1984), volume 69 of Lecture Notes in Control and Information Sciences. Springer, 1985.  Evarist Gin\u00b4e and Richard Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Mod-  els. Cambridge University Press, 2016.  Peter W. Glynn and Ward Whitt. Large deviations behavior of counting processes and their inverses.  Queueing Systems, 17:107-128, 1994.  Carl Graham and Denis Talay. Stochastic Simulation and Monte Carlo Methods: Mathematical Foundations of Stochastic Simulation, volume 68 of Stochastic Modeling and Applied Probabil- ity. Springer, 2013.  Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of  Algorithmic Differentiation. SIAM, 2nd edition, 2008.  Tatsunori Hashimoto, David Gifford, and Tommi Jaakkola. Learning population-level diffusions with generative RNNs. In Proceedings of the 33rd International Conference on Machine Learn- ing, pages 2417-2426, 2016.  Pierre Henry-Labord`ere, Xiaolu Tian, and Nizar Touzi. Unbiased simulation of stochastic differen-  tial equations. Annals of Applied Probability, 27(6):3305-3341, 2017.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551- 560, 1990.  Benton Jamison. The Markov processes of Schr\u00a8odinger. Zeitschrift f\u00a8ur Wahrscheinlichkeitstheorie  und verwandte Gebiete, 32(4):323-331, 1975.  Olav Kallenberg. Foundations of Modern Probability. Springer, 2nd edition, 2002.  Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery  Problems, volume 2033 of Lecture Notes in Mathematics. Springer, 2011.  Joseph Lehec. Representation formula for the entropy and functional inequalities. Annales de  l\u2019Institut Henri Poincar\u00b4e - Probabilit\u00b4es et Statistiques, 49(3):885-899, 2013.  Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic algorithms. In Proceedings of the 34th International Conference on Machine Learning, pages 2101-2110, 2017. SAMPLING AND INFERENCE IN GENERATIVE MODELS WITH LATENT DIFFUSIONS  Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algorithms for deep  learning. Journal of Machine Learning Research, 18:1-29, 2018.  Xin Li. Simultaneous approximation of multivariate functions and their derivatives by neural net-  works with one hidden layer. Neurocomputing, 12:327-343, 1996.  Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic gradient descent as approxi-  mate Bayesian inference. Journal of Machine Learning Research, 18:1-35, 2017.  Javier R. Movellan, Paul Mineiro, and Ruth J. Williams. A Monte Carlo EM approach for partially observable diffusion processes: theory and applications to neural networks. Neural Computation, 14:1507-1544, 2002.  Michele Pavon. Stochastic control and nonequilibrium thermodynamical systems. Applied Mathe-  matics and Optimization, 19:187-202, 1989.  Philip E. Protter. Stochastic Integration and Differential Equations. Springer, 2nd edition, 2005.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis. In Proceedings of the 2017 Conference on Learning Theory, 2017.  Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 2014 International Con- ference on Machine Learning, pages 1278-1286, 2014.  Tom Ryder, Andrew Golightly, A. Steven McGough, and Dennis Prangle. Black-box variational in- ference for stochastic differential equations. In Proceedings of the 35th International Conference on Machine Learning, pages 4423-4432, 2018.  Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural In Proceedings of the 34th International Conference on Machine Learning, pages  networks. 2979-2987, 2017.  Erwin Schr\u00a8odinger.  \u00a8Uber die Umkehrung der Naturgesetze. Sitzung ber Preuss. Akad. Wissen.,  Berlin Phys. Math., 144, 1931.  Sheunn-Jyi Sheu. Some estimates of the transition density of a nondegenerate diffusion Markov  process. Annals of Probability, 19(2):538-561, 1991.  Daniel W. Stroock. An Introduction to Partial Differential Equations for Probabilists. Cambridge  University Press, 2008.  Matus Telgarsky. Neural networks and rational functions. In Proceedings of the 34th International  Conference on Machine Learning, pages 3387-3393, 2017.  Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences (U.S.), 113(47): E7351-E7358, 2016. SAMPLING AND INFERENCE IN GENERATIVE MODELS WITH LATENT DIFFUSIONS  Lin Yang, Raman Arora, Vladimir Braverman, and Tuo Zhao. The physical systems behind opti-  mization algorithms. In Neural Information Processing Systems, 2018.  Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks,  94:103-114, 2017.  Joseph E. Yukich, Maxwell B. Stinchcombe, and Halbert White. Sup-norm approximation bounds for neural networks through probabilistic methods. IEEE Transactions on Information Theory, 41(4):1021-1027, July 1995.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. In Proceedings of the 2017 Conference on Learning Theory, pages 1980- 2022, 02 2017. "}, "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds", "abstract": "We study the complexity of training neural network models with one hidden nonlinear activation layer and an output weighted sum layer. We analyze Gradient Descent applied to learning a bounded target function on $n$ real-valued inputs.  %by training a neural network with a single hidden layer of nonlinear gates. We give an agnostic learning guarantee for GD: starting from a randomly initialized network, it converges in mean squared loss to the minimum error (in $2$-norm) of the best approximation of the target function using a polynomial of degree at most $k$. Moreover, for any $k$, the size of the network and number of iterations needed are both bounded by $n^{O(k)}\\log(1/\\epsilon)$. The core of our analysis is the following existence theorem, which is of independent interest:  for any $\\epsilon > 0$, any bounded function that has a degree $k$ polynomial approximation with error $\\epsilon_0$ (in $2$-norm), can be approximated to within error $\\epsilon_0 + \\epsilon$ as a linear combination of $n^{O(k)}\\cdot \\mbox{poly}(1/\\epsilon)$ {\\em randomly chosen} gates from any class of gates whose corresponding activation function has nonzero coefficients in its harmonic expansion for degrees up to $k$. In particular, this applies to training networks  of unbiased sigmoids and ReLUs.  We also rigorously explain the empirical finding that gradient descent discovers lower frequency Fourier components before higher frequency components.  We complement this result with nearly matching lower bounds in the Statistical Query model. GD fits well in the SQ framework since each training step is determined by an expectation over the input distribution. We show that any SQ algorithm that achieves significant improvement over a constant function with queries of tolerance some inverse polynomial in the input dimensionality $n$ must use $n^{\\Omega(k)}$ queries even when the target functions are restricted to a set of $n^{O(k)}$ degree-$k$ polynomials, and the input distribution is uniform over the unit sphere; for this class the information-theoretic lower bound is only $\\Theta(k \\log n)$.  Our approach for both parts is based on spherical harmonics. We view gradient descent as an operator on the space of functions, and study its dynamics. An essential tool is the Funk-Hecke theorem, which explains the eigenfunctions of this operator in the case of the mean squared loss.", "pdf_url": "http://proceedings.mlr.press/v99/vempala19a/vempala19a.pdf", "keywords": [], "reference": "Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pages 1908-1916, 2014.  2   GRADIENT DESCENT FOR ONE-HIDDEN-LAYER NEURAL NETWORKS  Besides their generality, an important feature of NNs is the ease of training them \u2014 gradient descent (GD) is used to minimize the error of the network, measured by a loss function of the current weights. This seems to work across a range of labeled data sets. Yet despite its tremendous success, there is no satisfactory explanation for the efficiency or effectiveness of this generic training algorithm.  In this paper we give nearly matching upper and lower bounds that help explain the phenomena seen in practice when training NNs. The upper bounds are for GD and the lower bounds are for all statistical query algorithms.  We consider NNs with n-dimensional inputs, a single hidden layer with m units having some nonlinear activation \u03c6 : R \u2192 R, and a single linear output unit. All units are without additive bias terms. We will consider inputs drawn from the uniform distribution on Sn\u22121. We initialize our NNs by choosing the vectors for each hidden-layer unit uniformly and indpenently from Sn\u22121 and setting all output-layer weights to 0. The specific GD procedure we consider is as follows: in each iteration, the gradient of the loss function is computed using a finite sample of examples, with the entire sample reused for each iteration. The output-layer weights are then modified by adding a fixed multiple of the estimated gradient, and the hidden-layer weights are kept fixed.  Our algorithmic result is an agnostic upper bound on the approximation error and time and sample complexity of GD with the standard mean squared loss function. Despite training only the output layer weights, our novel proof techniques avoid using any convexity in the problem. Since our analysis does not rely on reaching a global minimum, there is reason to hope the techniques will extend to nonconvex settings where we can in general expect only to find a local minimum. Prior results along this line were either for more complicated algorithms or more restricted settings; the closest is the work of Andoni et al. Andoni et al. (2014) where they assume the target function is a bounded degree polynomial. We illustrating the power of our proof technique by obtaining, as an immediate corollary of our convergence analaysis, a rigorous proof of the \u201cspectral bias\u201d of gradient descent observed experimentally in Rahaman et al. (2018).  The upper bound shows that to get close to the best possible degree k polynomial approximation of the data, it suffices to run GD on a NN with nO(k) units, using the same number of samples. It suffices to train the output layer weights alone. This is an agnostic guarantee. We prove a matching lower bound for solving this polynomial learning problem over the uniform distribution on the unit sphere, for any statistical query algorithm that uses tolerance inversely proportional to n\u2126(k). Thus, for this general agnostic learning problem, GD is as good as it gets.  The authors are grateful to Adam Kalai and Le Song for helpful discussions. The authors also thank Jo\u00a8el Bella\u00a8\u0131che and the anonymous referees for careful reading and many suggestions that improved the presentation. This work was supported in part by NSF grants CCF-1563838, CCF-1717349 and E2CDA-1640081.  Acknowlegments  References  Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pages 1908-1916, 2014. GRADIENT DESCENT FOR ONE-HIDDEN-LAYER NEURAL NETWORKS  Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.  IEEE Transactions on Information theory, 39(3):930-945, 1993.  George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734, 2018. "}, "Estimation of smooth densities in Wasserstein distance": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Estimation of smooth densities in Wasserstein distance", "abstract": "The Wasserstein distances are a set of metrics on probability distributions supported on $\\mathbb{R}^d$ with applications throughout statistics and machine learning. Often, such distances are used in the context of variational problems, in which the statistician employs in place of an unknown measure a proxy constructed on the basis of independent samples. This raises the basic question of how well measures can be approximated in Wasserstein distance. While it is known that an empirical measure comprising i.i.d.\u00a0samples is rate-optimal for general measures, no improved results were known for measures possessing smooth densities. We prove the first minimax rates for estimation of smooth densities for general Wasserstein distances, thereby showing how the curse of dimensionality can be alleviated for sufficiently regular measures. We also show how to construct discretely supported measures, suitable for computational purposes, which enjoy improved rates. Our approach is based on novel bounds between the Wasserstein distances and suitable Besov norms, which may be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v99/weed19a/weed19a.pdf", "keywords": ["Wasserstein distance", "nonparametric density estimation", "optimal transport"], "reference": "199-201, 1942.  L. Kantorovitch. On the translocation of masses. C. R. (Doklady) Acad. Sci. URSS (N.S.), 37:  Gaspard Monge. M\u00b4emoire sur la th\u00b4eorie des d\u00b4eblais et des remblais. Histoire de l\u2019Acad\u00b4emie royale  des sciences, 1:666-704, 1781.  Gabriel Peyr\u00b4e and Marco Cuturi. Computational optimal transport. Technical report, 2017.  C\u00b4edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,  2008.  2   ESTIMATION OF SMOOTH DENSITIES IN WASSERSTEIN DISTANCE  away from 0. Indeed, we show that the optimal rate for general densities is strictly worse than the corresponding rate for densities bounded below, no matter the smoothness. Our bounds are obtained via a new technical result, which shows that if two probability measures on [0, 1]d have densities bounded away from zero, then the Wasserstein distance between them can be controlled by a Besov norm of negative smoothness.  Algorithmic aspects are an important part of optimal transport problems. For practical appli- cations, the proposed estimators must therefore also be computationally tractable. We describe a method to produce computationally tractable atomic estimators via resampling from any estimator that outperforms the empirical distribution, under minimal assumptions. We study the computa- tional cost of this method, compared to the cost of using the empirical distribution with n atoms, and exhibit a trade-off between computational cost and statistical precision.  References  199-201, 1942.  L. Kantorovitch. On the translocation of masses. C. R. (Doklady) Acad. Sci. URSS (N.S.), 37:  Gaspard Monge. M\u00b4emoire sur la th\u00b4eorie des d\u00b4eblais et des remblais. Histoire de l\u2019Acad\u00b4emie royale  des sciences, 1:666-704, 1781.  Gabriel Peyr\u00b4e and Marco Cuturi. Computational optimal transport. Technical report, 2017.  C\u00b4edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,  2008. "}, "Estimating the Mixing Time of Ergodic Markov Chains": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Estimating the Mixing Time of Ergodic Markov Chains", "abstract": "We address the problem of estimating the mixing time $t_{\\mathsf{mix}}$ of an arbitrary ergodic finite Markov chain from a single trajectory of length $m$. The reversible case was addressed by Hsu et al. [2018+], who left the general case as an open problem. In the reversible case, the analysis is greatly facilitated by the fact that the Markov operator is self-adjoint, and Weyl\u2019s inequality allows for a dimension-free perturbation analysis of the empirical eigenvalues. As Hsu et al. point out, in the absence of reversibility (and hence, the non-symmetry of the pair probabilities matrix), the existing perturbation analysis has a worst-case exponential dependence on the number of states $d$. Furthermore, even if an eigenvalue perturbation analysis with better dependence on $d$ were available, in the non-reversible case the connection between the spectral gap and the mixing time is not nearly as straightforward as in the reversible case. Our key insight is to estimate the pseudo-spectral gap instead, which allows us to overcome the loss of self-adjointness and to achieve a polynomial dependence on $d$ and the minimal stationary probability $\\pi_\\star$. Additionally, in the reversible case, we obtain simultaneous nearly (up to logarithmic factors) minimax rates in $t_{\\mathsf{mix}}$ and precision $\\varepsilon$, closing a gap in Hsu et al., who treated $\\varepsilon$ as constant in the lower bounds. Finally, we construct fully empirical confidence intervals for the pseudo-spectral gap, which shrink to zero at a rate of roughly $1/\\sqrt m$, and improve the state of the art in even the reversible case.", "pdf_url": "http://proceedings.mlr.press/v99/wolfer19a/wolfer19a.pdf", "keywords": ["ergodic Markov chain", "mixing time", "non-reversible Markov chain"], "reference": "Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  12   ESTIMATING THE MIXING TIME OF ERGODIC MARKOV CHAINS  and applying Laplace \u03b1-smoothing. We then notice that (cid:98)\u03b3\u2020  k,r,\u03b1 = \u03b3  (cid:98)L  (cid:18)(cid:16)  (k,r,\u03b1)(cid:17)(cid:124)  (k,r,\u03b1)(cid:19)  (cid:98)L  , where  (cid:16)  (k,r,\u03b1)(cid:17)(cid:124)  (k,r,\u03b1)  (cid:98)L  (cid:98)L  (cid:16)  =  N(k,r,\u03b1) (cid:44)  (cid:17)\u22121/2 (cid:16)  D(k,r,\u03b1) N (cid:104) N (k,r)  ij + \u03b1  (cid:105)  N(k,r,\u03b1)(cid:17)(cid:124) (cid:16) , D(k,r,\u03b1) N  D(k,r,\u03b1) N (cid:16)  (cid:44) diag  (i,j)  (cid:17)\u22121  N(k,r,\u03b1) (cid:16)  D(k,r,\u03b1) N  (cid:17)\u22121/2  N (k,r) 1  + d\u03b1, . . . , N (k,r)  + d\u03b1  .  d  The derivation of the confidence intervals starts with an empirical version of the decomposition introduced for the point estimator. The subsequent analysis has two key components. The first is a perturbation bound for the stationary distribution in terms of the pseudo-spectral gap and the stability of the perturbation of matrix with respect to the (cid:107)\u00b7(cid:107)\u221e norm. More precisely, Lemma 19 guarantees that  (cid:107)(cid:98)\u03c0 \u2212 \u03c0(cid:107)\u221e \u2264 \u02dcO (1)  1 (cid:16)  (cid:13) (cid:13) (cid:13) (cid:99)M \u2212 M  (cid:13) (cid:13) (cid:13)\u221e  .  (cid:17)  \u03b3ps  (cid:99)M  The second component (Lemma 21) involves controlling the latter perturbation in terms of empiri- cally observable quantities. In particular,  ,  (cid:17)  (7.12)  (7.13)  (7.14)  (cid:13) (cid:13) (cid:13) (cid:99)M \u2212 M  (cid:13) (cid:13) (cid:13)\u221e  \u2264 \u02dcO (1)  (cid:114) d  Nmin  holds with high probability \u2014 which is an empirical version of the result of Wolfer and Kontorovich (2019, Theorem 1), achieved by constructing and analyzing appropriate row-martingales.  \u2020  \u221a  Reversible setting. Our analysis also yields improvements over the state of the art estimation procedure in the reversible setting, where Hsu et al. (2015) used the absolute spectral gap \u03b3(cid:63) of the additive reversiblization of the empirical transition matrix (cid:99)M as the estimator for the mixing time. Our analysis via row-martingales sharpens the confidence intervals roughly by a factor of O( d) over the previous method. The latter relied on entry-wise martingales together with the metric inequality (cid:107)A(cid:107)\u221e \u2264 d max(i,j)\u2208[d]2 |A(i, j)| , A \u2208 Rd\u00d7d. Additionally, we show that the computation complexity of the task can be reduced over non-trivial parameter regimes. We achieve this via iterative methods for computing the second largest eigenvalue, and by replacing an expen- sive pseudo-inverse computation by the already-computed estimator for \u03b3(cid:63) itself (Corollary 24). These computational improvements do not degrade the asymptotic behavior of the confidence inter- vals.  + (cid:99)M 2  We are thankful to Daniel Paulin for the insightful conversations, and to the anonymous referees for their valuable comments.  Acknowledgments  References  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005. ESTIMATING THE MIXING TIME OF ERGODIC MARKOV CHAINS  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing that In Foundations of Computer Science, 2000. Proceedings. 41st Annual  distributions are close. Symposium on, pages 259-269. IEEE, 2000.  Tu\u02d8gkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing close-  ness of discrete distributions. Journal of the ACM (JACM), 60(1):4, 2013.  Bhaswar Bhattacharya and Gregory Valiant. Testing closeness with unequal sized samples.  In  Advances in Neural Information Processing Systems, pages 2611-2619, 2015.  Olivier Bousquet, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Introduction to statistical learning the-  ory. In Advanced lectures on machine learning, pages 169-207. Springer, 2004.  Fang Chen, L\u00b4aszl\u00b4o Lov\u00b4asz, and Igor Pak. Lifting markov chains to speed up mixing. In Proceedings of the thirty-first annual ACM symposium on Theory of computing, pages 275-281. ACM, 1999.  Ting-Li Chen and Chii-Ruey Hwang. Accelerating reversible markov chains. Statistics & Proba-  bility Letters, 83(9):1956-1962, 2013.  Grace E Cho and Carl D Meyer. Comparison of perturbation bounds for the stationary distribution  of a markov chain. Linear Algebra and its Applications, 335(1-3):137-150, 2001.  Richard Combes and Mikael Touati. Computationally efficient estimation of the spectral gap of a  markov chain. arXiv preprint arXiv:1806.06047, 2018.  Constantinos Daskalakis, Nishanth Dikkala, and Nick Gravin. Testing symmetric markov chains from a single trajectory. In S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 385-409. PMLR, 06-09 Jul 2018. URL http://proceedings. mlr.press/v75/daskalakis18a.html.  Persi Diaconis, Susan Holmes, and Radford M Neal. Analysis of a nonreversible markov chain  sampler. Annals of Applied Probability, pages 726-752, 2000.  James Allen Fill. Eigenvalue bounds on convergence to stationarity for nonreversible markov chains, with an application to the exclusion process. The annals of applied probability, pages 62-87, 1991.  David Gamarnik. Extension of the pac framework to finite and countable markov chains. IEEE  Transactions on Information Theory, 49(1):338-345, 2003.  M Hildebrand. Rates of convergence for a non-reversible markov chain sampler. preprint, 1997.  Daniel J Hsu, Aryeh Kontorovich, and Csaba Szepesv\u00b4ari. Mixing time estimation in reversible markov chains from a single sample path. In Advances in neural information processing systems, pages 1459-1467, 2015.  Daniel J. Hsu, Aryeh Kontorovich, David A. Levin, Yuval Peres, Csaba Szepesv\u00b4ari, and Geoffrey Wolfer. Mixing time estimation in reversible markov chains from a single sample path. to appear in Annals of Applied Probability, 2018+. ESTIMATING THE MIXING TIME OF ERGODIC MARKOV CHAINS  Shmuel Kaniel. Estimates for some computational techniques in linear algebra. Mathematics of  Computation, 20(95):369-378, 1966.  Rajeeva L Karandikar and Mathukumalli Vidyasagar. Rates of uniform convergence of empirical  means with mixing processes. Statistics & probability letters, 58(3):297-307, 2002.  Dimitri Kazakos. The Bhattacharyya distance and detection between markov chains. IEEE Trans-  actions on Information Theory, 24(6):747-754, 1978.  Michael J. Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. On the learnability of discrete distributions. In Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr\u00b4eal, Qu\u00b4ebec, Canada, pages 273-282, 1994. doi: 10.1145/195058.195155. URL http://doi.acm.org/10.1145/ 195058.195155.  Jacek Kuczy\u00b4nski and Henryk Wo\u00b4zniakowski. Estimating the largest eigenvalue by the power and lanczos algorithms with a random start. SIAM journal on matrix analysis and applications, 13 (4):1094-1122, 1992.  Franc\u00b8ois Le Gall. Powers of tensors and fast matrix multiplication.  In Proceedings of the 39th  international symposium on symbolic and algebraic computation, pages 296-303. ACM, 2014.  David A Levin and Yuval Peres. Estimating the spectral gap of a reversible markov chain from a  short trajectory. arXiv preprint arXiv:1612.05330, 2016.  David Asher Levin, Yuval Peres, and Elizabeth Lee Wilmer. Markov chains and mixing times,  second edition. American Mathematical Soc., 2009.  Carl D Meyer, Jr. The role of the group generalized inverse in the theory of finite markov chains.  Siam Review, 17(3):443-464, 1975.  Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for non-iid processes. In Advances in  Neural Information Processing Systems, pages 1025-1032, 2008.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes.  In Advances in Neural Information Processing Systems, pages 1097-1104, 2009.  Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in markov chains. Foun-  dations and Trends R(cid:13) in Theoretical Computer Science, 1(3):237-354, 2006.  Radford M Neal. Improving asymptotic variance of mcmc estimators: Non-reversible chains are  better. arXiv preprint math/0407281, 2004.  Christopher Conway Paige. The computation of eigenvalues and eigenvectors of very large sparse  matrices. PhD thesis, University of London, 1971.  Daniel Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral  methods. Electronic Journal of Probability, 20, 2015.  Qian Qin, James P Hobert, and Kshitij Khare. Estimating the spectral gap of a trace-class markov  operator. arXiv preprint arXiv:1704.00850, 2017. ESTIMATING THE MIXING TIME OF ERGODIC MARKOV CHAINS  Yousef Saad. On the rates of convergence of the lanczos and the block-lanczos methods. SIAM  Journal on Numerical Analysis, 17(5):687-706, 1980.  Cosma Rohilla Shalizi and Aryeh Kontorovich. Predictive PAC learning and process decomposi-  tions. In Neural Information Processing Systems (NIPS), 2013.  Ingo Steinwart and Andreas Christmann. Fast learning from non-iid observations. In Advances in  neural information processing systems, pages 1768-1776, 2009.  Ingo Steinwart, Don Hush, and Clint Scovel. Learning from dependent observations. Journal of  Multivariate Analysis, 100(1):175-194, 2009.  Yi Sun, J\u00a8urgen Schmidhuber, and Faustino J Gomez.  Improving the asymptotic performance of markov chain monte-carlo by inserting vortices. In Advances in Neural Information Processing Systems, pages 2235-2243, 2010.  Hidemaro Suwa and Synge Todo. Markov chain monte carlo method without detailed balance.  Physical review letters, 105(12):120603, 2010.  Joel Tropp. Freedman\u2019s inequality for matrix martingales. Electronic Communications in Probabil-  ity, 16:262-270, 2011.  Alexandre B. Tsybakov. Introduction to nonparametric estimation, 2009. URL https://doi. org/10.1007/b13794. Revised and extended from the 2004 French original, Translated by Vladimir Zaiats.  Konstantin S Turitsyn, Michael Chertkov, and Marija Vucelja. Irreversible monte carlo algorithms  for efficient sampling. Physica D: Nonlinear Phenomena, 240(4-5):410-414, 2011.  Marija Vucelja. Liftinga nonreversible markov chain monte carlo algorithm. American Journal of  Physics, 84(12):958-968, 2016.  Bo Waggoner. Lp testing and learning of discrete distributions.  In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, ITCS 2015, Rehovot, Israel, Jan- doi: 10.1145/2688073.2688095. URL http: uary 11-13, 2015, pages 347-356, 2015. //doi.acm.org/10.1145/2688073.2688095.  Geoffrey Wolfer and Aryeh Kontorovich. Minimax learning of ergodic markov chains. In Aur\u00b4elien Garivier and Satyen Kale, editors, Proceedings of the 30th International Conference on Algorith- mic Learning Theory, volume 98 of Proceedings of Machine Learning Research, pages 904-930, Chicago, Illinois, 22-24 Mar 2019. PMLR. URL http://proceedings.mlr.press/ v98/wolfer19a.html.  Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals  of Probability, pages 94-116, 1994.  "}, "Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the O(1T) Convergence Rate": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the O(1T) Convergence Rate", "abstract": "Stochastic approximation (SA) is a classical approach for stochastic convex optimization. Previous studies have demonstrated that the convergence rate of SA can be improved by introducing either smoothness or strong convexity condition. In this paper, we make use of smoothness and strong convexity simultaneously to boost the convergence rate. Let $\\lambda$ be the modulus of strong convexity,  $\\kappa$ be the condition number, $F_*$ be the minimal risk, and $\\alpha>1$ be some small constant. First, we demonstrate that, in expectation, an $O(1/[\\lambda T^\\alpha] + \\kappa F_*/T)$ risk bound is attainable when $T = \\Omega(\\kappa^\\alpha)$. Thus, when $F_*$ is small, the convergence rate could be faster than $O(1/[\\lambda T])$ and approaches $O(1/[\\lambda T^\\alpha])$ in the ideal case. Second, to further benefit from small risk, we show that, in expectation, an $O(1/2^{T/\\kappa}+F_*)$ risk bound is achievable. Thus, the excess risk reduces exponentially until reaching $O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we emphasize that our proof is constructive and each risk bound is equipped with an efficient stochastic algorithm attaining that bound.", "pdf_url": "http://proceedings.mlr.press/v99/zhang19a/zhang19a.pdf", "keywords": ["Stochastic Approximation", "Stochastic Convex Optimization", "Excess Risk", "Smoothness", "Strong Convexity"], "reference": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  Proceedings of the 33rd International Conference on Machine Learning, pages 699-707, 2016.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1-51, 2017.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.  Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems 29, pages 3576-3584, 2016.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, pages 797-842, 2015.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paral- lelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. Journal of Machine Learning Research, 18(223):1-42, 2018.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  13   FAST RATES FOR STOCHASTIC APPROXIMATION  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In  Proceedings of the 33rd International Conference on Machine Learning, pages 699-707, 2016.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1-51, 2017.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.  Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems 29, pages 3576-3584, 2016.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, pages 797-842, 2015.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469-1492, 2012.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489-2512, 2014.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paral- lelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. Journal of Machine Learning Research, 18(223):1-42, 2018.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013. FAST RATES FOR STOCHASTIC APPROXIMATION  Sujin Kim, Raghu Pasupathy, and Shane G. Henderson. A Guide to Sample Average Approximation,  pages 207-243. 2015.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  G\u00b4abor Lugosi. Concentration-of-measure inequalities. Technical report, Department of Economics,  Pompeu Fabra University, 2009.  Mehrdad Mahdavi and Rong Jin. Passive learning with target risk. Annual Conference on Learning Theory, pages 252-269, 2013.  In Proceedings of the 26th  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Proceedings of the 28th Annual Conference on Learning Theory, 2015.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. ArXiv e-  prints, arXiv:1605.01288, 2016.  Eric Moulines and Francis R. Bach. Non-asymptotic analysis of stochastic approximation algo- rithms for machine learning. In Advances in Neural Information Processing Systems 24, pages 451-459, 2011.  I. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for non-strongly  convex optimization. Mathematical Programming, 175(1):69-107, 2019.  Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems 27, pages 1017-1025, 2014.  A. Nemirovski and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. John  Wiley & Sons Ltd, 1983.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Random gradient-free minimization of convex functions. Core discussion papers,  2011.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00b4as P\u00b4ocz\u00b4os, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on Machine Learning, 2016. FAST RATES FOR STOCHASTIC APPROXIMATION  Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo- nential convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25, pages 2672-2680, 2012.  Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong  growth condition. ArXiv e-prints, arXiv:1308.6370, 2013.  Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized  loss minimization. Journal of Machine Learning Research, 14:567-599, 2013.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver- gence results and optimal averaging schemes. In Proceedings of the 30th International Confe- rence on Machine Learning, pages 71-79, 2013.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on Stochastic Pro-  gramming: Modeling and Theory. SIAM, second edition, 2014.  M.V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computa-  tional Optimization and Applications, 11(1):23-35, 1998.  Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth  loss. ArXiv e-prints, arXiv:1009.3896, 2010.  Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In Proceedings of the 30th Annual Conference on Learning Theory, pages 1882-1919, 2017.  Andre Wibisono, Martin J Wainwright, Michael I. Jordan, and John C. Duchi. Finite sample con- vergence rates of zero-order stochastic optimization methods. In Advances in Neural Information Processing Systems 25, pages 1448-1456, 2012.  Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde- pendent access of full gradients. In Advance in Neural Information Processing Systems 26, pages 980-988, 2013a.  Lijun Zhang, Tianbao Yang, Rong Jin, and Xiaofei He. O(log T ) projections for stochastic op- timization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning, 2013b.  Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex opti- mization: O(1/n)- and O(1/n2)-type of risk bounds. In Proceedings of the 30th Annual Confe- rence on Learning Theory, pages 1954-1979, 2017a.  Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent al- In Proceedings of the 21st International Conference on Machine Learning, pages  gorithms. 919-926, 2004. FAST RATES FOR STOCHASTIC APPROXIMATION  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In Proceedings of the 30th Annual Conference on Learning Theory, pages 1980-2022, 2017b.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928-936, 2003.  "}, "Open Problem: Is Margin Sufficient for Non-Interactive Private Distributed Learning?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: Is Margin Sufficient for Non-Interactive Private Distributed Learning?", "abstract": "We ask whether every class of Boolean functions that has polynomial margin complexity can be PAC learned efficiently by a non-interactive locally differentially private algorithm.", "pdf_url": "http://proceedings.mlr.press/v99/daniely19a/daniely19a.pdf", "keywords": [], "reference": "Jayadev Acharya, Cl\u00b4ement L Canonne, and Himanshu Tyagi.  Inference under information con-  straints i: Lower bounds from chi-square contraction. arXiv preprint arXiv:1812.11476, 2018.  Apple\u2019s Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning Jour-  nal, 1(9), December 2017.  arXiv:1809.09165, 2018.  Amit Daniely and Vitaly Feldman. Learning without interaction requires separation. arXiv preprint  Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In 31st  Conference on Neural Information Processing Systems (NIPS), pages 3574-3583, 2017.  John Duchi and Ryan Rogers. Lower bounds for locally private estimation via communication  complexity. arXiv preprint arXiv:1902.00582, 2019.  John C. Duchi, Martin J. Wainwright, and Michael I. Jordan. Local privacy and minimax bounds:  Sharp rates for probability estimation. In NIPS, pages 1529-1537, 2013.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  \u00b4Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggregatable In ACM SIGSAC Conference on Computer and Commu-  privacy-preserving ordinal response. nications Security, pages 1054-1067, 2014.  Alexandre V. Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches  in privacy preserving data mining. In PODS, pages 211-222, 2003.  Matthew Joseph, Jieming Mao, Seth Neel, and Aaron Roth. The role of interactivity in local dif- ferential privacy. CoRR, abs/1904.03564, 2019. URL http://arxiv.org/abs/1904. 03564.  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, June 2011.  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  Adam D. Smith, Abhradeep Thakurta, and Jalaj Upadhyay. Is interaction necessary for distributed In 2017 IEEE Symposium on Security and Privacy, SP 2017, pages 58-77,  983-1006, 1998.  private learning? 2017.  4   OPEN PROBLEM: IS MARGIN SUFFICIENT FOR NON-INTERACTIVE PRIVATE DISTRIBUTED LEARNING?  STATP (\u03c4 ) for \u03c4 = \u0398(\u03b4/e2(cid:15)). These simulations are interactive if and only if the original algorithm was interactive. Hence one can equivalently formulate our open problem as a problem about the power of non-adaptive SQ algorithms. This formulations also allows to derive corollaries for ot- her models that are known to be equivalent to the SQ model (see (Daniely and Feldman, 2018) for additional details).  References  Jayadev Acharya, Cl\u00b4ement L Canonne, and Himanshu Tyagi.  Inference under information con-  straints i: Lower bounds from chi-square contraction. arXiv preprint arXiv:1812.11476, 2018.  Apple\u2019s Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning Jour-  nal, 1(9), December 2017.  arXiv:1809.09165, 2018.  Amit Daniely and Vitaly Feldman. Learning without interaction requires separation. arXiv preprint  Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In 31st  Conference on Neural Information Processing Systems (NIPS), pages 3574-3583, 2017.  John Duchi and Ryan Rogers. Lower bounds for locally private estimation via communication  complexity. arXiv preprint arXiv:1902.00582, 2019.  John C. Duchi, Martin J. Wainwright, and Michael I. Jordan. Local privacy and minimax bounds:  Sharp rates for probability estimation. In NIPS, pages 1529-1537, 2013.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  \u00b4Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggregatable In ACM SIGSAC Conference on Computer and Commu-  privacy-preserving ordinal response. nications Security, pages 1054-1067, 2014.  Alexandre V. Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches  in privacy preserving data mining. In PODS, pages 211-222, 2003.  Matthew Joseph, Jieming Mao, Seth Neel, and Aaron Roth. The role of interactivity in local dif- ferential privacy. CoRR, abs/1904.03564, 2019. URL http://arxiv.org/abs/1904. 03564.  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, June 2011.  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  Adam D. Smith, Abhradeep Thakurta, and Jalaj Upadhyay. Is interaction necessary for distributed In 2017 IEEE Symposium on Security and Privacy, SP 2017, pages 58-77,  983-1006, 1998.  private learning? 2017. OPEN PROBLEM: IS MARGIN SUFFICIENT FOR NON-INTERACTIVE PRIVATE DISTRIBUTED LEARNING?  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Stanley L. Warner. Randomized response: A survey technique for eliminating evasive answer bias.  J. of the American Statistical Association, 60(309):63-69, 1965. "}, "Open Problem: How fast can a multiclass test set be overfit?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: How fast can a multiclass test set be overfit?", "abstract": "We ask how many measurements of the accuracy on a multiclass benchmark are needed to achieve a given amount of overfitting.", "pdf_url": "http://proceedings.mlr.press/v99/feldman19b/feldman19b.pdf", "keywords": [], "reference": "Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competi-  tions. CoRR, abs/1502.04585, 2015. URL http://arxiv.org/abs/1502.04585.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638, 2015. doi: 10.1126/science.aaa9375. URL http://www.sciencemag.org/content/ 349/6248/636.abstract.  Vitaly Feldman, Roy Frostig, and Moritz Hardt. The advantages of multiple classes for reducing  overfitting from test set reuse. In ICML, 2019. To appear.  Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classi- fiers generalize to CIFAR-10? CoRR, abs/1806.00451, 2018. Study was extended to ImageNet in subsequent unpublished work (to appear shortly). Personal communication by the authors.  Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers  generalize to ImageNet? CoRR, abs/1902.10811, 2019.  4   MULTICLASS OVERFITTING  sampled i.i.d. from P , and produces f such that  E S\u223cP n  [accS(f )] =  1 m  + \u02dc\u2126  (cid:32)(cid:114)  (cid:33)  k nm  .  Naturally, this bound can only hold under additional restrictions on the range of parameters. It would be sufficient to cover the regime where k \u2264 n/m since beyond that regime tight upper and lower bounds are given in Feldman et al. (2019). From a more practical point of view, one should also restrict the attention to efficient algorithms. Still, the answer is not known even without this restriction. The answer is also not known for an even stronger class of attack algorithms that have access to points (but not labels) in the dataset S. An attack of this type is described in Feldman et al. (2019). For any S and k = \u2126(m log m) it outputs f such that:  accS(f ) = min  1,  + \u2126  (cid:26)  1 m  (cid:18) k log(k/m) n log m  (cid:19)(cid:27)  .  Figure 1 is based on a simulation of the attack underlying Theorem 1.2. It shows the number of queries at which a fixed advantage over 1/m is first attained, while the number of class labels m varies, on a randomly generated test set of size 100,000. The endpoints of the curves in the figure form lines of slope greater than 1 on a log-log scale. This might suggest that, to attain a fixed bias using the attack underlying Theorem 1.2, the number of queries k must indeed grow super-linearly with m.  References  Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competi-  tions. CoRR, abs/1502.04585, 2015. URL http://arxiv.org/abs/1502.04585.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638, 2015. doi: 10.1126/science.aaa9375. URL http://www.sciencemag.org/content/ 349/6248/636.abstract.  Vitaly Feldman, Roy Frostig, and Moritz Hardt. The advantages of multiple classes for reducing  overfitting from test set reuse. In ICML, 2019. To appear.  Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classi- fiers generalize to CIFAR-10? CoRR, abs/1806.00451, 2018. Study was extended to ImageNet in subsequent unpublished work (to appear shortly). Personal communication by the authors.  Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers  generalize to ImageNet? CoRR, abs/1902.10811, 2019. MULTICLASS OVERFITTING  Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.  Chhavi Yadav and L\u00b4eon Bottou. Cold case: The lost MNIST digits. 2019. "}, "Open Problem: Do Good Algorithms Necessarily Query Bad Points?": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: Do Good Algorithms Necessarily Query Bad Points?", "abstract": "Folklore results in the theory of Stochastic Approximation indicates the (minimax) optimality of Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) with polynomially decaying stepsizes and iterate averaging (Ruppert, 1988; Polyak and Juditsky, 1992) for classes of stochastic convex optimization. Basing of these folkore results and some recent developments, this manuscript considers a more subtle question: does any algorithm necessarily (information theoretically) have to query iterates that are sub-optimal infinitely often?", "pdf_url": "http://proceedings.mlr.press/v99/ge19b/ge19b.pdf", "keywords": ["Stochastic Approximation", "Stochastic Gradient Descent", "Iterate Averaging", "Minimax Optimality"], "reference": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  Rong Ge, Sham M. Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure. CoRR, 2019. URL https: //arxiv.org/abs/1904.12838.  Nicholas J. A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. CoRR, 2018. URL http://arxiv.org/abs/ 1812.05217.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par- allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint arXiv:1610.03774, 2016.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-  ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.  Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information  theoretically optimal. CoRR, 2019. URL https://arxiv.org/abs/1904.12443.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Texts in Statistics.  Springer, 1998. ISBN 9780387985022.  Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-  gence results and optimal averaging schemes. CoRR, abs/1212.1824, 2012.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.  4   DO GOOD ALGORITHMS NECESSARILY QUERY BAD POINTS?  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. How to make the gradients small stochastically. CoRR, abs/1801.02982, 2018.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  Rong Ge, Sham M. Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal, geometrically decaying learning rate procedure. CoRR, 2019. URL https: //arxiv.org/abs/1904.12838.  Nicholas J. A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. CoRR, 2018. URL http://arxiv.org/abs/ 1812.05217.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par- allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint arXiv:1610.03774, 2016.  Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-  ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.  Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information  theoretically optimal. CoRR, 2019. URL https://arxiv.org/abs/1904.12443.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer Texts in Statistics.  Springer, 1998. ISBN 9780387985022.  Arkadi S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-  gence results and optimal averaging schemes. CoRR, abs/1212.1824, 2012.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. "}, "Open Problem: Risk of Ruin in Multiarmed Bandits": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: Risk of Ruin in Multiarmed Bandits", "abstract": "We formalize a particular class of problems called \\textit{survival multiarmed bandits} (S-MAB), which constitutes a modified version of \\textit{budgeted multiarmed bandits} (B-MAB) where a true \\textit{risk of ruin} must be considered, bringing it closer to \\textit{risk-averse multiarmed bandits} (RA-MAB). In a S-MAB, pulling an arm can result in both positive and negative rewards. The agent has an initial budget that evolves in time with the received rewards. The goal is finding a good \\textit{exploration-exploitation-safety} trade-off, maximizing rewards while minimizing the probability of getting ruined (i.e. hitting a negative budget). Such simple and until now neglected modification in the MAB statement changes the way to approach the problem, asking for adapted algorithms and specific analytical tools, and also make it more likely related to some important real-world applications. We are interested in the following open problems which stem from such new MAB definition: (a) how can the regret be meaningfully defined in formal terms for a S-MAB given its multiobjective optimization nature? (b) can a S-MAB be reduced to a RA-MAB or a B-MAB, transferring their theoretical guarantees? (c) what kind of method or strategy must an agent follow to optimally solve a S-MAB?", "pdf_url": "http://proceedings.mlr.press/v99/perotto19a/perotto19a.pdf", "keywords": ["Budgeted Multiarmed Bandits", "Ruin Theory", "Risk-Averse Decision Making"], "reference": "S. Agrawal, N.R. Devanur, and L. Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. In 29th Conf. on Learning Theory (COLT), June 23-26, New York, pages 4-18, 2016.  J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In 23rd  Conf. on Learning Theory (COLT), June 27-29, Haifa, pages 41-53, 2010.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Mach. Learn., 47(2-3):235-256, 2002.  A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. J. ACM, 65(3):13:1-  13:55, 2018.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. JMLR, 12:1655-1695, 2011.  A. Cassel, S. Mannor, and A. Zeevi. A general approach to multi-armed bandits under risk criteria.  In 31st Conf. on Learning Theory (COLT), July 6-9, Stockholm, pages 1295-1306, 2018.  W. Ding, T. Qin, X.-D. Zhang, and T.-Y. Liu. Multi-armed bandit with budget constraint and variable  costs. In 27th AAAI Conf. on Artificial Intelligence, July 14-18, Bellevue, WA, USA, 2013.  N. Galichet, M. Sebag, and O. Teytaud. Exploration vs exploitation vs safety: Risk-aware multi- armed bandits. In 5th Asian Conf. on Mach. Learn. (ACML), Nov. 13-15, Canberra, pages 245- 260, 2013.  T. Koren, R. Livni, and Y. Mansour. Multi-armed bandits with metric movement costs. In 31st Conf. on Neural Information Processing Systems (NIPS), Dec. 4-9, Long Beach, pages 4122- 4131, 2017.  O.-A. Maillard. Robust risk-averse stochastic multi-armed bandits. In 24th Int. Conf. on Algorithmic  Learning Theory (ALT), Oct. 6-9, Singapore, pages 218-233, 2013.  A. Sani, A. Lazaric, and R. Munos. Risk-aversion in multi-armed bandits. In 26th Conf. on Neural  Information Processing Systems (NIPS), Dec. 3-6, Lake Tahoe, pages 3284-3292, 2012.  S. Vakili and Q. Zhao. Risk-averse multi-armed bandit problems under mean-variance measure.  Signal Processing, 10(6):1093-1111, 2016.  Y. Wu, R. Shariff, T. Lattimore, and C. Szepesvari. Conservative bandits. In 33rd Int. Conf. on  Mach. Learn. (ICML), June 20-22, New York, pages 1254-1262, 2016.  Y. Xia, T. Qin, W. Ding, H. Li, X.-D. Zhang, N. Yu, and T.-Y. Liu. Finite budget analysis of  multi-armed bandit problems. Neurocomputing, 258:13-29, 2017.  D.P. Zhou and C.J. Tomlin. Budget-constrained multi-armed bandits with multiple plays. In 32nd  AAAI Conf. on Artificial Intelligence, Feb. 2-7, New Orleans, 2018.  4   OPEN PROBLEM: RISK OF RUIN IN MULTIARMED BANDITS  a B-MAB, transferring their theoretical guarantees? and finally (c) what kind of method or strategy must an agent follow to optimally solve a S-MAB?  References  S. Agrawal, N.R. Devanur, and L. Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. In 29th Conf. on Learning Theory (COLT), June 23-26, New York, pages 4-18, 2016.  J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In 23rd  Conf. on Learning Theory (COLT), June 27-29, Haifa, pages 41-53, 2010.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Mach. Learn., 47(2-3):235-256, 2002.  A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. J. ACM, 65(3):13:1-  13:55, 2018.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. JMLR, 12:1655-1695, 2011.  A. Cassel, S. Mannor, and A. Zeevi. A general approach to multi-armed bandits under risk criteria.  In 31st Conf. on Learning Theory (COLT), July 6-9, Stockholm, pages 1295-1306, 2018.  W. Ding, T. Qin, X.-D. Zhang, and T.-Y. Liu. Multi-armed bandit with budget constraint and variable  costs. In 27th AAAI Conf. on Artificial Intelligence, July 14-18, Bellevue, WA, USA, 2013.  N. Galichet, M. Sebag, and O. Teytaud. Exploration vs exploitation vs safety: Risk-aware multi- armed bandits. In 5th Asian Conf. on Mach. Learn. (ACML), Nov. 13-15, Canberra, pages 245- 260, 2013.  T. Koren, R. Livni, and Y. Mansour. Multi-armed bandits with metric movement costs. In 31st Conf. on Neural Information Processing Systems (NIPS), Dec. 4-9, Long Beach, pages 4122- 4131, 2017.  O.-A. Maillard. Robust risk-averse stochastic multi-armed bandits. In 24th Int. Conf. on Algorithmic  Learning Theory (ALT), Oct. 6-9, Singapore, pages 218-233, 2013.  A. Sani, A. Lazaric, and R. Munos. Risk-aversion in multi-armed bandits. In 26th Conf. on Neural  Information Processing Systems (NIPS), Dec. 3-6, Lake Tahoe, pages 3284-3292, 2012.  S. Vakili and Q. Zhao. Risk-averse multi-armed bandit problems under mean-variance measure.  Signal Processing, 10(6):1093-1111, 2016.  Y. Wu, R. Shariff, T. Lattimore, and C. Szepesvari. Conservative bandits. In 33rd Int. Conf. on  Mach. Learn. (ICML), June 20-22, New York, pages 1254-1262, 2016.  Y. Xia, T. Qin, W. Ding, H. Li, X.-D. Zhang, N. Yu, and T.-Y. Liu. Finite budget analysis of  multi-armed bandit problems. Neurocomputing, 258:13-29, 2017.  D.P. Zhou and C.J. Tomlin. Budget-constrained multi-armed bandits with multiple plays. In 32nd  AAAI Conf. on Artificial Intelligence, Feb. 2-7, New Orleans, 2018. "}, "Open Problem: Monotonicity of Learning": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: Monotonicity of Learning", "abstract": "We pose the question to what extent a learning algorithm behaves monotonically in the following sense: does it perform better, in expectation, when adding one instance to the training set? We focus on empirical risk minimization and illustrate this property with several examples, two where it does hold and two where it does not. We also relate it to the notion of PAC-learnability.", "pdf_url": "http://proceedings.mlr.press/v99/viering19a/viering19a.pdf", "keywords": ["Finite sample behavior", "Learnability", "Monotonic learning"], "reference": "Shai Ben-David, Nathan Srebro, and Ruth Urner. Universal learning vs. no free lunch results. In  Philosophy and Machine Learning Workshop NIPS, 2011.  Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan. Minimizing the misclassifi-  cation error rate using a surrogate convex loss. In Proceedings ICML, pages 83-90, 2012.  Robert P W Duin. Small sample size generalization. In Proceedings of the Scandinavian Conference  on Image Analysis, volume 2, pages 957-964, 1995.  Wouter M Koolen, Peter Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and  stochastic fast rates in online learning. In NIPS, pages 4457-4465, 2016.  Jesse H Krijthe and Marco Loog. Projected estimators for robust semi-supervised classification.  Machine Learning, 106(7):993-1008, 2017.  Anqi Liu, Lev Reyzin, and Brian D Ziebart. Shift-pessimistic Active Learning Using Robust Bias-  aware Prediction. In Proceedings of AAAI-15, pages 2764-2770, 2015.  Marco Loog. Constrained parameter estimation for semi-supervised learning: the case of the nearest  mean classifier. In ECML PKDD 2010, pages 291-304. Springer, 2010.  Marco Loog and Robert P W Duin. The dipping phenomenon. In Proceedings of the IAPR S+SSPR,  pages 310-317. Springer, 2012.  Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural  networks III, pages 151-209. Springer, 1996.  Sarunas Raudys and Robert P W Duin. Expected classification error of the Fisher linear classifier  with pseudo-inverse covariance matrix. Pattern recognition letters, 19(5-6):385-392, 1998.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  4   OPEN PROBLEM: MONOTONICITY OF LEARNING  of demanding a lower loss, we may require that the loss does not degrade too much. Or we can demand the property to hold with high probability with respect to both samples.  More generally, we may ask: why and how does this behaviour occur? And maybe more impor- tantly: how can we provably avoid non-monotone behaviour? What conditions does a learner need to satisfy to be monotone? Perhaps particular loss functions lead to monotone learners? What if we allow for learning under regularization or other strategies deviating from strict ERM, for example improper learners or randomized decision rules?  Perhaps it is always possible to find a D for a given Z on which learners are non-monotone. In that case, is it possible to avoid non-monotone behaviour under some assumptions on D? Realize- ability or well-specification could be good candidate-assumptions on D. In fact, this raises the issue to what extent well-specified statistical models can actually be proven to behave monotonically. For instance, is Example II monotone if the problem is well-specified?  All in all, we believe the question of monotonicity of learning offers various tantalizing ques-  tions to study, some of which may yet have to be formulated.  References  Shai Ben-David, Nathan Srebro, and Ruth Urner. Universal learning vs. no free lunch results. In  Philosophy and Machine Learning Workshop NIPS, 2011.  Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan. Minimizing the misclassifi-  cation error rate using a surrogate convex loss. In Proceedings ICML, pages 83-90, 2012.  Robert P W Duin. Small sample size generalization. In Proceedings of the Scandinavian Conference  on Image Analysis, volume 2, pages 957-964, 1995.  Wouter M Koolen, Peter Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and  stochastic fast rates in online learning. In NIPS, pages 4457-4465, 2016.  Jesse H Krijthe and Marco Loog. Projected estimators for robust semi-supervised classification.  Machine Learning, 106(7):993-1008, 2017.  Anqi Liu, Lev Reyzin, and Brian D Ziebart. Shift-pessimistic Active Learning Using Robust Bias-  aware Prediction. In Proceedings of AAAI-15, pages 2764-2770, 2015.  Marco Loog. Constrained parameter estimation for semi-supervised learning: the case of the nearest  mean classifier. In ECML PKDD 2010, pages 291-304. Springer, 2010.  Marco Loog and Robert P W Duin. The dipping phenomenon. In Proceedings of the IAPR S+SSPR,  pages 310-317. Springer, 2012.  Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural  networks III, pages 151-209. Springer, 1996.  Sarunas Raudys and Robert P W Duin. Expected classification error of the Fisher linear classifier  with pseudo-inverse covariance matrix. Pattern recognition letters, 19(5-6):385-392, 1998.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014. "}, "Open Problem: The Oracle Complexity of Convex Optimization with Limited Memory": {"volumn": "v99", "url": "http://proceedings.mlr.press/v99/", "header": "Open Problem: The Oracle Complexity of Convex Optimization with Limited Memory", "abstract": "We note that known methods achieving the optimal oracle complexity for first order convex optimization require quadratic memory, and ask whether this is necessary, and more broadly seek to characterize the minimax number of first order queries required to optimize a convex Lipschitz function subject to a memory constraint.", "pdf_url": "http://proceedings.mlr.press/v99/woodworth19a/woodworth19a.pdf", "keywords": [], "reference": "David S Atkinson and Pravin M Vaidya. A cutting plane algorithm for convex programming that  uses analytic centers. Mathematical Programming, 69(1-3):1-43, 1995.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 109-115. ACM, 2002.  S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Branko Gr\u00fcnbaum et al. Partitions of mass-distributions and of convex bodies by hyperplanes. Pacific  Journal of Mathematics, 10(4):1257-1261, 1960.  4   OPEN PROBLEM: THE ORACLE COMPLEXITY OF CONVEX OPTIMIZATION WITH LIMITED MEMORY  4. Challenges  Ultimately, we would like fully understand what is and is not possible:  Question 1 ($500 or Two Star Michelin Meal) Provide a complete characterization of T (d, M, (cid:15)) and the possible (T, M ) trade-off, preferably up to constant factors, and at most up to factors poly-logarithmic in T and M .  The most interesting scaling of d and (cid:15) is when the dimension is larger then poly-logarithmic but smaller then polynomial in 1/(cid:15), so that d log 1/(cid:15) memory is less then quadratic memory, but 1/(cid:15)2 query complexity is not polynomial in d.  Even without understanding the entire trade-off, it would be interesting to study what can be done on its boundary. Perhaps the most important regime is the case of linear memory M = \u0398 (cid:0)d log LB (cid:1). (cid:15) (cid:1) , 1/(cid:15)). In particular, is Therefore, as a starting point, we ask to characterize T (d, \u0398 (cid:0)d polylog LB (cid:15) it possible to have query complexity polynomial in d with \u02dcO(d) memory?  Question 2 ($200 or One Star Michelin Meal) Can we have T (d, M = \u02dcO(d), (cid:15)) = O(poly d) when d = \u2126(logc 1/(cid:15)) but d = O(1/(cid:15)c) for all c?  At the other extreme, we might ask whether quadratic memory is necessary in order to achieve optimal query complexity:  Question 3 ($200 or One Star Michelin Meal) Can we have T (d, M = O(d2\u2212\u03b4), (cid:15)) = \u02dcO(d polylog 1/(cid:15)), for \u03b4 > 0, when d = \u2126(logc 1/(cid:15)) but d = O(1/(cid:15)c) for all c?  The above represent specific incursions into the unknown square in Figure 2. Any other such incursion would also be interesting, and provide either for a memory lower bound, or a trade-off improving over Gradient Descent and Center of Mass in some regime.  Question 4 ($100 or Michelin Bib Gourmand Meal) Resolve the possibility or impossibility of some trade-off (T, M ) polynomially inside the unknown square in Figure 2.  References  David S Atkinson and Pravin M Vaidya. A cutting plane algorithm for convex programming that  uses analytic centers. Mathematical Programming, 69(1-3):1-43, 1995.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 109-115. ACM, 2002.  S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13)  in Machine Learning, 8(3-4):231-357, 2015.  Branko Gr\u00fcnbaum et al. Partitions of mass-distributions and of convex bodies by hyperplanes. Pacific  Journal of Mathematics, 10(4):1257-1261, 1960. OPEN PROBLEM: THE ORACLE COMPLEXITY OF CONVEX OPTIMIZATION WITH LIMITED MEMORY  Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method  efficiency in optimization. 1983.  Cybernetics, 6(2):102-108, 1970.  Naum Z Shor. Convergence rate of the gradient descent method with dilatation of the space.  Blake Woodworth and Nathan Srebro. Lower bound for randomized first order convex optimization.  arXiv preprint arXiv:1709.03594, 2017.  "}}