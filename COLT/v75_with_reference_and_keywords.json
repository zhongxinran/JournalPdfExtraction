{"Conference on Learning Theory 2018: Preface": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Conference on Learning Theory 2018: Preface", "abstract": "Preface to the proceedings of the 31st Conference On Learning Theory.", "pdf_url": "http://proceedings.mlr.press/v75/bubeck18a/bubeck18a.pdf", "keywords": []}, "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations", "abstract": "We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can recover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb R^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.\u201917 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.", "pdf_url": "http://proceedings.mlr.press/v75/li18a/li18a.pdf", "keywords": ["Generalization theory", "Implicit regularization", "Matrix factorization", "Neural networks"], "reference": "Rados\u0142aw Adamczak, Alexander Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Quantita- tive estimates of the convergence of the empirical covariance matrix in log-concave ensembles. Journal of the American Mathematical Society, 23(2):535-561, 2010.  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. Technical report, Technical Report Preprint, 2017.  S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global Optimality of Local Search for Low Rank  Matrix Recovery. ArXiv e-prints, May 2016.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.  Sgd learns over- parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.  Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.  Comptes Rendus Mathematique, 346(9-10):589-592, 2008.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Yuxin Chen, Yuejie Chi, and Andrea J Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Transactions on Information Theory, 61(7): 4034-4059, 2015.  Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Par- seval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854-863, 2017.  Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize  for deep nets. arXiv preprint arXiv:1703.04933, 2017.  Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.  R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June  2017.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in Neural Information Processing Systems (NIPS), 2016. URL http://arxiv. org/abs/1605.07272.  13   ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  References  Rados\u0142aw Adamczak, Alexander Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Quantita- tive estimates of the convergence of the empirical covariance matrix in log-concave ensembles. Journal of the American Mathematical Society, 23(2):535-561, 2010.  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. Technical report, Technical Report Preprint, 2017.  S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global Optimality of Local Search for Low Rank  Matrix Recovery. ArXiv e-prints, May 2016.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.  Sgd learns over- parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.  Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.  Comptes Rendus Mathematique, 346(9-10):589-592, 2008.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  Journal of the ACM (JACM), 58(3):11, 2011.  Yuxin Chen, Yuejie Chi, and Andrea J Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Transactions on Information Theory, 61(7): 4034-4059, 2015.  Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Par- seval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pages 854-863, 2017.  Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize  for deep nets. arXiv preprint arXiv:1703.04933, 2017.  Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.  R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June  2017.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in Neural Information Processing Systems (NIPS), 2016. URL http://arxiv. org/abs/1605.07272. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.  Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre- bro. Implicit regularization in matrix factorization. arXiv preprint arXiv:1705.09280, 2017.  Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In 5th International Conference on  Learning Representations (ICLR 2017), 2017.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.  CoRR, abs/1609.05191, 2016. URL http://arxiv.org/abs/1609.05191.  Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. 2017.  Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe- ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.  Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances  in neural information processing systems, pages 950-957, 1992.  Richard Kueng, Holger Rauhut, and Ulrich Terstiege. Low rank matrix recovery from rank one  measurements. Applied and Computational Harmonic Analysis, 42(1):88-116, 2017.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014.  Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk mini- mization. Advances in Computational Mathematics, 25(1):161-193, 2006.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On  the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac- bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.  Behnam Neyshabur, Srinadh Bhojanapalli, and Nati Srebro. Exploring generalization in deep learn-  ing. In Advances in Neural Information Processing Systems, pages 5943-5952, 2017b.  Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi  Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning  Research, 12:3413-3430, 2011. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635-2670, 2010.  M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape  of over-parameterized shallow neural networks. ArXiv e-prints, July 2017.  Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees  for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.  Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable  data. arXiv preprint arXiv:1710.10345, 2017.  Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In ICML, 2013.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference  on Computational Learning Theory, pages 545-560. Springer, 2005.  Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In  Advances in neural information processing systems, pages 2645-2653, 2011.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. arXiv preprint  The marginal value of adaptive gradient methods in machine learning. arXiv:1705.08292, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.  Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-  connected neural networks. In Artificial Intelligence and Statistics, pages 83-91, 2017.  Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using  burer-monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016.  Kai Zhong, Prateek Jain, and Inderjit S Dhillon. Efficient matrix sensing using rank-1 gaussian mea- surements. In International Conference on Algorithmic Learning Theory, pages 3-18. Springer, 2015. ALGORITHMIC REGULARIZATION IN OVER-PARAMETERIZED MODELS  "}, "Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure", "abstract": "Recently, research in unsupervised learning has gravitated towards exploring statistical-computational gaps induced by sparsity. A line of work initiated in Berthet and Rigollet (2013) has aimed to explain these gaps through reductions to conjecturally hard problems from complexity theory. However, the delicate nature of average-case reductions has limited the development of techniques and often led to weaker hardness results that only apply to algorithms robust to different noise distributions or that do not need to know the parameters of the problem. We introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture. Our new lower bounds include:  Our results demonstrate that, despite the delicate nature of average-case reductions, using natural problems as intermediates can often be beneficial, as is the case in worst-case complexity. Our main technical contribution is to introduce a set of techniques for average-case reductions that: (1) maintain the level of signal in an instance of a problem; (2) alter its planted structure; and (3) map two initial high-dimensional distributions simultaneously to two target distributions approximately under total variation. We also give algorithms matching our lower bounds and identify the information-theoretic limits of the models we consider.", "pdf_url": "http://proceedings.mlr.press/v75/brennan18a/brennan18a.pdf", "keywords": ["Planted clique", "statistical-computational gap", "average-case complexity"], "reference": "Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv  preprint arXiv:1703.10146, 2017.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a random  graph. Random Structures and Algorithms, 13(3-4):457-466, 1998.  Noga Alon, Alexandr Andoni, Tali Kaufman, Kevin Matulef, Ronitt Rubinfeld, and Ning Xie. Test- In Proceedings of the thirty-ninth annual ACM  ing k-wise and almost k-wise independence. symposium on Theory of computing, pages 496-505. ACM, 2007.  Noga Alon, Sanjeev Arora, Rajsekar Manokaran, Dana Moshkovitz, and Omri Weinstein. Inap- proximability of densest \u03ba-subgraph from average case hardness. Unpublished manuscript, 1, 2011.  Brendan PW Ames and Stephen A Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Arash A Amini and Martin J Wainwright. High-dimensional analysis of semidefinite relaxations for  sparse principal components. The Annals of Statistics, 37(5B):2877-2921, 2009.  Benny Applebaum, Boaz Barak, and Avi Wigderson. Public-key cryptography from different as- sumptions. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 171-180. ACM, 2010.  Ery Arias-Castro, Nicolas Verzelen, et al. Community detection in dense random networks. The  Annals of Statistics, 42(3):940-969, 2014.  Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Uni-  versity Press, 2009.  Sanjeev Arora, Boaz Barak, Markus Brunnermeier, and Rong Ge. Computational complexity and information asymmetry in financial products. Communications of the ACM, 54(5):101-107, 2011.  Bonacina Ilario De Rezende Susanna Lauria Massimo Nordstr\u02ddom Jakob Atserias, Albert and  Alexander Razborov. Clique is hard on average for regular resolution. 2018.  Per Austrin, Mark Braverman, and Eden Chlamt\u00b4ac. Inapproximability of np-complete variants of  nash equilibrium. Theory of Computing, 9(3):117-142, 2013.  Pranjal Awasthi, Moses Charikar, Kevin A Lai, and Andrej Risteski. Label optimal regret bounds  for online local learning. pages 150-166, 2015.  13   REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  References  Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv  preprint arXiv:1703.10146, 2017.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  Noga Alon, Michael Krivelevich, and Benny Sudakov. Finding a large hidden clique in a random  graph. Random Structures and Algorithms, 13(3-4):457-466, 1998.  Noga Alon, Alexandr Andoni, Tali Kaufman, Kevin Matulef, Ronitt Rubinfeld, and Ning Xie. Test- In Proceedings of the thirty-ninth annual ACM  ing k-wise and almost k-wise independence. symposium on Theory of computing, pages 496-505. ACM, 2007.  Noga Alon, Sanjeev Arora, Rajsekar Manokaran, Dana Moshkovitz, and Omri Weinstein. Inap- proximability of densest \u03ba-subgraph from average case hardness. Unpublished manuscript, 1, 2011.  Brendan PW Ames and Stephen A Vavasis. Nuclear norm minimization for the planted clique and  biclique problems. Mathematical programming, 129(1):69-89, 2011.  Arash A Amini and Martin J Wainwright. High-dimensional analysis of semidefinite relaxations for  sparse principal components. The Annals of Statistics, 37(5B):2877-2921, 2009.  Benny Applebaum, Boaz Barak, and Avi Wigderson. Public-key cryptography from different as- sumptions. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 171-180. ACM, 2010.  Ery Arias-Castro, Nicolas Verzelen, et al. Community detection in dense random networks. The  Annals of Statistics, 42(3):940-969, 2014.  Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Uni-  versity Press, 2009.  Sanjeev Arora, Boaz Barak, Markus Brunnermeier, and Rong Ge. Computational complexity and information asymmetry in financial products. Communications of the ACM, 54(5):101-107, 2011.  Bonacina Ilario De Rezende Susanna Lauria Massimo Nordstr\u02ddom Jakob Atserias, Albert and  Alexander Razborov. Clique is hard on average for regular resolution. 2018.  Per Austrin, Mark Braverman, and Eden Chlamt\u00b4ac. Inapproximability of np-complete variants of  nash equilibrium. Theory of Computing, 9(3):117-142, 2013.  Pranjal Awasthi, Moses Charikar, Kevin A Lai, and Andrej Risteski. Label optimal regret bounds  for online local learning. pages 150-166, 2015. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Statistical and computational tradeoffs in biclustering. In NIPS 2011 workshop on computational trade-offs in statistical learning, volume 4, 2011.  Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient robust  sparse estimation in high dimensions. pages 169-212, 2017.  Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua Teng. Finding endogenously formed communities. In Proceedings of the twenty-fourth annual ACM- SIAM symposium on Discrete algorithms, pages 767-783. Society for Industrial and Applied Mathematics, 2013.  Nicolai Baldin and Quentin Berthet. Optimal link prediction with matrix logistic regression. arXiv  preprint arXiv:1803.07054, 2018.  Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical  gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.  Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization. IEEE Transactions on Information Theory, 2018.  Boaz Barak. The Complexity of Public-Key Cryptography, pages 45-77. Springer International Publishing, Cham, 2017. ISBN 978-3-319-57048-8. doi: 10.1007/978-3-319-57048-8 2. URL https://doi.org/10.1007/978-3-319-57048-8_2.  Boaz Barak, Samuel B Hopkins, Jonathan Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. In Foun- dations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 428-437. IEEE, 2016.  Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013a.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013b.  Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for signaling in bayesian zero-sum and network routing games. In Proceedings of the 2016 ACM Conference on Economics and Computation, pages 479-496. ACM, 2016.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an o(n1/4) approximation for densest k-subgraph. Proceedings of the forty-second ACM symposium on Theory of computing, pages 201-210, 2010.  Vijay Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Sum-of-squares certificates for max- ima of random tensors on the sphere. In LIPIcs-Leibniz International Proceedings in Informatics, volume 81. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Aharon Birnbaum, Iain M Johnstone, Boaz Nadler, and Debashis Paul. Minimax bounds for sparse  pca with noisy high-dimensional data. Annals of statistics, 41(3):1055, 2013.  Andrej Bogdanov and Luca Trevisan. On worst-case to average-case reductions for np problems.  SIAM Journal on Computing, 36(4):1119-1159, 2006.  Andrej Bogdanov, Luca Trevisan, et al. Average-case complexity. Foundations and Trends R(cid:13) in  Theoretical Computer Science, 2(1):1-106, 2006.  Cristina Butucea and Yuri I Ingster. Detection of a sparse submatrix of a high-dimensional noisy  matrix. Bernoulli, 19(5B):2652-2688, 2013.  T. Tony Cai and Yihong Wu. Statistical and computational limits for sparse matrix detection. arXiv  preprint arXiv:1801.00518, 2018.  T Tony Cai, Zongming Ma, Yihong Wu, et al. Sparse pca: Optimal rates and adaptive estimation.  The Annals of Statistics, 41(6):3074-3110, 2013.  Tony Cai, Tengyuan Liang, and Alexander Rakhlin. Computational and statistical boundaries for  submatrix localization in a large noisy matrix. arXiv preprint arXiv:1502.01988, 2015a.  Tony Cai, Zongming Ma, and Yihong Wu. Optimal estimation and rank detection for sparse spiked  covariance matrices. Probability theory and related fields, 161(3-4):781-815, 2015b.  Utkan Onur Candogan and Venkat Chandrasekaran. Finding planted subgraphs with few eigenval-  ues using the schur-horn relaxation. SIAM Journal on Optimization, 28(1):735-759, 2018.  Mireille Capitaine, Catherine Donati-Martin, Delphine F\u00b4eral, et al. The largest eigenvalues of finite rank deformation of large wigner matrices: convergence and nonuniversality of the \ufb02uctuations. The Annals of Probability, 37(1):1-47, 2009.  Siu On Chan, Dimitris Papailliopoulos, and Aviad Rubinstein. On the approximability of sparse  pca. In Conference on Learning Theory, pages 623-646, 2016.  Venkat Chandrasekaran and Michael I Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the National Academy of Sciences, 110(13):E1181-E1190, 2013.  Moses Charikar, Yonatan Naamad, and Jimmy Wu. On finding dense common subgraphs. arXiv  preprint arXiv:1802.06361, 2018.  61(5):2909-2923, 2015.  Yudong Chen. Incoherence-optimal matrix completion. IEEE Transactions on Information Theory,  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. Journal of Machine Learning Research, 17(27):1-57, 2016.  Eden Chlamtac, Michael Dinitz, and Robert Krauthgamer. Everywhere-sparse spanners via dense subgraphs. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 758-767. IEEE, 2012. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Eden Chlamt\u00b4a\u02c7c, Michael Dinitz, and Yury Makarychev. Minimizing the union: Tight approxi- mations for small set bipartite vertex expansion. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 881-899. SIAM, 2017.  E. Chlamt\u00b4a\u02c7c and P. Manurangsi. Sherali-adams integrality gaps matching the log-density threshold.  arXiv preprint arXiv:1804.07842, 2018.  Amin Coja-Oghlan. Finding large independent sets in polynomial expected time. In Annual Sym-  posium on Theoretical Aspects of Computer Science, pages 511-522. Springer, 2003.  Amin Coja-Oghlan and Charilaos Efthymiou. On independent sets in random graphs. Random  Structures & Algorithms, 47(3):436-486, 2015.  Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnfs. pages  815-830, 2016.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. pages 441-448, 2014.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high  probability. Combinatorics, Probability and Computing, 23(1):29-49, 2014.  Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. In Advances in  Neural Information Processing Systems, pages 334-342, 2014.  Yash Deshpande and Andrea Montanari. Finding hidden cliques of size(cid:112)N/e in nearly linear time.  Foundations of Computational Mathematics, 15(4):1069-1128, 2015a.  Yash Deshpande and Andrea Montanari. Improved sum-of-squares lower bounds for hidden clique  and hidden submatrix problems. In COLT, pages 523-562, 2015b.  Persi Diaconis and David Freedman. A dozen de finetti-style results in search of a theory. 23(S2):  397-423, 1987.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart.  Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. arXiv preprint arXiv:1611.03473, 2016.  Shaddin Dughmi. On the hardness of signaling. In Foundations of Computer Science (FOCS), 2014  IEEE 55th Annual Symposium on, pages 354-363. IEEE, 2014.  Kord Eickmeyer, Kristoffer Arnsfelt Hansen, and Elad Verbin. Approximating the minmax value of 3-player games within a constant is as hard as detecting planted cliques. In Electronic Colloquium on Computational Complexity (ECCC), volume 19, page 25, 2012.  Uriel Feige. Relations between average case complexity and approximation complexity. In Pro- ceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 534-543. ACM, 2002.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Structures and Algorithms, 16(2):195-208, 2000. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Uriel Feige and Robert Krauthgamer. The probable value of the lov\u00b4asz-schrijver relaxations for  maximum independent set. SIAM Journal on Computing, 32(2):345-370, 2003.  Uriel Feige and Eran Ofek. Finding a maximum independent set in a sparse random graph.  In Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 282-293. Springer, 2005.  Uriel Feige and Dorit Ron. Finding hidden cliques in linear time. In 21st International Meeting on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms (AofA\u201910), pages 189-204. Discrete Mathematics and Theoretical Computer Science, 2010.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo-  rithms and a lower bound for planted clique. arXiv preprint arXiv:1201.1214, 2012.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for detecting planted cliques. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 655-664. ACM, 2013.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. pages 77-86, 2015.  Delphine F\u00b4eral and Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of rank one deformation of large wigner  matrices. Communications in mathematical physics, 272(1):185-228, 2007.  David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs.  In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 369- 376. ACM, 2014.  Chao Gao, Zongming Ma, and Harrison H Zhou. Sparse cca: Adaptive estimation and computa-  tional barriers. The Annals of Statistics, 45(5):2074-2101, 2017.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef-  inite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, 2016a.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Information limits for recovering a hidden community.  pages 1894-1898, 2016b.  Bruce E Hajek, Yihong Wu, and Jiaming Xu. Computational lower bounds for community detection  on random graphs. In COLT, pages 899-928, 2015.  Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for  matrix completion. In COLT, pages 703-725, 2014.  Elad Hazan and Robert Krauthgamer. How hard is it to approximate the best nash equilibrium?  SIAM Journal on Computing, 40(1):79-91, 2011.  Shuichi Hirahara and Rahul Santhanam. On the average-case complexity of mcsp and its variants. In LIPIcs-Leibniz International Proceedings in Informatics, volume 79. Schloss Dagstuhl-Leibniz- Zentrum fuer Informatik, 2017. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Samuel B Hopkins and David Steurer. Efficient bayesian estimation from few samples: community detection and related problems. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 379-390. IEEE, 2017.  Samuel B Hopkins, Pravesh Kothari, Aaron Henry Potechin, Prasad Raghavendra, and Tselil Schramm. On the integrality gap of degree-4 sum of squares for planted clique. pages 1079- 1095, 2016.  Samuel B Hopkins, Pravesh K Kothari, Aaron Potechin, Prasad Raghavendra, Tselil Schramm, and David Steurer. The power of sum-of-squares for detecting hidden structures. Proceedings of the fifty-eighth IEEE Foundations of Computer Science, pages 720-731, 2017.  Mark Jerrum. Large cliques elude the metropolis process. Random Structures & Algorithms, 3(4):  Iain M Johnstone and Arthur Yu Lu.  Sparse principal components analysis. Unpublished  347-359, 1992.  manuscript, 2004.  Ari Juels and Marcus Peinado. Hiding cliques for cryptographic security. Designs, Codes and  Cryptography, 20(3):269-280, 2000.  Pascal Koiran and Anastasios Zouzias. Hidden cliques and the certification of the restricted isometry  property. IEEE Transactions on Information Theory, 60(8):4999-5006, 2014.  Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, and Aarti Singh. Minimax localization of structural information in large noisy matrices. In Advances in Neural Information Processing Systems, pages 909-917, 2011.  Pravesh K Kothari, Ryuhei Mori, Ryan O\u2019Donnell, and David Witmer. Sum of squares lower bounds  for refuting any csp. arXiv preprint arXiv:1701.04521, 2017.  Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse pca  up to the information limit? The Annals of Statistics, 43(3):1300-1322, 2015.  Leonid A Levin. Average case complete problems. SIAM Journal on Computing, 15(1):285-286,  Jerry Li. Robust sparse estimation tasks in high dimensions. arXiv preprint arXiv:1702.05860,  1986.  2017.  Tengyu Ma and Avi Wigderson. Sum-of-squares lower bounds for sparse pca.  In Advances in  Neural Information Processing Systems, pages 1612-1620, 2015.  Zongming Ma. Sparse principal component analysis and iterative thresholding. The Annals of  Statistics, 41(2):772-801, 2013.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. The Annals  of Statistics, 43(3):1089-1116, 2015.  Pascal Massart. Concentration inequalities and model selection, volume 6. Springer, 2007. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Laurent Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. In Proceed- ings of the forty-sixth annual ACM symposium on Theory of computing, pages 694-703. ACM, 2014.  Frank McSherry. Spectral partitioning of random graphs.  In Foundations of Computer Science,  2001. Proceedings. 42nd IEEE Symposium on, pages 529-537. IEEE, 2001.  Lorenz Minder and Dan Vilenchik. Small clique detection and approximate nash equilibria. pages  Andrea Montanari. Finding one community in a sparse graph. Journal of Statistical Physics, 161  673-685, 2009.  (2):273-299, 2015.  Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral methods: From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors. In Ad- vances in Neural Information Processing Systems, pages 217-225, 2015.  Elchanan Mossel, Joe Neeman, and Allan Sly. Stochastic block models and reconstruction. arXiv  preprint arXiv:1202.1499, 2012.  Combinatorica, pages 1-44, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for binary symmetric block  models. arXiv preprint arXiv:1407.1591, 2014.  Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance  model. Statistica Sinica, pages 1617-1642, 2007.  Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of small rank perturbations of hermitian random matrices.  Probability Theory and Related Fields, 134(1):127-173, 2006.  Amelia Perry, Alexander S Wein, and Afonso S Bandeira. Statistical limits of spiked tensor models.  arXiv preprint arXiv:1612.07728, 2016a.  Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and arXiv preprint  sub-optimality of pca for spiked random matrices and synchronization. arXiv:1609.05573, 2016b.  Jim Pitman. Some probabilistic aspects of set partitions. The American mathematical monthly, 104  (3):201-209, 1997.  Prasad Raghavendra and Tselil Schramm. Tight lower bounds for planted clique in the degree-4 sos  program. arXiv preprint arXiv:1507.05136, 2015.  Mustazee Rahman, Balint Virag, et al. Local algorithms for independent sets are half-optimal. The  Annals of Probability, 45(3):1543-1577, 2017.  Andrey A Shabalin, Victor J Weigman, Charles M Perou, Andrew B Nobel, et al. Finding large average submatrices in high dimensional data. The Annals of Applied Statistics, 3(3):985-1012, 2009. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  Nihar B Shah, Sivaraman Balakrishnan, and Martin J Wainwright. Feeling the bern: Adaptive In Information Theory (ISIT),  estimators for bernoulli probabilities of pairwise comparisons. 2016 IEEE International Symposium on, pages 1153-1157. IEEE, 2016.  Dan Shen, Haipeng Shen, and James Stephen Marron. Consistency of sparse pca in high dimension,  low sample size contexts. Journal of Multivariate Analysis, 115:317-333, 2013.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Nicolas Verzelen, Ery Arias-Castro, et al. Community detection in sparse random networks. The  Annals of Applied Probability, 25(6):3465-3510, 2015.  Van H Vu. Spectral norm of random matrices. In Proceedings of the thirty-seventh annual ACM  symposium on Theory of computing, pages 423-430. ACM, 2005.  Vincent Q Vu and Jing Lei. Minimax rates of estimation for sparse pca in high dimensions. In  AISTATS, volume 15, pages 1278-1286, 2012.  Tengyao Wang, Quentin Berthet, and Yaniv Plan. Average-case hardness of rip certification. In  Advances in Neural Information Processing Systems, pages 3819-3827, 2016a.  Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896-1930, 2016b.  Bin Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer,  1997.  arXiv:1703.02724, 2017.  Anru Zhang and Dong Xia. Tensor svd: Statistical and computational limits. arXiv preprint  Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Lower bounds on the performance of  polynomial-time algorithms for sparse linear regression. In COLT, pages 921-948, 2014. REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  ContentsIntroduction  2 Problem Formulations  3 Summary of Results  4 Techniques  A Preliminaries  2.1 Detection and Recovery Problems . 2.2 Problems  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  .  .  .  .  .  A.1 Figure for the Main Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Hardness Results from an Algorithmic Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Prior Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Notation .  . .  . .  . .  . .  . .  B Average-Case Reductions under Total Variation  C Densifying Planted Clique and Planted Independent Set  C.1 Detecting Planted Generalized Diagonals C.2 Planted Clique Lifting .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  D Rejection Kernels and Distributional Lifting  D.1 Rejection Kernels . . D.2 Distributional Lifting .  .  . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E Planted Dense Subgraph and Biclustering  E.1 Poisson Lifting and Lower Bounds for Low-Density PDS . . . . . . . . . . . . . . E.2 Gaussian Lifting and Lower Bounds for High-Density PDS and BC . . . . . . . . E.3 Lower Bounds for General PDS . . . . . . . . . . . . . . . . . . . . . . . . . . .  F Re\ufb02ection Cloning and Subgraph Stochastic Block Model  F.1 Re\ufb02ecting Cloning and Rank-1 Submatrix . . . . . . . . . . . . . . . . . . . . . . F.2 Sparse Spiked Wigner Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Subgraph Stochastic Block Model  G Random Rotations and Sparse PCA  H Algorithms and Information-Theoretic Thresholds  87 H.1 Biclustering, Planted Dense Subgraph and Independent Set . . . . . . . . . . . . . 87 94 H.2 Rank-1 Submatrix, Sparse Spiked Wigner and Subgraph SBM . . . . . . . . . . . H.3 Sparse PCA and Biased Sparse PCA . . . . . . . . . . . . . . . . . . . . . . . . . 106  I Detection-Recovery Reductions  J Future Directions2  5 5 611  22 22 23 24 2831 31 32  36 36 44  50 50 53 62  65 65 71 75115 REDUCIBILITY AND COMPUTATIONAL LOWER BOUNDS  "}, "Logistic Regression: The Importance of Being Improper": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Logistic Regression: The Importance of Being Improper", "abstract": "Learning linear predictors with the logistic loss\u2014both in stochastic and online settings\u2014is a fundamental task in machine learning and statistics, with direct connections to classification and boosting. Existing \u201cfast rates\u201d for this setting exhibit exponential dependence on the predictor norm, and Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting with the simple observation that the logistic loss is $1$-mixable, we design a new efficient improper learning algorithm for online logistic regression that circumvents the aforementioned lower bound with a regret bound exhibiting a doubly-exponential improvement in dependence on the predictor norm. This provides a positive resolution to a variant of the COLT 2012 open problem of McMahan and Streeter (2012) when improper learning is allowed. This improvement is obtained both in the online setting and, with some extra work, in the batch statistical setting with high probability. We also show that the improved dependence on predictor norm is near-optimal.  Leveraging this improved dependency on the predictor norm yields the following applications: (a) we give algorithms for online bandit multiclass learning with the logistic loss with an $\\tilde{O}(\\sqrt{n})$ relative mistake bound across essentially all parameter ranges, thus providing a solution to the COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an adaptive algorithm for online multiclass boosting with optimal sample complexity, thus partially resolving an open problem of Beygelzimer et al. (2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on the optimal rates for improper logistic regression with general function classes, thereby characterizing the extent to which our improvement for linear classes extends to other parametric and even nonparametric settings.", "pdf_url": "http://proceedings.mlr.press/v75/foster18a/foster18a.pdf", "keywords": [], "reference": "Jacob D. Abernethy and Alexander Rakhlin. An Efficient Bandit Algorithm for  T Regret in Online Multiclass  \u221a  Prediction? In Conference on Learning Theory, 2009.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-414,  2010.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate o (1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic  regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural  language processing. Comp. Linguistics, 22(1), 1996.  Joseph Berkson. Application of the logistic function to bio-assay. Journal of the American Statistical  Association, 39:357-365, 1944.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 19-26, 2011.  Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online boosting. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2323-2331, 2015.  Alina Beygelzimer, Francesco Orabona, and Chicheng Zhang. Efficient Online Bandit Multiclass Learning T ) Regret. In Inernational Conference on Machine Learning, pages 488-497, 2017.  with \u02dcO(  \u221a  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected  langevin monte carlo. Advances in Neural Information Processing Systems, 2015.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Minimax regret under log loss for general classes of experts. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pages 12-18, New York, NY, USA, 1999. ACM. ISBN 1-58113-167-4.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application  to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Elad Hazan and Satyen Kale. Newtron: an efficient bandit algorithm for online multiclass prediction. In  Advances in Neural Information Processing Systems, pages 891-899, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69(2):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y Levy. Logistic regression: Tight bounds for stochastic and online  optimization. In Proceedings of The 27th Conference on Learning Theory, pages 197-209, 2014.  David P. Helmbold and Manfred K. Warmuth. On weak learning. J. Comput. Syst. Sci., 50(3):551-573, 1995.  13   LOGISTIC REGRESSION: THE IMPORTANCE OF BEING IMPROPER  References  Jacob D. Abernethy and Alexander Rakhlin. An Efficient Bandit Algorithm for  T Regret in Online Multiclass  \u221a  Prediction? In Conference on Learning Theory, 2009.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit  problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-414,  2010.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate o (1/n). In Advances in neural information processing systems, pages 773-781, 2013.  Francis R Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic  regression. Journal of Machine Learning Research, 15(1):595-627, 2014.  Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural  language processing. Comp. Linguistics, 22(1), 1996.  Joseph Berkson. Application of the logistic function to bio-assay. Journal of the American Statistical  Association, 39:357-365, 1944.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 19-26, 2011.  Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online boosting. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2323-2331, 2015.  Alina Beygelzimer, Francesco Orabona, and Chicheng Zhang. Efficient Online Bandit Multiclass Learning T ) Regret. In Inernational Conference on Machine Learning, pages 488-497, 2017.  with \u02dcO(  \u221a  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with projected  langevin monte carlo. Advances in Neural Information Processing Systems, 2015.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Minimax regret under log loss for general classes of experts. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pages 12-18, New York, NY, USA, 1999. ACM. ISBN 1-58113-167-4.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application  to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Elad Hazan and Satyen Kale. Newtron: an efficient bandit algorithm for online multiclass prediction. In  Advances in Neural Information Processing Systems, pages 891-899, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69(2):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y Levy. Logistic regression: Tight bounds for stochastic and online  optimization. In Proceedings of The 27th Conference on Learning Theory, pages 197-209, 2014.  David P. Helmbold and Manfred K. Warmuth. On weak learning. J. Comput. Syst. Sci., 50(3):551-573, 1995. LOGISTIC REGRESSION: THE IMPORTANCE OF BEING IMPROPER  Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Proceedings of  the 21st International Conference on Artificial Intelligence and Statistics, 2018.  Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting.  In Advances in Neural  Information Processing Systems, pages 920-929, 2017.  Sham M Kakade and Andrew Y Ng. Online bounds for bayesian algorithms. In Advances in neural information  processing systems, pages 641-648, 2005.  Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Efficient bandit algorithms for online multiclass prediction. In Proceedings of the 25th international conference on Machine learning, pages 440-447. ACM, 2008.  Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793-800, 2009.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Fast algorithms for logconcave functions: Sampling, rounding, integra- tion and optimization. In 47th Annual IEEE Symposium on Foundations of Computer Science, pages 57-68. IEEE, 2006.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms. Random  Structures & Algorithms, 30(3):307-358, 2007.  H Brendan McMahan and Matthew Streeter. Open problem: Better bounds for online logistic regression. In  Conference on Learning Theory, pages 44-1, 2012.  Nishant A Mehta. Fast rates with high probability in exp-concave statistical learning. International Conference  on Artificial Intelligence and Statistics, 2017.  Neri Merhav and Meir Feder. Universal prediction. IEEE Transactions on Information Theory, 44:2124-2147,  1998.  Hariharan Narayanan and Alexander Rakhlin. Efficient sampling from time-varying log-concave distributions.  Journal of Machine Learning Research, 18:112:1-112:29, 2017.  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation  approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. In Conference on Learning  Theory, 2014.  Alexander Rakhlin and Karthik Sridharan. Sequential probability assignment with binary alphabets and large  classes of experts. CoRR, abs/1501.07340, 2015a.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss functions. CoRR,  abs/1501.06598, 2015b. URL http://arxiv.org/abs/1501.06598.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complexities.  Journal of Machine Learning Research, 2015a. LOGISTIC REGRESSION: THE IMPORTANCE OF BEING IMPROPER  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale  laws of large numbers. Probability Theory and Related Fields, 161(1-2):111-153, 2015b.  Shai Shalev-Shwartz and Yoram Singer. Convex repeated games and fenchel duality. In Advances in neural  information processing systems, pages 1265-1272, 2007.  Tim Van Erven, Peter D Gr\u00a8unwald, Nishant A Mehta, Mark D Reid, and Robert C Williamson. Fast rates in  statistical and online learning. Journal of Machine Learning Research, 16:1793-1861, 2015.  Vladimir Vovk. A game of prediction with expert advice. In Proceedings of the eighth annual conference on  Computational learning theory, pages 51-60. ACM, 1995.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In International  Conference on Machine Learning, pages 928-936, 2003.  "}, "Actively Avoiding Nonsense in Generative Models": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Actively Avoiding Nonsense in Generative Models", "abstract": "A generative model may generate utter nonsense when it is fit to maximize the likelihood of observed data. This happens due to \u201cmodel error,\u201d i.e., when the true data generating distribution does not fit within the class of generative models being learned. To address this, we propose a model of active distribution learning using a binary invalidity oracle that identifies some examples as clearly invalid, together with random positive examples sampled from the true distribution. The goal is to maximize the likelihood of the positive examples subject to the constraint of (almost) never generating examples labeled invalid by the oracle. Guarantees are agnostic compared to a class of probability distributions. We first show that proper learning may require exponentially many queries to the invalidity oracle. We then give an improper distribution learning algorithm that uses only polynomially many queries.", "pdf_url": "http://proceedings.mlr.press/v75/hanneke18a/hanneke18a.pdf", "keywords": ["Generative models", "active learning", "statistical learning"], "reference": "Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions,  uniform convergence, and learnability. Journal of the ACM, 44(4):615-631, 1997.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Dana Angluin. Computational learning theory: Survey and selected bibliography. In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, STOC \u201992, pages 351-369, New York, NY, USA, 1992. ACM.  Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In Proceedings of the 6th International Conference on Learning Representations, ICLR \u201918, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  Vitaly Feldman. On the power of membership queries in agnostic learning. Journal of Machine  Learning Research, 10(Feb):163-182, 2009.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor- mation Processing Systems 27, NIPS \u201914, pages 2672-2680. Curran Associates, Inc., 2014.  Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends R(cid:13) in Ma-  chine Learning, 7(2-3):131-309, 2014.  David Haussler. Decision theoretic generalizations of the PAC model for neural net and other  learning applications. Information and Computation, 100(1):78-150, 1992.  Jeffrey C. Jackson. An efficient membership-query algorithm for learning DNF with respect to the  uniform distribution. Journal of Computer and System Sciences, 55(3):414-440, 1997.  David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner, and Jos\u00b4e Miguel Hern\u00b4andez- Lobato. Learning a generative model for validity in complex discrete structures. In Proceedings of the 6th International Conference on Learning Representations, ICLR \u201918, 2018.  Dan Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, 2009.  Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks. http://  karpathy.github.io/2015/05/21/rnn-effectiveness/, May 2015.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda In Proceedings of the 26th Annual ACM Sellie. On the learnability of discrete distributions. Symposium on the Theory of Computing, STOC \u201994, pages 273-282, New York, NY, USA, 1994a. ACM.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Towards efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994b.  13   ACTIVELY AVOIDING NONSENSE IN GENERATIVE MODELS  References  Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions,  uniform convergence, and learnability. Journal of the ACM, 44(4):615-631, 1997.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Dana Angluin. Computational learning theory: Survey and selected bibliography. In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, STOC \u201992, pages 351-369, New York, NY, USA, 1992. ACM.  Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In Proceedings of the 6th International Conference on Learning Representations, ICLR \u201918, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  Vitaly Feldman. On the power of membership queries in agnostic learning. Journal of Machine  Learning Research, 10(Feb):163-182, 2009.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor- mation Processing Systems 27, NIPS \u201914, pages 2672-2680. Curran Associates, Inc., 2014.  Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends R(cid:13) in Ma-  chine Learning, 7(2-3):131-309, 2014.  David Haussler. Decision theoretic generalizations of the PAC model for neural net and other  learning applications. Information and Computation, 100(1):78-150, 1992.  Jeffrey C. Jackson. An efficient membership-query algorithm for learning DNF with respect to the  uniform distribution. Journal of Computer and System Sciences, 55(3):414-440, 1997.  David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner, and Jos\u00b4e Miguel Hern\u00b4andez- Lobato. Learning a generative model for validity in complex discrete structures. In Proceedings of the 6th International Conference on Learning Representations, ICLR \u201918, 2018.  Dan Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, 2009.  Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks. http://  karpathy.github.io/2015/05/21/rnn-effectiveness/, May 2015.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda In Proceedings of the 26th Annual ACM Sellie. On the learnability of discrete distributions. Symposium on the Theory of Computing, STOC \u201994, pages 273-282, New York, NY, USA, 1994a. ACM.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Towards efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994b. ACTIVELY AVOIDING NONSENSE IN GENERATIVE MODELS  Matt J. Kusner, Brooks Paige, and Jos\u00b4e Miguel Hern\u00b4andez-Lobato. Grammar variational autoen- coder. In Proceedings of the 34th International Conference on Machine Learning, ICML \u201917, pages 1945-1954. JMLR, Inc., 2017.  Cyril Labb\u00b4e and Dominique Labb\u00b4e. Duplicate and fake publications in the scientific literature: How  many SCIgen papers in computer science? Scientometrics, 94(1):379-396, 2013.  Shahar Mendelson and Roman Vershynin. Entropy and the combinatorial dimension. Inventiones  Mathematicae, 152(1):37-55, 2003.  Ronald Rosenfeld. Two decades of statistical language modeling: Where do we go from here?  Proceedings of the IEEE, 88(8):1270-1278, 2000.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Vladimir Vapnik and Alexey Chervonenkis. Theory of Pattern Recognition. Nauka, 1974.  Ian H. Witten, Radford M. Neal, and John G. Cleary. Arithmetic coding for data compression.  Communications of the ACM, 30(6):520-540, 1987.  "}, "A Faster Approximation Algorithm for the Gibbs Partition Function": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "A Faster Approximation Algorithm for the Gibbs Partition Function", "abstract": "We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x \\exp(-\\beta H(x))$ of a Gibbs distribution with a Hamilton $H(\\cdot)$, or more precisely the logarithm of the ratio $q=\\ln Z(0)/Z(\\beta)$. It has been recently shown how to approximate $q$ with high probability assuming the existence of an oracle that produces samples from the Gibbs distribution for a given parameter value in $[0,\\beta]$. The current best known approach due to Huber (2015) uses $O(q\\ln n\\cdot[\\ln q + \\ln \\ln n+\\varepsilon^{-2}])$  oracle calls on average where $\\varepsilon$ is the desired accuracy of approximation and $H(\\cdot)$ is assumed to lie in $\\{0\\}\\cup[1,n]$. We improve the complexity to $O(q\\ln n\\cdot\\varepsilon^{-2})$ oracle calls. We also show that the same complexity can be achieved if exact oracles are replaced with approximate sampling oracles that are within $O(\\frac{\\varepsilon^2}{q\\ln n})$ variation distance from exact oracles. Finally, we prove a lower bound of $\\Omega(q\\cdot \\varepsilon^{-2})$ oracle calls under a natural model of computation.", "pdf_url": "http://proceedings.mlr.press/v75/kolmogorov18a/kolmogorov18a.pdf", "keywords": ["Gibbs distribution", "partition function", "approximation"], "reference": "I. Bez\u00b4akov\u00b4a, D. \u02c7Stefankovi\u02c7c, V. V. Vazirani, and E. Vigoda. Accelerating simulated annealing for the permanent and combinatorial counting problems. SIAM J. Comput., 37:1429-1454, 2008.  G. P. Bhattacharjee. Algorithm AS 32: The incomplete gamma integral. Journal of the Royal  Statistical Society. Series C (Applied Statistics), 19(3):285-287, 1970.  Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, editors. Handbook of Markov  chain Monte Carlo. Chapman & Hall/CRC, 2011.  M. Dyer and A. Frieze. Computing the volume of convex bodies: A case where randomness prov- ably helps. In Proceedings of AMS Symposium on Probabilistic Combinatorics and Its Applica- tions 44, pages 123-170, 1991.  J. A. Fill and M. L. Huber. Perfect simulation of Vervaat perpetuities. Electron. J. Probab., 15:  96-109, 2010.  G. S. Fishman. Choosing sample path length and number of sample paths when starting in the  steady state. Oper. Res. Lett., 16:209-219, 1994.  Heng Guo and Mark Jerrum. Random cluster dynamics for the Ising model is rapidly mixing. The  Annals of Applied Probability, 28(2):1292-1313, 2018.  Mark Huber. Perfect sampling using bounding chains. Annals of Applied Probability, 14(2):734-  753, 2004.  Mark Huber. Approximation algorithms for the normalizing constant of Gibbs distributions.  arXiv:1206.2689v1, June 2012.  Mark Huber. Approximation algorithms for the normalizing constant of Gibbs distributions. The  Annals of Applied Probability, 25(2):974-985, 2015.  Mark Huber and Sarah Schott. Using TPA for Bayesian inference. Bayesian Statistics 9, pages  257-282, 2010.  Mark Huber and Sarah Schott. Random construction of interpolating sets for high-dimensional  integration. arXiv:1112.3692, December 2011.  Mark Huber and Sarah Schott. Random construction of interpolating sets for high-dimensional  integration. J. Appl. Prob., 51:92-105, 2014.  M. Jerrum. A very simple algorithm for estimating the number of k-colourings of a low-degree  graph. Random Structures and Algorithms, 7:157-165, 1995.  M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM  J. Comput., 22:1087-1116, 1993.  1178, December 1989.  Mark Jerrum and Alistair Sinclair. Approximating the permanent. SIAM J. COMPUT., 18(6):1149-  12   A FASTER APPROXIMATION ALGORITHM FOR THE GIBBS PARTITION FUNCTION  References  I. Bez\u00b4akov\u00b4a, D. \u02c7Stefankovi\u02c7c, V. V. Vazirani, and E. Vigoda. Accelerating simulated annealing for the permanent and combinatorial counting problems. SIAM J. Comput., 37:1429-1454, 2008.  G. P. Bhattacharjee. Algorithm AS 32: The incomplete gamma integral. Journal of the Royal  Statistical Society. Series C (Applied Statistics), 19(3):285-287, 1970.  Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, editors. Handbook of Markov  chain Monte Carlo. Chapman & Hall/CRC, 2011.  M. Dyer and A. Frieze. Computing the volume of convex bodies: A case where randomness prov- ably helps. In Proceedings of AMS Symposium on Probabilistic Combinatorics and Its Applica- tions 44, pages 123-170, 1991.  J. A. Fill and M. L. Huber. Perfect simulation of Vervaat perpetuities. Electron. J. Probab., 15:  96-109, 2010.  G. S. Fishman. Choosing sample path length and number of sample paths when starting in the  steady state. Oper. Res. Lett., 16:209-219, 1994.  Heng Guo and Mark Jerrum. Random cluster dynamics for the Ising model is rapidly mixing. The  Annals of Applied Probability, 28(2):1292-1313, 2018.  Mark Huber. Perfect sampling using bounding chains. Annals of Applied Probability, 14(2):734-  753, 2004.  Mark Huber. Approximation algorithms for the normalizing constant of Gibbs distributions.  arXiv:1206.2689v1, June 2012.  Mark Huber. Approximation algorithms for the normalizing constant of Gibbs distributions. The  Annals of Applied Probability, 25(2):974-985, 2015.  Mark Huber and Sarah Schott. Using TPA for Bayesian inference. Bayesian Statistics 9, pages  257-282, 2010.  Mark Huber and Sarah Schott. Random construction of interpolating sets for high-dimensional  integration. arXiv:1112.3692, December 2011.  Mark Huber and Sarah Schott. Random construction of interpolating sets for high-dimensional  integration. J. Appl. Prob., 51:92-105, 2014.  M. Jerrum. A very simple algorithm for estimating the number of k-colourings of a low-degree  graph. Random Structures and Algorithms, 7:157-165, 1995.  M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM  J. Comput., 22:1087-1116, 1993.  1178, December 1989.  Mark Jerrum and Alistair Sinclair. Approximating the permanent. SIAM J. COMPUT., 18(6):1149- A FASTER APPROXIMATION ALGORITHM FOR THE GIBBS PARTITION FUNCTION  Mark R. Jerrum, Leslie G. Valiant, and Vijay V. Vazirani. Random generation of combinatorial  structures from a uniform distribution. Theoret. Comput. Sci., 43(2-3):169-188, 1986.  J. F. C. Kingman. Poisson Processes. Clarendon Press, 1992.  James Matthews. Markov Chains for Sampling Matchings. PhD thesis, University of Edinburgh,  School of Informatics, 2008.  N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state  calculation by fast computing machines. J. Chem. Phys., 21:1087-1092, 1953.  James G. Propp and David B. Wilson. Exact sampling with coupled Markov chains and applications  to statistical mechanics. Random Structures and Algorithms, 9(1-2):223-252, 1996.  Robert H. Swendsen and Jian-Sheng Wang. Replica Monte Carlo simulation of spin-glasses. Phys.  Rev. Lett., 57(21):2607-2609, 1986.  Chem. Phys., 57:5457-5462, 1972.  J. P. Valleau and D. N. Card. Monte Carlo estimation of the free energy by multistage sampling. J.  E. Vigoda. Improved bounds for sampling colorings. In FOCS, pages 51-59, 1999.  D. \u02c7Stefankovi\u02c7c, S. Vempala, and E. Vigoda. Adaptive simulated annealing: A near-optimal connec-  tion between sampling and counting. J. of the ACM, 56(3):1-36, 2009.  M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational infer-  ence. Foundations and Trends in Machine Learning, 1(1-2):1-305, December 2008. A FASTER APPROXIMATION ALGORITHM FOR THE GIBBS PARTITION FUNCTION  "}, "Exponential Convergence of Testing Error for Stochastic Gradient Methods": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Exponential Convergence of Testing Error for Stochastic Gradient Methods", "abstract": "We consider binary classification problems with positive definite kernels and square loss, and study the convergence rates of stochastic gradient methods. We show that while the excess testing \\emph{loss} (squared loss) converges slowly to zero as the number of observations (and thus iterations) goes to infinity, the testing \\emph{error} (classification error) converges exponentially fast if low-noise conditions are assumed. To achieve these rates of convergence we show sharper high-probability bounds with respect to the number of observations for stochastic gradient descent.", "pdf_url": "http://proceedings.mlr.press/v75/pillaud-vivien18a/pillaud-vivien18a.pdf", "keywords": ["SGD", "positive-definite kernels", "margin condition", "binary classification"], "reference": "Robert A. Adams and John J.F. Fournier. Sobolev spaces, volume 140. Academic Press, 2003.  Jean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classi\ufb01ers. The  Annals of statistics, 35(2):608\u2013633, 2007.  12   EXPONENTIAL CONVERGENCE OF TESTING ERROR FOR STOCHASTIC GRADIENT METHODS  Theorem 9 shows that with probability at least 1 \u2212 4 exp (cid:0)\u2212\u03b42KR(n + 1)(cid:1), the predictions of  \u00afgtail n are perfect. We can also make the following observations:  \u2022 The idea of the proof (see "}, "Size-Independent  Sample Complexity of Neural Networks": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Size-Independent  Sample Complexity of Neural Networks", "abstract": "We study the sample complexity of learning neural networks, by  providing new bounds on their Rademacher complexity assuming norm constraints  on the parameter matrix of each layer. Compared to previous work, these  complexity bounds have improved dependence on the network depth, and under some  additional assumptions, are fully independent of the network size (both depth  and width). These results are derived using some novel techniques, which may be  of independent interest.", "pdf_url": "http://proceedings.mlr.press/v75/golowich18a/golowich18a.pdf", "keywords": ["Neural Networks", "Deep Learning", "Sample Complexity", "Rademacher Complexity"], "reference": "university press, 2009.  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. arXiv preprint arXiv:1706.08498, 2017.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On  the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural  networks. In Conference on Learning Theory, pages 1376-1401, 2015.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac- bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.  3   SIZE-INDEPENDENT SAMPLE COMPLEXITY OF NEURAL NETWORKS  controlling the spectral norm alone (and indeed, any Schatten p-norm control with p > 2) cannot lead to bounds independent of the size of the network.  References  university press, 2009.  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge  Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural  networks. arXiv preprint arXiv:1706.08498, 2017.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On  the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural  networks. In Conference on Learning Theory, pages 1376-1401, 2015.  Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac- bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding  deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. "}, "Underdamped Langevin MCMC: A non-asymptotic analysis": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Underdamped Langevin MCMC: A non-asymptotic analysis", "abstract": "We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave. We present a MCMC algorithm based on its discretization and show that it achieves $\\varepsilon$ error (in 2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same smoothness/concavity assumptions. The underdamped Langevin MCMC scheme can be viewed as a version of Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped Langevin MCMC methods in a number of application areas. We provide quantitative rates that support this empirical wisdom.", "pdf_url": "http://proceedings.mlr.press/v75/cheng18a/cheng18a.pdf", "keywords": [], "reference": "arXiv:1602.04177, 2016.  We gratefully acknowledge the support of the NSF through grant IIS-1619362 and the Army Re- search Office under grant number W911NF-17-1-0304.  Fabrice Baudoin. Wasserstein contraction properties for hypoelliptic diffusions. arXiv preprint  Michael Betancourt, Simon Byrne, Sam Livingstone, and Mark Girolami. The geometric founda-  tions of Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257-2298, 2017.  Franc\u00b8ois Bolley, Arnaud Guillin, and Florent Malrieu. Trend to equilibrium and particle approxima- tion for a weakly self-consistent Vlasov-Fokker-Planck equation. ESAIM: Mathematical Mod- elling and Numerical Analysis, 44(5):867-884, 2010.  Roger Brockett. Oscillatory descent for function minimization. In Current and future directions in  applied mathematics, pages 65-82. Springer, 1997.  Simone Calogero. Exponential convergence to equilibrium for kinetic Fokker-Planck equations.  Communications in Partial Differential Equations, 37(8):1357-1390, 2012.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. Journal of the Royal Statistical Society: Series B, 79(3):651-676, 2017.  Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the Langevin Monte  Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095, 2017.  Jean Dolbeault, Cl\u00b4ement Mouhot, and Christian Schmeiser. Hypocoercivity for linear kinetic equa- tions conserving mass. Transactions of the American Mathematical Society, 367(6):3807-3828, 2015.  Sever S. Dragomir. Some Gronwall Type Inequalities and Applications. Nova Science Publishers,  2003.  Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with the Un-  adjusted Langevin Algorithm. arXiv preprint arXiv:1605.01559, 2016.  Andreas Eberle, Arnaud Guillin, and Raphael Zimmer. Couplings and quantitative contraction rates  for Langevin dynamics. arXiv preprint arXiv:1703.01617, 2017.  Saul B Gelfand and Sanjoy K Mitter. Recursive stochastic algorithms for global optimization in Rd.  SIAM Journal on Control and Optimization, 29(5):999-1018, 1991.  Jack Gorham, Andrew B. Duncan, Sebastian J. Vollmer, and Lester Mackey. Measuring sample  quality with diffusions. arXiv preprint arXiv:1611.06972, 2016.  13   UNDERDAMPED LANGEVIN MCMC: A NON-ASYMPTOTIC ANALYSIS  ACKNOWLEDGMENTS  References  arXiv:1602.04177, 2016.  We gratefully acknowledge the support of the NSF through grant IIS-1619362 and the Army Re- search Office under grant number W911NF-17-1-0304.  Fabrice Baudoin. Wasserstein contraction properties for hypoelliptic diffusions. arXiv preprint  Michael Betancourt, Simon Byrne, Sam Livingstone, and Mark Girolami. The geometric founda-  tions of Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257-2298, 2017.  Franc\u00b8ois Bolley, Arnaud Guillin, and Florent Malrieu. Trend to equilibrium and particle approxima- tion for a weakly self-consistent Vlasov-Fokker-Planck equation. ESAIM: Mathematical Mod- elling and Numerical Analysis, 44(5):867-884, 2010.  Roger Brockett. Oscillatory descent for function minimization. In Current and future directions in  applied mathematics, pages 65-82. Springer, 1997.  Simone Calogero. Exponential convergence to equilibrium for kinetic Fokker-Planck equations.  Communications in Partial Differential Equations, 37(8):1357-1390, 2012.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. Journal of the Royal Statistical Society: Series B, 79(3):651-676, 2017.  Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the Langevin Monte  Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095, 2017.  Jean Dolbeault, Cl\u00b4ement Mouhot, and Christian Schmeiser. Hypocoercivity for linear kinetic equa- tions conserving mass. Transactions of the American Mathematical Society, 367(6):3807-3828, 2015.  Sever S. Dragomir. Some Gronwall Type Inequalities and Applications. Nova Science Publishers,  2003.  Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with the Un-  adjusted Langevin Algorithm. arXiv preprint arXiv:1605.01559, 2016.  Andreas Eberle, Arnaud Guillin, and Raphael Zimmer. Couplings and quantitative contraction rates  for Langevin dynamics. arXiv preprint arXiv:1703.01617, 2017.  Saul B Gelfand and Sanjoy K Mitter. Recursive stochastic algorithms for global optimization in Rd.  SIAM Journal on Control and Optimization, 29(5):999-1018, 1991.  Jack Gorham, Andrew B. Duncan, Sebastian J. Vollmer, and Lester Mackey. Measuring sample  quality with diffusions. arXiv preprint arXiv:1611.06972, 2016. UNDERDAMPED LANGEVIN MCMC: A NON-ASYMPTOTIC ANALYSIS  Thomas H. Gronwall. Note on the derivatives with respect to a parameter of the solutions of a  system of differential equations. Annals of Mathematics, 20:292-296, 1919.  Fr\u00b4ed\u00b4eric H\u00b4erau. Isotropic hypoellipticity and trend to the equilibrium for the Fokker-Planck equation  with high degree potential. pages 1-13, 2002.  Hendrik A. Kramers. Brownian motion in a field of force and the diffusion model of chemical  reactions. Physica, 7(4):284-304, 1940.  Walid Krichene, Alexandre Bayen, and Peter Bartlett. Accelerated mirror descent in continuous and discrete time. In Advances in Neural Information Processing Systems, pages 2845-2853, 2015.  Yin Tat Lee and Santosh S Vempala. Convergence Rate of Riemannian Hamiltonian Monte Carlo  and Faster Polytope Volume Computation. arXiv preprint arXiv:1710.06261, 2017.  Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. In  Advances in Neural Information Processing Systems, pages 2917-2925, 2015.  Oren Mangoubi and Aaron Smith. Rapid Mixing of Hamiltonian Monte Carlo on Strongly Log-  Concave Distributions. arXiv preprint arXiv:1708.07114, 2017.  St\u00b4ephane Mischler and Cl\u00b4ement Mouhot. Exponential stability of slowly decaying solutions to the  kinetic Fokker-Planck equation. arXiv preprint arXiv:1412.7487, 2014.  Peter M\u00a8orters and Yuval Peres. Brownian Motion. Cambridge University Press, 2010.  Radford M. Neal. MCMC using Hamiltonian dynamics.  In Handbook of Markov Chain Monte  Carlo. CRC Press, 2011.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate  O(1=k2). Soviet Mathematics Doklady, 27(2):372-376, 1983.  Grigorios A. Pavliotis. Stochastic Processes and Applications. Springer, 2016.  Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-  putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.  Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and  their discrete approximations. Bernoulli, pages 341-363, 1996.  John R Silvester. Determinants of block matrices. The Mathematical Gazette, 84(501):460-467,  2000.  Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510-2518, 2014.  C\u00b4edric Villani. Optimal Transport: Old and New. Springer Science and Business Media, 2008. UNDERDAMPED LANGEVIN MCMC: A NON-ASYMPTOTIC ANALYSIS  Cedric Villani. Hypocoercivity. American Mathematical Society, 2009.  Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351- E7358, 2016. doi: 10.1073/pnas.1614734113. URL http://www.pnas.org/content/ 113/47/E7351.abstract. UNDERDAMPED LANGEVIN MCMC: A NON-ASYMPTOTIC ANALYSIS  "}, "Online Variance Reduction for Stochastic Optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Online Variance Reduction for Stochastic Optimization", "abstract": "Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by  yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform \\emph{importance sampling} techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling data points, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.", "pdf_url": "http://proceedings.mlr.press/v75/borsos18a/borsos18a.pdf", "keywords": ["importance sampling", "variance reduction", "bandit feedback", "empirical risk minimization"], "reference": "Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo-  rithm for bandit linear optimization. In COLT, pages 263-274, 2008.  Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.  Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00b4arik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110-1119, 2016.  David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed- ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial and Applied Mathematics, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Leon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. In Advances  in neural information processing systems, pages 585-592, 1995.  Guillaume Bouchard, Th\u00b4eo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.  arXiv preprint arXiv:1506.09016, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.  Dominik Csiba and Peter Richt\u00b4arik.  Importance sampling for minibatches.  arXiv preprint  arXiv:1602.02283, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.  Matthew Faulkner, Michael Olson, Rishi Chandy, Jonathan Krause, K Mani Chandy, and Andreas Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors. In Information Processing in Sensor Networks (IPSN), 2011 10th International Confer- ence on, pages 13-24. IEEE, 2011.  13   ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  References  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo-  rithm for bandit linear optimization. In COLT, pages 263-274, 2008.  Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.  Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00b4arik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110-1119, 2016.  David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceed- ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial and Applied Mathematics, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Leon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. In Advances  in neural information processing systems, pages 585-592, 1995.  Guillaume Bouchard, Th\u00b4eo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.  arXiv preprint arXiv:1506.09016, 2015.  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.  Dominik Csiba and Peter Richt\u00b4arik.  Importance sampling for minibatches.  arXiv preprint  arXiv:1602.02283, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010.  Matthew Faulkner, Michael Olson, Rishi Chandy, Jonathan Krause, K Mani Chandy, and Andreas Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors. In Information Processing in Sensor Networks (IPSN), 2011 10th International Confer- ence on, pages 13-24. IEEE, 2011. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  David A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100-118,  1975.  Elad Hazan. A survey: The convex optimization approach to regret minimization. Optimization for  machine learning, pages 287-302, 2011.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance  reduction. In Advances in neural information processing systems, pages 315-323, 2013.  Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex pro- gramming algorithms. In Advances in Neural Information Processing Systems, pages 801-808, 2009.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  KDD Cup 2004. KDD Cup 2004. Protein Homology Dataset. http://osmot.cs.cornell.  edu/kddcup/, 2004. Accessed: 10.11.2016.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-  mization. COLT 2010, page 244, 2010.  Hongseok Namkoong, Aman Sinha, Steve Yadlowsky, and John C. Duchi. Adaptive sampling probabilities for non-smooth optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2574- 2583, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.  I Necoara, Y Nesterov, and F Glineur. A random coordinate descent method on large optimization  problems with linear constraints. 2011.  Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems, pages 1017-1025, 2014.  Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM  Journal on Optimization, 22(2):341-362, 2012.  Dmytro Perekrestenko, Volkan Cevher, and Martin Jaggi. Faster coordinate descent via adaptive im- portance sampling. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54. PMLR, 2017.  F. Salehi, L. E. Celis, and P. Thiran. Stochastic Optimization with Bandit Sampling. ArXiv e-prints,  August 2017.  Farnood Salehi, Patrick Thiran, and L Elisa Celis. Stochastic dual coordinate descent with bandit  sampling. arXiv preprint arXiv:1712.03010, 2017. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  David Sculley. Web-scale k-means clustering. In Proceedings of the 19th international conference  on World wide web, pages 1177-1178. ACM, 2010.  Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and  Trends R(cid:13) in Machine Learning, 4(2):107-194, 2012.  Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 761-769, 2016.  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image  recognition. ICLR, 2015.  Sebastian U Stich, Anant Raj, and Martin Jaggi. Safe adaptive importance sampling. In Advances in Neural Information Processing Systems 30, pages 4384-4394. Curran Associates, Inc., 2017.  Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1-9, 2015. ONLINE VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION  "}, "Information Directed Sampling and Bandits with Heteroscedastic Noise": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Information Directed Sampling and Bandits with Heteroscedastic Noise", "abstract": "In the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy evaluations. Typically, the observation noise is assumed to be independent of the evaluation point and to satisfy a tail bound uniformly on the domain; a restrictive assumption for many applications. In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret analysis framework, that is similar to the Bayesian framework of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize the regret-information ratio over all possible action sampling distributions. This further relies on concentration inequalities for online least squares regression in separable Hilbert spaces, which we generalize to the case of heteroscedastic noise. We then formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case recover known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting with heteroscedastic noise, that some of our methods can outperform UCB and Thompson Sampling, while staying competitive when the noise is homoscedastic.", "pdf_url": "http://proceedings.mlr.press/v75/kirschner18a/kirschner18a.pdf", "keywords": [], "reference": "2012.  Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis,  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved Algorithms for Linear Stochas- tic Bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320. 2011.  Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In AISTATS 2017-  20th International Conference on Artificial Intelligence and Statistics, 2017.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In International Conference on Machine Learning, pages 127-135, 2013.  Alexander C Aitken. IV. on least squares and linear combination of observations. Proceedings of  the Royal Society of Edinburgh, 55:42-48, 1936.  Andr\u00b4as Antos, Varun Grover, and Csaba Szepesv\u00b4ari. Active learning in heteroscedastic noise. The-  oretical Computer Science, 411(29):2712-2728, 2010.  John-Alexander M Assael, Ziyu Wang, Bobak Shahriari, and Nando de Freitas. Heteroscedastic  treed bayesian optimisation. arXiv preprint arXiv:1410.7172, 2014.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  Rajendra Bhatia and Chandler Davis. A better bound on the variance. The American Mathematical  Monthly, 107(4):353-357, 2000.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for sequential allocation  problems. Advances in Applied Mathematics, 17(2):122-142, 1996.  Kamalika Chaudhuri, Prateek Jain, and Nagarajan Natarajan. Active Heteroscedastic Regression. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Pro- ceedings of Machine Learning Research, pages 694-702. PMLR, August 2017.  Sayak Ray Chowdhury and Aditya Gopalan. On Kernelized Multi-armed Bandits. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 844-853. PMLR, August 2017.  Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.  Wesley Cowan, Junya Honda, and Michael N. Katehakis. Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem. arXiv:1504.05823 [cs, stat], April 2015.  13   INFORMATION DIRECTED SAMPLING AND BANDITS WITH HETEROSCEDASTIC NOISE  References  2012.  Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis,  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved Algorithms for Linear Stochas- tic Bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320. 2011.  Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In AISTATS 2017-  20th International Conference on Artificial Intelligence and Statistics, 2017.  Shipra Agrawal and Navin Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs.  In International Conference on Machine Learning, pages 127-135, 2013.  Alexander C Aitken. IV. on least squares and linear combination of observations. Proceedings of  the Royal Society of Edinburgh, 55:42-48, 1936.  Andr\u00b4as Antos, Varun Grover, and Csaba Szepesv\u00b4ari. Active learning in heteroscedastic noise. The-  oretical Computer Science, 411(29):2712-2728, 2010.  John-Alexander M Assael, Ziyu Wang, Bobak Shahriari, and Nando de Freitas. Heteroscedastic  treed bayesian optimisation. arXiv preprint arXiv:1410.7172, 2014.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  Rajendra Bhatia and Chandler Davis. A better bound on the variance. The American Mathematical  Monthly, 107(4):353-357, 2000.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for sequential allocation  problems. Advances in Applied Mathematics, 17(2):122-142, 1996.  Kamalika Chaudhuri, Prateek Jain, and Nagarajan Natarajan. Active Heteroscedastic Regression. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Pro- ceedings of Machine Learning Research, pages 694-702. PMLR, August 2017.  Sayak Ray Chowdhury and Aditya Gopalan. On Kernelized Multi-armed Bandits. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 844-853. PMLR, August 2017.  Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.  Wesley Cowan, Junya Honda, and Michael N. Katehakis. Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem. arXiv:1504.05823 [cs, stat], April 2015. INFORMATION DIRECTED SAMPLING AND BANDITS WITH HETEROSCEDASTIC NOISE  Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic Linear Optimization under Bandit  Feedback. In COLT, pages 355-366. Omnipress, 2008.  Rick Durrett. Probability: Theory and Examples. Cambridge university press, 2010.  Xiequan Fan, Ion Grama, and Quansheng Liu. Exponential inequalities for martingales with appli-  cations. Electronic Journal of Probability, 20, 2015.  David A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100-118,  1975.  Paul Goldberg, Christopher K. I. Williams, and Christopher M. Bishop. Regression with Input-  Dependent Noise: A Gaussian Process Treatment. 1998.  Sham M Kakade and Ambuj Tewari. On the Generalization Ability of Online Strongly Convex In Advances in Neural Information Processing Systems 21, pages  Programming Algorithms. 801-808. 2009.  Kristian Kersting, Christian Plagemann, Patrick Pfaff, and Wolfram Burgard. Most Likely Het- eroscedastic Gaussian Process Regression. In Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 393-400. ACM, 2007.  Tor Lattimore. A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis. In Advances  in Neural Information Processing Systems 30, pages 1583-1592. 2017.  Tor Lattimore and Csaba Szepesvari. The End of Optimism? An Asymptotic Analysis of Finite-  Armed Linear Bandits. In Artificial Intelligence and Statistics, pages 728-737, 2017.  Tor Lattimore, Koby Crammer, and Csaba Szepesvari. Linear Multi-Resource Allocation with Semi-Bandit Feedback. In Advances in Neural Information Processing Systems 28, pages 964- 972. Curran Associates, Inc., 2015.  Annie Marsden and Sergio Bacallado. Sequential Matrix Completion. arXiv:1710.08045 [cs, stat],  October 2017.  volume 1. 2006.  Y. Prokhorov. Convergence of Random Processes and Limit Theorems in Probability Theory. The-  ory of Probability & Its Applications, 1(2):157-214, January 1956.  Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning,  Daniel Russo and Benjamin Van Roy. Learning to Optimize via Information-Directed Sampling. In  Advances in Neural Information Processing Systems 27, pages 1583-1591. 2014.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.  The Journal of Machine Learning Research, 17(1):2442-2471, 2016.  Niranjan Srinivas, Andreas Krause, Matthias Seeger, and Sham M Kakade. Gaussian Process Opti- mization in the Bandit Setting: No Regret and Experimental Design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015-1022, 2010. INFORMATION DIRECTED SAMPLING AND BANDITS WITH HETEROSCEDASTIC NOISE  William R. Thompson. On the Likelihood that One Unknown Probability Exceeds Another in View  of the Evidence of Two Samples. Biometrika, 25(3/4):285-294, 1933.  Zi Wang and Stefanie Jegelka. Max-value Entropy Search for Efficient Bayesian Optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed- ings of Machine Learning Research, pages 3627-3635. PMLR, August 2017.  "}, "Testing Symmetric Markov Chains From a Single Trajectory": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Testing Symmetric Markov Chains From a Single Trajectory", "abstract": "The paper\u2019s abstract in valid LaTeX, without non-standard macros or \\cite commands. Classical distribution testing assumes access to i.i.d.\u00a0samples from the distribution that is being tested. We initiate the study of Markov chain  testing, assuming access to a {\\em single trajectory of a Markov Chain.} In particular, we observe a single trajectory $X_0,\\ldots,X_t,\\ldots$ of an unknown, symmetric, and finite state Markov Chain $\\cal M$. We do not control the starting state $X_0$, and we cannot restart the chain. Given our single trajectory, the goal is to test whether  $\\cal M$ is identical to a model Markov Chain ${\\cal M}\u2019$, or far from it under an appropriate notion of difference. We propose a measure of difference between two Markov chains, motivated by the early work of Kazakos [78],  which captures the scaling behavior of the total variation distance between trajectories sampled from the Markov chains as the length of these trajectories grows. We provide efficient testers and information-theoretic lower bounds for testing identity of symmetric Markov chains under our proposed measure of difference, which are tight up to logarithmic factors if the hitting times of the model chain ${\\cal M}\u2019$ is $\\tilde{O}(n)$ in  the size of the state space $n$.", "pdf_url": "http://proceedings.mlr.press/v75/daskalakis18a/daskalakis18a.pdf", "keywords": [], "reference": "Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for prop- In Advances in Neural Information Processing Systems 28: Annual erties of distributions. Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 3591-3599, 2015. URL http://papers.nips.cc/paper/ 5839-optimal-testing-for-properties-of-distributions.  Alan Agresti. Categorical Data Analysis. Wiley, 2012.  Flavia Barsotti, Anne Philippe, and Paul Rochet. Hypothesis testing for Markovian models with  random time observations. Journal of Statistical Planning and Inference, 173:87-98, 2016.  Maurice S Bartlett. The frequency goodness of fit test for probability chains.  In Mathematical Proceedings of the Cambridge Philosophical Society, volume 47, pages 86-95. Cambridge Univ Press, 1951.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. In Proceedings of the 42nd Annual Testing random variables for independence and identity. IEEE Symposium on Foundations of Computer Science, FOCS \u201901, pages 442-451, Washington, DC, USA, 2001. IEEE Computer Society.  14   TESTING SYMMETRIC MARKOV CHAINS FROM A SINGLE TRAJECTORY  From the construction above it is clear that one needs to observe a number of collisions to distinguish Q from a randomly chosen member of P. The proof proceeds by a careful analysis of these collision probabilities to bound the TV distance between words of length k from Q and from a randomly chosen \u00afP \u2208 P.  6. Open Questions  In this paper, we proposed a new framework for studying property testing questions on Markov chains. There seem to be multiple avenues for future research and abundant number of open prob- lems arising from this framework. We first list some questions which may be of interest here.  1. What is the optimal sample complexity for identity testing on symmetric Markov chains? In this paper, we show an upper bound of (cid:101)O (cid:0)HitTQ \u00b7 log (HitTQ) + n (cid:1) samples (Theorem 9). We conjecture that \u0398 (cid:0) n (cid:1) (same as our lower bound) is the right sample complexity for this \u03b5 problem and an explicit dependence on the hitting time of chain Q may not be necessary. It is implicitly captured to an extent by the guarantee we get from the parameter \u03b5.  \u03b5  2. As there is a natural operation of taking a convex combination of Markov chains, it is natural to ask how our spectral definition of distance 1 \u2212 \u03c1 between two symmetric chains changes if we substitute either P or Q with a convex combination of P and Q. How does the distance now relate to the original value?  [P, Q]\u221a  (cid:17)  (cid:16)  3. Given \u03b52 \u2265 \u03b51, and access to words from each of two chains, can we distinguish whether the two chains are \u2264 \u03b51-close or \u2265 \u03b52-far? This problem, known as two-sample testing in literature, is another interesting direction using our framework.  References  Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for prop- In Advances in Neural Information Processing Systems 28: Annual erties of distributions. Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 3591-3599, 2015. URL http://papers.nips.cc/paper/ 5839-optimal-testing-for-properties-of-distributions.  Alan Agresti. Categorical Data Analysis. Wiley, 2012.  Flavia Barsotti, Anne Philippe, and Paul Rochet. Hypothesis testing for Markovian models with  random time observations. Journal of Statistical Planning and Inference, 173:87-98, 2016.  Maurice S Bartlett. The frequency goodness of fit test for probability chains.  In Mathematical Proceedings of the Cambridge Philosophical Society, volume 47, pages 86-95. Cambridge Univ Press, 1951.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. In Proceedings of the 42nd Annual Testing random variables for independence and identity. IEEE Symposium on Foundations of Computer Science, FOCS \u201901, pages 442-451, Washington, DC, USA, 2001. IEEE Computer Society. TESTING SYMMETRIC MARKOV CHAINS FROM A SINGLE TRAJECTORY  Tu\u02d8gkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing close-  ness of discrete distributions. Journal of the ACM (JACM), 60(1):4, 2013.  Cl\u00b4ement L Canonne. A survey on distribution testing: Your data is big. but is it blue? In Electronic  Colloquium on Computational Complexity (ECCC), volume 22, pages 1-9, 2015.  Cl\u00b4ement L. Canonne, Dana Ron, and Rocco A. Servedio. Testing equivalence between distributions using conditional samples. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 1174- 1192, 2014. doi: 10.1137/1.9781611973402.87. URL http://dx.doi.org/10.1137/1. 9781611973402.87.  Cl\u00b4ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt Rubinfeld. Testing shape restrictions of discrete distributions. In Proceedings of the 33rd Symposium on Theoretical As- pects of Computer Science, STACS \u201916, pages 25:1-25:14, 2016.  Siu-On Chan, Ilias Diakonikolas, Gregory Valiant, and Paul Valiant. Optimal algorithms for testing closeness of discrete distributions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Sympo- sium on Discrete Algorithms, pages 1193-1203. Society for Industrial and Applied Mathematics, 2014.  Constantinos Daskalakis and Qinxuan Pan. Square Hellinger subadditivity for Bayesian networks and its applications to identity testing. In Proceedings of the 30th Conference on Learning Theory (COLT), 2017.  Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, and Paul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of the Twenty- Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013, pages 1833-1852, 2013. doi: 10.1137/1.9781611973105. 131. URL http://dx.doi.org/10.1137/1.9781611973105.131.  Constantinos Daskalakis, Gautam Kamath, and John Wright. Which distribution distances are sub-  linearly testable? arXiv preprint arXiv:1708.00002, 2017.  Ilias Diakonikolas and Daniel M. Kane. A new approach for testing properties of discrete distribu- tions. In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201916, pages 685-694, Washington, DC, USA, 2016. IEEE Computer Society.  Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Robust learning of fixed-structure bayesian  networks. arXiv preprint arXiv:1606.07384, 2016.  Ronald A. Fisher. The Design of Experiments. Macmillan, 1935.  Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. Interna-  tional statistical review, 70(3):419-435, 2002.  Leon J Gleser, David S Moore, et al. The effect of dependence on chi-squared and empiric distribu-  tion tests of fit. The Annals of Statistics, 11(4):1100-1108, 1983.  Oded Goldreich. A brief introduction to property testing., 2011. TESTING SYMMETRIC MARKOV CHAINS FROM A SINGLE TRAJECTORY  Daniel J Hsu, Aryeh Kontorovich, and Csaba Szepesv\u00b4ari. Mixing time estimation in reversible Markov chains from a single sample path. In Advances in neural information processing systems, pages 1459-1467, 2015.  Dimitri Kazakos. The Bhattacharyya distance and detection between Markov chains. IEEE Trans. Information Theory, 24(6):747-754, 1978. URL http://dx.doi.org/10.1109/TIT. 1978.1055967.  Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distributions.  Theory of Computing, 9(8):295-347, 2013.  I Molina, D Morales, L Pardo, and I Vajda. On size increase for goodness of fit tests when observa-  tions are positively dependent. Statistics & Risk Modeling, 20(1-4):399-414, 2002.  David S Moore et al. The effect of dependence on chi squared tests of fit. The Annals of Statistics,  10(4):1163-1171, 1982.  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data.  IEEE Transactions on Information Theory, 54(10):4750-4755, 2008.  Karl Pearson. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine Series 5, 50(302):157-175, 1900.  Jon N.K. Rao and Alastair J. Scott. The analysis of categorical data from complex sample surveys: Chi-squared tests for goodness of fit and independence in two-way tables. Journal of the Americal Statistical Association, 76(374):221-230, 1981.  Ronitt Rubinfeld. Taming big probability distributions. XRDS: Crossroads, The ACM Magazine for  Students, 19(1):24-28, 2012.  Vincent Y.F. Tan, Animashree Anandkumar, and Alan S. Willsky. Error exponents for composite hypothesis testing of Markov forest distributions. In Proceedings of the 2010 IEEE International Symposium on Information Theory, ISIT \u201910, pages 1613-1617, Washington, DC, USA, 2010. IEEE Computer Society.  Simon Tavare and Patricia M. E. Altham. Serial dependence of observations leading to contin- gency tables, and corrections to chi-squared statistics. Biometrika, 70(1):139-144, 1983. ISSN 00063444. URL http://www.jstor.org/stable/2335951.  Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity test- ing. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201914, pages 51-60, Washington, DC, USA, 2014. IEEE Computer Society. TESTING SYMMETRIC MARKOV CHAINS FROM A SINGLE TRAJECTORY  "}, "Detection limits in the high-dimensional spiked rectangular model": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Detection limits in the high-dimensional spiked rectangular model", "abstract": "We study the problem of detecting the presence of a single unknown spike in a rectangular data matrix, in a high-dimensional regime where the spike has fixed strength and the aspect ratio of the matrix converges to a finite limit. This setup includes Johnstone\u2019s spiked covariance model. We analyze the likelihood ratio of the spiked model against an \u201call noise\" null model of reference, and show it has asymptotically Gaussian fluctuations in a region below\u2014but in general not up to\u2014the so-called BBP threshold from random matrix theory. Our result parallels earlier findings of Onatski et al. (2013) and Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic approach capable of treating generic product priors. In particular, sparsity in the spike is allowed. Our approach operates through the principle of the cavity method from spin-glass theory.  The question of the maximal parameter region where asymptotic normality is expected to hold is left open. This region, not necessarily given by BBP, is shaped by the prior in a non-trivial way. We conjecture that this is the entire paramagnetic phase of an associated spin-glass model, and is defined by the vanishing of the replica-symmetric solution of Lesieur et al. (2015).", "pdf_url": "http://proceedings.mlr.press/v75/el-alaoui18a/el-alaoui18a.pdf", "keywords": ["Spiked random matrix models", "hypothesis testing", "likelihood ratio \ufb02uctuations", "spin glasses", "replica symmetry", "the cavity method"], "reference": "Michael Aizenman, Joel L Lebowitz, and David Ruelle. Some rigorous results on the Sherrington- Kirkpatrick spin glass model. Communications in Mathematical Physics, 112(1):3-20, 1987.  Arash A. Amini and Martin J. Wainwright. High-dimensional analysis of semidefinite relaxations  for sparse principal components. Annals of Statistics, 37(5B):2877-2921, 10 2009.  Antonio Auffinger and Wei-Kuo Chen. Free energy and complexity of spherical bipartite models.  Journal of Statistical Physics, 157(1):40-59, 2014.  Zhidong Bai and Jian-feng Yao. Central limit theorems for eigenvalues in a spiked population model. Annales de l\u2019Institut Henri Poincar\u00b4e, Probabilit\u00b4es et Statistiques, 44(3):447-474, 2008.  Zhidong Bai and Jianfeng Yao. On sample eigenvalues in a generalized spiked population model.  Journal of Multivariate Analysis, 106:167-177, 2012.  Jinho Baik and Ji Oon Lee. Fluctuations of the free energy of the spherical Sherrington-Kirkpatrick  model. Journal of Statistical Physics, 165(2):185-224, 2016.  Jinho Baik and Ji Oon Lee. Fluctuations of the free energy of the spherical Sherrington-Kirkpatrick model with ferromagnetic interaction. In Annales Henri Poincar\u00b4e, volume 18, pages 1867-1917. Springer, 2017a.  Jinho Baik and Ji Oon Lee. Free energy of bipartite spherical Sherrington-Kirkpatrick model. arXiv  preprint arXiv:1711.06364, 2017b.  Jinho Baik and Jack W Silverstein. Eigenvalues of large sample covariance matrices of spiked  population models. Journal of Multivariate Analysis, 97(6):1382-1408, 2006.  Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. Annals of Probability, 33(5):1643-1697, 2005.  Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization. In IEEE International Symposium on Information Theory (ISIT), pages 1137-1141. IEEE, 2017.  Jean Barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, and Lenka Zde- borov\u00b4a. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. In Advances in Neural Information Processing Systems (NIPS), pages 424-432, 2016.  Jean Barbier, Florent Krzakala, Nicolas Macris, L\u00b4eo Miolane, and Lenka Zdeborov\u00b4a. Phase tran- sitions, optimal errors and optimality of message-passing in generalized linear models. arXiv preprint arXiv:1708.03395, 2017.  Adriano Barra, Giuseppe Genovese, and Francesco Guerra. Equilibrium statistical mechanics of bipartite spin systems. Journal of Physics A: Mathematical and Theoretical, 44(24):245002, 2011.  13   DETECTION LIMITS IN SPIKED MODELS  References  Michael Aizenman, Joel L Lebowitz, and David Ruelle. Some rigorous results on the Sherrington- Kirkpatrick spin glass model. Communications in Mathematical Physics, 112(1):3-20, 1987.  Arash A. Amini and Martin J. Wainwright. High-dimensional analysis of semidefinite relaxations  for sparse principal components. Annals of Statistics, 37(5B):2877-2921, 10 2009.  Antonio Auffinger and Wei-Kuo Chen. Free energy and complexity of spherical bipartite models.  Journal of Statistical Physics, 157(1):40-59, 2014.  Zhidong Bai and Jian-feng Yao. Central limit theorems for eigenvalues in a spiked population model. Annales de l\u2019Institut Henri Poincar\u00b4e, Probabilit\u00b4es et Statistiques, 44(3):447-474, 2008.  Zhidong Bai and Jianfeng Yao. On sample eigenvalues in a generalized spiked population model.  Journal of Multivariate Analysis, 106:167-177, 2012.  Jinho Baik and Ji Oon Lee. Fluctuations of the free energy of the spherical Sherrington-Kirkpatrick  model. Journal of Statistical Physics, 165(2):185-224, 2016.  Jinho Baik and Ji Oon Lee. Fluctuations of the free energy of the spherical Sherrington-Kirkpatrick model with ferromagnetic interaction. In Annales Henri Poincar\u00b4e, volume 18, pages 1867-1917. Springer, 2017a.  Jinho Baik and Ji Oon Lee. Free energy of bipartite spherical Sherrington-Kirkpatrick model. arXiv  preprint arXiv:1711.06364, 2017b.  Jinho Baik and Jack W Silverstein. Eigenvalues of large sample covariance matrices of spiked  population models. Journal of Multivariate Analysis, 97(6):1382-1408, 2006.  Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. Annals of Probability, 33(5):1643-1697, 2005.  Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization. In IEEE International Symposium on Information Theory (ISIT), pages 1137-1141. IEEE, 2017.  Jean Barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, and Lenka Zde- borov\u00b4a. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. In Advances in Neural Information Processing Systems (NIPS), pages 424-432, 2016.  Jean Barbier, Florent Krzakala, Nicolas Macris, L\u00b4eo Miolane, and Lenka Zdeborov\u00b4a. Phase tran- sitions, optimal errors and optimality of message-passing in generalized linear models. arXiv preprint arXiv:1708.03395, 2017.  Adriano Barra, Giuseppe Genovese, and Francesco Guerra. Equilibrium statistical mechanics of bipartite spin systems. Journal of Physics A: Mathematical and Theoretical, 44(24):245002, 2011. DETECTION LIMITS IN SPIKED MODELS  Adriano Barra, Andrea Galluzzi, Francesco Guerra, Andrea Pizzoferrato, and Daniele Tantari. Mean field bipartite spin models treated with mechanical techniques. The European Physical Journal B, 87(3):74, 2014.  Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011.  Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120- 135, 2012.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. Annals of Statistics, 41(4):1780-1815, 2013.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. Oxford University Press, 2013.  Mireille Capitaine, Catherine Donati-Martin, and Delphine F\u00b4eral. The largest eigenvalues of finite rank deformation of large Wigner matrices: convergence and nonuniversality of the \ufb02uctuations. Annals of Probability, pages 1-47, 2009.  Sourav Chatterjee. Superconcentration and Related Topics. Springer, 2014.  Yash Deshpande, Emmanuel Abb\u00b4e, and Andrea Montanari. Asymptotic mutual information for the binary stochastic block model. In IEEE International Symposium on Information Theory (ISIT), pages 185-189. IEEE, 2016.  Edgar Dobriban. Sharp detection in PCA under correlations: all eigenvalues matter. Annals of  Statistics, 45(4):1810-1833, 2017.  Ahmed El Alaoui, Florent Krzakala, and Michael I Jordan. Finite size corrections and likelihood  ratio \ufb02uctuations in the spiked Wigner model. arXiv preprint arXiv:1710.02903, 2017.  Delphine F\u00b4eral and Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of rank one deformation of large Wigner  matrices. Communications in Mathematical Physics, 272(1):185-228, 2007.  Francesco Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communi-  cations in Mathematical Physics, 233(1):1-12, 2003.  Francesco Guerra and Fabio Lucio Toninelli. Quadratic replica coupling in the Sherrington- Kirkpatrick mean field spin glass model. Journal of Mathematical Physics, 43(7):3704-3716, 2002.  Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis.  Annals of Statistics, pages 295-327, 2001.  Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486):682-693, 2009.  Iain M Johnstone and Alexei Onatski. Testing in high-dimensional spiked models. arXiv preprint  arXiv:1509.07269, 2015. DETECTION LIMITS IN SPIKED MODELS  Satish Babu Korada and Nicolas Macris. Exact solution of the gauge symmetric p-spin glass model  on a complete graph. Journal of Statistical Physics, 136(2):205-230, 2009.  Florent Krzakala, Jiaming Xu, and Lenka Zdeborov\u00b4a. Mutual information in rank-one matrix esti-  mation. In Information Theory Workshop (ITW), pages 71-75. IEEE, 2016.  Olivier Ledoit and Michael Wolf. Some hypothesis tests for the covariance matrix when the dimen-  sion is large compared to the sample size. Annals of Statistics, pages 1081-1102, 2002.  Marc Lelarge and L\u00b4eo Miolane. Fundamental limits of symmetric low-rank matrix estimation. In Proceedings of the 30th Conference on Learning Theory, volume 65, pages 1297-1301. PMLR. arXiv preprint:1611.03888, 2017.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. MMSE of probabilistic low-rank matrix In Communication, Control, and estimation: Universality with respect to the output channel. Computing (Allerton), 2015 53rd Annual Allerton Conference on, pages 680-687. IEEE, 2015a.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. Phase transitions in sparse PCA.  In  IEEE International Symposium on Information Theory (ISIT), pages 1635-1639. IEEE, 2015b.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. Constrained low-rank matrix estima- tion: Phase transitions, approximate message passing and applications. Journal of Statistical Mechanics: Theory and Experiment, 2017(7), 2017.  L\u00b4eo Miolane. Fundamental limits of low-rank matrix estimation. arXiv preprint arXiv:1702.00473,  2017.  Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral methods: From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors. In Ad- vances in Neural Information Processing Systems, pages 217-225, 2015.  Boaz Nadler. Finite sample approximation results for principal component analysis: A matrix  perturbation approach. Annals of Statistics, pages 2791-2817, 2008.  Hidetoshi Nishimori. Statistical physics of spin glasses and information processing: an introduc-  tion, volume 111. Clarendon Press, 2001.  Alexei Onatski, Marcelo J Moreira, and Marc Hallin. Asymptotic power of sphericity tests for  high-dimensional data. Annals of Statistics, 41(3):1204-1231, 2013.  Alexei Onatski, Marcelo J Moreira, and Marc Hallin. Signal detection in high dimension: The  multispiked case. Annals of Statistics, 42(1):225-254, 2014.  Dmitry Panchenko. The free energy in a multi-species Sherrington-Kirkpatrick model. Annals of  Probability, 43(6):3494-3513, 2015.  Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance  model. Statistica Sinica, pages 1617-1642, 2007.  Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of small rank perturbations of Hermitian random matrices.  Probability Theory and Related Fields, 134(1):127-173, 2006. DETECTION LIMITS IN SPIKED MODELS  Sandrine P\u00b4ech\u00b4e. Deformed ensembles of random matrices.  In Proceedings of the International  Congress of Mathematicians, Seoul, volume III, pages 1059-1174. ICM, 2014.  Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. On the optimality and sub-optimality of PCA for spiked random matrix models. Annals of Statistics (to appear). arXiv preprint:1609.05573, 2016.  Gilles Pisier. Probabilistic methods in the geometry of Banach spaces, pages 167-241. Springer,  Berlin, Heidelberg, 1986.  pages 63-80. Springer, 2007.  Michel Talagrand. Mean field models for spin glasses: some obnoxious problems. In Spin Glasses,  Michel Talagrand. Mean field models for spin glasses. Volume I: Basic examples, volume 54.  Springer Science & Business Media, 2011a.  Michel Talagrand. Mean field models for spin glasses. Volume II: Advanced replica-symmetry and  low temperature, volume 55. Springer Science & Business Media, 2011b.  Aad W Van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.  "}, "Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification", "abstract": "We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson\u2019s small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that \\emph{more unstable} linear systems are \\emph{easier} to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.", "pdf_url": "http://proceedings.mlr.press/v75/simchowitz18a/simchowitz18a.pdf", "keywords": ["Linear dynamical systems", "autoregressive processes", "time series", "system identification", "empirical process theory"], "reference": "Independence. 2013.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Marco C Campi and Erik Weyer. Finite Sample Properties of System Identification Methods. IEEE  Transactions on Automatic Control, 47(8), 2002.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite Time Identi-  fication in Unstable Linear Systems. arXiv:1710.01852, 2017a.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite Time Analy-  sis of Optimal Adaptive Policies for Linear-Quadratic Systems. arXiv:1711.07230, 2017b.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient Descent Learns Linear Dynamical Sys-  tems. arXiv:1609.05191, 2016.  Elad Hazan, Karan Singh, and Cyril Zhang. Learning Linear Dynamical Systems via Spectral  Filtering. In Neural Information Processing Systems, 2017.  Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral Filtering for General  Linear Dynamical Systems. 2018.  Daniel Hsu, Sham M. Kakade, and Tong Zhang. Random Design Analysis of Ridge Regression.  Foundations of Computational Mathematics, 14, 2014.  Anthony W Knapp. Representation Theory of Semisimple Groups: An Overview Based on Examples  (PMS-36). Princeton University Press, 2016.  Vitaly Kuznetsov and Mehryar Mohri. Generalization Bounds for Non-Stationary Mixing Pro-  cesses. Machine Learning, 106(1), 2017.  Daniel J. McDonald, Cosma R. Shalizi, and Mark Schervish. Nonparametric Risk Bounds for  Time-Series Forecasting. Journal of Machine Learning Research, 18, 2017.  Shahar Mendelson. Learning without Concentration. In Conference on Learning Theory, 2014.  Mehryar Mohri and Afshin Rostamizadeh. Stability Bounds for Non-i.i.d. Processes. In Neural  Information Processing Systems, 2007a.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher Complexity Bounds for Non-I.I.D. Pro-  cesses. In Neural Information Processing Systems, 2007b.  Anders Rantzer. Concentration Bounds for Single Parameter Adaptive Control. In American Control  Conference, 2018.  Parikshit Shah, Badri Narayan Bhaskar, Gongguo Tang, and Benjamin Recht. Linear System Iden-  tification via Atomic Norm Regularization. In Conference on Decision and Control, 2012.  13   LEARNING WITHOUT MIXING  References  Independence. 2013.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Marco C Campi and Erik Weyer. Finite Sample Properties of System Identification Methods. IEEE  Transactions on Automatic Control, 47(8), 2002.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the Sample Com-  plexity of the Linear Quadratic Regulator. arXiv:1710.01688, 2017.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite Time Identi-  fication in Unstable Linear Systems. arXiv:1710.01852, 2017a.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite Time Analy-  sis of Optimal Adaptive Policies for Linear-Quadratic Systems. arXiv:1711.07230, 2017b.  Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient Descent Learns Linear Dynamical Sys-  tems. arXiv:1609.05191, 2016.  Elad Hazan, Karan Singh, and Cyril Zhang. Learning Linear Dynamical Systems via Spectral  Filtering. In Neural Information Processing Systems, 2017.  Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral Filtering for General  Linear Dynamical Systems. 2018.  Daniel Hsu, Sham M. Kakade, and Tong Zhang. Random Design Analysis of Ridge Regression.  Foundations of Computational Mathematics, 14, 2014.  Anthony W Knapp. Representation Theory of Semisimple Groups: An Overview Based on Examples  (PMS-36). Princeton University Press, 2016.  Vitaly Kuznetsov and Mehryar Mohri. Generalization Bounds for Non-Stationary Mixing Pro-  cesses. Machine Learning, 106(1), 2017.  Daniel J. McDonald, Cosma R. Shalizi, and Mark Schervish. Nonparametric Risk Bounds for  Time-Series Forecasting. Journal of Machine Learning Research, 18, 2017.  Shahar Mendelson. Learning without Concentration. In Conference on Learning Theory, 2014.  Mehryar Mohri and Afshin Rostamizadeh. Stability Bounds for Non-i.i.d. Processes. In Neural  Information Processing Systems, 2007a.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher Complexity Bounds for Non-I.I.D. Pro-  cesses. In Neural Information Processing Systems, 2007b.  Anders Rantzer. Concentration Bounds for Single Parameter Adaptive Control. In American Control  Conference, 2018.  Parikshit Shah, Badri Narayan Bhaskar, Gongguo Tang, and Benjamin Recht. Linear System Iden-  tification via Atomic Norm Regularization. In Conference on Decision and Control, 2012. LEARNING WITHOUT MIXING  Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-Asymptotic Analysis of  Robust Control from Coarse-Grained Identification. arXiv:1707.04791, 2017.  Roman Vershynin.  Introduction to the Non-Asymptotic Analysis of Random Matrices.  arXiv:1011.3027, 2011.  Mathukumalli Vidyasagar and Rajeeva L Karandikar. A learning theory approach to system identi-  fication and stochastic adaptive control. Journal of Process Control, 18(3), 2008.  John S White. The limiting distribution of the serial correlation coefficient in the explosive case.  The Annals of Mathematical Statistics, pages 1188-1197, 1958.  Bin Yu. Rates of Convergence for Empirical Processes of Stationary Mixing Sequences. The Annals  of Probability, 22(1), 1994. LEARNING WITHOUT MIXING  "}, "Active Tolerant Testing": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Active Tolerant Testing", "abstract": "In this work, we show that for a nontrivial hypothesis class $\\mathcal C$, we can estimate the distance of a target function $f$ to $\\mathcal C$ (estimate the error rate of the best $h\\in \\mathcal C$) using substantially fewer labeled examples than would be needed to actually {\\em learn} a good $h \\in \\mathcal C$.   Specifically, we show that for the class $\\mathcal C$ of unions of $d$ intervals on the line, in the active learning setting in which we have access to a pool of unlabeled examples drawn from an arbitrary underlying distribution $\\mathcal D$, we can estimate the error rate of the best $h \\in \\mathcal C$ to an additive error $\\epsilon$ with a number of label requests that is {\\em independent of $d$} and depends only on $\\epsilon$.  In particular, we make $O(\\frac{1}{\\epsilon^6}\\log \\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size $O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$.  This task of estimating the distance of an unknown $f$ to a given class $\\mathcal C$  is called {\\em tolerant testing} or {\\em distance estimation} in the testing literature, usually studied in a membership query model and with respect to the uniform distribution.  Our work extends that of Balcan et al. (2012) who solved the {\\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\\epsilon$).   We also consider the related problem of estimating the performance of a given learning algorithm $\\mathcal A$ in this setting.  That is, given a large pool of unlabeled examples drawn from distribution $\\mathcal D$, can we, from only a few label queries, estimate how well $\\mathcal A$ would perform if the entire dataset were labeled and given as training data to $\\mathcal A$?   We focus on $k$-Nearest Neighbor style algorithms, and also show how our results can be applied to the problem of hyperparameter tuning (selecting the best value of $k$ for the given learning problem).", "pdf_url": "http://proceedings.mlr.press/v75/blum18a/blum18a.pdf", "keywords": ["property testing", "agnostic learning", "algorithm estimation"], "reference": "Maria-Florina Balcan, Eric Blais, Avrim Blum, and Liu Yang. Active property testing.  In 53rd  Annual Symposium on Foundations of Computer Science (FOCS), pages 21\u201330. IEEE, 2012.  12   ACTIVE TOLERANT TESTING  Theorem 9 Suppose we consider the pth-power loss for p \u2208 N\u2217. There is an estimator E soft using O( p (cid:15)2 ) unlabeled examples when the unlabeled pool S has size N . The underlying distribution D is assumed unknown to the estimator. Moreover, the estimator has success probability at least 2  (cid:15)2 ) queries on N + O( 1  3 for any unlabeled pool S.  D (factive, (cid:15), S, k)  The proof of Theorem 9 is in "}, "Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods", "abstract": "The problem of Non-Gaussian Component Analysis (NGCA) is about finding a maximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points projected onto $E$ follow a non-Gaussian distribution. Vempala and Xiao (2011) proposed a local search algorithm, and showed that it was able to estimate $E$ accurately with polynomial time and sample complexity, if the dimension of $E$ is treated as a constant and with the assumption that all one-dimensional marginals of the non-Gaussian distribution over $E$ have non-Gaussian moments. In this paper, we propose a simple spectral algorithm called \\textsc{Reweighted PCA}, and prove that it possesses the same guarantee. The principle that underlies this approach is a new characterization of multivariate Gaussian distributions.", "pdf_url": "http://proceedings.mlr.press/v75/tan18a/tan18a.pdf", "keywords": [], "reference": "Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders. Algorithmica, 72 (1):215-236, 2015.  Patrick Billingsley. Probability and Measure - Third Edition. 1995.  Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, Vladimir Spokoiny, and Klaus-Robert M\u00a8uller. In Search of Non-Gaussian Components of a High-Dimensional Distribution. Journal of Machine Learning Research, 7(2):247-282, 2006.  S. Charles Brubaker and Santosh S. Vempala. Isotropic PCA and affine-invariant clustering. Pro- ceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS, pages 551-560, 2008.  E. C\u00b8 \u0131nlar. Probability and Stochastics. Graduate Texts in Mathematics. Springer New York, 2011.  Pierre Comon. Independent component analysis, A new concept? Signal Processing, 36(3):287-  314, 1994.  C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM Journal on  Numerical Analysis, 7(1):1-46, 1970.  Elmar Diederichs, Anatoli Juditsky, Vladimir Spokoiny, and Christof Sch\u00a8utte. Sparse non-Gaussian  component analysis. IEEE Transactions on Information Theory, 56(6):3033-3047, 2010.  13   POLYNOMIAL COMPLEXITY FOR NON-GAUSSIAN COMPONENT ANALYSIS  Second, just as it is extremely unlikely for r to be higher than 4, for a general non-Gaussian \u02dcX and a small, random \u03b1, it is extremely unlikely for any of the non-Gaussian values of \u03a6X,\u03b1 to be equal to the Gaussian one on the dot. This means that even though the guarantee for a single run of the base algorithm is for one direction, in practice we most probably can recover the entire subspace E simultaneously with just \u02c6\u03a6X,\u03b1 alone (as in the case in Corollary 10), albeit with a more sophisticated truncation technique.  6.1. Conjectures and questions  We conjecture that REWEIGHTED PCA actually recovers the entire non-Gaussian subspace E with in polynomial time and sample complexity if we fix m, but now allow d to vary. This would improve upon both our result and that of Vempala and Xiao (2011). The first Gaussian test for a random vector X using the distribution of its norm and dot product pairing also leads to further questions. For a fixed nonzero real number t, both of these appear in the formula for (cid:107)Yt(cid:107)2 2, where we set Yt := X + tX(cid:48), so it is natural to ask whether Reweighted PCA works with \u03a6Yt,\u03b1 alone for some t. In particular, does it work for t = \u22121? It is also an open question whether (cid:104)X, X(cid:48)(cid:105) alone is sufficient to test whether X is standard Gaussian.  Both authors are partially supported by NSF Grant DMS 1265782 and U.S. Air Force Grant FA9550- 14-1-0009.  Acknowledgements  References  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders. Algorithmica, 72 (1):215-236, 2015.  Patrick Billingsley. Probability and Measure - Third Edition. 1995.  Gilles Blanchard, Motoaki Kawanabe, Masashi Sugiyama, Vladimir Spokoiny, and Klaus-Robert M\u00a8uller. In Search of Non-Gaussian Components of a High-Dimensional Distribution. Journal of Machine Learning Research, 7(2):247-282, 2006.  S. Charles Brubaker and Santosh S. Vempala. Isotropic PCA and affine-invariant clustering. Pro- ceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS, pages 551-560, 2008.  E. C\u00b8 \u0131nlar. Probability and Stochastics. Graduate Texts in Mathematics. Springer New York, 2011.  Pierre Comon. Independent component analysis, A new concept? Signal Processing, 36(3):287-  314, 1994.  C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM Journal on  Numerical Analysis, 7(1):1-46, 1970.  Elmar Diederichs, Anatoli Juditsky, Vladimir Spokoiny, and Christof Sch\u00a8utte. Sparse non-Gaussian  component analysis. IEEE Transactions on Information Theory, 56(6):3033-3047, 2010. POLYNOMIAL COMPLEXITY FOR NON-GAUSSIAN COMPONENT ANALYSIS  Elmar Diederichs, Anatoli Juditsky, Arkadi Nemirovski, and Vladimir Spokoiny. Sparse non Gaus- sian component analysis by semidefinite programming. Machine Learning, 91(2):211-238, 2013.  A. Frieze, M. Jerrum, and Ravi Kannan. Learning linear transformations. Proceedings of 37th  Conference on Foundations of Computer Science, pages 359-368, 1996.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA and robust tensor decomposition. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing - STOC \u201914, number c, pages 584-593, New York, New York, USA, 2014. ACM Press.  Peter J Huber. Projection Pursuit. The Annals of Statistics, 13(2):435-475, 2007.  Aapo Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE  Transactions on Neural Networks, 10(3):626-634, may 1999.  Motoaki Kawanabe and Fabian J. Theis. Estimating non-Gaussian subspaces by characteristic func- tions. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelli- gence and Lecture Notes in Bioinformatics), 3889 LNCS:157-164, 2006.  Motoaki Kawanabe and Fabian J. Theis. Joint low-rank approximation for extracting non-Gaussian  subspaces. Signal Processing, 87(8):1890-1903, 2007.  Motoaki Kawanabe, Masashi Sugiyama, Gilles Blanchard, and Klaus-Robert M\u00a8uller. A new algo- rithm of non-Gaussian component analysis with radial kernel functions. Annals of the Institute of Statistical Mathematics, 59(1):57-75, 2006.  M. Kawanabe. Linear dimension reduction based on the fourth-order cumulant tensor. Proc. of  Artifical Neural Networks - ICANN 2005, pages 151-156, 2005.  Hiroaki Sasaki, Aapo Hyv\u00a8arinen, and Masashi Sugiyama. Clustering via mode seeking by direct estimation of the gradient of a log-density. Lecture Notes in Computer Science (including sub- series Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 8726 LNAI (PART 3):19-34, 2014.  Hiroaki Sasaki, Gang Niu, and Masashi Sugiyama. Non-Gaussian Component Analysis with Log- Density Gradient Estimation. International Conference on Artificial Intelligence and Statistics, 51:1-20, 2016.  Yan Shuo Tan. Energy optimization for distributions on the sphere and improvement to the welch  bounds. Electron. Commun. Probab., 22:12 pp., 2017.  Santosh S. Vempala and Ying Xiao. Structure from Local Optima: Learning Subspace Juntas via  Higher Order PCA. arXiv:1108.3329, 2011.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Yonina C. Eldar and Gitta Kutyniok, editors, Compressed Sensing, pages 210-268. Cambridge University Press, Cambridge, 2011.  Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis-Kahan theorem for statisticians.  Biometrika, 102(2):315-323, 2015. POLYNOMIAL COMPLEXITY FOR NON-GAUSSIAN COMPONENT ANALYSIS  "}, "Calibrating Noise to Variance in Adaptive Data Analysis": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Calibrating Noise to Variance in Adaptive Data Analysis", "abstract": "Datasets are often used multiple times and each successive analysis may depend on the outcome of previous analyses. Standard techniques for ensuring generalization and statistical validity do not account for this adaptive dependence. A recent line of work studies the challenges that arise from such adaptive data reuse by considering the problem of answering a sequence of \u201cqueries\u201d about the data distribution where each query may depend arbitrarily on answers to previous queries. The strongest results obtained for this problem rely on differential privacy \u2013 a strong notion of algorithmic stability with the important property that it \u201ccomposes\u201d well when data is reused. However the notion is rather strict, as it requires stability under replacement of an arbitrary data element. The simplest algorithm is to add Gaussian (or Laplace) noise to distort the empirical answers. However, analysing this technique using differential privacy yields suboptimal accuracy guarantees when the queries have low variance. Here we propose a relaxed notion of stability based on KL divergence that also composes adaptively. We show that our notion of stability implies a bound on the mutual information between the dataset and the output of the algorithm and then derive new generalization guarantees implied by bounded mutual information. We demonstrate that a simple and natural algorithm based on adding noise scaled to the standard deviation of the query provides our notion of stability. This implies an algorithm that can answer statistical queries about the dataset with substantially improved accuracy guarantees for low-variance queries. The only previous approach that provides such accuracy guarantees is based on a more involved differentially private median-of-means algorithm and its analysis exploits stronger \u201cgroup\u201d stability of the algorithm.", "pdf_url": "http://proceedings.mlr.press/v75/feldman18a/feldman18a.pdf", "keywords": [], "reference": "Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  2016. URL http://arxiv.org/abs/1604.03336.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016. URL http://arxiv.org/abs/1511.02513.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayo\ufb00. Learners that use little information. In ALT, pages 25-55, 2018. URL http://proceedings.mlr. press/v83/bassily18a.html.  Olivier Bousquet and Andr\u00e9 Elissee\ufb00. Stability and generalization. JMLR, 2:499-526, 2002.  URL http://www.jmlr.org/papers/v2/bousquet02a.html.  Mark Bun and Thomas Steinke. Concentrated di\ufb00erential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography Conference, pages 635-658. Springer Berlin Heidelberg, 2016. URL https://arxiv.org/abs/1605.02065.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, pages 265-284, 2006a. URL http://repository.cmu.edu/jpc/ vol7/iss3/2.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Annual Interna- tional Conference on the Theory and Applications of Cryptographic Techniques, pages 486-503. Springer Berlin Heidelberg, 2006b. URL https://www.iacr.org/archive/ eurocrypt2006/40040493/40040493.pdf.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. CoRR, abs/1506, 2015. Extended abstract in NIPS 2015.  Vitaly Feldman and Thomas Steinke. Generalization for adaptively-chosen estimators via stable median. In Conference on Learning Theory (COLT), 2017a. URL https: //arxiv.org/abs/1706.05069.  9   Calibrating Noise to Variance in Adaptive Data Analysis  Acknowledgements  We thank Adam Smith for his suggestion to analyze the generalization of ALKL stable algorithms via mutual information. This insight greatly simplified our initial analysis and allowed us to derive additional corollaries. We also thank Nati Srebro for pointing out the connection between our results and the PAC-Bayes generalization bounds. Part of this work was done while Vitaly Feldman was at IBM Research - Almaden and while visiting the Simons Institute, UC Berkeley.  References  Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  2016. URL http://arxiv.org/abs/1604.03336.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016. URL http://arxiv.org/abs/1511.02513.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayo\ufb00. Learners that use little information. In ALT, pages 25-55, 2018. URL http://proceedings.mlr. press/v83/bassily18a.html.  Olivier Bousquet and Andr\u00e9 Elissee\ufb00. Stability and generalization. JMLR, 2:499-526, 2002.  URL http://www.jmlr.org/papers/v2/bousquet02a.html.  Mark Bun and Thomas Steinke. Concentrated di\ufb00erential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography Conference, pages 635-658. Springer Berlin Heidelberg, 2016. URL https://arxiv.org/abs/1605.02065.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, pages 265-284, 2006a. URL http://repository.cmu.edu/jpc/ vol7/iss3/2.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Annual Interna- tional Conference on the Theory and Applications of Cryptographic Techniques, pages 486-503. Springer Berlin Heidelberg, 2006b. URL https://www.iacr.org/archive/ eurocrypt2006/40040493/40040493.pdf.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. CoRR, abs/1506, 2015. Extended abstract in NIPS 2015.  Vitaly Feldman and Thomas Steinke. Generalization for adaptively-chosen estimators via stable median. In Conference on Learning Theory (COLT), 2017a. URL https: //arxiv.org/abs/1706.05069. Calibrating Noise to Variance in Adaptive Data Analysis  Vitaly Feldman and Thomas Steinke. Calibrating noise to variance in adaptive data analysis.  CoRR, abs/1712.07196, 2017b. URL http://arxiv.org/abs/1712.07196.  M. Hardt and J. Ullman. Preventing false discovery in interactive data analysis is hard. In  FOCS, pages 454-463, 2014. URL https://arxiv.org/abs/1408.1655.  M. Kearns. E\ufb03cient noise-tolerant learning from statistical queries. Journal of the ACM, 45  (6):983-1006, 1998.  David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv:1307.2118, 2013. URL https://arxiv.org/abs/1307.2118.  arXiv preprint  David A. McAllester. Pac-bayesian model averaging. In COLT, pages 164-170, 1999. doi:  10.1145/307400.307435.  CSF, pages 263-275, 2017.  Ilya Mironov. R\u00e9nyi di\ufb00erential privacy. In Computer Security Foundations Symposium,  Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for  predictivity in learning theory. Nature, 428(6981):419-422, 2004.  Maxim Raginsky, Alexander Rakhlin, Matthew Tsao, Yihong Wu, and Aolin Xu. Information- theoretic analysis of stability and bias of learning algorithms. In 2016 IEEE Information Theory Workshop, ITW 2016, Cambridge, United Kingdom, September 11-14, 2016, pages 26-30, 2016.  Ryan Rogers, Aaron Roth, Adam Smith, and Om Thakkar. Max-information, di\ufb00erential In Foundations of Computer Science privacy, and post-selection hypothesis testing. (FOCS), 2016 IEEE 57th Annual Symposium on, pages 487-494. IEEE, 2016. URL https://arxiv.org/abs/1604.03924.  Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS, 2016. URL https://arxiv.org/abs/1511.05219.  Thomas Steinke. Adaptive data analysis. 2016. URL http://people.seas.harvard.edu/ ~madhusudan/courses/Spring2016/notes/thomas-notes-ada.pdf. Lecture Notes.  Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of preventing false discovery. In COLT, pages 1588-1628, 2015. URL http://jmlr.org/ proceedings/papers/v40/Steinke15.html.  Yu-Xiang Wang, Jing Lei, and Stephen E Fienberg. On-average kl-privacy and its equivalence to generalization for max-entropy mechanisms. In International Conference on Privacy in Statistical Databases, pages 121-134. Springer, 2016. URL https://arxiv.org/abs/ 1605.02277.  Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. CoRR, abs/1705.07809, 2017. URL http://arxiv.org/abs/1705. 07809. "}, "Accelerating Stochastic Gradient Descent for Least Squares Regression": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Accelerating Stochastic Gradient Descent for Least Squares Regression", "abstract": "There is widespread sentiment that fast gradient methods (e.g. Nesterov\u2019s acceleration, conjugate gradient, heavy ball) are not effective for the purposes of stochastic optimization due to their instability and error accumulation. Numerous works have attempted to quantify these instabilities in the face of either statistical or non-statistical. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes this conventional wisdom by showing that acceleration can be made robust to statistical errors.  In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent.  Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.", "pdf_url": "http://proceedings.mlr.press/v75/jain18a/jain18a.pdf", "keywords": ["Stochastic Approximation", "Acceleration", "Stochastic Gradient Descent", "Accelerated Stochastic Gradient Descent", "Least Squares Regression"], "reference": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. CoRR,  abs/1603.05953, 2016.  Dan Anbar.  On Optimal Estimation Methods Using Stochastic Approximation Proce- dures. University of California, 1971. URL http://books.google.com/books?id= MmpHJwAACAAJ.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.  Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algo-  rithms for machine learning. In NIPS 24, 2011.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  L\u00b4eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.  Louis Augustin Cauchy. M\u00b4ethode g\u00b4en\u00b4erale pour la r\u00b4esolution des syst\u00b4emes d\u2019\u00b4equations simultanees.  C. R. Acad. Sci. Paris, 1847.  mization, 19(3):1171-1183, 2008.  Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-  Alexandre D\u00b4efossez and Francis R. Bach. Averaged least-mean-squares: Bias-variance trade-offs  and optimal sampling distributions. In AISTATS, volume 38, 2015.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods with inexact oracle:  the strongly convex case. CORE Discussion Papers 2013016, 2013.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex  optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.  Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation with large step  sizes. The Annals of Statistics, 2015.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger  convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.  Vaclav Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics,  1(3), 1973.  Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal  point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015a.  13   ACCELERATING STOCHASTIC GRADIENT DESCENT  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 2012.  Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. CoRR,  abs/1603.05953, 2016.  Dan Anbar.  On Optimal Estimation Methods Using Stochastic Approximation Proce- dures. University of California, 1971. URL http://books.google.com/books?id= MmpHJwAACAAJ.  Francis R. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. Journal of Machine Learning Research (JMLR), volume 15, 2014.  Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algo-  rithms for machine learning. In NIPS 24, 2011.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with  convergence rate O(1/n). In NIPS 26, 2013.  L\u00b4eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.  Louis Augustin Cauchy. M\u00b4ethode g\u00b4en\u00b4erale pour la r\u00b4esolution des syst\u00b4emes d\u2019\u00b4equations simultanees.  C. R. Acad. Sci. Paris, 1847.  mization, 19(3):1171-1183, 2008.  Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-  Alexandre D\u00b4efossez and Francis R. Bach. Averaged least-mean-squares: Bias-variance trade-offs  and optimal sampling distributions. In AISTATS, volume 38, 2015.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods with inexact oracle:  the strongly convex case. CORE Discussion Papers 2013016, 2013.  Olivier Devolder, Franc\u00b8ois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex  optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.  Aymeric Dieuleveut and Francis R. Bach. Non-parametric stochastic approximation with large step  sizes. The Annals of Statistics, 2015.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger  convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.  Vaclav Fabian. Asymptotically efficient stochastic approximation; the RM case. Annals of Statistics,  1(3), 1973.  Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal  point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015a. ACCELERATING STOCHASTIC GRADIENT DESCENT  Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Competing with the empirical risk  minimizer in a single pass. In COLT, 2015b.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 2012.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 2013.  Anne Greenbaum. Behavior of slightly perturbed lanczos and conjugate-gradient recurrences. Lin-  ear Algebra and its Applications, 1989.  Magnus R. Hestenes and Eduard Stiefel. Methods of conjuate gradients for solving linear systems.  Journal of Research of the National Bureau of Standards, 1952.  Daniel J. Hsu, Sham M. Kakade, and Tong Zhang. Random design analysis of ridge regression.  Foundations of Computational Mathematics, 14(3):569-600, 2014.  Chonghai Hu, James T. Kwok, and Weike Pan. Accelerated gradient methods for stochastic opti-  mization and online learning. In NIPS 22, 2009.  Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paralleliz- ing stochastic approximation through mini-batching and tail-averaging. CoRR, abs/1610.03774, 2016.  Harold J. Kushner and Dean S. Clark. Stochastic Approximation Methods for Constrained and  Unconstrained Systems. Springer-Verlag, 1978.  Harold J. Kushner and George Yin. Stochastic approximation and recursive algorithms and appli-  cations. Springer-Verlag, 2003.  G. Lan. An optimal method for stochastic composite optimization. Tech. Report, GaTech., 2008.  Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. CoRR,  abs/1507.02000, 2015.  Erich L. Lehmann and George Casella. Theory of Point Estimation. Springer, 1998.  Hongzhou Lin, Julien Mairal, and Za\u00a8\u0131d Harchaoui. A universal catalyst for first-order optimization.  In NIPS, 2015.  Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling,  and the randomized kaczmarz algorithm. Mathematical Programming, 2016.  Arkadii S. Nemirovsky and David B. Yudin. Problem Complexity and Method Efficiency in Opti-  mization. John Wiley, 1983.  Yurii E. Nesterov. A method for unconstrained convex minimization problem with the rate of  convergence O(1/k2). Doklady AN SSSR, 269, 1983. ACCELERATING STOCHASTIC GRADIENT DESCENT  Yurii E. Nesterov.  Introductory lectures on convex optimization: A basic course, volume 87 of  Applied Optimization. Kluwer Academic Publishers, 2004.  Yurii E. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.  SIAM Journal on Optimization, 22(2):341-362, 2012.  Christopher C. Paige. The computation of eigenvalues and eigenvectors of very large sparse matri-  ces. PhD Thesis, University of London, 1971.  Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-  putational Mathematics and Mathematical Physics, 4, 1964.  Boris T. Polyak. Introduction to Optimization. Optimization Software, 1987.  Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, volume 30, 1992.  John G. Proakis. Channel identification for high speed digital communications. IEEE Transactions  on Automatic Control, 1974.  Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics  in convex programming. IEEE Transactions on Information Theory, 2011.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, vol. 22, 1951.  Sumit Roy and John J. Shynk. Analysis of the momentum lms algorithm. IEEE Transactions on  Acoustics, Speech and Signal Processing, 1990.  David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Tech. Re-  port, ORIE, Cornell University, 1988.  Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for  regularized loss minimization. In ICML, 2014.  Rajesh Sharma, William A. Sethares, and James A. Bucklew. Analysis of momentum adaptive  filtering algorithms. IEEE Transactions on Signal Processing, 1998.  Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Publishers, 2000.  Bernard Widrow and Samuel D. Stearns. Adaptive Signal Processing. Englewood Cliffs, NJ:  Prentice-Hall, 1985.  Ashia C. Wilson, Benjamin Recht, and Michael I. Jordan. A lyapunov analysis of momentum  methods in optimization. CoRR, abs/1611.02635, 2016.  Blake Woodworth and Nathan Srebro. Tight complexity bounds for optimizing composite objec-  tives. CoRR, abs/1605.08003, 2016.  Kun Yuan, Bicheng Ying, and Ali H. Sayed. On the in\ufb02uence of momentum acceleration on online  learning. Journal of Machine Learning Research (JMLR), volume 17, 2016. ACCELERATING STOCHASTIC GRADIENT DESCENT  "}, "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints", "abstract": "We study the generalization errors of \\emph{non-convex} regularized ERM procedures using Stochastic Gradient Langevin Dynamics (SGLD). Two theories are proposed with non-asymptotic discrete-time analysis, using stability and PAC-Bayesian theory respectively. The stability-based theory obtains a bound of $O\\left(\\frac{1}{n}L\\sqrt{\\beta T_N}\\right)$, where $L$ is Lipschitz parameter, $\\beta$ is inverse temperature, and $T_N$ is the sum of step sizes. For PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate, the contribution of each step decays exponentially through time, and the uniform Lipschitz constant is also replaced by actual norms of gradients along the optimization trajectory. Our bounds have reasonable dependence on aggregated step sizes, and do not explicitly depend on dimensions, norms or other capacity measures of the parameter. The bounds characterize how the noises in the algorithm itself controls the statistical learning behavior in non-convex problems, without uniform convergence in the hypothesis space, which sheds light on the effect of training algorithms on the generalization error for deep neural networks.", "pdf_url": "http://proceedings.mlr.press/v75/mou18a/mou18a.pdf", "keywords": ["algorithm-dependent generalization bound", "stochastic gradient Langevin dynamics", "stability", "PAC-Bayesian theory", "non-convex learning"], "reference": "Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected langevin monte carlo. arXiv preprint arXiv:1507.02564, 2015.  Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing  gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin  mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.  Imre Csisz\u00b4ar, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and  Trends R(cid:13) in Communications and Information Theory, 1(4):417-528, 2004.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Arnak S Dalalyan and Alexandre B Tsybakov. Sparse regression learning by aggregation and  langevin monte-carlo. Journal of Computer and System Sciences, 78(5):1423-1443, 2012.  Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning  algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pages 1884- 1892, 2016.  Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061-  1083, 1975.  13   GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  We thank Zhou Lu, Feicheng Wang and Xiang Wang for helpful discussions. This work was par- tially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and Center for Data Science, Beijing Institute of Big Data Research in Peking University. This work was done when X.Z. was visiting Peking University.  Acknowledgments  References  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2(Mar):499-526, 2002.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected langevin monte carlo. arXiv preprint arXiv:1507.02564, 2015.  Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing  gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.  Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters  in stochastic gradient descent. arXiv preprint arXiv:1610.08637, 2016.  Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin  mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.  Imre Csisz\u00b4ar, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and  Trends R(cid:13) in Communications and Information Theory, 1(4):417-528, 2004.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Arnak S Dalalyan and Alexandre B Tsybakov. Sparse regression learning by aggregation and  langevin monte-carlo. Journal of Computer and System Sciences, 78(5):1423-1443, 2012.  Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning  algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pages 1884- 1892, 2016.  Leonard Gross. Logarithmic sobolev inequalities. American Journal of Mathematics, 97(4):1061-  1083, 1975. GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  Istv\u00b4an Gy\u00a8ongy. Mimicking the one-dimensional marginal distributions of processes having an it\u02c6o  differential. Probability theory and related fields, 71(4):501-516, 1986.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.  Chris Junchi Li, Lei Li, Junyang Qian, and Jian-Guo Liu. Batch size matters: A diffusion approxi- mation framework on nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.  Qianxiao Li, Cheng Tai, and E Weinan. Dynamics of stochastic gradient algorithms. arXiv preprint  arXiv:1511.06251, 2015.  Junhong Lin and Lorenzo Rosasco. Optimal learning for multi-pass stochastic gradient methods. In  Advances in Neural Information Processing Systems, pages 4556-4564, 2016.  Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit regularization for multiple passes sgm. In International Conference on Machine Learning, pages 2340-2348, 2016.  Ben London. Generalization bounds for randomized learning with application to stochastic gradient  descent. In NIPS Workshop on Optimizing the Optimizers, 2016.  Peter A Markowich and C\u00b4edric Villani. On the trend to equilibrium for the fokker-planck equation:  an interplay between physics and functional analysis. Mat. Contemp, 19:1-29, 2000.  David A McAllester. Pac-bayesian stochastic model selection. Machine Learning, 51(1):5-21,  2003.  Tigran Nagapetyan, Andrew B Duncan, Leonard Hasenclever, Sebastian J Vollmer, Lukasz Szpruch, and Konstantinos Zygalakis. The true cost of stochastic gradient langevin dynamics. arXiv preprint arXiv:1706.02692, 2017.  Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.  Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-  rithms. arXiv preprint arXiv:1801.04295, 2018.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.  Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.  Analysis and Applications, 3(04):397-417, 2005.  Hannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pages 63-95. Springer,  1996. GENERALIZATION BOUNDS OF SGLD FOR NON-CONVEX LEARNING  Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In Advances in Neural Information Processing Systems, pages 6067-6077, 2017.  Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn- ing algorithms. In Advances in Neural Information Processing Systems, pages 2521-2530, 2017.  Nanyang Ye, Zhanxing Zhu, and Rafal K Mantiuk. Langevin dynamics with continuous tempering  for training deep neural networks. arXiv preprint arXiv:1703.04379, 2017.  Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Karthik Sridharan, Brando Miranda, Noah Golowich, and Tomaso Poggio. Theory of deep learning iii: Generalization properties of sgd. Technical report, Center for Brains, Minds and Machines (CBMM), 2017a.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient  langevin dynamics. arXiv preprint arXiv:1702.05575, 2017b.  "}, "Optimal approximation of continuous functions by very deep ReLU networks": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Optimal approximation of continuous functions by very deep ReLU networks", "abstract": "We consider approximations of general continuous functions on finite-dimensional cubes by general deep ReLU neural networks and study the approximation rates with respect to the modulus of continuity of the function and the total number of weights $W$ in the network. We establish the complete phase diagram of feasible approximation rates and show that it includes two distinct phases. One phase corresponds to slower approximations that can be achieved with constant-depth networks and continuous weight assignments. The other phase provides faster approximations at the cost of depths necessarily growing as a power law $L\\sim W^{\\alpha}, 0", "pdf_url": "http://proceedings.mlr.press/v75/yarotsky18a/yarotsky18a.pdf", "keywords": [], "reference": "Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.  Cambridge university press, 2009.  Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for  piecewise polynomial networks. Neural computation, 10(8):2159-2173, 1998.  Peter L Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc- dimension and pseudodimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: IEEE transactions on neural  A comparison between shallow and deep architectures. networks and learning systems, 25(8):1553-1565, 2014.  Helmut B\u00a8olcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Memory-optimal In Wavelets and Sparsity XVII, volume 10394, page  neural network approximation. 103940Q. International Society for Optics and Photonics, 2017.  Ronald A DeVore. Nonlinear approximation. Acta numerica, 7:51-150, 1998.  Ronald A DeVore, Ralph Howard, and Charles Micchelli. Optimal nonlinear approximation.  Manuscripta mathematica, 63(4):469-478, 1989.  Paul W Goldberg and Mark R Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning, 18(2-3):131-148, 1995.  Boris Hanin and Mark Sellke. Approximating Continuous Functions by ReLU Nets of  Minimal Width. arXiv preprint arXiv:1710.11278, 2017.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.  Paul C Kainen, V\u02c7era K\u02daurkov\u00b4a, and Andrew Vogt. Approximation by neural networks is  not continuous. Neurocomputing, 29(1):47-56, 1999.  10   Optimal approximation of continuous functions by very deep ReLU networks  Quantized network weights have been previously considered from the information- theoretic point of view in B\u00a8olcskei et al. (2017); Petersen and Voigtlaender (2017). In the present paper we do not use quantized weights in the statement of the approximation problem, but they appear in the solution (namely, we use them to store small-scale descrip- tions of the approximated function). One can expect that weight quantization may play an important role in the future development of the theory of deep networks.  The author thanks Alexandr Kuleshov and the anonymous referees for helpful comments and suggestions. The research was supported by the Skoltech SBI Bazykin/Yarotsky project.  Acknowledgments  References  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.  Cambridge university press, 2009.  Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for  piecewise polynomial networks. Neural computation, 10(8):2159-2173, 1998.  Peter L Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc- dimension and pseudodimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: IEEE transactions on neural  A comparison between shallow and deep architectures. networks and learning systems, 25(8):1553-1565, 2014.  Helmut B\u00a8olcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Memory-optimal In Wavelets and Sparsity XVII, volume 10394, page  neural network approximation. 103940Q. International Society for Optics and Photonics, 2017.  Ronald A DeVore. Nonlinear approximation. Acta numerica, 7:51-150, 1998.  Ronald A DeVore, Ralph Howard, and Charles Micchelli. Optimal nonlinear approximation.  Manuscripta mathematica, 63(4):469-478, 1989.  Paul W Goldberg and Mark R Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning, 18(2-3):131-148, 1995.  Boris Hanin and Mark Sellke. Approximating Continuous Functions by ReLU Nets of  Minimal Width. arXiv preprint arXiv:1710.11278, 2017.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.  Paul C Kainen, V\u02c7era K\u02daurkov\u00b4a, and Andrew Vogt. Approximation by neural networks is  not continuous. Neurocomputing, 29(1):47-56, 1999. Optimal approximation of continuous functions by very deep ReLU networks  Michael J Kearns and Robert E Schapire. E\ufb03cient distribution-free learning of proba- bilistic concepts. In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on, pages 382-391. IEEE, 1990.  Shiyu Liang and R. Srikant. Why deep neural networks? arXiv preprint arXiv:1610.04161,  2016.  Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. In Advances in Neural Information Processing Systems, pages 6232-6240, 2017.  Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning real and boolean functions:  When is deep better than shallow. arXiv preprint arXiv:1603.00988, 2016.  Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pages 2924-2932, 2014.  Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth func-  tions using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.  Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth  non-linear functions. arXiv preprint arXiv:1610.09887, 2016.  Akito Sakurai. Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks. In  Advances in Neural Information Processing Systems, pages 323-329, 1999.  J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU acti-  vation function. arXiv preprint arXiv:1708.06633, 2017.  Rupesh Kumar Srivastava, Klaus Gre\ufb00, and J\u00a8urgen Schmidhuber. Highway networks. arXiv  preprint arXiv:1505.00387, 2015.  Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485,  2016.  Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural  Networks, 94:103-114, 2017. "}, "Averaging Stochastic Gradient Descent on Riemannian Manifolds": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Averaging Stochastic Gradient Descent on Riemannian Manifolds", "abstract": "We consider the minimization of a function defined on a Riemannian manifold $\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to an averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming $k$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.", "pdf_url": "http://proceedings.mlr.press/v75/tripuraneni18a/tripuraneni18a.pdf", "keywords": ["Optimization", "Riemannian Manifold", "Stochastic Approximation", "k-PCA"], "reference": "135-158, 2012.  P.-A. Absil and J. Malick. Projection-like retractions on matrix manifolds. SIAM J. Optim., 22(1):  P-A Absil, R. Mahony, and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a  view on algorithmic computation. Acta Applicandae Mathematicae, 80(2):199-220, 2004.  P.-A. Absil, C.G. Baker, and K.A. Gallivan. Trust-region methods on Riemannian manifolds. Foun-  dations of Computational Mathematics, 7(3):303-330, Jul 2007.  P-A Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton  University Press, 2009.  Soc., 139(2):655-673, 2011.  B. Afsari. Riemannian Lp center of mass: existence, uniqueness, and convexity. Proc. Amer. Math.  Z. Allen-Zhu and Y. Li. First efficient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate. In Proceedings of the 58th Symposium on Foundations of Computer Science, FOCS \u201917, 2017.  A. Aswani, P. Bickel, and C. Tomlin. Regression on manifolds: estimation of the exterior derivative.  Ann. Statist., 39(1):48-81, 2011.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451-459, 2011.  A. Benveniste, P. Priouret, and M. M\u00b4etivier. Adaptive Algorithms and Stochastic Approximations.  Springer, 1990.  D. A Bini and B. Iannazzo. Computing the Karcher mean of symmetric positive definite matrices.  Linear Algebra and its Applications, 438(4):1700-1710, 2013.  S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Auto-  matic Control, 58(9):2217-2229, 2013.  L. Bottou. Online algorithms and stochastic approximations. In Online Learning and Neural Net-  works. Cambridge University Press, Cambridge, UK, 1998.  P. Bougerol and J. Lacroix. Products of Random Matrices with Applications to Schr\u00a8odinger Oper-  ators, volume 8 of Progress in Probability and Statistics. Birkh\u00a8auser, 1985.  N. Boumal. On intrinsic Cram\u00b4er-Rao bounds for Riemannian submanifolds and quotient manifolds.  IEEE Trans. Signal Process., 61(7):1809-1821, 2013.  N. Boumal and P.-A. Absil. RTRMC: A Riemannian trust-region method for low-rank matrix com-  pletion. In Advances in Neural Information Processing Systems 24, pages 406-414. 2011.  13   AVERAGING ON MANIFOLDS  acknowledges support from the Mathematical Data Science program of the Office of Naval Research under grant number N00014-15-1-2670.  References  135-158, 2012.  P.-A. Absil and J. Malick. Projection-like retractions on matrix manifolds. SIAM J. Optim., 22(1):  P-A Absil, R. Mahony, and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a  view on algorithmic computation. Acta Applicandae Mathematicae, 80(2):199-220, 2004.  P.-A. Absil, C.G. Baker, and K.A. Gallivan. Trust-region methods on Riemannian manifolds. Foun-  dations of Computational Mathematics, 7(3):303-330, Jul 2007.  P-A Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton  University Press, 2009.  Soc., 139(2):655-673, 2011.  B. Afsari. Riemannian Lp center of mass: existence, uniqueness, and convexity. Proc. Amer. Math.  Z. Allen-Zhu and Y. Li. First efficient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate. In Proceedings of the 58th Symposium on Foundations of Computer Science, FOCS \u201917, 2017.  A. Aswani, P. Bickel, and C. Tomlin. Regression on manifolds: estimation of the exterior derivative.  Ann. Statist., 39(1):48-81, 2011.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451-459, 2011.  A. Benveniste, P. Priouret, and M. M\u00b4etivier. Adaptive Algorithms and Stochastic Approximations.  Springer, 1990.  D. A Bini and B. Iannazzo. Computing the Karcher mean of symmetric positive definite matrices.  Linear Algebra and its Applications, 438(4):1700-1710, 2013.  S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Auto-  matic Control, 58(9):2217-2229, 2013.  L. Bottou. Online algorithms and stochastic approximations. In Online Learning and Neural Net-  works. Cambridge University Press, Cambridge, UK, 1998.  P. Bougerol and J. Lacroix. Products of Random Matrices with Applications to Schr\u00a8odinger Oper-  ators, volume 8 of Progress in Probability and Statistics. Birkh\u00a8auser, 1985.  N. Boumal. On intrinsic Cram\u00b4er-Rao bounds for Riemannian submanifolds and quotient manifolds.  IEEE Trans. Signal Process., 61(7):1809-1821, 2013.  N. Boumal and P.-A. Absil. RTRMC: A Riemannian trust-region method for low-rank matrix com-  pletion. In Advances in Neural Information Processing Systems 24, pages 406-414. 2011. AVERAGING ON MANIFOLDS  T. T. Cai, Z. Ma, and Y. Wu. Sparse PCA: optimal rates and adaptive estimation. Ann. Statist., 41  (6):3074-3110, 2013.  M. P. Do Carmo. Differential Geometry of Curves and Surfaces. Courier Dover Publications, 2016.  A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogonality con-  straints. SIAM journal on Matrix Analysis and Applications, 20(2):303-353, 1998.  V. Fabian. On asymptotic normality in stochastic approximation. Ann. Math. Statist, 39:1327-1332,  1968.  N. Flammarion and F. Bach. Stochastic composite least-squares regression with convergence rate O(1/n). In Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 831-875. PMLR, 07-10 Jul 2017.  R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1990.  R. Hosseini and S. Sra. Matrix manifold optimization for Gaussian mixtures. In Advances in Neural  Information Processing Systems, pages 910-918, 2015.  W. Huang, K. A Gallivan, and P-A Absil. A Broyden class of quasi-Newton methods for Rieman-  nian optimization. SIAM Journal on Optimization, 25(3):1660-1685, 2015.  M. Ishteva, P.-A. Absil, S. Van Huffel, and L. De Lathauwer. Best low multilinear rank approxima- tion of higher-order tensors, based on the Riemannian trust-region scheme. SIAM J. Matrix Anal. Appl., 32(1):115-135, 2011.  P. Jain, C. Jin, S. M. Kakade, P. Netrapalli, and A. Sidford. Streaming PCA: matching matrix Bern- stein and near-optimal finite sample guarantees for Oja\u2019s algorithm. In Conference on Learning Theory, pages 1147-1164, 2016.  H. Kushner and G G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer, 2003.  Applications, 24(1):1-16, 2002.  M. Moakher. Means and averaging in the group of rotations. SIAM Journal on Matrix Analysis and  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM J. Optim., 19(4):1574-1609, 2008.  Y. Nesterov and J.-P. Vial. Confidence level solutions for stochastic programming. Automatica J.  IFAC, 44(6):1559-1568, 2008.  can Mathematical Society, 1973.  M. B. Nevelson and R. Z. Hasminski. Stochastic Approximation and Recursive Estimation. Ameri-  E. Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical  Biology, 15(3):267-273, Nov 1982.  E. Oja and J. Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the  expectation of a random matrix. J. Math. Anal. Appl., 106(1):69-84, 1985. AVERAGING ON MANIFOLDS  B. T. Polyak. A new method of stochastic approximation type. Avtomatika i Telemekhanika, 51(7):  98-107, 1990.  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM  Journal on Control and Optimization, 30(4):838-855, 1992.  M. Rei\u00df and M. Wahl. Non-asymptotic upper bounds for the reconstruction error of PCA. arXiv  preprint arXiv:1609.03779, 2016.  W. Ring and B. Wirth. Optimization methods on Riemannian manifolds and their application to  shape space. SIAM Journal on Optimization, 22(2):596-627, 2012.  H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-  tics, pages 400-407, 1951.  D. Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical  report, Cornell University Operations Research and Industrial Engineering, 1988.  T. Sakai. Riemannian Geometry, volume 149 of Translations of Mathematical Monographs. Amer-  ican Mathematical Society, 1996.  arXiv:1702.05594, 2017.  H. Sato, H. Kasai, and B. Mishra. Riemannian stochastic variance reduced gradient. arXiv preprint  S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In  Proceedings of the International Conference on Learning Theory (COLT), 2009.  O. Shamir. Convergence of stochastic gradient descent for PCA.  In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 257-265. PMLR, 20-22 Jun 2016a.  O. Shamir. Fast stochastic algorithms for SVD and PCA: convergence properties and convexity. In  International Conference on Machine Learning, pages 248-256, 2016b.  S. T. Smith. Covariance, subspace, and intrinsic Cram\u00b4er-Rao bounds. IEEE Trans. Signal Process.,  53(5):1610-1630, 2005.  J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere II: recovery by Rieman-  nian trust-region method. IEEE Trans. Inform. Theory, 63(2):885-914, 2017.  C. Udriste. Convex Functions and Optimization Methods on Riemannian Manifolds, volume 297.  Springer Science & Business Media, 1994.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 1998.  S. Waldmann. Geometric wave equations. arXiv preprint arXiv:1208.4706, 2012.  B. Yang. Projection approximation subspace tracking. Trans. Sig. Proc., 43(1):95-107, January  1995.  H. Zhang and S. Sra. First-order methods for geodesically convex optimization. In Conference on  Learning Theory, pages 1617-1638, 2016. AVERAGING ON MANIFOLDS  H. Zhang, S. J. Reddi, and S. Sra. Riemannian SVRG: fast stochastic optimization on Riemannian manifolds. In Advances in Neural Information Processing Systems, pages 4592-4600, 2016. AVERAGING ON MANIFOLDS  "}, "Fitting a Putative Manifold to Noisy Data": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Fitting a Putative Manifold to Noisy Data", "abstract": "In the present work,  we give a solution to the following  question from manifold learning.  Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed  from a measure supported on a low dimensional twice  differentiable embedded manifold $M$, and corrupted by a small amount of  gaussian noise. How can we produce a manifold $M\u2019$ whose Hausdorff distance to $M$ is small and whose reach is not much smaller than the reach of $M$?", "pdf_url": "http://proceedings.mlr.press/v75/fefferman18a/fefferman18a.pdf", "keywords": ["Manifold learning", "Hausdorff distance", "reach"], "reference": "Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Comput., 15(6):1373-1396, 2003. ISSN 0899-7667. doi: http://dx.doi. org/10.1162/089976603321780317.  Jean-Daniel Boissonnat, Leonidas J. Guibas, and Steve Oudot. Manifold reconstruction in ar- bitrary dimensions using witness complexes. Discrete & Computational Geometry, 42(1): 37-70, 2009. doi: 10.1007/s00454-009-9175-1. URL http://dx.doi.org/10.1007/ s00454-009-9175-1.  Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46:255-308, January 2009. doi: 10.1090/S0273-0979-09-01249-X. URL http://www.ams.org/bull/ 2009-46-02/S0273-0979-09-01249-X/home.html.  Siu-Wing Cheng, Tamal K. Dey, and Edgar A. Ramos. Manifold reconstruction from point samples. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, Vancouver, British Columbia, Canada, January 23-25, 2005, pages 1018-1027, 2005. URL http://dl.acm.org/citation.cfm?id=1070432.1070579.  Sanjoy Dasgupta and Yoav Freund. Random projection trees and low dimensional manifolds. In Proceedings of the 40th annual ACM symposium on Theory of computing, STOC \u201908, pages 537-546, 2008.  David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591-5596, May 2003. doi: 10.1073/pnas.1031596100. URL http://dx.doi.org/10.1073/pnas. 1031596100.  Herbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93,  1959.  Herbert Federer. Geometric Measure Theory. Springer New York, 1969.  Charles Fefferman, Sergei Ivanov, Yaroslav Kurylev, Matti Lassas, and Hariharan Narayanan. the geometric whitney problem. CoRR,  Reconstruction and interpolation of manifolds i: abs/1508.00674, 2015. URL http://arxiv.org/abs/1508.00674.  Charles L. Fefferman, Sanjoy K. Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.  Journal of the Americal Mathematical Society, 2016.  Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Manifold estimation and singular deconvolution under hausdorff loss. Annals of Statistics, 40(2), 2012.  13   MANIFOLD FITTING  Ch.F. was partly supported AFOSR, grant DMS-1265524, and NSF, grant FA9550- 12-1-0425. S.I. was partly supported RFBR, grant 14-01-00062, Y.K. was partly supported by EPSRC and the AXA professorship, M.L. was supported by Academy of Finland, grants 273979 and 284715, and H.N. was partly supported by NSF grant DMS-1620102 and a Ramanujan Fellowship.  Acknowledgments  References  Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Comput., 15(6):1373-1396, 2003. ISSN 0899-7667. doi: http://dx.doi. org/10.1162/089976603321780317.  Jean-Daniel Boissonnat, Leonidas J. Guibas, and Steve Oudot. Manifold reconstruction in ar- bitrary dimensions using witness complexes. Discrete & Computational Geometry, 42(1): 37-70, 2009. doi: 10.1007/s00454-009-9175-1. URL http://dx.doi.org/10.1007/ s00454-009-9175-1.  Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46:255-308, January 2009. doi: 10.1090/S0273-0979-09-01249-X. URL http://www.ams.org/bull/ 2009-46-02/S0273-0979-09-01249-X/home.html.  Siu-Wing Cheng, Tamal K. Dey, and Edgar A. Ramos. Manifold reconstruction from point samples. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, Vancouver, British Columbia, Canada, January 23-25, 2005, pages 1018-1027, 2005. URL http://dl.acm.org/citation.cfm?id=1070432.1070579.  Sanjoy Dasgupta and Yoav Freund. Random projection trees and low dimensional manifolds. In Proceedings of the 40th annual ACM symposium on Theory of computing, STOC \u201908, pages 537-546, 2008.  David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591-5596, May 2003. doi: 10.1073/pnas.1031596100. URL http://dx.doi.org/10.1073/pnas. 1031596100.  Herbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93,  1959.  Herbert Federer. Geometric Measure Theory. Springer New York, 1969.  Charles Fefferman, Sergei Ivanov, Yaroslav Kurylev, Matti Lassas, and Hariharan Narayanan. the geometric whitney problem. CoRR,  Reconstruction and interpolation of manifolds i: abs/1508.00674, 2015. URL http://arxiv.org/abs/1508.00674.  Charles L. Fefferman, Sanjoy K. Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.  Journal of the Americal Mathematical Society, 2016.  Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Manifold estimation and singular deconvolution under hausdorff loss. Annals of Statistics, 40(2), 2012. MANIFOLD FITTING  Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Nonpara- metric ridge estimation. Ann. Statist., 42(4):1511-1545, 2014. doi: 10.1214/14-AOS1218. URL http://dx.doi.org/10.1214/14-AOS1218.  Bal\u00b4azs K\u00b4egl, Adam Krzyzak, Tam\u00b4as Linder, and Kenneth Zeger. Learning and design of principal IEEE Transactions on Pattern Analysis and Machine Intelligence, 22:281-297, 2000.  curves. ISSN 0162-8828. doi: http://doi.ieeecomputersociety.org/10.1109/34.841759.  Kitty Mohammed and Hariharan Narayanan. Manifold learning using kernel density estimation and local principal components analysis. CoRR, abs/1709.03615., 2017. URL http://arxiv. org/abs/1709.03615.  R. Narasimhan. Lectures on Topics in Analysis. Tata Institute of Fundamental Research, Bombay,  1965.  esis. In NIPS, 2010.  Hariharan Narayanan and Sanjoy Mitter. On the sample complexity of testing the manifold hypoth-  Hariharan Narayanan and Partha Niyogi. On the sample complexity of learning smooth cuts on a manifold. In Proc. of the 22nd Annual Conference on Learning Theory (COLT), June 2009.  Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. Discrete & Computational Geometry, 39(1-3):419- 441, 2008.  Umut Ozertem and Deniz Erdogmus. Locally defined principal curves and surfaces. Journal of  Machine Learning Research, 12:1249-1286, 2011.  Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embed-  ding. Science, 290:2323-2326, 2000.  Alexander J. Smola and Robert C. Williamson. Regularized principal manifolds. In In Computa-  tional Learning Theory: 4th European Conference, pages 214-229. Springer, 2001.  J. B. Tenenbaum, V. Silva, and J. C. Langford. A Global Geometric Framework for Nonlinear  Dimensionality Reduction. Science, 290(5500):2319-2323, 2000.  Kilian Q. Weinberger and Lawrence K. Saul. Unsupervised learning of image manifolds by semidef-  inite programming. Int. J. Comput. Vision, 70(1):77-90, 2006.  "}, "Private Sequential Learning": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Private Sequential Learning", "abstract": "We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, $v^*$, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner\u2019s queries, though not the responses, and tries to infer from them the value of $v^*$. The objective of the learner is to obtain an accurate estimate of $v^*$ using only a small number of queries, while simultaneously protecting her privacy by making $v^*$ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner\u2019s query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.", "pdf_url": "http://proceedings.mlr.press/v75/tsitsiklis18a/tsitsiklis18a.pdf", "keywords": ["sequential learning", "privacy", "bisection algorithm"], "reference": "Benny Chor, Eyal Kushilevitz, Oded Goldreich, and Madhu Sudan. Private information retrieval.  Journal of the ACM, 45(6):965-981, 1998.  Cynthia Dwork. Differential privacy: A survey of results. In International Conference on Theory  and Applications of Models of Computation, pages 1-19. Springer, 2008.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations  and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  William Gasarch. A survey on private information retrieval. In Bulletin of the EATCS. Citeseer,  2004.  Eyal Kushilevitz and Rafail Ostrovsky. Replication is not needed: Single database, computationally- private information retrieval. In Foundations of Computer Science (FOCS), volume 97, pages 364-373, 1997.  John Tsitsiklis and Kuang Xu. Delay-predictability tradeoffs in reaching a secret goal. Operations  Research, 66(2):587-596, 2018.  John N Tsitsiklis, Kuang Xu, and Zhi Xu.  Private sequential  learning.  arXiv preprint  arXiv:1805.02136, 2018. URL https://arxiv.org/abs/1805.02136.  Zhi Xu. Private sequential search and optimization. Master\u2019s thesis, Massachusetts Institute of  Technology, 2017. URL http://hdl.handle.net/1721.1/112054.  7   PRIVATE SEQUENTIAL LEARNING  A main take-away from the above results is about the price of privacy: it is not difficult to see that in the absence of a privacy constraint, the most efficient strategy, using a bisection search, can locate the true value with log(1/(cid:15)) queries. Our results thus demonstrate that the price of privacy is at most an additive factor of 2L.  The proof of Theorem 4 is given in Tsitsiklis et al. (2018). For the upper bound, we con- struct a certain Opportunistic Bisection (OB) query strategy, where the learner augments a bisection search with 2L additional \u201copportunistic\u201d queries in a randomized and symmetric manner, such the adversary cannot be certain whether the true value is discovered by the bisection search, or the opportunistic queries. For the lower bound, one may ask whether the additional 2L queries need to be distinct from the log(1/(cid:15)) queries used by the bisection search, or essentially, whether the query complexity could be further reduced by \u201cblending\u201d the queries for obfuscation with those for identifying the true value in a more effective manner. However, we show that such \u201cblending\u201d is not possible.  References  Benny Chor, Eyal Kushilevitz, Oded Goldreich, and Madhu Sudan. Private information retrieval.  Journal of the ACM, 45(6):965-981, 1998.  Cynthia Dwork. Differential privacy: A survey of results. In International Conference on Theory  and Applications of Models of Computation, pages 1-19. Springer, 2008.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations  and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  William Gasarch. A survey on private information retrieval. In Bulletin of the EATCS. Citeseer,  2004.  Eyal Kushilevitz and Rafail Ostrovsky. Replication is not needed: Single database, computationally- private information retrieval. In Foundations of Computer Science (FOCS), volume 97, pages 364-373, 1997.  John Tsitsiklis and Kuang Xu. Delay-predictability tradeoffs in reaching a secret goal. Operations  Research, 66(2):587-596, 2018.  John N Tsitsiklis, Kuang Xu, and Zhi Xu.  Private sequential  learning.  arXiv preprint  arXiv:1805.02136, 2018. URL https://arxiv.org/abs/1805.02136.  Zhi Xu. Private sequential search and optimization. Master\u2019s thesis, Massachusetts Institute of  Technology, 2017. URL http://hdl.handle.net/1721.1/112054. "}, "Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models", "abstract": "Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. % In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmarks models in neural networks. % We evaluate the mutual information (or \u201cfree entropy\u201d) from which we deduce the Bayes-optimal inference and generalization errors.  Our analysis applies to the high-dimensional limit where both the number of samples and dimensions are large and their ratio is fixed. % Non-rigorous predictions for the optimal inference and generalization errors existed for special cases of GLMs, e.g. for the perceptron in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. % Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions.", "pdf_url": "http://proceedings.mlr.press/v75/barbier18a/barbier18a.pdf", "keywords": ["high-dimensional inference | generalized linear model | Bayesian inference | perceptron | phase transitions | approximate message-passing algorithm"], "reference": "J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse  superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, 2017.  J. Barbier, M. Dia, N. Macris, and F. Krzakala. The mutual information in random linear estimation.  In 54th Annual Allerton Conf. on Communication, Control, and Computing, page 625, a.  Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove  replica formulas in bayesian inference. arXiv:1705.02780[v3], 2017.  Jean Barbier, Nicolas Macris, Mohamad Dia, and Florent Krzakala. Mutual information and opti-  mality of approximate message-passing in random linear estimation. arXiv:1701.05823, b.  M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications  to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785, 2011.  M. Bayati and A. Montanari. The lasso risk for gaussian matrices. IEEE Transactions on Informa-  tion Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Marc Lelarge, and Andrea Montanari. Universality in polytope phase transitions  and message passing algorithms. The Annals of Applied Probability, 25(2):753-822, 2015.  Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherrington-  kirkpatrick model. Communications in Mathematical Physics, 325(1):333366, 2014.  Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406, 2006.  3   PH. TRANSITIONS, OPT. ERRORS AND AMP IN HIGH-D. GENERALIZED LINEAR MODELS  A second object of focus is the algorithmic complexity: When is it possible to efficiently per- form these optimal estimations? To answer this question, we compare our information-theoretic results to the performance of the GAMP algorithm and its state evolution Rangan (2011). We deter- mine regions of parameters where this algorithm is or is not information-theoretically optimal. Up to technical assumptions, our results apply to all activation functions \u03d5 and priors P0, thus unifying a large volume of previous work where many particular functions have been analyzed on a case by case basis. This generality allows us to provide a unifying understanding of the types of phase transitions and phase diagrams that we can encounter in GLMs. Among other, we discuss the per- ceptron problem, one-bit compressed sensing, real valued-phase retrievial (or sign-less compressed sensing) and Relu-type measurements.  Acknowledgments  This work has been supported by funding from the SNSF (grant 200021-156672), from the ERC un- der the European Unions FP7 Grant Agreement 307087-SPARCS and the European Union\u2019s Hori- zon 2020 Research and Innovation Program 714608-SMiLe, as well as by the French Agence Na- tionale de la Recherche under grant ANR-17-CE23-0023-01 PAIL. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU and the Chaire de recherche sur les mod`eles et sciences des donn\u00b4ees, Fondation CFM pour la Recherche-ENS. Part of this work was done while L\u00b4eo Miolane was visiting EPFL  References  J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse  superposition codes. IEEE Transactions on Information Theory, 63(8):4894-4927, 2017.  J. Barbier, M. Dia, N. Macris, and F. Krzakala. The mutual information in random linear estimation.  In 54th Annual Allerton Conf. on Communication, Control, and Computing, page 625, a.  Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove  replica formulas in bayesian inference. arXiv:1705.02780[v3], 2017.  Jean Barbier, Nicolas Macris, Mohamad Dia, and Florent Krzakala. Mutual information and opti-  mality of approximate message-passing in random linear estimation. arXiv:1701.05823, b.  M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications  to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785, 2011.  M. Bayati and A. Montanari. The lasso risk for gaussian matrices. IEEE Transactions on Informa-  tion Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Marc Lelarge, and Andrea Montanari. Universality in polytope phase transitions  and message passing algorithms. The Annals of Applied Probability, 25(2):753-822, 2015.  Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherrington-  kirkpatrick model. Communications in Mathematical Physics, 325(1):333366, 2014.  Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406, 2006. PH. TRANSITIONS, OPT. ERRORS AND AMP IN HIGH-D. GENERALIZED LINEAR MODELS  David Donoho and Andrea Montanari. High dimensional robust m-estimation: asymptotic variance via approximate message passing. Probability Theory and Related Fields, 166:935-969, 2016.  David L Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations  by linear programming. Proc. Nat. Acad. Sci., 102(27):9446-9451, 2005.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com-  pressed sensing. Proc. Nat. Acad. Sci., 106(45):18914-18919, Nov 2009.  N. El Karoui, D. Bean, P. J. Bickel, C. Lim, and B. Yu. On robust regression with high-dimensional  predictors. Proc. Nat. Acad. Sci., 110(36):14557, 2013.  Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of  networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.  Francesco Guerra and Fabio Lucio Toninelli. The thermodynamic limit in mean field spin glass  models. Communications in Mathematical Physics, 230(1):71-79, 2002.  Dongning Guo and Sergio Verd\u00b4u. Randomly spread cdma: Asymptotics via statistical physics.  IEEE Transactions on Information Theory, 51(6):1983-2010, June 2005. ISSN 0018-9448.  G\u00b4eza Gy\u00a8orgyi. First-order transition to perfect generalization in a neural network with binary  synapses. Physical Review A, 41(12):7097, 1990.  P. McCullagh. Generalized linear models. Euro. Journal of Operational Research, 16(3):285, 1984.  M. M\u00b4ezard, G. Parisi, and MA Virasoro. Spin glass theory and beyond. World Sci. Publish., 1987.  Marc M\u00b4ezard. The space of interactions in neural networks: Gardner\u2019s computation with the cavity  method. Journal of Physics A: Mathematical and General, 22(12):2181-2190, 1989.  John Ashworth Nelder and R Jacob Baker. Generalized linear models. Wiley Online Library, 1972.  Sundeep Rangan. Generalized approximate message passing for estimation with random linear  mixing. In IEEE ISIT, pages 2168-2172, July 2011.  G. Reeves and H. Pfister. The replica-symmetric prediction for compressed sensing with gaussian  matrices is exact. In Inf. Theory (ISIT), 2016 IEEE International Symposium on, page 665.  Sebastian H. Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from  examples. Phys. Rev. A, 45:6056-6091, Apr 1992.  Claude E. Shannon. A mathematical theory of communication. Bell Syst. Tech. J., 27:623, 1948.  Toshiyuki Tanaka. A statistical-mechanics approach to large-system analysis of cdma multiuser  detectors. IEEE Transactions on Information Theory, 48(11):2888-2910, Nov 2002.  Timothy L. H. Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a  rule. Rev. Mod. Phys., 65:499-556, Apr 1993.  Lenka Zdeborov\u00b4a and Florent Krzakala. Statistical physics of inference: thresholds and algorithms.  Advances in Physics, 65(5):453-552, 2016. "}, "Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data", "abstract": "We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and  accounts for potential serial dependence by including  block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods.  When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.", "pdf_url": "http://proceedings.mlr.press/v75/chernozhukov18a/chernozhukov18a.pdf", "keywords": ["Conformal inference", "permutation and randomization", "dependent data", "groups"], "reference": "Vineeth N. Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. Conformal Prediction for  Reliable Machine Learning. Morgan Kaufmann, Boston, 2014.  Peter J Bickel, Ya\u00b4acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and  dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.  Richard C Bradley. Introduction to strong mixing conditions, volume 1. Kendrick Press Heber City,  2007.  Business Media, 2013.  Peter J Brockwell and Richard A Davis. Time series: theory and methods. Springer Science &  Evgeny Burnaev and Vladimir Vovk. Efficiency of conformalized ridge regression. In Conference  on Learning Theory, pages 605-622, 2014.  Xiaohong Chen and H. White. Improved rates and asymptotic normality for nonparametric neural network estimators. IEEE Transactions on Information Theory, 45(2):682-691, Mar 1999. ISSN 0018-9448. doi: 10.1109/18.749011.  Xiaohong Chen, Jeffrey Racine, and Norman R Swanson. Semiparametric arx neural-network mod- els with an application to forecasting in\ufb02ation. IEEE Transactions on neural networks, 12(4): 674-683, 2001.  10   CONFORMAL INFERENCE FOR DEPENDENT DATA  Strong mixing is a mild condition on dependence and is satisfied by many stochastic processes. For example, it is well known that any stationary Markov chains that are Harris recurrent and ape- riodic are strong mixing. Many common serially dependent processes such as ARMA with i.i.d. innovations can also be shown to be strong mixing.  4. Conclusion  This paper extends the applicability of conformal inference to general settings that allow for time series data. Our results are developed within the general framework of randomization inference. Our method is based on a carefully-designed randomization approach based on groups of permutations, which exhibit a block structure to account for the potential serial dependence in the data. When the data are i.i.d. or more generally exchangeable, our method exhibits exact, model-free validity. When the exchangeability condition does not hold, finite-sample performance bounds can still be obtained under weak conditions on the conformity score as long as transformations of the data serve as meaningful approximations for a stationary series.  We gratefully acknowledge research support from the National Science Foundation. We are very grateful to three anonymous referees for helpful comments.  Acknowledgements  References  Vineeth N. Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. Conformal Prediction for  Reliable Machine Learning. Morgan Kaufmann, Boston, 2014.  Peter J Bickel, Ya\u00b4acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and  dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.  Richard C Bradley. Introduction to strong mixing conditions, volume 1. Kendrick Press Heber City,  2007.  Business Media, 2013.  Peter J Brockwell and Richard A Davis. Time series: theory and methods. Springer Science &  Evgeny Burnaev and Vladimir Vovk. Efficiency of conformalized ridge regression. In Conference  on Learning Theory, pages 605-622, 2014.  Xiaohong Chen and H. White. Improved rates and asymptotic normality for nonparametric neural network estimators. IEEE Transactions on Information Theory, 45(2):682-691, Mar 1999. ISSN 0018-9448. doi: 10.1109/18.749011.  Xiaohong Chen, Jeffrey Racine, and Norman R Swanson. Semiparametric arx neural-network mod- els with an application to forecasting in\ufb02ation. IEEE Transactions on neural networks, 12(4): 674-683, 2001. CONFORMAL INFERENCE FOR DEPENDENT DATA  Victor Chernozhukov, Chris Hansen, and Martin Spindler. hdm: High-dimensional metrics. R  Journal, 8(2):185-199, 2016.  Victor Chernozhukov, Kaspar W\u00a8uthrich, and Yinchu Zhu. An exact and robust conformal inference  method for counterfactual and synthetic controls. arXiv:1712.09089, 2017.  Mikhail Dashevskiy and Zhiyuan Luo. Network traffic demand prediction with confidence.  In Global Telecommunications Conference, 2008. IEEE GLOBECOM 2008. IEEE, pages 1-5. IEEE, 2008.  Mikhail Dashevskiy and Zhiyuan Luo. Time series prediction with performance guarantee. IET ISSN 1751-8628. doi: 10.1049/iet-com.2010.  Communications, 5(8):1044-1051, May 2011. 0121.  Ronald A. Fisher. The Design of Experiments. Oliver & Boyd, 1935.  James D. Hamilton. Time series: theory and methods. Springer Science & Business Media, 1994.  Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Busi-  ness Media, 2005.  Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71-96, 2014.  Jing Lei, James Robins, and Larry Wasserman. Distribution-free prediction sets. Journal of the  American Statistical Association, 108(501):278-287, 2013.  Jing Lei, Allesandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore  functional data. Annals of Mathematics and Artificial Intelligence, 74:29-43, 2015.  Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution- free predictive inference for regression. Journal of the American Statistical Association, (just- accepted), 2017.  Harris Papadopoulos, Volodya Vovk, and Alex Gammermam. Conformal prediction with neural networks. In Tools with Artificial Intelligence, 2007. ICTAI 2007. 19th IEEE International Con- ference on, volume 2, pages 388-395. IEEE, 2007.  Emmanuel Rio. Asymptotic Theory of Weakly Dependent Random Processes. Springer, 2017.  Joseph P. Romano. On the behavior of randomization tests without a group invariance assumption.  Journal of the American Statistical Association, 85(411):686-692, 1990. ISSN 01621459.  Donald B. Rubin. Bayesianly justifiable and relevant frequency calculations for the applied statisti-  cian. The Annals of Statistics, 12(4):1151-1172, 12 1984. doi: 10.1214/aos/1176346785.  Vladimir Vovk. Conditional validity of inductive conformal predictors. Machine Learning, 92(2):  349-376, Sep 2013. ISSN 1573-0565. doi: 10.1007/s10994-013-5355-6.  Vladimir. Vovk, Alex. Gammerman, and Glenn. Shafer. Algorithmic Learning in a Random World.  Springer, 2005. CONFORMAL INFERENCE FOR DEPENDENT DATA  Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. On-line predictive linear regression. The  Annals of Statistics, 37(3):1566-1590, 2009. CONFORMAL INFERENCE FOR DEPENDENT DATA  "}, "Nonstochastic Bandits with Composite Anonymous Feedback": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Nonstochastic Bandits with Composite Anonymous Feedback", "abstract": "We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.", "pdf_url": "http://proceedings.mlr.press/v75/cesa-bianchi18a/cesa-bianchi18a.pdf", "keywords": ["Nonstochastic bandits", "composite losses", "delayed feedback", "bandit convex optimization"], "reference": "J.D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit  online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Chamy Allenberg, Peter Auer, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In International Conference on Algorithmic Learning Theory, pages 229-243. Springer, 2006.  Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: price of past mistakes. In Advances in Neural Information Processing Systems, pages 784-792, 2015.  R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from  regret to policy regret. In Proc. 29th ICML, 2012.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41.1-41.14. Microtome, 2012.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and System  Sciences, 78(5):1404-1422, 2012.  Nicolo Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and coopera-  tion in nonstochastic bandits. In Conference on Learning Theory, pages 605-622, 2016.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Online learning with composite loss func-  tions. In Conference on Learning Theory, pages 1214-1231, 2014a.  Ofer Dekel, Elad Hazan, and Tomer Koren. The blinded bandit: Learning with adaptive feedback.  In Advances in Neural Information Processing Systems, pages 1610-1618, 2014b.  Scott Garrabrant, Nate Soares, and Jessica Taylor. Asymptotic convergence in online learning with  unbounded delays. arXiv preprint arXiv:1604.05280, 2016.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization,  2(3-4):157-325, 2016.  Pooria Joulani, Andras Gyorgy, and Csaba Szepesv\u00b4ari. Online learning under delayed feedback. In  International Conference on Machine Learning, pages 1453-1461, 2013.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms. In AAAI, volume 16, pages 1744-1750, 2016.  Daniel Khashabi, Kent Quanrud, and Amirhossein Taghvaei. Adversarial delays in online strongly-  convex optimization. arXiv preprint arXiv:1605.06201, 2016.  13   COMPOSITE ANONYMOUS FEEDBACK  References  J.D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit  online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Chamy Allenberg, Peter Auer, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In International Conference on Algorithmic Learning Theory, pages 229-243. Springer, 2006.  Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: price of past mistakes. In Advances in Neural Information Processing Systems, pages 784-792, 2015.  R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from  regret to policy regret. In Proc. 29th ICML, 2012.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41.1-41.14. Microtome, 2012.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and System  Sciences, 78(5):1404-1422, 2012.  Nicolo Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and coopera-  tion in nonstochastic bandits. In Conference on Learning Theory, pages 605-622, 2016.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Online learning with composite loss func-  tions. In Conference on Learning Theory, pages 1214-1231, 2014a.  Ofer Dekel, Elad Hazan, and Tomer Koren. The blinded bandit: Learning with adaptive feedback.  In Advances in Neural Information Processing Systems, pages 1610-1618, 2014b.  Scott Garrabrant, Nate Soares, and Jessica Taylor. Asymptotic convergence in online learning with  unbounded delays. arXiv preprint arXiv:1604.05280, 2016.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization,  2(3-4):157-325, 2016.  Pooria Joulani, Andras Gyorgy, and Csaba Szepesv\u00b4ari. Online learning under delayed feedback. In  International Conference on Machine Learning, pages 1453-1461, 2013.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms. In AAAI, volume 16, pages 1744-1750, 2016.  Daniel Khashabi, Kent Quanrud, and Amirhossein Taghvaei. Adversarial delays in online strongly-  convex optimization. arXiv preprint arXiv:1605.06201, 2016. COMPOSITE ANONYMOUS FEEDBACK  John Langford, Alexander J Smola, and Martin Zinkevich. Slow learners are fast. Advances in  Neural Information Processing Systems, 22:2331-2339, 2009.  Travis Mandel, Yun-En Liu, Emma Brunskill, and Zoran Popovic. The queue method: Handling  delay, heuristics, prior data, and evaluation in bandits. In AAAI, pages 2849-2856, 2015.  Chris Mesterharm. On-line learning with delayed label feedback. In Algorithmic Learning Theory,  pages 399-413. Springer, 2005.  Gergely Neu, Andras Antos, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online Markov decision pro- cesses under bandit feedback. In Advances in Neural Information Processing Systems 23, pages 1804-1812. Curran Associates, Inc., 2010.  Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with de-  layed anonymous feedback. arXiv preprint arXiv:1709.06853, 2017.  Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. In Advances in Neural  Information Processing Systems, pages 1270-1278, 2015.  A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the Fourteenth International Conference on Artificial Intelli- gence and Statistics, pages 636-642, 2011.  O. Shamir and L. Szlak. Online learning with local permutations and delayed feedback. In Proc.  34th ICML, 2017.  pages 1523-1551, 2015.  Ohad Shamir. On the complexity of bandit linear optimization. In Conference on Learning Theory,  Claire Vernade, Olivier Capp\u00b4e, and Vianney Perchet. Stochastic bandit models for delayed conver- sions. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI, 2017.  Marcelo J Weinberger and Erik Ordentlich. On delayed prediction of individual sequences. IEEE  Transactions on Information Theory, 48(7):1959-1976, 2002.  "}, "Lower Bounds for Higher-Order Convex Optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Lower Bounds for Higher-Order Convex Optimization", "abstract": "State-of-the-art methods in mathematical optimization employ higher-order derivative information. We explore the limitations of higher-order optimization and prove that even for convex optimization, a polynomial dependence on the approximation guarantee and higher-order smoothness parameters is necessary. This refutes the hope that higher-order smoothness and higher-order derivatives can lead to dimension free polynomial time algorithms for convex optimization. As a special case, we show Nesterov\u2019s accelerated cubic regularization method and higher-order methods to be nearly tight.", "pdf_url": "http://proceedings.mlr.press/v75/agarwal18a/agarwal18a.pdf", "keywords": ["Convex Optimization", "Second-Order Optimization", "Higher-Order Optimization", "Newton\u2019s method", "Lower Bounds"], "reference": "J Abernethy and E Hazan. Faster convex optimization: Simulated annealing with an ef\ufb01cient uni-  versal barrier. arxiv 1507.02528, 2015.  Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar. Information- theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, pages 1\u20139, 2009.  Naman Agarwal, Brian Bullins, and Elad Hazan. Second order stochastic optimization for machine  learning in linear time. arXiv preprint arXiv:1602.03943, 2016.  12   LOWER BOUNDS FOR HIGHER-ORDER CONVEX OPTIMIZATION  Theorem 13 For any integer k, any T > 5k, \u03b4 \u2208 [0, 1], and any k-order (potentially randomized algorithm), there exists a k-differentiable convex function f \u2020 : Bd \u2192 R for d = \u2126(T 3 log(T 2/\u03b4)), such that with probability at least 1 \u2212 \u03b4 (over the randomness of the algorithm) for T steps of the algorithm every point y queried by the algorithm is such that  f \u2020(y) \u2265 min x\u2208Bd  f \u2020(x) +  1 \u221a 2  T  .  Moreover the function f \u2020 is guaranteed to be k-differentiable with Lipschitz constants Li bounded as  \u2200 i \u2264 k Li+1 \u2264 (20kT 2.5)i.  Due to space constraints the proof of Theorem 13 is included in the "}, "Log-concave sampling: Metropolis-Hastings algorithms are fast!": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Log-concave sampling: Metropolis-Hastings algorithms are fast!", "abstract": "We consider the problem of sampling from a strongly log-concave density in $\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by running a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step to ensure the correct stationary distribution. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds reveal that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most $\\delta$ for a density with condition number $\\kappa$, we show that MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as compared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established in past work on ULA.  We also demonstrate the gains of MALA over ULA for weakly log-concave densities.  Furthermore, we derive mixing time bounds for a zeroth-order method Metropolized random walk (MRW) and show that it mixes $\\mathcal{O}(\\kappa d)$ slower than MALA.", "pdf_url": "http://proceedings.mlr.press/v75/dwivedi18a/dwivedi18a.pdf", "keywords": ["MCMC", "sampling", "random walk", "Metropolis-adjusted Langevin algorithm", "convergence"], "reference": "Claude JP B\u00b4elisle, H Edwin Romeijn, and Robert L Smith. Hit-and-run algorithms for generating  multivariate distributions. Mathematics of Operations Research, 18(2):255-266, 1993.  Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the MALA algorithm. IMA Journal  of Numerical Analysis, 33(1):80-110, 2012.  Steve Brooks, Andrew Gelman, Galin L Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte  Carlo. Chapman and Hall/CRC, 2011.  3   MALA  ficient conditions for exponential convergence of the Langevin diffusion and its discretizations, with and without Metropolis-adjustment. However, they considered the distributions with f (x) = (cid:107)x(cid:107)\u03b1 2 and proved geometric convergence of ULA and MALA under some specific conditions. In a more general setting, Bou-Rabee and Hairer (2012) and Eberle (2014) derived non-asymptotic mixing time bounds for MALA. However, all these bounds are non-explicit in the case of logconcave sampling, and so makes it difficult to extract an explicit dependence in terms of the dimension d and error tol- erance \u03b4. In particular, Eberle (2014) made significant contribution to establishing the accept-reject rate of MALA by assuming differentiability of the target function to fourth order, but its final mix- ing rate is only explicit when the sampling domain is contained in a ball with constant radius. A precise characterization of this dependence is needed if one wants to make quantitative comparisons with other algorithms, including ULA and other Langevin-type schemes. With this context, one of the main contributions of our paper is to provide an explicit upper bound on the mixing time of the MALA algorithm.  This work contains two main results, both having to do with the mixing times of MCMC methods for sampling. As described above, our first and primary contribution is an explicit analysis of the mixing time of Metropolis adjusted Langevin Algorithm (MALA). A second contribution is to use similar techniques to analyze a zeroth-order method called Metropolized random walk (MRW) and derive a explicit non-asymptotic mixing time bound for it. Unlike the ULA, these methods make use of the Metropolis-hastings accept-reject step and consequently converge to the target distributions in the limit of infinite steps. Here we provide explicit non-asymptotic mixing time bounds for MALA and MRW and show that MALA converges significantly faster than ULA. In particular, we show that if the density is strongly log-concave and smooth, the \u03b4-mixing time for MALA scales as \u03bad log(1/\u03b4) which is significantly faster than ULA\u2019s convergence rate of order \u03ba2d/\u03b42. We also show that MRW mixes O (\u03bad) slowly when compared to MALA. Furthermore, if the density is weakly log-concave, we show that MALA converges in O (cid:0)d2/\u03b41.5(cid:1) time in comparison to the O (cid:0)d3/\u03b44(cid:1) mixing time for ULA. These results are summarized in Table 1.  Acknowledgments  This work was supported by Office of Naval Research grant DOD ONR-N00014 to MJW, and by ARO W911NF1710005, NSF-DMS 1613002 and the Center for Science of Information (CSoI), US NSF Science and Technology Center, under grant agreement CCF-0939370 to BY. In addition, MJW was partially supported by National Science Foundation grant NSF-DMS-1612948, and RD was partially supported by the Berkeley Fellowship.  References  Claude JP B\u00b4elisle, H Edwin Romeijn, and Robert L Smith. Hit-and-run algorithms for generating  multivariate distributions. Mathematics of Operations Research, 18(2):255-266, 1993.  Nawaf Bou-Rabee and Martin Hairer. Nonasymptotic mixing of the MALA algorithm. IMA Journal  of Numerical Analysis, 33(1):80-110, 2012.  Steve Brooks, Andrew Gelman, Galin L Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte  Carlo. Chapman and Hall/CRC, 2011. MALA  Random walk  Strongly log-concave  Weakly log-concave  ULA (Cheng and Bartlett, 2017)  O  ULA (Dalalyan, 2016)  MRW  MALA  (cid:19)  (cid:18) d\u03ba2 log((log \u03b2)/\u03b4) \u03b42 (cid:18) d\u03ba2 log2(\u03b2/\u03b4) \u03b42  (cid:19)  O  (cid:18)  O  d2\u03ba2 log  (cid:19)(cid:19)  (cid:18) \u03b2 \u03b4  (cid:18)  O  max (cid:8)d\u03ba, d0.5\u03ba1.5(cid:9) log  (cid:19)(cid:19)  (cid:18) \u03b2 \u03b4  \u02dcO  \u02dcO  (cid:19)  (cid:19)  (cid:18) dL2 \u03b46 (cid:18) d3L2 \u03b44 (cid:18) d4 L2.5 \u03b41.5 (cid:18) d2 L1.5 \u03b41.5  (cid:19)  (cid:19)  \u02dcO  \u02dcO  Table 1. Scalings of upper bounds on \u03b4-mixing time for different random walks in Rd with target \u03c0 \u221d e\u2212f . In the second column, we consider smooth and strongly log-concave densities, and report the bounds from a \u03b2-warm start for densities such that mId (cid:22) \u22072f (x) (cid:22) LId for any x \u2208 Rd and use \u03ba := L/m to denote the condition number of the density. The big-O notation hides universal constants. In the last column, we summarize the scaling for weakly log-concave smooth densities: 0 (cid:22) \u22072f (x) (cid:22) LId for all x \u2208 Rd. For this case, the \u02dcO notation is used to track scaling only with respect to d, \u03b4 and L and ignore dependence on the starting distribution and a few other parameters.  S\u00b4ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with  projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped Langevin  MCMC: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.  Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin  algorithm. arXiv preprint arXiv:1605.01559, 2016.  Alain Durmus, Eric Moulines, and Marcelo Pereyra. Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau. arXiv preprint arXiv:1612.07471, 2016.  Martin Dyer, Alan Frieze, and Ravi Kannan. A random polynomial-time algorithm for approximating  the volume of convex bodies. Journal of the ACM (JACM), 38(1):1-17, 1991.  Andreas Eberle. Error bounds for metropolis-hastings algorithms applied to perturbations of gaussian  measures in high dimensions. The Annals of Applied Probability, 24(1):337-377, 2014.  Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of  the Royal Statistical Society. Series B (Methodological), pages 549-603, 1994. Ravi Kannan, L\u00b4aszl\u00b4o Lov\u00b4asz, and Mikl\u00b4os Simonovits. Isoperimetric problems for convex bodies and  a localization lemma. Discrete & Computational Geometry, 13(1):541-559, 1995.  L\u00b4aszl\u00b4o Lov\u00b4asz. Hit-and-run mixes fast. Mathematical Programming, 86(3):443-461, 1999.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. The mixing rate of Markov chains, an isoperimetric inequality, and computing the volume. In Proceedings of 31st Annual Symposium on Foundations of Computer Science, 1990, pages 346-354. IEEE, 1990.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random Structures & Algorithms, 4(4):359-412, 1993.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing, 35(4):  985-1005, 2006.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms.  Random Structures & Algorithms, 30(3):307-358, 2007.  Kerrie L Mengersen, Richard L Tweedie, et al. Rates of convergence of the Hastings and Metropolis  algorithms. The Annals of Statistics, 24(1):101-121, 1996.  Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2  G Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378-384, 1981.  Marcelo Pereyra. Proximal Markov chain Monte Carlo algorithms. Statistics and Computing, 26(4):  (11), 2011.  745-760, 2016.  Gareth O Roberts and Osnat Stramer. Langevin diffusions and Metropolis-Hastings algorithms.  Methodology and computing in applied probability, 4(4):337-357, 2002.  Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and  their discrete approximations. Bernoulli, pages 341-363, 1996a.  Gareth O Roberts and Richard L Tweedie. Geometric convergence and central limit theorems for  multidimensional Hastings and Metropolis algorithms. Biometrika, 83(1):95-110, 1996b.  Gareth O Roberts, Jeffrey S Rosenthal, et al. General state space Markov chains and MCMC algo-  rithms. Probability Surveys, 1:20-71, 2004.  Santosh Vempala. Geometric random walks: a survey. Combinatorial and Computational Geometry,  52(573-612):2, 2005.  Tatiana Xifara, Chris Sherlock, Samuel Livingstone, Simon Byrne, and Mark Girolami. Langevin diffusions and the Metropolis-adjusted Langevin algorithm. Statistics & Probability Letters, 91: 14-19, 2014.  MALA "}, "Incentivizing Exploration by Heterogeneous Users": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Incentivizing Exploration by Heterogeneous Users", "abstract": "We consider the problem of incentivizing exploration with heterogeneous agents. In this problem, $N$ bandit arms provide vector-valued outcomes equal to an unknown arm-specific attribute vector, perturbed by independent noise.Agents arrive sequentially and choose arms to pull based on their own private and heterogeneous linear utility functions over attributes and the estimates of the arms\u2019 attribute vectors derived from observations of other agents\u2019 past pulls. Agents are myopic and selfish and thus would choose the arm with maximum estimated utility. A principal, knowing only the distribution from which agents\u2019 preferences are drawn, but not the specific draws, can offer arm-specific incentive payments to encourage agents to explore underplayed arms. The principal seeks to minimize the total expected cumulative regret incurred by agents relative to their best arms, while also making a small expected cumulative payment. We propose an algorithm that incentivizes arms played infrequently in the past whose probability of being played in the next round would be small without incentives. Under the assumption that each arm is preferred by at least a fraction $p > 0$ of agents, we show that this algorithm achieves expected cumulative regret of $O (N \\e^{2/p} + N \\log^3(T))$, using expected cumulative payments of $O(N^2 \\e^{2/p})$. If $p$ is known or the distribution over agent preferences is discrete, the exponential term $\\e^{2/p}$ can be replaced with suitable polynomials in $N$ and $1/p$. For discrete preferences, the regret\u2019s dependence on $T$ can be eliminated entirely, giving constant (depending only polynomially on $N$ and $1/p$) expected regret and payments. This constant regret stands in contrast to the $\\Theta(\\log(T))$ dependence of regret in standard multi-armed bandit problems. It arises because even unobserved heterogeneity in agent preferences causes exploitation of arms to also explore arms fully; succinctly, heterogeneity provides free exploration.", "pdf_url": "http://proceedings.mlr.press/v75/chen18a/chen18a.pdf", "keywords": ["Incentivizing Exploration", "Multi-Armed Bandits", "Social Learning"], "reference": "Muhammad Abulaish, Mohammad Najmud Doja, Tanvir Ahmad, et al. Feature and opinion min- ing for customer review summarization. In Proceedings of the 3rd International Conference on Pattern Recognition and Machine Intelligence, pages 219-224. Springer, 2009.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In Proceedings of the 15th ACM conference on Economics and Computation (EC), pages 5-22. ACM, 2014.  Li Han, David Kempe, and Ruixin Qiang. Incentivizing exploration with heterogeneous value of In Proceedings of the 11th International Conference on Web and Internet Economics  money. (WINE), pages 370-383. Springer, 2015.  Ilan Kremer, Yishay Mansour, and Motty Perry. Implementing the \u201cwisdom of the crowd\u201d. Journal  of Political Economy, 122(5):988-1012, 2014.  6. This will lead to a different dependence on N in the regret bound as well as the payment bound  12   INCENTIVIZING EXPLORATION BY HETEROGENEOUS USERS  5. Conclusion  We study the problem of incentivizing exploration with heterogeneous user preferences. We pro- posed an algorithm that achieves expected cumulative regret O(N e2/p+N log3(T )), using expected cumulative payments of O(N 2e2/p). It is possible to improve these bounds to polynomial (in N and 1/p) when p is known or the preference distribution is discrete. In fact, we conjecture that this should be possible even in the full generality of our model. As a first step towards such a polynomial bound, we can obtain an exponential dependence on 1/(pN ) by changing the probability threshold 6, which gives polynomial dependence unless some arm has a much smaller fraction to be of the population preferring it.  1 N log(s)  Taking this goal one step further, we would like to develop algorithms that do not require all arms to be preferred by a strictly positive fraction of agents. An alternate algorithm might only incentivize an arm if its estimated attribute vector is close enough to a Pareto frontier. The regret will then be \u2126(log(T )) when at least one arm falls below the Pareto frontier, as we no longer have free exploration of all arms. It is likely that a bound will deteriorate as the number of such unpreferred arms increases.  Finally, it would be desirable to generalize to utility functions beyond inner products. We believe that similar results hold for arbitrary Lipschitz-continuous utility functions of the arm\u2019s attribute vector, and that only minor modifications are necessary to the algorithm and proofs.  PF and BC were partially supported by NSF CMMI-1254298 and AFOSR FA9550-15-1-0038. DK was supported in part by NSF grant 1423618.  Acknowledgments  References  Muhammad Abulaish, Mohammad Najmud Doja, Tanvir Ahmad, et al. Feature and opinion min- ing for customer review summarization. In Proceedings of the 3rd International Conference on Pattern Recognition and Machine Intelligence, pages 219-224. Springer, 2009.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In Proceedings of the 15th ACM conference on Economics and Computation (EC), pages 5-22. ACM, 2014.  Li Han, David Kempe, and Ruixin Qiang. Incentivizing exploration with heterogeneous value of In Proceedings of the 11th International Conference on Web and Internet Economics  money. (WINE), pages 370-383. Springer, 2015.  Ilan Kremer, Yishay Mansour, and Motty Perry. Implementing the \u201cwisdom of the crowd\u201d. Journal  of Political Economy, 122(5):988-1012, 2014.  6. This will lead to a different dependence on N in the regret bound as well as the payment bound INCENTIVIZING EXPLORATION BY HETEROGENEOUS USERS  Chris J. Lintott, Kevin Schawinski, An \u02daAe Slosar, Kate Land, Steven Bamford, Daniel Thomas, M. Jordan Raddick, Robert C. Nichol, Alex Szalay, Dan Andreescu, Phil Murray, and Jan Vanden- berg. Galaxy zoo: morphologies derived from visual inspection of ga laxies from the sloan digital sky survey. Monthly Notices of the Royal Astronomical Society, 389(3):1179-1189, September 2008.  Chien-Liang Liu, Wen-Hoar Hsaio, Chia-Hoang Lee, Gen-Chi Lu, and Emery Jou. Movie rating IEEE Transactions on Systems, Man, and  and review summarization in mobile environment. Cybernetics, Part C (Applications and Reviews), 42(3):397-407, 2012.  Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit exploration. In Proceedings of the 16th ACM Conference on Economics and Computation (EC), pages 565-582. ACM, 2015.  Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian explo- ration: Incentivizing exploration in bayesian games. In Proceedings of the 17th ACM Conference on Economics and Computation (EC), 2016.  Yishay Mansour, Aleksandrs Slivkins, and Zhiwei Steven Wu. Bayesian exploration: Incentivizing exploration in bayesian games. In Proceedings of the 9th Innovations in Theoretical Computer Science (ITCS) conference, 2018.  Sven Schmit and Carlos Riquelme. Human interaction with recommendation systems: On bias and exploration. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.  Aleksandrs Slivkins. Incentivizing exploration via information asymmetry. ACM Crossroads, 24  (1):38-41, 2017.  Brian L. Sullivan, Christopher L. Wood, Marshall J. Iliff, Rick E. Bonney, Daniel Fink, and Steve Kelling. ebird: A citizen-based bird observation network in the biological sciences. Biological Conservation, 142(10):2282-2292, 2009.  Ye-Yi Wang and Sibel Yaman. Product or service review summarization using attributes, July 1  2010. US Patent App. 12/346,903.  Yexiang Xue, Bistra N. Dilkina, Theodoros Damoulas, Daniel Fink, Carla P. Gomes, and Steve Kelling. Improving your chances: Boosting citizen science discovery. In Proceedings of the 1st Conference on Human Computation and Crowdsourcing, 2013.  Shengjia Zhao, Enze Zhou, Ashish Sabharwal, and Stefano Ermon. Adaptive concentration inequal- ities for sequential decision problems. In Proceedings of the 30th Advances In Neural Information Processing Systems (NIPS), pages 1343-1351, 2016.  "}, "Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms", "abstract": "We study the problem of robustly learning multi-dimensional histograms.  A $d$-dimensional function $h: D \\to \\R$ is called a $k$-histogram if there exists a partition of the  domain $D \\subseteq \\R^d$ into $k$ axis-aligned rectangles such that $h$ is constant within each such rectangle. Let $f: D \\to \\R$ be a $d$-dimensional probability density function  and suppose that $f$ is $\\mathrm{OPT}$-close, in $L_1$-distance,  to an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in $L_1$-distance. We give an algorithm for this learning  problem that uses  $n = \\tilde{O}_d(k/\\eps^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any fixed dimension, our algorithm has optimal sample complexity, up to logarithmic factors, and runs in near-linear time. Prior to our work, the time complexity of the $d=1$ case was well-understood,  but significant gaps in our understanding remained even for $d=2$.", "pdf_url": "http://proceedings.mlr.press/v75/diakonikolas18a/diakonikolas18a.pdf", "keywords": [], "reference": "J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly- In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete linear time. Algorithms, SODA 2017, pages 1278-1289, 2017. Available at https://arxiv.org/abs/1506.00671.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S. Chaudhuri, R. Motwani, and V. R. Narasayya. Random sampling for histogram construction:  How much is enough? In SIGMOD Conference, pages 436-447, 1998.  G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for massive data: Samples, histograms, wavelets, sketches. Found. Trends databases, 4:1-294, 2012. ISSN 1931-7883.  A. De, I. Diakonikolas, and R. Servedio. Learning from satisfying assignments. In Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 478-497, 2015.  L. Devroye and L. Gy\u00f6rfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  1985.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  L. Devroye and G. Lugosi. Bin width selection in multivariate histograms by the combinatorial  method. Test, 13(1):129-145, 2004.  I. Diakonikolas, E. Grigorescu, J. Li, A. Natarajan, K. Onak, and L. Schmidt. Communication- efficient distributed learning of discrete distributions. In Advances in Neural Information Pro- cessing Systems 30 (NIPS 2017), pages 6394-6404, 2017.  D. Freedman and P. Diaconis. On the histogram as a density estimator:l2 theory. Zeitschrift f\u00fcr  Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57(4):453-476, 1981.  A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and M. Strauss. Surfing wavelets on streams: One-pass  summaries for approximate aggregate queries. In VLDB, 2001.  14   LEARNING MULTIDIMENSIONAL HISTOGRAMS  References  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly- In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete linear time. Algorithms, SODA 2017, pages 1278-1289, 2017. Available at https://arxiv.org/abs/1506.00671.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S. Chaudhuri, R. Motwani, and V. R. Narasayya. Random sampling for histogram construction:  How much is enough? In SIGMOD Conference, pages 436-447, 1998.  G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine. Synopses for massive data: Samples, histograms, wavelets, sketches. Found. Trends databases, 4:1-294, 2012. ISSN 1931-7883.  A. De, I. Diakonikolas, and R. Servedio. Learning from satisfying assignments. In Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 478-497, 2015.  L. Devroye and L. Gy\u00f6rfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  1985.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  L. Devroye and G. Lugosi. Bin width selection in multivariate histograms by the combinatorial  method. Test, 13(1):129-145, 2004.  I. Diakonikolas, E. Grigorescu, J. Li, A. Natarajan, K. Onak, and L. Schmidt. Communication- efficient distributed learning of discrete distributions. In Advances in Neural Information Pro- cessing Systems 30 (NIPS 2017), pages 6394-6404, 2017.  D. Freedman and P. Diaconis. On the histogram as a density estimator:l2 theory. Zeitschrift f\u00fcr  Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57(4):453-476, 1981.  A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and M. Strauss. Surfing wavelets on streams: One-pass  summaries for approximate aggregate queries. In VLDB, 2001. LEARNING MULTIDIMENSIONAL HISTOGRAMS  A. C. Gilbert, S. Guha, P. Indyk, Y. Kotidis, S. Muthukrishnan, and Martin Strauss. Fast, small-space  algorithms for approximate histogram maintenance. In STOC, pages 389-398, 2002.  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  S. Guha, N. Koudas, and K. Shim. Approximation and streaming algorithms for histogram con-  struction problems. ACM Trans. Database Syst., 31(1):396-438, 2006.  P. Indyk, R. Levi, and R. Rubinfeld. Approximating and Testing k-Histogram Distributions in Sub-  linear Time. In PODS, pages 15-22, 2012.  H. V. Jagadish, Nick Koudas, S. Muthukrishnan, Viswanath Poosala, Kenneth C. Sevcik, and Torsten Suel. Optimal histograms with quality guarantees. In VLDB, pages 275-286, 1998.  M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. On the learnability of  discrete distributions. In Proc. 26th STOC, pages 273-282, 1994.  J. Klemela. Multivariate histograms with data-dependent partitions. Statistica Sinica, 19(1):159-  176, 2009.  G. Lugosi and A. Nobel. Consistency of data-driven histogram methods for density estimation and  classification. Ann. Statist., 24(2):687-706, 04 1996.  S Muthukrishnan, Viswanath Poosala, and Torsten Suel. On rectangular partitionings in two dimen-  sions: Algorithms, complexity and applications. In ICDT, 1999.  K. Pearson. Contributions to the mathematical theory of evolution. ii. skew variation in homoge- neous material. Philosophical Trans. of the Royal Society of London, 186:343-414, 1895. doi: 10.1098/rsta.1895.0010.  D. W. Scott. On optimal and data-based histograms. Biometrika, 66(3):605-610, 1979.  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  1992.  B. W. Silverman. Density Estimation. Chapman and Hall, London, 1986.  N. Thaper, S. Guha, P. Indyk, and N. Koudas. Dynamic multidimensional histograms. In SIGMOD  Conference, pages 428-439, 2002.  R. Willett and R. D. Nowak. Multiscale poisson intensity and density estimation. IEEE Transactions  on Information Theory, 53(9):3171-3187, 2007. LEARNING MULTIDIMENSIONAL HISTOGRAMS  "}, "Time-Space Tradeoffs for Learning Finite Functions from Random Evaluations, with Applications to Polynomials": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Time-Space Tradeoffs for Learning Finite Functions from Random Evaluations, with Applications to Polynomials", "abstract": "We develop an extension of recent analytic methods for  obtaining time-space tradeoff lower bounds for problems of learning  from uniformly random labelled examples.  With our methods we can obtain bounds for learning concept classes of finite functions from random evaluations even when the sample space of random inputs can be significantly smaller than the concept class of functions and the function values can be from an arbitrary finite set. At the core of our results, we reduce the time-space complexity of learning from random evaluations to the question of how much the corresponding evaluation matrix amplifies the 2-norms of \u201calmost uniform\u201d probability distributions. To analyze the latter, we formulate it as a semidefinite program, and we analyze its dual.   In order to handle function values from arbitrary finite sets, we apply this norm amplification analysis to complex matrices. As applications that follow from our new techniques, we show that any algorithm that learns $n$-variate  polynomial functions of degree at most $d$ over $\\mathbb{F}_2$ with success at least $2^{-O(n)}$ from evaluations on randomly chosen inputs either requires space $\\Omega(nm/d)$ or $2^{\\Omega(n/d)}$ time where $m=(n/d)^{\\Theta(d)}$ is the dimension of the space of such polynomials.   These bounds are asymptotically optimal for polynomials of arbitrary constant degree since they match the tradeoffs achieved by natural learning algorithms for the problems. We extend these results to learning polynomials of degree at most $d$ over any odd prime field $\\mathbb{F}_p$ where we show that $\\Omega((mn/d)\\log p)$ space or time $p^{\\Omega(n/d)}$ is required. To derive our bounds for learning polynomials over finite fields, we show that an analysis of the dual of the corresponding semidefinite program follows from an understanding of the distribution of the bias of all degree $d$ polynomials with respect to uniformly random inputs.", "pdf_url": "http://proceedings.mlr.press/v75/beame18a/beame18a.pdf"}, "Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability", "abstract": "We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \\textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\\varepsilon$-neighborhood of this particular optimum within a short \\textit{recurrence time}; (2) it enters this $\\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \\textit{escape time}. We call this phenomenon \\textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality.", "pdf_url": "http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf", "keywords": [], "reference": "parameter. In ICML, 2017.  Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. Preprint, February 2018.  URL https://arxiv.org/abs/1708.08694.  Nils Berglund and Barbara Gentz. Geometric singular perturbation theory for stochastic differential  equations. J. Differential Equations, 191:1-54, 2003.  Rajendra Bhatia. Matrix Analysis. Springer, 1997.  Vivek S Borkar and Sanjoy K Mitter. A strong approximation theorem for stochastic recursive  algorithms. Journal of Optimization Theory and Applications, 100(3):499-513, 1999.  Anton Bovier and Frank Den Hollander. Metastability: A Potential-Theoretic Approach. Springer,  2016.  Anton Bovier, Michael Eckhoff, V\u00b4eronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes I: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society, 6(4):399-424, 2004.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dConvex until proven guilty:\u201d  dimension-free acceleration of gradient descent for non-convex functions. In ICML, 2017.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Torgny Lindvall. Lectures on the Coupling Method. Dover Publications, Mineola, NY, 1992.  Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.  Annals of Statistics, 2016. To appear.  Peter M\u00a8orters and Yuval Peres. Brownian Motion, volume 30. Cambridge University Press, 2010.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Science  & Business Media, 2013.  Press, 2004.  Enzo Olivieri and Maria Eul\u00b4alia Vares. Large Deviations and Metastability. Cambridge University  13   EMPIRICAL METASTABILITY OF LANGEVIN ALGORITHM  Acknowledgments  The authors would like to thank Matus Telgarsky for many enlightening discussions. The work of Belinda Tzen and Maxim Raginsky is supported in part by the NSF under CAREER award CCF-1254041, and in part by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370. Tengyuan Liang is supported by George C. Tiao Faculty Fellowship in Data Science Research.  References  parameter. In ICML, 2017.  Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex  Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. Preprint, February 2018.  URL https://arxiv.org/abs/1708.08694.  Nils Berglund and Barbara Gentz. Geometric singular perturbation theory for stochastic differential  equations. J. Differential Equations, 191:1-54, 2003.  Rajendra Bhatia. Matrix Analysis. Springer, 1997.  Vivek S Borkar and Sanjoy K Mitter. A strong approximation theorem for stochastic recursive  algorithms. Journal of Optimization Theory and Applications, 100(3):499-513, 1999.  Anton Bovier and Frank Den Hollander. Metastability: A Potential-Theoretic Approach. Springer,  2016.  Anton Bovier, Michael Eckhoff, V\u00b4eronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes I: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society, 6(4):399-424, 2004.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. \u201dConvex until proven guilty:\u201d  dimension-free acceleration of gradient descent for non-convex functions. In ICML, 2017.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017.  Torgny Lindvall. Lectures on the Coupling Method. Dover Publications, Mineola, NY, 1992.  Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.  Annals of Statistics, 2016. To appear.  Peter M\u00a8orters and Yuval Peres. Brownian Motion, volume 30. Cambridge University Press, 2010.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Science  & Business Media, 2013.  Press, 2004.  Enzo Olivieri and Maria Eul\u00b4alia Vares. Large Deviations and Metastability. Cambridge University EMPIRICAL METASTABILITY OF LANGEVIN ALGORITHM  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via Stochastic  Gradient Langevin Dynamics: a nonasymptotic analysis. In COLT, 2017.  Max Welling and Yee Whye Teh. Bayesian learning via Stochastic Gradient Langevin Dynamics.  In ICML, 2011.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of Stochastic Gradient  Langevin Dynamics. In COLT, 2017. EMPIRICAL METASTABILITY OF LANGEVIN ALGORITHM  "}, "Hardness of Learning Noisy Halfspaces using Polynomial Thresholds": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Hardness of Learning Noisy Halfspaces using Polynomial Thresholds", "abstract": "We prove the hardness of weakly learning halfspaces in the presence of adversarial noise using polynomial threshold functions (PTFs). In particular, we prove that for any constants $d \\in \\mathbb{Z}^+$ and $\\eps > 0$, it is NP-hard to decide: given a set of $\\{-1,1\\}$-labeled points in $\\mathbb{R}^n$  whether (YES Case) there exists a halfspace that classifies $(1-\\eps)$-fraction of the points correctly, or (NO Case) any degree-$d$ PTF classifies at most $(1/2 + \\eps)$-fraction of the points correctly. This strengthens to all constant degrees the previous NP-hardness of learning using degree-$2$ PTFs shown by Diakonikolas et al. (2011). The latter result had remained the only progress over the works of Feldman et al. (2006) and Guruswami et al. (2006) ruling out weakly proper learning adversarially noisy halfspaces.", "pdf_url": "http://proceedings.mlr.press/v75/bhattacharyya18a/bhattacharyya18a.pdf", "keywords": ["Learning", "Halfspaces", "PTFs", "Hardness"], "reference": "M. Alekhnovich, M. Braverman, V. Feldman, A. R. Klivans, and T. Pitassi. The complexity of  properly learning simple concept classes. J. Comp. Sys. Sci., 74(1):16-34, 2008.  E. Amaldi and V. Kann. On the approximability of minimizing nonzero variables or unsatisfied  relations in linear systems. Theoret. Comput. Sci., 209(1-2):237-260, 1998.  B. Applebaum, B. Barak, and D. Xiao. On basing lower-bounds for learning on worst-case as- sumptions. In Proc. 49th Annual IEEE Symposium on Foundations of Computer Science, pages 211-220, 2008.  R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random  projection. Machine Learning, 63(2):161-182, 2006.  P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for efficiently learning linear  separators with noise. J. ACM, 63(6):50:1-50:27, 2017.  S. Ben-David, N. Eiron, and P. M. Long. On the difficulty of approximately maximizing agree- ments. J. Comp. Sys. Sci., 66(3):496 - 514, 2003. ISSN 0022-0000. doi: http://dx.doi.org/ 10.1016/S0022-0000(03)00038-2. URL http://www.sciencedirect.com/science/ article/pii/S0022000003000382.  A. Blum and R. Kannan. Learning an intersection of a constant number of halfspaces over a uniform  distribution. J. Comp. Sys. Sci., 54(2):371-380, 1997.  A. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. In Machine Learning: From Theory to Applications - Cooperative Research at Siemens and MIT, pages 9-28, 1993.  A. Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial-time algorithm for learning noisy  linear threshold functions. Algorithmica, 22(1):35-52, 1998.  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statisti-  cal query model. J. ACM, 50(4):506-519, 2003.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-  Chervonenkis dimension. J. ACM, 36(4):929-965, 1989.  N. H. Bshouty and L. Burroughs. Maximizing agreements and coagnostic learning. Theoret. Com-  put. Sci., 350(1):24-39, 2006.  A. Carbery and J. Wright. Distributional and lq norm inequalities for polynomials over convex  bodies in Rn. Math. Res. Lett., 8(3):233-248, 2001.  E. Cohen. Learning noisy perceptrons by a perceptron in polynomial time. In Proc. 38th Annual  IEEE Symposium on Foundations of Computer Science, pages 514-523, 1997.  C. Cortes and V. Vapnik. Support-Vector networks. Machine Learning, 20(3):273-297, 1995.  A. Daniely. A PTAS for agnostically learning halfspaces. In Proc. 28th Annual ACM Workshop on  Computational Learning Theory, pages 484-502, 2015.  13   HARDNESS OF LEARNING HALFSPACES USING PTFS  References  M. Alekhnovich, M. Braverman, V. Feldman, A. R. Klivans, and T. Pitassi. The complexity of  properly learning simple concept classes. J. Comp. Sys. Sci., 74(1):16-34, 2008.  E. Amaldi and V. Kann. On the approximability of minimizing nonzero variables or unsatisfied  relations in linear systems. Theoret. Comput. Sci., 209(1-2):237-260, 1998.  B. Applebaum, B. Barak, and D. Xiao. On basing lower-bounds for learning on worst-case as- sumptions. In Proc. 49th Annual IEEE Symposium on Foundations of Computer Science, pages 211-220, 2008.  R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random  projection. Machine Learning, 63(2):161-182, 2006.  P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for efficiently learning linear  separators with noise. J. ACM, 63(6):50:1-50:27, 2017.  S. Ben-David, N. Eiron, and P. M. Long. On the difficulty of approximately maximizing agree- ments. J. Comp. Sys. Sci., 66(3):496 - 514, 2003. ISSN 0022-0000. doi: http://dx.doi.org/ 10.1016/S0022-0000(03)00038-2. URL http://www.sciencedirect.com/science/ article/pii/S0022000003000382.  A. Blum and R. Kannan. Learning an intersection of a constant number of halfspaces over a uniform  distribution. J. Comp. Sys. Sci., 54(2):371-380, 1997.  A. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. In Machine Learning: From Theory to Applications - Cooperative Research at Siemens and MIT, pages 9-28, 1993.  A. Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial-time algorithm for learning noisy  linear threshold functions. Algorithmica, 22(1):35-52, 1998.  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statisti-  cal query model. J. ACM, 50(4):506-519, 2003.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-  Chervonenkis dimension. J. ACM, 36(4):929-965, 1989.  N. H. Bshouty and L. Burroughs. Maximizing agreements and coagnostic learning. Theoret. Com-  put. Sci., 350(1):24-39, 2006.  A. Carbery and J. Wright. Distributional and lq norm inequalities for polynomials over convex  bodies in Rn. Math. Res. Lett., 8(3):233-248, 2001.  E. Cohen. Learning noisy perceptrons by a perceptron in polynomial time. In Proc. 38th Annual  IEEE Symposium on Foundations of Computer Science, pages 514-523, 1997.  C. Cortes and V. Vapnik. Support-Vector networks. Machine Learning, 20(3):273-297, 1995.  A. Daniely. A PTAS for agnostically learning halfspaces. In Proc. 28th Annual ACM Workshop on  Computational Learning Theory, pages 484-502, 2015. HARDNESS OF LEARNING HALFSPACES USING PTFS  A. Daniely. Complexity theoretic limitations on learning halfspaces. In Proc. 48th Annual ACM  Symposium on the Theory of Computing, pages 105-117, 2016.  A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning DNF\u2019s. In Proc.  29th Annual ACM Workshop on Computational Learning Theory, pages 815-830, 2016.  I. Diakonikolas, R. O\u2019Donnell, R. A. Servedio, and Y. Wu. Hardness results for agnostically learning low-degree polynomial threshold functions. In Proc. 22nd ACM-SIAM Symposium on Discrete Algorithms, pages 1590-1606, 2011.  V. Feldman, P. Gopalan, S. Khot, and A. K. Ponnuswami. On agnostic learning of parities, mono-  mials, and halfspaces. SIAM J. Comput., 39(2):606-645, 2009.  V. Feldman, V. Guruswami, P. Raghavendra, and Y. Wu. Agnostic learning of monomials by halfs-  paces is hard. SIAM J. Comput., 41(6):1558-1590, 2012.  P. Gopalan, S. Khot, and R. Saket. Hardness of reconstructing multivariate polynomials over finite  fields. SIAM J. Comput., 39(6):2598-2621, 2010.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. SIAM J. Comput.,  39(2):742-765, 2009.  V. Guruswami, P. Raghavendra, R. Saket, and Yi Wu. Bypassing UGC from some optimal geometric  inapproximability results. ACM Trans. Algorithms, 12(1):6:1-6:25, 2016.  J. H\u02daastad. Some optimal inapproximability results. J. ACM, 48(4):798-859, 2001.  D. Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Inform. and Comp., 100(1):78-150, 1992.  D. S. Johnson and F. P. Preparata. The densest hemisphere problem. Theoret. Comput. Sci., 6:  93-107, 1978.  A. T. Kalai, A. R. Klivans, Y. Mansour, and R. A. Servedio. Agnostically learning halfspaces. In  Proc. 46th Annual IEEE Symposium on Foundations of Computer Science, 2005.  A. T. Kalai, Y. Mansour, and E. Verbin. On agnostic boosting and parity learning. In Proc. 40th  Annual ACM Symposium on the Theory of Computing, pages 629-638, 2008.  M.J. Kearns, R.E. Schapire, and L.M. Sellie. Toward efficient agnostic learning. Machine Learning,  17(2-3):115-141, 1994.  S. Khot. On the power of unique 2-prover 1-round games. In Proc. 34th Annual ACM Symposium  on the Theory of Computing, pages 767-775, 2002.  S. Khot. personal communication, 2009.  S. Khot and R. Saket. On the hardness of learning intersections of two halfspaces. J. Comp. Sys.  Sci., 77(1):129-141, 2011.  A. R. Klivans and P. Kothari. Embedding hard learning problems into gaussian space. In Proc. 18th International Workshop on Randomization and Computation (RANDOM), pages 793-809, 2014. HARDNESS OF LEARNING HALFSPACES USING PTFS  A. R. Klivans and R. A. Servedio. Learning intersections of halfspaces with a margin. J. Comp.  Sys. Sci., 74(1):35-48, 2008.  A. R. Klivans and A. A. Sherstov. Cryptographic hardness for learning intersections of halfspaces.  J. Comp. Sys. Sci., 75(1):2-12, 2009.  A. R. Klivans, R. O\u2019Donnell, and R. A. Servedio. Learning intersections and thresholds of halfs-  paces. J. Comp. Sys. Sci., 68(4):808-840, 2004.  A. R. Klivans, P. M. Long, and R. A. Servedio. Learning halfspaces with malicious noise. J. Mach.  Learn. Res., 10:2715-2740, 2009.  M. Minsky and S. Papert. Perceptrons: an introduction to computational geometry. MIT Press,  Cambridge, MA, 1969.  1595-1598, 1990.  O.J. Murphy. Nearest neighbor pattern classification perceptrons. Proceedings of the IEEE, 78(10):  F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1962.  U. R\u00a8ukert, L. Richter, and S. Kramer. Quantitative association rules based on half-spaces. In Proc.  4th IEEE International Conference on Data Mining, pages 507-510, 2004.  S. Vempala. A random sampling based algorithm for learning the intersection of half-spaces. In Proc. 38th Annual IEEE Symposium on Foundations of Computer Science, pages 508-513, 1997.  E. Viola. The sum of D small-bias generators fools polynomials of degree D. Comput. Complexity,  18(2):209-217, 2009.  "}, "Best of both worlds: Stochastic & adversarial best-arm identification": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Best of both worlds: Stochastic & adversarial best-arm identification", "abstract": "We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: $\\backslash$emph{\\{}Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards?{\\}} First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.", "pdf_url": "http://proceedings.mlr.press/v75/abbasi-yadkori18a/abbasi-yadkori18a.pdf", "keywords": ["multi-armed bandits", "best-arm identification", "adversarial and stochastic rewards"], "reference": "Robin Allesiardo and Rapha\u00a8el F\u00b4eraud. Selection of learning experts. In International Joint Confer-  ence on Neural Networks, 2017.  Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic  multi-armed bandit problem. International Journal of Data Science and Analytics, 2017.  Jason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best-arm identification for contami-  nated bandits. arXiv preprint arXiv:1802.09514, 2018.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best-arm identification in multi-armed  bandits. In Conference on Learning Theory, 2010.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both In Conference on Learning Theory and arXiv preprint  stochastic and adversarial bandits. arXiv:1605.08722, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1), 2002.  S\u00b4ebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, 2013.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1), 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, 2012.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandit prob-  lems. In Algorithmic Learning Theory, 2009.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best-arm  identification bandit problem. In Conference on Learning Theory, 2016.  Alexandra Carpentier and Michal Valko. Extreme bandits. In Neural Information Processing Sys-  tems, 2014.  13   BEST OF BOTH WORLDS: STOCHASTIC & ADVERSARIAL BEST-ARM IDENTIFICATION  Acknowledgements  We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the Aus- tralian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS). The research presented was also supported by European CHIST-ERA project DELTA, French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universit\u00a8at Magdeburg associated-team north-european project Allocate, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003). We would like to thank Iosif Pinelis for a useful discussion on Bernstein inequalities.  References  Robin Allesiardo and Rapha\u00a8el F\u00b4eraud. Selection of learning experts. In International Joint Confer-  ence on Neural Networks, 2017.  Robin Allesiardo, Rapha\u00a8el F\u00b4eraud, and Odalric-Ambrym Maillard. The non-stationary stochastic  multi-armed bandit problem. International Journal of Data Science and Analytics, 2017.  Jason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best-arm identification for contami-  nated bandits. arXiv preprint arXiv:1802.09514, 2018.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best-arm identification in multi-armed  bandits. In Conference on Learning Theory, 2010.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both In Conference on Learning Theory and arXiv preprint  stochastic and adversarial bandits. arXiv:1605.08722, 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1), 2002.  S\u00b4ebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, 2013.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1), 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, 2012.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandit prob-  lems. In Algorithmic Learning Theory, 2009.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best-arm  identification bandit problem. In Conference on Learning Theory, 2016.  Alexandra Carpentier and Michal Valko. Extreme bandits. In Neural Information Processing Sys-  tems, 2014. BEST OF BOTH WORLDS: STOCHASTIC & ADVERSARIAL BEST-ARM IDENTIFICATION  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed Bandit and reinforcement-learning problems. Journal of Machine Learning Research, 7:1079-1105, 2006.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100-  118, 1975.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best-arm identification with fixed confidence. In  Conference on Learning Theory, 2016.  Kevin Jamieson and Ameet Talwalkar. Non-stochastic best-arm identification and hyperparameter  optimization. In International Conference on Artificial Intelligence and Statistics, 2016.  Rob Kaas and Jan M. Buhrman. Mean, median and mode in binomial distributions. Statistica  Neerlandica, 34(1):13-18, 1980.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection in  stochastic multi-armed bandits. In International Conference on Machine Learning, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.  In International Conference on Machine Learning, 2013.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, 2013.  Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560, 2016.  Andrea Locatelli, Maurilio Gutzeit, and Alexandra Carpentier. An optimal algorithm for the thresh-  olding bandit problem. In International Conference on Machine Learning, 2016.  Shie Mannor and John N. Tsitsiklis. The sample complexity of exploration in the multi-armed  bandit problem. Journal of Machine Learning Research, 5(Jun), 2004.  Oded Maron and Andrew Moore. Hoeffding Races: Accelerating model-selection search for clas-  sification and function approximation. In Neural Information Processing Systems, 1993.  Volodymyr Mnih, Csaba Szepesv\u00b4ari, and Jean-Yves Audibert. Empirical Bernstein stopping. In  International Conference on Machine Learning, 2008.  Yevgeny Seldin and G\u00b4abor Lugosi. An improved parametrization and analysis of the EXP3++ algo-  rithm for stochastic and adversarial bandits. In Conference on Learning Theory, 2017.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In International Conference on Machine Learning, 2014. BEST OF BOTH WORLDS: STOCHASTIC & ADVERSARIAL BEST-ARM IDENTIFICATION  "}, "Learning Patterns for Detection with Multiscale Scan Statistics": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Learning Patterns for Detection with Multiscale Scan Statistics", "abstract": "This paper addresses detecting anomalous patterns in images, time-series, and tensor data when the location and scale of the pattern and the pattern itself is unknown a priori. The multiscale scan statistic convolves the proposed pattern with the image at various scales and returns the maximum of the resulting tensor. Scale corrected multiscale scan statistics apply different standardizations at each scale, and the limiting distribution under the null hypothesis\u2014that the data is only noise\u2014is known for smooth patterns.  We consider the problem of simultaneously learning and detecting the anomalous pattern from a dictionary of smooth patterns and a database of many tensors. To this end, we show that the multiscale scan statistic is a subexponential random variable, and prove a chaining lemma for standardized suprema, which may be of independent interest. Then by averaging the statistics over the database of tensors we can learn the pattern and obtain Bernstein-type error bounds. We will also provide a construction of an $\\epsilon$-net of the location and scale parameters, providing a computationally tractable approximation with similar error bounds.", "pdf_url": "http://proceedings.mlr.press/v75/sharpnack18a/sharpnack18a.pdf", "keywords": ["Detection", "multiscale scan statistics", "pattern adaptation", "generic chaining"], "reference": "E. Arias-Castro, D.L. Donoho, and X. Huo. Near-optimal detection of geometric objects by fast  multiscale methods. IEEE Trans. Inform. Theory, 51(7):2402-2425, 2005.  Ery Arias-Castro, Emmanuel J. Cand`es, and Arnaud Durand. Detection of an anomalous cluster in  a network. Ann. Statist., 39(1):278-304, 2011.  Sean M. Brennan, Angela M. Mielke, David C. Torney, and Arthur B. Maccabe. Radiation detection with distributed sensor networks. Computer, 37(8):57-59, 2004. ISSN 0018-9162. doi: http: //doi.ieeecomputersociety.org/10.1109/MC.2004.103.  12   LEARNING PATTERNS FOR DETECTION  Wiener process. Suppose also that hi asymptotically powerful (has diminishing probability of type 1 and type 2 error) if  j \u2264 Lc for some 0 \u2264 c < 1 for all i, j, then the PAMSS is  \u221a  \u00b5 \u2212  2 \u00b7  (cid:80)n (cid:80)n  i=1 v2 hi i=1 vhi  \u2192 \u221e.  (10)  We take this result to mean that as long as the function class, |F|, does not grow exponentially in n, we achieve asymptotic power under the same conditions as if |F| = 1.  Proof We can see this because, under the assumptions, Vn \u2265 Cn log L for some C > 0 and \u221a n = \u03c9(Fn(\u03b4) log log L) making the  Vn/Mn \u2192 0. Furthermore, Fn(\u03b4) = o(  n) and so Mn/  \u221a  \u221a  term involving |F| lower order. Evaluating Mn, Vn gives us the result.  Let us conclude with a remark about the restrictiveness of the assumption that we have a finite function class F. It is known that functions of bounded variation have Haar wavelet coefficients that are bounded in a weak (cid:96)1 norm, Cohen et al. (2003). It is reasonable to discretize the allowed coefficient values and then restrict our function class to functions with k-sparse wavelet coefficients of m then the log-size of the class scales like k log m which is very manageable. One advantage with this approach is that the sparse Haar wavelets will naturally satisfy condition (TVC). It is outside of the scope of this work to extend the result to infinite function classes, but this would present a very interesting and important extension.  6. Conclusions  We have addressed learning and detecting patterns from a function class, F, using multiscale scan statistics. We have introduced the multiscale scan statistic and proved a subexponential concentra- tion bound for it, which relied on a novel chaining result for standardized suprema of subGaussian random fields (a result that may be of independent interest). We introduced the pattern adapted mul- tiscale scan statistic, that can learn patterns in a database of tensors (when the locations and scales vary). This result allowed us to prove Bernstein-type concentration for the PAMSS, meaning that we can learn finite function classes that grow exponentially with the sample size, n. With evidence that representation learning and detection are not incompatible, we anticipate that efficient methods for learning functions in this setting will emerge, by using modern tools from deep learning and multiscale methods.  Acknowledgements: JS is supported in part by NSF DMS-1712996.  References  E. Arias-Castro, D.L. Donoho, and X. Huo. Near-optimal detection of geometric objects by fast  multiscale methods. IEEE Trans. Inform. Theory, 51(7):2402-2425, 2005.  Ery Arias-Castro, Emmanuel J. Cand`es, and Arnaud Durand. Detection of an anomalous cluster in  a network. Ann. Statist., 39(1):278-304, 2011.  Sean M. Brennan, Angela M. Mielke, David C. Torney, and Arthur B. Maccabe. Radiation detection with distributed sensor networks. Computer, 37(8):57-59, 2004. ISSN 0018-9162. doi: http: //doi.ieeecomputersociety.org/10.1109/MC.2004.103. LEARNING PATTERNS FOR DETECTION  Y. Caron, P. Makris, and N. Vincent. A method for detecting artificial objects in natural environ- ments. In Proceedings 16th International Conference on Pattern Recognition, volume 1, pages 600-603. IEEE Comput. Soc., 2002.  Hock Peng Chan and Guenther Walther. Detection with the scan and the average likelihood ratio.  Statistica Sinica, pages 409-428, 2013.  Hock Peng Chan, Guenther Walther, et al. Optimal detection of multi-sample aligned sparse signals.  The Annals of Statistics, 43(5):1865-1895, 2015.  Albert Cohen, Wolfgang Dahmen, Ingrid Daubechies, Ronald DeVore, et al. Harmonic analysis of  the space bv. Revista Matematica Iberoamericana, 19(1):235-263, 2003.  D. Culler, D. Estrin, and M. Srivastava. Overview of sensor networks.  IEEE Computer, 37(8):  Laurens De Haan and Ana Ferreira. Extreme value theory: an introduction. Springer Science &  41-49, 2004.  Business Media, 2007.  Statistics, pages 124-152, 2001.  Statistics, 31(8):967-980, 2004.  Lutz Dumbgen and Vladimir G Spokoiny. Multiscale testing of qualitative hypotheses. Annals of  Joseph Glaz and Zhenkui Zhang. Multiple window discrete scan statistics. Journal of Applied  Joseph Glaz, Joseph I Naus, and Sylvan Wallenstein. Scan statistics. Springer, 2001.  G Haiman and C Preda. Estimation for the distribution of two-dimensional discrete scan statistics.  Methodology and Computing in Applied Probability, 8(3):373-382, 2006.  R. Heffernan, F. Mostashari, D. Das, A. Karpati, M. Kulldorff, and D. Weiss. Syndromic surveil- lance in public health practice, New York City. Emerging Infectious Diseases, 10(5):858-864, 2004.  D. James, B. D. Clymer, and P. Schmalbrock. Texture detection of simulated microcalcification susceptibility effects in magnetic resonance imaging of breasts. Journal of Magnetic Resonance Imaging, 13(6):876-881, 2001.  Zakhar Kabluchko. Extremes of the standardized gaussian noise. Stochastic Processes and their  Applications, 121(3):515-533, 2011.  Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Pro- ceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 180-191. VLDB Endowment, 2004.  Dimitris Manolakis and Gary Shaw. Detection algorithms for hyperspectral imaging applications.  IEEE signal processing magazine, 19(1):29-43, 2002.  Nathan Moon, Elizabeth Bullitt, Koen van Leemput, and Guido Gerig. Automatic brain and tumor segmentation. In MICCAI \u201902: Proceedings of the 5th International Conference on Medical Im- age Computing and Computer-Assisted Intervention-Part I, pages 372-379, London, UK, 2002. Springer-Verlag. ISBN 3-540-44224-3. LEARNING PATTERNS FOR DETECTION  Joseph I. Naus. The distribution of the size of the maximum cluster of points on a line. J. Amer.  Statist. Assoc., 60:532-538, 1965. ISSN 0162-1459.  Joseph I Naus and Sylvan Wallenstein. Multiple window and cluster size scan procedures. Method-  ology and Computing in Applied Probability, 6(4):389-400, 2004.  Daniel B Neill. Fast subset scan for spatial pattern detection. Journal of the Royal Statistical  Society: Series B (Statistical Methodology), 74(2):337-360, 2012.  Vladimir Pozdnyakov, Joseph Glaz, Martin Kulldorff, and J Michael Steele. A martingale approach  to scan statistics. Annals of the Institute of Statistical Mathematics, 57(1):21-37, 2005.  D. Pozo, F.J. Olmo, and L. Alados-Arboledas. Fire detection and growth monitoring using a multi- temporal technique on AVHRR mid-infrared and thermal channels. Remote Sensing of Environ- ment, 60(2):111-120, 1997.  Katharina Proksch, Frank Werner, and Axel Munk. Multiscale scanning in inverse problems. arXiv  preprint arXiv:1611.04537, 2016.  L.D. Rotz and J.M. Hughes. Advances in detecting and responding to threats from bioterrorism and  emerging infectious disease. Nature Medicine, pages S130-S136, 2004.  James Sharpnack and Ery Arias-Castro. Exact asymptotics for the scan statistic and fast alternatives.  Electronic Journal of Statistics, 10(2):2641-2684, 2016.  David O Siegmund and Keith J Worsley. Testing for a signal with unknown location and scale in a  stationary gaussian random field. The Annals of Statistics, pages 608-639, 1995.  Michel Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer  Science & Business Media, 2006.  AW Van der Vaart and JA Wellner. Weak convergence and empirical processes. Springer, New  York, 1996.  arXiv:1011.3027, 2010.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  Guenther Walther. Optimal and fast detection of spatial clusters with scan statistics. Ann. Statist.,  38(2):1010-1033, 2010. ISSN 0090-5364. doi: 10.1214/09-AOS732.  Xiao Wang and Joseph Glaz. Variable window scan statistics for normal data. Communications in  Statistics-Theory and Methods, 43(10-12):2489-2504, 2014.  Brian A White, Antonios Tsourdos, Immanuel Ashokaraj, S Subchan, and Rafal Zbikowski. Con- taminant cloud boundary monitoring using network of uav sensors. IEEE Sensors Journal, 8(10): 1681-1692, 2008.  Y Jeffrey Yang, Roy C Haught, and James A Goodrich. Real-time contaminant detection and classification in a drinking water pipe using conventional water quality sensors: Techniques and experimental results. Journal of environmental management, 90(8):2494-2506, 2009. LEARNING PATTERNS FOR DETECTION  "}, "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk", "abstract": "We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer.  We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods.  These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of  enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.", "pdf_url": "http://proceedings.mlr.press/v75/hand18a/hand18a.pdf", "keywords": ["deep learning", "generative modeling", "nonconvex optimization", "compressed sensing"], "reference": "Ali Ahmed, Benjamin Recht, and Justin Romberg. Blind deconvolution using convex programming.  IEEE Transactions on Information Theory, 60(3):1711-1732, 2014.  Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with  implications for training. CoRR, abs/1511.05653, 2015.  Bandeira, Boumal, and Voroninski. On the low-rank approach for semidefinite programs arising in  synchronization and community detection. JMLR, 49:1-22, 2016.  Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253-263, 2008.  Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using genera-  tive models. arXiv:1703.03208, 2017.  6   DEEP GENERATIVE PRIORS  In the case that A \u201c In, the RRIC is trivially satisfied, and we get the following corollary about  inverting multilayer neural networks.  Corollary 3 (Approximate Invertibility of Multilayer Neural Networks) If G is a d-layer neu- ral network such that Wi satisfies the WDC with constant (cid:15) for all i \u201c 1 . . . d, then the function f pxq \u201c }Gpxq \u00b4 Gpx0q}2 has no stationary points outside of a neighborhood around x0 and \u00b4\u03c1dx0.  In the case of a Gaussian network with Gaussian measurements, the WDC and RRIC are satis- fied with high probability if the network is sufficiently expansive and there are a sufficient number of measurements.  Proposition 4 Fix 0 \u0103 (cid:15) \u0103 1. Assume ni \u011b cni\u00b41 log ni\u00b41 for all i \u201c 1 . . . d and m \u0105 cdk log \u03a0d i\u201c1ni. Assume the entires of Wi are i.i.d. N p0, 1{niq, and the entries of A are i.i.d. N p0, 1{mq. Then, Wi satisfies the WDC with constant (cid:15) for all i and A satisfies the RRIC with \u0159 d i\u201c1 \u02dccnie\u00b4\u03b3ni\u00b41 \u00b4 \u02dcce\u00b4\u03b3m. Here, c and respect to G with constant (cid:15) with probability at least 1 \u00b4 \u03b3\u00b41 are constants that depend polynomially on (cid:15)\u00b41, and \u02dcc is a universal constant.  As stated after Theorem 1, no assumption is made on the independence between Wi and Wj for i \u2030 j. While Proposition 4 is stated for A P Rm\u02c6n with i.i.d. Gaussian entries, it also applies in the case of any random matrix that satisfies the following concentration of measure condition:  `  \u02d8  P  |}Ax}2  2 \u00b4 }x}2  2| \u011b (cid:15)}x}2 2  \u010f 2e\u00b4mc0p(cid:15)q,  for any fixed x P Rn, where c0p(cid:15)q is a positive constant depending only on (cid:15). In particular, Proposi- tion 4 and hence Theorem 1 extends to the case of where the entries of A are independent Bernoulli random variables (and the entries of Wi are Gaussian). See Baraniuk et al. (2008) for more.  Acknowledgments  PH is partially supported by NSF DMS-1464525.  References  Ali Ahmed, Benjamin Recht, and Justin Romberg. Blind deconvolution using convex programming.  IEEE Transactions on Information Theory, 60(3):1711-1732, 2014.  Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with  implications for training. CoRR, abs/1511.05653, 2015.  Bandeira, Boumal, and Voroninski. On the low-rank approach for semidefinite programs arising in  synchronization and community detection. JMLR, 49:1-22, 2016.  Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253-263, 2008.  Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using genera-  tive models. arXiv:1703.03208, 2017. DEEP GENERATIVE PRIORS  E. Cand`es, Y. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. SIAM  J. Imaging Sci., 6(1):199-225, 2013a. doi: 10.1137/110848074.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717, 2009.  Emmanuel J. Cand`es, Justin K. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207- 1223, 2006. ISSN 1097-0312. doi: 10.1002/cpa.20124.  Emmanuel J Cand`es, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Comm. Pure Appl. Math., 66(8):1241-1274, 2013b.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. Information Theory, IEEE Transactions on, 61(4):1985-2007, 2015.  Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739-747, 2015.  Yuxin Chen and Emmanuel Candes. The projected power method: An efficient algorithm for joint  alignment from pairwise differences. arXiv preprint arXiv:1609.05820, 2016.  David Donoho. For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution. Communications on Pure and Applied Mathematics, 59(6), 2006.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Wen Huang and Paul Hand. Blind deconvolution by a steepest descent algorithm on a quotient  manifold. arXiv preprint arXiv:1710.03309, 2017.  Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016.  Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. Rapid, robust, and reliable blind  deconvolution via nonconvex optimization. arXiv preprint arXiv:1606.04933, 2016.  Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.  Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.  S. Mallat. Group invariant scattering. Comm. Pure Appl. Math., 65(10):13311398, 2012.  Tyler Maunu, Teng Zhang, and Gilad Lerman. A well-tempered landscape for non-convex robust  subspace recovery. arXiv preprint arXiv:1706.03896, 2017. DEEP GENERATIVE PRIORS  Katta G. Murty and Santosh N. Kabadi. Some np-complete problems in quadratic and nonlinear  programming. Mathematical Programming, 39(2):117-129, Jun 1987.  Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play In Computer generative networks: Conditional iterative generation of images in latent space. Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 3510-3520. IEEE, 2017.  Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving  for simplicity: The all convolutional net. CoRR, abs/1412.6806, 2014.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory  (ISIT), 2016 IEEE International Symposium on, pages 2379-2383. IEEE, 2016. "}, "Small-loss bounds for online learning with partial information": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Small-loss bounds for online learning with partial information", "abstract": "We consider the problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, a decision maker selects an action from a finite set of alternatives. We develop a black-box approach for such problems where the learner observes as feedback only losses of a subset of the actions that includes the selected action. When losses of actions are non-negative, under the graph-based feedback model introduced by Mannor and Shamir, we offer algorithms that attain the so called \u201csmall-loss\u201d $o(\\alpha L^{\\star})$ regret bounds with high probability, where $\\alpha$ is the independence number of the graph, and $L^{\\star}$ is the loss of the best action. Prior to our work, there was no data-dependent guarantee for general feedback graphs even for pseudo-regret (without dependence on the number of actions, i.e. utilizing the increased information feedback). Taking advantage of the black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing in networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. In the special case of classical bandit and semi-bandit problems, we provide optimal small-loss,  high-probability guarantees of $\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of actions, answering open questions of Neu.  Previous bounds for bandits and semi-bandits were known only for pseudo-regret and only in expectation. We also offer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for fixed feedback graphs with clique-partition number at most $\\kappa$.", "pdf_url": "http://proceedings.mlr.press/v75/lykouris18a/lykouris18a.pdf", "keywords": ["Online Learning", "Bandits", "Feedback Graphs", "Regret", "High probability"], "reference": "Chamy Allenberg, Peter Auer, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In 17th International Conference on Algorithmic Learning Theory (ALT), 2006.  Noga Alon, Nicol Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. From bandits to experts: In 27th Annual Conference on Neural Information  A tale of domination and independence. Processing Systems (NIPS), 2013.  Noga Alon, Nicol`o Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Proceedings of the 28th Conference on Learning Theory (COLT), 2015.  Noga Alon, Nicol Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing (SICOMP), 46(6):1785-1826, 2017.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11:2785-2836, 2010. URL http:// portal.acm.org/citation.cfm?id=1953023.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochas- ISSN tic multiarmed bandit problem. 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77, January 2003.  Baruch Awerbuch and Robert D. Kleinberg. Adaptive routing with end-to-end feedback: distributed learning and geometric approaches. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing (STOC), 2004.  6   SMALL-LOSS BOUNDS FOR ONLINE LEARNING WITH PARTIAL INFORMATION  important benefit of the freezing technique: it can be extended to handle feedback graphs (via our dual-thresholding). We also combine freezing with multiplicative weights to develop an algorithm we dL(cid:63)) for the pure bandit term GREEN-IX which achieves optimal high-probability small-loss (cid:101)O( setting. Finally, combining freezing with the truncation idea, we obtain the corresponding result for semi-bandits; in contrast, the geometric resampling analysis does not seem to extend to high probability since it does not provide a handle on the variance of the estimated loss.  \u221a  We refer the reader to the full version of the paper that can be found here https://arxiv.org/ abs/1711.03639.  We thank Haipeng Luo for pointing out the connection to the contextual bandit setting, Jake Abernethy, Adam Kalai, and Santosh Vempala for a useful discussion about Follow the Perturbed Leader, and anonymous reviewers for their useful feedback. This work was supported by NSF grants CCF- 1563714 and CDS&E-MSS 1521544, and an NSF-CAREER Award 1750575  Full version  Acknowledgments  References  Chamy Allenberg, Peter Auer, L\u00b4aszl\u00b4o Gy\u00a8orfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In 17th International Conference on Algorithmic Learning Theory (ALT), 2006.  Noga Alon, Nicol Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. From bandits to experts: In 27th Annual Conference on Neural Information  A tale of domination and independence. Processing Systems (NIPS), 2013.  Noga Alon, Nicol`o Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Proceedings of the 28th Conference on Learning Theory (COLT), 2015.  Noga Alon, Nicol Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing (SICOMP), 46(6):1785-1826, 2017.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11:2785-2836, 2010. URL http:// portal.acm.org/citation.cfm?id=1953023.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochas- ISSN tic multiarmed bandit problem. 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77, January 2003.  Baruch Awerbuch and Robert D. Kleinberg. Adaptive routing with end-to-end feedback: distributed learning and geometric approaches. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing (STOC), 2004. SMALL-LOSS BOUNDS FOR ONLINE LEARNING WITH PARTIAL INFORMATION  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  Avrim Blum and Jason D. Hartline. Near-optimal online auctions. In Proceedings of the 16th Annual  ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.  Avrim Blum, Eyal Even-Dar, and Katrina Ligett. Routing without regret: On convergence to nash equilibria of regret-minimizing algorithms in routing games. In Proceedings of the 25th Annual ACM Symposium on Principles of Distributed Computing (PODC), 2006.  Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing (STOC), 2008.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, New York, NY, USA, 2006. ISBN 0521841089.  Nicol`o Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2013.  Alon Cohen, Tamir Hazan, and Tomer Koren. Online learning with feedback graphs without the graphs. In Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016.  Thomas M. Cover. Universal portfolios. Mathematical Finance, 1(1):1-29, 1991. ISSN 1467- 9965. doi: 10.1111/j.1467-9965.1991.tb00002.x. URL http://dx.doi.org/10.1111/j. 1467-9965.1991.tb00002.x.  Dylan J. Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and \u00b4Eva Tardos. Learning in games: Robustness of fast convergence. In 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119-139, August 1997. ISSN 0022-0000. doi: 10.1006/jcss.1997.1504. URL http://dx.doi.org/10.1006/jcss.1997.1504.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Mach. Learn., 69(2-3):169-192, December 2007. ISSN 0885-6125. doi: 10.1007/ s10994-007-5016-8. URL http://dx.doi.org/10.1007/s10994-007-5016-8.  Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Mach. Learn., 32(2):151-178, August 1998. ISSN 0885-6125. doi: 10.1023/A:1007424614876. URL http://dx.doi. org/10.1023/A:1007424614876.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. J. Comput. Syst. Sci., 71(3):291-307, October 2005. ISSN 0022-0000. doi: 10.1016/j.jcss.2004.10.016. URL http://dx.doi.org/10.1016/j.jcss.2004.10.016. SMALL-LOSS BOUNDS FOR ONLINE LEARNING WITH PARTIAL INFORMATION  Tom\u00b4as Koc\u00b4ak, Gergely Neu, Michal Valko, and R\u00b4emi Munos. Efficient learning by implicit explo- ration in bandit problems with side observations. In 28th Annual Conference on Neural Information Processing Systems (NIPS), 2014.  Tom\u00b4a\u02c7s Koc\u00b4ak, Gergely Neu, and Michal Valko. Online learning with noisy side observations. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS), 2016.  T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Adv. Appl. Math., 6(1):4-22, March 1985. ISSN 0196-8858. doi: 10.1016/0196-8858(85)90002-8. URL http: //dx.doi.org/10.1016/0196-8858(85)90002-8.  John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Proceedings of the 20th International Conference on Neural Information Processing Systems (NIPS), 2007.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108 (2):212-261, February 1994. ISSN 0890-5401. doi: 10.1006/inco.1994.1009. URL http: //dx.doi.org/10.1006/inco.1994.1009.  Thodoris Lykouris, Vasilis Syrgkanis, and \u00b4Eva Tardos. Learning and efficiency in games with dynamic population. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2016.  Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. In 25th  Annual Conference on Neural Information Processing Systems (NIPS), 2011.  Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Proceedings of the 27th  Annual Conference on Learning Theory (COLT), 2015a.  Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS), 2015b.  Gergely Neu and G\u00b4abor Bart\u00b4ok. An efficient algorithm for learning with semi-bandit feedback. In  24th International Conference on Algorithmic Learning Theory (ALT), 2013.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceed-  ings of the 26th Conference on Learning Theory (COLT), 2013.  Tim Roughgarden. Intrinsic robustness of the price of anarchy. Journal of the ACM, 2015.  Tim Roughgarden and Joshua R. Wang. Minimizing regret with multiple reserves. In Proceedings of  the 17th ACM Conference on Economics and Computation (EC), 2016.  Aristide C. Y. Tossou, Christos Dimitrakakis, and Devdatt Dubhashi. Thompson sampling for stochastic bandits with graph feedback. In 14th International Conference on Artificial Intelligence (AAAI), 2017. "}, "Empirical bounds for functions with weak interactions": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Empirical bounds for functions with weak interactions", "abstract": "We provide sharp empirical estimates of expectation, variance and normal approximation for a class of statistics whose variation in any argument does not change too much when another argument is modified. Examples of such weak interactions are furnished by U- and V-statistics, Lipschitz L-statistics and various error functionals of $\\ell_2$-regularized algorithms and Gibbs algorithms.", "pdf_url": "http://proceedings.mlr.press/v75/maurer18a/maurer18a.pdf", "keywords": ["List of keywords"], "reference": "M. A. Arcones. A Bernstein-type inequality for U-statistics and U-processes. Statistics and Proba-  bility Letters, 22(3):239-247, 1995.  J. Y. Audibert, R. Munos, and C. Szepesv\u00b4ari. Exploration-exploitation tradeoff using variance esti-  mates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902, 2009.  A. C. Berry. The accuracy of the Gaussian approximation to the sum of independent variates. Trans-  actions of the American Mathematical Society, 49(1):122-136, 1941.  K. Binder. Applications of Monte Carlo methods to statistical physics. Reports on Progress in  Physics, 60(5):487, 1997.  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities, Oxford University Press, 2013.  Q. Cao, Z. C. Guo, and Y. Ying. Generalization bounds for metric and similarity learning. Machine  Learning, 102(1):115-132, 2016.  S. Chatterjee. A new method of normal approximation. The Annals of Probability, 36.4:1584-1610,  L. H. Chen, L. Goldstein, and Q. M. Shao. Normal approximation by Stein\u2019s method. Springer  2008.  Science and Business Media, 2010.  Annals of Statistics, 844-874, 2008.  S. Cl\u00b4emenc\u00b8on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. The  B. Efron and C. Stein. The jackknife estimate of variance. The Annals of Statistics, 586-596, 1981.  W. Hoeffding. A class of statistics with asymptotically normal distribution. The Annals of Mathe-  matical Statistics, 293-325, 1948.  C. Houdr\u00b4e. The iterated jackknife estimate of variance. Statistics and Probability Letters, 35(2):197-  201, 1997.  220(4598):671-680, 1983.  S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science,  A. Maurer. Concentration inequalities for functions of independent variables. Random Structures  and Algorithms 29:121-138, 2006.  A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In COLT.  2009.  2017.  A. Maurer. A Second-order Look at Stability and Generalization. Conference on Learning Theory.  A. Maurer. A Bernstein-type inequality for functions of bounded interaction. Bernoulli (Forthcom-  ing), (see also arXiv preprint arXiv:1701.06191).  13   SHORT TITLE  References  M. A. Arcones. A Bernstein-type inequality for U-statistics and U-processes. Statistics and Proba-  bility Letters, 22(3):239-247, 1995.  J. Y. Audibert, R. Munos, and C. Szepesv\u00b4ari. Exploration-exploitation tradeoff using variance esti-  mates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902, 2009.  A. C. Berry. The accuracy of the Gaussian approximation to the sum of independent variates. Trans-  actions of the American Mathematical Society, 49(1):122-136, 1941.  K. Binder. Applications of Monte Carlo methods to statistical physics. Reports on Progress in  Physics, 60(5):487, 1997.  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities, Oxford University Press, 2013.  Q. Cao, Z. C. Guo, and Y. Ying. Generalization bounds for metric and similarity learning. Machine  Learning, 102(1):115-132, 2016.  S. Chatterjee. A new method of normal approximation. The Annals of Probability, 36.4:1584-1610,  L. H. Chen, L. Goldstein, and Q. M. Shao. Normal approximation by Stein\u2019s method. Springer  2008.  Science and Business Media, 2010.  Annals of Statistics, 844-874, 2008.  S. Cl\u00b4emenc\u00b8on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. The  B. Efron and C. Stein. The jackknife estimate of variance. The Annals of Statistics, 586-596, 1981.  W. Hoeffding. A class of statistics with asymptotically normal distribution. The Annals of Mathe-  matical Statistics, 293-325, 1948.  C. Houdr\u00b4e. The iterated jackknife estimate of variance. Statistics and Probability Letters, 35(2):197-  201, 1997.  220(4598):671-680, 1983.  S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science,  A. Maurer. Concentration inequalities for functions of independent variables. Random Structures  and Algorithms 29:121-138, 2006.  A. Maurer and M. Pontil. Empirical Bernstein bounds and sample variance penalization. In COLT.  2009.  2017.  A. Maurer. A Second-order Look at Stability and Generalization. Conference on Learning Theory.  A. Maurer. A Bernstein-type inequality for functions of bounded interaction. Bernoulli (Forthcom-  ing), (see also arXiv preprint arXiv:1701.06191). SHORT TITLE  C. McDiarmid. Concentration. In Probabilistic Methods of Algorithmic Discrete Mathematics, p.  195-248. Springer, Berlin, 1998.  T. Peel, S. Anthoine, and L. Ralaivola. Empirical Bernstein inequalities for u-statistics. In Advances  in Neural Information Processing Systems, pp. 1903-1911, 2010.  M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via Stochastic Gradient Langevin  Dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849.  J. M. Steele. An Efron-Stein inequality for nonsymmetric statistics, Annals of Statistics 14:753-758,  1986.  R. V. Mises. On the asymptotic distribution of differentiable statistical functions. The Annals of  Mathematical Statistics, 18(3):309-348, 1947.  The "}, "Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression", "abstract": "High-dimensional settings, where the data dimension ($d$) far exceeds the number of observations ($n$), are common in many statistical and machine learning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is hence natural to ask: what families of matrices satisfy the RE condition?  Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where $\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily {\\em verifiable} condition into the design process, and allows for generation of {\\em compressed} design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix $X$.", "pdf_url": "http://proceedings.mlr.press/v75/kasiviswanathan18a/kasiviswanathan18a.pdf", "keywords": [], "reference": "Radoslaw Adamczak, Alexander E Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Constructive Approximation, 34(1):61-88, 2011.  Afonso S Bandeira, Matthew Fickus, Dustin G Mixon, and Joel Moreira. Derandomizing restricted  isometries via the legendre symbol. Constructive Approximation, 43(3):409-424, 2016.  Afonso S Bandeira, Dustin G Mixon, and Joel Moreira. A conditional construction of restricted  isometries. International Mathematics Research Notices, 2017(2):372-381, 2017.  Peter J Bickel, Ya\u2019acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig  selector. The Annals of Statistics, pages 1705-1732, 2009.  Jean Bourgain, Stephen Dilworth, Kevin Ford, Sergei Konyagin, Denka Kutzarova, et al. Explicit constructions of rip matrices and related problems. Duke Mathematical Journal, 159(1):145-185, 2011.  Emmanuel Candes, Terence Tao, et al. The dantzig selector: Statistical estimation when p is much  larger than n. The Annals of Statistics, 35(6):2313-2351, 2007.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Mahdi Cheraghchi. Coding-theoretic methods for sparse recovery. In Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on, pages 909-916. IEEE, 2011.  Yohann De Castro. Optimal designs for lasso and dantzig selector using expander codes. IEEE  Transactions on Information Theory, 60(11):7293-7299, 2014.  Edgar Dobriban and Jianqing Fan. Regularity properties for sparse regression. Communications in  mathematics and statistics, 4(1):1-19, 2016.  Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge  University Press, 2012.  Mahdi Milani Fard, Yuri Grinberg, Joelle Pineau, and Doina Precup. Compressed least-squares  regression on sparse spaces. In AAAI, 2012.  13   RESTRICTED EIGENVALUE FROM STABLE RANKWITH APPLICATIONS TO SPARSE LINEAR REGRESSION  X\u03a6(cid:62)\u03a6\u03b8(cid:107)2/n + \u03bb(cid:107)\u03b81(cid:107) with \u03bb = \u0398(\u03c3(cid:107)X(cid:107)F log m/n at least 1 \u2212 \u03b2:  \u221a  m + (cid:107)X(cid:107)2  F/nm), satisfies with probability  1 n  (cid:107)X\u03b8comp \u2212 X\u03b8(cid:63)(cid:107)2 = O  (cid:32)  \u03c3(cid:107)X(cid:107)Fk3/2 log m \u221a  +  n  m  (cid:107)X(cid:107)2 Fk3/2 nm  (cid:33)  .  Remark 14 For small \u03c3, the dominant term in the error bound in Proposition 13 is the (cid:107)X(cid:107)2 Fk3/2/nm term. If we set, m = sr(X)/2, then (cid:107)X(cid:107)2 Fk3/2/nm = 2(cid:107)X(cid:107)2k3/2/n, and therefore in this case we \u221a n/k3/4). get a consistent prediction if (cid:107)X(cid:107) = o(  References  Radoslaw Adamczak, Alexander E Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Constructive Approximation, 34(1):61-88, 2011.  Afonso S Bandeira, Matthew Fickus, Dustin G Mixon, and Joel Moreira. Derandomizing restricted  isometries via the legendre symbol. Constructive Approximation, 43(3):409-424, 2016.  Afonso S Bandeira, Dustin G Mixon, and Joel Moreira. A conditional construction of restricted  isometries. International Mathematics Research Notices, 2017(2):372-381, 2017.  Peter J Bickel, Ya\u2019acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig  selector. The Annals of Statistics, pages 1705-1732, 2009.  Jean Bourgain, Stephen Dilworth, Kevin Ford, Sergei Konyagin, Denka Kutzarova, et al. Explicit constructions of rip matrices and related problems. Duke Mathematical Journal, 159(1):145-185, 2011.  Emmanuel Candes, Terence Tao, et al. The dantzig selector: Statistical estimation when p is much  larger than n. The Annals of Statistics, 35(6):2313-2351, 2007.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Mahdi Cheraghchi. Coding-theoretic methods for sparse recovery. In Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on, pages 909-916. IEEE, 2011.  Yohann De Castro. Optimal designs for lasso and dantzig selector using expander codes. IEEE  Transactions on Information Theory, 60(11):7293-7299, 2014.  Edgar Dobriban and Jianqing Fan. Regularity properties for sparse regression. Communications in  mathematics and statistics, 4(1):1-19, 2016.  Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge  University Press, 2012.  Mahdi Milani Fard, Yuri Grinberg, Joelle Pineau, and Doina Precup. Compressed least-squares  regression on sparse spaces. In AAAI, 2012. KASIVISWANATHAN RUDELSON  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the  lasso and generalizations. CRC Press, 2015.  Ata Kab\u00e1n. New bounds on compressive linear least squares regression. In The 17-th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), volume 33, pages 448-456, 2014.  Guillaume Lecu\u00e9 and Shahar Mendelson. Sparse recovery under weak moment assumptions. Journal  of the European Mathematical Society, 19(3):881-904, 2017.  Jason D Lee, Yuekai Sun, Qiang Liu, and Jonathan E Taylor. Communication-efficient sparse  regression: a one-shot approach. arXiv preprint arXiv:1503.04337, 2015.  Odalric Maillard and R\u00e9mi Munos. Compressed least-squares regression. In NIPS, 2009.  Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann. Uniform uncertainty principle for  bernoulli and subgaussian ensembles. Constructive Approximation, 28(3):277-289, 2008.  Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4), 2012.  Roberto Imbuzeiro Oliveira. The lower tail of random quadratic forms with applications to ordinary  least squares. Probability Theory and Related Fields, 166(3-4):1175-1194, 2016.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated  gaussian designs. JMLR, 11:2241-2259, 2010.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high- dimensional linear regression over-balls. Information Theory, IEEE Transactions on, 57(10): 6976-6994, 2011.  Mark Rudelson and Roman Vershynin. On sparse reconstruction from fourier and gaussian measure-  ments. Communications on Pure and Applied Mathematics, 61(8):1025-1045, 2008.  Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.  Electronic Communications in Probability, 18, 2013.  Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements. Infor-  mation Theory, IEEE Transactions on, 59(6):3434-3447, 2013.  Vidyashankar Sivakumar, Arindam Banerjee, and Pradeep K Ravikumar. Beyond sub-gaussian measurements: High-dimensional structured estimation with sub-exponential designs. In Advances in neural information processing systems, pages 2206-2214, 2015.  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), pages 267-288, 1996. RESTRICTED EIGENVALUE FROM STABLE RANKWITH APPLICATIONS TO SPARSE LINEAR REGRESSION  Ryan Tibshirani and Larry Wasserman. Sparsity and the lasso, http://www.stat.cmu.edu/  ~larry/=sml/sparsity.pdf, 2015.  Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained quadratic programming (lasso). Information Theory, IEEE Transactions on, 55(5), 2009.  David P. Woodruff. Sketching as a tool for numerical linear algebra. FnT-TCS, 10(1-2):1-157, 2014.  URL http://dx.doi.org/10.1561/0400000060.  Shuheng Zhou, John Lafferty, and Larry Wasserman. Compressed and privacy-sensitive sparse  regression. Information Theory, IEEE Transactions on, 55(2):846-866, 2009.  "}, "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent", "abstract": "Nesterov\u2019s accelerated gradient descent (AGD), an instance of the general family of \u201cmomentum methods,\u201d provably achieves faster convergence rate than gradient descent (GD) in the convex setting. While these methods are widely used in modern \\emph{nonconvex} applications, including training of deep neural networks, whether they are provably superior to GD in the nonconvex setting remains open. This paper studies a simple variant of Nesterov\u2019s\u00a0AGD, and shows that it escapes saddle points and finds a second-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations, matching the best known convergence rate, which is faster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the best of our knowledge, this is the first direct acceleration (single-loop) algorithm that is provably faster than GD in general nonconvex setting\u2014all previous nonconvex accelerated algorithms rely on more complex mechanisms such as nested loops and proximal terms. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases on each step even for nonconvex functions, and (2) a novel framework called\u00a0\\emph{improve or localize}, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization.", "pdf_url": "http://proceedings.mlr.press/v75/jin18a/jin18a.pdf", "keywords": [], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195-1199. ACM, 2017.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and  mirror descent. arXiv preprint arXiv:1407.1537, 2014.  Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Conference on Learning Theory, pages 361-382, 2016.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016.  S\u00b4ebastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov\u2019s accelerated  gradient descent. arXiv preprint arXiv:1506.08187, 2015.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. Convex until Proven Guilty: arXiv preprint  Dimension-free acceleration of gradient descent on non-convex functions. arXiv:1705.02766, 2017.  Coralia Cartis, Nicholas Gould, and Ph L Toint. On the complexity of steepest descent, Newton\u2019s and regularized Newton\u2019s methods for nonconvex unconstrained optimization problems. Siam journal on optimization, 20(6):2833-2852, 2010.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The  loss surface of multilayer networks. arXiv:1412.0233, 2014.  Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of o((cid:15)\u22123/2) for nonconvex optimization. Mathematical Program- ming, pages 1-32, 2014.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Identifying and attacking the saddle point problem in high-dimensional non-convex  Bengio. optimization. In Advances in Neural Information Processing Systems, pages 2933-2941, 2014.  13   AGD ESCAPES SADDLE POINTS FASTER THAN GD  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195-1199. ACM, 2017.  Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. arXiv  preprint arXiv:1711.06673, 2017.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and  mirror descent. arXiv preprint arXiv:1407.1537, 2014.  Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Conference on Learning Theory, pages 361-382, 2016.  Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873- 3881, 2016.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016.  S\u00b4ebastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov\u2019s accelerated  gradient descent. arXiv preprint arXiv:1506.08187, 2015.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. Convex until Proven Guilty: arXiv preprint  Dimension-free acceleration of gradient descent on non-convex functions. arXiv:1705.02766, 2017.  Coralia Cartis, Nicholas Gould, and Ph L Toint. On the complexity of steepest descent, Newton\u2019s and regularized Newton\u2019s methods for nonconvex unconstrained optimization problems. Siam journal on optimization, 20(6):2833-2852, 2010.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The  loss surface of multilayer networks. arXiv:1412.0233, 2014.  Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a worst-case iteration complexity of o((cid:15)\u22123/2) for nonconvex optimization. Mathematical Program- ming, pages 1-32, 2014.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Identifying and attacking the saddle point problem in high-dimensional non-convex  Bengio. optimization. In Advances in Neural Information Processing Systems, pages 2933-2941, 2014. AGD ESCAPES SADDLE POINTS FASTER THAN GD  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on Computational Learning Theory (COLT), 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and  stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape  saddle points efficiently. In International Conference on Machine Learning (ICML), 2017.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information  Processing Systems, pages 586-594, 2016.  Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and faster algo- rithms for solving linear systems. In Foundations of Computer Science (FOCS), pages 147-156. IEEE, 2013.  Huan Li and Zhouchen Lin. Provable accelerated gradient method for nonconvex low rank opti-  mization. arXiv preprint arXiv:1702.04959, 2017.  Song Mei, Theodor Misiakiewicz, Andrea Montanari, and Roberto I Oliveira. Solving SDPs for synchronization and maxcut problems via the Grothendieck inequality. In Conference on Learn- ing Theory (COLT), pages 1476-1515, 2017.  Ion Necoara, Yurii Nesterov, and Francois Glineur. Linear convergence of first order methods for  non-strongly convex optimization. arXiv preprint arXiv:1504.06298, 2015.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).  Soviet Mathematics Doklady, 27:372-376, 1983.  Yurii Nesterov. Introductory Lectures on Convex Programming Volume I: Basic course. Springer,  Yurii Nesterov. Introductory Lectures on Convex Optimization, volume 87. Springer Science &  1998.  Business Media, 2004.  Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.  SIAM Journal on Optimization, 22(2):341-362, 2012.  Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global perfor-  mance. Mathematical Programming, 108(1):177-205, 2006.  Michael O\u2019Neill and Stephen J Wright. Behavior of accelerated gradient methods near critical  points of nonconvex problems. arXiv preprint arXiv:1706.07993, 2017. AGD ESCAPES SADDLE POINTS FASTER THAN GD  Cl\u00b4ement W Royer and Stephen J Wright. Complexity analysis of second-order line-search algo-  rithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017.  Weijie Su, Stephen Boyd, and Emmanuel J Candes. A differential equation for modeling Nesterov\u2019s accelerated gradient method: theory and insights. Journal of Machine Learning Research, 17 (153):1-43, 2016.  Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial- ization and momentum in deep learning. In International conference on machine learning, pages 1139-1147, 2013.  A. Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 133:E7351-E7358, 2016.  Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A Lyapunov analysis of momentum  methods in optimization. arXiv preprint arXiv:1611.02635, 2016.  Yi Xu, Rong Jin, and Tianbao Yang. Neon+: Accelerated gradient methods for extracting negative  curvature for non-convex optimization. arXiv preprint arXiv:1712.01033, 2017. AGD ESCAPES SADDLE POINTS FASTER THAN GD  "}, "Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing", "abstract": "We consider the problem of minimizing a convex objective function $F$ when one can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes some structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function, making the task of minimizing $F$ intractable. To overcome this, prior work has often focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this paper we study the more general case when the noise has magnitude $\\alpha F(x) + \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm that finds an approximate minimizer of $F$ for this noise model. Previously, Markov chains, such as the stochastic gradient Langevin dynamics, have been used to arrive at approximate solutions to these optimization problems. However, for the noise model considered in this paper, no single temperature allows such a Markov chain to both mix quickly and concentrate near the global minimizer. We bypass this by combining \u201csimulated annealing\" with the stochastic gradient Langevin dynamics, and gradually decreasing the temperature of the chain in order to approach the global minimizer. As a corollary one can approximately minimize a nonconvex function that is close to a convex function; however, the closeness can deteriorate as one moves away from the optimum.", "pdf_url": "http://proceedings.mlr.press/v75/mangoubi18a/mangoubi18a.pdf", "keywords": ["Nonconvex optimization", "Stochastic gradient Langevin dynamics", "Simnulated annealing"], "reference": "David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Advances  in neural information processing systems, pages 494-501, 1989.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected Langevin Monte Carlo. In Advances in Neural Information Processing Systems, pages 1243-1251, 2015.  Ruobing Chen. Stochastic derivative-free optimization of noisy functions. Ph.D. Thesis, Lehigh  University, 2015.  Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region  method and random models. Mathematical Programming, pages 1-41, 2015.  K Andrew Cliffe, Mike B Giles, Robert Scheichl, and Aretha L Teckentrup. Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients. Computing and Visualization in Science, 14(1):3, 2011.  Patrick R Conrad, Andrew D Davis, Youssef M Marzouk, Natesh S Pillai, and Aaron Smith. Parallel local approximation MCMC for expensive models. SIAM/ASA Journal on Uncertainty Quantifi- cation, 6(1):339-373, 2018.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic In International Conference on Machine Learning, pages 1833-1841,  non-convex problems. 2016.  Mohamed Jebalia and Anne Auger. On multiplicative noise models for stochastic search. In Inter- national Conference on Parallel Problem Solving from Nature, pages 52-61. Springer, 2008.  Mohamed Jebalia, Anne Auger, and Nikolaus Hansen. Log-linear convergence and divergence of  the scale-invariant (1 + 1)-ES in noisy environments. Algorithmica, 59(3):425-460, 2011.  Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.  science, 220(4598):671-680, 1983.  13   CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES  References  David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 156- 163. ACM, 1991.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Con- ference on Learning Theory, pages 240-265, 2015.  Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Advances  in neural information processing systems, pages 494-501, 1989.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected Langevin Monte Carlo. In Advances in Neural Information Processing Systems, pages 1243-1251, 2015.  Ruobing Chen. Stochastic derivative-free optimization of noisy functions. Ph.D. Thesis, Lehigh  University, 2015.  Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region  method and random models. Mathematical Programming, pages 1-41, 2015.  K Andrew Cliffe, Mike B Giles, Robert Scheichl, and Aretha L Teckentrup. Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients. Computing and Visualization in Science, 14(1):3, 2011.  Patrick R Conrad, Andrew D Davis, Youssef M Marzouk, Natesh S Pillai, and Aaron Smith. Parallel local approximation MCMC for expensive models. SIAM/ASA Journal on Uncertainty Quantifi- cation, 6(1):339-373, 2018.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.  Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic In International Conference on Machine Learning, pages 1833-1841,  non-convex problems. 2016.  Mohamed Jebalia and Anne Auger. On multiplicative noise models for stochastic search. In Inter- national Conference on Parallel Problem Solving from Nature, pages 52-61. Springer, 2008.  Mohamed Jebalia, Anne Auger, and Nikolaus Hansen. Log-linear convergence and divergence of  the scale-invariant (1 + 1)-ES in noisy environments. Algorithmica, 59(3):425-460, 2011.  Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.  science, 220(4598):671-680, 1983. CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. To appear in Proceedings of STOC 2018, arXiv preprint arXiv:1710.06261, 2017.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic In Conference on Learning Theory,  gradient Langevin dynamics: a nonasymptotic analysis. pages 1674-1703, 2017.  Andrej Risteski and Yuanzhi Li. Algorithms and matching lower bounds for approximately-convex optimization. In Advances in Neural Information Processing Systems, pages 4745-4753, 2016.  Mark Rudelson and Roman Vershynin. Hanson-Wright inequality and sub-Gaussian concentration.  Electron. Commun. Probab, 18(82):1-9, 2013.  Yaron Singer and Jan Vondr\u00b4ak. Information-theoretic lower bounds for convex optimization with erroneous oracles. In Advances in Neural Information Processing Systems, pages 3204-3212, 2015.  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681- 688, 2011.  Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient  Langevin dynamics. In Conference on Learning Theory, pages 1980-2022, 2017. CONVEX OPTIMIZATION WITH UNBOUNDED NONCONVEX ORACLES  "}, "Learning Mixtures of Linear Regressions with Nearly Optimal Complexity": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Learning Mixtures of Linear Regressions with Nearly Optimal Complexity", "abstract": "Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity $N = \\tilde{O}(d)$ as well as nearly optimal computational complexity $\\tilde{O}(Nd)$, where $d$ is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting.", "pdf_url": "http://proceedings.mlr.press/v75/li18b/li18b.pdf", "keywords": [], "reference": "Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing  pain. In Advances in Neural Information Processing Systems, pages 974-982, 2016.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures.  arXiv preprint arXiv:1706.01596, 2017.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Arun T Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1040-1048, 2013.  Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory, pages 560- 604, 2014.  11   LEARNING MIXTURES OF LINEAR REGRESSIONS WITH NEARLY OPTIMAL COMPLEXITY  6. Conclusion  In this paper, we present a fixed parameter algorithm that solves mixture of linear regression under Gaussian inputs in time nearly linear in the sample size and the dimension. Moreover, our sample complexity also scales nearly linear with the dimension d. In our setting, we allow each mixture to have a different covariance matrix. Thus, unlike the case when the mixtures are spherical, even the best known algorithm for mixture of general Gaussians would require at least d2 sample complexity to recover the covariance. Our algorithm reduces the sample complexity significantly with the additional one dimensional linear information: it can recover the linear classifier (and thus recover the covariance as well) with ~O(d) samples. While the dependency on d is nearly optimal, we would also like to point out that when the total number of mixtures are too large, the sample complexity of our algorithm does suffer from an exponential term of k. We believe that with our current set of assumptions, the exponential dependency could be necessary: A lower bound of ek has been proved in (Moitra and Valiant, 2010) in the very similar setting of learning mixture of Gaussians.  One natural way to get around the exponential dependency is assuming that the covariance (cid:6)i and the hidden vectors wi satisfies some smoothness assumption (e.g., (Ge et al., 2015)). However, the level of smoothness is very subtle in our setting, since the na\u00a8\u0131ve application of smoothed analysis often leads to complexity with a large polynomial factor in the dimension. In this paper, near linearity in d is one of our main contributions. We believe that using smoothed analysis while preserving the nearly linear dependency on d is one of the important future directions.  Yingyu Liang would like to acknowledge that support for this research was provided by the Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin Madison with funding from the Wisconsin Alumni Research Foundation.  Acknowledgments  References  Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing  pain. In Advances in Neural Information Processing Systems, pages 974-982, 2016.  Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures.  arXiv preprint arXiv:1706.01596, 2017.  Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Arun T Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1040-1048, 2013.  Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory, pages 560- 604, 2014. LEARNING MIXTURES OF LINEAR REGRESSIONS WITH NEARLY OPTIMAL COMPLEXITY  Richard D De Veaux. Mixtures of linear regressions. Computational Statistics & Data Analysis, 8  (3):227-245, 1989.  Susana Faria and Gilda Soromenho. Fitting mixtures of linear regressions. Journal of Statistical  Computation and Simulation, 80(2):201-225, 2010.  Scott Gaffney and Padhraic Smyth. Trajectory clustering with mixtures of regression models. In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 63-72. ACM, 1999.  Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen- In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,  sions. pages 761-770. ACM, 2015.  Bettina Gr\u00a8un, Friedrich Leisch, et al. Applications of finite mixtures of regression models. 2007.  Nina Holden, Yuval Peres, and Alex Zhai. Gravitational allocation for uniform points on the sphere.  arXiv preprint arXiv:1704.08238, 2017.  Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.  Neural computation, 6(2):181-214, 1994.  Abbas Khalili and Jiahua Chen. Variable selection in finite mixture of regression models. Journal  of the american Statistical association, 102(479):1025-1038, 2007.  Jason M Klusowski, Dana Yang, and WD Brinda. Estimating the coefficients of a mixture of two  linear regressions by expectation maximization. arXiv preprint arXiv:1704.08231, 2017.  Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.  In Advances in Neural Information Processing Systems, pages 597-607, 2017.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93- 102. IEEE, 2010.  Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pages 1223-1231, 2016.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear  regression. In International Conference on Machine Learning, pages 613-621, 2014.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.  Solving a mixture of many ran- dom linear equations by tensor decomposition and alternating minimization. arXiv preprint arXiv:1608.05749, 2016.  Kai Zhong, Prateek Jain, and Inderjit S Dhillon. Mixed linear regression with multiple components.  In Advances in Neural Information Processing Systems, pages 2190-2198, 2016. LEARNING MIXTURES OF LINEAR REGRESSIONS WITH NEARLY OPTIMAL COMPLEXITY  "}, "Detecting Correlations with Little Memory and Communication": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Detecting Correlations with Little Memory and Communication", "abstract": "We study the problem of identifying correlations in multivariate data, under information constraints: Either on the amount of memory that can be used by the algorithm, or the amount of communication when the data is distributed across several machines. We prove a tight trade-off between the memory/communication complexity and the sample complexity, implying (for example) that to detect pairwise correlations with optimal sample complexity, the number of required memory/communication bits is at least quadratic in the dimension. Our results substantially improve those of Shamir (2014), which studied a similar question in a much more restricted setting. To the best of our knowledge, these are the first provable sample/memory/communication trade-offs for a practical estimation problem, using standard distributions, and in the natural regime where the memory/communication budget is larger than the size of a single data point. To derive our theorems, we prove a new information-theoretic result, which may be relevant for studying other information-constrained learning problems.", "pdf_url": "http://proceedings.mlr.press/v75/dagan18a/dagan18a.pdf", "keywords": [], "reference": "Rudolf Ahlswede and Peter G\u00e1cs. Spreading of sets in product spaces and hypercontraction of the  markov operator. The annals of probability, pages 925-939, 1976.  Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the fre-  quency moments. In STOC, 1996.  Z. Bar-Yossef, T. Jayram, R. Kumar, and D. Sivakumar. An information statistics approach to data  stream and communication complexity. In FOCS, 2002.  Ziv Bar-Yossef, Thathachar S Jayram, Ravi Kumar, and D Sivakumar. An information statistics ap- proach to data stream and communication complexity. Journal of Computer and System Sciences, 68(4):702-732, 2004.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning from small test spaces: Learning low degree polynomial functions. arXiv preprint arXiv:1708.02640, 2017.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011- 1020. ACM, 2016.  Harald Cram\u00e9r. Mathematical methods of statistics (PMS-9), volume 9. Princeton university press,  2016.  Michael Crouch, Andrew McGregor, Gregory Valiant, and David P Woodruff. Stochastic streams: Sample complexity vs. space complexity. In LIPIcs-Leibniz International Proceedings in Infor- matics, volume 57. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.  E. Ertin and L. Potter. Sequential detection with limited memory. In Statistical Signal Processing,  2003 IEEE Workshop on, pages 585-588, 2003.  Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical esti- mation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726- 2734, 2014.  Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for learning.  arXiv preprint arXiv:1708.02639, 2017.  M. Hellman and T. Cover. Learning with finite memory. Annals of Mathematical Statistics, pages  765-782, 1970.  TS Jayram. Hellinger strikes back: A note on the multi-party information complexity of and. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 562-573. Springer, 2009.  Gillat Kol, Ran Raz, and Avishay Tal. Time-space hardness of learning sparse parities. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1067-1080. ACM, 2017.  13   DETECTING CORRELATIONS WITH LITTLE MEMORY AND COMMUNICATION  References  Rudolf Ahlswede and Peter G\u00e1cs. Spreading of sets in product spaces and hypercontraction of the  markov operator. The annals of probability, pages 925-939, 1976.  Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the fre-  quency moments. In STOC, 1996.  Z. Bar-Yossef, T. Jayram, R. Kumar, and D. Sivakumar. An information statistics approach to data  stream and communication complexity. In FOCS, 2002.  Ziv Bar-Yossef, Thathachar S Jayram, Ravi Kumar, and D Sivakumar. An information statistics ap- proach to data stream and communication complexity. Journal of Computer and System Sciences, 68(4):702-732, 2004.  Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning from small test spaces: Learning low degree polynomial functions. arXiv preprint arXiv:1708.02640, 2017.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011- 1020. ACM, 2016.  Harald Cram\u00e9r. Mathematical methods of statistics (PMS-9), volume 9. Princeton university press,  2016.  Michael Crouch, Andrew McGregor, Gregory Valiant, and David P Woodruff. Stochastic streams: Sample complexity vs. space complexity. In LIPIcs-Leibniz International Proceedings in Infor- matics, volume 57. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.  E. Ertin and L. Potter. Sequential detection with limited memory. In Statistical Signal Processing,  2003 IEEE Workshop on, pages 585-588, 2003.  Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical esti- mation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726- 2734, 2014.  Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for learning.  arXiv preprint arXiv:1708.02639, 2017.  M. Hellman and T. Cover. Learning with finite memory. Annals of Mathematical Statistics, pages  765-782, 1970.  TS Jayram. Hellinger strikes back: A note on the multi-party information complexity of and. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 562-573. Springer, 2009.  Gillat Kol, Ran Raz, and Avishay Tal. Time-space hardness of learning sparse parities. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1067-1080. ACM, 2017. DETECTING CORRELATIONS WITH LITTLE MEMORY AND COMMUNICATION  L. Kontorovich. Statistical estimation with bounded memory. Statistics and Computing, 22(5):  1155-1164, 2012.  Eyal Kushilevitz and Noam Nisan. Communication complexity. Cambridge University Press, 1997.  Jason D Lee, Qiang Liu, Yuekai Sun, and Jonathan E Taylor. Communication-efficient sparse  regression. Journal of Machine Learning Research, 18(5):1-30, 2017.  Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business  Media, 2006.  F. Leighton and R. Rivest. Estimating a probability using finite memory. Information Theory, IEEE  Transactions on, 32(6):733-742, 1986.  Zhi-Quan Luo. Universal decentralized estimation in a bandwidth constrained sensor network. IEEE  Transactions on information theory, 51(6):2210-2219, 2005.  Dana Moshkovitz and Michal Moshkovitz. Mixing implies lower bounds for space bounded learn-  ing. In Conference on Learning Theory, pages 1516-1566, 2017a.  Michal Moshkovitz and Dana Moshkovitz. Mixing implies strong lower bounds for space bounded  learning. Electronic Colloquium on Computational Complexity (ECCC), 24:116, 2017b.  S. Muthukrishnan. Data streams: Algorithms and applications. Now Publishers Inc, 2005.  Yury Polyanskiy and Yihong Wu. Strong data-processing inequalities for channels and bayesian  networks. In Convexity and Concentration, pages 211-249. Springer, 2017.  C Radhakrishna Rao. Information and the accuracy attainable in the estimation of statistical param-  eters. In Breakthroughs in statistics, pages 235-247. Springer, 1992.  Ran Raz. Fast learning requires good memory: A time-space lower bound for parity learning. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 266- 275. IEEE, 2016.  Ran Raz. A time-space lower bound for a large class of learning problems. In Electronic Colloquium  on Computational Complexity (ECCC), volume 24, page 20, 2017.  Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and  estimation. In Advances in Neural Information Processing Systems, pages 163-171, 2014.  Jacob Steinhardt and John Duchi. Minimax rates for memory-bounded sparse linear regression. In  Conference on Learning Theory, pages 1564-1587, 2015.  Jacob Steinhardt, Gregory Valiant, and Stefan Wager. Memory, communication, and statistical  queries. In Conference on Learning Theory, pages 1490-1516, 2016.  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities  and the closest pair problem. Journal of the ACM (JACM), 62(2):13, 2015.  LG Valiant. Functionality in neural nets. In Proceedings of the first annual workshop on Computa-  tional learning theory, pages 28-39. Morgan Kaufmann Publishers Inc., 1988. DETECTING CORRELATIONS WITH LITTLE MEMORY AND COMMUNICATION  Yuchen Zhang, Martin J Wainwright, and John C Duchi. Communication-efficient algorithms for In Advances in Neural Information Processing Systems, pages 1502-  statistical optimization. 1510, 2012.  Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright.  Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, pages 2328-2336, 2013.  "}, "Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning", "abstract": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated using distinct stepsizes. In this work, we develop a novel recipe for their finite sample analysis. Using this, we provide a concentration bound, which is the first such result for a two-timescale SA. The type of bound we obtain is known as \u201clock-in probability\u201d. We also introduce a new projection scheme, in which the time between successive projections increases exponentially. This scheme allows one to elegantly transform a lock-in probability into a convergence rate result for projected two-timescale SA. From this latter result, we then extract key insights on stepsize selection.  As an application, we finally obtain convergence rates for the projected two-timescale RL algorithms GTD(0), GTD2, and TDC.", "pdf_url": "http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf", "keywords": [], "reference": "edition, 2012.  D. P. Bertsekas. Dynamic Programming and Optimal Control. Vol II. Athena Scientific, fourth  Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesv\u00b4ari. Convergent temporal-difference learning with arbitrary smooth function approxi- mation. In Advances in Neural Information Processing Systems, pages 1204-1212, 2009a.  Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic  algorithms. Automatica, 45(11), 2009b.  Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint. 2008.  Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.  Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td(0) with  function approximation. In AAAI, 2018.  Alexandre D\u00b4efossez and Francis Bach. Constant step size least-mean-square: Bias-variance trade-  offs and optimal sampling distributions. arXiv preprint arXiv:1412.0156, 2014.  L\u00b4aszl\u00b4o Gerencs\u00b4er. Rate of convergence of moments of spall\u2019s spsa method. In Control Conference  (ECC), 1997 European, pages 2192-2197. IEEE, 1997.  Morris W Hirsch, Stephen Smale, and Robert L Devaney. Differential equations, dynamical systems,  and an introduction to chaos. Academic press, 2012.  J Zico Kolter. The fixed points of off-policy td. In Advances in Neural Information Processing  Systems, pages 2169-2177, 2011.  Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic approx-  imation. Annals of applied probability, pages 796-819, 2004.  Vijaymohan Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering  and Computer Science, MIT, 6 2002.  Nathaniel Korda and LA Prashanth. On td (0) with function approximation: Concentration bounds  and a centered variant with exponential convergence. In ICML, pages 626-634, 2015.  H Kushner. A projected stochastic approximation method for adaptive filters and identifiers. IEEE  Transactions on Automatic Control, 25(4):836-838, 1980.  Harold J. Kushner and G. George Yin. Stochastic Approximation Algorithms and Applications.  1997.  systems. CRC Press, 1998.  Vangipuram Lakshmikantham and Sadashiv G Deo. Method of variation of parameters for dynamic  Chandrashekar Lakshminarayanan and Shalabh Bhatnagar. A stability criterion for two timescale  stochastic approximation schemes. Automatica, 79:108-114, 2017.  13   TWO-TIMESCALE STOCHASTIC APPROXIMATION  References  edition, 2012.  D. P. Bertsekas. Dynamic Programming and Optimal Control. Vol II. Athena Scientific, fourth  Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesv\u00b4ari. Convergent temporal-difference learning with arbitrary smooth function approxi- mation. In Advances in Neural Information Processing Systems, pages 1204-1212, 2009a.  Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic  algorithms. Automatica, 45(11), 2009b.  Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint. 2008.  Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.  Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td(0) with  function approximation. In AAAI, 2018.  Alexandre D\u00b4efossez and Francis Bach. Constant step size least-mean-square: Bias-variance trade-  offs and optimal sampling distributions. arXiv preprint arXiv:1412.0156, 2014.  L\u00b4aszl\u00b4o Gerencs\u00b4er. Rate of convergence of moments of spall\u2019s spsa method. In Control Conference  (ECC), 1997 European, pages 2192-2197. IEEE, 1997.  Morris W Hirsch, Stephen Smale, and Robert L Devaney. Differential equations, dynamical systems,  and an introduction to chaos. Academic press, 2012.  J Zico Kolter. The fixed points of off-policy td. In Advances in Neural Information Processing  Systems, pages 2169-2177, 2011.  Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic approx-  imation. Annals of applied probability, pages 796-819, 2004.  Vijaymohan Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering  and Computer Science, MIT, 6 2002.  Nathaniel Korda and LA Prashanth. On td (0) with function approximation: Concentration bounds  and a centered variant with exponential convergence. In ICML, pages 626-634, 2015.  H Kushner. A projected stochastic approximation method for adaptive filters and identifiers. IEEE  Transactions on Automatic Control, 25(4):836-838, 1980.  Harold J. Kushner and G. George Yin. Stochastic Approximation Algorithms and Applications.  1997.  systems. CRC Press, 1998.  Vangipuram Lakshmikantham and Sadashiv G Deo. Method of variation of parameters for dynamic  Chandrashekar Lakshminarayanan and Shalabh Bhatnagar. A stability criterion for two timescale  stochastic approximation schemes. Automatica, 79:108-114, 2017. TWO-TIMESCALE STOCHASTIC APPROXIMATION  Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics, pages 1347-1355, 2018.  Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample  analysis of proximal gradient td algorithms. In UAI, pages 504-513. Citeseer, 2015.  Hamid Reza Maei. Gradient temporal-difference learning algorithms. 2011.  Hamid Reza Maei, Csaba Szepesv\u00b4ari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy  learning control with function approximation. In ICML, pages 719-726, 2010.  Abdelkader Mokkadem and Mariane Pelletier. Convergence rate and averaging of nonlinear two- time-scale stochastic approximation algorithms. The Annals of Applied Probability, 16(3):1671- 1702, 2006.  Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180-1190, 2008.  James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient  approximation. IEEE transactions on automatic control, 37(3):332-341, 1992.  Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3  (1):9-44, 1988.  Richard S Sutton, Hamid R Maei, and Csaba Szepesv\u00b4ari. A convergent o(n) temporal-difference In Advances in neural  algorithm for off-policy learning with linear function approximation. information processing systems, pages 1609-1616, 2009a.  Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv\u00b4ari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993-1000. ACM, 2009b.  Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17:1-29, 2015.  Gerald Teschl. Ordinary differential equations and dynamical systems. 2004.  Gugan Thoppe and Vivek S Borkar. A concentration bound for stochastic approximation via alek-  seev\u2019s formula. arXiv:1506.08657, 2015.  John N Tsitsiklis, Benjamin Van Roy, et al. An analysis of temporal-difference learning with func-  tion approximation. IEEE transactions on automatic control, 42(5):674-690, 1997. TWO-TIMESCALE STOCHASTIC APPROXIMATION  "}, "Near-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Near-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities", "abstract": "We study the problem of learning multivariate log-concave densities with respect to a global loss function. We obtain the first upper bound on the sample complexity of the maximum likelihood estimator (MLE) for a log-concave density on $\\mathbb{R}^d$, for all $d \\geq 4$. Prior to this work, no finite sample upper bound was known for this estimator in more than $3$ dimensions. In more detail, we prove that for any $d \\geq 1$ and $\\epsilon>0$, given  $\\tilde{O}_d((1/\\epsilon)^{(d+3)/2})$ samples drawn from an unknown log-concave density $f_0$ on $\\mathbb{R}^d$, the MLE outputs a hypothesis $h$ that with high probability is $\\epsilon$-close to $f_0$, in squared Hellinger loss. A sample complexity lower bound of $\\Omega_d((1/\\epsilon)^{(d+1)/2})$ was previously known for any learning algorithm that achieves this guarantee. We thus establish that the sample complexity of the log-concave MLE is near-optimal, up to an $\\tilde{O}(1/\\epsilon)$ factor.", "pdf_url": "http://proceedings.mlr.press/v75/carpenter18a/carpenter18a.pdf", "keywords": [], "reference": "J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly- In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete linear time. Algorithms, SODA 2017, pages 1278-1289, 2017. Available at https://arxiv.org/abs/1506.00671.  N. Alon, J. Spencer, and P. Erdos. The Probabilistic Method. Wiley-Interscience, New York, 1992.  M. Y. An. Log-concave probability distributions: Theory and statistical testing. Technical Report  Economics Working Paper Archive at WUSTL, Washington University at St. Louis, 1995.  M. Bagnoli and T. Bergstrom. Log-concave probability and its applications. Economic Theory, ISSN 09382259. URL http://www.jstor.org/stable/  26(2):pp. 445-469, 2005. 25055959.  F. Balabdaoui and C. R. Doss. Inference for a two-component mixture of symmetric distributions  under log-concavity. Bernoulli, 24(2):1053-1071, 05 2018. doi: 10.3150/16-BEJ864.  F. Balabdaoui and J. A. Wellner. Estimation of a k-monotone density: Limit distribution theory and the spline connection. The Annals of Statistics, 35(6):pp. 2536-2564, 2007. ISSN 00905364.  F. Balabdaoui and J. A. Wellner. Estimation of a k-monotone density: characterizations, consistency  and minimax lower bounds. Statistica Neerlandica, 64(1):45-70, 2010.  F. Balabdaoui, K. Rufibach, and J. A. Wellner. Limit distribution theory for maximum likelihood estimation of a log-concave density. The Annals of Statistics, 37(3):pp. 1299-1331, 2009. ISSN 00905364.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  L. Birg\u00b4e. Estimating a density under order restrictions: Nonasymptotic minimax risk. Annals of  Statistics, 15(3):995-1012, 1987a.  L. Birg\u00b4e. On the risk of histograms for estimating decreasing densities. Annals of Statistics, 15(3):  1013-1022, 1987b.  H. D. Brunk. On the estimation of parameters restricted by inequalities. The Annals of Mathematical  Statistics, 29(2):pp. 437-454, 1958. ISSN 00034851.  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of  discrete distributions. In STACS, pages 25:1-25:14, 2016.  K.S. Chan and H. Tong. Testing for multimodality with dependent data. Biometrika, 91(1):113-123,  2004.  13   ON THE MLE OF MULTIVARIATE LOG-CONCAVE DENSITIES  References  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly- In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete linear time. Algorithms, SODA 2017, pages 1278-1289, 2017. Available at https://arxiv.org/abs/1506.00671.  N. Alon, J. Spencer, and P. Erdos. The Probabilistic Method. Wiley-Interscience, New York, 1992.  M. Y. An. Log-concave probability distributions: Theory and statistical testing. Technical Report  Economics Working Paper Archive at WUSTL, Washington University at St. Louis, 1995.  M. Bagnoli and T. Bergstrom. Log-concave probability and its applications. Economic Theory, ISSN 09382259. URL http://www.jstor.org/stable/  26(2):pp. 445-469, 2005. 25055959.  F. Balabdaoui and C. R. Doss. Inference for a two-component mixture of symmetric distributions  under log-concavity. Bernoulli, 24(2):1053-1071, 05 2018. doi: 10.3150/16-BEJ864.  F. Balabdaoui and J. A. Wellner. Estimation of a k-monotone density: Limit distribution theory and the spline connection. The Annals of Statistics, 35(6):pp. 2536-2564, 2007. ISSN 00905364.  F. Balabdaoui and J. A. Wellner. Estimation of a k-monotone density: characterizations, consistency  and minimax lower bounds. Statistica Neerlandica, 64(1):45-70, 2010.  F. Balabdaoui, K. Rufibach, and J. A. Wellner. Limit distribution theory for maximum likelihood estimation of a log-concave density. The Annals of Statistics, 37(3):pp. 1299-1331, 2009. ISSN 00905364.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  L. Birg\u00b4e. Estimating a density under order restrictions: Nonasymptotic minimax risk. Annals of  Statistics, 15(3):995-1012, 1987a.  L. Birg\u00b4e. On the risk of histograms for estimating decreasing densities. Annals of Statistics, 15(3):  1013-1022, 1987b.  H. D. Brunk. On the estimation of parameters restricted by inequalities. The Annals of Mathematical  Statistics, 29(2):pp. 437-454, 1958. ISSN 00034851.  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of  discrete distributions. In STACS, pages 25:1-25:14, 2016.  K.S. Chan and H. Tong. Testing for multimodality with dependent data. Biometrika, 91(1):113-123,  2004. ON THE MLE OF MULTIVARIATE LOG-CONCAVE DENSITIES  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  Y. Chen and R. J. Samworth. Smoothed log-concave maximum likelihood estimation with applica-  tions. Statist. Sinica, 23:1373-1398, 2013.  M. Cule, R. Samworth, and M. Stewart. Maximum likelihood estimation of a multi-dimensional  log-concave density. Journal of the Royal Statistical Society: Series B, 72:545-607, 2010.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of Inde-  pendent Integer Random Variables. In FOCS, pages 217-226, 2013.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and its applications. In Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, STOC \u201916, 2016.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  I. Diakonikolas, D. M. Kane, and A. Stewart. Optimal learning via the fourier transform In Proceedings of the 29th Confer- Full version available at  for sums of independent integer random variables. ence on Learning Theory, COLT 2016, pages 831-849, 2016a. https://arxiv.org/abs/1505.00662.  I. Diakonikolas, D. M. Kane, and A. Stewart. Properly learning poisson binomial distributions in almost polynomial time. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 850-878, 2016b. Full version available at https://arxiv.org/abs/1511.04066.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distri-  butions and its algorithmic applications. In Proceedings of STOC\u201916, 2016c.  I. Diakonikolas, D. M. Kane, and A. Stewart. Efficient Robust Proper Learning of Log-concave  Distributions. Arxiv report, 2016d.  I. Diakonikolas, D. M. Kane, and A. Stewart. Learning multivariate log-concave distributions. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, pages 711-727, 2017. URL http://proceedings.mlr.press/v65/diakonikolas17a.html.  C. R. Doss and J. A. Wellner. Global rates of convergence of the mles of log-concave and s-concave  densities. Ann. Statist., 44(3):954-981, 06 2016. ON THE MLE OF MULTIVARIATE LOG-CONCAVE DENSITIES  L. Dumbgen and K. Rufibach. Maximum likelihood estimation of a log-concave density and its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40-68, 2009.  A.-L. Foug`eres. Estimation de densit\u00b4es unimodales. Canadian Journal of Statistics, 25:375-387,  1997.  F. Gao and J. A. Wellner. On the rate of convergence of the maximum likelihood estimator of a  k-monotone density. Science in China Series A: Mathematics, 52:1525-1538, 2009.  Y. Gordon, M. Meyer, and S. Reisner. Constructing a polytope to approximate a convex body.  Geometriae Dedicata, 57(2):217-222, 1995.  U. Grenander. On the theory of mortality measurement. Skand. Aktuarietidskr., 39:125-153, 1956.  P. Groeneboom. Estimating a monotone density. In Proc. of the Berkeley Conference in Honor of  Jerzy Neyman and Jack Kiefer, pages 539-555, 1985.  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  Q. Han and J. A. Wellner. Approximation and estimation of s-concave densities via renyi diver-  gences. Ann. Statist., 44(3):1332-1359, 06 2016.  D. L. Hanson and G. Pledger. Consistency in concave regression. The Annals of Statistics, 4(6):pp.  1038-1050, 1976. ISSN 00905364.  of Statistics, 3:1567-1605, 2009.  H. K. Jankowski and J. A. Wellner. Estimation of a discrete monotone density. Electronic Journal  A. Kim, A. Guntuboyina, and R. J. Samworth. Adaptation in log-concave density estimation. ArXiv  e-prints, 2016. Available at http://arxiv.org/abs/1609.00861.  A. K. H. Kim and R. J. Samworth. Global rates of convergence in log-concave density estimation.  Ann. Statist., 44(6):2756-2779, 12 2016. Available at http://arxiv.org/abs/1404.2298.  R. Koenker and I. Mizera. Quasi-concave density estimation. Ann. Statist., 38(5):2998-3027, 2010.  L. Lov\u00b4asz and S. Vempala. The geometry of logconcave functions and sampling algorithms. Ran-  dom Structures and Algorithms, 30(3):307-358, 2007.  B.L.S. Prakasa Rao. Estimation of a unimodal density. Sankhya Ser. A, 31:23-36, 1969.  E. Robeva, B. Sturmfels, and C. Uhler. Geometry of Log-Concave Density Estimation. ArXiv  e-prints, 2017. Available at https://arxiv.org/abs/1704.01910.  R. J. Samworth. Recent progress in log-concave density estimation. ArXiv e-prints, 2017.  A. Saumard and J. A. Wellner. Log-concavity and strong log-concavity: A review. Statist. Surv., 8:  45-114, 2014. ON THE MLE OF MULTIVARIATE LOG-CONCAVE DENSITIES  R. P. Stanley. Log-concave and unimodal sequences in algebra, combinatorics, and geome- try. Annals of the New York Academy of Sciences, 576(1):500-535, 1989. ISSN 1749- 6632. doi: 10.1111/j.1749-6632.1989.tb16434.x. URL http://dx.doi.org/10.1111/ j.1749-6632.1989.tb16434.x.  G. Valiant and P. Valiant.  In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 142-155, 2016.  Instance optimal learning of discrete distributions.  G. Walther. 2009.  Inference and modeling with log-concave distributions. Stat. Science, 24:319-327,  E.J. Wegman. Maximum likelihood estimation of a unimodal density. I. and II. Ann. Math. Statist.,  41:457-471, 2169-2174, 1970.  J. A. Wellner. Nonparametric estimation of s-concave and log-concave densities: an alternative to maximum likelihood. Talk given at European Meeting of Statisticians, Amsterdam, 2015. Available at https://www.stat.washington.edu/jaw/RESEARCH/TALKS/EMS-2015.1-rev1.pdf.  "}, "More Adaptive Algorithms for Adversarial Bandits": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "More Adaptive Algorithms for Adversarial Bandits", "abstract": "We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of the first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss {\\it and} logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. The main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule.", "pdf_url": "http://proceedings.mlr.press/v75/wei18a/wei18a.pdf", "keywords": ["multi-armed bandit", "semi-bandit", "adaptive regret bounds", "optimistic online mirror descent", "increasing learning rate"], "reference": "Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008.  12   MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  wt,i  (cid:8) (cid:104)w, mt(cid:105) + D\u03c8t(w, w(cid:48)  We invoke BROAD-OMD with at = 0, \u02c6(cid:96)t,i = (cid:96)t,i1{it=i}  Note that a\u2217 can be different from the empirically best arm i\u2217 defined in Section 2. The expected regret in this setting is still with respect to i\u2217 and further takes into consideration the randomness over losses. In other words, we care about E(cid:96)1,...,(cid:96)T [Ei1,...,iT [RegT ]], abbreviated as E[RegT ] still. being the typical importance-weighted unbiased estimator, and a somewhat special choice of mt: mt,i = (cid:96)t,it for all i. This choice of mt is seemingly invalid since it depends on it, which is drawn after we have constructed wt based on mt itself. However, note that because mt now has identical coordinates, we have wt = argminw\u2208\u2206K t, independent of the actual value of mt. Therefore, the algorithm is still valid and is in fact equivalent to the vanilla log-barrier OMD of (Foster et al., 2016). Also note that we cannot define \u02c6(cid:96)t as in previous sections (in terms of mt) since it is not an unbiased estimator of (cid:96)t anymore (due to the randomness of mt). Although the algorithm is the same, using our analysis framework we actually derive a tighter t,i(\u02c6(cid:96)t,i \u2212 (cid:96)t,it)2 = i=1((cid:96)t,i1{it = i} \u2212 wt,i(cid:96)t,it)2. It turns out that based on this quantity alone, one can derive both a \u201csmall-loss\u201d bound for the adversarial setting and a logarithmic bound for the stochastic setting as shown below. We emphasize that the doubling trick of Algorithm 3 is essential to make the algorithm parameter-free, which is another key difference from (Foster et al., 2016).  bound in terms of the following quantity based on Theorem 7: (cid:80)T (cid:80)T  t)(cid:9) = argminw\u2208\u2206K  (cid:8)D\u03c8t(w, w(cid:48)  t)(cid:9) = w(cid:48)  i=1 w2  (cid:80)K  (cid:80)K  t=1  t=1  Theorem 10 BROAD-OMD with at = 0, mt,i = (cid:96)t,it, \u02c6(cid:96)t,i = (cid:96)t,i1{it=i} (Algorithm 3), guarantees  wt,i  , and the doubling trick  E [RegT ] = O  \uf8ed  \uf8eb  (cid:118) (cid:117) (cid:117) (cid:116)(K ln T )E  (cid:34) T  (cid:88)  K (cid:88)  t=1  i=1  ((cid:96)t,i1{it = i} \u2212 wt,i(cid:96)t,it)2  + K ln T  \uf8f8 .  (9)  (cid:35)  \uf8f6  (cid:1), while in the ad- This bound implies that in the stochastic setting, we have E [RegT ] = O (cid:0) K ln T versarial setting, we have E [RegT ] = O (cid:0)(cid:112)KLT,i\u2217 ln T + K ln T (cid:1) assuming non-negative losses.  \u2206  5. Conclusions and Discussions  In this work we develop and analyze a general bandit algorithm using techniques such as optimistic mirror descent, log-barrier regularizer, increasing learning rate, and so on. We show various appli- cations of this general framework, obtaining several more adaptive algorithms that improve previous works. Future directions include 1) improving the dependence on K for the path-length results; 2) obtaining second-order path-length bounds; 3) generalizing the results to the linear bandit problem.  Acknowledgement. CYW is grateful for the support of NSF Grant #1755781. The authors would like to thank Chi-Jen Lu for posing the problem of bandit path-length, and to thank Chi-Jen Lu and Yi-Te Hong for helpful discussions in this direction.  References  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient In Conference on Learning Theory, pages 263-274,  algorithm for bandit linear optimization. 2008. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In Conference on Learning Theory, pages 12-38, 2017.  Chamy Allenberg, Peter Auer, Laszlo Gyorfi, and Gy\u00a8orgy Ottucs\u00b4ak. Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In Algorithmic Learning Theory, volume 4264, pages 229-243. Springer, 2006.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In Conference on Learning Theory, pages 116-120, 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In Conference on Learning Theory, pages 42-1, 2012.  S\u00b4ebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016.  S\u00b4ebastien Bubeck, Michael B. Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-  armed bandits. arXiv preprint arXiv:1711.01037, 2017.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory, 2012.  Chao-Kai Chiang, Chia-Jung Lee, and Chi-Jen Lu. Beating bandits in gradually evolving worlds.  In Conference on Learning Theory, pages 210-227, 2013.  Ashok Cutkosky and Kwabena Boahen. Online learning without prior information. In Conference  on Learning Theory, 2017.  Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms  for zero-sum games. Games and Economic Behavior, 92:327-348, 2015.  Steven De Rooij, Tim Van Erven, Peter D Gr\u00a8unwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research, 15(1):1281-1316, 2014.  R\u00b4emy Degenne and Vianney Perchet. Anytime optimal algorithms in stochastic multi-armed ban-  dits. In International Conference on Machine Learning, pages 1587-1595, 2016.  Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems, pages 4734-4742, 2016.  Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In  Conference on Learning Theory, pages 176-196, 2014.  Aur\u00b4elien Garivier and Olivier Capp\u00b4e. The kl-ucb algorithm for bounded stochastic bandits and  beyond. In Conference On Learning Theory, pages 359-376, 2011.  S\u00b4ebastien Gerchinovitz and Tor Lattimore. Refined lower bounds for adversarial bandits. In Ad-  vances in Neural Information Processing Systems, pages 1198-1206, 2016.  Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning  Research, 12(Apr):1287-1311, 2011a.  Elad Hazan and Satyen Kale. A simple multi-armed bandit algorithm with optimal variation- bounded regret. In Proceedings of the 24th Annual Conference on Learning Theory, pages 817- 820, 2011b.  Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Opti-  mization, 2(3-4):157-325, 2016.  Wouter M Koolen and Tim Van Erven. Second-order quantile methods for experts and combinatorial  games. In Conference on Learning Theory, pages 1155-1175, 2015.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Tor Lattimore. Optimally confident ucb: Improved regret for finite-armed bandits. arXiv preprint  arXiv:1507.07880, 2015.  Haipeng Luo and Robert E Schapire. Achieving all with no parameters: Adanormalhedge.  In  Conference on Learning Theory, pages 1286-1304, 2015.  Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Small-loss bounds for online learning with  partial information. arXiv preprint arXiv:1711.03639, 2017.  H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. Journal of  Machine Learning Research, 18(90):1-50, 2017.  Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Conference on Learning  Theory, pages 1360-1375, 2015.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in  Neural Information Processing Systems, pages 577-585, 2016.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-  ence on Learning Theory, pages 993-1019, 2013a.  Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se- quences. In Advances in Neural Information Processing Systems, pages 3066-3074, 2013b.  Yevgeny Seldin and G\u00b4abor Lugosi. An improved parametrization and analysis of the exp3++ algo-  rithm for stochastic and adversarial bandits. In Conference on Learning Theory, 2017. MORE ADAPTIVE ALGORITHMS FOR ADVERSARIAL BANDITS  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In International Conference on Machine Learning, pages 1287-1295, 2014.  Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient  algorithm. In International Conference on Machine Learning, pages 1593-1601, 2014.  Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of In Advances in Neural Information Processing Systems, pages  regularized learning in games. 2989-2997, 2015.  Tim van Erven and Wouter M Koolen. Metagrad: Multiple learning rates in online learning. In  Advances in Neural Information Processing Systems, pages 3666-3674, 2016.  "}, "Efficient Convex Optimization with Membership Oracles": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Efficient Convex Optimization with Membership Oracles", "abstract": "We consider the problem of minimizing a convex function over a convex set given access only to an evaluation oracle for the function and a membership oracle for the set. We give a simple algorithm which solves this problem with $\\tilde{O}(n^{2})$ oracle calls and $\\tilde{O}(n^{3})$ additional arithmetic operations. Using this result, we obtain more efficient reductions among the five basic oracles for convex sets and functions defined by Gr{\u00f6}tschel, Lov{\u00e1}sz, and Schrijver (1988).", "pdf_url": "http://proceedings.mlr.press/v75/lee18a/lee18a.pdf", "keywords": ["Optimization", "Membership", "Separation", "Violation", "Validity", "Subgradient"], "reference": "Jacob D. Abernethy and Elad Hazan. Faster convex optimization: Simulated annealing with an In Proceedings of the 33nd International Conference on Machine efficient universal barrier. Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2520-2528, 2016. URL http://jmlr.org/proceedings/papers/v48/abernethy16.html.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. Journal of  the ACM (JACM), 51(4):540-556, 2004.  Paul Christiano, Jonathan A Kelner, Aleksander Madry, Daniel A Spielman, and Shang-Hua Teng. Electrical \ufb02ows, laplacian systems, and faster approximation of maximum \ufb02ow in undirected graphs. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 273-282. ACM, 2011.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. Geometric algorithms and combinato-  rial optimization, volume 2. Algorithms and Combinatorics, 1988.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Math. Oper. Res., 31(2):  253-266, 2006.  2   OPTIMIZATION WITH ORACLES  and Abernethy and Hazan (2016) provides further improvements of up to a factor of structured convex sets.  Our main result in this paper is an algorithm that minimizes a convex function over a convex set using only \u02dcO(n2) membership and evaluation queries. Interestingly, we obtain this result by first showing that we can implement a separation oracle for a convex set and a subgradient ora- cle for a function using only \u02dcO(n) membership queries and then using the known reduction from optimization to separation. We state the result informally below.  \u221a  n for more  Theorem 1 Let K be a convex set specified by a membership oracle, a point x0 \u2208 Rn, and numbers 0 < r < R such that B(x0, r) \u2286 K \u2286 B(x0, R). For any convex function f given by an evaluation oracle and (cid:15) > 0, there is a randomized algorithm that computes a point z \u2208 B(K, (cid:15)) such that  f (z) \u2264 min x\u2208K  f (x) + (cid:15)  max x\u2208K  f (x) \u2212 min x\u2208K  f (x)  (cid:18)  (cid:19)  with constant probability using O(n2 logO(1)( nR oracle and O(n3 logO(1)( nR  (cid:15)r )) total arithmetic operations.  (cid:15)r )) calls to the membership oracle and evaluation  See the full version for the formal statement, proofs, comparison to prior work, and further  consequences of this result.  Acknowledgments  The authors thank Sebastien Bubeck, Ben Cousins, Sham M. Kakade and Ravi Kannan for help- ful discussions, and Yan Kit Chim for making the illustrations. Finally, we thank Xiaodi Wu for pointing out some typos in a previous version of the paper.  References  Jacob D. Abernethy and Elad Hazan. Faster convex optimization: Simulated annealing with an In Proceedings of the 33nd International Conference on Machine efficient universal barrier. Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2520-2528, 2016. URL http://jmlr.org/proceedings/papers/v48/abernethy16.html.  Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. Journal of  the ACM (JACM), 51(4):540-556, 2004.  Paul Christiano, Jonathan A Kelner, Aleksander Madry, Daniel A Spielman, and Shang-Hua Teng. Electrical \ufb02ows, laplacian systems, and faster approximation of maximum \ufb02ow in undirected graphs. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 273-282. ACM, 2011.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. Geometric algorithms and combinato-  rial optimization, volume 2. Algorithms and Combinatorics, 1988.  A. T. Kalai and S. Vempala. Simulated annealing for convex optimization. Math. Oper. Res., 31(2):  253-266, 2006. OPTIMIZATION WITH ORACLES  Jonathan A Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time algo- rithm for approximate max \ufb02ow in undirected graphs, and its multicommodity generalizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 217-226. SIAM, 2014.  Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in o(sqrt(rank)) iterations and faster algorithms for maximum \ufb02ow. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 424-433. IEEE, 2014.  Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster algorithms for linear programming. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 230-249. IEEE, 2015.  Yin Tat Lee, Satish Rao, and Nikhil Srivastava. A new approach to computing maximum \ufb02ows In Proceedings of the forty-fifth annual ACM symposium on Theory of  using electrical \ufb02ows. computing, pages 755-764. ACM, 2013.  Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its impli- cations for combinatorial and convex optimization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1049-1065. IEEE, 2015.  L. Lov\u00b4asz and S. Vempala. Fast algorithms for logconcave functions: sampling, rounding, integra-  tion and optimization. In FOCS, pages 57-68, 2006.  Aleksander Madry. Navigating central path with electrical \ufb02ows: From \ufb02ows to matchings, and In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on,  back. pages 253-262. IEEE, 2013.  Jonah Sherman. Nearly maximum \ufb02ows in nearly linear time. In Foundations of Computer Science  (FOCS), 2013 IEEE 54th Annual Symposium on, pages 263-269. IEEE, 2013.  Jonah Sherman. Area-convexity, l\u221e regularization, and undirected multicommodity \ufb02ow.  In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 452-460, 2017. doi: 10.1145/3055399. 3055501. URL http://doi.acm.org/10.1145/3055399.3055501.  P. M. Vaidya. A new algorithm for minimizing convex functions over convex sets. Math. Prog., 73:  291-341, 1996. "}, "A General Approach to Multi-Armed Bandits Under Risk Criteria": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "A General Approach to Multi-Armed Bandits Under Risk Criteria", "abstract": "Different risk-related criteria have received recent interest in learning problems, where typically each case is  treated in a customized manner. In this paper we provide a more systematic approach to analyzing  such risk criteria within a stochastic multi-armed bandit (MAB) formulation. We identify a set of general conditions that yield a simple characterization of the oracle rule (which serves as the regret benchmark), and facilitate the design of upper confidence bound (UCB) learning policies. The conditions are derived from problem primitives, primarily focusing on the relation between the arm reward distributions and the (risk criteria) performance metric. Among other things, the work highlights some (possibly non-intuitive) subtleties that differentiate various criteria in conjunction with statistical properties of the arms. Our main findings are illustrated on several widely used objectives such as conditional value-at-risk, mean-variance, Sharpe-ratio, and more.", "pdf_url": "http://proceedings.mlr.press/v75/cassel18a/cassel18a.pdf", "keywords": ["Multi-Armed Bandit", "risk", "planning", "reinforcement learning", "Upper Confidence Bound"], "reference": "Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit  problem. In COLT, pages 39-1, 2012.  Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.  Mathematical finance, 9(3):203-228, 1999.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322-331. IEEE, 1995.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Nicolas Galichet, Michele Sebag, and Olivier Teytaud. Exploration vs exploitation vs safety: Risk-  aware multi-armed bandits. In ACML, pages 245-260, 2013.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Odalric-Ambrym Maillard. Robust risk-averse stochastic multi-armed bandits. Conference on Algorithmic Learning Theory, pages 218-233. Springer, 2013.  In International  Odalric-Ambrym Maillard, R\u00b4emi Munos, and Gilles Stoltz. A finite-time analysis of multi-armed  bandits problems with kullback-leibler divergences. In COLT, pages 497-514, 2011.  11   RISK CRITERIA IN MULTI-ARMED BANDITS  6. Open Problems and Future Directions  One main question that we leave open is the dependence of the regret on problem parameters such as the number of arms K, and the sub-optimality gaps \u2206i. As our regret analysis passes through the proxy regret, the optimal order of the regret remains open as well. This will likely be resolved by means of a matching lower bound. Future directions may include a more complete taxonomy of performance criteria, or an extension of this framework to different settings (e.g., adversarial or contextual). Additionally, we note that the majority of our proof techniques also apply to non- quasiconvex criteria. If such criteria are found to be of interest then extending the framework to this case may be appealing.  We thank Ron Amit, Guy Tennenholtz, Nir Baram and Nadav Merlis for helpful discussions of this work, and the anonymous reviewers for their helpful comments. This work was partially funded by the Israel Science Foundation under contract 1380/16 and by the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL).  Acknowledgments  References  Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit  problem. In COLT, pages 39-1, 2012.  Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.  Mathematical finance, 9(3):203-228, 1999.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322-331. IEEE, 1995.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Nicolas Galichet, Michele Sebag, and Olivier Teytaud. Exploration vs exploitation vs safety: Risk-  aware multi-armed bandits. In ACML, pages 245-260, 2013.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Odalric-Ambrym Maillard. Robust risk-averse stochastic multi-armed bandits. Conference on Algorithmic Learning Theory, pages 218-233. Springer, 2013.  In International  Odalric-Ambrym Maillard, R\u00b4emi Munos, and Gilles Stoltz. A finite-time analysis of multi-armed  bandits problems with kullback-leibler divergences. In COLT, pages 497-514, 2011. RISK CRITERIA IN MULTI-ARMED BANDITS  Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of  Probability, 18(3):1269-1283, 1990.  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American  Mathematical Society, 58(5):527-535, 1952.  R Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of  risk, 2:21-42, 2000.  Amir Sani, Alessandro Lazaric, and R\u00b4emi Munos. Risk-aversion in multi-armed bandits.  In  Advances in Neural Information Processing Systems, pages 3275-3283, 2012.  Michel Simonnet. The Strong Law of Large Numbers, pages 311-325. Springer New York, New York, NY, 1996. ISBN 978-1-4612-4012-9. doi: 10.1007/978-1-4612-4012-9 15. URL http: //dx.doi.org/10.1007/978-1-4612-4012-9_15.  Sattar Vakili and Qing Zhao. Risk-averse multi-armed bandit problems under mean-variance  measure. IEEE Journal of Selected Topics in Signal Processing, 10(6):1093-1111, 2016.  Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.  Alexander Zimin, Rasmus Ibsen-Jensen, and Krishnendu Chatterjee. Generalized risk-aversion in  stochastic multi-armed bandits. arXiv preprint arXiv:1405.0833, 2014. "}, "An Optimal Learning Algorithm for Online Unconstrained Submodular Maximization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "An Optimal Learning Algorithm for Online Unconstrained Submodular Maximization", "abstract": "We consider a basic problem at the interface of two fundamental fields: {\\em submodular optimization} and {\\em online learning}.  In the {\\em online unconstrained submodular maximization (online USM) problem}, there is a universe $[n]=\\{1,2,\\ldots,n\\}$ and a sequence of $T$ nonnegative (not necessarily monotone) submodular functions arrive over time.  The goal is to design a computationally efficient online algorithm, which chooses a subset of $[n]$ at each time step as a function only of the past, such that the accumulated value of the chosen subsets is as close as possible to the maximum total value of a fixed subset in hindsight.  Our main result is a polynomial-time  no-$\\frac12$-regret algorithm for this problem, meaning that for every sequence of nonnegative submodular functions, the algorithm\u2019s expected total value is at least $\\frac12$ times that of the best subset in hindsight, up to an error term sublinear in $T$. The factor of $\\tfrac 12$ cannot be improved upon by any polynomial-time online algorithm when the submodular functions are presented as value oracles. Previous work on the offline problem implies that picking a subset uniformly at random in each time step achieves zero $\\frac14$-regret. A byproduct of our techniques is an explicit subroutine for the two-experts problem that has an unusually strong regret guarantee: the total value of its choices is comparable to twice the total value of either expert on rounds it did not pick that expert. This subroutine may be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v75/roughgarden18a/roughgarden18a.pdf", "keywords": ["Online learning", "submodular optimization"], "reference": "A. A. Ageev and M. I. Sviridenko. An 0.828 approximation algorithm for the uncapacitated facility  location problem. Discrete Applied Mathematics, 93:149156, 1999.  7. As presented, our algorithm needs to know the time horizon T . This dependence can be removed with a standard  trick: simply guess T = 1, and double T while restarting the algorithm everytime the current guess is violated.  11   AN OPTIMAL ALGORITHM FOR ONLINE USM  Even without this proof, we might have expected that a claim like Corollary 2 should be true. After all, over time, a good algorithm for the two experts problem learns to pick the better (on average) expert. This corresponds to making an of\ufb02ine greedy decision, which according to the Buchbinder et al. (2015b) analysis is good enough to get a 1/3-approximation. However, there are some subtleties that can occur. For example, the subroutine can sometimes make mistakes, possibly picking a negative value over a positive one sometimes. The original Buchbinder et al. (2015b) analysis did not need to account for the possibility of such events, but our proofs implicitly handle them.  3. An Optimal No- 1  2-Regret Algorithm for Online USM  We have now identified a clear goal. In this section, we successfully give a no-regret algorithm for the USM Balance Subproblem. Note that this is the optimal value of \u03b1 in terms of \u03b1-regret, since Theorem 1 also transforms inapproximability of Online USM (nothing better than 1/2) into inapproximability of the USM Balance Subproblem (nothing better than 1). Due to our unusual definition of \u03b1-regret for the USM Balance Subproblem, this was nonobvious. It is perhaps suprising that such a simple algorithm manages to obtain the optimal approximation ratio; the brunt of the work is in the analysis.  Algorithm 2: USM BALANCER \u221a  Initialize x \u2190 1 2 for round t = 1 to T do  T .  .  Compute probability pt \u2190 x\u221a T Choose the item with probability pt for round t, and receive the point (\u03b1t, \u03b2t), Write (\u03b1t, \u03b2t) as the convex combination cu(+1, +1) + cr(+1, \u22121) + c(cid:96)(\u22121, +1). Perform update x \u2190 x + (1 \u2212 2pt)cu + cr \u2212 c(cid:96). Cap x back into the interval [0,  T ].  \u221a  end  Our proposed subroutine is Algorithm 2.7 We defer the proof of its regret to "}, "The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity", "abstract": "The mean field approximation to the Ising model is a canonical variational tool that is used for analysis and inference in Ising models. We provide a simple and optimal bound for the KL error of the mean field approximation for Ising models on general graphs, and extend it to higher order Markov random fields. Our bound improves on previous bounds obtained in work in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi and recent works by Basak and Mukherjee, and Eldan. Our bound is tight up to lower order terms.  Building on the methods used to prove the bound, along with techniques from combinatorics and optimization,  we study the algorithmic problem of estimating the (variational) free energy for Ising models and general Markov random fields. For a graph $G$ on $n$ vertices and interaction matrix $J$ with Frobenius norm $\\|{J} \\|_F$, we provide algorithms that approximate the free energy within an additive error of $\\epsilon n \\|J\\|_F$ in time $\\exp(poly(1/\\epsilon))$. We also show that approximation within $(n \\|J\\|_F)^{1-\\delta}$ is NP-hard for every $\\delta > 0$. Finally, we provide more efficient approximation algorithms, which find the optimal mean field approximation, for ferromagnetic Ising models and for Ising models satisfying Dobrushin\u2019s condition.", "pdf_url": "http://proceedings.mlr.press/v75/jain18b/jain18b.pdf", "keywords": [], "reference": "(8):1117-1129, 2007.  Nir Ailon and Noga Alon. Hardness of fully dense problems. Information and Computation, 205  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSP problems. In STOC, 2002.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSPs. J. Comput. System Sci., 67:212-243, 2003.  James Anderson and Carsten Peterson. A mean field theory learning algorithm for neural networks.  Complex Systems, 1:995-1019, 1987.  Anirban Basak and Sumit Mukherjee. Universality of the mean-field for the potts model. Probability  Theory and Related Fields, 168(3-4):557-600, 2017.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs ii. multiway cuts and statistical physics. Annals of Mathematics, 176(1):151-219, 2012.  Christian Borgs, Jennifer T Chayes, Henry Cohn, and Yufei Zhao. An lp theory of sparse graph convergence ii: Ld convergence, quotients, and right convergence. Annals of Probability, 46, 2018.  Amir Dembo and Andrea Montanari. Ising models on locally tree-like graphs. Ann. Appl. Probab., 20(2):565-592, 2010. ISSN 1050-5164. doi: 10.1214/09-AAP627. URL http://dx.doi. org/10.1214/09-AAP627.  Roland Lvovich Dobrushin. The description of a random field by means of conditional probabilities  and conditions of its regularity. Theor. Prob. Appl., 13:197-224, 1968.  Ronen Eldan. Gaussian-width gradient complexity, reverse log-sobolev inequalities and nonlinear  large deviations. arXiv preprint arXiv:1612.04346, 2016.  Richard S. Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Richard S Ellis and Charles M Newman. The statistics of curie-weiss models. Journal of Statistical  Physics, 19(2):149-161, 1978.  19(2):175-220, 1999.  Alan Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica,  Shayan Oveis Gharan and Luca Trevisan. A new regularity lemma and faster approximation al- gorithms for low threshold rank graphs. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 303-316. Springer, 2013.  Eslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields. doi: 10.1017/  Comb. Probab. Comput., 16(1):43-61, January 2007. S096354830600767X. URL http://dx.doi.org/10.1017/S096354830600767X.  ISSN 0963-5483.  13   THE MEAN-FIELD APPROXIMATION: INFORMATION INEQUALITIES, ALGORITHMS, AND COMPLEXITY  References  (8):1117-1129, 2007.  Nir Ailon and Noga Alon. Hardness of fully dense problems. Information and Computation, 205  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSP problems. In STOC, 2002.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSPs. J. Comput. System Sci., 67:212-243, 2003.  James Anderson and Carsten Peterson. A mean field theory learning algorithm for neural networks.  Complex Systems, 1:995-1019, 1987.  Anirban Basak and Sumit Mukherjee. Universality of the mean-field for the potts model. Probability  Theory and Related Fields, 168(3-4):557-600, 2017.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs ii. multiway cuts and statistical physics. Annals of Mathematics, 176(1):151-219, 2012.  Christian Borgs, Jennifer T Chayes, Henry Cohn, and Yufei Zhao. An lp theory of sparse graph convergence ii: Ld convergence, quotients, and right convergence. Annals of Probability, 46, 2018.  Amir Dembo and Andrea Montanari. Ising models on locally tree-like graphs. Ann. Appl. Probab., 20(2):565-592, 2010. ISSN 1050-5164. doi: 10.1214/09-AAP627. URL http://dx.doi. org/10.1214/09-AAP627.  Roland Lvovich Dobrushin. The description of a random field by means of conditional probabilities  and conditions of its regularity. Theor. Prob. Appl., 13:197-224, 1968.  Ronen Eldan. Gaussian-width gradient complexity, reverse log-sobolev inequalities and nonlinear  large deviations. arXiv preprint arXiv:1612.04346, 2016.  Richard S. Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Richard S Ellis and Charles M Newman. The statistics of curie-weiss models. Journal of Statistical  Physics, 19(2):149-161, 1978.  19(2):175-220, 1999.  Alan Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica,  Shayan Oveis Gharan and Luca Trevisan. A new regularity lemma and faster approximation al- gorithms for low threshold rank graphs. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 303-316. Springer, 2013.  Eslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields. doi: 10.1017/  Comb. Probab. Comput., 16(1):43-61, January 2007. S096354830600767X. URL http://dx.doi.org/10.1017/S096354830600767X.  ISSN 0963-5483. THE MEAN-FIELD APPROXIMATION: INFORMATION INEQUALITIES, ALGORITHMS, AND COMPLEXITY  Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combinato-  rial optimization, volume 2. Springer Science & Business Media, 2012.  Piotr Indyk. Sublinear time algorithms for metric space problems. In Proceedings of the thirty-first  annual ACM symposium on Theory of computing, pages 428-434. ACM, 1999.  Sorin Istrail. Statistical mechanics, three-dimensionality and np-completeness: I. universality of intracatability for the partition function of the ising model across non-planar surfaces (extended abstract). In STOC, 2000.  Tommi S Jaakkola and Michael I Jordan. Improving the mean field approximation via the use of  mixture distributions. In Learning in graphical models, pages 163-173. Springer, 1998.  Vishesh Jain, Frederic Koehler, and Elchanan Mossel. The vertex sample complexity of free energy  is polynomial. In Conference on Learning Theory, 2018.  M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for ising model (extended  abstract). In Automata, Languages and Programming, pages 462-475, 1990.  Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction  to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.  Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The ising partition function: Zeros and  deterministic approximation. In FOCS, 2017.  J. M. Mooij and H. J. Kappen. Sufficient conditions for convergence of the sum-product algorithm. IEEE Transactions on Information Theory, 53(12):4422-4437, Dec 2007. ISSN 0018-9448. doi: 10.1109/TIT.2007.909166.  Elchanan Mossel and Allan Sly. Exact thresholds for ising-gibbs samplers on general graphs. The  Annals of Probability, 41(1):294-328, 2013.  Giorgio Parisi. Statistical field theory. New York: Addison-Wesley, 1988.  Andrej Risteski. How to calculate partition functions using convex programming hierarchies: prov-  able bounds for variational methods. In COLT, 2016.  Allan Sly and Nike Sun. The computational hardness of counting in two-spin models on d-regular graphs. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 361-369. IEEE, 2012.  S. Tatikonda and M. I. Jordan. Loopy belief propagation and gibbs measures. In Uncertainty in  Artificial Intelligence (UAI), Proceedings of the Eighteenth Conference. 2002.  Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational  inference. Foundations and Trends in Machine Learning, 1(1-2):1-305, 2008. THE MEAN-FIELD APPROXIMATION: INFORMATION INEQUALITIES, ALGORITHMS, AND COMPLEXITY  "}, "Approximation beats concentration? An approximation view  on inference with smooth radial kernels": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Approximation beats concentration? An approximation view  on inference with smooth radial kernels", "abstract": "Positive definite kernels and their associated Reproducing Kernel Hilbert Spaces provide a mathematically compelling and practically competitive framework for learning from data.  In this paper we take the approximation theory point of view to explore various aspects of  smooth kernels related to their inferential properties. We  analyze eigenvalue decay of  kernels operators and  matrices,  properties of eigenfunctions/eigenvectors and \u201cFourier\u201d coefficients of functions in the kernel space restricted to a discrete set of data points. We also investigate the fitting capacity of kernels,  giving explicit bounds on the fat shattering dimension of the balls in  Reproducing Kernel Hilbert spaces.  Interestingly, the same properties that make kernels  very effective approximators for functions in their \u201cnative\u201d kernel space,  also limit their capacity to represent arbitrary functions.  We discuss various implications, including those for gradient descent type methods. It is important to note that most of our  bounds are measure independent.  Moreover,  at least in moderate dimension, the bounds for eigenvalues are much tighter than the bounds which can be obtained from the usual matrix concentration results. For example, we see that  eigenvalues of kernel matrices show nearly exponential decay with constants depending only on the kernel and the domain. We call this \u201capproximation beats concentration\u201d phenomenon as even when the data are sampled from a probability distribution, some of their aspects are better understood in terms of approximation theory.", "pdf_url": "http://proceedings.mlr.press/v75/belkin18a/belkin18a.pdf", "keywords": [], "reference": "Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive di- mensions, uniform convergence, and learnability. Journal of the ACM (JACM), 44(4): 615(cid:21)631, 1997.  M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand  kernel learning. ArXiv e-prints, 2018.  Theodoros Evgeniou and Massimiliano Pontil. On the v-\u03b3 dimension for regression in repro- ducing kernel hilbert spaces. In Algorithmic Learning Theory, pages 106(cid:21)117. Springer, 1999.  Ying Guo, Peter L Bartlett, John Shawe-Taylor, and Robert C Williamson. Covering num- In Proceedings of the twelfth annual conference on  bers for support vector machines. Computational learning theory, pages 267(cid:21)277. ACM, 1999.  Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on large-scale shallow learning. arXiv preprint arXiv:1703.10622, accepted to NIPS 2017, 2017.  G. Raskutti, M. Wainwright, and B. Yu. Early stopping and non-parametric regression: an  optimal data-dependent stopping rule. JMLR, 15(1):335(cid:21)366, 2014.  Michael Reed and Barry Simon. Functional analysis, vol. i, 1980.  Christian Rieger and Barbara Zwicknagl. Sampling inequalities for in(cid:28)nitely smooth func- tions, with applications to interpolation and machine learning. Advances in Computational Mathematics, 32(1):103, 2010.  Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators.  Journal of Machine Learning Research, 11(Feb):905(cid:21)934, 2010.  Gabriele Santin and Robert Schaback. Approximation of eigenfunctions in kernel-based  spaces. Advances in Computational Mathematics, 42(4):973(cid:21)993, 2016.  Robert Schaback and Holger Wendland. Approximation by positive de(cid:28)nite kernels. Advanced Problems in Constructive Approximation, pages 203(cid:21)222. Springer, 2002.  In  Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT press, 2001.  John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge  university press, 2004.  Tao Shi, Mikhail Belkin, and Bin Yu. Data spectroscopy: eigenspace of convolution operators  and clustering. The Annals of Statistics, 37, 6B:3960(cid:21)3984, 2009.  Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science &  Business Media, 2008.  13   Approximation beats concentration  References  Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive di- mensions, uniform convergence, and learnability. Journal of the ACM (JACM), 44(4): 615(cid:21)631, 1997.  M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand  kernel learning. ArXiv e-prints, 2018.  Theodoros Evgeniou and Massimiliano Pontil. On the v-\u03b3 dimension for regression in repro- ducing kernel hilbert spaces. In Algorithmic Learning Theory, pages 106(cid:21)117. Springer, 1999.  Ying Guo, Peter L Bartlett, John Shawe-Taylor, and Robert C Williamson. Covering num- In Proceedings of the twelfth annual conference on  bers for support vector machines. Computational learning theory, pages 267(cid:21)277. ACM, 1999.  Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on large-scale shallow learning. arXiv preprint arXiv:1703.10622, accepted to NIPS 2017, 2017.  G. Raskutti, M. Wainwright, and B. Yu. Early stopping and non-parametric regression: an  optimal data-dependent stopping rule. JMLR, 15(1):335(cid:21)366, 2014.  Michael Reed and Barry Simon. Functional analysis, vol. i, 1980.  Christian Rieger and Barbara Zwicknagl. Sampling inequalities for in(cid:28)nitely smooth func- tions, with applications to interpolation and machine learning. Advances in Computational Mathematics, 32(1):103, 2010.  Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators.  Journal of Machine Learning Research, 11(Feb):905(cid:21)934, 2010.  Gabriele Santin and Robert Schaback. Approximation of eigenfunctions in kernel-based  spaces. Advances in Computational Mathematics, 42(4):973(cid:21)993, 2016.  Robert Schaback and Holger Wendland. Approximation by positive de(cid:28)nite kernels. Advanced Problems in Constructive Approximation, pages 203(cid:21)222. Springer, 2002.  In  Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT press, 2001.  John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge  university press, 2004.  Tao Shi, Mikhail Belkin, and Bin Yu. Data spectroscopy: eigenspace of convolution operators  and clustering. The Annals of Statistics, 37, 6B:3960(cid:21)3984, 2009.  Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science &  Business Media, 2008. Approximation beats concentration  Ingo Steinwart and Clint Scovel. Fast rates for support vector machines using gaussian  kernels. The Annals of Statistics, pages 575(cid:21)607, 2007.  Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and  Trends R(cid:13) in Machine Learning, 8(1-2):1(cid:21)230, 2015.  Holger Wendland. Scattered data approximation, 2004.  Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning.  Constructive Approx., 26(2):289(cid:21)315, 2007.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under- standing deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http://arxiv.org/abs/1611.03530.  "}, "Non-Convex Matrix Completion Against a Semi-Random Adversary": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Non-Convex Matrix Completion Against a Semi-Random Adversary", "abstract": "Matrix completion is a well-studied problem with many machine learning applications. In practice, the problem is often solved by non-convex optimization algorithms. However, the current theoretical analysis for non-convex algorithms relies crucially on the assumption that each entry of the matrix is observed with exactly the same probability $p$, which is not realistic in practice. In this paper, we investigate a more realistic semi-random model, where the probability of observing each entry is {\\em at least} $p$.  Even with this mild semi-random perturbation, we can construct counter-examples where existing non-convex algorithms get stuck in bad local optima. In light of the negative results, we propose a pre-processing step that tries to re-weight the semi-random input, so that it becomes \u201csimilar\u201d to a random input. We give a nearly-linear time algorithm for this problem, and show that after our pre-processing, all the local minima of the non-convex objective can be used to approximately recover the underlying ground-truth matrix.", "pdf_url": "http://proceedings.mlr.press/v75/cheng18b/cheng18b.pdf", "keywords": ["Matrix Completion", "Non-Convex Optimization", "Semi-Random Model"], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Rudolf Ahlswede and Andreas J. Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret mini- mization beyond matrix multiplicative updates. In Proc. 46th ACM Symp. on Theory of Comput- ing, pages 237-245, 2015.  Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th ACM-SIAM Symp. on Discrete Algorithms, pages 1824-1831, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient and neural algorithms  for sparse coding. In Proc. 28th Conference on Learning Theory, page 113149, 2015.  Joshua D. Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-Ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-  perspectives. 1828, 2013.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proc. 31st Intl. Conf. on  Machine Learning, pages 1881-1889, 2014.  13   NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  5. Conclusions  In this paper, we showed that even though non-convex approaches for matrix completion are not robust in the semi-random model, but it is possible to fix them using a pre-processing step. The pre-processing step solves a few convex programs (packing SDPs) to ameliorate the in\ufb02uence of the semi-random adversary. Unlike the full convex relaxation for matrix completion, our pre-processing step runs in nearly-linear time. Combining our pre-processing step with non-convex optimization gives an algorithm that is robust in the semi-random model, and at the same time enjoys the effi- ciency of the non-convex approaches.  An immediate open problem is whether we can prove the output of the pre-processing step allows non-convex optimization to recover the ground truth exactly. More broadly, we hope this work will inspire new ideas that make non-convex optimization more robust.  This work is supported by NSF CCF-1704656. We thank Qingqing Huang, Andrej Risteski, Srinadh Bhojanapalli, Yin Tat Lee for discussions at various stages of the work. Yu Cheng is also supported in part by NSF CCF-1527084, CCF-1535972, CCF-1637397, IIS-1447554, and NSF CAREER Award CCF-1750140.  Acknowledgments  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi- mate local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Rudolf Ahlswede and Andreas J. Winter. Strong converse for identification via quantum channels.  IEEE Transactions on Information Theory, 48(3):569-579, 2002.  Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret mini- mization beyond matrix multiplicative updates. In Proc. 46th ACM Symp. on Theory of Comput- ing, pages 237-245, 2015.  Zeyuan Allen Zhu, Yin Tat Lee, and Lorenzo Orecchia. Using optimization to obtain a width- independent, parallel, simpler, and faster positive SDP solver. In Proc. 27th ACM-SIAM Symp. on Discrete Algorithms, pages 1824-1831, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient and neural algorithms  for sparse coding. In Proc. 28th Conference on Learning Theory, page 113149, 2015.  Joshua D. Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-Ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-  perspectives. 1828, 2013.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proc. 31st Intl. Conf. on  Machine Learning, pages 1881-1889, 2014. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of  Algorithms, 19(2):204-234, 1995.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. Information Theory, IEEE Transactions on, 56(5):2053-2080, 2010.  Ji Chen and Xiaodong Li. Memory-efficient kernel PCA via partial matrix sampling and nonconvex optimization: a model-free analysis of local minima. arXiv preprint arXiv:1711.01742, 2017.  Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent:  General statistical and algorithmic guarantees. arXiv preprint, 1509.03025, 2015.  Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efficient sampling for In Proc. 28th Conference on Learning  Gaussian graphical models via spectral sparsification. Theory, pages 364-390, 2015.  Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving SDD linear systems in nearly m log1/2 n time. In Proc. 45th ACM Symp. on Theory of Computing, pages 343-352, 2014.  Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer and  System Sciences, 63(4):639-671, 2001.  Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semirandom  graph. Random Structures and Algorithms, 16(2):195-208, 2000.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic  gradient for tensor decomposition. arXiv preprint arXiv:1503.02101, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Proc. 28th Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proc. 34th Intl. Conf. on Machine Learning, pages 1233-1242, 2017.  Moritz Hardt. Understanding alternating minimization for matrix completion. In Proc. 55th IEEE  Symp. on Foundations of Computer Science. IEEE, 2014.  Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In Proc.  27th Conference on Learning Theory, pages 638-678, 2014.  Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh. Matrix completion and low-rank SVD  via fast alternating least squares. Journal of Machine Learning Research, 2014.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proc. 44th ACM Symp. on Theory of Computing, pages 665-674. ACM, 2013. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Rahul Jain and Penghui Yao. A parallel approximation algorithm for positive semidefinite pro- In Proc. 52nd IEEE Symp. on Foundations of Computer Science, pages 463-471,  Mark Jerrum. Large cliques elude the metropolis process. Random Struct. Algorithms, 3(4):347-  gramming. 2011.  360, 1992.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape  saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.  Jonathan A. Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combi- natorial algorithm for solving SDD systems in nearly-linear time. In Proc. 44th ACM Symp. on Theory of Computing, pages 911-920, 2013.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010a.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. The Journal of Machine Learning Research, 11:2057-2078, 2010b.  Yehuda Koren. The Bellkor solution to the Net\ufb02ix grand prize. Net\ufb02ix prize documentation, 81,  2009.  Ioannis Koutis, Gary L. Miller, and Richard Peng. A nearly-m log n time solver for SDD linear systems. In Proc. 52nd IEEE Symp. on Foundations of Computer Science, pages 590-598, 2011.  Ludek Kucera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics,  57(2-3):193-212, 1995.  Rasmus Kyng and Sushant Sachdeva. Approximate Gaussian elimination for Laplacians - fast, sparse, and simple. In Proc. 57th IEEE Symp. on Foundations of Computer Science, pages 573- 582, 2016.  Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time. In  Proc. 56th IEEE Symp. on Foundations of Computer Science, pages 250-269, 2015.  Yin Tat Lee and He Sun. An SDP-based algorithm for linear-sized spectral sparsification. In Proc.  48th ACM Symp. on Theory of Computing, pages 678-687, 2017.  Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap- proximation via alternating minimization. In Proc. 33rd Intl. Conf. on Machine Learning, pages 2358-2367, 2016.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation algo- rithms for semi-random partitioning problems. In Proc. 43rd ACM Symp. on Theory of Comput- ing, pages 367-384. ACM, 2012.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clustering with noisy partial information. In Proc. 28th Conference on Learning Theory, pages 1321-1342, 2015. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proc. 21st ACM-  SIAM Symp. on Discrete Algorithms, pages 712-728, 2010.  Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11(Aug):2287-2322, 2010.  Ankur Moitra, William Perry, and Alexander S Wein. How robust are reconstruction thresholds for community detection? In Proc. 47th ACM Symp. on Theory of Computing, pages 828-841, 2016.  Sahand Negahban and Martin J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May):1665- 1697, 2012.  Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. arXiv preprint arXiv:1609.03240, 2016.  Richard Peng and Daniel A. Spielman. An efficient parallel solver for SDD linear systems. In Proc.  45th ACM Symp. on Theory of Computing, pages 333-342, 2014.  Richard Peng, Kanat Tangwongsan, and Peng Zhang. Faster and simpler width-independent parallel algorithms for positive semidefinite programming. arXiv preprint arXiv:1201.5135v3, 2016.  Amelia Perry and Alexander S Wein. A semidefinite program for unbalanced multisection in the stochastic block model. In International Conference on Sampling Theory and Applications (SampTA), pages 64-67. IEEE, 2017.  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12:3413-3430, 2011.  Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative  prediction. In Proc. 22nd Intl. Conf. on Machine Learning, pages 713-719. ACM, 2005.  Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geo-  metric functional analysis. Journal of the ACM, 54(4):21, 2007.  Christopher De Sa, Christopher R\u00b4e, and Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. In Proc. 32nd Intl. Conf. on Machine Learning, pages 2332-2341, 2015.  J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117,  2015.  Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM  Journal on Computing, 40(6):1913-1926, 2011.  Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM Journal on  Computing, 40(4):981-1025, 2011. NON-CONVEX MATRIX COMPLETION AGAINST A SEMI-RANDOM ADVERSARY  Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis Applications, 35(3):835-885, 2014.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proc. 18th Conference on  Learning Theory, pages 545-560, 2005.  Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakkola. Maximum-margin matrix factor- ization. In Proc. 16th Advances in Neural Information Processing Systems, pages 1329-1336, 2004.  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Proc.  56th IEEE Symp. on Foundations of Computer Science, pages 270-289. IEEE, 2015.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In Proc. 27th Advances in Neural Information Processing Systems, pages 559-567, 2015.  Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016.  "}, "The Vertex Sample Complexity of Free Energy is Polynomial": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "The Vertex Sample Complexity of Free Energy is Polynomial", "abstract": "The free energy is a key quantity which is associated to Markov random fields. Classical results in statistical physics show how, given an analytic formula of the free energy, it is possible to compute many key quantities associated with Markov random fields including quantities such as magnetization and the location of various phase transitions.  Given a massive Markov random field on $n$ nodes, can a small sample from it provide a rough approximation to the free energy $\\mathcal{F}_n = \\log{Z_n}$?  Results in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi show that  for Ising models on $n$ nodes and interactions of strength $\\Theta(1/n)$, an $\\epsilon$ approximation to $\\log Z_n / n$ can be achieved by sampling a randomly induced model on $2^{O(1/\\epsilon^2)}$ nodes. We show that the sampling complexity of this problem is {\\em polynomial in }$1/\\epsilon$. We further show a polynomial dependence on $\\epsilon$ cannot be avoided.  Our results are very general as they apply to higher order Markov random fields. For Markov random fields of order $r$, we obtain an algorithm that achieves $\\epsilon$ approximation using a number of samples polynomial in $r$ and $1/\\epsilon$ and running time that is $2^{O(1/\\epsilon^2)}$ up to polynomial factors in $r$ and $\\epsilon$. For ferromagnetic Ising models, the running time is polynomial in $1/\\epsilon$.  Our results are intimately connected to recent research on the regularity lemma and property testing, where the interest is in finding which properties can tested within $\\epsilon$ error in time polynomial in $1/\\epsilon$. In particular, our proofs build on results of Alon, de la Vega, Kannan and Karpinski, who also introduced the notion of polynomial vertex sample complexity. Another critical ingredient of the proof is an effective bound by the authors of this paper relating the variational free energy and the free energy.", "pdf_url": "http://proceedings.mlr.press/v75/jain18c/jain18c.pdf", "keywords": [], "reference": "N. Alon. Ranking tournaments. Siam Journal on Discrete Mathematics, 20(1):137-142, 2006.  Noga Alon and Asaf Shapira. A characterization of the (natural) graph properties testable with  one-sided error. SIAM Journal on Computing, 37(6):1703-1727, 2008.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSP problems. In STOC, 2002.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSPs. J. Comput. System Sci., 67:212-243, 2003.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. Advances in Mathematics, 219(6):1801-1851, 2008.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs ii. multiway cuts and statistical physics. Annals of Mathematics, 176(1):151-219, 2012.  Richard S. Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Alan Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica,  Lior Gishboliner and Asaf Shapira. Removal lemmas with polynomial bounds. arXiv preprint  Lior Gishboliner and Asaf Shapira. Efficient removal without efficient regularity. arXiv preprint  19(2):175-220, 1999.  arXiv:1611.10315, 2016.  arXiv:1709.08159, 2017.  Oded Goldreich, Shari Goldwasser, and Dana Ron. Property testing and its connection to learning  and approximation. Journal of the ACM (JACM), 45(4):653-750, 1998.  Vishesh Jain, Frederic Koehler, and Elchanan Mossel. The mean-field approximation: Information  inequalities, algorithms, and complexity. In Conference on Learning Theory, 2018.  M. Jerrum and A. Sinclair. Approximating the permanent. SIAM J. Comput., 18(6):1149-1178,  1989.  M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for ising model (extended  abstract). In Automata, Languages and Programming, pages 462-475, 1990.  M. Jerrum, A. Sinclair, and E. Vigoda. A polynomial-time approximation algorithm for the per- manent of a matrix with non-negative entries. Journal of the ACM, 51(4):671-697, 2004., 51(4): 671-697, 2004.  L\u00e1szl\u00f3 Lov\u00e1sz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012.  Andrej Risteski. How to calculate partition functions using convex programming hierarchies: prov-  able bounds for variational methods. In COLT, 2016.  10   THE VERTEX SAMPLE COMPLEXITY OF FREE ENERGY IS POLYNOMIAL  References  N. Alon. Ranking tournaments. Siam Journal on Discrete Mathematics, 20(1):137-142, 2006.  Noga Alon and Asaf Shapira. A characterization of the (natural) graph properties testable with  one-sided error. SIAM Journal on Computing, 37(6):1703-1727, 2008.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSP problems. In STOC, 2002.  Noga Alon, Fernandez de la Vega, Ravi Kannan, and Marek Karpinski. Random sampling and  approximation of MAX-CSPs. J. Comput. System Sci., 67:212-243, 2003.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. Advances in Mathematics, 219(6):1801-1851, 2008.  Christian Borgs, Jennifer T Chayes, L\u00e1szl\u00f3 Lov\u00e1sz, Vera T S\u00f3s, and Katalin Vesztergombi. Conver- gent sequences of dense graphs ii. multiway cuts and statistical physics. Annals of Mathematics, 176(1):151-219, 2012.  Richard S. Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Alan Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combinatorica,  Lior Gishboliner and Asaf Shapira. Removal lemmas with polynomial bounds. arXiv preprint  Lior Gishboliner and Asaf Shapira. Efficient removal without efficient regularity. arXiv preprint  19(2):175-220, 1999.  arXiv:1611.10315, 2016.  arXiv:1709.08159, 2017.  Oded Goldreich, Shari Goldwasser, and Dana Ron. Property testing and its connection to learning  and approximation. Journal of the ACM (JACM), 45(4):653-750, 1998.  Vishesh Jain, Frederic Koehler, and Elchanan Mossel. The mean-field approximation: Information  inequalities, algorithms, and complexity. In Conference on Learning Theory, 2018.  M. Jerrum and A. Sinclair. Approximating the permanent. SIAM J. Comput., 18(6):1149-1178,  1989.  M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for ising model (extended  abstract). In Automata, Languages and Programming, pages 462-475, 1990.  M. Jerrum, A. Sinclair, and E. Vigoda. A polynomial-time approximation algorithm for the per- manent of a matrix with non-negative entries. Journal of the ACM, 51(4):671-697, 2004., 51(4): 671-697, 2004.  L\u00e1szl\u00f3 Lov\u00e1sz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012.  Andrej Risteski. How to calculate partition functions using convex programming hierarchies: prov-  able bounds for variational methods. In COLT, 2016. THE VERTEX SAMPLE COMPLEXITY OF FREE ENERGY IS POLYNOMIAL  Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 1970.  Alistair Sinclair and Mark Jerrum. Approximate counting, uniform generation and rapidly mixing  markov chains. Information and Computation, 82(1):93-133, 1989.  Mohit Singh and Nisheeth K. Vishnoi. Entropy, optimization and counting. In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 50-59, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591803. URL http://doi.acm.org/10.1145/2591796.2591803.  Maurice Sion. On general minimax theorems. Pacific Journal of mathematics, 8(1):171-176, 1958.  Allan Sly and Nike Sun. The computational hardness of counting in two-spin models on d-regular graphs. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 361-369. IEEE, 2012.  "}, "Efficient Algorithms for Outlier-Robust Regression": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Efficient Algorithms for Outlier-Robust Regression", "abstract": "We give the first polynomial-time algorithm for performing linear or polynomial regression resilient to adversarial corruptions in both examples and labels. Given a sufficiently large (polynomial-size) training set drawn i.i.d. from distribution ${\\mathcal{D}}$ and subsequently corrupted on some fraction of points, our algorithm outputs a linear function whose squared error is close to the squared error of the best-fitting linear function with respect to ${\\mathcal{D}}$, assuming that the marginal distribution of $\\mathcal{D}$ over the input space is \\emph{certifiably hypercontractive}. This natural property is satisfied by many well-studied distributions such as Gaussian, strongly log-concave distributions and, uniform distribution on the hypercube among others.  We also give a simple statistical lower bound showing that some distributional assumption is necessary to succeed in this setting.  These results are the first of their kind and were not known to be even information-theoretically possible prior to our work.   Our approach is based on the sum-of-squares (SoS) method and is inspired by the recent applications of the method for parameter recovery problems in unsupervised learning. Our algorithm can be seen as a natural convex relaxation of the following conceptually simple non-convex optimization problem: find a linear function and a large subset of the input corrupted sample such that the least squares loss of the function over the subset is minimized over all possible large subsets.", "pdf_url": "http://proceedings.mlr.press/v75/klivans18a/klivans18a.pdf", "keywords": ["sum-of-squares", "regression", "robust learning"], "reference": "Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In COLT, volume 49 of JMLR Workshop and Conference Proceedings, pages 417-445. JMLR.org, 2016.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decom- position via the sum-of-squares method [extended abstract]. In STOC\u201915\u2014Proceedings of the 2015 ACM Symposium on Theory of Computing, pages 143-151. ACM, New York, 2015.  sistent robust regression.  Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar. Con- In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2107-2116, 2017. URL http://papers.nips.cc/paper/ 6806-consistent-robust-regression.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In  Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 47-60, 2017. doi: 10.1145/3055399.3055491. URL http://doi.acm.org/10.1145/3055399.3055491.  Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In ICML, volume 28 of JMLR Workshop and Conference Proceedings, pages 774-782. JMLR.org, 2013. URL http://jmlr.org/proceedings/papers/v28/.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. CoRR, abs/1604.06443, 2016.  9   Outlier-Robust Regression  Definition 5 (Certifiable hypercontractivity) For a function C : [k] \u2192 (cid:146)+, we say a distri- bution D on (cid:146)d is k-certifiably C-hypercontractive if for every r (cid:54) k/2, there\u2019s a degree k sum of squares proof of the following inequality in variable v:  (cid:18)  (cid:104)x, v(cid:105)2r (cid:54)  (cid:133) D  C(r) (cid:133) D  (cid:104)x, v(cid:105)2(cid:19) r  .  Many natural distribution families satisfy certifiable hypercontractivity with reasonably growing functions C. For instance, Gaussian distributions, uniform distribution on Boolean hypercube satisfy the definitions with C(r) (cid:3) cr for a fixed constant c. More generally, all distributions that are a\ufb03ne transformations of isotropic distributions satisfying the Poincar\u00e9 inequality (Kothari and Steinhardt, 2017a), are also certifiably hypercontractive. In particular, this includes all strongly log-concave distributions. Certifiable hypercontractivity also satisfies natural closure properties under simple operations such as a\ufb03ne transformations, taking bounded weight mixtures and taking products. We refer the reader to Kothari and Steurer (2017) for a more detailed overview where certifiable hypercontractivity is referred to as certifiable subgaussianity.  References  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In COLT, volume 49 of JMLR Workshop and Conference Proceedings, pages 417-445. JMLR.org, 2016.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decom- position via the sum-of-squares method [extended abstract]. In STOC\u201915\u2014Proceedings of the 2015 ACM Symposium on Theory of Computing, pages 143-151. ACM, New York, 2015.  sistent robust regression.  Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar. Con- In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2107-2116, 2017. URL http://papers.nips.cc/paper/ 6806-consistent-robust-regression.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In  Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 47-60, 2017. doi: 10.1145/3055399.3055491. URL http://doi.acm.org/10.1145/3055399.3055491.  Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In ICML, volume 28 of JMLR Workshop and Conference Proceedings, pages 774-782. JMLR.org, 2013. URL http://jmlr.org/proceedings/papers/v28/.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. CoRR, abs/1604.06443, 2016. Outlier-Robust Regression  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. CoRR, abs/1707.01242, 2017. URL http://arxiv.org/abs/1707.01242.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. Preprint, 2018a. URL https://arxiv.org/abs/1803.02815.  Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. E\ufb03cient algorithms and lower bounds for robust linear regression. Preprint, 2018b. URL https://arxiv.org/abs/1806.00040.  Simon S. Du, Sivaraman Balakrishnan, and Aarti Singh. Computationally e\ufb03cient robust estimation of sparse functionals. CoRR, abs/1702.07709, 2017. URL http://arxiv.org/ abs/1702.07709.  Matthew A. Herman and Thomas Strohmer. General deviants: An analysis of perturbations  in compressed sensing. J. Sel. Topics Signal Processing, 4(2):342-349, 2010.  Sam B. Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. 2017.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically learning halfspaces. SIAM J. Comput., 37(6):1777-1805, 2008. doi: 10.1137/060649057. URL https://doi.org/10.1137/060649057.  Adam R. Klivans, Philip M. Long, and Rocco A. Servedio. Learning halfspaces with malicious noise. Journal of Machine Learning Research, 10:2715-2740, 2009. doi: 10.1145/ 1577069.1755877. URL http://doi.acm.org/10.1145/1577069.1755877.  Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms.  CoRR, abs/1711.07465, 2017a. URL http://arxiv.org/abs/1711.07465.  Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms.  2017b.  Pravesh K. Kothari and David Steurer. Outlier-robust moment-estimation via sum-of-squares.  CoRR, abs/1711.11581, 2017. URL http://arxiv.org/abs/1711.11581.  Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and  covariance. CoRR, abs/1604.06968, 2016.  Po-Ling Loh and Martin J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. CoRR, abs/1109.3714, 2011. URL http://arxiv.org/abs/1109.3714.  Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with  sum-of-squares. CoRR, abs/1610.01980, 2016. Outlier-Robust Regression  Aaron Potechin and David Steurer. Exact tensor completion with sum-of-squares.  In  Proceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Nether- lands, 7-10 July 2017, pages 1619-1673, 2017. URL http://proceedings.mlr.press/v65/ potechin17a.html.  A. Prasad, A. Sai Suggala, S. Balakrishnan, and P. Ravikumar. Robust Estimation via Robust  Gradient Estimation. ArXiv e-prints, 2018.  Peter J. Rousseeuw and Annick M. Leroy. Robust regression and outlier detection. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics. John Wiley & Sons, Inc., New York, 1987. ISBN 0-471-85233-3. URL https://doi.org/10.1002/ 0471725382.  Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. IEEE Trans. Information Theory, 56(7):3561-3574, 2010. doi: 10.1109/TIT.2010.2048503. URL https://doi.org/10.1109/TIT.2010.2048503. "}, "Action-Constrained Markov Decision Processes With Kullback-Leibler Cost": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Action-Constrained Markov Decision Processes With Kullback-Leibler Cost", "abstract": "This paper concerns computation of optimal policies in which the one-step reward function contains a cost term that models Kullback-Leibler divergence with respect to nominal dynamics. This technique was introduced by Todorov in 2007, where it was shown under general conditions that the solution to the average-reward optimality equations reduce to a simple eigenvector problem. Since then many authors have sought to apply this technique to control problems and models of bounded rationality in economics.  A crucial assumption is that the input process is essentially unconstrained. For example, if the nominal dynamics include randomness from nature (e.g., the impact of wind on a moving vehicle), then the optimal control solution does not respect the exogenous nature of this disturbance.  This paper introduces a technique to solve a more general class of action-constrained MDPs. The main idea is to solve an entire parameterized family of MDPs, in which the parameter is a scalar weighting the one-step  reward function. The approach is new and practical even in the original unconstrained formulation.", "pdf_url": "http://proceedings.mlr.press/v75/busic18a/busic18a.pdf", "keywords": ["Markov decision processes", "Computational methods"], "reference": "W. H. Al-Sabban, L. F. Gonzalez, and R. N. Smith. Wind-energy based path planning for unmanned aerial vehicles using Markov Decision Processes. In Proc. IEEE Conf. Robotics and Automation (ICRA), pages 784-789. IEEE, 2013.  12  01-0.50ImRe0.5\u03b6=0\u03b6=1\u03b6=2\u03bb(\u02c7P\u03b6) MDPS WITH K-L COST  in which l P XL is the position on the grid. The figure shows only the values n \u201c 2 and n \u201c 4 (the most interesting to view because of obvious spatial variability).  If the position l \u201c pla, loq is far from the boundary of XL,  say, minpla, loq \u011b 4 and minpda \u00b4 la, do \u00b4 loq \u011b 4, then  ErUt | Lt \u201c l Nt \u201c ns \u00ab 0 and vpl, nq \u00ab \u03c9pl, nq,  \u03b6 \u201c 0  For the case \u03b6 \u201c 1 the vector field is transformed so that vectors near the target state point in this direction; for \u03b6 \u201c 2 this behavior is more apparent. For states far from the target the control effort seems to be lower - most likely the optimal policy waits for more favorable weather that will push the UAV in the North-East direction.  The eigenvalues of \u02c7P\u03b6 are shown in Fig. 4 for \u03b6 \u201c 0, 1, 2. Most of the eigenvalues are driven near zero for \u03b6 \u201c 2. Those  Figure 4: Eigenvalues of \u02c7P\u03b6  three that are independent of \u03b6 are the three eigenvalues of Q0, t0.9095, 0.9655, 1u.  While the vector field and eigenvalues change significantly when \u03b6 is doubled from 1 to 2, the cost to go J \u02da defined in (36) grows relatively slowly with \u03b6. Shown on the right hand side of Fig. 2 are comparisons for these two values of \u03b6. One plot with n \u201c 2 and the other n \u201c 4. The plot on the far right shows J \u02da  \u03b6 pl, \u03b6q for 0 \u010f \u03b6 \u010f 1 and l \u201c p1, 1q (the location farthest from l\u201a).  These plots are easily obtained because of the nature of the algorithm: the optimal policy and  value function are generated for any range of \u03b6 of interest.  4. Conclusions  The ODE approach for solving MDPs has simple structure for the class of models considered in this paper. We are currently looking at approaches to approximate dynamic programming as has been successful in the unconstrained model Todorov (2009).  It is likely that the ODE has special structure for other classes of MDPs, such as the \u201crational inattention\u201d framework of Sims (2006); Shafieepoorfard et al. (2016). The computational efficiency of this approach will depend in part on numerical properties of the ODE, such as its sensitivity for complex models. Applications to distributed control were the original motivation for this work, with particular attention to \u201cdemand dispatch\u201d Chen et al. (2017). It is believed that this paper will offer new computational tools in this ongoing research.  Funding from the ANR under grant ANR-16-CE05-0008, and NSF under awards EPCN 1609131, CPS 1646229 is gratefully acknowledged.  Acknowledgments  References  W. H. Al-Sabban, L. F. Gonzalez, and R. N. Smith. Wind-energy based path planning for unmanned aerial vehicles using Markov Decision Processes. In Proc. IEEE Conf. Robotics and Automation (ICRA), pages 784-789. IEEE, 2013.01-0.50ImRe0.5\u03b6=0\u03b6=1\u03b6=2\u03bb(\u02c7P\u03b6) MDPS WITH K-L COST  D. P. Bertsekas and S. E. Shreve. Stochastic Optimal Control: The Discrete-Time Case. Athena  Scientific, 1996.  A. Bu\u02c7si\u00b4c and S. Meyn. Ordinary differential equation methods for Markov decision processes and application to Kullback-Leibler control cost. SIAM Journal on Control and Optimization, 56(1): 343-366, 2018.  Y. Chen, U. Hashmi, J. Mathias, A. Bu\u02c7si\u00b4c, and S. Meyn. Distributed control design for balancing the grid using \ufb02exible loads. In IMA volume on the control of energy markets and grids. Springer, 2017.  A. Dembo and O. Zeitouni. Large Deviations Techniques And Applications. Springer-Verlag, New  York, second edition, 1998.  K. Doya. How can we learn efficiently to act optimally and \ufb02exibly? Proceedings of the National  Academy of Sciences, 106(28):11429-11430, 2009.  G. F. Franklin, M. L. Workman, and D. Powell. Digital Control of Dynamic Systems. Addison-  Wesley Longman Publishing Co., Inc., Boston, MA, USA, 3rd edition, 1997.  P. Guan, M. Raginsky, and R. M. Willett. Online Markov decision processes with Kullback-Leibler  control cost. IEEE Trans. Automat. Control, 59(6):1423-1438, June 2014.  M. K\u00b4arn\u00b4y. Towards fully probabilistic control design. Automatica, 32(12):1719 -1722, 1996.  I. Kontoyiannis and S. P. Meyn. Spectral theory and limit theorems for geometrically ergodic  Markov processes. Ann. Appl. Probab., 13:304-362, 2003.  S. Meyn, P. Barooah, A. Bu\u02c7si\u00b4c, Y. Chen, and J. Ehren. Ancillary service to the grid using intelligent  deferrable loads. IEEE Trans. Automat. Control, 60(11):2847-2862, Nov 2015.  M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley  & Sons, 2014.  P. J. Schweitzer. Perturbation theory and finite Markov chains. J. Appl. Prob., 5:401-403, 1968.  E. Shafieepoorfard, M. Raginsky, and S. P. Meyn. Rationally inattentive control of Markov pro-  cesses. SIAM J. Control Optim., 54(2):987-1016, 2016.  C. A. Sims. Rational inattention: Beyond the linear-quadratic case. The American economic review,  pages 158-163, 2006.  R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057-1063, Cambridge, MA, USA, 1999. MIT Press.  E. Todorov. Linearly-solvable Markov decision problems. In B. Sch\u00a8olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1369-1376. MIT Press, Cambridge, MA, 2007. MDPS WITH K-L COST  E. Todorov. Efficient computation of optimal actions. Proceedings of the National Academy of  Sciences, 106(28):11478-11483, 2009. "}, "Fundamental Limits of Weak Recovery with Applications to Phase Retrieval": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Fundamental Limits of Weak Recovery with Applications to Phase Retrieval", "abstract": "In phase retrieval we want to recover an unknown signal $\\boldsymbol x\\in\\mathbb C^d$ from $n$ quadratic measurements of the form  $y_i = |\u27e8\\boldsymbol a_i,\\boldsymbol x\u27e9|^2+w_i$ where $\\boldsymbol a_i\\in \\mathbb C^d$ are known sensing vectors and  $w_i$ is measurement noise. We ask the following \\emph{weak recovery} question: what is the minimum number of measurements $n$ needed to produce an estimator $\\hat{\\boldsymbol x}(\\boldsymbol y)$ that is positively correlated with  the signal $\\boldsymbol x$? We consider the case of Gaussian  vectors $\\boldsymbol a_i$. We prove that \u2013 in the high-dimensional limit \u2013 a sharp phase transition takes place, and we locate the threshold in the regime of vanishingly small noise. For $n\\le d-o(d)$ no estimator can do significantly better than random and achieve a strictly positive correlation. For $n\\ge d+o(d)$ a simple spectral estimator achieves a positive correlation.  Surprisingly, numerical simulations with the same spectral estimator  demonstrate promising performance with realistic sensing matrices.  Spectral methods are used to initialize non-convex optimization algorithms in phase retrieval, and our approach can boost the performance in this setting as well. Our impossibility result is based on classical information-theory arguments. The spectral algorithm computes the leading eigenvector of a weighted empirical covariance matrix. We obtain a sharp characterization of the spectral properties of this random matrix  using tools from free probability and generalizing a recent result by Lu and Li.  Both the upper and lower bound generalize beyond phase retrieval to measurements $y_i$ produced according to a generalized linear model.  As a byproduct of our analysis, we compare the threshold of the proposed spectral method with that of a message passing algorithm.", "pdf_url": "http://proceedings.mlr.press/v75/mondelli18a/mondelli18a.pdf", "keywords": ["Spectral initialization", "phase transition", "mutual information", "second moment method", "phase retrieval", "free probability In this work1", "we consider the problem of recovering a signal x of dimension d", "given n generalized linear measurements More specifically", "the measurements are drawn independently according to the conditional distribution"], "reference": "Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on Learning Theory (COLT), pages 113-149, Paris, France, July 2015.  Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A \ufb02exible convex relaxation. In Proc. of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 252-260, Fort Lauderdale, FL, June 2017.  2. Here optimality is understood with respect to the weak recovery threshold.  4   FUNDAMENTAL LIMITS OF WEAK RECOVERY WITH APPLICATIONS TO PHASE RETRIEVAL  The lower bound is proved by estimating the conditional entropy via the second moment method. The spectral algorithm computes the eigenvector corresponding to the largest eigenvalue of a  matrix of the form:  Dn =  T (yi)aia\u2217 i ,  1 n  n (cid:88)  i=1  (9)  (10)  where T : R \u2192 R is a pre-processing function. For \u03b4 large enough (and a suitable choice of T ), we expect the resulting eigenvector \u02c6x(y) to be positively correlated with the true signal x. The recent paper (Lu and Li, 2017) computed exactly the threshold value \u03b4u, under the assumption that the measurement vectors are real Gaussian, and T is non-negative.  Here, we generalize the result of (Lu and Li, 2017) by removing the assumption that T (y) \u2265 0 and by considering the complex case. Armed with this result, we compute the optimal2 pre- processing function T \u2217 \u03b4 (y) for the general model (1). Our upper bound \u03b4u is the phase transition In the case of phase retrieval (as \u03c3 \u2192 0), this pre- location for this optimal spectral method. processing function is given by  T \u2217 \u03b4 (y) =  y \u2212 1 \u221a  ,  y +  \u03b4 \u2212 1  and achieves weak recovery for any \u03b4 > \u03b4u = 1. In the limit \u03b4 \u2193 1, this converges to the limiting function T \u2217(y) = 1 \u2212 (1/y).  While the expression (10) is remarkably simple, it is somewhat counter-intuitive. Earlier meth- ods (Cand\u00e8s et al., 2015c; Chen and Cand\u00e8s, 2015; Lu and Li, 2017) use T (y) \u2265 0 and try to extract information from the large values of yi. The function (10) has a large negative part for small y, in particular when \u03b4 is close to 1. Furthermore, it extracts useful information from data points with yi small. One possible interpretation is that the points in which the measurement vector is basically orthogonal to the unknown signal are not informative, hence we penalize them.  Our analysis applies to Gaussian measurement matrices. However, the proposed spectral method  works well also on real images and realistic measurement matrices.  We also compare our spectral approach to message passing algorithms. In particular, we prove that, for \u03b4 < \u03b4u (i.e. in the regime in which the spectral approach fails), message passing converges to an un-informative fixed point, even if initialized in a state that is correlated with the true signal x. Vice versa, for \u03b4 > \u03b4u (i.e. in the regime in which the spectral approach achieves weak recovery), we consider a linearized message passing algorithm, and prove that the un-informative fixed point is unstable.  References  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on Learning Theory (COLT), pages 113-149, Paris, France, July 2015.  Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A \ufb02exible convex relaxation. In Proc. of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 252-260, Fort Lauderdale, FL, June 2017.  2. Here optimality is understood with respect to the weak recovery threshold. FUNDAMENTAL LIMITS OF WEAK RECOVERY WITH APPLICATIONS TO PHASE RETRIEVAL  Radu Balan, Pete Casazza, and Dan Edidin. On signal reconstruction without phase. Applied and  Computational Harmonic Analysis, 20(3):345-356, 2006.  Afonso S. Bandeira, Jameson Cahill, Dustin G. Mixon, and Aaron A. Nelson. Saving phase: In- jectivity and stability for phase retrieval. Applied and Computational Harmonic Analysis, 37(1): 106-125, 2014.  T. Tony Cai, Xiaodong Li, and Zongming Ma. Optimal rates of convergence for noisy sparse phase  retrieval via thresholded Wirtinger \ufb02ow. The Annals of Statistics, 44(5):2221-2251, 2016.  Emmanuel J. Cand\u00e8s, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):1241-1274, 2013.  Emmanuel J. Cand\u00e8s, Yonina C. Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval  via matrix completion. SIAM Review, 57(2):225-251, 2015a.  Emmanuel J. Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval from coded diffrac-  tion patterns. Applied and Computational Harmonic Analysis, 39(2):277-299, 2015b.  Emmanuel J. Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow:  Theory and algorithms. IEEE Trans. Inform. Theory, 61(4):1985-2007, 2015c.  Yuxin Chen and Emmanuel J. Cand\u00e8s. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739-747, 2015.  Yuxin Chen and Emmanuel J. Cand\u00e8s. The projected power method: An efficient algorithm for  joint alignment from pairwise differences. arXiv:1609.05820, 2016.  Yuxin Chen and Emmanuel J. Cand\u00e8s. Solving random quadratic systems of equations is nearly as easy as solving linear systems. Communications on Pure and Applied Mathematics, 70:0822- 0883, 2017.  Aldo Conca, Dan Edidin, Milena Hering, and Cynthia Vinzant. An algebraic characterization of injectivity in phase retrieval. Applied and Computational Harmonic Analysis, 38(2):346-356, 2015.  John C. Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite optimiza-  tion for robust phase retrieval. arXiv:1705.02356, 2017.  J. R. Fienup. Phase retrieval algorithms: A comparison. Applied Optics, 21(15):2758-2769, Apr.  1982.  Ralph W. Gerchberg. A practical algorithm for the determination of the phase from image and  diffraction plane pictures. Optik, 35:237-246, 1972.  Tom Goldstein and Christoph Studer.  Phasemax: Convex phase retrieval via basis pursuit.  arXiv:1610.07531, 2016. FUNDAMENTAL LIMITS OF WEAK RECOVERY WITH APPLICATIONS TO PHASE RETRIEVAL  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proc. of the 45th Ann. ACM Symp. on Theory of Computing (STOC), pages 665-674, Palo Alto, CA, June 2013. ACM.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Trans. Inform. Theory, 56(6):2980-2998, 2010.  Kiryung Lee, Yanjun Li, Marius Junge, and Yoram Bresler. Blind recovery of sparse signals from  subsampled convolution. IEEE Trans. Inform. Theory, 63(2):802-821, 2017.  Gen Li, Yuantao Gu, and Yue M. Lu. Phase retrieval using iterative projections: Dynamics in the large systems limit. In Proc. of the 53rd Annual Allerton Conf. on Commun., Control, and Computing (Allerton), pages 1114-1118, Monticello, IL, Oct. 2015.  Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. Rapid, robust, and reliable blind  deconvolution via nonconvex optimization. arXiv:1606.04933, 2016.  Yue M. Lu and Gen Li. Phase transitions of spectral initialization for high-dimensional nonconvex  estimation. arXiv:1702.06435, 2017.  Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-  tion. In Advances in Neural Information Processing Systems, pages 2796-2804, 2013.  Philip Schniter and Sundeep Rangan. Compressive phase retrieval via generalized approximate  message passing. IEEE Transactions on Signal Processing, 63(4):1043-1055, 2015.  Mahdi Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample  complexity barriers via nonconvex optimization. arXiv:1702.06175, 2017.  Ir\u00e8ne Waldspurger, Alexandre d\u2019Aspremont, and St\u00e9phane Mallat. Phase recovery, maxcut and  complex semidefinite programming. Mathematical Programming, 149(1-2):47-81, 2015.  Gang Wang and Georgios B. Giannakis. Solving random systems of quadratic equations via trun- cated generalized gradient \ufb02ow. In Advances in Neural Information Processing Systems, pages 568-576, 2016.  Gang Wang, Georgios B. Giannakis, and Yonina C. Eldar. Solving systems of random quadratic  equations via truncated amplitude \ufb02ow. arXiv:1605.08285, 2016.  Gang Wang, Georgios B. Giannakis, Yousef Saad, and Jie Chen. Solving almost all systems of  random quadratic equations. arXiv:1705.10407, 2017.  Ke Wei. Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study.  Inverse Problems, 31(12), 2015.  Huishuai Zhang and Yingbin Liang. Reshaped Wirtinger Flow for solving quadratic system of equations. In Advances in Neural Information Processing Systems, pages 2622-2630, 2016. "}, "Cutting plane methods can be extended into nonconvex optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Cutting plane methods can be extended into nonconvex optimization", "abstract": "We show that it is possible to obtain an $O(\\epsilon^{-4/3})$ runtime \u2014 including computational cost \u2014 for finding $\\epsilon$-stationary points of nonconvex functions using cutting plane methods. This improves on the best known epsilon dependence achieved by cubic regularized Newton of $O(\\epsilon^{-3/2})$ as proved by Nesterov and Polyak (2006). Our techniques utilize the convex until proven guilty principle proposed by Carmon, Duchi, Hinder, and Sidford (2017).", "pdf_url": "http://proceedings.mlr.press/v75/hinder18a/hinder18a.pdf", "keywords": ["optimization", "cutting plane methods", "nonconvex", "stationary point", "local minima"], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima for nonconvex optimization in linear time. Symposium on Theory of Computing, 2017.  Ernesto G Birgin, JL Gardenghi, Jos\u00b4e Mario Mart\u00b4\u0131nez, Sandra Augusta Santos, and Ph L Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order reg- ularized models. Mathematical Programming, 163(1-2):359-368, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford.  \u2018Convex until proven guilty\u2019: dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of 34th International Conference on Machine Learning, pages 654-663, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017b.  2   HINDER  Our result can be contrasted with the results of Birgin, Gardenghi, Mart\u00b4\u0131nez, Santos, and Toint (2017) who gives a runtime of O((T3+?)(cid:15)\u2212(p+1)/p) where the ? denotes the cost of finding a sta- tionary point of a pth order regularized problem. Letting p = 1 gives gradient descent and p = 2 cubic regularized Newton. However, for p > 2 all known methods for solving pth order have (cid:15)- dependencies that cause the computational runtime to scale at best with O((cid:15)\u22123/2) corresponding to cubic regularized Newton. Therefore our major contribution is to show it is possible to obtain an O((cid:15)\u22124/3) runtime \u2014 including computational cost \u2014 for finding (cid:15)-stationary points of nonconvex functions. See Table 1 for a comparison of our results with existing results.  Lipschitz method  \u2207f  gradient descent  \u2207f, \u22072f Carmon et al. (2017a) \u2207f, \u22073f Carmon et al. (2017a)  \u22072f  cubic reg. Nesterov and Polyak (2006)  Birgin et al.  \u2207pf  pth reg. (2017). \u2207f, \u22073f This paper. Thm 1. \u2207f, \u22073f This paper. Thm 2.  runtime  dimension-free lower bound (Car- mon et al., 2017b,c)  T1(cid:15)\u22122 T1(cid:15)\u22122 T1(cid:15)\u22127/4 T1(cid:15)\u221212/7 T1(cid:15)\u22125/3 T1(cid:15)\u22128/5 (T2 + d\u03c9)(cid:15)\u22123/2 T2(cid:15)\u22123/2  (Tp+?)(cid:15)\u2212 p+1  p  Tp(cid:15)\u2212(p+1)/p  ((T1 + d\u03c9)d + T2)(cid:15)\u22124/3  (T3 + d4)(cid:15)\u22124/3 T3(cid:15)\u22124/3  Table 1: Comparison of the runtime of different algorithms for finding stationary points of non- convex functions. The question mark is a placeholder for the time to solve a pth order regularization problem.  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding ap- proximate local minima for nonconvex optimization in linear time. Symposium on Theory of Computing, 2017.  Ernesto G Birgin, JL Gardenghi, Jos\u00b4e Mario Mart\u00b4\u0131nez, Sandra Augusta Santos, and Ph L Toint. Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order reg- ularized models. Mathematical Programming, 163(1-2):359-368, 2017.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-  convex optimization. arXiv preprint arXiv:1611.00756, 2016.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford.  \u2018Convex until proven guilty\u2019: dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of 34th International Conference on Machine Learning, pages 654-663, 2017a.  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points I. arXiv preprint arXiv:1710.11606, 2017b. CUTTING PLANE METHODS CAN BE EXTENDED INTO NONCONVEX OPTIMIZATION1  Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary  points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017c.  Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle  points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017.  Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global perfor-  mance. Mathematical Programming, 108(1):177-205, 2006.  Cl\u00b4ement W Royer and Stephen J Wright. Complexity analysis of second-order line-search algo-  rithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017. "}, "An Analysis of the t-SNE Algorithm for Data Visualization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "An Analysis of the t-SNE Algorithm for Data Visualization", "abstract": "A first line of attack in exploratory data analysis is \\emph{data visualization}, i.e., generating a 2-dimensional representation of data that makes \\emph{clusters} of similar points visually identifiable. Standard Johnson-Lindenstrauss dimensionality reduction does not produce data visualizations. The \\emph{t-SNE} heuristic of van der Maaten and Hinton, which is based on non-convex optimization, has become the \\emph{de facto} standard for visualization in a wide range of applications. This work gives a formal framework for the problem of data visualization \u2013 finding a 2-dimensional embedding of clusterable data that correctly separates individual clusters to make them visually identifiable. We then give a rigorous analysis of the performance of t-SNE under a natural, deterministic condition on the \u201cground-truth\u201d clusters (similar to conditions assumed in earlier analyses of clustering) in the underlying data. These are the first provable guarantees on t-SNE for constructing good data visualizations.  We show that our deterministic condition is satisfied by considerably general probabilistic generative models for clusterable data such as mixtures of well-separated log-concave distributions. Finally, we give theoretical evidence that t-SNE provably succeeds in \\emph{partially} recovering cluster structure even when the above deterministic condition is not met.", "pdf_url": "http://proceedings.mlr.press/v75/arora18a/arora18a.pdf", "keywords": ["Clustering", "t-SNE", "Visualization"], "reference": "Walid M. Abdelmoula, Benjamin Balluff, Sonja Englert, Jouke Dijkstra, Marcel J. T. Reinders, Axel Walch, Liam A. McDonnell, and Boudewijn P. F. Lelieveldt. Data-driven identification of prognostic tumor subpopulations using spatially mapped t-sne of mass spectrometry imaging data. Proceedings of the National Academy of Sciences, 113(43):12244-12249, 2016.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In 18th  Annual Conference on Learning Theory, pages 458-469, 2005.  6   ARORA HU KOTHARI  We show that the t-SNE heuristic can visualize clusters under assumptions similar to the more sophisticated methods in previous theoretical work.  Finally, we show that even when the conditions in Definition 1.4 are not met, t-SNE can still provably visualize at least one cluster in the original data in some cases. As an example, using a more fine-grained analysis, we show that t-SNE computes a partial visualization for data obtained from a mixture of two concentric (thus, no mean separation at all!) spherical Gaussians with vari- ances differing by a constant factor.  Theorem 1.7 (Informal) Let X be generated from an equal-weighted mixture of two Gaussians N (0, \u03c32 2) such that 1.5 \u2264 \u03c32/\u03c31 \u2264 10. Then t-SNE with early exaggeration on input X outputs a (1 \u2212 d\u2212\u2126(1))-partial visualization of X where C1 is (1 \u2212 d\u2212\u2126(1))-visible.  1) and N (0, \u03c32  1.2. Related Work  This paper continues the line of work focused on analyzing gradient descent and related heuristics for non-convex optimization problems, examples of which we have discussed before. Theoretically analyzing t-SNE, in particular, was recently considered in a work of Linderman and Steinerberger (2017) who showed that running t-SNE with early exaggeration causes points from the same cluster to move towards each other (i.e., embedding of any cluster shrinks). As discussed before, however, this does not imply that t-SNE ends up with a visualization as all the clusters could potentially collapse into each other. Another work by Shaham and Steinerberger (2017) derived a theoretical property of SNE, but their result is only nontrivial when the number of clusters is significantly larger than the number of points per cluster, which is an unrealistic assumption.  Mixture models are natural average-case generative models for clusterable data which have been studied as benchmarks for analyzing various clustering algorithms and have a long history of theo- retical work. By now, a sequence of results (Dasgupta et al., 2007, 2006; Arora and Kannan, 2005; Vempala and Wang, 2004; Achlioptas and McSherry, 2005; Kannan et al., 2005; Vempala, 2007; Hsu and Kakade, 2013; Ge et al., 2015b; Kalai et al., 2012; Belkin and Sinha, 2010; Kalai et al., 2010; Kothari and Steinhardt, 2017; Hopkins and Li, 2017; Diakonikolas et al., 2017) have identified efficient algorithms for clustering data from such models under various natural assumptions.  This research was done with support from NSF, ONR, Simons Foundation, Mozilla Research, and Schmidt Foundation.  Acknowledgments  References  Walid M. Abdelmoula, Benjamin Balluff, Sonja Englert, Jouke Dijkstra, Marcel J. T. Reinders, Axel Walch, Liam A. McDonnell, and Boudewijn P. F. Lelieveldt. Data-driven identification of prognostic tumor subpopulations using spatially mapped t-sne of mass spectrometry imaging data. Proceedings of the National Academy of Sciences, 113(43):12244-12249, 2016.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In 18th  Annual Conference on Learning Theory, pages 458-469, 2005. AN ANALYSIS OF THE T-SNE ALGORITHM FOR DATA VISUALIZATION  Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians. The  Annals of Applied Probability, 15(1A):69-92, 2005.  Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models\u2014going beyond SVD. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science\u2014FOCS 2012, pages 1-10. IEEE Computer Soc., Los Alamitos, CA, 2012.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom- plete dictionaries. In COLT, volume 35 of JMLR Workshop and Conference Proceedings, pages 779-806. JMLR.org, 2014.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, pages  103-112. IEEE Computer Society, 2010.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Global optimality of local search  for low rank matrix recovery. In NIPS, pages 3873-3881, 2016.  Anirban Dasgupta, John E. Hopcroft, Ravi Kannan, and Pradipta Prometheus Mitra. Spectral clus- tering by recursive partitioning. In ESA, volume 4168 of Lecture Notes in Computer Science, pages 256-267. Springer, 2006.  Anirban Dasgupta, John E. Hopcroft, Ravi Kannan, and Pradipta Prometheus Mitra. Spectral clus-  tering with limited independence. In SODA, pages 1036-1045. SIAM, 2007.  Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS, pages 634-644, 1999.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. List-decodable robust mean estimation  and learning mixtures of spherical gaussians. CoRR, abs/1711.07211, 2017.  Ilir Gashi, Vladimir Stankovic, Corrado Leita, and Olivier Thonnard. An experimental study of diversity with off-the-shelf antivirus engines. In Proceedings of The Eighth IEEE International Symposium on Networking Computing and Applications, NCA 2009, July 9-11, 2009, Cambridge, Massachusetts, USA, pages 4-11, 2009.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic In COLT, volume 40 of JMLR Workshop and Conference  gradient for tensor decomposition. Proceedings, pages 797-842. JMLR.org, 2015a.  Rong Ge, Qingqing Huang, and Sham M. Kakade. Learning mixtures of gaussians in high dimen-  sions. In STOC, pages 761-770. ACM, 2015b.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. CoRR,  abs/1605.07272, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A  unified geometric analysis. In ICML, pages 1233-1242, 2017.  Philippe Hamel and Douglas Eck. Learning features from music audio with deep belief networks.  In ISMIR, pages 339-344. International Society for Music Information Retrieval, 2010. ARORA HU KOTHARI  Geoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor embedding. In NIPS, pages 833-840,  Samuel B. Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. CoRR,  2002.  abs/1711.07454, 2017.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In ITCS\u201913\u2014Proceedings of the 2013 ACM Conference on Innovations in Theoretical Computer Science, pages 11-19. ACM, New York, 2013.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two  gaussians. In STOC, pages 553-562. ACM, 2010.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Commun.  ACM, 55(2):113-120, 2012.  Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mix-  ture models. In COLT, pages 444-457, 2005.  Jon M. Kleinberg. An impossibility theorem for clustering. In NIPS, pages 446-453, 2002.  Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms. CoRR,  abs/1711.07465, 2017.  arXiv:1706.02582, 2017.  George C Linderman and Stefan Steinerberger. Clustering with t-sne, provably. arXiv preprint  Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Non-square In AISTATS,  matrix sensing without spurious local minima via the burer-monteiro approach. pages 65-74, 2017.  Uri Shaham and Stefan Steinerberger. Stochastic neighbor embedding separates well-separated  clusters. arXiv preprint arXiv:1702.02670, 2017.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: overview and  the geometric picture. IEEE Trans. Information Theory, 63(2):853-884, 2017.  Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine  learning research, 9(Nov):2579-2605, 2008.  Santosh Vempala. Spectral algorithms for learning and clustering. In COLT, volume 4539 of Lecture  Notes in Computer Science, pages 3-4. Springer, 2007.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of  Computer and System Sciences, 68(4):841-860, 2004.  Izhar Wallach and Ryan Lilien. The protein-small-molecule database (psmdb), a non-redundant structural resource for the analysis of protein-ligand binding. Bioinformatics, 25(5):615-20, 2009. "}, "Adaptivity to Smoothness in X-armed bandits": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Adaptivity to Smoothness in X-armed bandits", "abstract": "We study the stochastic continuum-armed bandit problem from the angle of adaptivity to \\emph{unknown regularity} of the reward function $f$. We prove that there exists no strategy for the cumulative regret that adapts optimally to the \\emph{smoothness} of $f$. We show however that such minimax optimal adaptive strategies exist if the learner is given \\emph{extra-information} about $f$. Finally, we complement our positive results with matching lower bounds.", "pdf_url": "http://proceedings.mlr.press/v75/locatelli18a/locatelli18a.pdf", "keywords": ["bandits with infinitely many arms", "minimax rates", "adaptivity", "smoothness"], "reference": "Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In Conference on Learning Theory, pages 12-38, 2017.  R Agrawal. The continuum-armed bandit problem. SIAM Journal on Control and Optimization, 33:  1926-1951, 1995.  Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of statistics, 35(2):608-633, 2007.  Peter Auer, Ronald Ortner, and Csaba Szepesv\u00e1ri. Improved rates for the stochastic continuum-armed bandit problem. In International Conference on Computational Learning Theory, pages 454-468. Springer, 2007.  Pierre C Bellec. Adaptive confidence sets in shape restricted regression.  arXiv preprint  arXiv:1601.05766, 2016.  Lucien Birg\u00e9 and Pascal Massart. From model selection to adaptive estimation. In Festschrift for  lucien le cam, pages 55-87. Springer, 1997.  S. Bubeck, G. Stoltz, and J. Yu. Lipschitz bandits without the lipschitz constant. In Algorithmic  Learning Theory, pages 144-158. Springer, 2011a.  S\u00e9bastien Bubeck, R\u00e9mi Munos, and Gilles Stoltz. Pure exploration in finitely-armed and continuous-  armed bandits. Theoretical Computer Science, 412(19):1832-1852, 2011b.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011c.  S\u00e9bastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-armed  bandits. In Conference on Learning Theory, pages 122-134, 2013.  Adam D Bull et al. Adaptive-treed bandits. Bernoulli, 21(4):2289-2307, 2015.  T Tony Cai, Mark G Low, et al. Adaptive confidence balls. The Annals of Statistics, 34(1):202-228,  2006.  T Tony Cai, Mark G Low, Yin Xia, et al. Adaptive confidence intervals for regression functions  under shape constraints. The Annals of Statistics, 41(2):722-750, 2013.  13   ADAPTIVITY IN X -ARMED BANDITS  Acknowledgments  The authors would like to thank Samory Kpotufe for many useful discussions. This work is partly funded by the Deutsche Foschungsgemeinschaft (DFG, German Research Foundation) on the Emmy Noether grant for the project MuSyAD (CA 1488/1-1) \u2018anomaly detection in complex systems\u2019, on the GK 2297 MathCoRe on \u201cMathematical Complexity Reduction\" - 314838170, GRK 2297 MathCoRe, and on the SFB 1294 Data Assimilation on \u201cData Assimilation \u2014 The seamless integration of data and models\", Project A03.  References  Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In Conference on Learning Theory, pages 12-38, 2017.  R Agrawal. The continuum-armed bandit problem. SIAM Journal on Control and Optimization, 33:  1926-1951, 1995.  Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of statistics, 35(2):608-633, 2007.  Peter Auer, Ronald Ortner, and Csaba Szepesv\u00e1ri. Improved rates for the stochastic continuum-armed bandit problem. In International Conference on Computational Learning Theory, pages 454-468. Springer, 2007.  Pierre C Bellec. Adaptive confidence sets in shape restricted regression.  arXiv preprint  arXiv:1601.05766, 2016.  Lucien Birg\u00e9 and Pascal Massart. From model selection to adaptive estimation. In Festschrift for  lucien le cam, pages 55-87. Springer, 1997.  S. Bubeck, G. Stoltz, and J. Yu. Lipschitz bandits without the lipschitz constant. In Algorithmic  Learning Theory, pages 144-158. Springer, 2011a.  S\u00e9bastien Bubeck, R\u00e9mi Munos, and Gilles Stoltz. Pure exploration in finitely-armed and continuous-  armed bandits. Theoretical Computer Science, 412(19):1832-1852, 2011b.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011c.  S\u00e9bastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-armed  bandits. In Conference on Learning Theory, pages 122-134, 2013.  Adam D Bull et al. Adaptive-treed bandits. Bernoulli, 21(4):2289-2307, 2015.  T Tony Cai, Mark G Low, et al. Adaptive confidence balls. The Annals of Statistics, 34(1):202-228,  2006.  T Tony Cai, Mark G Low, Yin Xia, et al. Adaptive confidence intervals for regression functions  under shape constraints. The Annals of Statistics, 41(2):722-750, 2013. ADAPTIVITY IN X -ARMED BANDITS  Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal  algorithms. In International Conference on Machine Learning, pages 521-529, 2014.  Eric Cope. Regret and convergence bounds for immediate-reward reinforcement learning with continuous action spaces. IEEE Transactions on Automatic Control, 54(6):1243-1253, 2009.  Aur\u00e9lien Garivier, Pierre M\u00e9nard, and Gilles Stoltz. Explore first, exploit next: The true shape of  regret in bandit problems. arXiv preprint arXiv:1602.07182, 2016.  Georgii Ksenofontovich Golubev. Adaptive asymptotically minimax estimators of smooth signals.  Problemy Peredachi Informatsii, 23(1):57-67, 1987.  Jean-Bastien Grill, Michal Valko, and R\u00e9mi Munos. Black-box optimization of noisy functions with unknown smoothness. In Advances in Neural Information Processing Systems, pages 667-675, 2015.  S. Hanneke. Adaptive rates of convergence in active learning. Proceedings of the 22nd Annual  Conference on Learning Theory, COLT 2009, pages 353-364.  Marc Hoffmann and Richard Nickl. On adaptive inference and confidence bands. The Annals of  Statistics, pages 2383-2409, 2011.  Anatoli Juditsky and Sophie Lambert-Lacroix. Nonparametric confidence set estimation. 2003.  Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identification in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1-42, 2016.  Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Proceedings of the 17th International Conference on Neural Information Processing Systems, pages 697-704. MIT Press, 2004.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681-690. ACM, 2008.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. arXiv  preprint arXiv:1312.1277, 2013.  V. Koltchinskii. Rademacher complexities and bounding the excess risk of active learning. Journal  of Machine Learning Research, 11:2457-2485, 2010.  Oleg V Lepski and VG Spokoiny. Optimal pointwise adaptive methods in nonparametric estimation.  The Annals of Statistics, pages 2512-2546, 1997.  Andrea Locatelli, Alexandra Carpentier, and Samory Kpotufe. Adaptivity to noise parameters In Satyen Kale and Ohad Shamir, editors, Proceedings of in nonparametric active learning. the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1383-1416, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR. URL http: //proceedings.mlr.press/v65/locatelli-andrea17a.html. ADAPTIVITY IN X -ARMED BANDITS  Stanislav Minsker. Plug-in approach to active learning. Journal of Machine Learning Research, 13  (Jan):67-90, 2012.  Stanislav Minsker. Estimation of extreme values and associated level sets of a regression function  via selective sampling. In Conference on Learning Theory, pages 105-121, 2013.  R\u00e9mi Munos. Optimistic Optimization of Deterministic Functions without the Knowledge of its  Smoothness. In Advances in Neural Information Processing Systems, 2011.  Aleksandrs Slivkins. Multi-armed bandits on implicit metric spaces.  In Advances in Neural  Information Processing Systems, pages 1602-1610, 2011.  Alexandre B Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statistics,  pages 135-166, 2004.  Alexandre B Tsybakov. Introduction to nonparametric estimation. revised and extended from the  2004 french original. translated by vladimir zaiats, 2009.  Michal Valko, Alexandra Carpentier, and R\u00e9mi Munos. Stochastic simultaneous optimistic optimiza-  tion. In International Conference on Machine Learning, pages 19-27, 2013.  Jia Yuan Yu and Shie Mannor. Unimodal bandits. In Proceedings of the 28th International Conference  on International Conference on Machine Learning, pages 41-48. Omnipress, 2011.  "}, "Black-Box Reductions for Parameter-free Online Learning in Banach Spaces": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Black-Box Reductions for Parameter-free Online Learning in Banach Spaces", "abstract": "We introduce several new black-box reductions that significantly improve the design of adaptive and parameter-free online learning algorithms by simplifying analysis, improving regret guarantees, and sometimes even improving runtime. We reduce parameter-free online learning to online exp-concave optimization, we reduce optimization in a Banach space to one-dimensional optimization, and we reduce optimization over a constrained domain to unconstrained optimization. All of our reductions run as fast as online gradient descent. We use our new techniques to improve upon the previously best regret bounds for parameter-free learning, and do so for arbitrary norms.", "pdf_url": "http://proceedings.mlr.press/v75/cutkosky18a/cutkosky18a.pdf", "keywords": [], "reference": "2006.  [1] S\u00b4ebastien Bubeck, Nikhil R Devanur, Zhiyi Huang, and Rad Niazadeh. Online auctions and multi-scale  online learning. arXiv preprint arXiv:1705.09700, 2017.  [2] Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university press,  [3] Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order Perceptron algorithm. SIAM  Journal on Computing, 34(3):640-668, 2005.  [4] Kamalika Chaudhuri, Yoav Freund, and Daniel J Hsu. A parameter-free hedging algorithm. In Advances  in neural information processing systems, pages 297-305, 2009.  [5] Ashok Cutkosky and Kwabena Boahen. Online learning without prior information.  In Satyen Kale and Ohad Shamir, editors, Proc. of the 2017 Conference on Learning Theory, volume 65 of Proc. of Machine Learning Research, pages 643-677, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR.  [6] Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and  losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  [7] Ashok Cutkosky and Kwabena A Boahen. Stochastic and adversarial online learning without hyperpa-  rameters. In Advances in Neural Information Processing Systems, pages 5066-5074, 2017.  [8] Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances in  Neural Information Processing Systems 28, pages 3375-3383. 2015.  [9] Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems, pages 6022-6032, 2017.  [10] Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Sufficient statistics and the  burkholder method. arXiv preprint arXiv:1803.07617, 2018.  [11] Petr H\u00b4ajek, Vicente Montesinos Santaluc\u00b4\u0131a, Jon Vanderwerff, and V\u00b4aclav Zizler. Biorthogonal systems  in Banach spaces. Springer Science & Business Media, 2007.  [12] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex opti-  mization. Machine Learning, 69(2-3):169-192, 2007.  [13] Elad Hazan, Alexander Rakhlin, and Peter L Bartlett. Adaptive online gradient descent. In Advances in  Neural Information Processing Systems, pages 65-72, 2008.  [14] Tomer Koren and Roi Livni. Affine-invariant online optimization and the low-rank experts problem. In Advances in Neural Information Processing Systems 30, pages 4750-4758. Curran Associates, Inc., 2017.  [15] Wojciech Kot\u0142owski. Scale-invariant unconstrained online learning. In Proc. of ALT, 2017.  [16] Roberto Lucchetti. Convexity and well-posed problems. Springer Science & Business Media, 2006.  [17] Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, and John Langford. Efficient second order online  learning by sketching. In Advances in Neural Information Processing Systems, pages 902-910, 2016.  [18] Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex opti-  mization. In Advances in neural information processing systems, pages 2402-2410, 2012.  13   BLACK-BOX REDUCTIONS FOR PARAMETER-FREE ONLINE LEARNING IN BANACH SPACES  References  2006.  [1] S\u00b4ebastien Bubeck, Nikhil R Devanur, Zhiyi Huang, and Rad Niazadeh. Online auctions and multi-scale  online learning. arXiv preprint arXiv:1705.09700, 2017.  [2] Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university press,  [3] Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order Perceptron algorithm. SIAM  Journal on Computing, 34(3):640-668, 2005.  [4] Kamalika Chaudhuri, Yoav Freund, and Daniel J Hsu. A parameter-free hedging algorithm. In Advances  in neural information processing systems, pages 297-305, 2009.  [5] Ashok Cutkosky and Kwabena Boahen. Online learning without prior information.  In Satyen Kale and Ohad Shamir, editors, Proc. of the 2017 Conference on Learning Theory, volume 65 of Proc. of Machine Learning Research, pages 643-677, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR.  [6] Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and  losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  [7] Ashok Cutkosky and Kwabena A Boahen. Stochastic and adversarial online learning without hyperpa-  rameters. In Advances in Neural Information Processing Systems, pages 5066-5074, 2017.  [8] Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances in  Neural Information Processing Systems 28, pages 3375-3383. 2015.  [9] Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems, pages 6022-6032, 2017.  [10] Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Sufficient statistics and the  burkholder method. arXiv preprint arXiv:1803.07617, 2018.  [11] Petr H\u00b4ajek, Vicente Montesinos Santaluc\u00b4\u0131a, Jon Vanderwerff, and V\u00b4aclav Zizler. Biorthogonal systems  in Banach spaces. Springer Science & Business Media, 2007.  [12] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex opti-  mization. Machine Learning, 69(2-3):169-192, 2007.  [13] Elad Hazan, Alexander Rakhlin, and Peter L Bartlett. Adaptive online gradient descent. In Advances in  Neural Information Processing Systems, pages 65-72, 2008.  [14] Tomer Koren and Roi Livni. Affine-invariant online optimization and the low-rank experts problem. In Advances in Neural Information Processing Systems 30, pages 4750-4758. Curran Associates, Inc., 2017.  [15] Wojciech Kot\u0142owski. Scale-invariant unconstrained online learning. In Proc. of ALT, 2017.  [16] Roberto Lucchetti. Convexity and well-posed problems. Springer Science & Business Media, 2006.  [17] Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, and John Langford. Efficient second order online  learning by sketching. In Advances in Neural Information Processing Systems, pages 902-910, 2016.  [18] Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex opti-  mization. In Advances in neural information processing systems, pages 2402-2410, 2012. BLACK-BOX REDUCTIONS FOR PARAMETER-FREE ONLINE LEARNING IN BANACH SPACES  [19] H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. Journal of  Machine Learning Research (JMLR), 18(90):1-50, 2017.  [20] H Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations. In Conference on Learning Theory (COLT), pages 1020-1039, 2014.  [21] Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information Pro-  cessing Systems, pages 1806-1814, 2013.  [22] Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochastic  learning. In Advances in Neural Information Processing Systems, pages 1116-1124, 2014.  [23] Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in  Neural Information Processing Systems 29, pages 577-585, 2016.  [24] Francesco Orabona and Tatiana Tommasi. Training deep networks without learning rates through coin  betting. In Advances in Neural Information Processing Systems, pages 2157-2167, 2017.  [25] Iosif Pinelis. Rosenthal-type inequalities for martingales in 2-smooth banach spaces. Theory Probab.  Appl., 59(4):699-706, 2015.  University, 2007.  [26] Shai Shalev-Shwartz. Online learning: Theory, algorithms, and applications. PhD thesis, Hebrew  [27] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In Ad-  vances in Neural Information Processing Systems 23, pages 2199-2207, 2010.  [28] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In  Advances in neural information processing systems, pages 2645-2653, 2011.  [29] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint  arXiv:1002.4862, 2010.  [30] Tim van Erven and Wouter M Koolen. MetaGrad: Multiple learning rates in online learning. In Ad-  vances in Neural Information Processing Systems 29, pages 3666-3674. 2016.  [31] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc.  of the 20th International Conference on Machine Learning (ICML-03), pages 928-936, 2003. BLACK-BOX REDUCTIONS FOR PARAMETER-FREE ONLINE LEARNING IN BANACH SPACES  "}, "A Data Prism: Semi-verified learning in the small-alpha regime": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "A Data Prism: Semi-verified learning in the small-alpha regime", "abstract": "We consider a simple model of unreliable or crowdsourced data where there is an underlying set of $n$ binary variables, each \u201cevaluator\u201d contributes a (possibly unreliable or adversarial) estimate of the values of some subset of $r$ of the variables, and the learner is given the true value of a \\emph{constant} number of variables.   We show that, provided an $\\alpha$-fraction of the evaluators are \u201cgood\u201d (either correct, or with independent noise rate $p < 1/2$), then the true values of a $(1-\\eps)$ fraction of the $n$ underlying variables can be deduced as long as $r > \\log_{2-2p}(1/\\alpha)$. For example, if the fraction of \u201cgood\u201d evaluators is larger than $1/16$ and there is no noise in their responses, then accurate recovery is possible provided each worker evaluates a random set of $4$ items.  This result is optimal in that if $r \\leq \\log_{2-2p}(1/\\alpha)$ the large dataset can contain no information.   This setting can be viewed as an instance of the  \\emph{semi-verified} learning model introduced by Charikar, Steinhardt, and Valiant, which explores the tradeoff between the number of items evaluated by each worker and the fraction of \u201cgood\u201d evaluators. In the standard adversarial setting, our algorithm requires $\\tilde{O}\\left(n^{\\log_{2-2p}(1/\\alpha)}\\right)$ evaluators. However, the algorithm runs in near linear time, $\\tilde{O}_{r,\\eps}(n)$, and hence would require only a near-linear number of evaluations in the weaker model in which the adversary\u2019s responses to each $r$-tuple of items are independent of the set of evaluations collected.  These settings and results can also be viewed as examining a general class of semi-adversarial CSPs with a planted assignment. This extreme parameter regime, where the fraction of reliable data is small (inverse exponential in the amount of data provided by each source), is relevant to a number of practical settings.  For example, the setting where you collect a dataset on customer preferences, with each customer specifying preferences for a small (constant) number of items, and the goal is to ascertain the preferences of a specific demographic of interest.   Our results show that this large dataset (which lacks demographic information) can be leveraged together with the preferences of the demographic of interest for a \\emph{constant} (polynomial in $1/\\alpha$ but independent of $n$), number of randomly selected items, to recover an accurate estimate of the entire set of preferences, even if the  fraction of the original dataset contributed by the demographic of interest is inverse exponential in the number of preferences supplied by each customer.   In this sense, our results can be viewed as a \u201cdata prism\u201d allowing one to extract the behavior of specific cohorts from a large, mixed, dataset.", "pdf_url": "http://proceedings.mlr.press/v75/meister18a/meister18a.pdf", "keywords": ["Semi-Verified Learning", "Robust Statistics", "Cohort Analysis", "Crowdsourcing"], "reference": "925-936, 2010.  Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):  M. Charikar, J. Steinhardt, and G. Valiant. Learning from untrusted data. In Symposium on Theory of  Computing (to appear), 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655-664. IEEE, 2016.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise.  arXiv preprint arXiv:1707.01242, 2017a.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and  learning mixtures of spherical gaussians. arXiv preprint arXiv:1711.07211, 2017b.  12   A DATA PRISM  To this end, our algorithm only ever considers \u201csingle-hop\u201d implications of proposed assignments: an assignment to a set of r variables is considered \u201coptimistic\u201d if it directly implies values for a significant fraction of the other variables. It is easy to imagine extending this definition to also consider longer chains of implication. Perhaps a specific assignment to r variables would imply values to c1 additional variables, which in turn would imply values to c2 variables, etc. Indeed, in the basic setting of r =2, this approach can be realized to yield an algorithm that only requires constraints on a random subset of size O(n3/2), as opposed to O(n2) constraints.  From a computational perspective, it seems unlikely that such an approach could be pushed to yield an efficient algorithm for the regime in which fewer than nr/2 sets of r variables have nontrivial constraints. Indeed, even for random instances of r\u2212SAT with a planted solution, efficient algorithms below this threshold have been elusive (see, for example, the recent related work on random CSPs with planted assignments (Feldman et al., 2015; Raghavendra et al., 2016)).  From a purely information theoretic perspective\u2014the picture is not entirely clear either. In contrast to random CSPs with planted assignments, for which constraints are placed on random r-tuples and the constraints are chosen randomly subject to respecting the planted assignment, our setting is complicated by the adversarial nature of the constraints that are placed on the r-tuples. In a fully adversarial CSP model, for which both the choices of the r-tuples as well as the constraints themselves are chosen adversarially\u2014to the best of our knowledge\u2014very little is known. Of course, in this setting, the goal is to find a satisfying assignment (that might not necessarily correspond to the planted assignment). In the semi-adversarial CSP model, where the identities of the r-tuple sets are picked at random, and the adversary chooses the constraints, our results show that we can recover an assignment, provided we can selectively query O(n) (cid:1) possible constraints. In these settings it is not immediately clear how to analyze the extent to of the (cid:0)n r which implications \u201cpropagate\u201d. A second difficulty is that the goal of our setting is not just to find a satisfying assignment, but to find something close to a specific planted assignment. Our results imply, for the settings we consider, that there are at most a constant number of solution clusters. It seems interesting to investigate the extent to which this holds for semi-adversarial CSPs with fewer constraints, perhaps with constraints chosen adversarially corresponding to only N (cid:28)(cid:0)n (cid:1) random r-tuples; in this setting it seems r plausible that N =nr/2 is the threshold between a constant and super-constant number of such solution clusters, though this might be difficult to prove.  References  925-936, 2010.  Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):  M. Charikar, J. Steinhardt, and G. Valiant. Learning from untrusted data. In Symposium on Theory of  Computing (to appear), 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655-664. IEEE, 2016.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise.  arXiv preprint arXiv:1707.01242, 2017a.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and  learning mixtures of spherical gaussians. arXiv preprint arXiv:1711.07211, 2017b. A DATA PRISM  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability problems with planted solutions. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 77-86. ACM, 2015.  Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics: the  approach based on influence functions, volume 114. John Wiley & Sons, 2011.  David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-  chervonenkis dimension. Journal of Combinatorial Theory, Series A, 69(2):217-232, 1995.  Peter J Huber. Robust statistics. Springer, 2011.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries.  IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.  Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms. arXiv  preprint arXiv:1711.07465, 2017.  preprint arXiv:1711.11581, 2017.  Pravesh K Kothari and David Steurer. Outlier-robust moment-estimation via sum-of-squares. arXiv  Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Founda- tions of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 665-674. IEEE, 2016.  Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random csps below the spectral  threshold. arXiv preprint arXiv:1605.00058, 2016.  Jacob Steinhardt, Gregory Valiant, and Moses Charikar. Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction. In Advances in Neural Information Processing Systems, pages 4439-4447, 2016.  Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the presence  of arbitrary outliers. In Innovations in Theoretical Computer Science (ITCS), 2018.  John W Tukey. A survey of sampling from contaminated distributions. Contributions to probability and  statistics, 2:448-485, 1960.  "}, "A Direct Sum Result for the Information Complexity of Learning": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "A Direct Sum Result for the Information Complexity of Learning", "abstract": "How many bits of information are required to PAC learn a class of hypotheses of VC dimension $d$? The mathematical setting we follow is that of Bassily et al., where the value of interest is the mutual information $\\mathrm{I}(S;A(S))$ between the input sample $S$ and the hypothesis outputted by the learning algorithm $A$. We introduce a class of functions of VC dimension $d$ over the domain $\\mathcal{X}$ with information complexity at least $\\Omega \\left(d\\log \\log \\frac{|\\mathcal{X}|}{d}\\right)$ bits for any consistent and proper algorithm (deterministic or random). Bassily et al. proved a similar (but quantitatively weaker) result for the case $d=1$. The above result is in fact a special case of a more general phenomenon we explore. We define the notion of {\\em information complexity} of a given class of functions $\\cH$. Intuitively, it is the minimum amount of information that an algorithm for $\\mathcal{X}$ must retain about its input to ensure consistency and properness. We prove a direct sum result for information complexity in this context; roughly speaking, the information complexity sums when combining several classes.", "pdf_url": "http://proceedings.mlr.press/v75/nachum18a/nachum18a.pdf", "keywords": ["PAC Learning", "Information Theory", "VC Dimension", "Direct Sum"], "reference": "Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 464-473. IEEE, 2014.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use little information. In Proceedings of the 29th international conference on algorithmic learning theory. To appear, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Mark Braverman. Interactive information complexity. In In Proceedings of the 44th annual ACM  Symposium on Theory of Computing, STOC12, 2012.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2ed  edition, 2006.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265-284. Springer, 2006.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 117-126. ACM, 2015.  11   A DIRECT SUM RESULT FOR THE INFORMATION COMPLEXITY OF LEARNING  sample, h(x) = y. For any x that did not appear in the sample, h(x) is sampled uniformly from {0, 1}. This algorithm has mutual information that does not grow with the size of the domain (it is O(m)). This is not too meaningful, as this algorithm is not a PAC learner. But it illustrates that the lower bound breaks somewhere, and it would be worthwhile to identify exactly how far the assumptions can be pushed before it breaks.  A different and interesting direction is to prove upper bounds on information complexity. First, we would like to understand whether the lower bound presented here is sharp. Better yet: Can we provide explicit general constructions for learning algorithms that obtain the information complex- ity, i.e., retain the minimal amount of information possible? Following the theorem of Bassily et al. (2018) stating that compression entails learning, this would yield a novel class of learning algo- rithms for all hypothesis classes in which the information complexity is o(m) - a strong result that might even have practical applications.  Lastly, and perhaps most interestingly, one may also consider the converse of that theorem: Is there a sense in which low information complexity is a necessary condition for learnability? Are the concepts equivalent?  References  Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 464-473. IEEE, 2014.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use little information. In Proceedings of the 29th international conference on algorithmic learning theory. To appear, 2018.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam\u2019s razor.  Information processing letters, 24(6):377-380, 1987.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Mark Braverman. Interactive information complexity. In In Proceedings of the 44th annual ACM  Symposium on Theory of Computing, STOC12, 2012.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2ed  edition, 2006.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265-284. Springer, 2006.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 117-126. ACM, 2015. A DIRECT SUM RESULT FOR THE INFORMATION COMPLEXITY OF LEARNING  Peter D Gr\u00a8unwald. The minimum description length principle. MIT press, 2007.  Marcus Hutter. Universal algorithmic intelligence: A mathematical top down approach. In Artificial  General Intelligence, pages 227-290. Springer, 2007.  Mauricio Karchmer, Ran Raz, and Avi Wigderson. Super-logarithmic depth lower bounds via the direct sum in communication complexity. Computational Complexity, 5(3-4):191-204, 1995.  Eyal Kushilevitz and Noam Nisan. Communication Complexity. Cambridge University Press, 1997.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical  report, Technical report, University of California, Santa Cruz, 1986.  Li Ming and Paul Vit\u00b4anyi. An introduction to Kolmogorov complexity and its applications. Springer  Heidelberg, 1997.  ACM (JACM), 63(3):21, 2016.  Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. Journal of the  Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465-471, 1978.  Ryan Rogers, Aaron Roth, Adam Smith, and Om Thakkar. Max-information, differential privacy, and post-selection hypothesis testing. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 487-494. IEEE, 2016.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Maurice Sion. On general minimax theorems. Pacific journal of mathematics, 8(1):171-176, 1958.  Ray J Solomonoff. A formal theory of inductive inference, part I. Information and control, 7(1):  1-22, 1964.  1928.  Vladimir N Vapnik and Alexey Ya Chervonenkis. On the uniform convergence of relative frequen-  cies of events to their probabilities. Measures of Complexity, 16(2):11, 1971.  John Von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320,  John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. 1944.  Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn- ing algorithms. In Advances in Neural Information Processing Systems, pages 2521-2530, 2017.  "}, "Online learning over a finite action set with limited switching": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Online learning over a finite action set with limited switching", "abstract": "We study the value of switching actions in the Prediction From Experts (PFE) problem and Adversarial Multi-Armed Bandits (MAB) problem. First, we revisit the well-studied and practically motivated setting of PFE with switching costs. Many algorithms are known to achieve the minimax optimal order of $O(\\sqrt{T \\log n})$ in \\textit{expectation} for both regret and number of switches, where $T$ is the number of iterations and $n$ the number of actions. However, no \\textit{high probability} guarantees are known. Our main technical contribution is the first algorithms which with high probability achieve this optimal order for both regret and number of switches. This settles an open problem of [Devroye et al., 2015], directly implies the first high probability guarantees for several problems of interest, and is efficiently adaptable to the related problem of online combinatorial optimization with limited switching. \\par Next, to investigate the value of switching actions at a more granular level, we introduce the setting of \\textit{switching budgets}, in which the algorithm is limited to $S \\leq T$ switches between actions. This entails a limited number of free switches, in contrast to the unlimited number of expensive switches allowed in the switching cost setting. Using the above result and several reductions, we unify previous work and completely characterize the complexity of this switching budget setting up to small polylogarithmic factors: for both the PFE and MAB problems, for all switching budgets $S \\leq T$, and for both expectation and high probability guarantees. For PFE, we show that the optimal rate is of order $\\tilde{\\Theta}(\\sqrt{T\\log n})$ for $S = \\Omega(\\sqrt{T\\log n})$, and $\\min(\\tilde{\\Theta}(\\tfrac{T\\log n}{S}), T)$ for $S = O(\\sqrt{T \\log n})$. Interestingly, the bandit setting does not exhibit such a phase transition; instead we show the minimax rate decays steadily as $\\min(\\tilde{\\Theta}(\\tfrac{T\\sqrt{n}}{\\sqrt{S}}), T)$ for all ranges of $S \\leq T$. These results recover and generalize the known minimax rates for the (arbitrary) switching cost setting.", "pdf_url": "http://proceedings.mlr.press/v75/altschuler18a/altschuler18a.pdf", "keywords": [], "reference": "Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive  adversary: from regret to policy regret. ICML, 2012.  4   Online learning over a finite action set with limited switching  (a) Switching-budget PFE.  (b) Switching-budget MAB.  Figure 1: Complexity landscape of online learning over a finite action set with limited switching. Axes are plotted in log-log scale. Polylogarithmic factors in T are hidden for simplicity.  i.e. online linear optimization over a combinatorial polytope, where o\ufb04ine optimization can be done e\ufb03ciently.  We also investigate the switching budget setting for the PFE and MAB problems. The above result and standard reductions allow us to completely characterize the complexity of this switching budget setting up to small polylogarithmic factors: for both the PFE and MAB problems, for all switching budgets S \u2264 T , and for both expectation and high probability guarantees. For PFE, we show the optimal rate is of order \u02dc\u0398( T log n) for S = \u2126( T log n). Interestingly, the bandit ), T ) for S = O( setting does not exhibit such a phase transition; instead we show the minimax rate decays steadily as min( \u02dc\u0398( T  T log n), and min( \u02dc\u0398( T log n  ), T ) for all ranges of S \u2264 T .  \u221a  \u221a  \u221a  S  \u221a n\u221a S  .  2. Acknowledgements.  We are indebted to Elad Hazan for numerous fruitful discussions and for suggesting the switching-budget setting to us. We also thank Yoram Singer, Tomer Koren, David Martins, Vianney Perchet, and Jonathan Weed for helpful discussions.  Part of this work was done while JA was visiting the Simons Institute for the Theory of Computing, which was partially supported by the DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF grant #CCF-1740425. JA is also supported by NSF Graduate Research Fellowship 1122374.  References  Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive  adversary: from regret to policy regret. ICML, 2012. Online learning over a finite action set with limited switching  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11(Oct):2785-2836, 2010.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1): 1-122, 2012.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge  university press, 2006.  Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44 (3):427-485, 1997.  Nicolo Cesa-Bianchi, G\u00b4abor Lugosi, and Gilles Stoltz. Minimizing regret with label e\ufb03cient  prediction. IEEE Transactions on Information Theory, 51(6):2152-2162, 2005.  Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: T 2/3 regret. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 459-467. ACM, 2014.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Random-walk perturbations for online combinatorial optimization. IEEE Transactions on Information Theory, 61(7):4099-4106, 2015.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Sascha Geulen, Berthold V\u00a8ocking, and Melanie Winkler. Regret minimization for online bu\ufb00ering problems using the weighted majority algorithm. In COLT, pages 132-143, 2010.  Adam Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision problems. Journal  of Computer and System Sciences, 71(3):291-307, 2005.  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information  and computation, 108(2):212-261, 1994. "}, "Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent", "abstract": "We study \\emph{smoothed online convex optimization}, a version of online convex optimization where the learner incurs a penalty for changing her actions between rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio of any online algorithm, where $d$ is the dimension of the action space, we ask under what conditions this bound can be beaten. We introduce a novel algorithmic framework for this problem, Online Balanced Descent (OBD), which works by iteratively projecting the previous point onto a carefully chosen level set of the current cost function so as to balance the switching costs and hitting costs. We demonstrate the generality of the OBD framework by showing how, with different choices of \u201cbalance,\u201d OBD can improve upon state-of-the-art performance guarantees for both competitive ratio and regret; in particular, OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 + O(1/\\alpha)$,  for locally polyhedral costs, where $\\alpha$ measures the \u201csteepness\u201d of the costs.  We also prove bounds on the dynamic regret of OBD when the balance is performed in the dual space that are dimension-free and imply that OBD has sublinear static regret.", "pdf_url": "http://proceedings.mlr.press/v75/chen18b/chen18b.pdf", "keywords": [], "reference": "Jacob Abernethy, Peter L Bartlett, Niv Buchbinder, and Isabelle Stanton. A regularization approach to metrical task systems. In International Conference on Algorithmic Learning Theory, pages 270-284. Springer, 2010.  R Agrawal, M Hegde, and D Teneketzis. Multi-armed bandit problems with multiple plays and  switching cost. Stochastics and Stochastic Reports, 29(4):437-459, 1990.  Lachlan Andrew, Siddharth Barman, Katrina Ligett, Minghong Lin, Adam Meyerson, Alan Royt- man, and Adam Wierman. A tale of two metrics: Simultaneous bounds on competitiveness and regret. In Conference on Learning Theory, pages 741-763, 2013.  Antonios Antoniadis, Neal Barcelo, Michael Nugent, Kirk Pruhs, Kevin Schewior, and Michele  Scquizzato. Chasing Convex Bodies and Functions, pages 68-81. 2016.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Masoud Badiei, Na Li, and Adam Wierman. Online convex optimization with ramp constraints. In  IEEE Conference on Decision and Control, pages 6730-6736, 2015.  Nikhil Bansal, Anupam Gupta, Ravishankar Krishnaswamy, Kirk Pruhs, Kevin Schewior, and Clif- ford Stein. A 2-competitive algorithm for online convex optimization with switching costs. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 96-109, 2015.  Avrim Blum and Carl Burch. On-line learning and the metrical task system problem. Machine  Learning, 39(1):35-58, Apr 2000.  Avrim Blum, Howard Karloff, Yuval Rabani, and Michael Saks. A decomposition theorem and bounds for randomized server problems. In Foundations of Computer Science, pages 197-207, 1992.  Avrim Blum, Shuchi Chawla, and Adam Kalai. Static optimality and dynamic search-optimality in lists and trees. In Proc. of ACM-SIAM Symposium on Discrete Algorithms, pages 1-8, 2002.  Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. Cambridge Uni-  versity Press, 2005.  Allan Borodin, Nathan Linial, and Michael E. Saks. An optimal on-line algorithm for metrical task  system. J. ACM, 39(4):745-763, October 1992.  13   SMOOTHED ONLINE CONVEX OPTIMIZATION IN HIGH DIMENSIONS VIA ONLINE BALANCED DESCENT  space d, assuming the diameter of the space is normalized to 1. The key difference is that the bound in Corollary 11 is independent of the size of the gradients of the cost functions, unlike in the case of OGD. This can be viewed as a significant benefit that results from the fact that OBD steps in a direction normal to where it lands, rather than where it starts.  Finally, note that Theorem 10 and Corollary 11 additionally provide bounds on the static regret T ) which matches the  of OBD, by setting L = D. In this case, Corollary 11 gives a bound of O( lower bound in the setting where there are no switching costs (Hazan et al., 2016).  \u221a  References  Jacob Abernethy, Peter L Bartlett, Niv Buchbinder, and Isabelle Stanton. A regularization approach to metrical task systems. In International Conference on Algorithmic Learning Theory, pages 270-284. Springer, 2010.  R Agrawal, M Hegde, and D Teneketzis. Multi-armed bandit problems with multiple plays and  switching cost. Stochastics and Stochastic Reports, 29(4):437-459, 1990.  Lachlan Andrew, Siddharth Barman, Katrina Ligett, Minghong Lin, Adam Meyerson, Alan Royt- man, and Adam Wierman. A tale of two metrics: Simultaneous bounds on competitiveness and regret. In Conference on Learning Theory, pages 741-763, 2013.  Antonios Antoniadis, Neal Barcelo, Michael Nugent, Kirk Pruhs, Kevin Schewior, and Michele  Scquizzato. Chasing Convex Bodies and Functions, pages 68-81. 2016.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Masoud Badiei, Na Li, and Adam Wierman. Online convex optimization with ramp constraints. In  IEEE Conference on Decision and Control, pages 6730-6736, 2015.  Nikhil Bansal, Anupam Gupta, Ravishankar Krishnaswamy, Kirk Pruhs, Kevin Schewior, and Clif- ford Stein. A 2-competitive algorithm for online convex optimization with switching costs. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 96-109, 2015.  Avrim Blum and Carl Burch. On-line learning and the metrical task system problem. Machine  Learning, 39(1):35-58, Apr 2000.  Avrim Blum, Howard Karloff, Yuval Rabani, and Michael Saks. A decomposition theorem and bounds for randomized server problems. In Foundations of Computer Science, pages 197-207, 1992.  Avrim Blum, Shuchi Chawla, and Adam Kalai. Static optimality and dynamic search-optimality in lists and trees. In Proc. of ACM-SIAM Symposium on Discrete Algorithms, pages 1-8, 2002.  Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. Cambridge Uni-  versity Press, 2005.  Allan Borodin, Nathan Linial, and Michael E. Saks. An optimal on-line algorithm for metrical task  system. J. ACM, 39(4):745-763, October 1992. CHEN GOEL WIERMAN  S. Bubeck, M. B. Cohen, J. R. Lee, Y. Tat Lee, and A. Madry. k-server via multiscale entropic  regularization. ArXiv e-prints, 2017.  S\u00b4ebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends  in Machine Learning, 8(3-4):231-357, 2015.  Niv Buchbinder, Shahar Chen, Joshep Seffi Naor, and Ohad Shamir. Unified algorithms for online  learning and competitive analysis. In Conference on Learning Theory, pages 5-1, 2012.  Niv Buchbinder, Shahar Chen, and Joseph Seffi Naor. Competitive analysis via regularization. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 436-444. Society for Industrial and Applied Mathematics, 2014.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. A new look at shifting regret.  CoRR, abs/1202.3323, 2012.  Niangjun Chen, Anish Agarwal, Adam Wierman, Siddharth Barman, and Lachlan LH Andrew. Online convex optimization using predictions. In ACM SIGMETRICS Performance Evaluation Review, volume 43, pages 191-204. ACM, 2015.  Niangjun Chen, Joshua Comden, Zhenhua Liu, Anshul Gandhi, and Adam Wierman. Using predic- tions in online optimization: Looking forward with an eye on the past. SIGMETRICS Perform. Eval. Rev., 44(1):193-206, June 2016.  Joel Friedman and Nathan Linial. On convex body chasing. Discrete & Computational Geometry,  9(3):293-321, Mar 1993.  Gautam Goel, Niangjun Chen, and Adam Wierman. Thinking fast and slow: Optimization decom-  position across timescales. arXiv preprint arXiv:1704.07785, 2017.  Sudipto Guha and Kamesh Munagala. Multi-armed bandits with metric switching costs.  In In- ternational Colloquium on Automata, Languages, and Programming, pages 496-507. Springer, 2009.  Eric C Hall and Rebecca M Willett. Dynamical models and tracking regret in online convex pro-  gramming. arXiv preprint arXiv:1301.1254, 2013.  Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends in Optimiza-  tion, 2(3-4):157-325, 2016.  Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal of Machine  Learning Research, 1:281-309, 2001.  Longbo Huang and Michael J. Neely. Delay reduction via lagrange multipliers in stochastic network  optimization. IEEE Transactions on Automatic Control, 56(4):842-857, 2011.  Vinay Joseph and Gustavo de Veciana.  Jointly optimizing multi-user rate adaptation for video transport over wireless systems: Mean-fairness-variability tradeoffs. In IEEE INFOCOM, pages 567-575, 2012. SMOOTHED ONLINE CONVEX OPTIMIZATION IN HIGH DIMENSIONS VIA ONLINE BALANCED DESCENT  Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization. Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09.pdf, 2009.  Seung-Jun Kim and Geogios B Giannakis. Real-time electricity pricing for demand response using  online convex optimization. In IEEE Innovative Smart Grid Tech., pages 1-5, 2014.  Taehwan Kim, Yisong Yue, Sarah Taylor, and Iain Matthews. A decision tree framework for spa- tiotemporal sequence prediction. In ACM International Conference on Knowledge Discovery and Data Mining, pages 577-586, 2015.  Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear  predictors. Information and Computation, 132(1):1-63, 1997.  Tomer Koren, Roi Livni, and Yishay Mansour. Multi-armed bandits with metric movement costs.  In Advances in Neural Information Processing Systems, pages 4122-4131, 2017.  Brian Kulis and Peter L Bartlett. Implicit online learning. In Proceedings of the 27th International  Conference on Machine Learning (ICML-10), pages 575-582. Citeseer, 2010.  Yingying Li, Guannan Qu, and Na Li. Online optimization with predictions and switching costs:  Fast algorithms and the fundamental limit. arXiv preprint arXiv:1801.07780, 2018.  Minghong Lin, Adam Wierman, Lachlan L. H. Andrew, and Thereska Eno. Dynamic right-sizing  for power-proportional data centers. In IEEE INFOCOM, pages 1098-1106, 2011.  Minghong Lin, Zhenhua Liu, Adam Wierman, and Lachlan LH Andrew. Online algorithms for  geographical load balancing. In IEEE Green Computing Conference, pages 1-10, 2012.  Tan Lu, Minghua Chen, and Lachlan LH Andrew. Simple and effective dynamic provisioning for power-proportional data centers. IEEE Transactions on Parallel and Distributed Systems, 24(6): 1161-1171, 2013.  Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity  and method efficiency in optimization. 1983.  Yu. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):  127-152, May 2005.  Kirk Pruhs. Errata, 2018. URL http://people.cs.pitt.edu/\u02dckirk/Errata.html.  Igal Sason and Sergio Verd\u00b4u. Upper bounds on the relative entropy and r\u00b4enyi divergence as a function of total variation distance for finite alphabets. In IEEE Information Theory Workshop, pages 214-218, 2015.  Hao Wang, Jianwei Huang, Xiaojun Lin, and Hamed Mohsenian-Rad. Exploring smart grid and data center interactions for electric power load balancing. ACM SIGMETRICS Performance Evalua- tion Review, 41(3):89-94, 2014. CHEN GOEL WIERMAN  Manfred K Warmuth and Arun K Jagota. Continuous and discrete-time nonlinear gradient descent: Relative loss bounds and convergence. In Electronic proceedings of the 5th International Sympo- sium on Artificial Intelligence and Mathematics. Citeseer, 1997.  Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.  Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.  Francesco Zanini, David Atienza, Luca Benini, and Giovanni De Micheli. Multicore thermal man- agement with model predictive control. In IEEE. European Conf. Circuit Theory and Design, pages 711-714, 2009.  Francesco Zanini, David Atienza, Giovanni De Micheli, and Stephen P Boyd. Online convex In The Great lakes sym-  optimization-based algorithm for thermal management of MPSoCs. posium on VLSI, pages 203-208, 2010.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  International Conference on Machine Learning, pages 928-936, 2003.  "}, "Faster Rates for Convex-Concave Games": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Faster Rates for Convex-Concave Games", "abstract": "We consider the use of no-regret algorithms to compute equilibria for particular classes of convex-concave games. While standard regret bounds would lead to convergence rates on the order of $O(T^{-1/2})$, recent work \\citep{RS13,SALS15} has established $O(1/T)$ rates by taking advantage of a particular class of optimistic prediction algorithms. In this work we go further, showing that for a particular class of games one achieves a $O(1/T^2)$ rate, and we show how this applies to the Frank-Wolfe method and recovers a similar bound \\citep{D15}. We also show that such no-regret techniques can even achieve a linear rate, $O(\\exp(-T))$, for equilibrium computation under additional curvature assumptions.", "pdf_url": "http://proceedings.mlr.press/v75/abernethy18a/abernethy18a.pdf", "keywords": ["Online learning", "zero-sum games", "Frank-Wolfe", "fast rates"], "reference": "Jacob Abernethy and Jun-Kun Wang. Frank-wolfe and equilibrium computation. NIPS, 2017.  Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari. Online linear optimization via  smoothing. In COLT, pages 807-823, 2014.  Ilan Adler. The equivalence of linear programs and zero-sum games. International Journal of Game Theory, 42(1):165-177, Feb 2013. ISSN 1432-1270. doi: 10.1007/s00182-012-0328-8. URL https://doi.org/10.1007/s00182-012-0328-8.  Francis  Bach.  Convex  relaxations  of  structured matrix  factorizations.  https://arxiv.org/pdf/1309.3117.pdf, 2013.  Dimitri P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals.  Journal of Optimization Theory and Applications, 1973.  David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathe-  matics, 6(1):1-8, 1956.  examples. Springer, 2006.  Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization theory and  Stephen Boyd. Convex optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, , and  Shenghuo Zhu. Online optimization with gradual variations. 2012.  Vladimir F. Demyanov and Aleksandr M. Rubinov. Approximate methods in optimization prob-  lems. Elsevier Publishing Company,, 1970.  John Duchi, Peter L. Bartlett, and Martin Wainwright. Randomized smoothing for stochastic opti-  mization. SIAM Journal on Optimization (SIOPT), 2012.  Joseph C. Dunn. Rates of convergence for conditional gradient algorithms near singular and non-  singular extremals. SIAM Journal on Control and Optimization, 1979.  Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research  logistics quarterly, 3(1-2):95-110, 1956.  13   FASTER RATES FOR CONVEX-CONCAVE GAMES  J.A., K.A.L, and J.W. gratefully acknowledge financial support from the National Science Founda- tion, award IIS 1453304. K.Y.L. is supported by the ETH Zurich Postdoctoral Fellowship and Marie Curie Actions for People COFUND program. J.W. also thanks the support from ARC-TRIAD Fel- lowship.  Acknowledgments  References  Jacob Abernethy and Jun-Kun Wang. Frank-wolfe and equilibrium computation. NIPS, 2017.  Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari. Online linear optimization via  smoothing. In COLT, pages 807-823, 2014.  Ilan Adler. The equivalence of linear programs and zero-sum games. International Journal of Game Theory, 42(1):165-177, Feb 2013. ISSN 1432-1270. doi: 10.1007/s00182-012-0328-8. URL https://doi.org/10.1007/s00182-012-0328-8.  Francis  Bach.  Convex  relaxations  of  structured matrix  factorizations.  https://arxiv.org/pdf/1309.3117.pdf, 2013.  Dimitri P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals.  Journal of Optimization Theory and Applications, 1973.  David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathe-  matics, 6(1):1-8, 1956.  examples. Springer, 2006.  Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization theory and  Stephen Boyd. Convex optimization. Cambridge University Press, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, , and  Shenghuo Zhu. Online optimization with gradual variations. 2012.  Vladimir F. Demyanov and Aleksandr M. Rubinov. Approximate methods in optimization prob-  lems. Elsevier Publishing Company,, 1970.  John Duchi, Peter L. Bartlett, and Martin Wainwright. Randomized smoothing for stochastic opti-  mization. SIAM Journal on Optimization (SIOPT), 2012.  Joseph C. Dunn. Rates of convergence for conditional gradient algorithms near singular and non-  singular extremals. SIAM Journal on Control and Optimization, 1979.  Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research  logistics quarterly, 3(1-2):95-110, 1956. ABERNETHY LAI LEVY WANG  Yoav Freund and Robert E Schapire. Game theory, on-line prediction and boosting. In Proceedings of the ninth annual conference on Computational learning theory, pages 325-332. ACM, 1996.  Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29(1-2):79-103, 1999.  Dan Garber and Elad Hazan. Faster rates for the frank-wolfe method over strongly-convex sets.  ICML, 2015.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,  3:97-139, 1957.  Elad Hazan. Introduction to online convex optimization. 2014.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Sham Kakade and Shai Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for  online optimization. 2009.  Sham M. Kakade, Shai Shalev-shwartz, and Ambuj Tewari. On the duality of strong convexity and  strong smoothness: Learning applications and matrix regularization. 2009.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 2005a.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005b.  Evgeny S Levitin and Boris T Polyak. Constrained minimization methods. USSR Computational  mathematics and mathematical physics, 1966.  Kfir Levy. Online to of\ufb02ine conversions, universality and adaptive minibatch sizes. 2017.  Haipeng Luo. Introduction to online learning: Lecture 6. 2017.  J von Neumann, Oskar Morgenstern, et al. Theory of games and economic behavior, 1944.  E. S. Polovinkin. Strongly convex analysis. Sbornik: Mathematics 187(2):259, 1996.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. COLT,  Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable  sequences. NIPS, 2013b.  Alexander Rakhlin and Karthik Sridharan. Statistical learning theory and sequential prediction.  2013a.  2016. FASTER RATES FOR CONVEX-CONCAVE GAMES  Tyrrell Rockafellar. Convex analysis. Princeton University Press, 1996.  Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and  Trends R(cid:13) in Machine Learning, 4(2):107-194, 2012.  Maurice Sion. On general minimax theorems. Pacific J. Math, 8(1):171-176, 1958.  Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of  regularized learning in games. NIPS, 2015.  J v. Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.  Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. 2005.  Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical  risk minimization. 2015.  "}, "\\ell_1 Regression using Lewis Weights Preconditioning and Stochastic Gradient Descent": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "\\ell_1 Regression using Lewis Weights Preconditioning and Stochastic Gradient Descent", "abstract": "We present preconditioned stochastic gradient descent (SGD) algorithms for the $\\ell_1$ minimization problem $\\min_{\\boldsymbol{\\mathit{x}}}\\|\\boldsymbol{\\mathit{A}} \\boldsymbol{\\mathit{x}} - \\boldsymbol{\\mathit{b}}\\|_1$ in the overdetermined case, where there are far more constraints than variables. Specifically, we have $\\boldsymbol{\\mathit{A}} \\in \\mathbb{R}^{n \\times d}$ for $n \\gg d$. Commonly known as the Least Absolute Deviations problem, $\\ell_1$ regression can be used to solve many important combinatorial problems, such as minimum cut and shortest path. SGD-based algorithms are appealing for their simplicity and practical efficiency. Our primary insight is that careful preprocessing can yield preconditioned matrices $\\tilde{\\boldsymbol{\\mathit{A}}}$ with strong properties (besides good condition number and low-dimension) that allow for faster convergence of gradient descent. In particular, we precondition using Lewis weights to obtain an isotropic matrix with fewer rows and strong upper bounds on all row norms. We leverage these conditions to find a good initialization, which we use along with recent smoothing reductions and accelerated stochastic gradient descent algorithms to achieve $\\epsilon$ relative error in $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + d^{2.5} \\epsilon^{-2})$ time with high probability, where $nnz(\\boldsymbol{\\mathit{A}})$ is the number of non-zeros in $\\boldsymbol{\\mathit{A}}$. This improves over the previous best result using gradient descent for $\\ell_1$ regression. We also match the best known running times for interior point methods in several settings. Finally, we also show that if our original matrix $\\boldsymbol{\\mathit{A}}$ is approximately isotropic and the row norms are approximately equal, we can give an algorithm that avoids using fast matrix multiplication and obtains a running time of $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + s d^{1.5}\\epsilon^{-2} + d^2\\epsilon^{-2})$, where $s$ is the maximum number of non-zeros in a row of $\\boldsymbol{\\mathit{A}}$. In this setting, we beat the best interior point methods for certain parameter regimes.", "pdf_url": "http://proceedings.mlr.press/v75/durfee18a/durfee18a.pdf", "keywords": ["(cid:96)1 regression", "stochastic gradient descent", "Lewis weights"], "reference": "Zeyuan Allen Zhu. Katyusha: the first direct acceleration of stochastic gradient methods. In Pro- ceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 1200-1205, 2017. doi: 10.1145/3055399. 3055448. URL http://doi.acm.org/10.1145/3055399.3055448.  Zeyuan Allen-Zhu and Elad Hazan. Optimal black-box reductions between optimization objectives. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pages 1614-1622, USA, 2016. Curran Associates Inc. ISBN 978-1-5108-3881-9. URL http://dl.acm.org/citation.cfm?id=3157096.3157277.  S\u00b4ebastien Bubeck, Michael B Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method for arXiv preprint  (cid:96)p regression provably beyond self-concordance and in input-sparsity time. arXiv:1711.01328, 2017. Available at: https://arxiv.org/abs/1711.01328.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  Hui Han Chin, Aleksander Madry, Gary L. Miller, and Richard Peng. Runtime guarantees for re- gression problems. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 269-282, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1859- 4. doi: 10.1145/2422436.2422469. URL http://doi.acm.org/10.1145/2422436. 2422469.  Kenneth L. Clarkson. Subgradient and sampling algorithms for (cid:96)1 regression. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201905, pages 257-266, Philadelphia, PA, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0-89871- 585-7. URL http://dl.acm.org/citation.cfm?id=1070432.1070469.  Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng, and David P. Woodruff. The fast cauchy transform and faster robust linear regression. In Proceed- ings of the Twenty-fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201913, pages 466-477, Philadelphia, PA, USA, 2013. Society for Industrial and Applied Mathematics. ISBN 978-1-611972-51-1. URL http://dl.acm.org/citation.cfm?id=2627817. 2627851.  Michael B. Cohen and Richard Peng. (cid:96)p row sampling by lewis weights. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing, STOC \u201915, pages 183-192, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3536-2. doi: 10.1145/2746539.2746567. URL http://doi.acm.org/10.1145/2746539.2746567.  Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, ITCS \u201915, pages 181-190, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3333-7. doi: 10.1145/2688073.2688113. URL http: //doi.acm.org/10.1145/2688073.2688113.  13   (cid:96)1 REGRESSION USING LEWIS WEIGHTS AND SGD  References  Zeyuan Allen Zhu. Katyusha: the first direct acceleration of stochastic gradient methods. In Pro- ceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 1200-1205, 2017. doi: 10.1145/3055399. 3055448. URL http://doi.acm.org/10.1145/3055399.3055448.  Zeyuan Allen-Zhu and Elad Hazan. Optimal black-box reductions between optimization objectives. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pages 1614-1622, USA, 2016. Curran Associates Inc. ISBN 978-1-5108-3881-9. URL http://dl.acm.org/citation.cfm?id=3157096.3157277.  S\u00b4ebastien Bubeck, Michael B Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method for arXiv preprint  (cid:96)p regression provably beyond self-concordance and in input-sparsity time. arXiv:1711.01328, 2017. Available at: https://arxiv.org/abs/1711.01328.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  Hui Han Chin, Aleksander Madry, Gary L. Miller, and Richard Peng. Runtime guarantees for re- gression problems. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 269-282, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1859- 4. doi: 10.1145/2422436.2422469. URL http://doi.acm.org/10.1145/2422436. 2422469.  Kenneth L. Clarkson. Subgradient and sampling algorithms for (cid:96)1 regression. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201905, pages 257-266, Philadelphia, PA, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0-89871- 585-7. URL http://dl.acm.org/citation.cfm?id=1070432.1070469.  Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng, and David P. Woodruff. The fast cauchy transform and faster robust linear regression. In Proceed- ings of the Twenty-fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201913, pages 466-477, Philadelphia, PA, USA, 2013. Society for Industrial and Applied Mathematics. ISBN 978-1-611972-51-1. URL http://dl.acm.org/citation.cfm?id=2627817. 2627851.  Michael B. Cohen and Richard Peng. (cid:96)p row sampling by lewis weights. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing, STOC \u201915, pages 183-192, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3536-2. doi: 10.1145/2746539.2746567. URL http://doi.acm.org/10.1145/2746539.2746567.  Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, ITCS \u201915, pages 181-190, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3333-7. doi: 10.1145/2688073.2688113. URL http: //doi.acm.org/10.1145/2688073.2688113. (cid:96)1 REGRESSION USING LEWIS WEIGHTS AND SGD  Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Mahoney. Sam- pling algorithms and coresets for (cid:96)p regression. SIAM J. Comput., 38(5):2060-2078, February 2009. ISSN 0097-5397. doi: 10.1137/070696507. URL http://dx.doi.org/10.1137/ 070696507.  A. M. Davie and A. J. Stothers. Improved bound for complexity of matrix multiplication. Pro- ceedings of the Royal Society of Edinburgh: Section A Mathematics, 143(2):351-369, 2013. doi: 10.1017/S0308210511001648.  John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic  optimization. SIAM Journal on Optimization, 22(2):674-701, 2012.  Mart\u00b4\u0131n Farach-Colton and Meng-Tsung Tsai. Exact sublinear binomial sampling. Algorithmica, ISSN 0178-4617. doi: 10.1007/s00453-015-0077-8. URL  73(4):637-651, December 2015. http://dx.doi.org/10.1007/s00453-015-0077-8.  F. G. Foster. On the stochastic matrices associated with certain queuing processes. The Annals of Mathematical Statistics, 24(3):355-360, 1953. ISSN 00034851. URL http://www.jstor. org/stable/2236286.  Nick Harvey.  Matrix concentration and sparsification. Numerical Linear Algebra (RandNLA): Theory and Practice\u201d, 2012. http://www.drineas.org/RandNLA/slides/Harvey RandNLA@FOCS 2012.pdf.  Workshop on \u201cRandomized Available at:  Franc\u00b8ois Le Gall. Powers of tensors and fast matrix multiplication.  In Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation, ISSAC \u201914, pages 296-303, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2501-1. doi: 10.1145/2608628.2608664. URL http://doi.acm.org/10.1145/2608628.2608664.  Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster algorithms for linear In Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of programming. Computer Science (FOCS), FOCS \u201915, pages 230-249, Washington, DC, USA, 2015. IEEE ISBN 978-1-4673-8191-8. doi: 10.1109/FOCS.2015.23. URL http: Computer Society. //dx.doi.org/10.1109/FOCS.2015.23.  D. Lewis. Finite dimensional subspaces of (cid:96)p. Studia Mathematica, 63(2):207-212, 1978. URL  http://eudml.org/doc/218208.  Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC \u201913, pages 91-100, New York, NY, USA, 2013a. ACM. ISBN 978-1-4503-2029-0. doi: 10.1145/2488608.2488621. URL http://doi.acm. org/10.1145/2488608.2488621.  Xiangrui Meng and Michael W. Mahoney. Robust regression on mapreduce.  In Proceedings of the 30th International Conference on International Conference on Machine Learning - Vol- ume 28, ICML\u201913, pages III-888-III-896. JMLR.org, 2013b. URL http://dl.acm.org/ citation.cfm?id=3042817.3043036. (cid:96)1 REGRESSION USING LEWIS WEIGHTS AND SGD  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19(4):1574-1609, January 2009. ISSN 1052- 6234. doi: 10.1137/070704277. URL http://dx.doi.org/10.1137/070704277.  Yu. Nesterov and J. Ph. Vial. Confidence level solutions for stochastic programming. Automatica, 44(6):1559-1568, June 2008. ISSN 0005-1098. doi: 10.1016/j.automatica.2008.01.017. URL http://dx.doi.org/10.1016/j.automatica.2008.01.017.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate  O(1/k2). In Soviet Mathematics Doklady, volume 27, pages 372-376, 1983.  Yurii Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization, 16(1):235-249, May 2005a. ISSN 1052-6234. doi: 10.1137/S1052623403422285. URL http://dx.doi.org/10.1137/S1052623403422285.  Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103 (1):127-152, May 2005b. ISSN 0025-5610. doi: 10.1007/s10107-004-0552-5. URL http: //dx.doi.org/10.1007/s10107-004-0552-5.  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Math- doi: 10.1007/  ematical Programming, 110(2):245-259, March 2007. s10107-006-0001-8. URL http://dx.doi.org/10.1007/s10107-006-0001-8.  ISSN 0025-5610.  Yurii Nesterov. Unconstrained convex minimization in relative scale. Mathematics of Operations ISSN 0364-765X. doi: 10.1287/moor.1080.0348.  Research, 34(1):180-193, February 2009. URL http://dx.doi.org/10.1287/moor.1080.0348.  Hua Ouyang and Alexander Gray. Stochastic smoothing for nonsmooth minimizations: Accelerat- ing SGD by exploiting structure. In Proceedings of the 29th International Coference on Interna- tional Conference on Machine Learning, ICML\u201912, pages 1523-1530, USA, 2012. Omnipress. ISBN 978-1-4503-1285-1. URL http://dl.acm.org/citation.cfm?id=3042573. 3042768.  Stephen Portnoy. On computation of regression quantiles: Making the laplacian tortoise faster. Lecture Notes-Monograph Series, 31:187-200, 1997. ISSN 07492170. URL http://www. jstor.org/stable/4355977.  Stephen Portnoy and Roger Koenker. The gaussian hare and the laplacian tortoise: computability of squared-error versus absolute-error estimators. Statistical Science, 12(4):279-300, 11 1997. doi: 10.1214/ss/1030037960. URL http://dx.doi.org/10.1214/ss/1030037960.  Andrzej Ruszczynski and Wojciech Syski. ent method with on-line stepsize rules. plications, 114(2):512 - 527, 1986. 1016/0022-247X(86)90104-6. article/pii/0022247X86901046.  On convergence of the stochastic subgradi- Journal of Mathematical Analysis and Ap- http://dx.doi.org/10. doi: URL http://www.sciencedirect.com/science/  ISSN 0022-247X.  Sushant Sachdeva and Nisheeth K. Vishnoi. Faster algorithms via approximation theory. Founda- tions and Trends R(cid:13) in Theoretical Computer Science, 9(2):125-210, March 2014. ISSN 1551- 305X. doi: 10.1561/0400000065. URL http://dx.doi.org/10.1561/0400000065. (cid:96)1 REGRESSION USING LEWIS WEIGHTS AND SGD  Christian Sohler and David P. Woodruff. Subspace embeddings for the (cid:96)1-norm with applications. In Proceedings of the Forty-third Annual ACM Symposium on Theory of Computing, STOC \u201911, pages 755-764, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0691-1. doi: 10.1145/ 1993636.1993736. URL http://doi.acm.org/10.1145/1993636.1993736.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Com- doi: 10.1007/ URL http://dx.doi.org/10.1007/s10208-011-9099-z.  putational Mathematics, 12(4):389-434, August 2012. s10208-011-9099-z. Available at http://arxiv.org/abs/1004.4389.  ISSN 1615-3375.  Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.  In Pro- ceedings of the Forty-fourth Annual ACM Symposium on Theory of Computing, STOC \u201912, pages 887-898, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1245-5. doi: 10.1145/2213977. 2214056. URL http://doi.acm.org/10.1145/2213977.2214056.  David P. Woodruff and Qin Zhang. Subspace embeddings and (cid:96)p-regression using exponential random variables. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12- 14, 2013, Princeton University, NJ, USA, pages 546-567, 2013. URL http://jmlr.org/ proceedings/papers/v30/Woodruff13.html.  Jiyan Yang, Yin-Lam Chow, Christopher R\u00b4e, and Michael W. Mahoney. Weighted SGD for (cid:96)p regression with randomized preconditioning. In Proceedings of the Twenty-seventh Annual ACM- SIAM Symposium on Discrete Algorithms, SODA \u201916, pages 558-569, Philadelphia, PA, USA, 2016. Society for Industrial and Applied Mathematics. ISBN 978-1-611974-33-1. URL http: //dl.acm.org/citation.cfm?id=2884435.2884476.  "}, "Optimal Single Sample Tests for Structured versus Unstructured Network Data": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Optimal Single Sample Tests for Structured versus Unstructured Network Data", "abstract": "We study the problem of testing, using only a single sample, between mean field distribu- tions (like Curie-Weiss, Erd\u0151s-R\u00e9nyi) and structured Gibbs distributions (like Ising model on sparse graphs and Exponential Random Graphs). Our goal is to test without know- ing the parameter values of the underlying models: only the structure of dependencies is known. We develop a new approach that applies to both the Ising and Exponential Random Graph settings based on a general and natural statistical test. The test can dis- tinguish the hypotheses with high probability above a certain threshold in the (inverse) temperature parameter, and is optimal in that below the threshold no test can distinguish the hypotheses. The thresholds do not correspond to the presence of long-range order in the models. By aggregating information at a global scale, our test works even at very high temperatures. The proofs are based on distributional approximation and sharp concentration of quadratic forms, when restricted to Hamming spheres. The restriction to Hamming spheres is necessary, since otherwise any scalar statistic is useless without explicit knowledge of the temperature parameter. At the same time, this restriction changes the behavior of the functions under consideration, making it hard to directly apply standard methods (i.e., Stein\u2019s method) for concentration of weakly dependent variables. Instead, we carry out an additional tensorization argument using a Markov chain that respects the symmetry of the Hamming sphere.", "pdf_url": "http://proceedings.mlr.press/v75/bresler18a/bresler18a.pdf", "keywords": [], "reference": "Anirban Basak and Sumit Mukherjee. Universality of the mean-field for the Potts model.  Probability Theory and Related Fields, 168(3-4):557-600, 2017.  Shankar Bhamidi, Guy Bresler, and Allan Sly. Mixing time of exponential random graphs.  The Annals of Applied Probability, pages 2146-2170, 2011.  St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A  nonasymptotic theory of independence. Oxford university press, 2013.  Guy Bresler and Dheeraj M Nagaraj. Stein\u2019s method for stationary distributions of markov  chains and application to Ising models. arXiv preprint arXiv:1712.05743, 2017.  S\u00e9bastien Bubeck, Jian Ding, Ronen Eldan, and Mikl\u00f3s Z R\u00e1cz. Testing for high-dimensional  geometry in random graphs. Random Structures & Algorithms, 49(3):503-532, 2016.  Clement L Canonne, Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Testing  Bayesian networks. In Conference on Learning Theory, pages 370-448, 2017.  Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph. d. thesis). arXiv  preprint math/0507526, 2005.  Sourav Chatterjee. Stein\u2019s method for concentration inequalities. Probability theory and  related fields, 138(1):305-321, 2007.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising models.  arXiv preprint arXiv:1612.03147, 2016.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Concentration of mul- tilinear functions of the Ising model with applications to network data. In Advances in Neural Information Processing Systems, pages 12-22, 2017.  Peter de Jong. A central limit theorem for generalized quadratic forms. Probability Theory  and Related Fields, 75(2):261-277, 1987.  Ronen Eldan and Renan Gross. Exponential random graphs behave like mixtures of stochas-  tic block models. arXiv preprint arXiv:1707.01227, 2017.  Richard Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Chao Gao and John La\ufb00erty. Testing network structure using relations between small  subgraph probabilities. arXiv preprint arXiv:1704.06742, 2017.  Reza Gheissari, Eyal Lubetzky, and Yuval Peres. Concentration inequalities for polynomials  of contracting Ising models. arXiv preprint arXiv:1706.00121, 2017.  Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, and Ulrike von Luxburg. Two-sample tests for large random graphs using network statistics. Proceedings of Machine Learning Research vol, 65:1-24, 2017.  13   Comparing Structured and Unstructured Network Data  References  Anirban Basak and Sumit Mukherjee. Universality of the mean-field for the Potts model.  Probability Theory and Related Fields, 168(3-4):557-600, 2017.  Shankar Bhamidi, Guy Bresler, and Allan Sly. Mixing time of exponential random graphs.  The Annals of Applied Probability, pages 2146-2170, 2011.  St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A  nonasymptotic theory of independence. Oxford university press, 2013.  Guy Bresler and Dheeraj M Nagaraj. Stein\u2019s method for stationary distributions of markov  chains and application to Ising models. arXiv preprint arXiv:1712.05743, 2017.  S\u00e9bastien Bubeck, Jian Ding, Ronen Eldan, and Mikl\u00f3s Z R\u00e1cz. Testing for high-dimensional  geometry in random graphs. Random Structures & Algorithms, 49(3):503-532, 2016.  Clement L Canonne, Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Testing  Bayesian networks. In Conference on Learning Theory, pages 370-448, 2017.  Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph. d. thesis). arXiv  preprint math/0507526, 2005.  Sourav Chatterjee. Stein\u2019s method for concentration inequalities. Probability theory and  related fields, 138(1):305-321, 2007.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising models.  arXiv preprint arXiv:1612.03147, 2016.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Concentration of mul- tilinear functions of the Ising model with applications to network data. In Advances in Neural Information Processing Systems, pages 12-22, 2017.  Peter de Jong. A central limit theorem for generalized quadratic forms. Probability Theory  and Related Fields, 75(2):261-277, 1987.  Ronen Eldan and Renan Gross. Exponential random graphs behave like mixtures of stochas-  tic block models. arXiv preprint arXiv:1707.01227, 2017.  Richard Ellis. Entropy, large deviations, and statistical mechanics. Springer, 2007.  Chao Gao and John La\ufb00erty. Testing network structure using relations between small  subgraph probabilities. arXiv preprint arXiv:1704.06742, 2017.  Reza Gheissari, Eyal Lubetzky, and Yuval Peres. Concentration inequalities for polynomials  of contracting Ising models. arXiv preprint arXiv:1706.00121, 2017.  Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, and Ulrike von Luxburg. Two-sample tests for large random graphs using network statistics. Proceedings of Machine Learning Research vol, 65:1-24, 2017. Comparing Structured and Unstructured Network Data  F G\u00f6tze and A Tikhomirov. Asymptotic distribution of quadratic forms and applications.  Journal of Theoretical Probability, 15(2):423-475, 2002.  Peter Hall. Central limit theorem for integrated square error of multivariate nonparametric  density estimators. Journal of multivariate analysis, 14(1):1-16, 1984.  Hamid Javadi and Andrea Montanari. A statistical model for motifs detection. arXiv  preprint arXiv:1511.05254, 2015.  Abraham Mart\u00edn del Campo, Sarah Cepeda, and Caroline Uhler. Exact goodness-of-fit  testing for the ising model. Scandinavian Journal of Statistics, 44(2):285-306, 2017.  Sumit Mukherjee. Consistent estimation in the two star exponential random graph model.  arXiv preprint arXiv:1310.4526, 2013.  Nathan Ross et al. Fundamentals of stein\u2019s method. Probability Surveys, 8:210-293, 2011.  Vladimir I Rotar et al. Limit theorems for polylinear forms. Journal of Multivariate analysis,  9(4):511-530, 1979. Comparing Structured and Unstructured Network Data  "}, "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation", "abstract": "Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a \\emph{simple and explicit finite time analysis} of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for  analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. A final section of the paper shows that all of our main results extend to the study of a variant of Q-learning applied to optimal stopping problems.", "pdf_url": "http://proceedings.mlr.press/v75/bhandari18a/bhandari18a.pdf", "keywords": ["Reinforcement learning", "temporal difference learning", "finite sample bounds", "stochastic gradient descent"], "reference": "3(1):9-44, 1988.  Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning,  John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function  approximation. IEEE TRANSACTIONS ON AUTOMATIC CONTROL, 42(5), 1997.  2   FINITE TIME ANALYSIS OF TD  precise conditions for the asymptotic convergence of TD with linear function approximation, and provided counterexamples when these conditions are violated. With guarantees of asymptotic con- vergence in place, a natural next step is to understand the algorithm\u2019s statistical efficiency. How much data is required to reach a given level of accuracy? Can one give uniform bounds on this, or could data-requirements explode depending on the problem instance? Twenty years after the work of Tsitsiklis and Van Roy (1997), such questions remain largely unsettled.  In this work, we take a step toward correcting this by providing a simple and explicit finite time analysis of temporal difference learning. We draw inspiration from the analysis of projected stochastic gradient descent. These analyses are simple-enough so that they are frequently taught in machine learning courses-and the explicit bounds they produce provide clear assurance of the robustness of SGD. Unfortunately, there are critical differences between TD and SGD, and as such these simple analyses do not apply to TD. Instead, past work on TD has needed to invoke powerful results from the theory of stochastic approximation. In this work, we uncover an approach to ana- lyzing TD which, except for a few crucial steps, leverages the standard tools for finite time analysis of SGD. In addition to the several novel guarantees we derive in the paper, we feel the analysis offers insight into the dynamics of TD, and we hope our approach helps future researchers derive stronger bounds and principled improvements to the algorithm.  References  3(1):9-44, 1988.  Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning,  John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function  approximation. IEEE TRANSACTIONS ON AUTOMATIC CONTROL, 42(5), 1997. "}, "Privacy-preserving Prediction": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Privacy-preserving Prediction", "abstract": "Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.", "pdf_url": "http://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf", "keywords": [], "reference": "R. Bassily, A. Smith, and A. Thakurta. Private empirical risk minimization: Efficient algorithms  and tight error bounds. In FOCS, pages 464-473, 2014.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman.  Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016.  Raef Bassily, Om Thakkar, and Abhradeep Thakurta. Model-agnostic private learning via stability.  CoRR, abs/1803.05101, 2018. URL http://arxiv.org/abs/1803.05101.  Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity  for private learning and private data release. In TCC, pages 437-454, 2010.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private  learners. In ITCS, pages 97-110, 2013.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. JMLR, 2:499-526, 2002.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and  learning of threshold functions. In FOCS, pages 634-649, 2015.  Nicholas Carlini, Chang Liu, Jernej Kos, \u00b4Ulfar Erlingsson, and Dawn Song. The secret sharer: Me- asuring unintended neural network memorization & extracting secrets. CoRR, abs/1802.08232, 2018. URL http://arxiv.org/abs/1802.08232.  Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning.  In COLT, pages 155-186, 2011.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. Journal of Machine Learning Research, 12:1069-1109, 2011.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. CoRR, abs/1803.10266, 2018.  URL http://arxiv.org/abs/1803.10266. Extended abstract in COLT 2018.  8   PRIVACY-PRESERVING PREDICTION  demonstrate that given a non-private learning algorithm has error of at most \u03b1 (such as in the PAC model), there exists an algorithm that answers m prediction queries for points chosen i.i.d. from the m\u03b1 \u00b7 log m (for compari- same distribution with error O(\u03b1) and privacy parameter (cid:15) scaling as m scaling for son, a direct application of composition theorems for differential privacy implies an arbitrary sequence of queries). They then analyze the sample complexity of semi-supervised (or, equivalently, label-private) learning algorithm that is obtained by labeling a public unlabeled dataset using their algorithm for answering prediction queries.  \u221a  \u221a  We remark that all these works do not examine the problem of private prediction itself and focus on the aggregation-based approaches. Recall that in private prediction, it is the privacy of the training data for the predictor (model) that is being protected.  References  R. Bassily, A. Smith, and A. Thakurta. Private empirical risk minimization: Efficient algorithms  and tight error bounds. In FOCS, pages 464-473, 2014.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman.  Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016.  Raef Bassily, Om Thakkar, and Abhradeep Thakurta. Model-agnostic private learning via stability.  CoRR, abs/1803.05101, 2018. URL http://arxiv.org/abs/1803.05101.  Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity  for private learning and private data release. In TCC, pages 437-454, 2010.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private  learners. In ITCS, pages 97-110, 2013.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. JMLR, 2:499-526, 2002.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and  learning of threshold functions. In FOCS, pages 634-649, 2015.  Nicholas Carlini, Chang Liu, Jernej Kos, \u00b4Ulfar Erlingsson, and Dawn Song. The secret sharer: Me- asuring unintended neural network memorization & extracting secrets. CoRR, abs/1802.08232, 2018. URL http://arxiv.org/abs/1802.08232.  Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning.  In COLT, pages 155-186, 2011.  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical  risk minimization. Journal of Machine Learning Research, 12:1069-1109, 2011.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. CoRR, abs/1803.10266, 2018.  URL http://arxiv.org/abs/1803.10266. Extended abstract in COLT 2018. PRIVACY-PRESERVING PREDICTION  Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC, pages 371-380,  2009.  Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy, volume 9.  2014. URL http://dx.doi.org/10.1561/0400000042.  Cynthia Dwork and Adam Smith. Differential privacy for statistics: What we know and what we  want to learn. Journal of Privacy and Confidentiality, 1(2):135-154, 2009.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Vitaly Feldman and Thomas Steinke. Generalization for adaptively-chosen estimators via stable  median. In Conference on Learning Theory (COLT), 2017.  Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via  communication complexity. SIAM J. Comput., 44(6):1740-1764, 2015.  Jihun Hamm, Yingjun Cao, and Mikhail Belkin. Learning privately from multiparty data.  In Maria Florina Balcan and Kilian Q. Weinberger, editors, ICML, volume 48 of Proceedings of Machine Learning Research, pages 555-563, 2016. URL http://proceedings.mlr. press/v48/hamm16.html.  Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of sto- In ICML, pages 1225-1234, 2016. URL http://jmlr.org/  chastic gradient descent. proceedings/papers/v48/hardt16.html.  D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning  applications. Information and Computation, 100(1):78-150, 1992. ISSN 0890-5401.  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, June 2011.  M. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine Learning, 17  (2-3):115-141, 1994.  Daniel Kifer, Adam D. Smith, and Abhradeep Thakurta. Private convex optimization for empiri- cal risk minimization with applications to high-dimensional regression. In COLT, pages 25.1- 25.40, 2012. URL http://www.jmlr.org/proceedings/papers/v23/kifer12/ kifer12.pdf.  Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen. Understanding membership inferences on well-generalized learning mo- dels. CoRR, abs/1802.04889, 2018. URL http://arxiv.org/abs/1802.04889.  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS, pages  94-103, 2007.  Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in  private data analysis. In STOC, pages 75-84, 2007. PRIVACY-PRESERVING PREDICTION  Nicolas Papernot, Mart\u00b4\u0131n Abadi, \u00b4Ulfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semi- supervised knowledge transfer for deep learning from private training data. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.  Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Er- lingsson. Scalable private learning with PATE. International Conference on Learning Represen- tations, 2018. URL https://openreview.net/forum?id=rkZB1XbRZ.  Manas A. Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggregation  of locally trained classifiers. In NIPS, pages 1876-1884, 2010.  Anand D. Sarwate and Kamalika Chaudhuri. Signal processing and machine learning with diffe- rential privacy: Algorithms and challenges for continuous data. IEEE Signal Process. Mag., 30 (5):86-94, 2013.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at- tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy, SP 2017, pages 3-18, 2017.  Adam Smith and Abhradeep Guha Thakurta. Differentially private feature selection via stability  arguments, and the robustness of the lasso. In COLT, 2013.  Kunal Talwar, Abhradeep Thakurta, and Li Zhang.  In NIPS, pages 3025-3033, 5729-nearly-optimal-private-lasso.  2015.  Nearly optimal private LASSO. URL http://papers.nips.cc/paper/  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:  Faster and more general. In NIPS, pages 2719-2728, 2017.  Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey F. Naughton. Bolt- In SIGMOD,  on differential privacy for scalable stochastic gradient descent-based analytics. pages 1307-1322, 2017. "}, "An Estimate Sequence for Geodesically Convex Optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "An Estimate Sequence for Geodesically Convex Optimization", "abstract": "We propose a Riemannian version of Nesterov\u2019s Accelerated Gradient algorithm (\\textsc{Ragd}), and show that for \\emph{geodesically} smooth and strongly convex problems, within a neighborhood of the minimizer whose radius depends on the condition number as well as the sectional curvature of the manifold, \\textsc{Ragd} converges to the minimizer with acceleration. Unlike the algorithm in (Liu et al., 2017) that requires the exact solution to a nonlinear equation which in turn may be intractable, our algorithm is constructive and computationally tractable. Our proof exploits a new estimate sequence and a novel bound on the nonlinear metric distortion, both ideas may be of independent interest.", "pdf_url": "http://proceedings.mlr.press/v75/zhang18a/zhang18a.pdf", "keywords": ["Riemannian optimization", "geodesically convex optimization", "Nesterov\u2019s accelerated gradient method", "nonlinear optimization"], "reference": "University Press, 2009.  The authors thank the anonymous reviewers for helpful feedback. This work was supported in part by NSF-IIS-1409802 and the DARPA Lagrange grant.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton  Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local  minima for nonconvex optimization in linear time. CoRR, abs/1611.01146, 2016.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror  descent. arXiv:1407.1537, 2014.  Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9, et al. Metric measure spaces with Riemannian Ricci curvature  bounded from below. Duke Mathematical Journal, 163(7):1405-1490, 2014.  Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds for smooth and strongly  convex optimization problems. arXiv:1503.06833, 2015.  11   AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  6. Discussion  In this work, we proposed a Riemannian generalization of the accelerated gradient algorithm and developed its convergence and complexity analysis. For the first time (to the best of our knowledge), we show gradient based algorithms on Riemannian manifolds can be accelerated, at least in a neighborhood of the minimizer. Central to our analysis are the two main technical contributions of our work: a new estimate sequence (Lemma 4), which relaxes the assumption of Nesterov\u2019s original construction and handles metric distortion on Riemannian manifolds; a tangent space distance comparison theorem (Theorem 10), which provides sufficient conditions for bounding the metric distortion and could be of interest for a broader range of problems on Riemannian manifolds.  Despite not matching the standard convex results, our result exposes the key difficulty of analyzing Nesterov-style algorithms on Riemannian manifolds, an aspect missing in previous work. Critically, the convergence analysis relies on bounding a new distortion term per each step. Furthermore, we observe that the side length sequence d(yk, vk+1) can grow much greater than d(yk, x\u2217), even if we reduce the \u201cstep size\u201d hk in Algorithm 1, defeating any attempt to control the distortion globally by modifying the algorithm parameters. This is a benign feature in vector space analysis, since (8) trivially holds nonetheless; however it poses a great difficulty for analysis in nonlinear space. Note the stark contrast to (stochastic) gradient descent, where the step length can be effectively controlled by reducing the step size, hence bounding the distortion terms globally (Zhang and Sra, 2016).  A topic of future interest is to study whether assumption (8) can be further relaxed, while maintaining that overall the algorithm still converges. By bounding the squared distance distortion in every step, our analysis provides guarantee for the worst-case scenario, which seems unlikely to happen in practice. It would be interesting to conduct experiments to see how often (8) is violated versus how often it is loose. It would also be interesting to construct some adversarial problem case (if any) and study the complexity lower bound of gradient based Riemannian optimization, to see if geodesically convex optimization is strictly more difficult than convex optimization. Generalizing the current analysis to non-strongly g-convex functions is another interesting direction.  Acknowledgments  References  University Press, 2009.  The authors thank the anonymous reviewers for helpful feedback. This work was supported in part by NSF-IIS-1409802 and the DARPA Lagrange grant.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton  Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local  minima for nonconvex optimization in linear time. CoRR, abs/1611.01146, 2016.  Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror  descent. arXiv:1407.1537, 2014.  Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9, et al. Metric measure spaces with Riemannian Ricci curvature  bounded from below. Duke Mathematical Journal, 163(7):1405-1490, 2014.  Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir. On lower and upper bounds for smooth and strongly  convex optimization problems. arXiv:1503.06833, 2015. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  Hedy Attouch, J\u00e9r\u00f4me Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, 137(1-2):91-129, 2013.  Miroslav Bac\u00e1k. Convex analysis and optimization in Hadamard spaces, volume 22. Walter de Gruyter GmbH  & Co KG, 2014.  Nicolas Boumal, P-A Absil, and Coralia Cartis. Global rates of convergence for nonconvex optimization on  manifolds. arXiv:1605.08101, 2016a.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016b.  S\u00e9bastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to Nesterov\u2019s accelerated gradient  descent. arXiv:1506.08187, 2015.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society Providence, 2001.  Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex  optimization. CoRR, abs/1611.00756, 2016.  Yair Carmon, Oliver Hinder, John C Duchi, and Aaron Sidford. \" convex until proven guilty\": Dimension-free  acceleration of gradient descent on non-convex functions. arXiv preprint arXiv:1705.02766, 2017.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  OP Ferreira and PR Oliveira. Proximal point algorithm on Riemannian manifolds. Optimization, 51(2):  257-270, 2002.  Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In Conference  on Learning Theory, pages 658-695, 2015.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified  geometric analysis. arXiv:1704.00708, 2017.  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic  programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In  Advances in Neural Information Processing Systems, pages 315-323, 2013.  J\u00fcrgen Jost. Riemannian Geometry and Geometric Analysis. Springer Science & Business Media, 2011.  Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the  sixteenth annual ACM symposium on Theory of computing, pages 302-311. ACM, 1984.  Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic variance reduced gradient on  Grassmann manifold. arXiv:1605.07367, 2016.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing  Systems, pages 586-594, 2016. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  Leonid G Khachiyan. Polynomial algorithms in linear programming. USSR Computational Mathematics and  Mathematical Physics, 20(1):53-72, 1980.  Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via  integral quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.  Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order methods In Advances in Neural Information  for geodesically convex optimization on Riemannian manifolds. Processing Systems, pages 4875-4884, 2017.  Wolfgang Meyer. Toponogov\u2019s theorem and applications. SMR, 404:9, 1989.  Bamdev Mishra and Rodolphe Sepulchre. Riemannian preconditioning. SIAM Journal on Optimization, 26(1):  635-660, 2016.  optimization. Wiley, 1983.  Arkadi\u02d8\u0131 Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method efficiency in  Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In  Soviet Mathematics Doklady, volume 27(2), pages 372-376, 1983.  Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media,  2004.  B.T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and  Mathematical Physics, 3(4):864-878, January 1963.  Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alexander J. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 314-323, 2016.  Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average  gradient. arXiv:1309.2388, 2013.  Ohad Shamir. A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate. In International  Conference on Machine Learning (ICML-15), pages 144-152, 2015.  Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510-2518, 2014.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the  geometric picture. IEEE Transactions on Information Theory, 63(2):853-884, 2017.  Constantin Udriste. Convex functions and optimization methods on Riemannian manifolds, volume 297.  Springer Science & Business Media, 1994.  Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in  optimization. Proceedings of the National Academy of Sciences, page 201614734, 2016.  Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In 29th Annual  Conference on Learning Theory (COLT), pages 1617-1638, 2016.  Hongyi Zhang, Sashank J. Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on  Riemannian manifolds. In Advances in Neural Information Processing Systems 29, 2016. AN ESTIMATE SEQUENCE FOR GEODESICALLY CONVEX OPTIMIZATION  "}, "The Externalities of Exploration and How Data Diversity Helps Exploitation": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "The Externalities of Exploration and How Data Diversity Helps Exploitation", "abstract": "Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future.  Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups.  Motivated by these concerns, we initiate the study of the externalities of exploration\u2014the undesirable side effects that the presence of one party may impose on another\u2014under the linear contextual bandits model.  We introduce the notion of a group externality, measuring the extent to which the presence of one population of users (the majority) impacts the rewards of another (the minority). We show that this impact can, in some cases, be negative, and that, in a certain sense, no algorithm can avoid it.  We then move on to study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary.  We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\\tilde{O}(T^{1/3})$. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish if one runs the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.", "pdf_url": "http://proceedings.mlr.press/v75/raghavan18a/raghavan18a.pdf", "keywords": [], "reference": "Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic  bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning (ICML), 2014.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Miro Dudik, John Langford, Lihong Li, Luong Hoang, Dan Melamed, Siddhartha Sen, Robert Schapire, and Alex Slivkins. Multiworld test- ing: A system for experimentation, learning, and decision-making. A white paper, available at https://github.com/Microsoft/mwt-ds/raw/master/images/MWT-WhitePaper.pdf, 2016.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making contextual decisions with low technical debt. CoRR arXiv:1606.03966, 2017.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Exploiting the natural exploration in  contextual bandits. CoRR arXiv:1704.09011, 2017.  Alberto Bietti, Alekh Agarwal, and John Langford. Practical evaluation and optimization of con-  textual bandit algorithms. CoRR arXiv:1802.04064, 2018.  Kostas Bimpikis, Yiangos Papanastasiou, and Nicos Savva. Crowdsourcing exploration. Manage-  ment Science, 2017. Forthcoming.  Sarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna Wallach. Exploring or ex- ploiting? Social and ethical implications of autonomous experimentation in AI. Available at SSRN: https://ssrn.com/abstract=2846909, also appeared at the Workshop on Fairness, Account- ability, and Transparency in Machine Learning, 2016.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-  armed Bandit Problems. Foundations and Trends in Machine Learning, 5(1), 2012.  L. Elisa Celis and Nisheeth K Vishnoi. Fair personalization. CoRR arXiv:1707.02260, also ap- peared at the Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2017.  Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex ge- ometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  Yeon-Koo Che and Johannes H\u00a8orner. Optimal design for social learning. Preprint, 2015.  Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism  prediction instruments. CoRR arXiv:1703.00056, 2017.  13   THE EXTERNALITIES OF EXPLORATION AND HOW DATA DIVERSITY HELPS EXPLOITATION  References  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic  bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning (ICML), 2014.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Miro Dudik, John Langford, Lihong Li, Luong Hoang, Dan Melamed, Siddhartha Sen, Robert Schapire, and Alex Slivkins. Multiworld test- ing: A system for experimentation, learning, and decision-making. A white paper, available at https://github.com/Microsoft/mwt-ds/raw/master/images/MWT-WhitePaper.pdf, 2016.  Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making contextual decisions with low technical debt. CoRR arXiv:1606.03966, 2017.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Exploiting the natural exploration in  contextual bandits. CoRR arXiv:1704.09011, 2017.  Alberto Bietti, Alekh Agarwal, and John Langford. Practical evaluation and optimization of con-  textual bandit algorithms. CoRR arXiv:1802.04064, 2018.  Kostas Bimpikis, Yiangos Papanastasiou, and Nicos Savva. Crowdsourcing exploration. Manage-  ment Science, 2017. Forthcoming.  Sarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna Wallach. Exploring or ex- ploiting? Social and ethical implications of autonomous experimentation in AI. Available at SSRN: https://ssrn.com/abstract=2846909, also appeared at the Workshop on Fairness, Account- ability, and Transparency in Machine Learning, 2016.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-  armed Bandit Problems. Foundations and Trends in Machine Learning, 5(1), 2012.  L. Elisa Celis and Nisheeth K Vishnoi. Fair personalization. CoRR arXiv:1707.02260, also ap- peared at the Workshop on Fairness, Accountability, and Transparency in Machine Learning, 2017.  Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex ge- ometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  Yeon-Koo Che and Johannes H\u00a8orner. Optimal design for social learning. Preprint, 2015.  Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism  prediction instruments. CoRR arXiv:1703.00056, 2017. THE EXTERNALITIES OF EXPLORATION AND HOW DATA DIVERSITY HELPS EXPLOITATION  Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.  John D Cook. Upper and lower bounds for the normal distribution function, 2009.  Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and linden-  strauss. Random Structures & Algorithms, 22(1):60-65, 2003.  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness  through awareness. In Innovations in Theoretical Computer Science (ITCS), 2012.  Peter Frazier, David Kempe, Jon M. Kleinberg, and Robert Kleinberg. Incentivizing exploration. In  ACM Conference on Economics and Computation (ACM EC), 2014.  Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning.  In  Advances in Neural Information Processing Systems (NIPS), 2016.  Svante Janson.  Tail bounds for sums of geometric and exponential variables.  CoRR  arXiv:1709.08157, 2017.  Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems (NIPS), 2016.  Sampath Kannan, Jamie Morgenstern, Aaron Roth, Bo Waggoner, and Zhiwei Steven Wu. A smoothed analysis of the greedy algorithm for the linear contextual bandit problem. CoRR arXiv:1801.03423, 2018.  Michael Kearns, Aaron Roth, and Zhiwei Steven Wu. Meritocratic fairness for cross-population  selection. In International Conference on Machine Learning (ICML), 2017.  Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair deter-  mination of risk scores. In Innovations in Theoretical Computer Science (ITCS), 2017.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping  experts and bandits. Machine Learning, 80(2-3):245-272, 2010.  Ilan Kremer, Yishay Mansour, and Motty Perry. Implementing the wisdom of the crowd. Journal  of Political Economy, 122:988-1012, 2014.  John Langford and Tong Zhang. The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits.  In Advances in Neural Information Processing Systems (NIPS), 2007.  Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to per- sonalized news article recommendation. In International World Wide Web Conference (WWW), 2010.  Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Cali-  brated fairness in bandits. CoRR arXiv:1707.01875, 2017. THE EXTERNALITIES OF EXPLORATION AND HOW DATA DIVERSITY HELPS EXPLOITATION  Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit  exploration. In ACM Conference on Economics and Computation (ACM EC), 2015.  Yishay Mansour, Aleksandrs Slivkins, and Zhiwei Steven Wu. Competing bandits: Learning under  competition. In Innovations in Theoretical Computer Science (ITCS), 2018.  Philippe Rigollet and Assaf Zeevi. Nonparametric Bandits with Covariates.  In Conference on  Learning Theory (COLT), 2010.  Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex  algorithm usually takes polynomial time. Journal of the ACM, 51(3):385-463, 2004.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009. "}, "Efficient Contextual Bandits in Non-stationary Worlds": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Efficient Contextual Bandits in Non-stationary Worlds", "abstract": "Most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications.  In this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   We analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. When competing with the best policy at each time, one of our algorithms achieves regret $\\mathcal{O}(\\sqrt{ST})$ if there are $T$ rounds with $S$ stationary periods, or more generally $\\mathcal{O}(\\Delta^{1/3}T^{2/3})$ where $\\Delta$ is some non-stationarity measure. These results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic Exp4 algorithm. The dynamic regret result is also the first one for efficient and fully adversarial contextual bandit. Furthermore, while the results above require tuning a parameter based on the unknown quantity $S$ or $\\Delta$, we also develop a parameter free algorithm achieving regret $\\min\\{S^{1/4}T^{3/4}, \\Delta^{1/5}T^{4/5}\\}$. This improves and generalizes the best existing result $\\Delta^{0.18}T^{0.82}$ by Karnin and Anava (2016) which only holds for the two-armed bandit problem.", "pdf_url": "http://proceedings.mlr.press/v75/luo18a/luo18a.pdf", "keywords": [], "reference": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming In Proceedings of the 31st  the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning, 2014.  Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In 30th Annual Conference on Learning Theory (COLT), 2017.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In 29th Annual Conference on Learning Theory (COLT), 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial  bandits. In 25th Annual Conference on Learning Theory (COLT), 2012.  Deepayan Chakrabarti, Ravi Kumar, Filip Radlinski, and Eli Upfal. Mortal multi-armed bandits. In  Advances in neural information processing systems, pages 273-280, 2009.  Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In Pro-  ceedings of the 32nd International Conference on Machine Learning, 2015.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016.  Elad Hazan and C. Seshadhri. Adaptive algorithms for online decision problems.  In Electronic  Colloquium on Computational Complexity (ECCC), volume 14, 2007.  Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151-  178, 1998.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in neural information processing systems, 2008.  13   EFFICIENT CONTEXTUAL BANDITS IN NON-STATIONARY WORLDS  References  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming In Proceedings of the 31st  the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning, 2014.  Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of  bandit algorithms. In 30th Annual Conference on Learning Theory (COLT), 2017.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas-  tic and adversarial bandits. In 29th Annual Conference on Learning Theory (COLT), 2016.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-  stationary rewards. In Advances in Neural Information Processing Systems 27, 2014.  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations  Research, 63(5):1227-1244, 2015.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.  Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteri-  ors. Journal of Machine Learning Research, 3(Nov):363-396, 2002.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial  bandits. In 25th Annual Conference on Learning Theory (COLT), 2012.  Deepayan Chakrabarti, Ravi Kumar, Filip Radlinski, and Eli Upfal. Mortal multi-armed bandits. In  Advances in neural information processing systems, pages 273-280, 2009.  Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In Pro-  ceedings of the 32nd International Conference on Machine Learning, 2015.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning.  In  Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, 2016.  Elad Hazan and C. Seshadhri. Adaptive algorithms for online decision problems.  In Electronic  Colloquium on Computational Complexity (ECCC), volume 14, 2007.  Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151-  178, 1998.  Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences.  In  Advances in Neural Information Processing Systems 29, 2016.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in neural information processing systems, 2008. EFFICIENT CONTEXTUAL BANDITS IN NON-STATIONARY WORLDS  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, 2016a.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, pages 1977-1985, 2016b.  Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efficient algorithms for ad- versarial contextual learning. In Proceedings of the 33rd International Conference on Machine Learning, 2016a.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems, 2016b.  Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. arXiv preprint  arXiv:1801.03265, 2018.  Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic  environments. In Advances in Neural Information Processing Systems 29, 2016.  Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou. Strongly adaptive regret implies opti-  mally dynamic regret. arXiv preprint arXiv:1701.07570, 2017. EFFICIENT CONTEXTUAL BANDITS IN NON-STATIONARY WORLDS  "}, "Langevin Monte Carlo and JKO splitting": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Langevin Monte Carlo and JKO splitting", "abstract": "Algorithms based on discretizing Langevin diffusion are popular tools for sampling from high-dimensional distributions. We develop novel connections between such Monte Carlo algorithms, the theory of Wasserstein gradient flow, and the operator splitting approach to solving PDEs. In particular, we show that a proximal version of the Unadjusted Langevin Algorithm corresponds to a scheme that alternates between solving the gradient flows of two specific functionals on the space of probability measures. Using this perspective, we derive some new non-asymptotic results on the convergence properties of this algorithm.", "pdf_url": "http://proceedings.mlr.press/v75/bernton18a/bernton18a.pdf", "keywords": ["Langevin Monte Carlo", "Fokker-Planck", "Wasserstein gradient \ufb02ow", "operator splitting", "proximal operators"], "reference": "Luigi Ambrosio, Nicola Gigli, and Guiseppe Savar\u00b4e. Gradient Flows in Metric Spaces and in the  Space of Probability Measures. Birkh\u00a8auser Verlag AG, Basel, second edition, 2005.  Luigi Ambrosio, Giuseppe Savar\u00b4e, and Lorenzo Zambotti. Existence and stability for Fokker- Planck equations with log-concave reference measure. Probability theory and related fields, 145 (3):517-564, 2009.  Jean-David Benamou, Guillaume Carlier, Quentin M\u00b4erigot, and Edouard Oudet. Discretization of functionals involving the Monge-Amp`ere operator. Numerische mathematik, 134(3):611-636, 2016.  Adrien Blanchet and J\u00b4er\u02c6ome Bolte. A family of functional inequalities: Lojasiewicz inequalities  and displacement convex functions. arXiv preprint arXiv:1612.02619, 2016.  Malcolm Bowles and Martial Agueh. Weak solutions to a fractional Fokker-Planck equation via  splitting and Wasserstein gradient \ufb02ow. Applied Mathematics Letters, 42:30-35, 2015.  Martin Burger, Marzena Franek, and Carola-Bibiane Sch\u00a8onlieb. Regularized regression and density estimation based on optimal transport. Applied Mathematics Research eXpress, 2012(2):209- 253, 2012.  Jos\u00b4e A Carrillo, Alina Chertock, and Yanghong Huang. A finite-volume method for nonlinear nonlocal equations with a gradient \ufb02ow structure. Communications in Computational Physics, 17(1):233-258, 2015a.  Jose Antonio Carrillo, Yanghong Huang, Francesco Saverio Patacchini, and Gershon Wolansky. Numerical study of a particle method for gradient \ufb02ows. arXiv preprint arXiv:1512.03029, 2015b.  Jos\u00b4e Antonio Carrillo, Katy Craig, and Francesco S Patacchini. A blob method for diffusion. arXiv  preprint arXiv:1709.09195, 2017.  Emmanuel C\u00b4epa. Probl`eme de Skorohod multivoque. The Annals of Probability, 26(2):500-532,  1998.  11   LMC AND JKO SPLITTING  first-order convergence. Recently, Plazotta (2018) developed a variational formulation of the BDF2 scheme applicable to the estimation of gradient \ufb02ows. It is also likely that the growing literature on Langevin Monte Carlo and its variations can lead to new time discretization schemes that are of both practical and theoretical interest to the gradient \ufb02ow community.  Acknowledgments  I\u2019m greatly indebted to Nicolas Chopin and Marco Cuturi for hosting my visit to ENSAE ParisTech and CREST, where the material in this paper was developed. I\u2019d also like to thank L\u00b4ena\u00a8\u0131c Chizat, Arnak Dalalyan, Jeremy Heng, Pierre E. Jacob, Boris Muzellec and Gabriel Peyr\u00b4e for interesting conversations about optimal transport, gradient \ufb02ows, and Monte Carlo sampling. This material is based upon research supported by the Chateaubriand Fellowship of the Office for Science & Technology of the Embassy of France in the United States.  References  Luigi Ambrosio, Nicola Gigli, and Guiseppe Savar\u00b4e. Gradient Flows in Metric Spaces and in the  Space of Probability Measures. Birkh\u00a8auser Verlag AG, Basel, second edition, 2005.  Luigi Ambrosio, Giuseppe Savar\u00b4e, and Lorenzo Zambotti. Existence and stability for Fokker- Planck equations with log-concave reference measure. Probability theory and related fields, 145 (3):517-564, 2009.  Jean-David Benamou, Guillaume Carlier, Quentin M\u00b4erigot, and Edouard Oudet. Discretization of functionals involving the Monge-Amp`ere operator. Numerische mathematik, 134(3):611-636, 2016.  Adrien Blanchet and J\u00b4er\u02c6ome Bolte. A family of functional inequalities: Lojasiewicz inequalities  and displacement convex functions. arXiv preprint arXiv:1612.02619, 2016.  Malcolm Bowles and Martial Agueh. Weak solutions to a fractional Fokker-Planck equation via  splitting and Wasserstein gradient \ufb02ow. Applied Mathematics Letters, 42:30-35, 2015.  Martin Burger, Marzena Franek, and Carola-Bibiane Sch\u00a8onlieb. Regularized regression and density estimation based on optimal transport. Applied Mathematics Research eXpress, 2012(2):209- 253, 2012.  Jos\u00b4e A Carrillo, Alina Chertock, and Yanghong Huang. A finite-volume method for nonlinear nonlocal equations with a gradient \ufb02ow structure. Communications in Computational Physics, 17(1):233-258, 2015a.  Jose Antonio Carrillo, Yanghong Huang, Francesco Saverio Patacchini, and Gershon Wolansky. Numerical study of a particle method for gradient \ufb02ows. arXiv preprint arXiv:1512.03029, 2015b.  Jos\u00b4e Antonio Carrillo, Katy Craig, and Francesco S Patacchini. A blob method for diffusion. arXiv  preprint arXiv:1709.09195, 2017.  Emmanuel C\u00b4epa. Probl`eme de Skorohod multivoque. The Annals of Probability, 26(2):500-532,  1998. LMC AND JKO SPLITTING  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint  arXiv:1705.09048, 2017.  Philippe Cl\u00b4ement and Jan Maas. A Trotter product formula for gradient \ufb02ows in metric spaces.  Journal of Evolution Equations, 11(2):405-427, 2011.  Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. arXiv preprint arXiv:1412.7392, 2014.  Arnak S. Dalalyan. Further and stronger analogy between sampling and optimization: Langevin  Monte Carlo and gradient descent. arXiv preprint arXiv:1704.04752, 2017.  Arnak S. Dalalyan and Avetik G. Karagulyan. User-friendly guarantees for the Langevin Monte  Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095, 2017.  Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the Unadjusted  Langevin Algorithm. arXiv preprint arXiv:1605.01559, 2016a.  Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with the Un-  adjusted Langevin Algorithm. arXiv preprint arXiv:1605.01559, 2016b.  Alain Durmus and Eric Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin  algorithm. The Annals of Applied Probability, 27(3):1551-1587, 2017.  Alain Durmus, Eric Moulines, and Marcelo Pereyra. Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau. arXiv preprint arXiv:1612.07471, 2016.  Edwin Grappin. Model Averaging in Large Scale Learning. PhD thesis, Universit\u00b4e Paris-Saclay,  2018.  Helge Holden, Kenneth H Karlsen, Knut-Andreas Lie, and Nils Henrik Risebro. Splitting Methods for Partial Differential Equations with Rough Solutions. European Mathematical Society, 2010.  S\u00f8ren F. Jarner and Ernst Hansen. Geometric ergodicity of Metropolis algorithms. Stochastic  processes and their applications, 85(2):341-361, 2000.  Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-  Planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.  Guillaume Legendre and Gabriel Turinici. Second-order in time schemes for gradient \ufb02ows in Wasserstein and geodesic metric spaces. Comptes Rendus Mathematique, 355(3):345-353, 2017.  Robert J. McCann. A convexity principle for interacting gases. Advances in Mathematics, 128(1):  153-179, 1997.  Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer  Science & Business Media, 2013.  Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1  (3):127-239, 2014. LMC AND JKO SPLITTING  Grigorios A. Pavliotis. Stochastic processes and applications. Springer, 2014.  Marcelo Pereyra. Proximal Markov chain Monte Carlo algorithms. Statistics and Computing, 26  (4):745-760, 2016.  Sciences, 8(4):2323-2351, 2015.  Gabriel Peyr\u00b4e. Entropic approximation of Wasserstein gradient \ufb02ows. SIAM Journal on Imaging  Natesh S. Pillai, Andrew M. Stuart, and Alexandre H. Thi\u00b4ery. Optimal scaling and diffusion limits for the Langevin algorithm in high dimensions. The Annals of Applied Probability, 22(6):2320- 2356, 2012.  Simon Plazotta. A BDF2-approach for the non-linear Fokker-Planck equation. arXiv preprint  arXiv:1801.09603, 2018.  Gareth O. Roberts and Osnat Stramer. Langevin diffusions and Metropolis-Hastings algorithms.  Methodology and computing in applied probability, 4(4):337-357, 2002.  Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions and  their discrete approximations. Bernoulli, 2(4):341-363, 1996.  Filippo Santambrogio. {Euclidean, Metric, and Wasserstein} Gradient Flows: an overview. arXiv  preprint arXiv:1609.03890, 2016.  Igor Stojkovi\u00b4c. Geometric approach to evolution problems in metric spaces. PhD thesis, Mathe-  matical Institute, Faculty of Science, Leiden University, 2011.  C\u00b4edric Villani. Optimal transport, old and new. Springer-Verlag New York, 2008.  Tatiana Xifara, Chris Sherlock, Samuel Livingstone, Simon Byrne, and Mark Girolami. Langevin diffusions and the Metropolis-adjusted Langevin algorithm. Statistics & Probability Letters, 91: 14-19, 2014.  "}, "Subpolynomial trace reconstruction for random strings \\{and arbitrary deletion probability": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Subpolynomial trace reconstruction for random strings \\{and arbitrary deletion probability", "abstract": "The deletion-insertion channel takes as input a bit string ${\\bf x}\\in \\{0,1\\}^{n}$, and outputs a string where bits have been deleted and inserted independently at random. The trace reconstruction problem is to recover $\\bf x$ from many independent outputs (called \u201ctraces\u201d) of the deletion-insertion channel applied to $\\bf x$. We show that if $\\bf x$ is chosen uniformly at random, then $\\exp(O(\\log^{1/3} n))$ traces suffice to reconstruct $\\bf x$ with high probability. For the deletion channel with deletion probability $q<1/2$ the earlier upper bound was $\\exp(O(\\log^{1/2} n))$. The case of $q\\geq 1/2$ or the case where insertions are allowed has not been previously analysed, and therefore the earlier upper bound was as for worst-case strings, i.e., $\\exp(O( n^{1/3}))$. A key ingredient in our proof is a delicate two-step alignment procedure where we estimate the location in each trace corresponding to a given bit of $\\bf x$. The alignment is done by viewing the strings as random walks, and comparing the increments in the walk associated with the input string and the trace, respectively.", "pdf_url": "http://proceedings.mlr.press/v75/holden18a/holden18a.pdf", "keywords": ["Trace reconstruction", "Deletion channel", "Sample complexity"], "reference": "Kazuoki Azuma. Weighted sums of certain dependent random variables. T\u02c6ohoku Math. J. (2), 19: 357-367, 1967. ISSN 0040-8735. URL https://doi.org/10.2748/tmj/1178243286.  Tuvgkan Batu, Sampath Kannan, Sanjeev Khanna, and Andrew McGregor. Reconstructing strings from random traces. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 910-918. ACM, New York, 2004.  P. Borwein and T. Erd\u00b4elyi. Littlewood-type problems on subarcs of the unit circle. Indiana Univ. Math. J., 46(4):1323-1346, 1997. ISSN 0022-2518. doi: 10.1512/iumj.1997.46.1435. URL http://dx.doi.org/10.1512/iumj.1997.46.1435.  A. De, R. O\u2019Donnell, and R. Servedio. Optimal mean-based algorithms for trace reconstruction. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 17, pages 1047-1056, New York, NY, USA, 2017. ACM.  Geoffrey Grimmett. Percolation, volume 321 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, second edition, 1999. ISBN 3-540-64902-6. URL https://doi.org/10.1007/978-3-662-03981-6.  T. E. Harris. A lower bound for the critical probability in a certain percolation process. Proc.  Cambridge Philos. Soc., 56:13-20, 1960.  Wassily Hoeffding. Probability inequalities for sums of bounded random variables. J. Amer. Statist. Assoc., 58:13-30, 1963. ISSN 0162-1459. URL http://links.jstor.org/sici?sici= 0162-1459(196303)58:301<13:PIFSOB>2.0.CO;2-D&origin=MSN.  Thomas Holenstein, Michael Mitzenmacher, Rina Panigrahy, and Udi Wieder. Trace reconstruction with constant deletion probability and related results. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 389-398. ACM, New York, 2008.  Sampath Kannan and Andrew McGregor. More on reconstructing strings from random traces: insertions and deletions. In Proceedings of the International Symposium on Information Theory (ISIT), pages 297-301. IEEE, 2005.  10   SUBPOLYNOMIAL TRACE RECONSTRUCTION  as the coefficients of a particular polynomial. Then we deduce the theorem by applying a result of Borwein and Erd\u00b4elyi (1997), which says that the modulus of certain polynomials cannot be too small everywhere on a small boundary arc of the unit disk.  Applying Theorem 3 with m = O(log n) allows us to determine xk+1, using our estimate \u03c42 to f (k\u2217). We apply the theorem repeatedly with all possible pairs of strings x(1) and x(2), such that the initial part of the strings are given by x(k\u2217 : k), and we consider the traces (cid:101)x(\u03c42 + (cid:98)log4/9 n(cid:99) : \u221e). Using the alignment result of Theorem 2 we can show that the random shift satisfies the assumptions of Theorem 3 with high probability. If one of the strings x(i) is equal to our input string x, then we can use (2) to determine from our (cid:100)exp(M log1/3 n)(cid:101) traces which of the two input strings is correct. It is sufficient to consider finitely many candidate strings x(1) and x(2), since strings which differ only for bits very far out are unlikely to affect the part of the trace we consider.  References  Kazuoki Azuma. Weighted sums of certain dependent random variables. T\u02c6ohoku Math. J. (2), 19: 357-367, 1967. ISSN 0040-8735. URL https://doi.org/10.2748/tmj/1178243286.  Tuvgkan Batu, Sampath Kannan, Sanjeev Khanna, and Andrew McGregor. Reconstructing strings from random traces. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 910-918. ACM, New York, 2004.  P. Borwein and T. Erd\u00b4elyi. Littlewood-type problems on subarcs of the unit circle. Indiana Univ. Math. J., 46(4):1323-1346, 1997. ISSN 0022-2518. doi: 10.1512/iumj.1997.46.1435. URL http://dx.doi.org/10.1512/iumj.1997.46.1435.  A. De, R. O\u2019Donnell, and R. Servedio. Optimal mean-based algorithms for trace reconstruction. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 17, pages 1047-1056, New York, NY, USA, 2017. ACM.  Geoffrey Grimmett. Percolation, volume 321 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, second edition, 1999. ISBN 3-540-64902-6. URL https://doi.org/10.1007/978-3-662-03981-6.  T. E. Harris. A lower bound for the critical probability in a certain percolation process. Proc.  Cambridge Philos. Soc., 56:13-20, 1960.  Wassily Hoeffding. Probability inequalities for sums of bounded random variables. J. Amer. Statist. Assoc., 58:13-30, 1963. ISSN 0162-1459. URL http://links.jstor.org/sici?sici= 0162-1459(196303)58:301<13:PIFSOB>2.0.CO;2-D&origin=MSN.  Thomas Holenstein, Michael Mitzenmacher, Rina Panigrahy, and Udi Wieder. Trace reconstruction with constant deletion probability and related results. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 389-398. ACM, New York, 2008.  Sampath Kannan and Andrew McGregor. More on reconstructing strings from random traces: insertions and deletions. In Proceedings of the International Symposium on Information Theory (ISIT), pages 297-301. IEEE, 2005. SUBPOLYNOMIAL TRACE RECONSTRUCTION  Vladimir I. Levenshtein. Efficient reconstruction of sequences. IEEE Trans. Inform. Theory, 47(1): 2-22, 2001a. ISSN 0018-9448. doi: 10.1109/18.904499. URL http://dx.doi.org/10. 1109/18.904499.  Vladimir I. Levenshtein. Efficient reconstruction of sequences from their subsequences or su- ISSN 0097-3165. doi:  persequences. 10.1006/jcta.2000.3081. URL http://dx.doi.org/10.1006/jcta.2000.3081.  J. Combin. Theory Ser. A, 93(2):310-332, 2001b.  Andrew McGregor, Eric Price, and Sofya Vorotnikova. Trace reconstruction revisited. In Algorithms\u2014 ESA 2014, volume 8737 of Lecture Notes in Comput. Sci., pages 689-700. Springer, Heidel- berg, 2014. doi: 10.1007/978-3-662-44777-2 57. URL http://dx.doi.org/10.1007/ 978-3-662-44777-2_57.  Michael Mitzenmacher. A survey of results for deletion channels and related synchronization ISSN 1549-5787. doi: 10.1214/08-PS141. URL  channels. Probab. Surv., 6:1-33, 2009. http://dx.doi.org/10.1214/08-PS141.  F. Nazarov and Y. Peres. Trace reconstruction with exp(O(n1/3)) samples. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 17, pages 1042-1046, New York, NY, USA, 2017. ACM.  Y. Peres and A. Zhai. Average-case reconstruction for the deletion channel: subpolynomially many  traces suffice, 2017. To appear in FOCS.  Krishnamurthy Viswanathan and Ram Swaminathan. Improved string reconstruction over insertion- deletion channels. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 399-408. ACM, New York, 2008.  "}, "An explicit analysis of the entropic penalty in linear programming": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "An explicit analysis of the entropic penalty in linear programming", "abstract": "Solving linear programs by using entropic penalization has recently attracted new interest in the optimization community, since this strategy forms the basis for the fastest-known algorithms for the optimal transport problem, with many applications in modern large-scale machine learning.  Crucial to these applications has been an analysis of how quickly solutions to the penalized program approach true optima to the original linear program.  More than 20 years ago, Cominetti and San Mart\u00edn showed that this convergence is exponentially fast; however, their proof is asymptotic and does not give any indication of how accurately the entropic program approximates the original program for any particular choice of the penalization parameter.  We close this long-standing gap in the literature regarding entropic penalization by giving a new proof of the exponential convergence, valid for any linear program.  Our proof is non-asymptotic, yields explicit constants, and has the virtue of being extremely simple.  We provide matching lower bounds and show that the entropic approach does not lead to a near-linear time approximation scheme for the linear assignment problem.", "pdf_url": "http://proceedings.mlr.press/v75/weed18a/weed18a.pdf", "keywords": ["Entropic penalization", "optimal transport", "assignment problem", "Sinkhorn algorithm"], "reference": "David J. Aldous. The \u03b6(2) limit in the random assignment problem. Random Structures Algorithms, 18(4):381-418, 2001. ISSN 1042-9832. URL https://doi.org/10.1002/rsa.1015.  Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms In Chris Umans, editor, 58th IEEE Annual Symposium on Foundations for matrix scaling. of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 890-901. IEEE Computer Society, 2017. ISBN 978-1-5386-3464-6. doi: 10.1109/FOCS.2017.87. URL https://doi.org/10.1109/FOCS.2017.87.  Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algo- rithms for optimal transport via sinkhorn iteration. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 1961- 1971, 2017. URL http://papers.nips.cc/paper/6792-near-linear-time- approximation-algorithms-for-optimal-transport-via-sinkhorn- iteration.  12   THE ENTROPIC PENALTY IN LINEAR PROGRAMMING  Proof [Proof of Lemma 8] By definition h(\u03c1) = \u03c1 log 1 that  \u03c1 + (1 \u2212 \u03c1) log 1  1\u2212\u03c1 , so it suffices to show  (1 \u2212 \u03c1) \u03c1  log  1 1 \u2212 \u03c1  \u2264 1  \u2200\u03c1 \u2208 [0, 1] .  (1\u2212\u03c1) \u03c1  This inequality is easily verified by noting that the derivative of the left side is nonpositive on (0, 1) and lim\u03c1\u21920+ Proof [Proof of Lemma 15] It is trivial to see that all X \u2208 B satisfy (cid:80) any X \u2208 B,  ij Xij = n, so R1 = n. For  1\u2212\u03c1 = 1.  log 1  H(X) =  Xij log  1 Xij  (cid:88)  ij (cid:88)  i (cid:88)  i  =  =  (cid:88)  j  Xij log  1 Xij  H(Xi) ,  where Xi denotes the ith row of X. Since each row of X is a nonnegative vector of dimension n whose entries sum to 1, for each 1 \u2264 i \u2264 n the bound 0 \u2264 H(Xi) \u2264 log n holds. Therefore 0 \u2264 H(X) \u2264 n log n for all X \u2208 B, which proves that RH \u2264 n log n.  This work was supported in part by NSF Graduate Research Fellowship DGE-1122374. The author would like to thank J. Altschuler and P. Rigollet for useful discussions, as well as the anonymous referees for their suggestions.  Acknowledgments  References  David J. Aldous. The \u03b6(2) limit in the random assignment problem. Random Structures Algorithms, 18(4):381-418, 2001. ISSN 1042-9832. URL https://doi.org/10.1002/rsa.1015.  Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms In Chris Umans, editor, 58th IEEE Annual Symposium on Foundations for matrix scaling. of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 890-901. IEEE Computer Society, 2017. ISBN 978-1-5386-3464-6. doi: 10.1109/FOCS.2017.87. URL https://doi.org/10.1109/FOCS.2017.87.  Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algo- rithms for optimal transport via sinkhorn iteration. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 1961- 1971, 2017. URL http://papers.nips.cc/paper/6792-near-linear-time- approximation-algorithms-for-optimal-transport-via-sinkhorn- iteration. THE ENTROPIC PENALTY IN LINEAR PROGRAMMING  Jean-Yves Audibert, S\u00e9bastien Bubeck, and G\u00e1bor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00e9.  Itera- tive bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2):A1111-A1138, 2015.  Jean-David Benamou, Guillaume Carlier, and Luca Nenna. A numerical method to solve multi- marginal optimal transport problems with Coulomb cost. In Splitting Methods in Communication, Imaging, Science, and Engineering, Sci. Comput., pages 577-601. Springer, Cham, 2016.  Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport. arXiv  preprint arXiv:1710.06276, 2017.  bridge, 2004. ISBN 0-521-83378-7.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, Cam-  L. M. Br\u00e8gman. A relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming. \u017d. Vy\u02c7cisl. Mat. i Mat. Fiz., 7:620-631, 1967. ISSN 0044-4669.  Richard A. Brualdi. Combinatorial Matrix Classes, volume 108 of Encyclopedia of Mathematics and its Applications. Cambridge University Press, Cambridge, 2006. ISBN 978-0-521-86565- 4; 0-521-86565-4. doi: 10.1017/CBO9780511721182. URL http://dx.doi.org/10. 1017/CBO9780511721182.  S\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231-357, 2015. doi: 10.1561/2200000050. URL https://doi. org/10.1561/2200000050.  Guillaume Carlier, Vincent Duval, Gabriel Peyr\u00e9, and Bernhard Schmitzer. Convergence of entropic schemes for optimal transport and gradient \ufb02ows. SIAM Journal on Mathematical Analysis, 49 (2):1385-1418, 2017.  Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, Learning, and Games. Cambridge Uni- ISBN 978-0-521-84108-5; 0-521-84108-9. doi: 10.1017/  versity Press, Cambridge, 2006. CBO9780511546921. URL http://dx.doi.org/10.1017/CBO9780511546921.  St\u00e9phane Chr\u00e9tien and Alfred O. Hero, III. Kullback proximal algorithms for maximum-likelihood estimation. IEEE Trans. Inform. Theory, 46(5):1800-1810, 2000. ISSN 0018-9448. doi: 10. 1109/18.857792. URL http://dx.doi.org/10.1109/18.857792.  Michael B. Cohen, Aleksander M \u02dbadry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and bal- ancing via box constrained newton\u2019s method and interior point methods. In Chris Umans, editor, 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 902-913. IEEE Computer Society, 2017. ISBN 978-1-5386- 3464-6. doi: 10.1109/FOCS.2017.88. URL https://doi.org/10.1109/FOCS.2017. 88. THE ENTROPIC PENALTY IN LINEAR PROGRAMMING  R. Cominetti and J. San Mart\u00edn. Asymptotic analysis of the exponential penalty trajectory in linear programming. Math. Programming, 67(2, Ser. A):169-187, 1994. ISSN 0025-5610. doi: 10. 1007/BF01582220. URL http://dx.doi.org/10.1007/BF01582220.  Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Pro- cessing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 2292-2300, 2013. URL http://papers.nips.cc/paper/4927- sinkhorn-distances-lightspeed-computation-of-optimal-transport.  Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pages 685-693. JMLR.org, 2014. URL http://jmlr.org/proceedings/papers/v32/cuturi14.html.  Andreea Denitiu, Stefania Petra, Claudius Schn\u00f6rr, and Christoph Schn\u00f6rr. An entropic perturbation approach to TV-minimization for limited-data tomography. In Elena Barcucci, Andrea Frosini, and Simone Rinaldi, editors, Discrete Geometry for Computer Imagery - 18th IAPR International Conference, DGCI 2014, Siena, Italy, September 10-12, 2014. Proceedings, volume 8668 of Lecture Notes in Computer Science, pages 262-274. Springer, 2014. ISBN 978-3-319-09954-5. doi: 10.1007/978-3-319-09955-2_22. URL https://doi.org/10.1007/978-3-319- 09955-2_22.  Arnaud Dessein, Nicolas Papadakis, and Jean-Luc Rouas. Regularized optimal transport and the  ROT mover\u2019s distance. arXiv preprint arXiv:1610.06447, 2016.  Simone Di Marino, Augusto Gerolin, and Luca Nenna. Optimal transportation theory with repulsive costs. In Topological optimization and optimal transport, volume 17 of Radon Ser. Comput. Appl. Math., pages 204-256. De Gruyter, Berlin, 2017.  Juan D\u00edaz, Tom\u00e1s Rau, and Jorge Rivera. A matching estimator based on a bilevel optimization  problem. Review of Economics and Statistics, 97(4):803-812, 2015.  S. C. Fang. An unconstrained convex programming view of linear programming. Z. Oper. Res., 36 (2):149-161, 1992. ISSN 0340-9422. URL https://doi.org/10.1007/BF01417214.  S.-C. Fang, J. R. Rajasekera, and H.-S. J. Tsao. Entropy optimization and mathematical pro- gramming, volume 8 of International Series in Operations Research & Management Science. ISBN 0-7923-9939-0. URL https: Kluwer Academic Publishers, Boston, MA, 1997. //doi.org/10.1007/978-1-4615-6131-6.  Shu-Cherng Fang and H-S Jacob Tsao. Linear programming with entropic perturbation. Zeitschrift  f\u00fcr Operations Research, 37(2):171-186, 1993.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997. THE ENTROPIC PENALTY IN LINEAR PROGRAMMING  Aude Genevay, Marco Cuturi, Gabriel Peyr\u00e9, and Francis R. Bach. Stochastic optimization for large-scale optimal transport. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Is- abelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3432-3440, 2016. URL http://papers.nips.cc/paper/ 6566-stochastic-optimization-for-large-scale-optimal-transport.  David P Helmbold and Manfred K Warmuth. Learning permutations with exponential weights.  Journal of Machine Learning Research, 10(Jul):1705-1736, 2009.  Edwin T Jaynes. On the rationale of maximum-entropy methods. Proceedings of the IEEE, 70(9):  939-952, 1982.  Anatoli Juditsky, Philippe Rigollet, and Alexandre Tsybakov. Learning by mirror averaging. Ann.  Statist., 36(5):2183-2206, 2008. ISSN 0090-5364.  Wouter M. Koolen, Manfred K. Warmuth, and Jyrki Kivinen. Hedging structured concepts. In Adam Tauman Kalai and Mehryar Mohri, editors, COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 93-105. Omnipress, 2010. ISBN 978-0-9822529-2-5. URL http://colt2010.haifa.il.ibm.com/papers/ COLT2010proceedings.pdf#page=101.  JJ Kosowsky and Alan L Yuille. The invisible hand algorithm: Solving the assignment problem  with statistical physics. Neural Networks, 7(3):477-490, 1994.  Christian L\u00e9onard. A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport. Discrete Contin. Dyn. Syst., 34(4):1533-1574, 2014. ISSN 1078-0947. doi: 10.3934/ dcds.2014.34.1533. URL http://dx.doi.org/10.3934/dcds.2014.34.1533.  Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential prop- arXiv preprint  erties of Sinkhorn approximation for learning with Wasserstein distance. arXiv:1805.11897, 2018.  Marc M\u00e9zard and Andrea Montanari.  Information, physics, and computation. Oxford Grad- uate Texts. Oxford University Press, Oxford, 2009. doi: 10. 1093/acprof:oso/9780198570837.001.0001. URL https://doi.org/10.1093/acprof: oso/9780198570837.001.0001.  ISBN 978-0-19-857083-7.  Aleksander M \u02dbadry. Navigating central path with electrical \ufb02ows: From \ufb02ows to matchings, and back. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA, pages 253-262. IEEE Computer Society, 2013. ISBN 978- 0-7695-5135-7. doi: 10.1109/FOCS.2013.35. URL https://doi.org/10.1109/FOCS. 2013.35.  Gabriel Peyr\u00e9 and Marco Cuturi. Computational optimal transport. Book draft, 2017.  Philippe Rigollet. Personal communication, 2017. THE ENTROPIC PENALTY IN LINEAR PROGRAMMING  Philippe Rigollet and Alexandre Tsybakov. Exponential screening and optimal rates of sparse esti- mation. Ann. Statist., 39(2):731-771, 2011. ISSN 0090-5364. doi: 10.1214/10-AOS854. URL http://dx.doi.org/10.1214/10-AOS854.  Bernhard Schmitzer. Stabilized sparse scaling algorithms for entropy regularized transport prob-  lems. arXiv preprint arXiv:1610.06519, 2016.  Alexander Schrijver. Combinatorial optimization. Polyhedra and efficiency. Vol. A, volume 24 of  Algorithms and Combinatorics. Springer-Verlag, Berlin, 2003. ISBN 3-540-44389-4.  Erwin Schr\u00f6dinger. \u00dcber die Umkehrung der Naturgesetze. Angewandte Chemie, 44(30):636-636,  1931.  Meisam Sharify, St\u00e9phane Gaubert, and Laura Grigori. Solution of the optimal assignment problem  by diagonal scaling algorithms. arXiv preprint arXiv:1104.3830, 2011.  Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The  American Mathematical Monthly, 74(4):402-405, 1967.  Justin Solomon, Fernando De Goes, Gabriel Peyr\u00e9, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transporta- tion on geometric domains. ACM Transactions on Graphics (TOG), 34(4):66, 2015. "}, "Efficient active learning of sparse halfspaces": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Efficient active learning of sparse halfspaces", "abstract": "We study the problem of efficient PAC active learning of homogeneous linear classifiers (halfspaces) in $\\mathbb{R}^d$, where the goal is to learn a halfspace with low error using as few label queries as possible. Under the extra assumption that there is a $t$-sparse halfspace that performs well on the data ($t \\ll d$), we would like our active learning algorithm to be {\\em attribute efficient}, i.e. to have label requirements sublinear in $d$. In this paper, we provide a computationally efficient algorithm that achieves this goal. Under certain distributional assumptions on the data, our algorithm achieves a label complexity of $O(t \\cdot \\mathrm{polylog}(d, \\frac 1 \\epsilon))$. In contrast, existing algorithms in this setting are either computationally inefficient, or subject to label requirements polynomial in $d$ or $\\frac 1 \\epsilon$.", "pdf_url": "http://proceedings.mlr.press/v75/zhang18b/zhang18b.pdf", "keywords": [], "reference": "Jayadev Acharya, Arnab Bhattacharyya, and Pritish Kamath. Improved bounds for universal one- bit compressive sensing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages 2353-2357. IEEE, 2017.  Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. One-bit compressed sensing with  non-gaussian measurements. Linear Algebra and its Applications, 441:222-239, 2014.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear separators under bounded noise. In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, volume 40 of JMLR Proceedings, pages 167-190. JMLR.org, 2015.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning and 1-bit compressed sensing under asymmetric noise. In Proceedings of The 28th Conference on Learning Theory, COLT 2016, 2016.  Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently  learning linear separators with noise. Journal of the ACM (JACM), 63(6):50, 2017.  11   EFFICIENT ACTIVE LEARNING OF SPARSE HALFSPACES  \u2022 Can we extend our algorithm to work under \u03b7-bounded noise, when \u03b7 is arbitrarily close to 1 2 ? Recall that the results of Zhang and Chaudhuri (2014) imply a computationally inefficient (1\u22122\u03b7)2 ln 1 algorithm with a label complexity of O( t ln d (cid:15) ) in this setting, which state of the art computationally efficient algorithms (e.g. Awasthi et al., 2016) cannot achieve.  \u2022 Can we design attribute and computationally efficient active learning algorithms that work under broader distributions? Existing results in the active learning and one-bit compressed sensing literature have made substantial progress on settings when the unlabeled distribu- tion is \u03b1-stable (Li, 2016), subgaussian (Ai et al., 2014; Chen and Banerjee, 2015), or s- concave (Balcan and Zhang, 2017); an attribute and computationally efficient, statistically consistent recovery algorithm under any of the above settings would be a step forward.  \u2022 In one-bit compressed sensing, under the symmetric noise condition (Plan and Vershynin, 2013b), algorithms with sample complexity polynomial in 1 (cid:15) have been proposed (Plan and Vershynin, 2013b; Zhang et al., 2014; Zhu and Gu, 2015). Can we develop adaptive one- bit compressed sensing algorithms with O(t polylog(d, 1 (cid:15) )) measurement complexity in this setting?  Acknowledgments  I am grateful to Daniel Hsu for suggesting this research direction to me, and many insightful discus- sions along this line. I would also like to thank Pranjal Awasthi, Jie Shen and Hongyang Zhang for helpful initial conversations about the results in this paper. I thank the anonymous COLT review- ers for their thoughtful comments. Special thanks to Yue Liu, who provided unconditional support throughout this research project.  References  Jayadev Acharya, Arnab Bhattacharyya, and Pritish Kamath. Improved bounds for universal one- bit compressive sensing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages 2353-2357. IEEE, 2017.  Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. One-bit compressed sensing with  non-gaussian measurements. Linear Algebra and its Applications, 441:222-239, 2014.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear separators under bounded noise. In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, volume 40 of JMLR Proceedings, pages 167-190. JMLR.org, 2015.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning and 1-bit compressed sensing under asymmetric noise. In Proceedings of The 28th Conference on Learning Theory, COLT 2016, 2016.  Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently  learning linear separators with noise. Journal of the ACM (JACM), 63(6):50, 2017. EFFICIENT ACTIVE LEARNING OF SPARSE HALFSPACES  M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In COLT, 2013.  M.-F. Balcan, A. Z. Broder, and T. Zhang. Margin based active learning. In COLT, 2007.  M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. J. Comput. Syst. Sci., 75  (1):78-89, 2009.  Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning al- gorithms under s-concave distributions. In Advances in Neural Information Processing Systems, pages 4799-4808, 2017.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Avrim Blum. Learning boolean functions in an infinite attribute space.  In Proceedings of the  twenty-second annual ACM symposium on Theory of computing, pages 64-72. ACM, 1990.  Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Ap-  plied and computational harmonic analysis, 27(3):265-274, 2009.  Petros T Boufounos and Richard G Baraniuk. 1-bit compressive sensing. In Information Sciences  and Systems, 2008. CISS 2008. 42nd Annual Conference on, pages 16-21. IEEE, 2008.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: IEEE transactions on information theory, 52(12):5406-5425,  Universal encoding strategies? 2006.  Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pages 693-703. Springer, 2002.  Lin Chen, Seyed Hamed Hassani, and Amin Karbasi. Near-optimal active learning of halfspaces  via query synthesis in the noisy setting. In AAAI, 2017.  Sheng Chen and Arindam Banerjee. One-bit compressed sensing with the k-support norm.  In  Artificial Intelligence and Statistics, pages 138-146, 2015.  David A. Cohn, Les E. Atlas, and Richard E. Ladner. Improving generalization with active learning.  Machine Learning, 15(2):201-221, 1994.  S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.  Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767-1781,  2011.  Sanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-based ac- tive learning. In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005, Proceedings, pages 249-263, 2005. EFFICIENT ACTIVE LEARNING OF SPARSE HALFSPACES  David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-  1306, 2006.  Vitaly Feldman. Attribute-efficient and non-adaptive learning of parities and dnf expressions. Jour-  nal of Machine Learning Research, 8(Jul):1431-1460, 2007.  Rahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 337-344. ACM, 2009.  Sivakant Gopi, Praneeth Netrapalli, Prateek Jain, and Aditya Nori. One-bit compressed sensing: Provable support and vector recovery. In International Conference on Machine Learning, pages 154-162, 2013.  Ankit Gupta, Robert Nowak, and Benjamin Recht. Sample complexity for 1-bit compressed sensing and sparse classification. In Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on, pages 1553-1557. IEEE, 2010.  S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.  Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends R(cid:13) in Ma-  chine Learning, 7(2-3):131-309, 2014.  Steve Hanneke, Varun Kanade, and Liu Yang. Learning with a drifting target concept. In Interna-  tional Conference on Algorithmic Learning Theory, pages 149-164. Springer, 2015.  Jarvis Haupt and Richard Baraniuk. Robust support recovery using sparse compressive sensing matrices. In Information Sciences and Systems (CISS), 2011 45th Annual Conference on, pages 1-6. IEEE, 2011.  Laurent Jacques, Jason N Laska, Petros T Boufounos, and Richard G Baraniuk. Robust 1-bit com- pressive sensing via binary stable embeddings of sparse vectors. IEEE Transactions on Informa- tion Theory, 59(4):2082-2102, 2013.  Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793-800, 2009.  Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994.  Adam R Klivans and Rocco A Servedio. Toward attribute efficient learning of decision lists and  parities. Journal of Machine Learning Research, 7(Apr):587-602, 2006.  Sanjeev R Kulkarni, Sanjoy K Mitter, and John N Tsitsiklis. Active learning using arbitrary binary  valued queries. Machine Learning, 11(1):23-35, 1993.  Ping Li. One scan 1-bit compressed sensing. In Artificial Intelligence and Statistics, pages 1515-  1523, 2016. EFFICIENT ACTIVE LEARNING OF SPARSE HALFSPACES  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-  rithm. Machine Learning, 2(4):285-318, 1987.  Philip M Long and Rocco Servedio. Attribute-efficient learning of decision lists and linear threshold In Advances in Neural Information Processing  functions under unconcentrated distributions. Systems, pages 921-928, 2007.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algo-  rithms. Random Structures & Algorithms, 30(3):307-358, 2007.  Pascal Massart and \u00b4Elodie N\u00b4ed\u00b4elec. Risk bounds for statistical learning. The Annals of Statistics,  pages 2326-2366, 2006.  ing, 24(2):227-234, 1995.  Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on comput-  Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming. Communi-  cations on Pure and Applied Mathematics, 66(8):1275-1297, 2013a.  Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, 2013b.  Rocco Servedio, Li-Yang Tan, and Justin Thaler. Attribute-efficient learning andweight-degree tradeoffs for polynomial threshold functions. In Conference on Learning Theory, pages 14-1, 2012.  Rocco A Servedio. Computational sample complexity and attribute-efficient learning. Journal of  Computer and System Sciences, 60(1):161-178, 2000.  Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 52(55-66):11,  2010.  Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver- gence results and optimal averaging schemes. In International Conference on Machine Learning, pages 71-79, 2013.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. In Advances in Neural Information Processing Systems, pages 1056-1066, 2017.  Chicheng Zhang and Kamalika Chaudhuri. Beyond disagreement-based agnostic active learning. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Informa- tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 442-450, 2014.  Lijun Zhang, Jinfeng Yi, and Rong Jin. Efficient algorithms for robust one-bit compressive sensing.  In International Conference on Machine Learning, pages 820-828, 2014.  Rongda Zhu and Quanquan Gu. Towards a lower sample complexity for robust one-bit compressed  sensing. In International Conference on Machine Learning, pages 739-747, 2015. EFFICIENT ACTIVE LEARNING OF SPARSE HALFSPACES  "}, "Marginal Singularity, and the Benefits of Labels in Covariate-Shift": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Marginal Singularity, and the Benefits of Labels in Covariate-Shift", "abstract": "We present new minimax results that concisely capture the relative benefits of source and target labeled data, under {covariate-shift}. Namely, we show that, in general classification settings, the benefits of target labels are controlled by a \\emph{transfer-exponent} $\\gamma$ that encodes how \\emph{singular} $Q$ is locally w.r.t. $P$, and interestingly allows situations where transfer did not seem possible under previous insights. In fact, our new minimax analysis \u2013 in terms of $\\gamma$ \u2013 reveals a \\emph{continuum of regimes} ranging from situations where target labels have little benefit, to regimes where target labels dramatically improve classification.  We then show that a recently proposed semi-supervised procedure can be extended to adapt to unknown $\\gamma$, and therefore requests target labels only when beneficial, while achieving nearly minimax transfer rates.", "pdf_url": "http://proceedings.mlr.press/v75/kpotufe18a/kpotufe18a.pdf", "keywords": ["Transfer learning", "covariate-shift", "nonparametric classification", "nearest-neighbors"], "reference": "Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of Statistics, 35(2):608-633, 2007.  Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target samples. In International Conference on Algorithmic Learning Theory, pages 139-153. Springer, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort- man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010a.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain adap- tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129-136, 2010b.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In Inter-  national Conference on Machine Learning, pages 1870-1879, 2015.  Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye.  Joint transfer and batch-mode active learning. In Sanjoy Dasgupta and David McAllester, editors, Pro- ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 253-261, 2013.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation.  In  Advances in neural information processing systems, pages 2456-2464, 2011.  Corinna Cortes, Mehryar Mohri, and Andr\u00b4es Munoz Medina. Adaptation based on general- ized discrepancy. Machine Learning Research, forthcoming. URL http://www. cs. nyu. edu/\u02dc mohri/pub/daj. pdf.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions. arXiv preprint arXiv:1411.0894, 2014.  Pascal Germain, Amaury Habrard, Franc\u00b8ois Laviolette, and Emilie Morvant. A pac-bayesian ap- proach for domain adaptation with specialization to linear classifiers. In International Conference on Machine Learning, pages 738-746, 2013.  Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Sch\u00a8olkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pages 601-608, 2007.  Samory Kpotufe. Lipschitz density-ratios, structured data, and data-driven tuning.  In Artificial  Intelligence and Statistics, pages 1320-1328, 2017.  4   MINIMAX TRANSFER  References  Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The  Annals of Statistics, 35(2):608-633, 2007.  Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target samples. In International Conference on Algorithmic Learning Theory, pages 139-153. Springer, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort- man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010a.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain adap- tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129-136, 2010b.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In Inter-  national Conference on Machine Learning, pages 1870-1879, 2015.  Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye.  Joint transfer and batch-mode active learning. In Sanjoy Dasgupta and David McAllester, editors, Pro- ceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 253-261, 2013.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.  In Advances in Neural Information Processing Systems, pages 3437-3445, 2014.  Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation.  In  Advances in neural information processing systems, pages 2456-2464, 2011.  Corinna Cortes, Mehryar Mohri, and Andr\u00b4es Munoz Medina. Adaptation based on general- ized discrepancy. Machine Learning Research, forthcoming. URL http://www. cs. nyu. edu/\u02dc mohri/pub/daj. pdf.  S\u00b4ebastien Gadat, Thierry Klein, and Cl\u00b4ement Marteau. Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions. arXiv preprint arXiv:1411.0894, 2014.  Pascal Germain, Amaury Habrard, Franc\u00b8ois Laviolette, and Emilie Morvant. A pac-bayesian ap- proach for domain adaptation with specialization to linear classifiers. In International Conference on Machine Learning, pages 738-746, 2013.  Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Sch\u00a8olkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pages 601-608, 2007.  Samory Kpotufe. Lipschitz density-ratios, structured data, and data-driven tuning.  In Artificial  Intelligence and Statistics, pages 1320-1328, 2017. MINIMAX TRANSFER  Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the benefits of labels in  covariate-shift. arXiv preprint arXiv:1803.01833, 2018.  Oleg V Lepski and VG Spokoiny. Optimal pointwise adaptive methods in nonparametric estimation.  The Annals of Statistics, pages 2512-2546, 1997.  Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds  and algorithms. arXiv preprint arXiv:0902.3430, 2009a.  Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial  r\u00b4enyi divergence. Intelligence, pages 367-374. AUAI Press, 2009b.  Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with drift- ing distributions. In International Conference on Algorithmic Learning Theory, pages 124-138. Springer, 2012.  Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence.  Dataset shift in machine learning. The MIT Press, 2009.  Avishek Saha, Piyush Rai, Hal Daum\u00b4e, Suresh Venkatasubramanian, and Scott L DuVall. Active su- pervised domain adaptation. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 97-112. Springer, 2011.  Richard J Samworth et al. Optimal weighted nearest neighbour classifiers. The Annals of Statistics,  40(5):2733-2763, 2012.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adapta- tion. In Advances in neural information processing systems, pages 1433-1440, 2008.  Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine  learning. Cambridge University Press, 2012.  Liu Yang, Steve Hanneke, and Jaime Carbonell. A theory of transfer learning with applications to  active learning. Machine learning, 90(2):161-189, 2013. "}, "Learning Single-Index Models in Gaussian Space": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Learning Single-Index Models in Gaussian Space", "abstract": "We consider regression problems where the response is a smooth but non-linear function of a $k$-dimensional projection of $p$ normally-distributed covariates, contaminated with additive Gaussian noise. The goal is to recover the range of the $k$-dimensional projection, i.e., the index space. This model is called the multi-index model, and the $k=1$ case is called the single-index model. For the single-index model, we characterize the population landscape of a natural semi-parametric maximum likelihood objective in terms of the link function and prove that it has no spurious local minima. We also propose and analyze an efficient iterative procedure that recovers the index space up to error $\\epsilon$ using a sample size $\\tilde{O}(p^{O(R^2/\\mu)} + p/\\epsilon^2)$, where $R$ and $\\mu$, respectively, parameterize the smoothness of the link function and the signal strength. When a multi-index model is incorrectly specified as a single-index model, we prove that essentially the same procedure, with sample size $\\tilde{O}(p^{O(kR^2/\\mu)} + p/\\epsilon^2)$, returns a vector that is $\\epsilon$-close to being completely in the index space.", "pdf_url": "http://proceedings.mlr.press/v75/dudeja18a/dudeja18a.pdf", "keywords": ["Single-index models", "multi-index models", "non-convex optimization", "semi-parametric models"], "reference": "Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. One-bit compressed sensing with  non-gaussian measurements. Linear Algebra and its Applications, 441:222-239, 2014.  Dmitry Babichev and Francis Bach. Slice inverse regression with score functions. 2016.  David R Brillinger. A generalized linear model with \u201cGaussian\u201d regressor variables. In Selected  Works of David Brillinger, pages 589-606. Springer, 2012.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.  A Carbery and J Wright. Distributional and Lq norm inequalities for polynomials over convex  bodies in Rn. Mathematical research letters, 8(3):233-248, 2001.  Arnak S Dalalyan, Anatoly Juditsky, and Vladimir Spokoiny. A new algorithm for estimating the effective dimension-reduction subspace. Journal of Machine Learning Research, 9(Aug):1647- 1678, 2008.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points: online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  Joel L Horowitz. Springer, 2009.  Semiparametric and nonparametric methods in econometrics, volume 12.  Marian Hristache, Anatoli Juditsky, J\u00a8org Polzehl, and Vladimir Spokoiny. Structure adaptive ap-  proach for dimension reduction. The Annals of Statistics, 29(6):1537-1566, 2001a.  Marian Hristache, Anatoli Juditsky, and Vladimir Spokoiny. Direct estimation of the index coeffi-  cient in a single-index model. Annals of Statistics, pages 595-623, 2001b.  Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical  Association, 86(414):316-327, 1991.  Ker-Chau Li. On principal Hessian directions for data visualization and dimension reduction: An- other application of Stein\u2019s lemma. Journal of the American Statistical Association, 87(420): 1025-1039, 1992.  Matey Neykov, Zhaoran Wang, and Han Liu. Agnostic estimation for misspecified phase retrieval  models. In Advances in Neural Information Processing Systems, pages 4089-4097, 2016.  Ryan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.  Yaniv Plan, Roman Vershynin, and Elena Yudovina. High-dimensional estimation with geometric  constraints. Information and Inference: A Journal of the IMA, 6(1):1-40, 2017.  Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint  arXiv:1510.06096, 2015.  13   LEARNING SINGLE-INDEX MODELS  References  Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. One-bit compressed sensing with  non-gaussian measurements. Linear Algebra and its Applications, 441:222-239, 2014.  Dmitry Babichev and Francis Bach. Slice inverse regression with score functions. 2016.  David R Brillinger. A generalized linear model with \u201cGaussian\u201d regressor variables. In Selected  Works of David Brillinger, pages 589-606. Springer, 2012.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.  A Carbery and J Wright. Distributional and Lq norm inequalities for polynomials over convex  bodies in Rn. Mathematical research letters, 8(3):233-248, 2001.  Arnak S Dalalyan, Anatoly Juditsky, and Vladimir Spokoiny. A new algorithm for estimating the effective dimension-reduction subspace. Journal of Machine Learning Research, 9(Aug):1647- 1678, 2008.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points: online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape  design. arXiv preprint arXiv:1711.00501, 2017.  Joel L Horowitz. Springer, 2009.  Semiparametric and nonparametric methods in econometrics, volume 12.  Marian Hristache, Anatoli Juditsky, J\u00a8org Polzehl, and Vladimir Spokoiny. Structure adaptive ap-  proach for dimension reduction. The Annals of Statistics, 29(6):1537-1566, 2001a.  Marian Hristache, Anatoli Juditsky, and Vladimir Spokoiny. Direct estimation of the index coeffi-  cient in a single-index model. Annals of Statistics, pages 595-623, 2001b.  Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical  Association, 86(414):316-327, 1991.  Ker-Chau Li. On principal Hessian directions for data visualization and dimension reduction: An- other application of Stein\u2019s lemma. Journal of the American Statistical Association, 87(420): 1025-1039, 1992.  Matey Neykov, Zhaoran Wang, and Han Liu. Agnostic estimation for misspecified phase retrieval  models. In Advances in Neural Information Processing Systems, pages 4089-4097, 2016.  Ryan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.  Yaniv Plan, Roman Vershynin, and Elena Yudovina. High-dimensional estimation with geometric  constraints. Information and Inference: A Journal of the IMA, 6(1):1-40, 2017.  Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint  arXiv:1510.06096, 2015. LEARNING SINGLE-INDEX MODELS  Martin Wainwright. Basic tail and concentration bounds, 2015. URL https://www.stat. berkeley.edu/\u02dcmjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf.  "}, "Hidden Integrality of SDP Relaxations for Sub-Gaussian Mixture Models": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Hidden Integrality of SDP Relaxations for Sub-Gaussian Mixture Models", "abstract": "We consider the problem of finding discrete clustering structures under Sub-Gaussian Mixture Models. We establish a hidden integrality property of a semidefinite programming (SDP) relaxation for this problem: while the optimal solutions to the SDP are not integer-valued in general, their estimation errors can be upper bounded by the error of an idealized integer program. The error of the integer program, and hence that of the SDP, are further shown to decay exponentially in the signal-to-noise ratio. To the best of our knowledge, this is the first exponentially decaying error bound for convex relaxations of mixture models. A special case of this result shows that in certain regimes the SDP solutions are in fact integral and exact, improving on existing exact recovery results for convex relaxations. More generally, our result establishes sufficient conditions for the SDP to correctly recover the cluster memberships of at least $(1-\\delta)$ fraction of the points for any $\\delta\\in(0,1)$. Error bounds for estimating cluster centers can also be derived directly from our results.", "pdf_url": "http://proceedings.mlr.press/v75/fei18a/fei18a.pdf", "keywords": ["Sub-Gaussian Mixture Models", "semidefinite programming", "integer programming"], "reference": "Emmanuel Abbe. Community detection and the stochastic block model: recent developments. Journal of Machine Learning Research, to appear, 2017. URL http://www.princeton. edu/\u02dceabbe/publications/sbm_jmlr_4.pdf.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  International Conference on Computational Learning Theory, pages 458-469. Springer, 2005.  Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of euclidean sum-  of-squares clustering. Machine Learning, 75(2):245-248, 2009.  Brendan P. W. Ames and Stephen A. Vavasis. Convex optimization for the planted k-disjoint-clique  problem. Mathematical Programming, 143(1-2):299-337, 2014.  Arash A. Amini and Elizaveta Levina. On semidefinite relaxations for the block model. arXiv  preprint arXiv:1406.5647, 2014.  arXiv:1206.3204, 2012.  Pranjal Awasthi and Or Sheffet.  Improved spectral-norm bounds for clustering. arXiv preprint  Pranjal Awasthi and Aravindan Vijayaraghavan. Clustering semi-random mixtures of gaussians.  arXiv preprint arXiv:1711.08841, 2017.  Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, and Rachel Ward. Relax, no need to round: Integrality of clustering formulations. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, pages 191-200. ACM, 2015.  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algo- rithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Moses Charikar, Sudipto Guha, \u00b4Eva Tardos, and David B. Shmoys. A constant-factor approximation In Proceedings of the 31st Annual ACM Symposium on  algorithm for the k-median problem. Theory of Computing, pages 1-10. ACM, 1999.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transactions on  Information Theory, 60(10):6440-6455, 2014.  Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Foundations of  Computer Science, pages 634-644. IEEE, 1999.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for  mixtures of two gaussians. arXiv preprint arXiv:1609.00368, 2016.  13   HIDDEN INTEGRALITY OF SDP RELAXATIONS FOR SUB-GAUSSIAN MIXTURE MODELS  Y. Fei and Y. Chen were partially supported by the National Science Foundation CRII award 1657420 and grant 1704828.  Acknowledgments  References  Emmanuel Abbe. Community detection and the stochastic block model: recent developments. Journal of Machine Learning Research, to appear, 2017. URL http://www.princeton. edu/\u02dceabbe/publications/sbm_jmlr_4.pdf.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  International Conference on Computational Learning Theory, pages 458-469. Springer, 2005.  Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of euclidean sum-  of-squares clustering. Machine Learning, 75(2):245-248, 2009.  Brendan P. W. Ames and Stephen A. Vavasis. Convex optimization for the planted k-disjoint-clique  problem. Mathematical Programming, 143(1-2):299-337, 2014.  Arash A. Amini and Elizaveta Levina. On semidefinite relaxations for the block model. arXiv  preprint arXiv:1406.5647, 2014.  arXiv:1206.3204, 2012.  Pranjal Awasthi and Or Sheffet.  Improved spectral-norm bounds for clustering. arXiv preprint  Pranjal Awasthi and Aravindan Vijayaraghavan. Clustering semi-random mixtures of gaussians.  arXiv preprint arXiv:1711.08841, 2017.  Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, and Rachel Ward. Relax, no need to round: Integrality of clustering formulations. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science, pages 191-200. ACM, 2015.  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algo- rithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Moses Charikar, Sudipto Guha, \u00b4Eva Tardos, and David B. Shmoys. A constant-factor approximation In Proceedings of the 31st Annual ACM Symposium on  algorithm for the k-median problem. Theory of Computing, pages 1-10. ACM, 1999.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transactions on  Information Theory, 60(10):6440-6455, 2014.  Sanjoy Dasgupta. Learning mixtures of gaussians. In 40th Annual Symposium on Foundations of  Computer Science, pages 634-644. IEEE, 1999.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for  mixtures of two gaussians. arXiv preprint arXiv:1609.00368, 2016. HIDDEN INTEGRALITY OF SDP RELAXATIONS FOR SUB-GAUSSIAN MIXTURE MODELS  Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1-38, 1977.  Yingjie Fei and Yudong Chen. Exponential error rates of SDP for block models: Beyond  Grothendieck\u2019s inequality. arXiv preprint arXiv:1705.08391, 2017.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via Grothendieck\u2019s  inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016.  Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, and Soledad Villar. Probably certifiably correct  k-means clustering. Mathematical Programming, 165(2):605-642, 2017.  Kamal Jain, Mohammad Mahdian, and Amin Saberi. A new greedy approach for facility location problems. In Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing, pages 731-740. ACM, 2002.  Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright, and Michael I. Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances in Neural Information Processing Systems, pages 4116-4124, 2016.  Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and Angela Y. Wu. A local search approximation algorithm for k-means clustering. Computational Geometry, 28(2-3):89-112, 2004.  Jason M. Klusowski and W. D. Brinda. Statistical guarantees for estimating the centers of a two-  component gaussian mixture by em. arXiv preprint arXiv:1608.02280, 2016.  Michael Krivelevich and Dan Vilenchik. Semirandom models as benchmarks for coloring algo- rithms. In Third Workshop on Analytic Algorithmics and Combinatorics (ANALCO), pages 211- 221, 2006.  Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 299-308. IEEE Computer Society, 2010.  Shi Li and Ola Svensson. Approximating k-median via pseudo-approximation. SIAM Journal on  Computing, 45(2):530-547, 2016.  Xiaodong Li, Yang Li, Shuyang Ling, Thomas Strohmer, and Ke Wei. When do birds of a feather \ufb02ock together? K-means, proximity, and conic programming. arXiv preprint arXiv:1710.06008, 2017.  Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28  (2):129-137, 1982.  Yu Lu and Harrison H. Zhou. Statistical and computational guarantees of Lloyd\u2019s algorithm and its  variants. arXiv preprint arXiv:1612.02099, 2016. HIDDEN INTEGRALITY OF SDP RELAXATIONS FOR SUB-GAUSSIAN MIXTURE MODELS  Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-means problem is NP-hard. In International Workshop on Algorithms and Computation, pages 274-285. Springer, 2009.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning communities In 29th Annual Conference on Learning Theory, pages 1258-1291,  in the presence of errors. 2016.  Dustin G. Mixon, Soledad Villar, and Rachel Ward. Clustering subgaussian mixtures by semidefi-  nite programming. Information and Inference: A Journal of the IMA, page iax001, 2017.  Andrea Montanari and Subhabrata Sen. Semidefinite programs on sparse random graphs and their application to community detection. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 814-827, Cambridge, MA, USA, June 2016. doi: 10. 1145/2897518.2897548. URL http://doi.acm.org/10.1145/2897518.2897548.  Abhinav Nellore and Rachel Ward. Recovery guarantees for exemplar-based clustering. Information  and Computation, 245:165-180, 2015.  Samet Oymak and Babak Hassibi. Finding dense clusters via \u201dlow rank+ sparse\u201d decomposition.  arXiv preprint arXiv:1104.5186, 2011.  Karl Pearson. Method of moments and method of maximum likelihood. Biometrika, 28(1/2):34-59,  1936.  Jiming Peng and Yu Wei. Approximating k-means-type clustering via semidefinite programming.  SIAM Journal on Optimization, 18(1):186-205, 2007.  Jiming Peng and Yu Xia. A new theoretical framework for k-means-type clustering. In Foundations  and Advances in Data Mining, pages 79-96. Springer, 2005.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of  Computer and System Sciences, 68(4):841-860, 2004.  Roman Vershynin. High-dimensional probability. 2017.  Ji Xu, Daniel J. Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures In Advances in Neural Information Processing Systems, pages 2676-2684,  of two gaussians. 2016.  Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient EM for  multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017.  "}, "Counting Motifs with Graph Sampling": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Counting Motifs with Graph Sampling", "abstract": "Applied researchers often construct a network from data that has been collected from a random sample of nodes, with the goal to infer properties of the parent network from the sampled version. Two of the most widely used sampling schemes are , where we sample each vertex independently with probability $p$ and observe the subgraph induced by the sampled vertices, and , where we additionally observe the edges between the sampled vertices and their neighbors. In this paper, we study the problem of estimating the number of motifs as induced subgraphs under both models from a statistical perspective. We show that: for parent graph $G$ with maximal degree $d$, for any connected motif $h$ on $k$ vertices, to estimate the number of copies of $h$ in $G$, denoted by $s=\\mathsf{s}(h,G)$,  with a multiplicative error of $\\epsilon$,   The matching minimax lower bounds are established using certain algebraic properties of subgraph counts. These results allow us to quantify how much more informative neighborhood sampling is than subgraph sampling, as empirically verified by experiments on synthetic and real-world data. We also address the issue of adaptation to the unknown maximum degree, and study specific problems for parent graphs with additional structures, e.g., trees or planar graphs.", "pdf_url": "http://proceedings.mlr.press/v75/klusowski18a/klusowski18a.pdf", "keywords": ["Graph sampling", "network motifs", "graph homomorphism numbers", "HorvitzThompson estimator", "minimax lower bounds"], "reference": "Maryam Aliakbarpour, Amartya Shankha Biswas, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and Anak Yodpinyanee. Sublinear-time algorithms for counting star subgraphs via edge sampling. Algorithmica, pages 1-30.  Coren L. Apicella, Frank W. Marlowe, James H. Fowler, and Nicholas A. Christakis. Social networks and cooperation in hunter-gatherers. Nature, 481(7382):497-501, 01 2012. URL http://dx.doi.org/10.1038/nature10736.  Peter J. Bickel, Aiyou Chen, and Elizaveta Levina. The method of moments and degree distributions for network models. Ann. Statist., 39(5):2280-2301, 2011. ISSN 0090-5364. doi: 10.1214/11-AOS904. URL http://dx.doi.org/10.1214/11-AOS904.  Norman Biggs. On cluster expansions in graph theory and physics. Quart. J. Math. Oxford Ser. (2), 29(114):159-173, 1978. ISSN 0033-5606. doi: 10.1093/qmath/29.2.159. URL http://dx.doi.org/10.1093/qmath/29.2.159.  T.T. Cai and M. G. Low. Testing composite hypotheses, Hermite polynomials and optimal  estimation of a nonsmooth functional. 39(2):1012-1041, 2011.  Michael Capobianco. Estimating the connectivity of a graph. In Y. Alavi, D. R. Lick, and A. T. White, editors, Graph Theory and Applications, pages 65-74, Berlin, Heidelberg, 1972. Springer Berlin Heidelberg.  Arun Chandrasekhar and Randall Lewis. Econometrics of sampled networks. 2011.  Hao Chen, Nancy Zhang, et al. Graph-based change-point detection. The Annals of Statis-  tics, 43(1):139-176, 2015.  Lynna Chu and Hao Chen. Asymptotic distribution-free change-point detection for modern  data. arXiv preprint arXiv:1707.00167, 2017.  Graham Cormode and Nick Du\ufb03eld. Sampling for big data: a tutorial.  In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1975-1975. ACM, 2014.  Talya Eden, Amit Levi, Dana Ron, and C. Seshadhri. Approximately counting triangles in sublinear time. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science\u2014FOCS 2015, pages 614-633. IEEE Computer Soc., Los Alamitos, CA, 2015.  Paul Erd\u00a8os, L\u00b4aszl\u00b4o Lov\u00b4asz, and Joel Spencer. Strong independence of graphcopy functions. In Graph theory and related topics (Proc. Conf., Univ. Waterloo, Waterloo, Ont., 1977), pages 165-172. Academic Press, New York-London, 1979.  Uriel Feige. On sums of independent random variables with unbounded variance and estimating the average degree in a graph. SIAM J. Comput., 35(4):964-984, 2006. ISSN 0097-5397. doi: 10.1137/S0097539704447304. URL http://dx.doi.org/10.1137/ S0097539704447304.  13   Counting Motifs with Graph Sampling  References  Maryam Aliakbarpour, Amartya Shankha Biswas, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and Anak Yodpinyanee. Sublinear-time algorithms for counting star subgraphs via edge sampling. Algorithmica, pages 1-30.  Coren L. Apicella, Frank W. Marlowe, James H. Fowler, and Nicholas A. Christakis. Social networks and cooperation in hunter-gatherers. Nature, 481(7382):497-501, 01 2012. URL http://dx.doi.org/10.1038/nature10736.  Peter J. Bickel, Aiyou Chen, and Elizaveta Levina. The method of moments and degree distributions for network models. Ann. Statist., 39(5):2280-2301, 2011. ISSN 0090-5364. doi: 10.1214/11-AOS904. URL http://dx.doi.org/10.1214/11-AOS904.  Norman Biggs. On cluster expansions in graph theory and physics. Quart. J. Math. Oxford Ser. (2), 29(114):159-173, 1978. ISSN 0033-5606. doi: 10.1093/qmath/29.2.159. URL http://dx.doi.org/10.1093/qmath/29.2.159.  T.T. Cai and M. G. Low. Testing composite hypotheses, Hermite polynomials and optimal  estimation of a nonsmooth functional. 39(2):1012-1041, 2011.  Michael Capobianco. Estimating the connectivity of a graph. In Y. Alavi, D. R. Lick, and A. T. White, editors, Graph Theory and Applications, pages 65-74, Berlin, Heidelberg, 1972. Springer Berlin Heidelberg.  Arun Chandrasekhar and Randall Lewis. Econometrics of sampled networks. 2011.  Hao Chen, Nancy Zhang, et al. Graph-based change-point detection. The Annals of Statis-  tics, 43(1):139-176, 2015.  Lynna Chu and Hao Chen. Asymptotic distribution-free change-point detection for modern  data. arXiv preprint arXiv:1707.00167, 2017.  Graham Cormode and Nick Du\ufb03eld. Sampling for big data: a tutorial.  In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1975-1975. ACM, 2014.  Talya Eden, Amit Levi, Dana Ron, and C. Seshadhri. Approximately counting triangles in sublinear time. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science\u2014FOCS 2015, pages 614-633. IEEE Computer Soc., Los Alamitos, CA, 2015.  Paul Erd\u00a8os, L\u00b4aszl\u00b4o Lov\u00b4asz, and Joel Spencer. Strong independence of graphcopy functions. In Graph theory and related topics (Proc. Conf., Univ. Waterloo, Waterloo, Ont., 1977), pages 165-172. Academic Press, New York-London, 1979.  Uriel Feige. On sums of independent random variables with unbounded variance and estimating the average degree in a graph. SIAM J. Comput., 35(4):964-984, 2006. ISSN 0097-5397. doi: 10.1137/S0097539704447304. URL http://dx.doi.org/10.1137/ S0097539704447304. Klusowski Wu  Chao Gao and John La\ufb00erty. Testing network structure using relations between small  subgraph probabilities. arXiv preprint arXiv:1704.06742, 2017.  Pablo M Gleiser and Leon Danon. Community structure in jazz. Advances in Complex  Systems, 6(04):565-573, 2003.  Oded Goldreich and Dana Ron. Approximating average parameters of graphs. Random ISSN 1042-9832. doi: 10.1002/rsa.20203.  Structures Algorithms, 32(4):473-493, 2008. URL http://dx.doi.org/10.1002/rsa.20203.  Mira Gonen, Dana Ron, and Yuval Shavitt. Counting stars and other small subgraphs in sublinear-time. SIAM J. Discrete Math., 25(3):1365-1411, 2011. ISSN 0895-4801. doi: 10.1137/100783066. URL http://dx.doi.org/10.1137/100783066.  Jing-Dong J Han, Denis Dupuy, Nicolas Bertin, Michael E Cusick, and Marc Vidal. Ef- fect of sampling on topology predictions of protein-protein interaction networks. Nature Biotechnology, 23(7):839, 2005.  Mark S. Handcock and Krista J. Gile. Modeling social networks from sampled data. Ann. Appl. Stat., 4(1):5-25, 2010. ISSN 1932-6157. URL https://doi.org/10.1214/ 08-AOAS221.  Denis Hanson. On the product of the primes. Canad. Math. Bull., 15:33-37, 1972. ISSN 0008-4395. doi: 10.4153/CMB-1972-007-7. URL http://dx.doi.org/10.4153/ CMB-1972-007-7.  Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replace- ment from a finite universe. Journal of the American Statistical Association, 47(260): 663-685, 1952.  Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Minimax estimation of functionals of discrete distributions. IEEE Transactions on Information Theory, 61(5): 2835-2885, 2015.  Jason M. Klusowski and Yihong Wu. Estimating the number of connected components in  a graph via subgraph sampling. arXiv preprint arXiv:1801.04339, 2018.  W. L. Kocay. Some new methods in reconstruction theory. In Combinatorial mathematics, IX (Brisbane, 1981), volume 952 of Lecture Notes in Math., pages 89-114. Springer, Berlin-New York, 1982.  Eric D Kolaczyk. Statistical Analysis of Network Data: Methods and Models. Springer  Science & Business Media, 2009.  Eric D. Kolaczyk. Topics at the Frontier of Statistics and Network Analysis: (Re)Visiting the Foundations. SemStat Elements. Cambridge University Press, 2017. doi: 10.1017/ 9781108290159.  Oleg Lepski, Arkady Nemirovski, and Vladimir Spokoiny. On estimation of the Lr norm of  a regression function. 113(2):221-253, 1999. Counting Motifs with Graph Sampling  Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 631-636. ACM, 2006.  L\u00b4aszl\u00b4o Lov\u00b4asz. Large Networks and Graph Limits, volume 60. American Mathematical  Society, 2012.  arXiv:1010.5159, 2010.  L\u00b4aszl\u00b4o Lov\u00b4asz and Bal\u00b4azs Szegedy. The graph theoretic moment problem. arXiv preprint  Ron Milo, Shai Shen-Orr, Shalev Itzkovitz, Nadav Kashtan, Dmitri Chklovskii, and Uri Alon. Network motifs: simple building blocks of complex networks. Science, 298(5594): 824-827, 2002.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition model. Probab. Theory Related Fields, 162(3-4):431-461, 2015. ISSN 0178-8051. URL https://doi.org/10.1007/s00440-014-0576-6.  M. Nair. On Chebyshev-type inequalities for primes. Amer. Math. Monthly, 89(2):126- 129, 1982. ISSN 0002-9890. doi: 10.2307/2320934. URL http://dx.doi.org/10.2307/ 2320934.  Natasa Pr\u02c7zulj, Derek G Corneil, and Igor Jurisica. Modeling interactome: scale-free or  geometric? Bioinformatics, 20(18):3508-3515, 2004.  Peter Uetz, Loic Giot, Gerard Cagney, Traci A Mansfield, Richard S Judson, James R Knight, Daniel Lockshon, Vaibhav Narayan, Maithreyan Srinivasan, Pascale Pochart, et al. A comprehensive analysis of protein-protein interactions in saccharomyces cere- visiae. Nature, 403(6770):623, 2000.  Stanley Wasserman and Katherine Faust. Social network analysis: Methods and applica-  tions, volume 8. Cambridge university press, 1994.  Douglas B. West. Introduction to graph theory. Prentice Hall, Inc., Upper Saddle River,  NJ, 1996. ISBN 0-13-227828-6.  Hassler Whitney. The coloring of graphs. Ann. of Math. (2), 33(4):688-718, 1932. ISSN  0003-486X. doi: 10.2307/1968214. URL http://dx.doi.org/10.2307/1968214.  Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. IEEE Transactions on Information Theory, 62(6): 3702-3720, 2016.  "}, "Approximate Nearest Neighbors in Limited  Space": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Approximate Nearest Neighbors in Limited  Space", "abstract": "We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point  $y$,  finds a point $x \\in X$ whose distance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an accuracy parameter $\\epsilon \\in (0,1)$.  Our main result is a data structure that occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space, assuming all point coordinates are integers in the range  $\\{-n^{O(1)} \\ldots n^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This improves over the best previously known space bound of         $O(\\epsilon^{-2} n \\log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984).  We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem.", "pdf_url": "http://proceedings.mlr.press/v75/indyk18a/indyk18a.pdf", "keywords": ["nearest neighbor", "quantization", "distance estimation", "metric compression", "distance sketches", "dimension reduction"], "reference": "Dimitris Achlioptas. Database-friendly random projections.  In Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 274-281. ACM, 2001.  Alexandr Andoni and Piotr Indyk. Nearest neighbors in high-dimensional spaces. CRC Handbook  of Discrete and Computational Geometry, 2017.  J Lawrence Carter and Mark N Wegman. Universal classes of hash functions. Journal of computer  and system sciences, 18(2):143-154, 1979.  A. Efros. How to stop worrying and learn to love nearest neighbors. https://nn2017.mit.edu/wp-  content/uploads/sites/5/2017/12/Efros-NIPS-NN-17.pdf, 2017.  Chirag Gupta, Arun Sai Suggala, Ankit Goyal, Harsha Vardhan Simhadri, Bhargavi Paranjape, Ashish Kumar, Saurabh Goyal, Raghavendra Udupa, Manik Varma, and Prateek Jain. Protonn: Compressed and accurate knn for resource-scarce devices. In International Conference on Ma- chine Learning, pages 1331-1340, 2017.  Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards re-  moving the curse of dimensionality. Theory of computing, 8(1):321-350, 2012.  Piotr Indyk and Tal Wagner. Near-optimal (euclidean) metric compression. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 710-723. SIAM, 2017.  Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Practical data-dependent metric compression with provable guarantees. In Advances in Neural Information Processing Systems, pages 2614-2623, 2017.  Thathachar S Jayram and David P Woodruff. Optimal bounds for johnson-lindenstrauss transforms and streaming problems with subconstant error. ACM Transactions on Algorithms (TALG), 9(3): 26, 2013.  Jeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou.  Faiss: A library for efficient simi- larity search. https://code.facebook.com/posts/1373769912645926/faiss-a-library-for-efficient- similarity-search/, 2017a.  Jeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-scale similarity search with gpus. arXiv  preprint arXiv:1702.08734, 2017b.  William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.  Contemporary mathematics, 26(189-206):1, 1984.  13   This work was supported by grants from the MITEI-Shell program, Amazon Research Award and Simons Investigator Award.  Acknowledgments  References  Dimitris Achlioptas. Database-friendly random projections.  In Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 274-281. ACM, 2001.  Alexandr Andoni and Piotr Indyk. Nearest neighbors in high-dimensional spaces. CRC Handbook  of Discrete and Computational Geometry, 2017.  J Lawrence Carter and Mark N Wegman. Universal classes of hash functions. Journal of computer  and system sciences, 18(2):143-154, 1979.  A. Efros. How to stop worrying and learn to love nearest neighbors. https://nn2017.mit.edu/wp-  content/uploads/sites/5/2017/12/Efros-NIPS-NN-17.pdf, 2017.  Chirag Gupta, Arun Sai Suggala, Ankit Goyal, Harsha Vardhan Simhadri, Bhargavi Paranjape, Ashish Kumar, Saurabh Goyal, Raghavendra Udupa, Manik Varma, and Prateek Jain. Protonn: Compressed and accurate knn for resource-scarce devices. In International Conference on Ma- chine Learning, pages 1331-1340, 2017.  Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards re-  moving the curse of dimensionality. Theory of computing, 8(1):321-350, 2012.  Piotr Indyk and Tal Wagner. Near-optimal (euclidean) metric compression. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 710-723. SIAM, 2017.  Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Practical data-dependent metric compression with provable guarantees. In Advances in Neural Information Processing Systems, pages 2614-2623, 2017.  Thathachar S Jayram and David P Woodruff. Optimal bounds for johnson-lindenstrauss transforms and streaming problems with subconstant error. ACM Transactions on Algorithms (TALG), 9(3): 26, 2013.  Jeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou.  Faiss: A library for efficient simi- larity search. https://code.facebook.com/posts/1373769912645926/faiss-a-library-for-efficient- similarity-search/, 2017a.  Jeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-scale similarity search with gpus. arXiv  preprint arXiv:1702.08734, 2017b.  William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.  Contemporary mathematics, 26(189-206):1, 1984. Daniel Kane, Raghu Meka, and Jelani Nelson. Almost optimal explicit johnson-lindenstrauss fam- ilies. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech- niques, pages 628-639. Springer, 2011.  Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. Efficient search for approximate nearest  neighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457-474, 2000.  Marco Molinaro, David P Woodruff, and Grigory Yaroslavtsev. Beating the direct sum theorem in communication complexity with implications for sketching. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1738-1756. Society for Industrial and Applied Mathematics, 2013.  Ilan Newman. Private vs. common random bits in communication complexity. Information pro-  cessing letters, 39(2):67-71, 1991.  Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. Nearest-neighbor methods in learning  and vision: theory and practice (neural information processing). The MIT press, 2006.  Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. A survey on learning to hash. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 40(4):769-790, 2018.  Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data: a  survey. Proceedings of the IEEE, 104(1):34-57, 2016. "}, "Breaking the 1\\sqrt{n} Barrier: Faster Rates for Permutation-based Models in Polynomial Time": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Breaking the 1\\sqrt{n} Barrier: Faster Rates for Permutation-based Models in Polynomial Time", "abstract": "Many applications, including rank aggregation and crowd-labeling, can be modeled in terms of a bivariate isotonic matrix with unknown permutations acting on its rows and columns.  We consider the problem of estimating such a matrix based on noisy observations of a subset of its entries, and design and analyze a polynomial-time algorithm that improves upon the state of the art. In particular, our results imply that any such $n \\times n$ matrix can be estimated efficiently in the normalized Frobenius norm at rate $\\widetilde{\\mathcal O}(n^{-3/4})$, thus narrowing the gap between $\\widetilde{\\mathcal O}(n^{-1})$ and $\\widetilde{\\mathcal O}(n^{-1/2})$, which were hitherto the rates of the most statistically and computationally efficient methods, respectively.", "pdf_url": "http://proceedings.mlr.press/v75/mao18a/mao18a.pdf", "keywords": ["permutation-based models", "ranking", "pairwise comparisons", "crowd-labeling", "statisticalcomputational gap", "shape-constrained estimation"], "reference": "Ralph A. Bradley and Milton E. Terry. Rank analysis of incomplete block designs. I. The method  of paired comparisons. Biometrika, 39:324-345, 1952.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 268-276. ACM, New York, 2008.  Sabyasachi Chatterjee and Sumit Mukherjee. On estimation in tournaments and graphs under mono-  tonicity constraints. arXiv preprint arXiv:1603.04556, 2016.  Sourav Chatterjee. Matrix estimation by universal singular value thresholding. Ann. Statist., 43(1):  177-214, 2015.  Alexander Philip Dawid and Allan M. Skene. Maximum likelihood estimation of observer error-  rates using the EM algorithm. Applied Statistics, pages 20-28, 1979.  Nicolas Flammarion, Cheng Mao, and Philippe Rigollet. Optimal rates of statistical seriation. arXiv  preprint arXiv:1607.02435, 2016.  R. Duncan Luce. Individual choice behavior: A theoretical analysis. John Wiley & Sons, Inc., New  York; Chapman & Hall, Ltd., London, 1959.  Cheng Mao, Jonathan Weed, and Philippe Rigollet. Minimax rates and efficient algorithms for noisy sorting. In Firdaus Janoos, Mehryar Mohri, and Karthik Sridharan, editors, Proceedings of Algorithmic Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 821-847. PMLR, 07-09 Apr 2018.  Ashwin Pananjady, Cheng Mao, Vidya Muthukumar, Martin J. Wainwright, and Thomas A. Cour- tade. Worst-case vs average-case design for estimation from fixed pairwise comparisons. arXiv preprint arXiv:1707.06217, 2017a.  Ashwin Pananjady, Martin J. Wainwright, and Thomas A. Courtade. Denoising linear models with In Information Theory (ISIT), 2017 IEEE International Symposium on, pages  permuted data. 446-450. IEEE, 2017b.  Nihar B. Shah, Sivaraman Balakrishnan, and Martin J. Wainwright. A permutation-based model for crowd labeling: Optimal estimation and robustness. arXiv preprint arXiv:1606.09632, 2016a.  Nihar B. Shah, Sivaraman Balakrishnan, and Martin J. Wainwright. Feeling the Bern: Adaptive In Information Theory (ISIT),  estimators for Bernoulli probabilities of pairwise comparisons. 2016 IEEE International Symposium on, pages 1153-1157. IEEE, 2016b.  5   FASTER RATES FOR PERMUTATION-BASED MODELS  This work was supported in part by grants NSF CAREER DMS-1541099, NSF DMS-1541100, NSF-DMS-1612948 and DOD ONR-N00014.  Acknowledgments  References  Ralph A. Bradley and Milton E. Terry. Rank analysis of incomplete block designs. I. The method  of paired comparisons. Biometrika, 39:324-345, 1952.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 268-276. ACM, New York, 2008.  Sabyasachi Chatterjee and Sumit Mukherjee. On estimation in tournaments and graphs under mono-  tonicity constraints. arXiv preprint arXiv:1603.04556, 2016.  Sourav Chatterjee. Matrix estimation by universal singular value thresholding. Ann. Statist., 43(1):  177-214, 2015.  Alexander Philip Dawid and Allan M. Skene. Maximum likelihood estimation of observer error-  rates using the EM algorithm. Applied Statistics, pages 20-28, 1979.  Nicolas Flammarion, Cheng Mao, and Philippe Rigollet. Optimal rates of statistical seriation. arXiv  preprint arXiv:1607.02435, 2016.  R. Duncan Luce. Individual choice behavior: A theoretical analysis. John Wiley & Sons, Inc., New  York; Chapman & Hall, Ltd., London, 1959.  Cheng Mao, Jonathan Weed, and Philippe Rigollet. Minimax rates and efficient algorithms for noisy sorting. In Firdaus Janoos, Mehryar Mohri, and Karthik Sridharan, editors, Proceedings of Algorithmic Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 821-847. PMLR, 07-09 Apr 2018.  Ashwin Pananjady, Cheng Mao, Vidya Muthukumar, Martin J. Wainwright, and Thomas A. Cour- tade. Worst-case vs average-case design for estimation from fixed pairwise comparisons. arXiv preprint arXiv:1707.06217, 2017a.  Ashwin Pananjady, Martin J. Wainwright, and Thomas A. Courtade. Denoising linear models with In Information Theory (ISIT), 2017 IEEE International Symposium on, pages  permuted data. 446-450. IEEE, 2017b.  Nihar B. Shah, Sivaraman Balakrishnan, and Martin J. Wainwright. A permutation-based model for crowd labeling: Optimal estimation and robustness. arXiv preprint arXiv:1606.09632, 2016a.  Nihar B. Shah, Sivaraman Balakrishnan, and Martin J. Wainwright. Feeling the Bern: Adaptive In Information Theory (ISIT),  estimators for Bernoulli probabilities of pairwise comparisons. 2016 IEEE International Symposium on, pages 1153-1157. IEEE, 2016b. FASTER RATES FOR PERMUTATION-BASED MODELS  Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, and Martin J. Wainwright. Stochastically transitive models for pairwise comparisons: statistical and computational issues. IEEE Trans. Inform. Theory, 63(2):934-959, 2017.  Louis L. Thurstone. A law of comparative judgment. Psychological review, 34(4):273, 1927. "}, "Unleashing Linear Optimizers for Group-Fair Learning and Optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Unleashing Linear Optimizers for Group-Fair Learning and Optimization", "abstract": "Most systems and learning algorithms optimize average performance or average loss \u2013 one reason being computational complexity. However, many objectives of practical interest are more complex than simply average loss. This arises, for example, when balancing performance or loss with fairness across people. We prove that, from a computational perspective, optimizing arbitrary objectives that take into account performance over a small number of groups is not significantly harder to optimize than average performance. Our main result is a polynomial-time reduction that uses a linear optimizer to optimize an arbitrary (Lipschitz continuous) function of performance over a (constant) number of possibly-overlapping groups. This includes fairness objectives over small numbers of groups, and we further point out that other existing notions of fairness such as individual fairness can be cast as convex optimization and hence more standard convex techniques can be used. Beyond learning, our approach applies to multi-objective optimization, more generally.", "pdf_url": "http://proceedings.mlr.press/v75/alabi18a/alabi18a.pdf", "keywords": ["optimization", "fairness", "learning", "online algorithms", "projection"], "reference": "Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk, and John Langford. A reductions approach to  fair classification. In FATML, 2017.  Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk, John Langford, and Hanna Wallach. A reduc-  tions approach to fair classification. In ICML, 2018.  Solon Barocas and Andrew D Selbst. Big data\u2019s disparate impact. Cal. L. Rev., 104:671, 2016.  R\u00b4obert Busa-Fekete, Bal\u00b4azs Sz\u00a8or\u00b4enyi, Paul Weng, and Shie Mannor. Multi-objective bandits: Optimizing the generalized gini index. In International Conference on Machine Learning, pages 625-634, 2017.  Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism  prediction instruments. Big data, 5(2):153-163, 2017.  Kimberle Crenshaw. Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics. U. Chi. Legal F., page 139, 1989.  Ilias Diakonikolas. Approximation of Multiobjective Optimization Problems. PhD thesis, New York,  NY, USA, 2011. AAI3454077.  Tammy Drezner, Zvi Drezner, and Jeffery Guyse. Equitable service by a facility: Minimizing the  gini coefficient. Computers & Operations Research, 36(12):3240-3246, 2009.  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, pages 214-226, New York, NY, USA, 2012. ACM.  Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Mark DM Leiserson. Decoupled In Sorelle A. Friedler and Christo classifiers for group-fair and efficient machine learning. Wilson, editors, Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pages 119-133, New York, NY, USA, 23-24 Feb 2018. PMLR.  Corrado Gini. Variabilit`a e mutabilit`a. Reprinted in Memorie di metodologica statistica (Ed. Pizetti  E, Salvemini, T). Rome: Libreria Eredi Virgilio Veschi, 1912.  13   UNLEASHING LINEAR OPTIMIZERS FOR GROUP-FAIR LEARNING AND OPTIMIZATION  7. Conclusions  This paper enables application designers and data scientists to optimize complex continuous ob- jectives, with possibly many local non-global minima, that account for differences across various groups. It is not our claim or belief that important issues such as fairness can be captured solely by such group statistics. However, with this ability, domain experts are hopefully better equipped to design objectives that, when optimized, lead to desirable outcomes.  Acknowledgments. We would like to thank Miro Dudk, Salil Vadhan, and the anonymous COLT reviewers for helpful comments.  References  Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk, and John Langford. A reductions approach to  fair classification. In FATML, 2017.  Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk, John Langford, and Hanna Wallach. A reduc-  tions approach to fair classification. In ICML, 2018.  Solon Barocas and Andrew D Selbst. Big data\u2019s disparate impact. Cal. L. Rev., 104:671, 2016.  R\u00b4obert Busa-Fekete, Bal\u00b4azs Sz\u00a8or\u00b4enyi, Paul Weng, and Shie Mannor. Multi-objective bandits: Optimizing the generalized gini index. In International Conference on Machine Learning, pages 625-634, 2017.  Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism  prediction instruments. Big data, 5(2):153-163, 2017.  Kimberle Crenshaw. Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics. U. Chi. Legal F., page 139, 1989.  Ilias Diakonikolas. Approximation of Multiobjective Optimization Problems. PhD thesis, New York,  NY, USA, 2011. AAI3454077.  Tammy Drezner, Zvi Drezner, and Jeffery Guyse. Equitable service by a facility: Minimizing the  gini coefficient. Computers & Operations Research, 36(12):3240-3246, 2009.  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, pages 214-226, New York, NY, USA, 2012. ACM.  Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Mark DM Leiserson. Decoupled In Sorelle A. Friedler and Christo classifiers for group-fair and efficient machine learning. Wilson, editors, Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pages 119-133, New York, NY, USA, 23-24 Feb 2018. PMLR.  Corrado Gini. Variabilit`a e mutabilit`a. Reprinted in Memorie di metodologica statistica (Ed. Pizetti  E, Salvemini, T). Rome: Libreria Eredi Virgilio Veschi, 1912. UNLEASHING LINEAR OPTIMIZERS FOR GROUP-FAIR LEARNING AND OPTIMIZATION  Gabriel Goh, Andrew Cotter, Maya Gupta, and Michael P Friedlander. Satisfying real-world goals with dataset constraints. In Advances in Neural Information Processing Systems 29, pages 2415-2423. 2016.  Fabrizio Grandoni, R. Ravi, Mohit Singh, and Rico Zenklusen. New approaches to multi-objective  optimization. Math. Program., 146(1-2):525-554, 2014.  David Ingold and Spencer Soper. Amazon doesnt consider the race of its customers. should it?, April 2016. URL https://www.bloomberg.com/graphics/2016-amazon-same-day/.  Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1),  pages 427-435, 2013.  Matthew Joseph, Michael J. Kearns, Jamie H. Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 325-333, 2016.  Sham M. Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation  algorithms. SIAM J. Comput., 39(3):1088-1106, 2009.  Adam Tauman Kalai and Santosh Vempala. Efficient algorithms for online decision problems. J.  Comput. Syst. Sci., 71(3):291-307, 2005.  Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learning. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008, pages 629-638, 2008.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT \u201992, pages 341-352, New York, NY, USA, 1992. ACM.  Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymander-  ing: Auditing and learning for subgroup fairness. CoRR, abs/1711.05144, 2017.  Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In 8th Innovations in Theoretical Computer Science Conference, ITCS 2017, January 9-11, 2017, Berkeley, CA, USA, pages 43:1-43:23, 2017.  Aditya Krishna Menon and Robert C. Williamson. The cost of fairness in classification. CoRR,  abs/1705.09055, 2017.  Wolfgang Mulzer and Yannik Stein. Computational aspects of the colorful carath\u00b4eodory theorem.  arXiv preprint arXiv:1412.3347, 2014.  Harikrishna Narasimhan, Harish G. Ramaswamy, Aadirupa Saha, and Shivani Agarwal. Consistent multiclass algorithms for complex performance measures. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2398-2407, 2015. UNLEASHING LINEAR OPTIMIZERS FOR GROUP-FAIR LEARNING AND OPTIMIZATION  Wlodzimierz Ogryczak, Hanan Luss, Michal Pi\u00b4oro, Dritan Nace, and Artur Tomaszewski. Fair optimization and networks: A survey. J. Applied Mathematics, 2014:612018:1-612018:25, 2014. URL https://doi.org/10.1155/2014/612018.  Christos H Papadimitriou and Mihalis Yannakakis. On the approximability of trade-offs and optimal access of web sources. In Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on, pages 86-92. IEEE, 2000.  Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing f-measures by cost-sensitive classification. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2123-2131, 2014.  R. Ravi, Madhav V. Marathe, S. S. Ravi, Daniel J. Rosenkrantz, and Harry B. Hunt III. Many birds with one stone: multi-objective approximation algorithms. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, May 16-18, 1993, San Diego, CA, USA, pages 438-447, 1993.  John Rawls. A theory of justice. 1971.  Hinrich Sch\u00a8utze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information  retrieval, volume 39. Cambridge University Press, 2008.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Arthur Warburton. Approximation of pareto optima in multiple-objective, shortest-path problems.  Operations Research, 35(1):70-79, 1987.  John A Weymark. Generalized gini inequality indices. Mathematical Social Sciences, 1(4):409-430,  1981.  Blake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, and Nathan Srebro. Learning non- discriminatory predictors. In Proceedings of the 2017 Conference on Learning Theory, volume 65, pages 1920-1953, Amsterdam, Netherlands, 07-10 Jul 2017.  Marcela Zuluaga, Guillaume Sergent, Andreas Krause, and Markus P\u00a8uschel. Active learning for multi-objective optimization. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 462-470, 2013.  8. "}, "The Many Faces of Exponential Weights in Online Learning": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "The Many Faces of Exponential Weights in Online Learning", "abstract": "A standard introduction to online learning might place Online Gradient Descent at its center and then proceed to develop generalizations and extensions like Online Mirror Descent and second-order methods. Here we explore the alternative approach of putting Exponential Weights (EW) first. We show that many standard methods and their regret bounds then follow as a special case by plugging in suitable surrogate losses and playing the EW posterior mean. For instance, we easily recover Online Gradient Descent by using EW with a Gaussian prior on linearized losses, and, more generally, all instances of Online Mirror Descent based on regular Bregman divergences also correspond to EW with a prior that depends on the mirror map. Furthermore, appropriate quadratic surrogate losses naturally give rise to Online Gradient Descent for strongly convex losses and to Online Newton Step. We further interpret several recent adaptive methods (iProd, Squint, and a variation of Coin Betting for experts) as a series of closely related reductions to exp-concave surrogate losses that are then handled by Exponential Weights. Finally, a benefit of our EW interpretation is that it opens up the possibility of sampling from the EW posterior distribution instead of playing the mean. As already observed by Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear Optimization.", "pdf_url": "http://proceedings.mlr.press/v75/hoeven18a/hoeven18a.pdf", "keywords": [], "reference": "Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo- rithm for bandit linear optimization. In Proceedings of the 21th Annual Conference on Learning Theory (COLT), pages 263-274, 2008.  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-point methods for full-information  and bandit online learning. IEEE Trans. Information Theory, 58(7):4164-4175, 2012.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with Breg-  man divergences. The Journal of Machine Learning Research, 6:1705-1749, 2005.  S\u00b4ebastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self- concordant barrier. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), pages 279-279, 2015.  S\u00b4ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), pages 41.1-41.14, 2012.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge university  press, 2006.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Thomas M. Cover. Universal portfolios. Mathematical Finance, 1:1-29, 1991.  Imre Csisz\u00b4ar. I-divergence geometry of probability distributions and minimization problems. The  Annals of Probability, 3(1):146-158, 1975.  Varsha Dani, Thomas Hayes, and Sham Kakade. The price of bandit information for online op- timization. In Advances in Neural Information Processing Systems 20 (NIPS), pages 345-352, 2007.  13   THE MANY FACES OF EXPONENTIAL WEIGHTS  Acknowledgments  The authors would like to thank Wouter Koolen for extensive discussions underlying Theorems 3, 5, 8 and 12. A precursor to Theorem 4 previously appeared in Van der Hoeven\u2019s master\u2019s thesis (Van der Hoeven, 2016). He was supported by the Netherlands Organization for Scientific Research (NWO grant TOP2EW.15.211). Kot\u0142owski was supported by the Polish National Science Centre (grant no. 2016/22/E/ST6/00299).  References  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algo- rithm for bandit linear optimization. In Proceedings of the 21th Annual Conference on Learning Theory (COLT), pages 263-274, 2008.  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-point methods for full-information  and bandit online learning. IEEE Trans. Information Theory, 58(7):4164-4175, 2012.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with Breg-  man divergences. The Journal of Machine Learning Research, 6:1705-1749, 2005.  S\u00b4ebastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self- concordant barrier. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), pages 279-279, 2015.  S\u00b4ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), pages 41.1-41.14, 2012.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge university  press, 2006.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Thomas M. Cover. Universal portfolios. Mathematical Finance, 1:1-29, 1991.  Imre Csisz\u00b4ar. I-divergence geometry of probability distributions and minimization problems. The  Annals of Probability, 3(1):146-158, 1975.  Varsha Dani, Thomas Hayes, and Sham Kakade. The price of bandit information for online op- timization. In Advances in Neural Information Processing Systems 20 (NIPS), pages 345-352, 2007. THE MANY FACES OF EXPONENTIAL WEIGHTS  Travis Dick, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online learning in Markov decision processes with changing cost sequences. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 512-520, 2014.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations  and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  Tim van Erven and Wouter M. Koolen. Metagrad: Multiple learning rates in online learning. In  Advances in Neural Information Processing Systems 29 (NIPS), pages 3666-3674, 2016.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Peter D. Gr\u00a8unwald. The minimum description length principle. MIT press, 2007.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,  2(3-4):157-325, 2016.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  David P. Helmbold and Manfred K. Warmuth. Learning permutations with Exponential Weights.  Journal of Machine Learning Research, 10:1705-1736, 2009.  Dirk van der Hoeven. Is Mirror Descent a special case of Exponential Weights? Master\u2019s thesis, Lei- den University, The Netherlands, 2016. Available from http://pub.math.leidenuniv. nl/\u02dchoevendvander/.  Shunsuke Ihara. Information Theory for Continuous Systems, volume 2. World Scientific, 1993.  Adam Kalai and Santosh Vempala. Efficient algorithms for universal portfolios. Journal of Machine  Learning Research, 3(Nov):423-440, 2002.  Jyrki Kivinen and Manfred K. Warmuth. Exponentiated Gradient versus Gradient Descent for linear  predictors. Information and Computation, 132(1):1-63, 1997.  Wouter M. Koolen. The relative entropy bound for Squint. Blog August 13: http://blog.  wouterkoolen.info/Squint_PAC/post.html, 2015.  Wouter M. Koolen. Exploiting curvature using Exponential Weights. Blog September 6: http:  //blog.wouterkoolen.info/EW4Quadratic/post.html, 2016.  Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combi- In Proceedings of The 28th Conference on Learning Theory (COLT), pages  natorial games. 1155-1175, 2015.  Wouter M. Koolen, Peter Gr\u00a8unwald, and Tim van Erven. Combining adversarial guarantees and stochastic fast rates in online learning. In Advances in Neural Information Processing Systems 29 (NIPS), pages 4457-4465, 2016. THE MANY FACES OF EXPONENTIAL WEIGHTS  Raphail Krichevsky and Victor Trofimov. The performance of universal encoding. IEEE Transac-  tions on Information Theory, 27(2):199-207, 1981.  Nick Littlestone and Manfred K. Warmuth. The Weighted Majority algorithm. Information and  Computation, 108(2):212-261, 1994.  Hariharan Narayanan and Alexander Rakhlin. Efficient sampling from time-varying log-concave  distributions. The Journal of Machine Learning Research, 18(1):4017-4045, 2017.  Frank Nielsen and Richard Nock. Entropies and cross-entropies of exponential families. In 17th IEEE International Conference on Image Processing (ICIP), pages 3621-3624. IEEE, 2010.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in  Neural Information Processing Systems 29 (NIPS), pages 577-585, 2016.  Francesco Orabona, Koby Crammer, and Nicol`o Cesa-Bianchi. A generalized online mirror descent with applications to classification and regression. Machine Learning, 99(3):411-435, 2015.  Laurent Orseau, Tor Lattimore, and Shane Legg. Soft-Bayes: Prod for mixtures of experts with log-loss. In International Conference on Algorithmic Learning Theory 28 (ALT), pages 372-399, 2017.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the Leader if  you can, Hedge if you must. Journal of Machine Learning Research, 15:1281-1316, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Volodimir G. Vovk. Aggregating strategies.  In Proceedings of the 3rd Annual Conference on  Learning Theory (COLT), pages 371-383, 1990.  Volodimir G. Vovk. Competitive on-line statistics. International Statistical Review, 69(2):213-248,  2001.  Qun Xie and Andrew R. Barron. Asymptotic minimax regret for data compression, gambling, and  prediction. IEEE Transactions on Information Theory, 46(2):431-445, 2000.  Martin Zinkevich. Online convex programming and generalized infinitesimal Gradient Ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML), pages 928-936, 2003.  "}, "Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem", "abstract": "We study sampling as optimization in the space of measures. We focus on gradient flow-based optimization with the Langevin dynamics as a case study. We investigate the source of the bias of the unadjusted Langevin algorithm (ULA) in discrete time, and consider how to remove or reduce the bias. We point out the difficulty is that the heat flow is exactly solvable, but neither its forward nor backward method is implementable in general, except for Gaussian data. We propose the symmetrized Langevin algorithm (SLA), which should have a smaller bias than ULA, at the price of implementing a proximal gradient step in space. We show SLA is in fact consistent for Gaussian target measure, whereas ULA is not. We also illustrate various algorithms explicitly for Gaussian target measure with Gaussian data, including gradient descent, proximal gradient, and Forward-Backward, and show they are all consistent.", "pdf_url": "http://proceedings.mlr.press/v75/wibisono18a/wibisono18a.pdf", "keywords": [], "reference": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Gradient \ufb02ows in metric spaces and in the  space of probability measures. Springer Science & Business Media, 2008.  Espen Bernton. Langevin Monte Carlo and JKO splitting. In Proceedings of the 2018 Conference on Learning Theory, volume 75 of Proceedings of Machine Learning Research, Stockholm, Sweden, 06-09 Jul 2018. PMLR.  Eric A. Carlen and Wilfrid Gangbo. Constrained steepest descent in the 2-Wasserstein metric.  Annals of Mathematics, pages 807-846, 2003.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceed- ings of Algorithmic Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 186-211. PMLR, 07-09 Apr 2018.  Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. In Proceedings of the 2018 Conference on Learning Theory, volume 75 of Proceedings of Machine Learning Research, Stockholm, Sweden, 06-09 Jul 2018. PMLR.  Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017a.  Arnak S. Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent. In Proceedings of the 2017 Conference on Learning Theory,  12   SAMPLING AS OPTIMIZATION IN THE SPACE OF MEASURES  5. Discussion and future work  In this paper we have studied sampling as optimization in the space of measures. We started with the question of whether we can have a consistent discretization of the Langevin dynamics that converges exponentially fast under LSI. We have seen that the difficulties are twofold: First, relative entropy is a composite optimization problem in the space of measures, so we have to work with composite algorithms. Second, the heat \ufb02ow is exactly solvable, but neither its forward nor backward methods are implementable. Therefore, unbiased algorithms such as the FB algorithm are not implementable for the Langevin dynamics. The basic discretization known as ULA is implementable but biased. We also proposed a symmetrized variant of ULA which should have a smaller bias, at the price of implementing the proximal gradient step in space.  We have focused on the Langevin dynamics, which is the gradient \ufb02ow dynamics for minimizing relative entropy. More generally, we can apply more sophisticated optimization techniques, such as acceleration, to sampling. There is a second-order variant of the Langevin dynamics known as the underdamped Langevin dynamics, which is the stochastic version of the second-order heavy ball dynamics for optimization, and has been shown to have better convergence properties than the Langevin dynamics (Cheng et al., 2018). However, it is interesting to consider whether we can also apply the acceleration principle directly in the space of measures, for example via the variational Lagrangian approach (Wibisono et al., 2016).  References  Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Gradient \ufb02ows in metric spaces and in the  space of probability measures. Springer Science & Business Media, 2008.  Espen Bernton. Langevin Monte Carlo and JKO splitting. In Proceedings of the 2018 Conference on Learning Theory, volume 75 of Proceedings of Machine Learning Research, Stockholm, Sweden, 06-09 Jul 2018. PMLR.  Eric A. Carlen and Wilfrid Gangbo. Constrained steepest descent in the 2-Wasserstein metric.  Annals of Mathematics, pages 807-846, 2003.  Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. In Proceed- ings of Algorithmic Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 186-211. PMLR, 07-09 Apr 2018.  Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis. In Proceedings of the 2018 Conference on Learning Theory, volume 75 of Proceedings of Machine Learning Research, Stockholm, Sweden, 06-09 Jul 2018. PMLR.  Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3): 651-676, 2017a.  Arnak S. Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent. In Proceedings of the 2017 Conference on Learning Theory, SAMPLING AS OPTIMIZATION IN THE SPACE OF MEASURES  volume 65 of Proceedings of Machine Learning Research, pages 678-689, Amsterdam, Nether- lands, 07-10 Jul 2017b. PMLR.  Arnak S. Dalalyan and Avetik G. Karagulyan. User-friendly guarantees for the Langevin Monte  Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095v2, 2017.  Glaydston de Carvalho Bento, Jo\u02dcao Xavier da Cruz Neto, and Paulo Roberto Oliveira. A new approach to the proximal point method: Convergence on general Riemannian manifolds. Journal of Optimization Theory and Applications, 168(3):743-755, 2016.  Amir Dembo, Thomas M. Cover, and Joy A. Thomas. Information theoretic inequalities. IEEE  Transactions on Information Theory, 37(6):1501-1518, 1991.  Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted  Langevin algorithm. arXiv preprint arXiv:1605.01559v2, 2016.  Raaz Dwivedi, Yuansi Chen, Martin J. Wainwright, and Bin Yu. Log-concave sampling: Metropolis- Hastings algorithms are fast! In Proceedings of the 2018 Conference on Learning Theory, vol- ume 75 of Proceedings of Machine Learning Research, Stockholm, Sweden, 06-09 Jul 2018. PMLR.  O.P. Ferreira. Proximal subgradient and a characterization of Lipschitz function on Riemannian  manifolds. Journal of Mathematical Analysis and Applications, 313(2):587-597, 2006.  O.P. Ferreira and P.R. Oliveira. Subgradient algorithm on Riemannian manifolds. Journal of Opti-  mization Theory and Applications, 97(1):93-104, 1998.  O.P. Ferreira and P.R. Oliveira. Proximal point algorithm on Riemannian manifolds. Optimization,  51(2):257-270, 2002.  Guillaume Garrigos, Lorenzo Rosasco, and Silvia Villa. Convergence of the forward-backward algorithm: Beyond the worst case with the help of geometry. arXiv preprint arXiv:1703.09477v3, 2017.  Leonard Gross. Logarithmic Sobolev inequalities. American Journal of Mathematics, 97(4):1061-  1083, 1975.  Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical integration: Structure- preserving algorithms for ordinary differential equations, volume 31. Springer Science & Busi- ness Media, second edition, 2006.  Richard Holley and Daniel Stroock. Logarithmic Sobolev inequalities and stochastic Ising models.  Journal of Statistical Physics, 46(5):1159-1194, 1987.  Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-  Planck equation. SIAM Journal on Mathematical Analysis, 29(1):1-17, January 1998.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal- In Joint European Conference on  gradient methods under the Polyak-Lojasiewicz condition. Machine Learning and Knowledge Discovery in Databases, pages 795-811. Springer, 2016. SAMPLING AS OPTIMIZATION IN THE SPACE OF MEASURES  Stanislaw Lojasiewicz. Une propri\u00b4et\u00b4e topologique des sous-ensembles analytiques r\u00b4eels. Les  \u00b4equations aux d\u00b4eriv\u00b4ees partielles, 117:87-89, 1963.  Michael C. Mackey. Time\u2019s Arrow: The Origins of Thermodynamics Behavior. Springer-Verlag,  1992.  Willard L. Miranker. A well posed problem for the backward heat equation. Proceedings of the  American Mathematical Society, 12(2):243-247, 1961.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Applied Optimiza-  tion. Kluwer, Boston, 2004.  Felix Otto and C\u00b4edric Villani. Generalization of an inequality by Talagrand and links with the  logarithmic Sobolev inequality. Journal of Functional Analysis, 173(2):361-400, 2000.  Boris T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki  i Matematicheskoi Fiziki, 3(4):643-653, 1963.  Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions and  their discrete approximations. Bernoulli, 2(4):341-363, 1996.  A. J. Stam. Some inequalities satisfied by the quantities of information of Fisher and Shannon.  Information and Control, 2(2):101-112, 1959.  Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics, 48  (4):1005-1026, 2011.  Michel Talagrand. Transportation cost for Gaussian and other product measures. Geometric &  Functional Analysis GAFA, 6(3):587-600, 1996.  C\u00b4edric Villani. A short proof of the \u201dconcavity of entropy power\u201d. IEEE Transactions on Informa-  tion Theory, 46(4):1695-1696, 2000.  C\u00b4edric Villani. Topics in optimal transportation. Number 58 in Graduate Studies in Mathematics.  American Mathematical Society, 2003.  C\u00b4edric Villani. Optimal Transport: Old and New, volume 338 of Grundlehren der mathematischen  Wissenschaften. Springer Berlin Heidelberg, 2008.  Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351- E7358, 2016.  Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Re- search, pages 1617-1638, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. SAMPLING AS OPTIMIZATION IN THE SPACE OF MEASURES  "}, "Online Learning: Sufficient Statistics and the Burkholder Method": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Online Learning: Sufficient Statistics and the Burkholder Method", "abstract": "We uncover a fairly general principle in online learning: If a regret inequality can be (approximately) expressed as a function of certain \"sufficient statistics\" for the data sequence, then there exists a special Burkholder function that 1) can be used algorithmically to achieve the regret bound and 2) only depends on these sufficient statistics, not the entire data sequence,  so that the online strategy is only required to keep the sufficient statistics in memory. This characterization is achieved by bringing the full power of the Burkholder Method\u2014originally developed for certifying probabilistic martingale inequalities\u2014to bear on the online learning setting. To demonstrate the scope and effectiveness of the Burkholder method, we develop a novel online strategy for matrix prediction that attains a regret bound corresponding to the variance term in matrix concentration inequalities. We also present a linear-time/space prediction strategy for parameter-free supervised learning with linear classes and general smooth norms.", "pdf_url": "http://proceedings.mlr.press/v75/foster18b/foster18b.pdf", "keywords": [], "reference": "423-462, 1954.  Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, June 2001.  Raghu Raj Bahadur et al. Sufficiency and statistical decision functions. Ann. Math. Statist, 25(3):  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  arXiv preprint arXiv:1404.5236, 2014.  Ahron Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algo-  rithms, and engineering applications, volume 2. Siam, 2001.  Donald L. Burkholder. A geometrical characterization of banach spaces in which martingale  difference sequences are unconditional. The Annals of Probability, pages 997-1011, 1981.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and losses. In Advances in Neural Information Processing Systems 29, pages 748-756. 2016.  Ashok Cutkosky and Kwabena A. Boahen. Online learning without prior information. The 30th  Annual Conference on Learning Theory, 2017.  Ashok Cutkosky and Francesco Orabona. Black-Box Reductions for Parameter-free Online Learning  in Banach Spaces. Conference on Learning Theory, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  R.A. Fisher. On the mathematical foundations of theoretical statistics. Phil. Trans. R. Soc. Lond. A,  222(594-604):309-368, 1922.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances in  Neural Information Processing Systems, pages 3375-3383, 2015.  Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems 30, pages 6020-6030. 2017a.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Zigzag: A new approach to adaptive  online learning. The 30th Annual Conference on Learning Theory, 2017b.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012.  13   ONLINE LEARNING: SUFFICIENT STATISTICS AND THE BURKHOLDER METHOD  References  423-462, 1954.  Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning, 43(3):211-246, June 2001.  Raghu Raj Bahadur et al. Sufficiency and statistical decision functions. Ann. Math. Statist, 25(3):  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  arXiv preprint arXiv:1404.5236, 2014.  Ahron Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algo-  rithms, and engineering applications, volume 2. Siam, 2001.  Donald L. Burkholder. A geometrical characterization of banach spaces in which martingale  difference sequences are unconditional. The Annals of Probability, pages 997-1011, 1981.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and losses. In Advances in Neural Information Processing Systems 29, pages 748-756. 2016.  Ashok Cutkosky and Kwabena A. Boahen. Online learning without prior information. The 30th  Annual Conference on Learning Theory, 2017.  Ashok Cutkosky and Francesco Orabona. Black-Box Reductions for Parameter-free Online Learning  in Banach Spaces. Conference on Learning Theory, 2018.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  R.A. Fisher. On the mathematical foundations of theoretical statistics. Phil. Trans. R. Soc. Lond. A,  222(594-604):309-368, 1922.  Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances in  Neural Information Processing Systems, pages 3375-3383, 2015.  Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems 30, pages 6020-6030. 2017a.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Zigzag: A new approach to adaptive  online learning. The 30th Annual Conference on Learning Theory, 2017b.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012. ONLINE LEARNING: SUFFICIENT STATISTICS AND THE BURKHOLDER METHOD  Tuomas Hyt\u00f6nen, Jan van Neerven, Mark Veraar, and Lutz Weis. Analysis in Banach spaces.  Springer, 2016.  (1):164-177, 1996.  Mathematics, 11(3):267-288, 1973.  computation, 108(2):212-261, 1994.  Adrian Stephen Lewis. Convex analysis on the hermitian matrices. SIAM Journal on Optimization, 6  Elliott H Lieb. Convex trace functions and the wigner-yanase-dyson conjecture. Advances in  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and  Lester Mackey, Michael I Jordan, Richard Y Chen, Brendan Farrell, Joel A Tropp, et al. Matrix concentration inequalities via the method of exchangeable pairs. The Annals of Probability, 42(3): 906-945, 2014.  Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In Advances in Neural Information Processing Systems, pages 2724-2732, 2013.  Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations. In Proceedings of The 27th Conference on Learning Theory, pages 1020-1039, 2014.  Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229-251, 2004.  Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and  method efficiency in optimization. 1983.  Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.  Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochas- tic learning. In Advances in Neural Information Processing Systems, pages 1116-1124, 2014.  Francesco Orabona and D\u00e1vid P\u00e1l. Coin betting and parameter-free online learning. Advances in  Neural Information Processing Systems, 2016.  Adam Os\u02dbekowski. Two inequalities for the first moments of a martingale, its square function and its  maximal function. Bulletin Polish Acad. Sci. Math., 53:441-449, 2005.  Adam Os\u02dbekowski. Sharp martingale and semimartingale inequalities. Monografie Matematyczne, 72,  2012.  Adam Os\u02dbekowski. Personal communication. 2017.  Gilles Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics,  20:326-350, 1975. ISSN 0021-2172.  A. Rakhlin, K. Sridharan, and A. Tewari. Online learning via sequential complexities. Journal of  Machine Learning Research, 2015. ONLINE LEARNING: SUFFICIENT STATISTICS AND THE BURKHOLDER METHOD  Alexander Rakhlin and Karthik Sridharan. On equivalence of martingale tail bounds and deterministic  regret inequalities. Conference on Learning Theory, pages 1704-1722, 2017.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010.  Alexander Rakhlin, Ohad Shamir, and Karthiks Sridharan. Relax and randomize: From value to algorithms. In Advances in Neural Information Processing Systems 25, pages 2150-2158, 2012.  Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In  Advances in neural information processing systems, pages 2645-2653, 2011.  Joel Tropp. Freedman\u2019s inequality for matrix martingales. Electronic Communications in Probability,  16:262-270, 2011.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Volodimir Vovk. Competitive on-line linear regression. In NIPS \u201997: Proceedings of the 1997 conference on Advances in neural information processing systems 10, pages 364-370, Cambridge, MA, USA, 1998. MIT Press.  Volodimir G Vovk. Aggregating strategies. Proc. of Computational Learning Theory, 1990, 1990.  "}, "Minimax Bounds on Stochastic Batched Convex Optimization": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Minimax Bounds on Stochastic Batched Convex Optimization", "abstract": "We study the stochastic batched convex optimization problem, in which we use many \\emph{parallel} observations to optimize a convex function given limited rounds of interaction.  In each of $M$ rounds, an algorithm may query for information at $n$ points, and after issuing all $n$ queries, it receives unbiased noisy function and/or (sub)gradient evaluations at the $n$ points.  After $M$ such rounds, the algorithm must output an estimator.  We provide lower and upper bounds on the performance of such batched convex optimization algorithms in zeroth and first-order settings for Lipschitz convex and smooth strongly convex functions.  Our rates of convergence (nearly) achieve the fully sequential rate once $M = O(d \\log \\log n)$, where $d$ is the problem dimension, but the rates may exponentially degrade as the dimension $d$ increases, in distinction from fully sequential settings.", "pdf_url": "http://proceedings.mlr.press/v75/duchi18a/duchi18a.pdf", "keywords": ["Stochastic convex optimization", "batched optimization", "parallel computing"], "reference": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. IEEE Transactions on Information Theory, 58 (5):3235-3249, 2012.  G. Ballard, J. Demmel, O. Holtz, and O. Schwartz. Minimizing communication in numerical linear  algebra. SIAM Journal on Matrix Analysis and Applications, 32(3):866-901, 2011.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013a.  N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the Twenty-Fourth ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1190-1204, 2013b.  G. Dantzig. On the non-existence of tests of Student\u2019s hypothesis having power functions indepen-  dent of \u03c3. The Annals of Mathematical Statistics, 11(2):186-192, 1940.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.  J. C. Duchi. Introductory lectures on stochastic convex optimization. In Park City Mathematics Institute Graduate Summer School: Collected Lectures. American Mathematical Society (forth- coming), 2017.  J. C. Duchi, P. L. Bartlett, and M. J. Wainwright. Randomized smoothing for stochastic optimiza-  tion. SIAM Journal on Optimization, 22(2):674-701, 2012.  S. Fuller and L. Millett. The Future of Computing Performance: Game Over or Next Level? Na-  tional Academies Press, 2011.  ence, 104:121-145, 2002.  1983.  J. Hardwick and Q. F. Stout. Optimal few-stage designs. Journal of Statistical Planning and Infer-  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley,  F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochastic  gradient descent. In Advances in Neural Information Processing Systems 24, 2011.  V. Perchet, P. Rigollet, S. Chassang, and E. Snowberg. Batched bandit problems. Annals of Statis-  tics, 44(2):660681, 2016.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.  In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, pages 3-24, 2013.  A. Smith, A. Thakurta, and J. Upadhyay. Is interaction necessary for distributed private learning?  In IEEE Symposium on Security and Privacy, 2017.  13   MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. IEEE Transactions on Information Theory, 58 (5):3235-3249, 2012.  G. Ballard, J. Demmel, O. Holtz, and O. Schwartz. Minimizing communication in numerical linear  algebra. SIAM Journal on Matrix Analysis and Applications, 32(3):866-901, 2011.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013a.  N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the Twenty-Fourth ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1190-1204, 2013b.  G. Dantzig. On the non-existence of tests of Student\u2019s hypothesis having power functions indepen-  dent of \u03c3. The Annals of Mathematical Statistics, 11(2):186-192, 1940.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.  J. C. Duchi. Introductory lectures on stochastic convex optimization. In Park City Mathematics Institute Graduate Summer School: Collected Lectures. American Mathematical Society (forth- coming), 2017.  J. C. Duchi, P. L. Bartlett, and M. J. Wainwright. Randomized smoothing for stochastic optimiza-  tion. SIAM Journal on Optimization, 22(2):674-701, 2012.  S. Fuller and L. Millett. The Future of Computing Performance: Game Over or Next Level? Na-  tional Academies Press, 2011.  ence, 104:121-145, 2002.  1983.  J. Hardwick and Q. F. Stout. Optimal few-stage designs. Journal of Statistical Planning and Infer-  A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley,  F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochastic  gradient descent. In Advances in Neural Information Processing Systems 24, 2011.  V. Perchet, P. Rigollet, S. Chassang, and E. Snowberg. Batched bandit problems. Annals of Statis-  tics, 44(2):660681, 2016.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.  In Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory, pages 3-24, 2013.  A. Smith, A. Thakurta, and J. Upadhyay. Is interaction necessary for distributed private learning?  In IEEE Symposium on Security and Privacy, 2017. MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION  C. Stein. A two-sample test for a linear hypothesis whose power is independent of the variance.  Annals of Mathematical Statistics, 16(3):243-258, 1945.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.  A. Wald. Contributions to the theory of statistical estimation and testing hypotheses. Annals of  Mathematical Statistics, 10(4):299-326, 1939. MINIMAX BOUNDS ON STOCHASTIC BATCHED CONVEX OPTIMIZATION  "}, "Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints", "abstract": "We consider parameter estimation in distributed networks, where each sensor in the network observes an independent sample from an underlying distribution and has $k$ bits to communicate its sample to a centralized processor which computes an estimate of a desired parameter. We develop lower bounds for the minimax risk of estimating the underlying parameter under squared $\\ell_2$ loss for a large class of distributions. Our results show that under mild regularity conditions, the communication constraint reduces the effective sample size by a factor of $d$ when $k$ is small, where $d$ is the dimension of the estimated parameter. Furthermore, this penalty reduces at most exponentially with increasing $k$, which is the case for some models, e.g., estimating high-dimensional distributions. For other models however, we show that the sample size reduction is re-mediated only linearly with increasing $k$, e.g. when some sub-Gaussian structure is available. We apply our results to the distributed setting with product Bernoulli model, multinomial model, and dense/sparse Gaussian location models which recover or strengthen existing results. Our approach significantly deviates from existing approaches for developing information-theoretic lower bounds for communication-efficient estimation. We circumvent the need for strong data processing inequalities used in prior work and develop a geometric approach which builds on a new representation of the communication constraint. This approach allows us to strengthen and generalize existing results with simpler and more transparent proofs.", "pdf_url": "http://proceedings.mlr.press/v75/han18a/han18a.pdf", "keywords": ["Distributed estimation", "Minimax lower bound", "High-dimensional geometry", "Blackboard communication protocol", "Strong data processing inequality"], "reference": "Jayadev Acharya, Cl\u00b4ement L Canonne, and Himanshu Tyagi. Distributed simulation and distributed  inference. arXiv preprint arXiv:1804.06952, 2018.  Shun-ichi Amari. On optimal data compression in multiterminal statistical inference. IEEE Trans-  actions on Information Theory, 57(9):5577-5587, 2011.  Maria Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, commu-  nication complexity and privacy. In Conference on Learning Theory, pages 26-1, 2012.  Ziv Bar-Yossef, Thathachar S Jayram, Ravi Kumar, and D Sivakumar. An information statistics ap- proach to data stream and communication complexity. Journal of Computer and System Sciences, 68(4):702-732, 2004.  Z Birnbaum and W-f Orlicz. \u00a8Uber die verallgemeinerung des begriffes der zueinander konjugierten  potenzen. Studia Mathematica, 3(1):1-67, 1931.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011- 1020. ACM, 2016.  Hal Daum\u00b4e, Jeff M Phillips, Avishek Saha, and Suresh Venkatasubramanian. Efficient protocols for distributed classification and optimization. In International Conference on Algorithmic Learning Theory, pages 154-168. Springer, 2012.  Hal Daume III, Jeff Phillips, Avishek Saha, and Suresh Venkatasubramanian. Protocols for learning classifiers on distributed data. In Artificial Intelligence and Statistics, pages 282-290, 2012.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, and Ludwig Schmidt. Communication-efficient distributed learning of discrete distributions. In Advances in Neural Information Processing Systems, pages 6394-6404, 2017.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),, pages  rates. 429-438. IEEE, 2013.  Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical esti- mation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726- 2734, 2014.  13   DISTRIBUTED PARAMETER ESTIMATION  References  Jayadev Acharya, Cl\u00b4ement L Canonne, and Himanshu Tyagi. Distributed simulation and distributed  inference. arXiv preprint arXiv:1804.06952, 2018.  Shun-ichi Amari. On optimal data compression in multiterminal statistical inference. IEEE Trans-  actions on Information Theory, 57(9):5577-5587, 2011.  Maria Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, commu-  nication complexity and privacy. In Conference on Learning Theory, pages 26-1, 2012.  Ziv Bar-Yossef, Thathachar S Jayram, Ravi Kumar, and D Sivakumar. An information statistics ap- proach to data stream and communication complexity. Journal of Computer and System Sciences, 68(4):702-732, 2004.  Z Birnbaum and W-f Orlicz. \u00a8Uber die verallgemeinerung des begriffes der zueinander konjugierten  potenzen. Studia Mathematica, 3(1):1-67, 1931.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 1011- 1020. ACM, 2016.  Hal Daum\u00b4e, Jeff M Phillips, Avishek Saha, and Suresh Venkatasubramanian. Efficient protocols for distributed classification and optimization. In International Conference on Algorithmic Learning Theory, pages 154-168. Springer, 2012.  Hal Daume III, Jeff Phillips, Avishek Saha, and Suresh Venkatasubramanian. Protocols for learning classifiers on distributed data. In Artificial Intelligence and Statistics, pages 282-290, 2012.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof Onak, and Ludwig Schmidt. Communication-efficient distributed learning of discrete distributions. In Advances in Neural Information Processing Systems, pages 6394-6404, 2017.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),, pages  rates. 429-438. IEEE, 2013.  Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed statistical esti- mation and dimensionality. In Advances in Neural Information Processing Systems, pages 2726- 2734, 2014. HAN \u00a8OZG \u00a8UR WEISSMAN  Jaroslav H\u00b4ajek. A characterization of limiting distributions of regular estimates. Zeitschrift f\u00a8ur  Wahrscheinlichkeitstheorie und verwandte Gebiete, 14(4):323-330, 1970.  Jaroslav H\u00b4ajek. Local asymptotic minimax and admissibility in estimation. In Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 1, pages 175-194, 1972.  Yanjun Han, Ayfer \u00a8Ozg\u00a8ur, and Tsachy Weissman. Distributed statistical estimation of high- In Information Theory, 2018. ISIT 2018. IEEE  dimensional and nonparametric distributions. International Symposium on. IEEE, 2018.  Ildar Abdulovich Ibragimov and Rafail Z Has\u2019minskii. Statistical estimation: asymptotic theory,  volume 16. Springer Science & Business Media, 2013.  E Kushilevitz and N Nisan. Communication complexity. cambridge university press, 1997.  Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical  Soc., 2005.  Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and  probabilistic analysis. Cambridge University Press, 2005.  Walter Rudin. Real and complex analysis. Tata McGraw-Hill Education, 1987.  Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and  estimation. In Advances in Neural Information Processing Systems, pages 163-171, 2014.  A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2008.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  A Wyner. A theorem on the entropy of certain binary sequences and applications-ii. IEEE Trans-  actions on Information Theory, 19(6):772-777, 1973.  Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds on Bayes risk in decentralized  estimation. IEEE Transactions on Information Theory, 63(3):1580-1600, 2017.  Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright.  Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, pages 2328-2336, 2013. DISTRIBUTED PARAMETER ESTIMATION  "}, "Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance", "abstract": "We present \\emph{Local Moment Matching (LMM)}, a unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance. We construct an efficiently computable estimator that achieves the minimax rates in estimating the distribution up to permutation, and show that the plug-in approach of our unlabeled distribution estimator is \u201cuniversal\" in estimating symmetric functionals of discrete distributions. Instead of doing best polynomial approximation explicitly as in existing literature of functional estimation, the plug-in approach conducts polynomial approximation implicitly and attains the optimal sample complexity for the entropy, power sum and support size functionals.", "pdf_url": "http://proceedings.mlr.press/v75/han18b/han18b.pdf", "keywords": ["Distribution Estimation", "Functional Estimation", "Minimax Risk", "Wasserstein Distance"], "reference": "Jayadev Acharya, Alon Orlitsky, and Shengjun Pan. Recent results on pattern maximum likelihood. In Networking and Information Theory, 2009. ITW 2009. IEEE Information Theory Workshop on, pages 251-255. IEEE, 2009.  Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. The complexity  of estimating R\u00b4enyi entropy. SODA, 2015.  Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified arXiv preprint  maximum likelihood approach for optimal distribution property estimation. arXiv:1611.02960, 2016.  Chris Anderson. The long tail. Wired magazine, 12(10):170-177, 2004.  Daniel Berend and Aryeh Kontorovich. A sharp estimate of the binomial mean absolute deviation  with applications. Statistics & Probability Letters, 83(4):1254-1259, 2013.  Yuheng Bu, Shaofeng Zou, Yingbin Liang, and Venugopal V Veeravalli. Estimation of KL diver- gence between large-alphabet distributions. In 2016 IEEE International Symposium on Informa- tion Theory (ISIT), pages 1118-1122. IEEE, 2016.  T Tony Cai and Mark G Low. Testing composite hypotheses, Hermite polynomials and optimal  estimation of a nonsmooth functional. The Annals of Statistics, 39(2):1012-1041, 2011.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A Servedio. Learning k-modal distribu- tions via testing. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1371-1385, 2012.  Ronald A DeVore. Degree of approximation. Approximation theory II, 241(242):117-161, 1976.  Ilias Diakonikolas. Beyond histograms: Structure and distribution estimation. In Workshop of the  46th ACM Symposium on Theory of Computing, 2014.  Zeev Ditzian and Vilmos Totik. Moduli of smoothness. Springer, 1987.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  Bradley Efron and Ronald Thisted. Estimating the number of unsen species: How many words did shakespeare know? Biometrika, 63(3):pp. 435-447, 1976. ISSN 00063444. URL http: //www.jstor.org/stable/2335721.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of discrete distributions under  (cid:96)1 loss. IEEE Transactions on Information Theory, 61(11):6343-6354, 2015.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax rate-optimal estimation of divergences  between discrete distributions. arXiv preprint arXiv:1605.09124, 2016.  Yanjun Han, Jiantao Jiao, Rajarshi Mukherjee, and Tsachy Weissman. On estimation of Lr-norms  in Gaussian white noise models. arXiv preprint arXiv:1710.03863, 2017a.  13   LOCAL MOMENT MATCHING  References  Jayadev Acharya, Alon Orlitsky, and Shengjun Pan. Recent results on pattern maximum likelihood. In Networking and Information Theory, 2009. ITW 2009. IEEE Information Theory Workshop on, pages 251-255. IEEE, 2009.  Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. The complexity  of estimating R\u00b4enyi entropy. SODA, 2015.  Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified arXiv preprint  maximum likelihood approach for optimal distribution property estimation. arXiv:1611.02960, 2016.  Chris Anderson. The long tail. Wired magazine, 12(10):170-177, 2004.  Daniel Berend and Aryeh Kontorovich. A sharp estimate of the binomial mean absolute deviation  with applications. Statistics & Probability Letters, 83(4):1254-1259, 2013.  Yuheng Bu, Shaofeng Zou, Yingbin Liang, and Venugopal V Veeravalli. Estimation of KL diver- gence between large-alphabet distributions. In 2016 IEEE International Symposium on Informa- tion Theory (ISIT), pages 1118-1122. IEEE, 2016.  T Tony Cai and Mark G Low. Testing composite hypotheses, Hermite polynomials and optimal  estimation of a nonsmooth functional. The Annals of Statistics, 39(2):1012-1041, 2011.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A Servedio. Learning k-modal distribu- tions via testing. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1371-1385, 2012.  Ronald A DeVore. Degree of approximation. Approximation theory II, 241(242):117-161, 1976.  Ilias Diakonikolas. Beyond histograms: Structure and distribution estimation. In Workshop of the  46th ACM Symposium on Theory of Computing, 2014.  Zeev Ditzian and Vilmos Totik. Moduli of smoothness. Springer, 1987.  John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with  applications to statistical estimation. arXiv preprint arXiv:1311.2669, 2013.  Bradley Efron and Ronald Thisted. Estimating the number of unsen species: How many words did shakespeare know? Biometrika, 63(3):pp. 435-447, 1976. ISSN 00063444. URL http: //www.jstor.org/stable/2335721.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of discrete distributions under  (cid:96)1 loss. IEEE Transactions on Information Theory, 61(11):6343-6354, 2015.  Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax rate-optimal estimation of divergences  between discrete distributions. arXiv preprint arXiv:1605.09124, 2016.  Yanjun Han, Jiantao Jiao, Rajarshi Mukherjee, and Tsachy Weissman. On estimation of Lr-norms  in Gaussian white noise models. arXiv preprint arXiv:1710.03863, 2017a. HAN JIAO WEISSMAN  Yanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. Optimal rates of entropy estimation  over Lipschitz balls. arXiv preprint arXiv:1711.02141, 2017b.  Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American statistical association, 58(301):13-30, 1963.  Jiantao Jiao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. Minimax estimation of functionals of discrete distributions. Information Theory, IEEE Transactions on, 61(5):2835-2885, 2015.  Jiantao Jiao, Yanjun Han, and Tsachy Weissman. Minimax estimation of the L1 distance. In 2016  IEEE International Symposium on Information Theory (ISIT), pages 750-754. IEEE, 2016.  Jiantao Jiao, Yanjun Han, and Tsachy Weissman. Minimax estimation of the L1 distance. arXiv  preprint arXiv:1705.00807, 2017.  Sudeep Kamath, Alon Orlitsky, Venkatadheeraj Pichapati, and Ananda Theertha Suresh. On learn- ing distributions from their samples. In Proceedings of The 28th Conference on Learning Theory, pages 1066-1100, 2015.  Leonid Vasilevich Kantorovich and G Sh Rubinstein. On a space of completely additive functions.  Vestnik Leningrad. Univ, 13(7):52-59, 1958.  Weihao Kong and Gregory Valiant. Spectrum estimation from samples. The Annals of Statistics, 45  (5):2218-2247, 2017.  Oleg Lepski, Arkady Nemirovski, and Vladimir Spokoiny. On estimation of the Lr norm of a  regression function. Probability theory and related fields, 113(2):221-253, 1999.  George G Lorentz, Manfred von Golitschek, and Yuly Makovoz. Constructive approximation:  advanced problems, volume 304. Springer Berlin, 1996.  Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and  probabilistic analysis. Cambridge University Press, 2005.  I. Olkin and M. Sobel. Admissible and minimax estimation for the multinomial distribution and for  independent binomial distributions. The Annals of Statistics, 7:284-290, 1979.  Alon Orlitsky and Ananda Theertha Suresh. Competitive distribution estimation: Why is good- turing good. In Advances in Neural Information Processing Systems, pages 2143-2151, 2015.  Alon Orlitsky, Narayana P Santhanam, Krishnamurthy Viswanathan, and Junan Zhang. On model- ing profiles instead of values. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 426-435. AUAI Press, 2004.  Alon Orlitsky, Ananda Theertha Suresh, and Yihong Wu. Optimal prediction of the number of unseen species. Proceedings of the National Academy of Sciences, 113(47):13283-13288, 2016.  Dmitri S Pavlichin, Jiantao Jiao, and Tsachy Weissman. Approximate profile maximum likelihood.  arXiv preprint arXiv:1712.07177, 2017. LOCAL MOMENT MATCHING  Giovanni Peccati and Murad S Taqqu. Some facts about charlier polynomials. In Wiener Chaos:  Moments, Cumulants and Diagrams, pages 171-175. Springer, 2011.  Aditi Raghunathan, Greg Valiant, and James Zou. Estimating the unseen from multiple populations.  arXiv preprint arXiv:1707.03854, 2017.  M. Rutkowska. Minimax estimation of the parameters of the multivariate hypergeometric and multi-  nomial distributions. Zastos. Mat., 16:9-21, 1977.  H. Steinhaus. The problem of estimation. The Annals of Mathematical Statistics, 28:633-648, 1957.  Kevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters. In Advances  in Neural Information Processing Systems, pages 5780-5789, 2017.  S. Trybula. Some problems of simultaneous minimax estimation. The Annals of Mathematical  Statistics, 29:245-253, 1958.  A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2008.  Gregory Valiant and Paul Valiant. Estimating the unseen: an n/ log n-sample estimator for en- tropy and support size, shown optimal via new CLTs. In Proceedings of the 43rd annual ACM symposium on Theory of computing, pages 685-694. ACM, 2011a.  Gregory Valiant and Paul Valiant. The power of linear estimators. In Foundations of Computer  Science (FOCS), 2011 IEEE 52nd Annual Symposium on, pages 403-412. IEEE, 2011b.  Gregory Valiant and Paul Valiant.  Instance optimal learning. arXiv preprint arXiv:1504.05321,  2015.  Gregory Valiant and Paul Valiant. Estimating the unseen: Improved estimators for entropy and other  properties. Journal of the ACM (JACM), 64(6):37, 2017.  Paul Valiant and Gregory Valiant. Estimating the unseen: improved estimators for entropy and other properties. In Advances in Neural Information Processing Systems, pages 2157-2165, 2013.  Pascal O Vontobel. The bethe approximation of the pattern maximum likelihood distribution. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on. IEEE, 2012.  Abraham Wald. Statistical decision functions. Wiley, 1950.  Christopher Stroude Withers. Bias reduction by Taylor series. Communications in Statistics-Theory  and Methods, 16(8):2369-2383, 1987.  Yihong Wu and Pengkun Yang. Chebyshev polynomials, moment matching, and optimal estimation  of the unseen. arXiv preprint arXiv:1504.01227, 2015.  Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. IEEE Transactions on Information Theory, 62(6):3702-3720, 2016a.  Yihong Wu and Pengkun Yang. Sample complexity of the distinct elements problem. arXiv preprint  arXiv:1612.03375, 2016b. HAN JIAO WEISSMAN  "}, "Iterate Averaging as Regularization for Stochastic Gradient Descent": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Iterate Averaging as Regularization for Stochastic Gradient Descent", "abstract": "We propose and analyze a variant of the classic  Polyak\u2013Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.", "pdf_url": "http://proceedings.mlr.press/v75/neu18a/neu18a.pdf", "keywords": ["stochastic gradient descent", "least squares", "regularization", "Tikhonov regularization"], "reference": "Katy S. Azoury and Manfred K. Warmuth. Relative loss bounds for on-line density estimation with  the exponential family of distributions. Machine Learning Journal, 43(3):211\u2013246, 2001.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26 (NIPS), pages 773\u2013781, 2013.  Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.  Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.  Ernesto De Vito, Andrea Caponnetto, and Lorenzo Rosasco. Model selection for regularized least- squares algorithm in learning theory. Foundations of Computational Mathematics, 5(1):59\u201385, 2005.  Alexandre D\u00b4efossez and Francis Bach. Constant step size least-mean-square: Bias-variance trade- offs and optimal sampling distributions. In Proceedings of the 25th International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), pages 205\u2013213, 2015.  Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-  sizes. The Annals of Statistics, 44(4):1363\u20131399, 2016.  Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver- gence rates for least-squares regression. Journal of Machine Learning Research, 18(101):1\u201351, 2017.  Amir-massoud Farahmand. Regularization in reinforcement learning. PhD thesis, University of  Alberta, 2011.  12   ITERATE AVERAGING AS REGULARIZATION FOR SGD  The extremely technical proof of this theorem is presented in "}, "Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form", "abstract": "Semidefinite programs (SDP) are important in learning and combinatorial optimization with numerous applications. In pursuit of low-rank solutions and low complexity algorithms, we consider the Burer\u2013Monteiro factorization approach for solving SDPs. For a large class of SDPs, upon random perturbation of the cost matrix, with high probability, we show that all approximate second-order stationary points are approximate global optima for the penalty formulation of appropriately rank-constrained SDPs, as long as the number of constraints scales sub-quadratically with the desired rank. Our result is based on a simple penalty function formulation of the rank-constrained SDP along with a smoothed analysis to avoid worst-case cost matrices. We particularize our results to two applications, namely, Max-Cut and matrix completion.", "pdf_url": "http://proceedings.mlr.press/v75/bhojanapalli18a/bhojanapalli18a.pdf", "keywords": [], "reference": "Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv  preprint arXiv:1703.10146, 2017.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combinato-  rial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Kurt M Anstreicher. The volumetric barrier for semidefinite programming. Mathematics of Opera-  tions Research, 25(3):365-380, 2000.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227-236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite program- ming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Proceedings of The 29th Conference on Learning Theory, COLT 2016, New York, NY, June 23-26, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  A.I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete &  Computational Geometry, 13(1):189-202, 1995. doi: 10.1007/BF02574037.  Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press,  2014.  R. Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.  Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster  semi-definite optimization. In Conference on Learning Theory, pages 530-582, 2016a.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873-3881, 2016b.  13   SMOOTHED ANALYSIS FOR FACTORED, PENALIZED SDPS  NB thanks Dustin Mixon for many interesting conversations on the applicability of smoothed analysis to low-rank SDPs. NB was supported in part by NSF award DMS-1719558. SB thanks Alex Dimakis for pointing to the book by G\u00e4rtner and Matousek (2012) on SDPs.  Acknowledgments  References  Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv  preprint arXiv:1703.10146, 2017.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combinato-  rial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Kurt M Anstreicher. The volumetric barrier for semidefinite programming. Mathematics of Opera-  tions Research, 25(3):365-380, 2000.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227-236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite program- ming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Proceedings of The 29th Conference on Learning Theory, COLT 2016, New York, NY, June 23-26, 2016.  Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143-151. ACM, 2015.  A.I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete &  Computational Geometry, 13(1):189-202, 1995. doi: 10.1007/BF02574037.  Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press,  2014.  R. Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.  Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster  semi-definite optimization. In Conference on Learning Theory, pages 530-582, 2016a.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873-3881, 2016b. SMOOTHED ANALYSIS FOR FACTORED, PENALIZED SDPS  N. Boumal, P.-A. Absil, and C. Cartis. Global rates of convergence for nonconvex optimization on  manifolds. IMA Journal of Numerical Analysis, 2018a. doi: 10.1093/imanum/drx080.  N. Boumal, V. Voroninski, and A.S. Bandeira. Deterministic guarantees for Burer-Monteiro factor-  izations of smooth semidefinite programs. arXiv preprint arXiv:1804.02008, 2018b.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex burer-monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016.  Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite  programs via low-rank factorization. Mathematical Programming, 95(2):329-357, 2003.  Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite  programming. Mathematical Programming, 103(3):427-444, 2005.  Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational mathematics, 9(6):717-772, 2009.  Coralia Cartis, Nicholas IM Gould, and Ph L Toint. Complexity bounds for second-order optimality  in unconstrained optimization. Journal of Complexity, 28(1):93-108, 2012.  Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with  quadratic activation. arXiv preprint arXiv:1803.01206, 2018.  Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067-1077, 2017.  Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruction.  In Proceedings of the 24th Annual Conference on Learning Theory, pages 315-340, 2011.  Dan Garber. Faster projection-free convex optimization over the spectrahedron. In Advances in  Neural Information Processing Systems, pages 874-882, 2016.  Dan Garber and Elad Hazan. Sublinear time algorithms for approximate semidefinite programming.  Mathematical Programming, 158(1-2):329-361, 2016.  Bernd G\u00e4rtner and Jiri Matousek. Approximation algorithms and semidefinite programming. Springer  Science & Business Media, 2012.  Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances in  Neural Information Processing Systems, pages 3656-3666, 2017.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016. SMOOTHED ANALYSIS FOR FACTORED, PENALIZED SDPS  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233-1242, 2017.  Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6): 1115-1145, 1995.  Moritz Hardt. Understanding alternating minimization for matrix completion. arXiv preprint  arXiv:1312.0925, 2013.  Elad Hazan. Sparse approximate solutions to semidefinite programs. In LATIN 2008: Theoretical  Informatics, pages 306-316. Springer, 2008.  U. Helmke and M.A. Shayman. Critical points of matrix least squares distance functions. Linear Al- gebra and its Applications, 215:1-19, 1995. doi: https://doi.org/10.1016/0024-3795(93)00070-G.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the 45th annual ACM Symposium on theory of computing, pages 665-674. ACM, 2013.  Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle  points efficiently. arXiv preprint arXiv:1703.00887, 2017a.  Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle  points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.  Michel Journ\u00e9e, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327-2351, 2010.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. Information Theory, IEEE Transactions on, 56(6):2980-2998, 2010.  Kartik Krishnan and John E Mitchell. Properties of a cutting plane method for semidefinite program-  ming. 2003.  Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine learning research, 5(Jan):27-72, 2004.  Song Mei, Theodor Misiakiewicz, Andrea Montanari, and Roberto Imbuzeiro Oliveira. Solving sdps for synchronization and maxcut problems via the grothendieck inequality. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1476-1515, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR.  Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing,  24(2):227-234, 1995. SMOOTHED ANALYSIS FOR FACTORED, PENALIZED SDPS  Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1): 1665-1697, 2012.  Yurii Nesterov and Arkadi Nemirovski. Polynomial barrier methods in convex programming. Ekonom.  i Mat. Metody, 24(6):1084-1091, 1988.  Yurii Nesterov and Arkadi Nemirovski. Self-concordant functions and polynomial-time methods in convex programming. USSR Academy of Sciences, Central Economic & Mathematic Institute, 1989.  Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.  Mathematical Programming, 108(1):177-205, 2006.  Yurii Nesterov, Arkadi Nemirovski, and Yinyu Ye. Interior-point polynomial algorithms in convex  programming, volume 13. SIAM, 1994.  Hoi H Nguyen. Random matrices: repulsion in spectrum. arXiv preprint arXiv:1709.06682, 2017.  Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial Intelligence and Statistics, pages 65-74, 2017.  G. Pataki. On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues. Mathematics of operations research, 23(2):339-358, 1998. doi: 10.1287/moor.23.2. 339.  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on  pattern analysis and machine intelligence, 22(8):888-905, 2000.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval.  preprint  arXiv:1602.06664, 2016.  preprint arXiv:1411.8003, 2014.  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv  Lieven Vandenberghe and Stephen Boyd. Semidefinite programming. SIAM review, 38(1):49-95,  1996.  R. Vershynin. High-dimensional probability, 2016.  H. Wolkowicz. Some applications of optimization in matrix theory. Linear algebra and its applica-  tions, 40:101-118, 1981.  Alp Yurtsever, Madeleine Udell, Joel Tropp, and Volkan Cevher. Sketchy decisions: Convex low- rank matrix optimization with optimal storage. In Artificial Intelligence and Statistics, pages 1188-1196, 2017. SMOOTHED ANALYSIS FOR FACTORED, PENALIZED SDPS  Zhihui Zhu, Qiuwei Li, Gongguo Tang, and Michael B Wakin. Global optimality in low-rank matrix  optimization. arXiv preprint arXiv:1702.07945, 2017.  "}, "Certified Computation from Unreliable Datasets": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Certified Computation from Unreliable Datasets", "abstract": "A wide range of learning tasks require human input in labeling massive data. The collected data though are usually low quality and contain inaccuracies and errors. As a result, modern science and business face the problem of learning from unreliable data sets. In this work, we provide a generic approach that is based on \\textit{verification} of only few records of the data set to guarantee high quality learning outcomes for various optimization objectives. Our method, identifies small sets of critical records and verifies their validity. We show that many problems only need $\\text{poly}(1/\\varepsilon)$ verifications, to ensure that the output of the computation is at most a factor of $(1 \\pm \\varepsilon)$ away from the truth. For any given instance, we provide an \\textit{instance optimal} solution that verifies the minimum possible number of records to approximately certify correctness. Then using this instance optimal formulation of the problem we prove our main result: \u201cevery function that satisfies some Lipschitz continuity condition can be certified with a small number of verifications\u201d. We show that the required Lipschitz continuity condition is satisfied even by some NP-complete problems, which illustrates the generality and importance of this theorem. In case this certification step fails, an invalid record will be identified. Removing these records and repeating until success, guarantees that the result will be accurate and will depend only on the verified records. Surprisingly, as we show, for several computation tasks more efficient methods are possible. These methods always guarantee that the produced result is not affected by the invalid records, since any invalid record that affects the output will be detected and verified.", "pdf_url": "http://proceedings.mlr.press/v75/gouleakis18a/gouleakis18a.pdf", "keywords": ["unreliable data set", "verification", "Lipschitz continuity"], "reference": "ABCNews.  Rapper  Pitbull  visits Kodiak http://abcnews.go.com/blogs/  entertainment/2012/08/rapper-pitbull-visits-kodiak-alaska-and-gets-bear-repellent/. 2012.  Nir Ailon, Bernard Chazelle, Seshadhri Comandur, and Ding Liu. Property-preserving data re- ISSN 0178-4617. doi: 10.1007/  construction. Algorithmica, 51(2):160-182, April 2008. s00453-007-9075-9. URL http://dx.doi.org/10.1007/s00453-007-9075-9.  Arnab Bhattacharyya, Elena Grigorescu, Madhav Jha, Kyomin Jung, Sofya Raskhodnikova, and David P. Woodruff. Lower bounds for local monotonicity reconstruction from transitive-closure spanners. doi: 10.1137/ 100808186. URL http://dx.doi.org/10.1137/100808186.  SIAM Journal on Discrete Mathematics, 26(2):618-646, 2012.  M. Blum, M. Luby, and R. Rubinfeld. Self-testing/correcting with applications to numerical prob- In Proceedings of the Twenty-second Annual ACM Symposium on Theory of Comput- ISBN 0-89791-361-2. doi:  lems. ing, STOC \u201990, pages 73-83, New York, NY, USA, 1990. ACM. 10.1145/100216.100225. URL http://doi.acm.org/10.1145/100216.100225.  Cl\u00b4ement L. Canonne, Dana Ron, and Rocco A. Servedio. Testing equivalence between distributions using conditional samples. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, pages 1174-1192, 2014.  Clement L. Canonne, Themis Gouleakis, and Ronitt Rubinfeld. Sampling correctors. In Proceed- ings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ITCS \u201916, ISBN 978-1-4503-4057-1. doi: 10.1145/ pages 93-102, New York, NY, USA, 2016. ACM. 2840728.2840729. URL http://doi.acm.org/10.1145/2840728.2840729.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47-60. ACM, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893, 2017a.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 73-84. IEEE, 2017b.  13   CERTIFIED COMPUTATION FROM UNRELIABLE DATASETS  The authors were supported by NSF CCF-1551875, CCF-1617730, CCF-1733808, and IIS-1741137.  Acknowledgements  References  ABCNews.  Rapper  Pitbull  visits Kodiak http://abcnews.go.com/blogs/  entertainment/2012/08/rapper-pitbull-visits-kodiak-alaska-and-gets-bear-repellent/. 2012.  Nir Ailon, Bernard Chazelle, Seshadhri Comandur, and Ding Liu. Property-preserving data re- ISSN 0178-4617. doi: 10.1007/  construction. Algorithmica, 51(2):160-182, April 2008. s00453-007-9075-9. URL http://dx.doi.org/10.1007/s00453-007-9075-9.  Arnab Bhattacharyya, Elena Grigorescu, Madhav Jha, Kyomin Jung, Sofya Raskhodnikova, and David P. Woodruff. Lower bounds for local monotonicity reconstruction from transitive-closure spanners. doi: 10.1137/ 100808186. URL http://dx.doi.org/10.1137/100808186.  SIAM Journal on Discrete Mathematics, 26(2):618-646, 2012.  M. Blum, M. Luby, and R. Rubinfeld. Self-testing/correcting with applications to numerical prob- In Proceedings of the Twenty-second Annual ACM Symposium on Theory of Comput- ISBN 0-89791-361-2. doi:  lems. ing, STOC \u201990, pages 73-83, New York, NY, USA, 1990. ACM. 10.1145/100216.100225. URL http://doi.acm.org/10.1145/100216.100225.  Cl\u00b4ement L. Canonne, Dana Ron, and Rocco A. Servedio. Testing equivalence between distributions using conditional samples. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, pages 1174-1192, 2014.  Clement L. Canonne, Themis Gouleakis, and Ronitt Rubinfeld. Sampling correctors. In Proceed- ings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ITCS \u201916, ISBN 978-1-4503-4057-1. doi: 10.1145/ pages 93-102, New York, NY, USA, 2016. ACM. 2840728.2840729. URL http://doi.acm.org/10.1145/2840728.2840729.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceed- ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47-60. ACM, 2017.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. arXiv preprint arXiv:1703.00893, 2017a.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Foundations of Computer Science (FOCS), 2017 IEEE 58th Annual Symposium on, pages 73-84. IEEE, 2017b. CERTIFIED COMPUTATION FROM UNRELIABLE DATASETS  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a gaussian: Getting optimal error, efficiently. In Proceedings of the Twenty- Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2683-2702, 2018. doi: 10.1137/1.9781611975031.171. URL https://doi.org/10.1137/1.9781611975031.171.  Anhai Doan, Raghu Ramakrishnan, and Alon Y Halevy. Crowdsourcing systems on the world-wide  web. Communications of the ACM, 54(4):86-96, 2011.  Dimitris Fotakis, Christos Tzamos, and Manolis Zampetakis. Mechanism design with selective verification. In Proceedings of the 2016 ACM Conference on Economics and Computation, pages 771-788. ACM, 2016.  Oded Goldreich, Shafi Goldwasser, and Dana Ron. Property testing and its connection to learning and approximation. In 37th Annual Symposium on Foundations of Computer Science, FOCS \u201996, Burlington, Vermont, USA, 14-16 October, 1996, pages 339-348, 1996. doi: 10.1109/SFCS. 1996.548493. URL https://doi.org/10.1109/SFCS.1996.548493.  Themistoklis Gouleakis, Christos Tzamos, and Manolis Zampetakis. Faster sublinear algorithms using conditional sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1743-1757. SIAM, 2017.  Frank R Hampel, Peter J Rousseeuw, Elvezio M Ronchetti, and Werner A Stahel. Robust statistics:  the approach based on in\ufb02uence functions. 1980.  Peter J Huber. Robust statistics. In International Encyclopedia of Statistical Science, pages 1248-  1251. Springer, 2011.  M. Jha and S. Raskhodnikova. Testing and reconstruction of lipschitz functions with applications to data privacy. In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science, pages 433-442, Oct 2011. doi: 10.1109/FOCS.2011.13.  Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 205-214. ACM, 2011.  Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 665-674. IEEE, 2016.  Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley Series in Probability and Statistics. John Wiley & Sons, 2002. ISBN 9780471183860. Second edition.  Donald B. Rubin. Multiple imputation for nonresponse in surveys. John Wiley & Sons, 1987.  Michael Saks and C. Seshadhri. Local monotonicity reconstruction. SIAM Journal on Computing, 39(7):2897-2926, 2010. doi: 10.1137/080728561. URL http://dx.doi.org/10.1137/ 080728561. CERTIFIED COMPUTATION FROM UNRELIABLE DATASETS  Joseph L. Schafer. Analysis of incomplete multivariate data. CRC press, 1997.  Jacob Steinhardt, Gregory Valiant,  and Moses Charikar.  Adversarial Information Processing  and delinquents: in Neural ral Spain, 6440-avoiding-imposters-and-delinquents-adversarial-crowdsourcing-and-peer-prediction.  imposters In Advances crowdsourcing and peer prediction. 29: on Neu- 2016, December Barcelona, 5-10, URL http://papers.nips.cc/paper/  Information Processing  Annual Conference  4439-4447,  Avoiding  Systems  Systems  pages  2016.  2016,  Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. How much spam can you take? an analysis of crowdsourcing results to increase accuracy. In Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR\u201911), pages 21-26, 2011.  Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a high-quality workforce with me- chanical turk. Proceedings of computational social science and the wisdom of crowds (NIPS), pages 1-5, 2010. CERTIFIED COMPUTATION FROM UNRELIABLE DATASETS  "}, "Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon", "abstract": "In reinforcement learning (RL), problems with long planning horizons are perceived as very challenging. The recent advances in PAC RL, however, show that the sample complexity of RL does not depend on planning horizon except at a superficial level. How can we explain such a difference? Noting that the technical assumptions in these upper bounds might have hidden away the challenges of long horizons, we ask the question: \\emph{can we prove a lower bound with a horizon dependence when such assumptions are removed?} We also provide a few observations on the desired characteristics of the lower bound construction.", "pdf_url": "http://proceedings.mlr.press/v75/jiang18a/jiang18a.pdf", "keywords": ["reinforcement learning", "sample complexity", "planning horizon"], "reference": "Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In  Advances in Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00b4emi Munos. Minimax regret bounds for reinforcement learning. In  Proceedings of the 34th International Conference on Machine Learning, pages 263-272, 2017.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances  in Neural Information Processing Systems, 2015.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of  Machine Learning Research, 11(Apr):1563-1600, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision pro-  cesses with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations.  In  Advances in Neural Information Processing Systems, 2016.  Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In Algorithmic Learning Theory (ALT), 2012.  Andrew Ng. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings  of the 16th International Conference on Machine Learning, 1999.  Ian Osband and Benjamin Van Roy. On lower bounds for regret  in reinforcement learning.  arXiv preprint  arXiv:1608.02732, 2016.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstrac-  tion in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.  4   OPEN PROBLEM: THE DEPENDENCE OF SAMPLE COMPLEXITY LOWER BOUNDS ON PLANNING HORIZON  varies in the family of MDPs of interest, the upper bound argument will be broken as there are HAS \u201c1-switch\u201d non-stationary policies. In fact, we find that such a situation can be created by adding a small amount of instantaneous reward to the lazy-chain style constructions. The difficulty is that the algorithm may not need to know the switching timing precisely. In the cases we have inspected, the algorithm can basically discretize [H] into intervals of length O((cid:15)H) and guarantee that one of those O(1/(cid:15)) switching timings is (cid:15)-optimal. Thus, the construction is still subject to a variant of the Monte-Carlo upper bound argument.  2.4. Alternative formulations  The results for regret minimization are in similar situations. The lower and upper bounds for the episodic setting are closed under reward uniformity and asymptotic assumptions (Azar et al., 2017). In the average-reward case, the notion of horizon is replaced by the MDP\u2019s diameter, D; here the lower and the upper bounds still have a gap of  D (Jaksch et al., 2010; Agrawal and Jia, 2017).  \u221a  We also welcome resolution of our problem in more realistic and challenging settings beyond tabular RL, such as rich observations and function approximation (Krishnamurthy et al., 2016; Jiang et al., 2017). While a richer setting enables more powerful lower bounds, existing work have not leveraged the power yet (e.g., Jiang et al., 2017, Thm 6 still uses a tabular construction).  Acknowledgements We thank Christoph Dann and John Langford for insightful discussions.  References  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In  Advances in Neural Information Processing Systems, 2017.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00b4emi Munos. Minimax regret bounds for reinforcement learning. In  Proceedings of the 34th International Conference on Machine Learning, pages 263-272, 2017.  Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Advances  in Neural Information Processing Systems, 2015.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of  Machine Learning Research, 11(Apr):1563-1600, 2010.  Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision pro-  cesses with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, 2017.  Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observations.  In  Advances in Neural Information Processing Systems, 2016.  Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In Algorithmic Learning Theory (ALT), 2012.  Andrew Ng. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings  of the 16th International Conference on Machine Learning, 1999.  Ian Osband and Benjamin Van Roy. On lower bounds for regret  in reinforcement learning.  arXiv preprint  arXiv:1608.02732, 2016.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstrac-  tion in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999. "}, "Open problem: Improper learning of mixtures of Gaussians": {"volumn": "v75", "url": "http://proceedings.mlr.press/v75/", "header": "Open problem: Improper learning of mixtures of Gaussians", "abstract": "We ask whether there exists an efficient unsupervised learning algorithm for mixture of Gaussians in the over-complete case (number of mixtures is larger than the dimension). The notion of learning is taken to be worst-case compression-based, to allow for improper learning.", "pdf_url": "http://proceedings.mlr.press/v75/hazan18a/hazan18a.pdf", "keywords": [], "reference": "Sanjeev Arora, Ravi Kannan, et al. Learning mixtures of separated nonspherical gaussians. The  Annals of Applied Probability, 15(1A):69-92, 2005.  Sanjoy Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engi-  neering, University of California, San Diego, 2008.  Moritz Hardt. Blog post. http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html, 2014.  Published: 2014-04-22.  Elad Hazan and Tengyu Ma. A non-generative framework and convex relaxations for unsupervised  learning. In Advances in Neural Information Processing Systems, pages 3306-3314, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Communica-  tions of the ACM, 55(2):113-120, 2012.  Konstantin Makarychev, Yury Makarychev, Maxim Sviridenko, and Justin Ward. A bi-criteria  approximation algorithm for k means. arXiv preprint arXiv:1507.04227, 2015.  3   Open Problem: Improper Learning of Mixtures of Gaussians  where M \u2212\u2020 denotes the pseudo inverse, and the objective becomes  (cid:107)x \u2212 M \u03b1x(cid:107)2 = (cid:107)x \u2212 M M \u2212\u2020x(cid:107)2  min \u03b1x  Thus, we\u2019re left with the optimization problem of  Ex\u223cD[(cid:107)x \u2212 M M \u2020x(cid:107)2]  min M  which amounts to PCA.  Thus, by finding the k-PCA of the data, we can ensure reconstruction error less than the best k-MOG model, which satisfies our definition of improper unsupervised learning. PCA is also an e\ufb03cient algorithm.  In terms of representation size, however, things are not as satisfying. To represent the encoding by PCA up to (cid:15) error, one needs k log R (cid:15) bits. This is in contrast to the log k bits required by k-MOG. This is also the reason why PCA only works up to k = d, i.e. the underconstrained case.  4. The Question  We o\ufb00er 100$ for the solver of the following question: design a simple and e\ufb03cient algorithm for k-MOG with only poly(log(k), log 1 (cid:15) ) compression size, that works for the overconstrained case. Bi-criteria approximation algorithms for k-means may be a good place to start, such as in (Makarychev et al., 2015). Alternatively, prove an impossibility for the existence of any such e\ufb03cient algorithm.  An additional 100$ is o\ufb00ered for a solution of the analogue of this question to the hypothesis class of general Gaussians, that may have non-identity convariance. Spectral auto-encoders may be a good place to start (Hazan and Ma, 2016).  References  Sanjeev Arora, Ravi Kannan, et al. Learning mixtures of separated nonspherical gaussians. The  Annals of Applied Probability, 15(1A):69-92, 2005.  Sanjoy Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engi-  neering, University of California, San Diego, 2008.  Moritz Hardt. Blog post. http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html, 2014.  Published: 2014-04-22.  Elad Hazan and Tengyu Ma. A non-generative framework and convex relaxations for unsupervised  learning. In Advances in Neural Information Processing Systems, pages 3306-3314, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. In Conference on Learning Theory, pages 38-1, 2012.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Communica-  tions of the ACM, 55(2):113-120, 2012.  Konstantin Makarychev, Yury Makarychev, Maxim Sviridenko, and Justin Ward. A bi-criteria  approximation algorithm for k means. arXiv preprint arXiv:1507.04227, 2015. Open Problem: Improper Learning of Mixtures of Gaussians  Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated gaussians.  arXiv preprint arXiv:1710.11592, 2017. "}}