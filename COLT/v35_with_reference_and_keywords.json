{"Preface": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Preface", "abstract": "", "pdf_url": "http://proceedings.mlr.press/v35/balcan14.pdf", "keywords": []}, "Open Problem: Tightness of maximum likelihood semidefinite relaxations": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Tightness of maximum likelihood semidefinite relaxations", "abstract": "We have observed an interesting, yet unexplained, phenomenon: Semidefinite programming (SDP) based relaxations of maximum likelihood estimators (MLE) tend to be tight in recovery problems with noisy data, even when MLE cannot exactly recover the ground truth. Several results establish tightness of SDP based relaxations in the regime where exact recovery from MLE is possible. However, to the best of our knowledge, their tightness is not understood beyond this regime. As an illustrative example, we focus on the generalized Procrustes problem.", "pdf_url": "http://proceedings.mlr.press/v35/bandeira14.pdf", "keywords": ["Convex relaxations", "Maximum likelihood estimator", "Procrustes problem"], "reference": "E. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Linear inverse problems on Erd\u02ddos-R\u00b4enyi graphs: Information-theoretic limits and efficient recovery. IEEE International Symposium on Information Theory (ISIT2014), to appear, 2014.  D. Amelunxen and P. B\u00a8urgisser. Intrinsic volumes of symmetric cones and applications in convex  programming. Mathematical Programming, pages 1-26, 2014.  A. S. Bandeira, C. Kennedy, and A. Singer. Approximating the little grothendieck problem over the  orthogonal and unitary groups. Available online at arXiv:1308.5207 [cs.DS], 2013.  A. S. Bandeira, M. Charikar, A. Singer, and A. Zhu. Multireference alignment using semidefinite  programming. 5th Innovations in Theoretical Computer Science (ITCS 2014), 2014.  E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. Infor-  mation Theory, IEEE Transactions on, 56(5):2053-2080, May 2010.  K. N. Chaudhury, Y. Khoo, and A. Singer. Global registration of multiple point clouds using  semidefinite programming. arXiv:1306.5226 [cs.CV], 2013.  A. Nemirovski. Sums of random symmetric matrices and quadratic optimization under orthogonal-  ity constraints. Math. Program., 109(2-3):283-317, 2007.  O. Ozyesil, A. Singer, and R. Basri. Camera motion estimation by convex programming. Available  online at http://arxiv.org/abs/1312.5047 [cs.CV], 2013.  3  00.250.50.75100.20.40.60.81\u03c3p00.250.50.75100.20.40.60.81\u03c3p OPEN PROBLEM: TIGHTNESS OF MAXIMUM LIKELIHOOD SEMIDEFINITE RELAXATIONS  Figure 1: Fraction of trials (among 100) on which rank recovery was observed, for various values of noise \u03c3. The left plot corresponds to the Procrustes problem (setting of Conjecture 1) when d = 3, n = 20, m = 30 and the right plot corresponds to the multi reference alignment problem treated in Bandeira et al. (2014). Both plots suggest that rank recovery happens with high probability, below certain noise levels.  The multireference alignment consists in estimating a d-dimensional signal by observing n shifted noisy copies of it. For the sake of brevity, we will not describe the problem or the SDP based relaxation here (and refer the reader to Bandeira et al. (2014)) but take the opportunity to con- jecture that a similar phenomenon happens: Below a certain noise level the solution to SDP-based relaxation in Bandeira et al. (2014) has rank d, thus matching the quasi-MLE. Although not going into details, we note that the SDP for this problem is considerably different than (3), in particular, it has \u2126 (cid:0)n2d2(cid:1) positivity constraints. Also, this problem is discrete and so exact recovery is possible. However, exact recovery can be shown to be only possible for asymptotically vanishing levels of noise and here we conjecture rank recovery happens for a constant level of noise.  References  E. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Linear inverse problems on Erd\u02ddos-R\u00b4enyi graphs: Information-theoretic limits and efficient recovery. IEEE International Symposium on Information Theory (ISIT2014), to appear, 2014.  D. Amelunxen and P. B\u00a8urgisser. Intrinsic volumes of symmetric cones and applications in convex  programming. Mathematical Programming, pages 1-26, 2014.  A. S. Bandeira, C. Kennedy, and A. Singer. Approximating the little grothendieck problem over the  orthogonal and unitary groups. Available online at arXiv:1308.5207 [cs.DS], 2013.  A. S. Bandeira, M. Charikar, A. Singer, and A. Zhu. Multireference alignment using semidefinite  programming. 5th Innovations in Theoretical Computer Science (ITCS 2014), 2014.  E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. Infor-  mation Theory, IEEE Transactions on, 56(5):2053-2080, May 2010.  K. N. Chaudhury, Y. Khoo, and A. Singer. Global registration of multiple point clouds using  semidefinite programming. arXiv:1306.5226 [cs.CV], 2013.  A. Nemirovski. Sums of random symmetric matrices and quadratic optimization under orthogonal-  ity constraints. Math. Program., 109(2-3):283-317, 2007.  O. Ozyesil, A. Singer, and R. Basri. Camera motion estimation by convex programming. Available  online at http://arxiv.org/abs/1312.5047 [cs.CV], 2013.00.250.50.75100.20.40.60.81\u03c3p00.250.50.75100.20.40.60.81\u03c3p "}, "Open Problem: A (Missing) Boosting-type Convergence Result for AdaBoost.MH with Factorized Multi-class Classifiers": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: A (Missing) Boosting-type Convergence Result for AdaBoost.MH with Factorized Multi-class Classifiers", "abstract": "In (K\u00e9gl, 2014), we recently showed empirically that AdaBoost.MH is one of the best multi-class boosting algorithms when the classical one-against-all base classifiers, proposed in the seminal paper of Schapire and Singer\u00a0(1999), are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector. In a slightly different setup, a similar factorization coupled with an iterative optimization of the two factors also proved to be an excellent approach (Gao and Koller, 2011). The main algorithmic advantage of our approach over the original setup of Schapire and Singer\u00a0(1999) is that trees can be built in a straightforward way by using the binary classifier at inner nodes. In this open problem paper we take a step back to the basic setup of boosting generic multi-class factorized (Hamming) classifiers (so no trees), and state the classical problem of boosting-like convergence of the training error. Given a vote vector, training the classifier leads to a standard weighted binary classification problem. The main difficulty of proving the convergence is that, unlike in binary AdaBoost, the sum of the weights in this weighted binary classification problem is less than one, which means that the lower bound on the edge, coming from the weak learning condition, shrinks. To show the convergence, we need a (uniform) lower bound on the sum of the weights in this derived binary classification problem.", "pdf_url": "http://proceedings.mlr.press/v35/kegl14.pdf", "keywords": []}, "Open Problem: Finding Good Cascade Sampling Processes for the Network Inference Problem": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Finding Good Cascade Sampling Processes for the Network Inference Problem", "abstract": "Information spreads across social and technological networks, but often the network structures are hidden and we only observe the traces left by the diffusion processes, called cascades. It is known that, under a popular continuous-time diffusion model, as long as the model parameters satisfy a natural incoherence condition, it is possible to recover the correct network structure with high probability if we observe O(d^3 \\log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. However, the incoherence condition depends, in a non-trivial way, on the source (node) distribution of the cascades, which is typically unknown. Our open problem is whether it is possible to design an active algorithm which samples the source locations in a sequential manner and achieves the same or even better sample complexity, e.g., o(d_i^3 \\log N), than previous work.", "pdf_url": "http://proceedings.mlr.press/v35/gomezrodriguez14.pdf", "keywords": [], "reference": "207-214, 2005.  E. Adar and L. A. Adamic. Tracking Information Epidemics in Blogspace.  In Web Intelligence, pages  H. Daneshmand, M. Gomez-Rodriguez, L. Song, and B. Sch\u00a8olkopf. Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm. In Proc. of the 31st International Conference on Machine Learning (ICML), 2014.  N. Du, L. Song, A. Smola, and M. Yuan. Learning Networks of Heterogeneous In\ufb02uence. In Advances in  Neural Information Processing Systems (NIPS), 2012.  3   THE NETWORK INFERENCE PROBLEM  from the model. Suppose that the regularization parameter \u03bbn is selected to satisfy  \u03bbn \u2265 8k3  (cid:114)  2 \u2212 \u03b5 \u03b5  log N n  .  n > Ld3  i log N,  Then, under some technical conditions, there exist positive constants L and K, independent of (n, N, di), such that if  then the following properties hold with probability at least 1 \u2212 2 exp(\u2212K\u03bb2  nn):  1. For each node i \u2208 V, the l1-regularized network inference problem defined in Eq. 2 has a  unique solution, and so uniquely specifies a set of incoming edges of node i.  2. For each node i \u2208 V, the estimated set of incoming edges does not include any false edges  (3)  (4)  and include all true edges.  3. Active Source Sampling  The success of the network inference algorithm in equation (2) relies on the fulfillment of the above mentioned incoherence condition on the Hessian, Q\u2217, of the population log-likelihood E[(cid:96)n], where the expectation here is taken over the distribution P(s) of the source nodes, and the random gene- rative process of the diffusion model given a source node s. This condition captures the intuition that, node i and any of its neighbors should get infected together in a cascade more often than node i and any of its non-neighbors. Unfortunately, the incoherence condition depends, in a non-trivial way, on the network structure, diffusion parameters, and the source distribution P(s) (Daneshmand et al., 2014), which are all unknown during the network inference stage.  Previous work has typically assumed the network structure, diffusion parameters, observation window and source distribution to be fixed, and source locations are sampled passively from the latter. However, in practice, the source locations to sample from may be determined actively in a sequential manner, potentially based on the information gathered from previous source locations. Therefore, we propose the following open problem:  Open Problem: Suppose there exists an unknown P(s) where the incoherence conditions hold for the diffusion model. Under what conditions, can we design an \u201cactive\u201d algorithm which samples the source location intelligently and achieves the sample complexity in Theo- rem 1, or even better sample complexity, e.g., o(d3  i log N )?  References  207-214, 2005.  E. Adar and L. A. Adamic. Tracking Information Epidemics in Blogspace.  In Web Intelligence, pages  H. Daneshmand, M. Gomez-Rodriguez, L. Song, and B. Sch\u00a8olkopf. Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm. In Proc. of the 31st International Conference on Machine Learning (ICML), 2014.  N. Du, L. Song, A. Smola, and M. Yuan. Learning Networks of Heterogeneous In\ufb02uence. In Advances in  Neural Information Processing Systems (NIPS), 2012. GOMEZ-RODRIGUEZ SONG SCH \u00a8OLKOPF  Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha.  Scalable in\ufb02uence estimation in continuous-time diffusion networks. In Advances in Neural Information Processing Systems, pages 3147- 3155, 2013a.  Nan Du, Le Song, Hyenkyun Woo, and Hongyuan Zha. Uncover topic-sensitive information diffusion net- works. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 229-237, 2013b.  M. Gomez Rodriguez, D. Balduzzi, and B. Sch\u00a8olkopf. Uncovering the Temporal Dynamics of Diffusion  Networks. In Proc. of the 28th International Conference on Machine Learning (ICML), 2011.  M. Gomez-Rodriguez, J. Leskovec, and A. Krause. Inferring Networks of Diffusion and In\ufb02uence. In Proc. of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2010.  E. M. Rogers. Diffusion of Innovations. Free Press, New York, fourth edition, 1995. "}, "Open Problem: Tensor Decompositions: Algorithms up to the Uniqueness Threshold?": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Tensor Decompositions: Algorithms up to the Uniqueness Threshold?", "abstract": "", "pdf_url": "http://proceedings.mlr.press/v35/bhaskara14b.pdf", "keywords": [], "reference": "Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of tensor decompo-  sitions with applications to polynomial identifiability. CoRR, abs/1304.8087, 2013.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In Symposium on the Theory of Computing (STOC), 2014.  L. De Lathauwer, J. Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of  underdetermined mixtures. IEEE Trans. on Signal Processing, 55(6):2965-2973, 2007.  Navin Goyal, Santosh Vempala and Ying Xiao. Fourier PCA.  In Symposium on the Theory of  Computing (STOC), 2014.  Richard A Harshman. Foundations of the parafac procedure: models and conditions for an explana-  tory multimodal factor analysis. 1970.  Joseph B Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with appli- cation to arithmetic complexity and statistics. Linear algebra and applications, 18(2):95-138, 1977.  3   OPEN PROBLEM: TENSOR DECOMPOSITIONS: ALGORITHMS UP TO THE UNIQUENESS THRESHOLD?  To put this in context, Jennrich\u2019s uniqueness theorem is proven by giving an algorithm for decom- posing T and identifying conditions under which it is stable to noise (we need this in applications to learning if we want finite sample complexity bounds) Goyal et al. (2013), Bhaskara et al. (2014). Is there an algorithmic proof of the above uniqueness theorem or its robust analogues Bhaskara et al. (2013)?  If we move to higher order tensors then we can handle R >> n but by \ufb02attening the tensor and reducing back to an order three tensor with larger dimensions, and invoking Jennrich\u2019s algorithm as depicted in the following equation:  T =  R (cid:88)  i=1  ai \u2297 bi (cid:124) (cid:123)(cid:122) (cid:125) factor  \u2297 ci \u2297 di (cid:124) (cid:123)(cid:122) (cid:125) factor  \u2297 ei  (cid:124)(cid:123)(cid:122)(cid:125) factor  This approach is robust to noise in various settings, including in the model of smoothed analysis Bhaskara et al. (2014). De Lathauwer et al. (2007) gives algorithms for R = \u2126(n2) for fourth order tensors through an entirely different approach.  But the limits of what should algorithmically be possible for order three tensors are far from It could be that it is computationally hard to decompose T even under the conditions of clear. Kruskal\u2019s uniqueness theorem. We also remark that if our only condition on the factors involves only their Kruskal rank, then it is known that Kruskal\u2019s uniqueness theorem is tight. A random third order tensor generated by choosing its R factors uniformly at random will be unique even for R = \u2126(n2), but there are analogies with open problems in circuit complexity that lead us to believe that finding a decomposition might be hard in this random setting even though it is unique.  A reward of $100 is offered for a resolution of this open problem.  References  Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of tensor decompo-  sitions with applications to polynomial identifiability. CoRR, abs/1304.8087, 2013.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In Symposium on the Theory of Computing (STOC), 2014.  L. De Lathauwer, J. Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of  underdetermined mixtures. IEEE Trans. on Signal Processing, 55(6):2965-2973, 2007.  Navin Goyal, Santosh Vempala and Ying Xiao. Fourier PCA.  In Symposium on the Theory of  Computing (STOC), 2014.  Richard A Harshman. Foundations of the parafac procedure: models and conditions for an explana-  tory multimodal factor analysis. 1970.  Joseph B Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with appli- cation to arithmetic complexity and statistics. Linear algebra and applications, 18(2):95-138, 1977. "}, "Open Problem: The Statistical Query Complexity of Learning Sparse Halfspaces": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: The Statistical Query Complexity of Learning Sparse Halfspaces", "abstract": "We consider the long-open problem of attribute-efficient learning of halfspaces. In this problem the learner is given random examples labeled by an unknown halfspace function f on \\mathbbR^n. Further f is r-sparse, that is it depends on at most r out of n variables. An attribute-efficient learning algorithm is an algorithm that can output a hypothesis close to f using a polynomial in r and \\log n number of examples (Blum, 1992). Despite a number of attempts and some partial progress, there are no efficient algorithms or hardness results for the problem. We propose a potentially easier question: what is the query complexity of this learning problem in the statistical query (SQ) model of Kearns (1998). We show that, as in the case of general PAC learning, the query complexity of attribute-efficient SQ learning of any concept class can be characterized by a combinatorial parameter of the concept class. The proposed question is then equivalent to estimating the value of this parameter for the concept class of halfspaces. A potentially simpler problem is to estimate this parameter for the concept class of decision lists, a subclass of halfspaces.", "pdf_url": "http://proceedings.mlr.press/v35/feldman14c.pdf", "keywords": [], "reference": "Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In Proceedings of STOC, pages 253-262, 1994.  Avrim Blum. Learning boolean functions in an infinite attribute space. Machine Learning, 9:373-  386, 1992.  H. Buhrman, N. Vereshchagin, and R. de Wolf. On computation and communication with small bias. In Proceedings of IEEE Conference on Computational Complexity, pages 24-32, 2007.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning  halfspaces over sparse vectors. In NIPS, pages 145-153, 2013.  S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. SIAM Journal on Com-  puting, 29(3):854-879, 1999.  V. Feldman. Attribute efficient and non-adaptive learning of parities and DNF expressions. Journal  of Machine Learning Research, (8):1431-1460, 2007.  V. Feldman. Evolvability from learning algorithms. In Proceedings of STOC, pages 619-628, 2008.  V. Feldman. Distribution-specific agnostic boosting. In Proceedings of Innovations in Computer  Science, pages 241-250, 2010.  V. Feldman. A complete characterization of statistical query learning with applications to evolv-  ability. Journal of Computer System Sciences, 78(5):1444-1459, 2012.  Vitaly Feldman and Varun Kanade. Computational bounds on statistical query learning. In COLT,  pages 16.1-16.22, 2012.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo-  rithms and a lower bound for planted clique. In STOC, pages 655-664. ACM, 2013.  M. Goldmann, J. H\u02daastad, and A. Razborov. Majority gates vs. general weighted threshold gates.  Computational Complexity, 2:277-300, 1992.  J. H\u02daastad. On the size of weights for threshold gates. SIAM Journal on Discrete Mathematics, 7(3):  484-492, 1994.  D. Haussler. Quantifying inductive bias: AI learning algorithms and Valiant\u2019s learning framework.  Artificial Intelligence, 46(2):177-221, 1988.  J. Jackson and M. Craven. Learning sparse perceptrons. In Advances in Neural Information Pro-  cessing Systems 8, pages 654-660, 1996.  4   FELDMAN  References  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal  component detection. In COLT, pages 1046-1066, 2013.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In Proceedings of STOC, pages 253-262, 1994.  Avrim Blum. Learning boolean functions in an infinite attribute space. Machine Learning, 9:373-  386, 1992.  H. Buhrman, N. Vereshchagin, and R. de Wolf. On computation and communication with small bias. In Proceedings of IEEE Conference on Computational Complexity, pages 24-32, 2007.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning  halfspaces over sparse vectors. In NIPS, pages 145-153, 2013.  S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. SIAM Journal on Com-  puting, 29(3):854-879, 1999.  V. Feldman. Attribute efficient and non-adaptive learning of parities and DNF expressions. Journal  of Machine Learning Research, (8):1431-1460, 2007.  V. Feldman. Evolvability from learning algorithms. In Proceedings of STOC, pages 619-628, 2008.  V. Feldman. Distribution-specific agnostic boosting. In Proceedings of Innovations in Computer  Science, pages 241-250, 2010.  V. Feldman. A complete characterization of statistical query learning with applications to evolv-  ability. Journal of Computer System Sciences, 78(5):1444-1459, 2012.  Vitaly Feldman and Varun Kanade. Computational bounds on statistical query learning. In COLT,  pages 16.1-16.22, 2012.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo-  rithms and a lower bound for planted clique. In STOC, pages 655-664. ACM, 2013.  M. Goldmann, J. H\u02daastad, and A. Razborov. Majority gates vs. general weighted threshold gates.  Computational Complexity, 2:277-300, 1992.  J. H\u02daastad. On the size of weights for threshold gates. SIAM Journal on Discrete Mathematics, 7(3):  484-492, 1994.  D. Haussler. Quantifying inductive bias: AI learning algorithms and Valiant\u2019s learning framework.  Artificial Intelligence, 46(2):177-221, 1988.  J. Jackson and M. Craven. Learning sparse perceptrons. In Advances in Neural Information Pro-  cessing Systems 8, pages 654-660, 1996. OPEN PROBLEM: THE STATISTICAL QUERY COMPLEXITY OF LEARNING SPARSE HALFSPACES  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  983-1006, 1998.  Adam R. Klivans and Rocco A. Servedio. Toward attribute efficient learning of decision lists and  parities. Journal of Machine Learning Research, 7:587-602, 2006.  N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algo-  rithm. Machine Learning, 2:285-318, 1987.  Philip M. Long and Rocco A. Servedio. Attribute-efficient learning of decision lists and linear  threshold functions under unconcentrated distributions. In NIPS, pages 921-928, 2006.  R. Servedio. Computational sample complexity and attribute-efficient learning. Journal of Com-  puter and System Sciences, 60(1):161-178, 2000.  R. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning  Research, 4:633-648, 2003. ISSN 1533-7928.  Rocco A. Servedio, Li-Yang Tan, and Justin Thaler. Attribute-efficient learning and weight-degree  tradeoffs for polynomial threshold functions. In COLT, pages 14.1-14.19, 2012.  Shai Shalev-Shwartz, Ohad Shamir, and Eran Tromer. Using more data to speed-up training time.  In AISTATS, pages 1019-1027, 2012.  Alexander A. Sherstov. Halfspace matrices. Computational Complexity, 17(2):149-178, 2008.  B. Sz\u00a8or\u00b4enyi. Characterizing statistical query learning:simplified notions and proofs. In Proceedings  of ALT, pages 186-200, 2009.  222-227, 1977.  Andrew Yao. Probabilistic computations: Toward a unified measure of complexity. In FOCS, pages  "}, "Open Problem: Online Local Learning": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Online Local Learning", "abstract": "In many learning problems, we attempt to infer \\emphglobal structure in the interest of making \\emphlocal predictions.  For example, we might try to infer the skills of the competitors in a tournament in order to predict who will win a match, or we might try to predict characteristics of users and films in order to predict which users will like which films.  In even relatively simple settings of this type, it is typically NP-hard to find the latent data which best explain some observations.  But do these complexity-theoretic obstructions actually prevent us from making good predictions?  Because each prediction depends on only a small number of variables, it might be possible to make good predictions without actually finding a good global assignment.  This may seem to be a purely technical distinction, but recent work has shown that several local prediction problems actually \\emphare easy even though the corresponding global inference problem is hard.  The question we pose is: how general is this phenomenon?", "pdf_url": "http://proceedings.mlr.press/v35/christiano14.pdf", "keywords": [], "reference": "Jacob Abernethy.  Can we learn to gamble ef\ufb01ciently?  In Adam Tauman Kalai and Mehryar Mohri, editors, COLT, pages 318\u2013319. Omnipress, 2010. ISBN 978-0-9822529-2-5. URL http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings. pdf#page=326.  Paul Christiano. Online local learning via semide\ufb01nite programming. In STOC: ACM Symposium  on Theory of Computing (STOC), 2014.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satis\ufb01ability problems with planted solutions. CoRR, abs/1311.4821, 2013. URL http://arxiv.org/ abs/1311.4821.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix  prediction. CoRR, abs/1204.0136, 2012.  Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. Machine Learning, 80(2-3):245\u2013272, 2010. URL http://dx.doi.org/ 10.1007/s10994-010-5178-7.  Laurent. A comparison of the sherali-adams, lovasz-schrijver, and lasserre relaxations for 0-1 pro-  gramming. MOR: Mathematics of Operations Research, 28, 2003.  Grant Schoenebeck. Linear level lasserre lower bounds for certain k-CSPs. In FOCS, pages 593\u2013 602. IEEE Computer Society, 2008. URL http://doi.ieeecomputersociety.org/ 10.1109/FOCS.2008.74.  4. Appendix  4.1. Online gambling  In the online gambling problem, in each round a learner is given a pair of teams xi, xj and asked to predict which team will win in an upcoming match; after making their prediction, they learn the result of the match. The goal of the learner is to predict as many matches correctly as possible. The learner\u2019s regret is measured against the space of strategies which choose a \ufb01xed ranking of the teams and always predict that the higher-ranked team will win.  3   OPEN PROBLEM: ONLINE LOCAL LEARNING  It is possible that the same SDP relaxations, together with standard approaches to online linear optimization, could be used to achieve optimal regret bounds for r = 2 and general L. In the case of r > 2, however, we show in the "}, "Open Problem: Shifting Experts on Easy Data": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Shifting Experts on Easy Data", "abstract": "A number of online algorithms have been developed that have small additional loss (regret) compared to the best \u201cshifting expert\u201d. In this model, there is a set of experts and the comparator is the best partition of the trial sequence into a small number of segments, where the expert of smallest loss is chosen in each segment. The regret is typically defined for worst-case data / loss sequences. There has been a recent surge of interest in online algorithms that combine good worst-case guarantees with much improved performance on easy data. A practically relevant class of easy data is the case when the loss of each expert is iid and the best and second best experts have a gap between their mean loss. In the full information setting, the FlipFlop algorithm by De Rooij et al. (2014) combines the best of the iid optimal Follow-The-Leader (FL) and the worst-case-safe Hedge algorithms, whereas in the bandit information case SAO by Bubeck and Slivkins (2012) competes with the iid optimal UCB and the worst-case-safe EXP3. We ask the same question for the shifting expert problem. First, we ask what are the simple and efficient algorithms for the shifting experts problem when the loss sequence in each segment is iid with respect to a fixed but unknown distribution. Second, we ask how to efficiently unite the performance of such algorithms on easy data with worst-case robustness. A particular intriguing open problem is the case when the comparator shifts within a small subset of experts from a large set under the assumption that the losses in each segment are iid.", "pdf_url": "http://proceedings.mlr.press/v35/warmuth14.pdf", "keywords": [], "reference": "NIPS, pages 1-9, 2010.  Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past poste-  riors. Journal of Machine Learning Research, 3:363-396, 2002.  3   Open Problem: Shifting Experts on Easy Data  an expert from the current weight vector, the learner predicts with the expert of maximum weight.  The resulting regret bounds for these algorithms should be log n per segment under the assumption that there is fixed gap between the mean loss of the best and the mean loss of the second best in each segment.  Problem: Long-term memory for iid shifting experts One of the most interesting cases of expert algorithms is the long term memory setting where the best expert shifts within a small subset. In the worst case setting, this was solved with a multiplicative update followed by certain mixing updates (Bousquet and Warmuth, 2002; Koolen et al., 2012). What are the optimal algorithms for the long-term memory setting when the segments are assumed to be iid? Intuitively the algorithm has to locally follow the leader but help with recovering to the new local leader when the (hidden) segment changes. It also has to recover faster towards experts that have done well in the past (long term memory).  If n is the total number of experts and s the size of the relevant subset, then the question is whether there is an algorithm that achieves expected regret log n per member in the subset and log s per shift within the subset. Note that this is a version of Yoav Freund\u2019s (Freund, 2000) original open problem which initiated the long-term memory work of Bousquet and Warmuth (2002) that focused on the worst-case.  Problem: IID and worst-case-optimal shifting experts Ideally we want a single algorithm that can handle both the malignant worst-case and the benevolent iid case. For the base problem, this has been done in a whole line of research by fancy tunings of the Hedge algorithm (see Van Erven et al., 2011; De Rooij et al., 2014) and most recently with the FL algorithm applied to the dropout perturbed losses (Van Erven et al., 2014). In the bandit feedback model, the best of both worlds was achieved by Bubeck and Slivkins (2012). However it is not at all clear how these algorithm can be adapted to the shifting expert setting, let alone the long-term memory scenario.  Wrap-up The importance of this open problem stems directly from the practical abun- dance of non-stationary data, and the desire to learn from it using online algorithms. We hope to have sketched an important family of problems. The details of the iid shifting experts problem can be worked out in many ways. One particular choice is whether we want to compare on each segment with the loss of the best expert on the sampled data or in expectation. Another is whether the segmentation is itself generated randomly, or chosen adversarially. In the simplest scenario, the algorithm knows the parameters of the problem like the gap, the number of trials, the number of segments in the comparator partition, and the number of distinct best experts. The ultimate online algorithm should of course adapt to these practically unknown parameters on the \ufb02y.  References  NIPS, pages 1-9, 2010.  Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past poste-  riors. Journal of Machine Learning Research, 3:363-396, 2002. Warmuth Koolen  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial  bandits. Journal of Machine Learning Research - Proceedings Track, 23:42.1-42.23, 2012.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. Mirror Descent meets Fixed  Share (and feels no regret). In NIPS Proceedings, pages 989-997, 2012.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, Hedge if you must. Journal of Machine Learning Research, 2014. To appear.  Yoav Freund. Private communication, 2000.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32:151-178, 1998.  Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal of Machine  Learning Research, 1:281-309, 2001.  Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader.  Journal of Machine Learning Research, 6:639-660, 2005.  Adam Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005. ISSN 0022-0000.  Wouter M. Koolen, Dmitri Adamskiy, and Manfred K. Warmuth. Putting Bayes to sleep.  In Advances in Neural Information Processing Systems (NIPS) 25, pages 135-143, December 2012.  Wojciech Kot(cid:32)lowski. Follow the leader bound in the iid case. Private communication, September  2013.  Nick Littlestone and Manfred K. Warmuth. The Weighted Majority algorithm. Information and  Computation, 108(2):212-261, 1994.  Tim van Erven, Steven de Rooij, Wouter M. Koolen, and Peter Gr\u00a8unwald. Adaptive Hedge. In Advances in Neural Information Processing Systems (NIPS) 24, pages 1656-1664, December 2011.  Tim van Erven, Wojciech Kot(cid:32)lowski, and Manfred K. Warmuth. Follow the leader with dropout  perturbations. In Proceedings of the 27th Annual Conference on Learning Theory, 2014.  Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, 1998. "}, "Open Problem: Efficient Online Sparse Regression": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Open Problem: Efficient Online Sparse Regression", "abstract": "In practical scenarios, it is often necessary to be able to make predictions with very limited access to the features of any example. We provide one natural formulation as an online sparse regression problem with squared loss, and ask whether it is possible to achieve sublinear regret with efficient algorithms (i.e. polynomial running time in the natural parameters of the problem).", "pdf_url": "http://proceedings.mlr.press/v35/kale14b.pdf", "keywords": [], "reference": "Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. E\ufb03cient learning with par- tially observed attributes. Journal of Machine Learning Research, 12:2857-2878, 2011.  Elad Hazan and Tomer Koren. Linear regression with limited observation. In ICML, 2012.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online  convex optimization. Machine Learning, 69(2-3):169-192, 2007.  B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing, 25  (2):227-234, 1995.  William J. Welch. Algorithmic complexity: three NP-hard problems in computational  statistics. Journal of Statistical Computation and Simulation, 15(1):17-25, 1982.  Navid Zolghadr, G\u00b4abor Bart\u00b4ok, Russell Greiner, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari.  Online learning with costly features and labels. In NIPS, pages 1241-1249, 2013.  3. The dependence on d is a small polynomial which we did not attempt to optimize.  3   Open Problem: Efficient Online Sparse Regression  w at time t can be rewritten as  w(cid:62)xtx(cid:62)  t w \u2212 2w(cid:62)(ytxt) + y2 t .  So an unbiased estimator for the loss can be constructed by sampling coordinates and using importance weighting to construct unbiased estimators for the two quantities  xtx(cid:62) t  and  ytxt  (1)  In collaboration with Dean Foster, we can give an algorithm that has better running time, O(dk) per iteration, but worse3 regret, O(poly(d) T ). Also, this algorithm requires access to k(cid:48) \u2265 k + 2 features per example, whereas the comparator is allowed access to only k features.  \u221a  The idea is the following. Treat each subset S of [d] of size at most k as an expert, and run the standard exponential weights algorithm on these experts. Each expert S internally runs stochastic gradient descent on weight vectors that are non-zero only on the coordinates in S. In each round, the learner samples an expert (i.e. a subset St) from the distribution generated by the exponential weights algorithm, obtains the features indexed by St, and uses the internal weight vector of the expert St to make its prediction \u02c6yt. Next, using the extra k(cid:48) \u2212 k features that are available to the learner, it constructs stochastic gradients for all experts by sampling a uniformly random subset of coordinates of size k(cid:48) \u2212 k and using importance weighting to construct unbiased estimators for the quantities in (1). Overall, using the known regret bounds for the exponential weights algorithm and stochastic gradient descent, we obtain the stated regret bound. The running time is dominated by the exponential weights algorithm which needs to update weights for all the experts in each round, and is hence O(dk) per round.  References  Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. E\ufb03cient learning with par- tially observed attributes. Journal of Machine Learning Research, 12:2857-2878, 2011.  Elad Hazan and Tomer Koren. Linear regression with limited observation. In ICML, 2012.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online  convex optimization. Machine Learning, 69(2-3):169-192, 2007.  B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing, 25  (2):227-234, 1995.  William J. Welch. Algorithmic complexity: three NP-hard problems in computational  statistics. Journal of Statistical Computation and Simulation, 15(1):17-25, 1982.  Navid Zolghadr, G\u00b4abor Bart\u00b4ok, Russell Greiner, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari.  Online learning with costly features and labels. In NIPS, pages 1241-1249, 2013.  3. The dependence on d is a small polynomial which we did not attempt to optimize. "}, "Distribution-independent Reliable Learning": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Distribution-independent Reliable Learning", "abstract": "We study several questions in the \\emphreliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than other types. A positive reliable classifier is one that makes no false positive errors.  The goal in the \\emphpositive reliable agnostic framework is to output a hypothesis with the following properties: (i) its false positive error rate is at most \u03b5, (ii) its false negative error rate is at most \u03b5more than that of the best positive reliable classifier from the class.  A closely related notion is \\emphfully reliable agnostic learning, which considers \\emphpartial classifiers that are allowed to predict \u201cunknown\u201d on some inputs. The best fully reliable partial classifier is one that makes no errors and minimizes the probability of predicting \u201cunknown\u201d, and the goal in fully reliable learning is to output a hypothesis that is almost as good as the best fully reliable partial classifier from a class. For distribution-independent learning,  the best known algorithms for PAC learning typically utilize polynomial threshold representations, while the state of the art agnostic learning algorithms use point-wise polynomial approximations.  We show that \\emphone-sided polynomial approximations, an intermediate notion between polynomial threshold representations and point-wise polynomial approximations, suffice for learning in the reliable agnostic settings. We then show that majorities can be fully reliably learned and disjunctions of majorities can be positive reliably learned, through constructions of appropriate one-sided polynomial approximations.  Our fully reliable algorithm for majorities provides the first evidence that fully reliable learning may be strictly easier than agnostic learning.  Our algorithms also satisfy strong attribute-efficiency properties, and in many cases they provide smooth tradeoffs between sample complexity and running time.", "pdf_url": "http://proceedings.mlr.press/v35/kanade14.pdf", "keywords": ["reliable learning", "agnostic learning", "attribute-efficiency", "polynomial approximations"], "reference": "Scott Aaronson and Yaoyun Shi. Quantum lower bounds for the collision and the element distinct-  ness problems. J. ACM, 51(4):595\u2013605, 2004.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.  Peter L. Bartlett and Marten H. Wegkamp. Classi\ufb01cation with a reject option using a hinge loss.  Journal of Machine Learning Research, 9:1823\u20131840, 2008.  Nader H Bshouty and Lynn Burroughs. Maximizing agreements with one-sided error with applica-  tions to heuristic learning. Machine Learning, 59(1-2):99\u2013123, 2005.  13   DISTRIBUTION-INDEPENDENT RELIABLE LEARNING  6. Discussion  We have shown that concept classes with low one-sided approximate degree can be efficiently learned in the reliable agnostic model. As we have seen, one-sided approximate degree is an in- termediate notion that lies between threshold degree and approximate degree; we have identified important concept classes, such as majorities and intersections of majorities, whose one-sided ap- proximate degree is strictly smaller than its approximate degree. Consequently, we have obtained reliable (in some cases, even fully reliable) agnostic learning algorithms that are strictly more effi- cient than the fastest known agnostic ones. We have thereby given the first evidence that even fully reliable agnostic learning may be strictly easier than agnostic learning.  The notion of one-sided polynomial approximation has only been introduced very recently (Bun and Thaler (2013a)), and previously had only been used to prove lower bounds. By giving the first algorithmic application of one-sided polynomial approximations, our work lends further credence to the notion that these approximations are fundamental objects worthy of further study in their own right. Just as threshold degree and approximate degree have found applications (both posi- tive and negative) in many domains outside of learning theory, we hope that one-sided polynomial approximations will as well. Identifying such applications is a significant direction for further work. Our work does raise several open questions specific to one-sided polynomial approximations. Here we highlight two. We have shown that halfspaces of weight at most W have one-sided ap- proximate degree \u02dcO(W 1/2), and yet there exist halfspaces with one-sided approximate degree \u2126(n). However, the (non-explicit) halfspace from Sherstov (2013b) that we used to demonstrate the \u2126(n) lower bound has weight 2\u2126(n) (see "}, "Learning without concentration": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Learning without concentration", "abstract": "We obtain sharp bounds on the convergence rate of Empirical Risk Minimization performed in a convex class and with respect to the squared loss, without any boundedness assumptions on class members or on the target. Rather than resorting to a concentration-based argument, the method relies on a \u2018small-ball\u2019 assumption and thus holds for heavy-tailed sampling and heavy-tailed targets. Moreover, the resulting estimates scale correctly with the \u2018noise level\u2019 of the problem. When applied to the classical, bounded scenario, the method always improves the known estimates.", "pdf_url": "http://proceedings.mlr.press/v35/mendelson14.pdf", "keywords": [], "reference": "33(4):1497-1537, 2005.  P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics,  P.L. Bartlett, S. Mendelson, and J. Neeman. \u21131-regularized linear regression: Persistence and oracle  inequalities. Probability Theory and Related Fields, 154:193-224, 2012.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Independence. Oxford University Press, 2013.  D. Chafa\u00a8\u0131, O. Gu\u00b4edon, G. Lecu\u00b4e, and A. Pajor. Interacrions between compressed sensing, random  matrices and high dimensional geometry, volume 37. SMF, 2012.  V. de la Pe\u02dcna and E. Gin\u00b4e. Decoupling: From Dependence to Independence. Springer-Verlag, 1999.  A.W. Van der Vaart and J.A. Wellner. Weak convergence and empirical processes. Springer Verlag,  E. Gin\u00b4e and J. Zinn. Some limit theorems for empirical processes. Annals of Probability, 12(4):  V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems,  volume 2033 of Lecture notes in Matehmatics. Springer, 2011.  V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix without  concentration. preprint, arXiv:1312.3580.  G. Lecu\u00b4e and S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. preprint,  G. Lecu\u00b4e and S. Mendelson. Compressed sensing under weak moment assumptions. preprint,  1996.  929-989, 1984.  arXiv:1305.4825, a.  arXiv:1401.2188, b.  G. Lecu\u00b4e and S. Mendelson. Necessary moment conditions for exact reconstruction via basis pur-  suit. preprint, arXiv:1404.3116, c.  M. Ledoux and M. Talagrand. Probability in Banach spaces. Isoperimetry and processes, volume 23  of Ergebnisse der Mathematik und ihrer Grenzgebiete. Springer-Verlag, 1991.  S. Mendelson. Learning without concentration, extended version. preprint, arXiv:1401.0304, a.  S. Mendelson. Learning without concentration for general loss functions. preprint, b.  S. Mendelson. A remark on the diameter of random sections of convex bodies.  preprint,  arXiv:1312.3608, c.  sis, 20(4):988-1027, 2010.  S. Mendelson. Empirical processes with a bounded \u03c81 diameter. Geometric and Functional Analy-  S. Mendelson and G. Paouris. On generic chaining and the smallest singular values of random  matrices with heavy tails. Journal of Functional Analysis, 262(9):3775-3811, 2012.  14   MENDELSON  References  33(4):1497-1537, 2005.  P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics,  P.L. Bartlett, S. Mendelson, and J. Neeman. \u21131-regularized linear regression: Persistence and oracle  inequalities. Probability Theory and Related Fields, 154:193-224, 2012.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Independence. Oxford University Press, 2013.  D. Chafa\u00a8\u0131, O. Gu\u00b4edon, G. Lecu\u00b4e, and A. Pajor. Interacrions between compressed sensing, random  matrices and high dimensional geometry, volume 37. SMF, 2012.  V. de la Pe\u02dcna and E. Gin\u00b4e. Decoupling: From Dependence to Independence. Springer-Verlag, 1999.  A.W. Van der Vaart and J.A. Wellner. Weak convergence and empirical processes. Springer Verlag,  E. Gin\u00b4e and J. Zinn. Some limit theorems for empirical processes. Annals of Probability, 12(4):  V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems,  volume 2033 of Lecture notes in Matehmatics. Springer, 2011.  V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix without  concentration. preprint, arXiv:1312.3580.  G. Lecu\u00b4e and S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. preprint,  G. Lecu\u00b4e and S. Mendelson. Compressed sensing under weak moment assumptions. preprint,  1996.  929-989, 1984.  arXiv:1305.4825, a.  arXiv:1401.2188, b.  G. Lecu\u00b4e and S. Mendelson. Necessary moment conditions for exact reconstruction via basis pur-  suit. preprint, arXiv:1404.3116, c.  M. Ledoux and M. Talagrand. Probability in Banach spaces. Isoperimetry and processes, volume 23  of Ergebnisse der Mathematik und ihrer Grenzgebiete. Springer-Verlag, 1991.  S. Mendelson. Learning without concentration, extended version. preprint, arXiv:1401.0304, a.  S. Mendelson. Learning without concentration for general loss functions. preprint, b.  S. Mendelson. A remark on the diameter of random sections of convex bodies.  preprint,  arXiv:1312.3608, c.  sis, 20(4):988-1027, 2010.  S. Mendelson. Empirical processes with a bounded \u03c81 diameter. Geometric and Functional Analy-  S. Mendelson and G. Paouris. On generic chaining and the smallest singular values of random  matrices with heavy tails. Journal of Functional Analysis, 262(9):3775-3811, 2012. LEARNING WITHOUT CONCENTRATION  S. Mendelson and G. Paouris. On the singular values of random matrices. Journal of the European  Mathematics Society, 2014.  S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Reconstruction and subgaussian operators.  Geometric and Functional Analysis, 17(4):1248-1282, 2007. "}, "Uniqueness of Ordinal Embedding": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Uniqueness of Ordinal Embedding", "abstract": "Ordinal embedding refers to the following problem: all we know about an unknown set of \t \t\tpoints x_1,\\ldots, x_n \u2208\\mathbbR^d are ordinal constraints of the form \\|x_i - x_j\\| < \\|x_k - x_l\\|; the task is to construct a realization y_1,\\ldots, y_n \u2208\\mathbbR^d that preserves these ordinal constraints. It has been conjectured since the 1960ies that upon knowledge of all ordinal constraints a large but finite set of points can be approximately reconstructed up to a similarity transformation. The main result of our paper is a formal proof of this conjecture.", "pdf_url": "http://proceedings.mlr.press/v35/kleindessner14.pdf", "keywords": ["non-metric multidimensional scaling", "monotone mapping", "isotonic mapping"], "reference": "S. Agarwal, J. Wills, L. Cayton, G. Lanckriet, D. Kriegman, and S. Belongie. Generalized non- metric multidimensional scaling. In International Conference on Artificial Intelligence and Statis- tics (AISTATS), 2007.  N. Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal  query complexity. JMLR, 13:137-164, 2012.  P. Alestalo, D. Trotsenko, and J. V\u00a8ais\u00a8al\u00a8a. Isometric approximation. Israel Journal of Mathematics,  125:61-82, 2001.  1953.  2005.  F. Beckman and D. Quarles. On isometries of Euclidean spaces. Proc. Amer. Math. Soc., 4:810-815,  I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer,  J. Dattorro. Convex optimization and Euclidean distance geometry. Meboo Publishing, 2005.  K. Jamieson and R. Nowak. Low-dimensional embedding using adaptively selected ordinal data. In  Conference on Communication, Control, and Computing, 2011a.  K. Jamieson and R. Nowak. Active ranking using pairwise comparisons. In Neural Information  Processing Systems (NIPS), 2011b.  Psychometrika, 29(1):1-27, 1964a.  129, 1964b.  J. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.  J. Kruskal. Nonmetric multidimensional scaling: A numerical method. Psychometrika, 29(2):115-  Y. Lan, J. Guo, X. Cheng, and T. Liu. Statistical consistency of ranking methods in a rank-  differentiable probability space. In Neural Information Processing Systems (NIPS), 2012.  B. McFee and G. Lanckriet. Partial order embedding with multiple kernels. In International Con-  ference on Machine Learning (ICML), 2009.  B. McFee and G. Lanckriet. Metric learning to rank.  In International Conference on Machine  Learning (ICML), 2010.  B. McFee and G. Lanckriet. Learning multi-modal similarity. JMLR, 12:491-523, 2011.  M. McKemie and J. V\u00a8ais\u00a8al\u00a8a. Spherical maps of Euclidean spaces. Results in Mathematics, 35:  145-160, 1999.  H. Ouyang and A. Gray. Learning dissimilarities by ranking: from SDP to QP. In International  Conference on Machine Learning (ICML), 2008.  M. Quist and G. Yona. Distributional scaling: An algorithm for structure-preserving embedding of  metric and nonmetric spaces. JMLR, 5:399-420, 2004.  14   KLEINDESSNER VON LUXBURG  References  S. Agarwal, J. Wills, L. Cayton, G. Lanckriet, D. Kriegman, and S. Belongie. Generalized non- metric multidimensional scaling. In International Conference on Artificial Intelligence and Statis- tics (AISTATS), 2007.  N. Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal  query complexity. JMLR, 13:137-164, 2012.  P. Alestalo, D. Trotsenko, and J. V\u00a8ais\u00a8al\u00a8a. Isometric approximation. Israel Journal of Mathematics,  125:61-82, 2001.  1953.  2005.  F. Beckman and D. Quarles. On isometries of Euclidean spaces. Proc. Amer. Math. Soc., 4:810-815,  I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer,  J. Dattorro. Convex optimization and Euclidean distance geometry. Meboo Publishing, 2005.  K. Jamieson and R. Nowak. Low-dimensional embedding using adaptively selected ordinal data. In  Conference on Communication, Control, and Computing, 2011a.  K. Jamieson and R. Nowak. Active ranking using pairwise comparisons. In Neural Information  Processing Systems (NIPS), 2011b.  Psychometrika, 29(1):1-27, 1964a.  129, 1964b.  J. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.  J. Kruskal. Nonmetric multidimensional scaling: A numerical method. Psychometrika, 29(2):115-  Y. Lan, J. Guo, X. Cheng, and T. Liu. Statistical consistency of ranking methods in a rank-  differentiable probability space. In Neural Information Processing Systems (NIPS), 2012.  B. McFee and G. Lanckriet. Partial order embedding with multiple kernels. In International Con-  ference on Machine Learning (ICML), 2009.  B. McFee and G. Lanckriet. Metric learning to rank.  In International Conference on Machine  Learning (ICML), 2010.  B. McFee and G. Lanckriet. Learning multi-modal similarity. JMLR, 12:491-523, 2011.  M. McKemie and J. V\u00a8ais\u00a8al\u00a8a. Spherical maps of Euclidean spaces. Results in Mathematics, 35:  145-160, 1999.  H. Ouyang and A. Gray. Learning dissimilarities by ranking: from SDP to QP. In International  Conference on Machine Learning (ICML), 2008.  M. Quist and G. Yona. Distributional scaling: An algorithm for structure-preserving embedding of  metric and nonmetric spaces. JMLR, 5:399-420, 2004. UNIQUENESS OF ORDINAL EMBEDDING  R. Rosales and G. Fung. Learning sparse metrics via linear programming. In International Confer-  ence on Knowledge Discovery and Data Mining (KDD), 2006.  I. Schoenberg. Metric spaces and completely monotone functions. Ann. of Math., 39(4):811-841,  B. Shaw and T. Jebara. Structure preserving embedding. In International Conference on Machine  Learning (ICML), 2009.  R. Shepard. The analysis of proximities: Multidimensional scaling with an unknown distance func-  tion (I). Psychometrika, 27(2):125-140, 1962a.  R. Shepard. The analysis of proximities: Multidimensional scaling with an unknown distance func-  tion (II). Psychometrika, 27(3):219-246, 1962b.  R. Shepard. Metric structures in ordinal data. Journal of Mathematical Psychology, 3(2):287-315,  1938.  1966.  C. Sherman. Nonmetric multidimensional scaling: A Monte Carlo study of the basic parameters.  Psychometrika, 37(3):323-355, 1972.  O. Tamuz, C. Liu, S. Belongie, O. Shamir, and A. Kalai. Adaptively learning the crowd kernel. In  International Conference on Machine Learning (ICML), 2011.  F. Wauthier, M. Jordan, and N. Jojic. Efficient ranking from pairwise comparisons. In International  Conference on Machine Learning (ICML), 2013.  F. Young. Nonmetric multidimensional scaling: Recovery of metric information. Psychometrika,  35(4):455-473, 1970. KLEINDESSNER VON LUXBURG  "}, "Bayes-Optimal Scorers for Bipartite Ranking": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Bayes-Optimal Scorers for Bipartite Ranking", "abstract": "We address the following seemingly simple question: what is the Bayes-optimal scorer for a bipartite ranking risk? The answer to this question helps establish the consistency of the minimisation of surrogate bipartite risks, and elucidates the relationship between bipartite ranking and other established learning problems. We show that the answer is non-trivial in general, but may be easily determined for certain special cases using the theory of proper losses. Our analysis immediately establishes equivalences between several seemingly disparate risks for bipartite ranking, such as minimising a suitable class-probability estimation risk, and minimising the p-norm push risk proposed by Rudin (2009).", "pdf_url": "http://proceedings.mlr.press/v35/menon14.pdf", "keywords": ["Bipartite ranking", "p-norm push", "class-probability estimation", "proper losses"], "reference": "Shivani Agarwal. The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list. In SIAM International Conference on Data Mining (SDM), pages 839-850, 2011.  Shivani Agarwal. Surrogate regret bounds for the area under the ROC curve via strongly proper  losses. In Conference on Learning Theory (COLT), pages 338-353, 2013.  Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth. Generalization bounds for the area under the ROC curve. Journal of Machine Learning Research, 6:393-425, December 2005.  Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford, and Gregory B . Sorkin. Robust reductions from ranking to classification. Machine Learning, 72 (1-2):139-153, 2008. doi: 10.1007/s10994-008-5058-6.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  Stephen P. Boyd, Corinna Cortes, Mehryar Mohri, and Ana Radovanovic. Accuracy at the top. In  Advances In Neural Information Processing Systems (NIPS), pages 962-970, 2012.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation and classification: Structure and applications. www-stat.wharton.upenn.edu/\u02dcbuja, 2005. Unpublished manuscript.  Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul- lender. Learning to rank using gradient descent. In Proceedings of the 22nd International Con- ference on Machine learning (ICML), pages 89-96, New York, NY, USA, 2005. ACM. doi: 10.1145/1102351.1102363.  St\u00b4ephan Cl\u00b4emenc\u00b8on and Nicolas Vayatis. Ranking the best instances. Journal of Machine Learning  Research, 8:2671-2699, December 2007.  St\u00b4ephan Cl\u00b4emenc\u00b8on, G\u00b4abor Lugosi, and Nicolas Vayatis. Ranking and Empirical Minimization of  U-statistics. The Annals of Statistics, 36(2):844-874, April 2008.  William W. Cohen, Robert E. Schapire, and Yoram Singer. Learning to order things. Journal of  Artificial Intelligence Research, 10(1):243-270, May 1999.  Luc Devroye, L\u00b4aszl\u00b4o Gy\u00a8orfi, and G\u00b4abor Lugosi. A Probabilistic Theory of Pattern Recognition.  Springer, 1996.  New York, 1999.  S\u00b8 eyda Ertekin and Cynthia Rudin. On equivalence relationships between classification and ranking  algorithms. Journal of Machine Learning Research, 12:2905-2929, Oct 2011.  Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley Interscience,  14   MENON WILLIAMSON  References  Shivani Agarwal. The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list. In SIAM International Conference on Data Mining (SDM), pages 839-850, 2011.  Shivani Agarwal. Surrogate regret bounds for the area under the ROC curve via strongly proper  losses. In Conference on Learning Theory (COLT), pages 338-353, 2013.  Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth. Generalization bounds for the area under the ROC curve. Journal of Machine Learning Research, 6:393-425, December 2005.  Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford, and Gregory B . Sorkin. Robust reductions from ranking to classification. Machine Learning, 72 (1-2):139-153, 2008. doi: 10.1007/s10994-008-5058-6.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk  bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  Stephen P. Boyd, Corinna Cortes, Mehryar Mohri, and Ana Radovanovic. Accuracy at the top. In  Advances In Neural Information Processing Systems (NIPS), pages 962-970, 2012.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation and classification: Structure and applications. www-stat.wharton.upenn.edu/\u02dcbuja, 2005. Unpublished manuscript.  Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul- lender. Learning to rank using gradient descent. In Proceedings of the 22nd International Con- ference on Machine learning (ICML), pages 89-96, New York, NY, USA, 2005. ACM. doi: 10.1145/1102351.1102363.  St\u00b4ephan Cl\u00b4emenc\u00b8on and Nicolas Vayatis. Ranking the best instances. Journal of Machine Learning  Research, 8:2671-2699, December 2007.  St\u00b4ephan Cl\u00b4emenc\u00b8on, G\u00b4abor Lugosi, and Nicolas Vayatis. Ranking and Empirical Minimization of  U-statistics. The Annals of Statistics, 36(2):844-874, April 2008.  William W. Cohen, Robert E. Schapire, and Yoram Singer. Learning to order things. Journal of  Artificial Intelligence Research, 10(1):243-270, May 1999.  Luc Devroye, L\u00b4aszl\u00b4o Gy\u00a8orfi, and G\u00b4abor Lugosi. A Probabilistic Theory of Pattern Recognition.  Springer, 1996.  New York, 1999.  S\u00b8 eyda Ertekin and Cynthia Rudin. On equivalence relationships between classification and ranking  algorithms. Journal of Machine Learning Research, 12:2905-2929, Oct 2011.  Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley Interscience, BAYES-OPTIMAL SCORERS FOR BIPARTITE RANKING  Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933-969, December 2003.  Wei Gao and Zhi-Hua Zhou. On the consistency of AUC optimization. CoRR, abs/1208.0645, 2012.  Izrail M. Gelfand and Sergei V. Fomin. Calculus of Variations. Dover, 2000.  Mariano Giaquinta and Stefan Hildebrandt. Calculus of Variations I: The Lagrangian formalism.  Springer-Verlag, Berlin, 2nd edition, 2004.  Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries for ordinal regression. In A.J. Smola, P.L. Bartlett, B. Sch\u00a8olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 115-132, Cambridge, MA, 2000. MIT Press.  Palaniappan Kannappan. Functional equations and inequalities with applications. New York, NY:  Springer, 2009. doi: 10.1007/978-0-387-89492-8.  Donald E. Knuth. Two notes on notation. American Mathematical Monthly, 99(5):403-422, May  1992. doi: 10.2307/2325085.  Wojciech Kotlowski, Krzysztof Dembczynski, and Eyke H\u00a8ullermeier. Bipartite ranking through minimization of univariate loss. In International Conference on Machine Learning (ICML), pages 1113-1120, 2011.  Aditya Krishna Menon and Robert C. Williamson. Bipartite ranking: risk, optimality, and equiva-  lences. Submitted to JMLR, 2014.  Harikrishna Narasimhan and Shivani Agarwal. On the relationship between binary classification, In Advances In Neural Information  bipartite ranking, and binary class probability estimation. Processing Systems (NIPS), pages 2913-2921, 2013.  Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, 2nd edition,  2006.  Mark D. Reid and Robert C. Williamson. Surrogate regret bounds for proper losses. In International Conference on Machine Learning (ICML), pages 897-904, New York, NY, USA, 2009. ACM. doi: 10.1145/1553374.1553489.  Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning  Research, 11:2387-2422, December 2010.  Mark D. Reid and Robert C. Williamson. Information, divergence and risk for binary experiments.  Journal of Machine Learning Research, 12:731-817, Mar 2011.  Fred S. Roberts. Measurement theory with Applications to Decision Making, Utility, and the So- cial Sciences, volume 7 of Encyclopedia of Mathematics and Its Applications. Addison-Wesley, Reading, MA, 1984.  Cynthia Rudin. The p-norm push: A simple convex ranking algorithm that concentrates at the top  of the list. Journal of Machine Learning Research, 10:2233-2271, December 2009. MENON WILLIAMSON  Cynthia Rudin and Robert E. Schapire. Margin-based ranking and an equivalence between Ad-  aBoost and RankBoost. Journal of Machine Learning Research, 10:2193-2232, 2009.  Emir H. Shuford Jr., Arthur Albert, and H. Edward Massengill. Admissible probability measure-  ment procedures. Psychometrika, 31(2):125-145, 1966. doi: 10.1007/BF02289503.  Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approxima-  tion, 26(2):225-287, 2007. doi: 10.1007/s00365-006-0662-3.  S. Joshua Swamidass, Chlo\u00b4e-Agathe Azencott, Kenny Daily, and Pierre Baldi. A CROC stronger than ROC. Bioinformatics, 26(10):1348-1356, May 2010. doi: 10.1093/bioinformatics/btq140.  Erik N. Torgersen. Comparison of Statistical Experiments. Cambridge University Press, 1991.  John L. Troutman. Variational Calculus and Optimal Control: Optimization with Elementary Con-  vexity. Undergraduate Texts in Mathematics. Springer, 1996.  Kazuki Uematsu and Yoonkyung Lee. On theoretically optimal ranking functions in bipartite rank- ing. http://www.stat.osu.edu/\u02dcyklee/mss/bipartrank.rev.pdf, 2012. Un- published manuscript.  Elodie Vernet, Mark D. Reid, and Robert C. Williamson. Composite multiclass losses. In J. Shawe- Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS) 24, pages 1224-1232, 2011.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. The Annals of Statistics, 32:56-134, March 2004. BAYES-OPTIMAL SCORERS FOR BIPARTITE RANKING  "}, "Multiarmed Bandits With Limited Expert Advice": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Multiarmed Bandits With Limited Expert Advice", "abstract": "We consider the problem of minimizing regret in the setting of advice-efficient multiarmed bandits with expert advice. We give an algorithm for the setting of K arms and N experts out of which we are allowed to query and use only M experts\u2019 advice in each round, which has a regret bound of \\tildeO\\left(\\sqrt\\frac\\min{K, M} NM T\\right) after T rounds. We also prove that any algorithm for this problem must have expected regret at least \\tilde\u03a9\\left(\\sqrt\\frac\\min{K, M} NMT\\right), thus showing that our upper bound is nearly tight. This solves the COLT 2013 open problem of Seldin et al. (2013).", "pdf_url": "http://proceedings.mlr.press/v35/kale14a.pdf", "keywords": [], "reference": "Sanjeev Arora, Elad Hazan, and Satyen Kale. The Multiplicative Weights Update Method: a Meta-  Algorithm and Applications. Theory of Computing, 8(1):121-164, 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11:2785-2836, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Minimax policies for combinatorial prediction games. Journal of Machine Learning Research - Proceedings Track, 19:107-132, 2011.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):  Martin Raab and Angelika Steger. \u201cBalls into Bins\u201d - A Simple and Tight Analysis. In RANDOM,  212-261, 1994.  pages 159-170, 1998.  Yevgeny Seldin, Koby Crammer, and Peter Bartlett. Open Problem: Adversarial Multiarmed Ban-  dits with Limited Advice. In COLT, 2013.  Yevgeny Seldin, Peter L. Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with  limited advice and multiarmed bandits with paid observations. In ICML, 2014.  13   MULTIARMED BANDITS WITH LIMITED EXPERT ADVICE  References  Sanjeev Arora, Elad Hazan, and Satyen Kale. The Multiplicative Weights Update Method: a Meta-  Algorithm and Applications. Theory of Computing, 8(1):121-164, 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11:2785-2836, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Minimax policies for combinatorial prediction games. Journal of Machine Learning Research - Proceedings Track, 19:107-132, 2011.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):  Martin Raab and Angelika Steger. \u201cBalls into Bins\u201d - A Simple and Tight Analysis. In RANDOM,  212-261, 1994.  pages 159-170, 1998.  Yevgeny Seldin, Koby Crammer, and Peter Bartlett. Open Problem: Adversarial Multiarmed Ban-  dits with Limited Advice. In COLT, 2013.  Yevgeny Seldin, Peter L. Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with  limited advice and multiarmed bandits with paid observations. In ICML, 2014. KALE  "}, "Learning Sparsely Used Overcomplete Dictionaries": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Learning Sparsely Used Overcomplete Dictionaries", "abstract": "We consider the problem of learning sparsely used overcomplete dictionaries, where each observation is  a sparse combination of elements from an unknown overcomplete dictionary. We establish exact recovery when the dictionary elements are mutually incoherent. Our method consists of a clustering-based initialization step, which provides an approximate estimate   of the true dictionary with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following alternating steps: 1) estimation of the dictionary coefficients for each observation through \\ell_1 minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions, our method converges at a linear rate to the true dictionary as well as the true coefficients for each observation.", "pdf_url": "http://proceedings.mlr.press/v35/agarwal14a.pdf", "keywords": ["Dictionary learning", "sparse coding", "overcomplete dictionaries", "alternating minimization", "lasso"], "reference": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learning sparsely used overcomplete dictionaries via alternating minimization. arXiv preprint arXiv:1310.7991, 2013a.  Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli. Exact recovery of sparsely used  overcomplete dictionaries. arXiv preprint arXiv:1309.1952, 2013b.  S. Arora, R. Ge, and A. Moitra. New Algorithms for Learning Incoherent and Overcomplete Dic-  tionaries. ArXiv e-prints, August 2013.  Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. Smooth sparse coding via marginal  regression for learning sparse representations. In ICML, 2013.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep  learning: A review and new perspectives. arXiv preprint arXiv:1206.5538, 2012.  Michael Elad. Sparse and redundant representations: from theory to applications in signal and  image processing. Springer, 2010.  Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame de- sign. In Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, volume 5, pages 2443-2446. IEEE, 1999.  Rahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for  sparse recovery with restricted isometry property. In ICML, 2009.  Quan Geng, Huan Wang, and John Wright.  mization for dictionary learning. URL:http://arxiv.org/abs/1101.5672.  arXiv preprint arXiv:1101:5672, 2011.  On the local correctness of \u21131 mini- Preprint,  R\u00b4emi Gribonval and Karin Schnass. Dictionary Identification - Sparse Matrix-Factorisation via  \u2113 1-Minimisation. IEEE Transactions on Information Theory, 56(7):3523-3539, 2010.  Christopher J Hillar and Friedrich T Sommer. Ramsey theory reveals the conditions when sparse  coding on subsampled data is unique. arXiv preprint arXiv:1106.3616, 2011.  Rodolphe Jenatton, Julien Mairal, Francis R Bach, and Guillaume R Obozinski. Proximal methods for sparse hierarchical dictionary learning. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 487-494, 2010.  Rodolphe Jenatton, R\u00b4emi Gribonval, and Francis Bach. sparse dictionary learning in the presence of noise. http://hal.inria.fr/hal-00737152.  Local stability and robustness of URL Technical report, 2012.  Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. In  Advances in neural information processing systems, pages 801-808, 2006.  Michael S Lewicki and Terrence J Sejnowski. Learning overcomplete representations. Neural  computation, 12(2):337-365, 2000.  14   AGARWAL ANANDKUMAR JAIN NETRAPALLI TANDON  References  Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learning sparsely used overcomplete dictionaries via alternating minimization. arXiv preprint arXiv:1310.7991, 2013a.  Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli. Exact recovery of sparsely used  overcomplete dictionaries. arXiv preprint arXiv:1309.1952, 2013b.  S. Arora, R. Ge, and A. Moitra. New Algorithms for Learning Incoherent and Overcomplete Dic-  tionaries. ArXiv e-prints, August 2013.  Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. Smooth sparse coding via marginal  regression for learning sparse representations. In ICML, 2013.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep  learning: A review and new perspectives. arXiv preprint arXiv:1206.5538, 2012.  Michael Elad. Sparse and redundant representations: from theory to applications in signal and  image processing. Springer, 2010.  Kjersti Engan, Sven Ole Aase, and J Hakon Husoy. Method of optimal directions for frame de- sign. In Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, volume 5, pages 2443-2446. IEEE, 1999.  Rahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for  sparse recovery with restricted isometry property. In ICML, 2009.  Quan Geng, Huan Wang, and John Wright.  mization for dictionary learning. URL:http://arxiv.org/abs/1101.5672.  arXiv preprint arXiv:1101:5672, 2011.  On the local correctness of \u21131 mini- Preprint,  R\u00b4emi Gribonval and Karin Schnass. Dictionary Identification - Sparse Matrix-Factorisation via  \u2113 1-Minimisation. IEEE Transactions on Information Theory, 56(7):3523-3539, 2010.  Christopher J Hillar and Friedrich T Sommer. Ramsey theory reveals the conditions when sparse  coding on subsampled data is unique. arXiv preprint arXiv:1106.3616, 2011.  Rodolphe Jenatton, Julien Mairal, Francis R Bach, and Guillaume R Obozinski. Proximal methods for sparse hierarchical dictionary learning. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 487-494, 2010.  Rodolphe Jenatton, R\u00b4emi Gribonval, and Francis Bach. sparse dictionary learning in the presence of noise. http://hal.inria.fr/hal-00737152.  Local stability and robustness of URL Technical report, 2012.  Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. In  Advances in neural information processing systems, pages 801-808, 2006.  Michael S Lewicki and Terrence J Sejnowski. Learning overcomplete representations. Neural  computation, 12(2):337-365, 2000. LEARNING SPARSELY USED OVERCOMPLETE DICTIONARIES  Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. Sparse coding for multitask  and transfer learning. arXiv preprint arXiv:1209.0738, 2012.  Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. Sparse coding for multitask  and transfer learning. In ICML, 2013.  Nishant Mehta and Alexander G Gray. Sparsity-based generalization bounds for predictive sparse coding. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 36-44, 2013.  Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy  employed by v1? Vision research, 37(23):3311-3325, 1997.  Holger Rauhut. Compressive sensing, structured random matrices and recovery of functions in high  dimensions. In Oberwolfach Reports, volume 7, pages 1990-1993, 2010.  Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In  Proc. of Conf. on Learning Theory, 2012.  Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, and Andreas Spanias. Learning stable  multilevel dictionaries for sparse representation of images. ArXiv 1303.0448, 2013.  J.A. Tropp and A.C. Gilbert. Signal recovery from random measurements via orthogonal matching  pursuit. Information Theory, IEEE Transactions on, 53(12):4655-4666, 2007.  Daniel Vainsencher, Shie Mannor, and Alfred M Bruckstein. The sample complexity of dictionary  learning. The Journal of Machine Learning Research, 12:3259-3281, 2011.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010. "}, "Community Detection via Random and Adaptive Sampling": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Community Detection via Random and Adaptive Sampling", "abstract": "In this paper, we consider networks consisting of a finite number of non-overlapping communities. To extract these communities, the interaction between pairs of nodes may be sampled from a large available data set, which allows a given node pair to be sampled several times. When a node pair is sampled, the observed outcome is a binary random variable, equal to 1 if nodes interact and to 0 otherwise. The outcome is more likely to be positive if nodes belong to the same communities. For a given budget of node pair samples or observations, we wish to jointly design a sampling strategy (the sequence of sampled node pairs) and a clustering algorithm that recover the hidden communities with the highest possible accuracy. We consider both non-adaptive and adaptive sampling strategies, and for both classes of strategies, we derive fundamental performance limits satisfied by any sampling and clustering algorithm. In particular, we provide necessary conditions for the existence of algorithms recovering the communities accurately as the network size grows large. We also devise simple algorithms that accurately reconstruct the communities when this is at all possible, hence proving that the proposed necessary conditions for accurate community detection are also sufficient. The classical problem of community detection in the stochastic block model can be seen as a particular instance of the problems consider here. But our framework covers more general scenarios where the sequence of sampled node pairs can be designed in an adaptive manner. The paper provides new results for the stochastic block model, and extends the analysis to the case of adaptive sampling.", "pdf_url": "http://proceedings.mlr.press/v35/yun14.pdf", "keywords": [], "reference": "S. Balakrishnan, M. Xu, A. Krishnamurthy, and A. Singh. Noise thresholds for spectral clustering.  In NIPS, pages 954-962, 2011.  R. B. Boppana. Eigenvalues and graph bisection: An average-case analysis.  In Foundations of  Computer Science, 1987., 28th Annual Symposium on, pages 280-285. IEEE, 1987.  S. Chatterjee. Matrix estimation by universal singular value thresholding.  arXiv preprint  arXiv:1212.1247, 2012.  K. Chaudhuri, F. C. Graham, and A. Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. Journal of Machine Learning Research-Proceedings Track, 23:35-1, 2012.  Y. Chen, S. Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in Neural Information  Processing Systems 25, pages 2213-2221. 2012.  A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability  & Computing, 19(2):227-284, 2010.  A. Condon and R. Karp. Algorithms for graph partitioning on the planted partition model. Random  Structures and Algorithms, 18(2):116-140, 2001.  A. Dasgupta, J. Hopcroft, R. Kannan, and P. Mitra. Spectral clustering by recursive partitioning. In  Algorithms-ESA 2006, pages 256-267. Springer, 2006.  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Inference and phase transitions in the detec-  tion of modules in sparse networks. Phys. Rev. Lett., 107, Aug 2011.  U. Feige and E. Ofek. Spectral techniques applied to sparse random graphs. Random Structures &  Algorithms, 27(2):251-275, 2005.  S. Heimlicher, M. Lelarge, and L. Massouli\u00b4e. Community detection in the labelled stochastic block  model. arXiv preprint arXiv:1209.2910, 2012.  P. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5  (2):109 - 137, 1983.  M. Jerrum and G. B. Sorkin. The metropolis algorithm for graph bisection. Discrete Applied  Mathematics, 82(13):155 - 175, 1998.  F. Krzakala, M. M\u00b4ezard, F. Sausset, Y. F. Sun, and L. Zdeborov\u00b4a. Statistical-physics-based recon-  struction in compressed sensing. Phys. Rev. X, 2:021005, May 2012.  S. Kudekar, T.J. Richardson, and R.L. Urbanke. Threshold saturation via spatial coupling: Why convolutional ldpc ensembles perform so well over the bec. Information Theory, IEEE Transac- tions on, 57(2):803-834, 2011.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6(1):4-22, 1985.  13   COMMUNITY DETECTION VIA ADAPTIVE SAMPLING  References  S. Balakrishnan, M. Xu, A. Krishnamurthy, and A. Singh. Noise thresholds for spectral clustering.  In NIPS, pages 954-962, 2011.  R. B. Boppana. Eigenvalues and graph bisection: An average-case analysis.  In Foundations of  Computer Science, 1987., 28th Annual Symposium on, pages 280-285. IEEE, 1987.  S. Chatterjee. Matrix estimation by universal singular value thresholding.  arXiv preprint  arXiv:1212.1247, 2012.  K. Chaudhuri, F. C. Graham, and A. Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. Journal of Machine Learning Research-Proceedings Track, 23:35-1, 2012.  Y. Chen, S. Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in Neural Information  Processing Systems 25, pages 2213-2221. 2012.  A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability  & Computing, 19(2):227-284, 2010.  A. Condon and R. Karp. Algorithms for graph partitioning on the planted partition model. Random  Structures and Algorithms, 18(2):116-140, 2001.  A. Dasgupta, J. Hopcroft, R. Kannan, and P. Mitra. Spectral clustering by recursive partitioning. In  Algorithms-ESA 2006, pages 256-267. Springer, 2006.  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Inference and phase transitions in the detec-  tion of modules in sparse networks. Phys. Rev. Lett., 107, Aug 2011.  U. Feige and E. Ofek. Spectral techniques applied to sparse random graphs. Random Structures &  Algorithms, 27(2):251-275, 2005.  S. Heimlicher, M. Lelarge, and L. Massouli\u00b4e. Community detection in the labelled stochastic block  model. arXiv preprint arXiv:1209.2910, 2012.  P. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5  (2):109 - 137, 1983.  M. Jerrum and G. B. Sorkin. The metropolis algorithm for graph bisection. Discrete Applied  Mathematics, 82(13):155 - 175, 1998.  F. Krzakala, M. M\u00b4ezard, F. Sausset, Y. F. Sun, and L. Zdeborov\u00b4a. Statistical-physics-based recon-  struction in compressed sensing. Phys. Rev. X, 2:021005, May 2012.  S. Kudekar, T.J. Richardson, and R.L. Urbanke. Threshold saturation via spatial coupling: Why convolutional ldpc ensembles perform so well over the bec. Information Theory, IEEE Transac- tions on, 57(2):803-834, 2011.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6(1):4-22, 1985. YUN PROUTIERE  (a) p = 10\u22123 and q = p/20  (b) p = 10\u22122 and q = p/2  (c) p = 10\u22121 and q = p/2  Figure 1: The average fraction of misclassified nodes under SP (URS-1 sampling strategy) and  ASP.  L. Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. CoRR,  abs/1311.3085, 2013.  F. McSherry. Spectral partitioning of random graphs. In Foundations of Computer Science, 2001.  Proceedings. 42nd IEEE Symposium on, pages 529-537. IEEE, 2001.  E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. arXiv preprint  E. Mossel, J. Neeman, and A. Sly. A Proof Of The Block Model Threshold Conjecture. ArXiv  M. Newman. Spectral methods for network community detection and graph partitioning. Phys. Rev.  arXiv:1202.1499, 2012.  e-prints, November 2013.  E, 8, 2013.  K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic block-  model. The Annals of Statistics, 39(4):1878-1915, 08 2011.  O. Shamir and N. Tishby. Spectral clustering on a budget. In International Conference on Artificial  Intelligence and Statistics, pages 661-669, 2011.  T. Tao. Topics in random matrix theory, volume 132. AMS Bookstore, 2012.  K. Voevodski, M. Balcan, H. R\u00a8oglin, S. Teng, and Y. Xia. Active clustering of biological sequences.  The Journal of Machine Learning Research, 13:203-225, 2012.  "}, "A second-order bound with excess losses": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "A second-order bound with excess losses", "abstract": "We study online aggregation of the predictions of experts, and first show new second-order regret bounds in the standard setting, which are obtained via a version of the Prod algorithm (and also a version of the polynomially weighted average algorithm) with multiple learning rates. These bounds are in terms of excess losses, the differences between the instantaneous losses suffered by the algorithm and the ones of a given expert. We then demonstrate the interest of these bounds in the context of experts that report their confidences as a number in the interval [0,1] using a generic reduction to the standard setting. We conclude by two other applications in the standard setting, which improve the known bounds in case of small excess losses and show a bounded regret against i.i.d. sequences of losses.", "pdf_url": "http://proceedings.mlr.press/v35/gaillard14.pdf", "keywords": [], "reference": "Dmitry Adamskiy, Wouter M. Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at  adaptive regret. In Algorithmic Learning Theory (ALT), pages 290-304, 2012.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64:48-75, 2002.  David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathe-  matics, 6:1-8, 1956.  13   A SECOND-ORDER BOUND WITH EXCESS LOSSES  Therefore, using that expectations of conditional expectations are unconditional expectations,  E[Rk(cid:63),T ] (cid:62) \u03b1 E[ST ]  and E  r2 k(cid:63),t  (cid:54) E[ST ]  where ST =  (1 \u2212 pk(cid:63),t) .  (19)  Substituting these inequalities in (10) using Jensen\u2019s inequality for  \u00b7 , we get  (cid:35)  (cid:34) T  (cid:88)  t=1  T (cid:88)  t=1  \u221a  E[ST ] (cid:54) \u039e1  \u221a  ln K \u03b1  (cid:112)E[ST ] +  \u039e2 \u03b1  .  Solving the quadratic inequality (see Lemma 10) yields E[ST ] (cid:54) (cid:0)(\u039e1 (19) this bounds E t=1 r2 prove the claimed bound on the expected regret.  (cid:104)(cid:80)T  k(cid:63),t  (cid:105)  , which we substitute into (10), together with Jensen\u2019s inequality, to  \u221a  ln K)/\u03b1 + (cid:112)\u039e2/\u03b1(cid:1)2  . By  Now, to get the high-probability bound, we apply Theorem 12 to Xt = Yt/2 (cid:54) 1 a.s. and  Vt = Wt/4 and use the bounds (16) and (18). We find that, with probability at least 1 \u2212 \u03b4,  \u03b1ST (cid:54) Rk(cid:63),T + 3(cid:112)(4 + ST ) ln(\u03b3/\u03b4) + 2 ln(\u03b3/\u03b4) (cid:54) Rk(cid:63),T + 3(cid:112)ST ln(\u03b3/\u03b4) + 8 ln(\u03b3/\u03b4)  (cid:104)  1 + ln(cid:0)1 + E[ST ]/4(cid:1)(cid:105)  where \u03b3 (cid:54) 1 + (1/2e) bound (10) on the regret with (17) yields Rk(cid:63),T (cid:54) \u039e1 at least 1 \u2212 \u03b4,  and where we used (cid:112)ln(\u03b3/\u03b4) (cid:62) 1. Combining the ST ln K + \u039e2, so that, still with probability  \u221a  \u03b1ST (cid:54)  \u221a  (cid:16)  \u039e1  ln K + 3(cid:112)ln(\u03b3/\u03b4)  (cid:17) (cid:112)  (cid:16)  ST +  8 ln(\u03b3/\u03b4) + \u039e2  (cid:17)  .  Solving for  \u221a  (cid:112)  ST (cid:54)  \u039e1  ST with Lemma 10 and using that \u03b1 (cid:54) 1, this implies \u221a  \u221a  ln K + 3(cid:112)ln(\u03b3/\u03b4)  +  1 \u221a \u03b1  (cid:112)8 ln(\u03b3/\u03b4) + \u039e2 (cid:54) \u039e1  \u03b1  ln K \u03b1  +  (cid:114)  \u039e2 \u03b1  +  6 \u03b1  (cid:114)  ln  .  \u03b3 \u03b4  Substitution into the (deterministic) regret bound Rk(cid:63),T (cid:54) \u039e1  ST ln K+\u039e2 concludes the proof.  \u221a  Van Erven was supported by NWO Rubicon grant 680-50-1112.  Acknowledgments  References  Dmitry Adamskiy, Wouter M. Koolen, Alexey Chernov, and Vladimir Vovk. A closer look at  adaptive regret. In Algorithmic Learning Theory (ALT), pages 290-304, 2012.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64:48-75, 2002.  David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathe-  matics, 6:1-8, 1956. GAILLARD STOLTZ VAN ERVEN  Avrim Blum. Empirical support for winnow and weighted-majority algorithms: Results on a calen-  dar scheduling domain. Machine Learning, 26:5-23, 1997.  Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning  Research, 8:1307-1324, 2007.  N. Cesa-Bianchi and G. Lugosi. Potential-based algorithms in on-line prediction and game theory.  Machine Learning, 3(51):239-261, 2003.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, G\u00b4abor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient pre-  diction. IEEE Trans. Inform. Theory, 51:77-92, 2005.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2/3):321-352, 2007.  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts.  CoRR, abs/1006.0475, 2010.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), pages 6.1-6.20, 2012.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Available at arXiv:1301.0534 [cs.LG], 2013.  Marie Devaine, Pierre Gaillard, Yannig Goude, and Gilles Stoltz. Forecasting electricity consump-  tion by aggregating specialized experts. Machine Learning, 90(2):231-260, 2013.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. 12(Jul):2121-2159, 2011.  David Freedman. On tail probabilities for martingales. Annals of Probability, 3:100-118, 1975.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 334-343, 1997.  Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. Ex-  tended version available at arXiv:1301.0534 [cs.LG], 2014.  L\u00b4aszl\u00b4o Gy\u00a8orfi and Gy\u00a8orgy Ottucs\u00b4ak. Sequential prediction of unbounded stationary time series.  IEEE Transactions on Information Theory, 53(5):1866-1872, 2007.  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: regret bounded by variation in  costs. Machine Learning, 80(2-3):165-188, 2010. A SECOND-ORDER BOUND WITH EXCESS LOSSES  Wouter M. Koolen, Dmitry Adamskiy, and Manfred K. Warmuth. Putting Bayes to sleep. Advances in Neural Information Processing Systems 25 (NIPS 2012), pages 135-143, 2013.  In  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm.  Information and  Computation, 108(2):212-261, 1994.  Tim van Erven, Peter Gr\u00a8unwald, Wouter M. Koolen, and Steven de Rooij. Adaptive Hedge. In  Advances in Neural Information Processing Systems 25 (NIPS 2012), 2011.  Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, 1998.  Olivier Wintenberger. Optimal learning with bernstein online aggregation. Extended version avail-  able at arXiv:1404.1356 [stat.ML], 2014. GAILLARD STOLTZ VAN ERVEN  "}, "Logistic Regression: Tight Bounds for Stochastic and Online Optimization": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization", "abstract": "The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012).", "pdf_url": "http://proceedings.mlr.press/v35/hazan14a.pdf", "keywords": ["Logistic regression", "Stochastic optimization", "Online learning", "Lower bounds"], "reference": "Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. arXiv preprint arXiv:1303.6149, 2013.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781. 2013.  Yaroslav Bulatov. Log loss or hinge loss? http://yaroslavvb.blogspot.co.il/2007/  06/log-loss-or-hinge-loss.html, June 2007.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman  distances. Machine Learning, 48(1-3):253-285, 2002.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. The Annals of Statistics, 28(2):337-407, 2000.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic and  online optimization. arXiv preprint arXiv:1405.3843, 2014.  John Langford. Optimal proxy loss for classification. http://hunch.net/?p=547, April  2009.  Brendan H McMahan and Matthew J Streeter. Open problem: Better bounds for online logistic  regression. Journal of Machine Learning Research-Proceedings Track, 23:44-1, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  ICML, pages 928-936, 2003.  13   TIGHT BOUNDS FOR LOGISTIC REGRESSION  a distribution over instances such that the induced expected loss function is approximately linear around its optimum.  An interesting feature of our results is that our regret/convergence bounds apply to a finite range of T , and are different than the known asymptotic bounds. Arguably, the range of T for which our results apply is the important one in practice (sub-exponential in the size of the hypothesis class). Are there other natural settings in which regret bounds for bounded number of iterations differ from the asymptotic bound?  The research leading to these results has received funding from the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 336078 - ERC-SUBLRN.  Acknowledgments  References  Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for  logistic regression. arXiv preprint arXiv:1303.6149, 2013.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781. 2013.  Yaroslav Bulatov. Log loss or hinge loss? http://yaroslavvb.blogspot.co.il/2007/  06/log-loss-or-hinge-loss.html, June 2007.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.  Michael Collins, Robert E Schapire, and Yoram Singer. Logistic regression, adaboost and bregman  distances. Machine Learning, 48(1-3):253-285, 2002.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical  view of boosting. The Annals of Statistics, 28(2):337-407, 2000.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Elad Hazan, Tomer Koren, and Kfir Y. Levy. Logistic regression: Tight bounds for stochastic and  online optimization. arXiv preprint arXiv:1405.3843, 2014.  John Langford. Optimal proxy loss for classification. http://hunch.net/?p=547, April  2009.  Brendan H McMahan and Matthew J Streeter. Open problem: Better bounds for online logistic  regression. Journal of Machine Learning Research-Proceedings Track, 23:44-1, 2012.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  ICML, pages 928-936, 2003. "}, "Higher-Order Regret Bounds with Switching Costs": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Higher-Order Regret Bounds with Switching Costs", "abstract": "This work examines online linear optimization with full information and switching costs (SCs) and focuses on regret bounds that depend on properties of the loss sequences. The SCs considered are bounded functions of a pair of decisions, and regret is augmented with the total SC. We show under general conditions that for any normed SC, \u03c3(\\mathbfx,\\mathbfx\u2019)=\\|\\mathbfx-\\mathbfx\u2019\\|, regret \\textitcannot be bounded given only a bound Q on the quadratic variation of losses. With an additional bound \u039bon the total length of losses, we prove O(\\sqrtQ+\u039b) regret for Regularized Follow the Leader (RFTL). Furthermore, an O(\\sqrtQ) bound holds for RFTL given a cost \\|\\mathbfx-\\mathbfx\u2019\\|^2. By generalizing the Shrinking Dartboard algorithm, we also show an expected regret bound for the best expert setting with any SC, given bounds on the total loss of the best expert and the quadratic variation of any expert. As SCs vanish, all our bounds depend purely on quadratic variation. We apply our results to pricing options in an arbitrage-free market with proportional transaction costs. In particular, we upper bound the price of \u201cat the money\u201d call options, assuming bounds on the quadratic variation of a stock price and the minimum of summed gains and summed losses.", "pdf_url": "http://proceedings.mlr.press/v35/gofer14.pdf", "keywords": ["Online Learning", "Regret Minimization", "Switching Costs", "Online Linear Optimization", "Option Pricing"]}, "The Complexity of Learning Halfspaces using Generalized Linear Methods": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "The Complexity of Learning Halfspaces using Generalized Linear Methods", "abstract": "Many popular learning algorithms (E.g. Regression, Fourier-Transform based algorithms, Kernel SVM and Kernel ridge regression) operate by reducing the problem to a convex optimization problem over a set of functions. These methods offer the currently best approach to several central problems such as learning half spaces and learning DNF\u2019s. In addition they are widely used in numerous application domains. Despite their importance, there are still very few proof techniques to show limits on the power of these algorithms. We study the performance of this approach in the problem of (agnostically and improperly) learning halfspaces with margin \u03b3. Let D be a distribution over labeled examples. The \u03b3-margin error of a hyperplane h is the probability of an example to fall on the wrong side of h or at a distance \\le\u03b3from it. The \u03b3-margin error of the best h is denoted \\mathrmErr_\u03b3(D).  An \u03b1(\u03b3)-approximation algorithm receives \u03b3,\u03b5as input and, using i.i.d. samples of D, outputs a classifier with error rate \\le \u03b1(\u03b3)\\mathrmErr_\u03b3(D) + \u03b5.  Such an algorithm is efficient if it uses \\mathrmpoly(\\frac1\u03b3,\\frac1\u03b5) samples and runs in time polynomial in the sample size. The best approximation ratio achievable by an efficient algorithm is O\\left(\\frac1/\u03b3\\sqrt\\log(1/\u03b3)\\right) and is achieved using an algorithm from the above class. Our main result shows that the approximation ratio of every efficient algorithm from this family must be \\ge \u03a9\\left(\\frac1/\u03b3\\mathrmpoly\\left(\\log\\left(1/\u03b3\\right)\\right)\\right), essentially matching the best known upper bound.", "pdf_url": "http://proceedings.mlr.press/v35/daniely14a.pdf", "keywords": [], "reference": "Martin Anthony and Peter Bartlet. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 1999.  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere:  An Introduction, volume 2044. Springer, 2012.  P. L. Bartlett, M. I. Jordan, and J. D. McAuli\ufb00e. Convexity, classification, and risk bounds.  Journal of the American Statistical Association, 101:138-156, 2006.  S. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error  rate using a surrogate convex loss. In ICML, 2012.  Shai Ben-David, Nadav Eiron, and Hans Ulrich Simon. Limitations of learning via embed- dings in euclidean half spaces. The Journal of Machine Learning Research, 3:441-461, 2003.  A. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-  accuracy tradeo\ufb00s. In NIPS, 2012.  E. Blais, R. O\u2019Donnell, and K Wimmer. Polynomial regression under arbitrary product  distributions. In COLT, 2008.  bridge University Press, 2000.  N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cam-  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to  improper learning complexity. arXiv preprint arXiv:1311.2272, 2013.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foun- dations of Computer Science, 2006.  G.B. Folland. A course in abstract harmonic analysis. CRC, 1994.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Pro-  ceedings of the 47th Foundations of Computer Science (FOCS), 2006.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In  Proceedings of the 46th Foundations of Computer Science (FOCS), 2005.  A.R. Klivans and R. Servedio. Learning DNF in time 2 \u02dcO(n1/3). In STOC, pages 258-265.  ACM, 2001.  Kosaku Yosida. Functional Analysis. Springer-Verlag, Heidelberg, 1963.  Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the Fourier spectrum.  In STOC, pages 455-464, May 1991.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier trans-  form, and learnability. In FOCS, pages 574-579, October 1989.  14   Daniely Shalev-Shwartz  References  Martin Anthony and Peter Bartlet. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 1999.  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere:  An Introduction, volume 2044. Springer, 2012.  P. L. Bartlett, M. I. Jordan, and J. D. McAuli\ufb00e. Convexity, classification, and risk bounds.  Journal of the American Statistical Association, 101:138-156, 2006.  S. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error  rate using a surrogate convex loss. In ICML, 2012.  Shai Ben-David, Nadav Eiron, and Hans Ulrich Simon. Limitations of learning via embed- dings in euclidean half spaces. The Journal of Machine Learning Research, 3:441-461, 2003.  A. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-  accuracy tradeo\ufb00s. In NIPS, 2012.  E. Blais, R. O\u2019Donnell, and K Wimmer. Polynomial regression under arbitrary product  distributions. In COLT, 2008.  bridge University Press, 2000.  N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cam-  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to  improper learning complexity. arXiv preprint arXiv:1311.2272, 2013.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foun- dations of Computer Science, 2006.  G.B. Folland. A course in abstract harmonic analysis. CRC, 1994.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Pro-  ceedings of the 47th Foundations of Computer Science (FOCS), 2006.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In  Proceedings of the 46th Foundations of Computer Science (FOCS), 2005.  A.R. Klivans and R. Servedio. Learning DNF in time 2 \u02dcO(n1/3). In STOC, pages 258-265.  ACM, 2001.  Kosaku Yosida. Functional Analysis. Springer-Verlag, Heidelberg, 1963.  Eyal Kushilevitz and Yishay Mansour. Learning decision trees using the Fourier spectrum.  In STOC, pages 455-464, May 1991.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier trans-  form, and learnability. In FOCS, pages 574-579, October 1989. The complexity of learning halfspaces using generalized linear methods  P.M. Long and R.A. Servedio. Learning large-margin halfspaces with more malicious noise.  In NIPS, 2011.  J. Matousek. Lectures on discrete geometry, volume 212. Springer, 2002.  V.D. Milman and G. Schechtman. Asymptotic Theory of Finite Dimensional Normed Spaces: Isoperimetric Inequalities in Riemannian Manifolds, volume 1200. Springer, 2002.  F. Rosenblatt. The perceptron: A probabilistic model for information storage and organiza- tion in the brain. Psychological Review, 65:386-407, 1958. (Reprinted in Neurocomputing (MIT Press, 1988).).  S. Saitoh. Theory of reproducing kernels and its applications. Longman Scientific & Tech-  nical England, 1988.  IJ Schoenberg. Positive definite functions on spheres. Duke. Math. J., 1942.  B. Sch\u00a8olkopf, C. Burges, and A. Smola, editors. Advances in Kernel Methods - Support  Vector Learning. MIT Press, 1998.  S. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the  0-1 loss. SIAM Journal on Computing, 40:1623-1646, 2011.  I. Steinwart and A. Christmann. Support vector machines. Springer, 2008.  R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B.,  58(1):267-288, 1996.  V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.  Manfred K Warmuth and SVN Vishwanathan. Leaving the span. In Learning Theory, pages  366-381. Springer, 2005.  T. Zhang. Statistical behavior and consistency of classification methods based on convex  risk minimization. The Annals of Statistics, 32:56-85, 2004.  "}, "Optimal learners for multiclass problems": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Optimal learners for multiclass problems", "abstract": "The fundamental theorem of statistical learning states that for \\emphbinary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for \\emphmulticlass prediction.  We start by proving a surprising result: a generic optimal multiclass learner must be \\emphimproper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundamental question of \u201chow to learn\u201d? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et el (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005)", "pdf_url": "http://proceedings.mlr.press/v35/daniely14b.pdf", "keywords": [], "reference": "2000.  2000.  N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, second edition,  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50: 74-86, 1995.  M. Collins. Discriminative reranking for natural language parsing. In Machine Learning,  M. Collins. Discriminative training methods for hidden Markov models: Theory and ex- periments with perceptron algorithms. In Conference on Empirical Methods in Natural Language Processing, 2002.  Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In New developments in parsing technology, pages 19-55. Springer, 2005.  K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based  vector machines. Journal of Machine Learning Research, 2:265-292, 2001.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and  the erm principle. In COLT, 2011.  A. Daniely, S. Sabato, and S. Shalev-Shwartz. multiclass learning approaches: A theoretical  comparision with implications. In NIPS, 2012.  T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting  output codes. Journal of Artificial Intelligence Research, 2:263-286, January 1995.  David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0, 1}-functions on  randomly drawn points. In FOCS, pages 100-109, October 1988.  J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan. Phoneme alignment based on  discriminative learning. In Interspeech, 2005.  J. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models In International Conference on Machine  for segmenting and labeling sequence data. Learning, pages 282-289, 2001.  N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished  manuscript, November 1986.  J. Matousek. Lectures on discrete geometry, volume 212. Springer, 2002.  B. K. Natarajan. On learning sets and functions. Mach. Learn., 4:67-97, 1989.  Benjamin I Rubinstein, Peter L Bartlett, and J Hyam Rubinstein. Shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds. In Advances in Neural Infor- mation Processing Systems, pages 1193-1200, 2006.  14   Daniely Shalev-Shwartz  References  2000.  2000.  N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, second edition,  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50: 74-86, 1995.  M. Collins. Discriminative reranking for natural language parsing. In Machine Learning,  M. Collins. Discriminative training methods for hidden Markov models: Theory and ex- periments with perceptron algorithms. In Conference on Empirical Methods in Natural Language Processing, 2002.  Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In New developments in parsing technology, pages 19-55. Springer, 2005.  K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based  vector machines. Journal of Machine Learning Research, 2:265-292, 2001.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and  the erm principle. In COLT, 2011.  A. Daniely, S. Sabato, and S. Shalev-Shwartz. multiclass learning approaches: A theoretical  comparision with implications. In NIPS, 2012.  T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting  output codes. Journal of Artificial Intelligence Research, 2:263-286, January 1995.  David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0, 1}-functions on  randomly drawn points. In FOCS, pages 100-109, October 1988.  J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan. Phoneme alignment based on  discriminative learning. In Interspeech, 2005.  J. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models In International Conference on Machine  for segmenting and labeling sequence data. Learning, pages 282-289, 2001.  N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished  manuscript, November 1986.  J. Matousek. Lectures on discrete geometry, volume 212. Springer, 2002.  B. K. Natarajan. On learning sets and functions. Mach. Learn., 4:67-97, 1989.  Benjamin I Rubinstein, Peter L Bartlett, and J Hyam Rubinstein. Shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds. In Advances in Neural Infor- mation Processing Systems, pages 1193-1200, 2006. Multiclass Learning  S. Shalev-Shwartz, J. Keshet, and Y. Singer. Learning to align polyphonic music.  In  Proceedings of the 5th International Conference on Music Information Retrieval, 2004.  Hans Ulrich Simon and Bal\u00b4azs Sz\u00a8or\u00b4enyi. One-inclusion hypergraph density revisited. In-  formation Processing Letters, 110(8):341-344, 2010.  B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.  A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing visual features for multiclass and multiview object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29(5):854-869, 2007.  I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning In Proceedings of the Twenty-First  for interdependent and structured output spaces. International Conference on Machine Learning, 2004.  V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, XVI(2):264- 280, 1971.  "}, "Stochastic Regret Minimization via Thompson Sampling": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Stochastic Regret Minimization via Thompson Sampling", "abstract": "The Thompson Sampling (TS) policy  is a widely implemented algorithm for the stochastic multi-armed bandit (MAB) problem. Given a prior distribution over possible parameter settings of the underlying reward distributions of the arms, at each time instant, the policy plays an arm with probability equal to the probability that this arm has largest mean reward conditioned on the current posterior distributions of the arms.  This policy generalizes the celebrated \u201cprobability matching\u201d heuristic which has been experimentally and widely observed in human decision making. However, despite its ubiquity, the Thompson Sampling policy is poorly understood. Our goal in this paper is to make progress towards understanding the empirical success of this policy. We proceed using the lens of approximation algorithms and problem definitions from stochastic optimization. We focus on an objective function termed \\em stochastic regret that captures the expected number of times the policy plays an arm that is not the eventual best arm, where the expectation is over the prior distribution. Given such a definition, we show that TS is a 2\u2013approximation to the optimal decision policy in two extreme but canonical scenarios. One such scenario is the two-armed bandit problem which is used as a calibration point in all bandit literature. The second scenario is stochastic optimization where the outcome of a random variable is revealed in a single play to a high or low deterministic value. We show that the 2 approximation is tight in both these scenarios. We provide an uniform analysis framework that in theory is capable of proving our  conjecture that the TS policy is a 2\u2013approximation to the optimal decision policy for minimizing stochastic regret, for any prior distribution and any time horizon.", "pdf_url": "http://proceedings.mlr.press/v35/guha14.pdf", "keywords": ["Thompson Sampling", "Multi-armed bandits", "Stochastic optimization"], "reference": "S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem.  Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. CoRR,  abs/1209.3353, 2012b.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Babu, N. Borisov, S. Duan, H. Herodotou, and V. Thummala. Automated experiment-driven  management of (database) systems. Proc. of HotOS, 2009.  S\u00b4ebastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson  sampling. NIPS, pages 638-646, 2013.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for sequential alloca-  tion problems. Advances in Applied Mathematics, 17(2):122 - 142, 1996.  Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages  2249-2257, 2011.  B. C. Dean, M. X. Goemans, and J. Vondrak. Approximating the stochastic knapsack problem: The benefit of adaptivity. In FOCS \u201904: Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science, pages 208-217, 2004.  A. Demberel, J. Chase, and S. Babu. Re\ufb02ective control for an elastic cloud appliation: An automated  experiment workbench. Proc. of HotCloud, 2009.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient  optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.  A. Garivier and O. Capp\u00b4e. The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond.  In Proc. COLT, 2011.  J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments.  Progress in statistics (European Meeting of Statisticians), 1972.  A. Goel, S. Khanna, and B. Null. The ratio index for budgeted learning, with applications. In SODA,  pages 18-27, 2009.  ICML, 2014.  58(1), 2010.  A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex bandit problems. In  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. J. ACM,  H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based optimization of mapreduce  programs. Proc. of VLDB, 2011.  13   THOMPSON SAMPLING  References  S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem.  Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. CoRR,  abs/1209.3353, 2012b.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Babu, N. Borisov, S. Duan, H. Herodotou, and V. Thummala. Automated experiment-driven  management of (database) systems. Proc. of HotOS, 2009.  S\u00b4ebastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson  sampling. NIPS, pages 638-646, 2013.  Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for sequential alloca-  tion problems. Advances in Applied Mathematics, 17(2):122 - 142, 1996.  Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages  2249-2257, 2011.  B. C. Dean, M. X. Goemans, and J. Vondrak. Approximating the stochastic knapsack problem: The benefit of adaptivity. In FOCS \u201904: Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science, pages 208-217, 2004.  A. Demberel, J. Chase, and S. Babu. Re\ufb02ective control for an elastic cloud appliation: An automated  experiment workbench. Proc. of HotCloud, 2009.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient  optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.  A. Garivier and O. Capp\u00b4e. The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond.  In Proc. COLT, 2011.  J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments.  Progress in statistics (European Meeting of Statisticians), 1972.  A. Goel, S. Khanna, and B. Null. The ratio index for budgeted learning, with applications. In SODA,  pages 18-27, 2009.  ICML, 2014.  58(1), 2010.  A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex bandit problems. In  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. J. ACM,  H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based optimization of mapreduce  programs. Proc. of VLDB, 2011. GUHA MUNAGALA  Emilie Kaufmann, Nathaniel Korda, and Remi Munos. Thompson sampling: An asymptotically  optimal finite-time analysis. Proceedings of ALT, 2012.  T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6:4-22, 1985.  15(3):1091-1114, 1987.  Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. Ann. Statist.,  L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news  article recommendation. In WWW, pages 661-670, 2010.  H. Robbins. Some aspects of the sequential design of experiments. Bulletin American Mathematical  Society, 55:527-535, 1952.  Daniel Russo and Benjamin Van-Roy. Learning to optimize via posterior sampling. CORR; http:  //arxiv.org/abs/1301.2609, 2013.  Steven L. Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in  Business and Industry, 26(6):639-658, 2010.  William R. Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):pp. 285-294, 1933.  N. Vulkan. An economist\u2019s perspective on probability matching. Journal of Economic Surveys,  1992.  "}, "Approachability in unknown games: Online learning meets multi-objective optimization": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Approachability in unknown games: Online learning meets multi-objective optimization", "abstract": "In the standard setting of approachability there are two players and a target set. The players play a repeated vector-valued game where one of them wants to have the average vector-valued payoff converge to the target set which the other player tries to exclude. We revisit the classical setting and consider the setting where the player has a preference relation between target sets: she wishes to approach the smallest (\u201cbest\u201d) set possible given the observed average payoffs in hindsight. Moreover, as opposed to previous works on approachability, and in the spirit of online learning, we do not assume that there is a known game structure with actions for two players. Rather, the player receives an arbitrary vector-valued reward vector at every round. We show that it is impossible, in general, to approach the best target set in hindsight. We further propose a concrete strategy that approaches a non-trivial relaxation of the best-in-hindsight given the actual rewards. Our approach does not require projection onto a target set and amounts to switching between scalar regret minimization algorithms that are performed in episodes.", "pdf_url": "http://proceedings.mlr.press/v35/mannor14.pdf", "keywords": ["Online learning", "multi-objective optimization", "approachability"], "reference": "Y. Azar, U. Feige, M. Feldman, and M. Tennenholtz. Sequential decision making with vector  outcomes. In Proceedings of ITCS, 2014.  W. Beibei and K.J.R. Liu. Advances in cognitive radio networks: a survey. IEEE Journal of Selected  Topics in Signal Processing, 5(1):5-23, 2011.  A. Bernstein and N. Shimkin. Response-based approachability and its application to generalized  no-regret algorithms. arXiv:1312.7658 [cs.LG], 2014.  A. Bernstein, S. Mannor, and N. Shimkin. Opportunistic strategies for generalized no-regret prob-  lems. In Proceedings of COLT, pages 158-171, 2013.  D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathemat-  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK,  ics, 6:1-8, 1956.  2004.  13   APPROACHABILITY IN UNKNOWN GAMES  c : \u2206(A) \u00d7 \u2206(B) \u2192 Rg. (With no loss of generality we can assume that the payoff function takes values in a bounded nonnegative interval.) The vector m of our general formulation corresponds to  m(y) =  (cid:20) u( \u00b7 , y) c( \u00b7 , y)  (cid:21)  .  The payoff set P to be be approached given the constraints is [M, +\u221e), that is, payoffs are to be maximized given the constraints: P\u03b1 = [M \u2212 \u03b1, +\u221e). Abusing the notation by not distinguishing between m(y) and y, we denote the maximal payoff under the constraint by \u03c6(cid:63)(y) = max(cid:8)u(x, y) : x \u2208 \u2206(A) s.t. c(x, y) \u2208 \u0393(cid:9) .  This target function corresponds to (4) in the same way as \u03c6\u03a8 corresponds to (9). Mannor et al. (2009) exactly proceed as we did in Section 3: they first show that \u03c6(cid:63) is unachievable in general and then show that the relaxed goal cav[\u03c6(cid:63)] can be achieved. They propose a computationally complex strategy to do so (based on calibration) but Bernstein and Shimkin (2014) already noted that simpler and more tractable strategies could achieve cav[\u03c6(cid:63)] as well.  The target function \u03c6x(cid:63)  , which we proved above to be achievable, improves on cav[\u03c6(cid:63)], even though, as in the remark concluding Section 5.1, it is difficult to quantify in general how much we gain. One should look at specific examples to quantify the improvement from cav[\u03c6(cid:63)] to \u03d5\u03c8 (as we do in Mannor et al., 2014, Section C). The added value in our approach mostly lies in the versatility: we do not need to assume that some known game is taking place.  Vianney Perchet acknowledges funding from the ANR, under grants ANR-10-BLAN-0112 and ANR-13-JS01-0004-01. Shie Mannor was partially supported by the ISF under contract 890015. Gilles Stoltz would like to thank Investissements d\u2019Avenir (ANR-11-IDEX-0003 / Labex Ecodec / ANR-11-LABX-0047) for financial support.  Acknowledgments  References  Y. Azar, U. Feige, M. Feldman, and M. Tennenholtz. Sequential decision making with vector  outcomes. In Proceedings of ITCS, 2014.  W. Beibei and K.J.R. Liu. Advances in cognitive radio networks: a survey. IEEE Journal of Selected  Topics in Signal Processing, 5(1):5-23, 2011.  A. Bernstein and N. Shimkin. Response-based approachability and its application to generalized  no-regret algorithms. arXiv:1312.7658 [cs.LG], 2014.  A. Bernstein, S. Mannor, and N. Shimkin. Opportunistic strategies for generalized no-regret prob-  lems. In Proceedings of COLT, pages 158-171, 2013.  D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathemat-  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK,  ics, 6:1-8, 1956.  2004. N. Cesa-Bianchi and G. Lugosi. Potential-based algorithms in on-line prediction and game theory.  Machine Learning, 3(51):239-261, 2003.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with  expert advice. Machine Learning, 66(2/3):321-352, 2007.  S. de Rooij, T. van Erven, P.D. Gr\u00a8unwald, and W. Koolen. Follow the leader if you can, hedge if  you must. Journal of Machine Learning Research, 2014. In press.  E. Even-Dar, R. Kleinberg, S. Mannor, and Y. Mansour. Online learning for global cost functions.  In Proceedings of COLT, 2009.  J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer-Verlag, 2001.  T.-F. Hou. Approachability in a two-person game. The Annals of Mathematical Statistics, 42:  735-744, 1971.  C.-L. Hwang and A.S. Md Masud. Multiple Objective Decision Making, Methods and Applications:  a state-of-the-art survey. Springer-Verlag, 1979.  S. Mannor and J. N. Tsitsiklis. Approachability in repeated games: Computational aspects and a  Stackelberg variant. Games and Economic Behavior, 66(1):315-325, 2009.  S. Mannor, J.N. Tsitsiklis, and J.Y. Yu. Online learning with sample path constraints. Journal of  Machine Learning Research, 10:569-590, 2009.  S. Mannor, V. Perchet, and G. Stoltz. Approachability in unknown games: Online learning meets  multi-objective optimization. arXiv:1402.2043 [stat.ML], 2014.  K. Miettinen. Nonlinear Multiobjective Optimization. Springer, 1999.  H. Simon. Cognitive radio: brain-empowered wireless communications. IEEE Journal on Selected  Areas in Communications, 23(2):201-220, 2005.  X. Spinat. A necessary and sufficient condition for approachability. Mathematics of Operations  Research, 27:31-44, 2002. APPROACHABILITY IN UNKNOWN GAMES  "}, "Belief propagation, robust reconstruction and optimal recovery of block models": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Belief propagation, robust reconstruction and optimal recovery of block models", "abstract": "We consider the problem of reconstructing sparse symmetric block models with two blocks and connection probabilities a/n and b/n for inter- and intra-block edge probabilities respectively. It was recently shown that one can do better than a random guess if and only if (a-b)^2 > 2(a+b). Using a variant of Belief Propagation, we give a reconstruction algorithm that is \\emphoptimal in the sense that if (a-b)^2 > C (a+b) for some constant C then our algorithm maximizes the fraction of the nodes labelled correctly. Along the way we prove some results of independent interest regarding \\em robust reconstruction for the Ising model on regular and Poisson trees.", "pdf_url": "http://proceedings.mlr.press/v35/mossel14.pdf", "keywords": [], "reference": "P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other modularities. Proceedings of the National Academy of Sciences, 106(50):21068- 21073, 2009.  P. M. Bleher, J. Ruiz, and V. A. Zagrebnov. On the purity of the limiting Gibbs state for  the Ising model on the Bethe lattice. J. Statist. Phys., 79(1-2):473-482, 1995.  C. Borgs, J. Chayes, E. Mossel, and S. Roch. The Kesten-Stigum reconstruction bound is tight for roughly symmetric binary channels. In Proceedings of IEEE FOCS 2006, pages 518-530, 2006.  13   Belief Propagation, Robust Reconstructionand Optimal Recovery of Block Models  need to \u201calign\u201d these multiple applications of BBPartition. This is the purpose of u\u2217: since u\u2217 has high degree and BBPartition labels most vertices correctly, it follows from the law of large numbers that with high probability, BBPartition gives most of u\u2217\u2019s neighbors the same label as u\u2217. (For this to hold, we need independence between the label of u\u2217 and the output of BBPartition; this is why we remove u\u2217 from the graph before running BBPartition.)  From now on, suppose without loss of generality that \u03c3u\u2217 = +. Thanks to the previous paragraph and Theorem 19, we see that the relabelling in lines 1 and 1 correctly aligns W + v with V +:  Lemma 22 There is some 0 \u2264 \u03b4 < 1 with W + v defined as in line 1 or line 1.  2 such that for any v \u2208 V \\ U , |W +  v \u2206V +| \u2264 \u03b4n a.a.s.,  To complete the proof of (8) (and hence Theorem 21), we need to discuss the coupling between graphs and trees. We will invoke a lemma from Mossel et al. (2013) which says that a neighborhood in G can be coupled with a multi-type branching process. Indeed, let T be a Galton-Watson tree with o\ufb00spring distribution Pois((a + b)/2) and let \u03c3(cid:48) be a labelling on it, given by running the tree broadcast process with parameter \u03b7 = b/(a + b). We write TR for T \u222a NR; that is, the part of T which has depth at most R.  Lemma 23 For any fixed v \u2208 G, there is a coupling between (G, \u03c3) and (T, \u03c3(cid:48)) such that  (B(v, R), \u03c3B(v,R)) = (TR, \u03c3(cid:48)  ) a.a.s.  TR  Let us therefore examine the labelling {\u03beu : u \u2208 S(v, R)} produced in line 1 of Algo- rithm 1. Since \u03be is independent of the edges from B(v, R \u2212 1) to G(cid:48), it follows that for every neighbor w \u2208 G(cid:48) of u \u2208 B(v, R\u22121), we may generate (independently of the other neighbors) \u03bew by \ufb02ipping \u03c3w with probability (1 \u2212 o(1))\u03b4. Hence, we see that \u03be can be coupled a.a.s. with \u03c4 (cid:48), where \u03c4 (cid:48) w (independently for each w) with probability \u03b4. In other words, the joint distribution of B(v, R) and {\u03beu : u \u2208 S(v, R)} is a.a.s. the same as the joint distribution of TR and {\u03c4 (cid:48) u : u \u2208 \u2202TR}. Hence, by Theorem 11,  w is defined by \ufb02ipping the label of \u03c3(cid:48)  lim n\u2192\u221e  Pr(Yv,R(\u03be) = \u03c3v) = pT (a, b).  By line 1 of Algorithm 1, this completes the proof of (8).  References  P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other modularities. Proceedings of the National Academy of Sciences, 106(50):21068- 21073, 2009.  P. M. Bleher, J. Ruiz, and V. A. Zagrebnov. On the purity of the limiting Gibbs state for  the Ising model on the Bethe lattice. J. Statist. Phys., 79(1-2):473-482, 1995.  C. Borgs, J. Chayes, E. Mossel, and S. Roch. The Kesten-Stigum reconstruction bound is tight for roughly symmetric binary channels. In Proceedings of IEEE FOCS 2006, pages 518-530, 2006. Mossel Neeman Sly  T.N. Bui, S. Chaudhuri, F.T. Leighton, and M. Sipser. Graph bisection algorithms with  good average case behavior. Combinatorica, 7(2):171-191, 1987.  A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics,  Probability and Computing, 19(02):227-284, 2010.  A. Condon and R.M. Karp. Algorithms for graph partitioning on the planted partition  model. Random Structures and Algorithms, 18(2):116-140, 2001.  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physics Review E, 84:066106, Dec 2011.  M.E. Dyer and A.M. Frieze. The solution of some random NP-hard problems in polynomial  expected time. Journal of Algorithms, 10(4):451-489, 1989.  W. S. Evans, C. Kenyon, Yuval Y. Peres, and L. J. Schulman. Broadcasting on trees and  the Ising model. Ann. Appl. Probab., 10(2):410-433, 2000.  P.W. Holland, K.B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social  Networks, 5(2):109 - 137, 1983. ISSN 0378-8733.  Alexander T Ihler, John Iii, and Alan S Willsky. Loopy belief propagation: Convergence and e\ufb00ects of message errors. In Journal of Machine Learning Research, pages 905-936, 2005.  S. Janson and E. Mossel. Robust reconstruction on trees is determined by the second  eigenvalue. Ann. Probab., 32:2630-2649, 2004.  M. Jerrum and G.B. Sorkin. The Metropolis algorithm for graph bisection. Discrete Applied  Mathematics, 82(1-3):155-175, 1998.  H. Kesten and B. P. Stigum. Additional limit theorems for indecomposable multidimensional  Galton-Watson processes. Ann. Math. Statist., 37:1463-1481, 1966.  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zde- borov, and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of Sciences, 110(52):20935-20940, 2013.  J. Leskovec, K.J. Lang, A. Dasgupta, and M.W. Mahoney. Statistical properties of com- In Proceeding of the 17th  munity structure in large social and information networks. international conference on World Wide Web, pages 695-704. ACM, 2008.  L. Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property.  arXiv:1311:3085, 2014.  F. McSherry. Spectral partitioning of random graphs. In 42nd IEEE Symposium on Foun-  dations of Computer Science, pages 529-537. IEEE, 2001.  Andrea Montanari, Elchanan Mossel, and Allan Sly. The weak limit of Ising models on  locally tree-like graphs. Probability Theory and Related Fields, 152:31-51, 2012. Belief Propagation, Robust Reconstructionand Optimal Recovery of Block Models  E. Mossel, J. Neeman, and A. Sly.  Stochastic block models and reconstruction.  arXiv:1202.4124, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold con-  jecture. arXiv:1311.4115, 2014a.  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction,  and optimal recovery of block models. arXiv:1309.1380, 2014b.  A. Sly. Reconstruction for the Potts model. Annals of Probability, 39(4):1365-1406, 2011.  T.A.B. Snijders and K. Nowicki. Estimation and prediction for stochastic blockmodels for  graphs with latent block structure. Journal of Classification, 14(1):75-100, 1997.  S.H. Strogatz. Exploring complex networks. Nature, 410(6825):268-276, 2001. "}, "Sample Compression for Multi-label Concept Classes": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Sample Compression for Multi-label Concept Classes", "abstract": "This paper studies labeled sample compression for multi-label concept classes. For a specific extension of the notion of VC-dimension to multi-label classes, we prove that every maximum multi-label class of dimension d has a sample compression scheme in which every sample is compressed to a subset of size at most d. We further show that every multi-label class of dimension 1 has a sample compression scheme using only sets of size at most 1. As opposed to the binary case, the latter result is not immediately implied by the former, since there are multi-label concept classes of dimension 1 that are not contained in maximum classes of dimension 1.", "pdf_url": "http://proceedings.mlr.press/v35/samei14.pdf", "keywords": ["multi-label concept classes", "sample compression", "VC dimension"], "reference": "N. Alon. On the density of sets of vectors. Discrete Mathematics, 46(2):199-202, 1983.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. M. Long. Characterizations of learnability for classes of {0, ..., n}-valued functions. Journal of Computer and System Sciences, 50(1):74-86, 1995.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-  Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. In COLT, volume 19 of JMLR Proceedings, pages 207-232. JMLR.org, 2011.  S. Floyd and M. K. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis  dimension. Machine Learning, 21(3):269-304, 1995.  S. A. Goldman and M. J. Kearns. On the complexity of teaching. Journal of Computer and System  Sciences, 50:20-31, 1995.  L. Gurvits. Linear algebraic proofs of VC-dimension based inequalities.  In Proceedings of the Third European Conference on Computational Learning Theory, EuroCOLT \u201997, pages 238-250, London, UK, 1997. Springer-Verlag.  D. Haussler and P. M. Long. A generalization of Sauer\u2019s lemma. Journal of Combinatorial Theory,  Series A, 71(2):219-240, 1995.  N. Littlestone and M. Warmuth. Relating data compression and learnability. unpublished notes,  1986.  B. K. Natarajan. On learning sets and functions. Machine Learning, 4(1):67-97, 1989.  D. Pollard. Empirical Processes: Theory and Applications. NSF-CBMS Regional Conference Series  in Probability and Statistics, 2:pp. i-iii+v+vii-viii+1-86, 1990.  B. I. P. Rubinstein, P. L. Bartlett, and J. H. Rubinstein. Shifting: One-inclusion mistake bounds and  sample compression. Journal of Computer and System Sciences, 75(1):37-59, 2009.  N. Sauer. On the density of families of sets. J. Comb. Theory, Ser. A, 13(1):145-147, 1972.  A. Shinohara and S. Miyano. Teachability in computational learning. New Generation Computing,  8(4):337-347, 1991.  Letters, 110(8-9):341-344, 2010.  (4):299-300, 1997.  H. U. Simon and B. Sz\u00a8or\u00b4enyi. One-inclusion hypergraph density revisited. Information Processing  R. Smolensky. Well-known bound for the VC-dimension made easy. Computational Complexity, 6  14   SAMEI SEMUKHIN YANG ZILLES  References  N. Alon. On the density of sets of vectors. Discrete Mathematics, 46(2):199-202, 1983.  S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. M. Long. Characterizations of learnability for classes of {0, ..., n}-valued functions. Journal of Computer and System Sciences, 50(1):74-86, 1995.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-  Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the ERM  principle. In COLT, volume 19 of JMLR Proceedings, pages 207-232. JMLR.org, 2011.  S. Floyd and M. K. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis  dimension. Machine Learning, 21(3):269-304, 1995.  S. A. Goldman and M. J. Kearns. On the complexity of teaching. Journal of Computer and System  Sciences, 50:20-31, 1995.  L. Gurvits. Linear algebraic proofs of VC-dimension based inequalities.  In Proceedings of the Third European Conference on Computational Learning Theory, EuroCOLT \u201997, pages 238-250, London, UK, 1997. Springer-Verlag.  D. Haussler and P. M. Long. A generalization of Sauer\u2019s lemma. Journal of Combinatorial Theory,  Series A, 71(2):219-240, 1995.  N. Littlestone and M. Warmuth. Relating data compression and learnability. unpublished notes,  1986.  B. K. Natarajan. On learning sets and functions. Machine Learning, 4(1):67-97, 1989.  D. Pollard. Empirical Processes: Theory and Applications. NSF-CBMS Regional Conference Series  in Probability and Statistics, 2:pp. i-iii+v+vii-viii+1-86, 1990.  B. I. P. Rubinstein, P. L. Bartlett, and J. H. Rubinstein. Shifting: One-inclusion mistake bounds and  sample compression. Journal of Computer and System Sciences, 75(1):37-59, 2009.  N. Sauer. On the density of families of sets. J. Comb. Theory, Ser. A, 13(1):145-147, 1972.  A. Shinohara and S. Miyano. Teachability in computational learning. New Generation Computing,  8(4):337-347, 1991.  Letters, 110(8-9):341-344, 2010.  (4):299-300, 1997.  H. U. Simon and B. Sz\u00a8or\u00b4enyi. One-inclusion hypergraph density revisited. Information Processing  R. Smolensky. Well-known bound for the VC-dimension made easy. Computational Complexity, 6 SAMPLE COMPRESSION FOR MULTI-LABEL CONCEPT CLASSES  V. N. Vapnik. Inductive principles of the search for empirical dependences (methods based on weak In Proceedings of the 2nd Annual Workshop on Com- convergence of probability measures). putational Learning Theory, COLT \u201989, pages 3-21, San Francisco, CA, USA, 1989. Morgan Kaufmann Publishers Inc.  E. Welzl. Complete range spaces. unpublished notes, 1987.  E. Welzl and G. Woeginger. On Vapnik-Chervonenkis dimension one. unpublished notes, 1987.  "}, "Finding a most biased coin with fewest flips": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Finding a most biased coin with fewest flips", "abstract": "We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin whose posterior probability of being most biased is at least 1-\u03b4for a given \u03b4. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy\u2014a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any given starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs mathematical tools from the area of Markov games.", "pdf_url": "http://proceedings.mlr.press/v35/chandrasekaran14.pdf", "keywords": ["algorithms", "learning", "bandits", "biased coin", "Bayesian", "ranking and selection", "sequential selection"], "reference": "Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best Arm Identification in Multi-Armed Bandits. In Proceedings of the 23rd Conference on Learning Theory, COLT \u201910, pages 41-53, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, May 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multi-  armed Bandit Problem. SIAM Journal of Computing, 32:48-77, Jan 2003.  R. E. Bechhofer. A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances. The Annals of Mathematical Statistics, 25:16-39, 1954.  Robert E. Bechhofer, Thomas J. Santner, and David M. Goldsman. Design and Analysis of Ex- periments for Statistical Selection, Screening, and Multiple Comparisons. Wiley-Interscience, 1995.  Donald A. Berry and Bert Fristedt. Bandit Problems: Sequential Allocation of Experiments (Mono-  graphs on Statistics and Applied Probability). Chapman & Hall, Oct 1985.  Justin Boesel, Barry L. Nelson, and Seong-Hee Kim. Using Ranking and Selection to \u201cClean Up\u201d  after Simulation Optimization. Operations Research, 51(5):814-825, 2003.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob- In Proceedings of the 20th international conference on Algorithmic learning theory,  lems. ALT\u201909, pages 23-37, 2009.  Stephen E. Chick and Noah Gans. Economic Analysis of Simulation Selection Problems. Manage-  ment Science, 55(3):421-437, 2009.  Stephen E. Chick and Koichiro Inoue. New Two-Stage and Sequential Procedures for Selecting the  Best Simulated System. Operations Research, 49(5):732-743, 2001.  Vincent Cicirello and Stephen Smith. The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection. In 20th National Conference on Artificial Intelligence, AAAI \u201905, pages 1355-1361, 2005.  Ioana Dumitriu, Prasad Tetali, and Peter Winkler. On Playing Golf with Two Balls. SIAM Journal  of Discrete Mathematics, 16:604-615, Apr 2003.  S. N. Ethier and Davar Khoshnevisan. Bounds on Gambler\u2019s Ruin Probabilities in Terms of Mo-  ments. Methodology and Computing in Applied Probability, 4(1):55-68, Mar 2002.  13   FINDING A MOST BIASED COIN WITH FEWEST FLIPS  Acknowledgments  We thank Santosh Vempala for valuable comments.  References  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best Arm Identification in Multi-Armed Bandits. In Proceedings of the 23rd Conference on Learning Theory, COLT \u201910, pages 41-53, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit  Problem. Machine Learning, 47:235-256, May 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multi-  armed Bandit Problem. SIAM Journal of Computing, 32:48-77, Jan 2003.  R. E. Bechhofer. A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances. The Annals of Mathematical Statistics, 25:16-39, 1954.  Robert E. Bechhofer, Thomas J. Santner, and David M. Goldsman. Design and Analysis of Ex- periments for Statistical Selection, Screening, and Multiple Comparisons. Wiley-Interscience, 1995.  Donald A. Berry and Bert Fristedt. Bandit Problems: Sequential Allocation of Experiments (Mono-  graphs on Statistics and Applied Probability). Chapman & Hall, Oct 1985.  Justin Boesel, Barry L. Nelson, and Seong-Hee Kim. Using Ranking and Selection to \u201cClean Up\u201d  after Simulation Optimization. Operations Research, 51(5):814-825, 2003.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob- In Proceedings of the 20th international conference on Algorithmic learning theory,  lems. ALT\u201909, pages 23-37, 2009.  Stephen E. Chick and Noah Gans. Economic Analysis of Simulation Selection Problems. Manage-  ment Science, 55(3):421-437, 2009.  Stephen E. Chick and Koichiro Inoue. New Two-Stage and Sequential Procedures for Selecting the  Best Simulated System. Operations Research, 49(5):732-743, 2001.  Vincent Cicirello and Stephen Smith. The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection. In 20th National Conference on Artificial Intelligence, AAAI \u201905, pages 1355-1361, 2005.  Ioana Dumitriu, Prasad Tetali, and Peter Winkler. On Playing Golf with Two Balls. SIAM Journal  of Discrete Mathematics, 16:604-615, Apr 2003.  S. N. Ethier and Davar Khoshnevisan. Bounds on Gambler\u2019s Ruin Probabilities in Terms of Mo-  ments. Methodology and Computing in Applied Probability, 4(1):55-68, Mar 2002. CHANDRASEKARAN KARP  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC Bounds for Multi-armed Bandit and Markov Decision Processes. In Proceedings of the 15th Annual Conference on Computational Learning Theory, COLT \u201902, pages 255-270, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. J. Mach. Learn. Res., 7: 1079-1105, December 2006.  Peter I. Frazier, Warren B. Powell, and Savas Dayanik. A Knowledge-Gradient Policy for Sequential Information Collection. SIAM Journal on Control and Optimization, 47(5):2410-2439, 2008.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and S\u00b4ebastien Bubeck. Multi- Bandit Best Arm Identification. In Advances in Neural Information Processing Systems, NIPS \u201911, pages 2222-2230, 2011.  John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed Bandit Allocation Indices. Wiley,  2nd edition, 2011.  Shanti S. Gupta and Klaus J. Miescke. Bayesian look ahead one-stage sampling allocations for selection of the best population. Journal of Statistical Planning and Inference, 54(2):229-244, 1996.  S. H. Kim and B. L. Nelson. Selecting the best system. Handbooks in Operations Research and  Management Science: Simulation, pages 501-534, 2006.  T.L. Lai and H. Robbins. Asymptotically Efficient Adaptive Allocation Rules. Advances in Applied  Mathematics, 6(1):4-22, 1985.  Shie Mannor and John N. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed  Bandit Problem. Journal of Machine Learning Research, 5:623-648, Dec 2004.  Oded Maron and Andrew Moore. Hoeffding Races: Accelerating Model Selection Search for Clas- sification and Function Approximation. In Advances in Neural Information Processing Systems, volume 6, pages 59-66, April 1994.  E. Paulson. A Sequential Procedure for Selecting the Population with the Largest Mean from k  Normal Populations. The Annals of Mathematical Statistics, 35:174-180, 1964.  J. Pichitlamken and B. L. Nelson. Selection-of-the-best procedures for optimization via simulation.  In Proceedings of the 2001 Winter Simulation Conference, pages 401-407, 2001. "}, "Volumetric Spanners: an Efficient Exploration Basis for Learning ": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Volumetric Spanners: an Efficient Exploration Basis for Learning ", "abstract": "Numerous machine learning problems require an \\it exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.", "pdf_url": "http://proceedings.mlr.press/v35/hazan14b.pdf", "keywords": ["Learning basis", "Multi-Armed Bandit", "Active Learning", "Spanners", "Convex Geometry"], "reference": "J.D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit  online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Minimax policies for combinatorial prediction games. In Sham M. Kakade and Ulrike von Luxburg, editors, COLT, volume 19 of JMLR Proceedings, pages 107-132. JMLR.org, 2011.  Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. Com-  put. Syst. Sci., 74(1):97-114, 2008.  Keith Ball. An elementary introduction to modern convex geometry. In Flavors of Geometry, pages  1-58. Univ. Press, 1997.  Peter L. Bartlett, Varsha Dani, Thomas P. Hayes, Sham Kakade, Alexander Rakhlin, and Ambuj Tewari. High-probability regret bounds for bandit online linear optimization. In COLT, pages 335-342, 2008.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012.  12   HAZAN KARNIN MEKA  for any point in Rd, assuming access to a membership oracle for K. Here, we could use Lemma 10 in order to approximately sample from the required distribution.  A naive analysis results in a running time polynomial in both d and T for each such step, com- pleting the proof of the existence of a solution with polynomial running time. A more refined analysis for exactly such processes can be found in (Dick et al., 2014). Their analysis of the con- tinuous exponential weights algorithm (i.e., the geometric hedge algorithm) in their section 3.2 and 3.3, gives rise to a sampling technique that approximates the required log-concave distribution in a manner that incurs an additional O( T ) regret, at the running time cost of poly(d, log(T )) per sample. As the purpose of this paper is to provide an algorithm of polynomial running time, we defer the complete analysis of the running time to the full version of the paper.  \u221a  Discussion: Notice that to obtain a (p, \u03b5)-exp-volumetric spanner for a log-concave distribution p over a body K we simply choose sufficiently many i.i.d samples from p. Since in the above algorithm pt is always log-concave, it follows that S(cid:48) t consists of i.i.d samples from pt, meaning that if we would not have required S(cid:48)(cid:48) t , the exploration and exploration strategies would be the same! Since we still require the set S(cid:48)(cid:48) t , there exists a need for a separate exploration strategy. \u221a Interestingly, the 2 d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of Dani et al. (2007).  The authors would like to thank Carmel Domshlak for helpful discussions in earlier stages of this work.  The first author is supported by the European Union\u2019s Seventh Framework Programme (FP7/2007-  2013) under grant agreement n\u25e6 336078 - ERC-SUBLRN.  Acknowledgments  References  J.D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit  online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Minimax policies for combinatorial prediction games. In Sham M. Kakade and Ulrike von Luxburg, editors, COLT, volume 19 of JMLR Proceedings, pages 107-132. JMLR.org, 2011.  Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. Com-  put. Syst. Sci., 74(1):97-114, 2008.  Keith Ball. An elementary introduction to modern convex geometry. In Flavors of Geometry, pages  1-58. Univ. Press, 1997.  Peter L. Bartlett, Varsha Dani, Thomas P. Hayes, Sham Kakade, Alexander Rakhlin, and Ambuj Tewari. High-probability regret bounds for bandit online linear optimization. In COLT, pages 335-342, 2008.  Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM  Journal on Computing, 41(6):1704-1721, 2012. VOLUMETRIC SPANNERS  S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed  Bandit Problems, volume 5 of Foundations and Trends in Machine Learning. NOW, 2012.  S\u00b4ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. Journal of Machine Learning Research - Pro- ceedings Track, 23:41.1-41.14, 2012.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. J. Comput. Syst. Sci., 78(5):1404-  1422, 2012.  S. Damla Ahipasaoglu, Peng Sun, and Michael J. Todd. Linear convergence of a modified frankwolfe algorithm for computing minimum-volume enclosing ellipsoids. Optimization Meth- ods and Software, 23(1):5-19, 2008.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2007.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit  feedback. In COLT, pages 355-366, 2008.  Travis Dick, Andras Gyorgy, and Csaba Szepesvari. Online learning in markov decision processes with changing cost sequences. In Proceedings of The 31st International Conference on Machine Learning, pages 512-520, 2014.  Elad Hazan, Zohar Shay Karnin, and Raghu Mehka. Volumetric spanners and their applications to  machine learning. CoRR, abs/1312.6214, 2013.  Martin Henk. L\u00a8owner-John ellipsoids. Documenta Mathematica, pages 95-106, 2012.  F. John. Extremum Problems with Inequalities as Subsidiary Conditions. In K. O. Friedrichs, O. E. Neugebauer, and J. J. Stoker, editors, Studies and Essays: Courant Anniversary Volume, pages 187-204. Wiley-Interscience, New York, 1948.  Sham M Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation  algorithms. SIAM Journal on Computing, 39(3):1088-1106, 2009.  Leonid G Khachiyan. Rounding of polytopes in the real number model of computation. Mathemat-  ics of Operations Research, 21(2):307-320, 1996.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algo-  rithms. Random Structures & Algorithms, 30(3):307-358, 2007.  M. Rudelson. Random vectors in the isotropic position. Journal of Functional Analysis, 164(1):  60-72, 1999. HAZAN KARNIN MEKA  "}, "lil\u2019 UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "lil\u2019 UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits", "abstract": "The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples.  The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art.", "pdf_url": "http://proceedings.mlr.press/v35/jamieson14.pdf", "keywords": ["Multi-armed bandit", "upper confidence bound (UCB)", "iterated logarithm"], "reference": "Yasin Abbasi-Yadkori, Csaba Szepesv\u00b4ari, and David Tax. Improved algorithms for linear stochastic  bandits. In Advances in Neural Information Processing Systems, pages 2312-2320, 2011.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best arm identification in multi-armed  bandits. COLT 2010-Proceedings, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs. Biometrics, 14(3):408-429, 1958.  S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Pro- ceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  DA Darling and Herbert Robbins.  Iterated logarithm inequalities.  In Herbert Robbins Selected  Papers, pages 254-258. Springer, 1985.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  R. H. Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals  of Mathematical Statistics, 35(1):pp. 36-72, 1964. ISSN 00034851.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Team SequeL. Best arm iden- tification: A unified approach to fixed budget and fixed confidence. In NIPS, pages 3221-3229, 2012.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On finding the largest  mean among many. arXiv preprint arXiv:1306.3917, 2013.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.  In Proceedings of the 30th International Conference on Machine Learning, 2013.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. COLT, 2013.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004.  Edward Paulson. A sequential procedure for selecting the population with the largest mean from k  normal populations. The Annals of Mathematical Statistics, 35(1):174-180, 1964.  14   JAMIESON MALLOY NOWAK BUBECK  References  Yasin Abbasi-Yadkori, Csaba Szepesv\u00b4ari, and David Tax. Improved algorithms for linear stochastic  bandits. In Advances in Neural Information Processing Systems, pages 2312-2320, 2011.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and R\u00b4emi Munos. Best arm identification in multi-armed  bandits. COLT 2010-Proceedings, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002.  Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs. Biometrics, 14(3):408-429, 1958.  S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Pro- ceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  DA Darling and Herbert Robbins.  Iterated logarithm inequalities.  In Herbert Robbins Selected  Papers, pages 254-258. Springer, 1985.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  R. H. Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals  of Mathematical Statistics, 35(1):pp. 36-72, 1964. ISSN 00034851.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Team SequeL. Best arm iden- tification: A unified approach to fixed budget and fixed confidence. In NIPS, pages 3221-3229, 2012.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On finding the largest  mean among many. arXiv preprint arXiv:1306.3917, 2013.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.  In Proceedings of the 30th International Conference on Machine Learning, 2013.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. COLT, 2013.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004.  Edward Paulson. A sequential procedure for selecting the population with the largest mean from k  normal populations. The Annals of Mathematical Statistics, 35(1):174-180, 1964. LIL\u2019 UCB : AN OPTIMAL EXPLORATION ALGORITHM FOR MULTI-ARMED BANDITS  "}, "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning", "abstract": "From concentration inequalities for the suprema of Gaussian or Rademacher processes an inequality is derived. It is applied to sharpen existing and to derive novel bounds on the empirical Rademacher complexities of unit balls in various norms appearing in the context of structured sparsity and multitask dictionary learning or matrix factorization. A key role is played by the largest eigenvalue of the data covariance matrix.", "pdf_url": "http://proceedings.mlr.press/v35/maurer14.pdf", "keywords": ["Concentration inequalities", "multitask learning", "Rademacher complexity", "risk bounds", "structured sparsity"], "reference": "R. K. Ando, T. Zhang. A framework for learning predictive structures from multiple tasks  and unlabeled data. Journal of Machine Learning Research, 6:1817-1853, 2005.  A, Argyriou, R. Foygel, N. Srebro. Sparse prediction with the k\u2212support norm. Advances  in Neural Information Processing Systems 25, pages 1466-1474, 2012.  F. R. Bach, G.R.G. Lanckriet and M. I. Jordan. Multiple kernels learning, conic duality, and the SMO algorithm. Proc. 21st International Conference on Machine Learning, 2004.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian Complexities: Risk Bounds  and Structural Results. Journal of Machine Learning Research, 3:463-482, 2002.  J. Baxter. A Model of Inductive Bias Learning, Journal of Artificial Intelligence Research,  12:149-198, 2000.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. Proc.  16th Annual Conference on Computational Learning Theory, pages 567-580, 2003.  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities using the entropy method,  Annals of Probability, 31(3):1145-1712, 2003  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities, Oxford University Press,  2013.  2001.  C. Cortes, M. Mohri, A. Rostamizadeh. Generalization bounds for learning kernels. Proc.  27th International Conference on Machine Learning (ICML 2010), 2010.  F. Cucker and S. Smale. On the mathematical foundations of learning, Bulletin of the  American Mathematical Society, 39(1):1-49, 2001.  L. Jacob, G. Obozinski, J. P. Vert. Group Lasso with overlap and graph Lasso. Proc. 26th  International Conference on Machine Learning (ICML 2009), pages 433-440, 2009.  S. M. Kakade, S. Shalev-Shwartz, A. Tewari. Regularization techniques for learning with  matrices. Journal of Machine Learning Research, 13:1865-1890, 2012.  V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the gen-  eralization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.  M. Ledoux, M. Talagrand. Probability in Banach Spaces, Springer, 1991.  M.Ledoux. The Concentration of Measure Phenomenon, AMS Surveys and Monographs 89,  H. Liu, M. Palatucci, and J. Zhang. Blockwise coordinate descent procedures for the multi- task lasso, with applications to neural semantic basis discovery. Proc. 27th International Conference on Machine Learning, pages 82-89, 2009.  A. Maurer. Concentration inequalities for functions of independent variables. Random Struc-  tures and Algorithms, 29:121-138, 2006.  13   An Inequality with Applications...  References  R. K. Ando, T. Zhang. A framework for learning predictive structures from multiple tasks  and unlabeled data. Journal of Machine Learning Research, 6:1817-1853, 2005.  A, Argyriou, R. Foygel, N. Srebro. Sparse prediction with the k\u2212support norm. Advances  in Neural Information Processing Systems 25, pages 1466-1474, 2012.  F. R. Bach, G.R.G. Lanckriet and M. I. Jordan. Multiple kernels learning, conic duality, and the SMO algorithm. Proc. 21st International Conference on Machine Learning, 2004.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian Complexities: Risk Bounds  and Structural Results. Journal of Machine Learning Research, 3:463-482, 2002.  J. Baxter. A Model of Inductive Bias Learning, Journal of Artificial Intelligence Research,  12:149-198, 2000.  S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. Proc.  16th Annual Conference on Computational Learning Theory, pages 567-580, 2003.  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities using the entropy method,  Annals of Probability, 31(3):1145-1712, 2003  S. Boucheron, G. Lugosi, P. Massart. Concentration Inequalities, Oxford University Press,  2013.  2001.  C. Cortes, M. Mohri, A. Rostamizadeh. Generalization bounds for learning kernels. Proc.  27th International Conference on Machine Learning (ICML 2010), 2010.  F. Cucker and S. Smale. On the mathematical foundations of learning, Bulletin of the  American Mathematical Society, 39(1):1-49, 2001.  L. Jacob, G. Obozinski, J. P. Vert. Group Lasso with overlap and graph Lasso. Proc. 26th  International Conference on Machine Learning (ICML 2009), pages 433-440, 2009.  S. M. Kakade, S. Shalev-Shwartz, A. Tewari. Regularization techniques for learning with  matrices. Journal of Machine Learning Research, 13:1865-1890, 2012.  V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the gen-  eralization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.  M. Ledoux, M. Talagrand. Probability in Banach Spaces, Springer, 1991.  M.Ledoux. The Concentration of Measure Phenomenon, AMS Surveys and Monographs 89,  H. Liu, M. Palatucci, and J. Zhang. Blockwise coordinate descent procedures for the multi- task lasso, with applications to neural semantic basis discovery. Proc. 27th International Conference on Machine Learning, pages 82-89, 2009.  A. Maurer. Concentration inequalities for functions of independent variables. Random Struc-  tures and Algorithms, 29:121-138, 2006. Maurer Pontil Romera-Paredes  A. Maurer. Thermodynamics and concentration. Bernoulli, 18(2):434-454, 2012.  A. Maurer, M. Pontil. Structured sparsity and generalization. Journal of Machine Learning  Research, 13:671-690, 2012.  A. Maurer and M. Pontil. Excess risk bounds for multitask learning with trace norm regu-  larization. Proc. 26th Annual Conference on Learning Theory, pages 55-76, 2013.  A. Maurer, M. Pontil, B. Romera-Paredes. Sparse coding for multitask and transfer learning.  Proc. 30th International Conference on Machine Learning, pages 343-351, 2013  C. McDiarmid. Concentration. In Probabilistic Methods of Algorithmic Discrete Mathemat-  ics, pages 195-248, Springer, 1998.  C. A. Micchelli, J. M. Morales, M. Pontil. Regularizers for structured sparsity. Advances in  Computational Mathematics, 38(3):455-489, 2013.  S. Negahban, M. Wainwright. Joint support recovery under high-dimensional scaling: Bene- fits and perils of (cid:96)1/(cid:96)\u221e regularization. Advances in Neural Information Processing Systems 21 pages 1161-1168, 2008.  Y. Ying and C. Campbell. Generalization bounds for learning the kernel problem. Proc.  22nd Conference on Learning Theory, 2009.  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B (Statistical Methodology), 68(1): 49-67, 2006. An Inequality with Applications...  4. "}, "On the Complexity of AB Testing": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "On the Complexity of AB Testing", "abstract": "A/B testing refers to the task of determining the best option among two alternatives that yield random outcomes. We provide distribution-dependent lower bounds for the performance of A/B testing that improve over the results currently available both in the fixed-confidence (or \u03b4-PAC) and fixed-budget settings. When the distribution of the outcomes are Gaussian, we prove that the complexity of the fixed-confidence and fixed-budget settings are equivalent, and that uniform sampling of both alternatives is optimal only in the case of equal variances. In the common variance case, we also provide a stopping rule that terminates faster than existing fixed-confidence algorithms. In the case of Bernoulli distributions, we show that the complexity of fixed-budget setting is smaller than that of fixed-confidence setting and that uniform sampling of both alternatives\u2014though not optimal\u2014is advisable in practice when combined with an appropriate stopping criterion.", "pdf_url": "http://proceedings.mlr.press/v35/kaufmann14.pdf"}, "Elicitation and Identification of Properties": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Elicitation and Identification of Properties", "abstract": "Properties of distributions are real-valued functionals such as the mean, quantile or conditional value at risk. A property is elicitable if there exists a  scoring function such that minimization of the associated risks recovers the property. We extend existing results to characterize the elicitability of properties in a general setting. We further relate elicitability to identifiability (a notion introduced by Osband) and provide a general formula describing all scoring functions for an elicitable property. Finally, we draw some connections to the theory of coherent risk measures.", "pdf_url": "http://proceedings.mlr.press/v35/steinwart14.pdf", "keywords": ["Elicitation", "property", "scoring function", "identification function", "risk measure", "expectile"], "reference": "Jacob D. Abernethy and Rafael M. Frongillo. A characterization of scoring rules for linear proper- ties. COLT 2012. Journal of Machine Learning Research \u2014 Workshop and Conference Proceed- ings, 23:27.1-27.13, 2012.  Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.  Mathematical Finance, 9(3):203-228, 1999.  Heinz Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.  Fred Alois Behringer. Convexity is equivalent to midpoint convexity combined with strict quasi-  convexity. Optimization, 24:219-228, 1992.  Fabio Bellini and Valeria Bignozzi. Elicitable risk measures. Preprint, RiskLab, Department of  Mathematics, ETH Zurich, December 2013.  Fabio Bellini, Bernhard Klar, Alfred M\u00a8uller, and Emanuela Rosazza Gianin. Generalized quantiles  as risk measures. Insurance: Mathematics and Economics, 54:41-48, 2014.  Vladimir I. Bogachev. Measure Theory, Vol. I. Springer-Verlag, Berlin, 2007a.  Vladimir I. Bogachev. Measure Theory, Vol. II. Springer-Verlag, Berlin, 2007b.  Charles Castaing and Michel Valadier. Convex Analysis and Measurable Multifunctions. Springer,  Berlin, 1977.  P. Laurie Davies. On locally uniformly linearizable high breakdown location and scale functionals.  The Annals of Statistics, 26(3):1103-1125, June 1998.  Nicolae Dinculeanu. Vector Integration and Stochastic Integration in Banach Spaces. John Wiley  & Sons, New York, 2000.  Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical  Association, 106(494):746-762, 2011.  Jun-ya Gotoh and Stan Uryasev. Support vector machines based on convex risk functionals and general norms. Technical Report 2013-6, Risk Management and Financial Engineering Lab, University of Florida, November 2013.  Jun-ya Gotoh, Akiko Takeda, and Rei Yamamoto. Interaction between financial risk measures and machine learning methods. Computational Management Science, pages 1-38, June 2013. doi: 10.1007/s10287-013-0175-5.  13   ELICITATION AND IDENTIFICATION OF PROPERTIES  Much of the work of this paper was done while Ingo Steinwart and Chlo\u00b4e Pasin were visiting ANU and NICTA. The work was supported by the Australian Research Council and NICTA, both funded by the Australian Government. Thanks to the referees for their comments.  Acknowledgments  References  Jacob D. Abernethy and Rafael M. Frongillo. A characterization of scoring rules for linear proper- ties. COLT 2012. Journal of Machine Learning Research \u2014 Workshop and Conference Proceed- ings, 23:27.1-27.13, 2012.  Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.  Mathematical Finance, 9(3):203-228, 1999.  Heinz Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.  Fred Alois Behringer. Convexity is equivalent to midpoint convexity combined with strict quasi-  convexity. Optimization, 24:219-228, 1992.  Fabio Bellini and Valeria Bignozzi. Elicitable risk measures. Preprint, RiskLab, Department of  Mathematics, ETH Zurich, December 2013.  Fabio Bellini, Bernhard Klar, Alfred M\u00a8uller, and Emanuela Rosazza Gianin. Generalized quantiles  as risk measures. Insurance: Mathematics and Economics, 54:41-48, 2014.  Vladimir I. Bogachev. Measure Theory, Vol. I. Springer-Verlag, Berlin, 2007a.  Vladimir I. Bogachev. Measure Theory, Vol. II. Springer-Verlag, Berlin, 2007b.  Charles Castaing and Michel Valadier. Convex Analysis and Measurable Multifunctions. Springer,  Berlin, 1977.  P. Laurie Davies. On locally uniformly linearizable high breakdown location and scale functionals.  The Annals of Statistics, 26(3):1103-1125, June 1998.  Nicolae Dinculeanu. Vector Integration and Stochastic Integration in Banach Spaces. John Wiley  & Sons, New York, 2000.  Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical  Association, 106(494):746-762, 2011.  Jun-ya Gotoh and Stan Uryasev. Support vector machines based on convex risk functionals and general norms. Technical Report 2013-6, Risk Management and Financial Engineering Lab, University of Florida, November 2013.  Jun-ya Gotoh, Akiko Takeda, and Rei Yamamoto. Interaction between financial risk measures and machine learning methods. Computational Management Science, pages 1-38, June 2013. doi: 10.1007/s10287-013-0175-5. STEINWART PASIN WILLIAMSON ZHANG  Kyrill Grant and Tilmann Gneiting. Consistent scoring functions for quantiles. In M. Banerjee, F. Bunea, J. Huang, V. Koltchinskii, and M. H. Maathuis, editors, From Probability to Statistics and Back: High-Dimensional Models and Processes - A Festschrift in Honor of Jon A. Wellner, volume Volume 9, pages 163-173. Institute of Mathematical Statistics, Beachwood, Ohio, 2013.  Xiaolin Huang, Lei Shi, and Johan A.K. Suykens. Asymmetric least squares support vector machine  classifiers. Computational Statistics and Data Analysis, 70:395-405, 2014.  Peter J. Huber. Robust Statistics. Wiley, New York, 1981.  M. Chris Jones. Expectiles and M-quantiles are quantiles. Statistics and Probability Letters, 20:  149-153, 1994.  Shigeo Kusuoka. On law-invariant coherent risk measures.  In Advances in Mathematical Eco-  nomics, volume 3, pages 83-95. Springer, 2001.  Nicolas Lambert, David Pennock, and Yoav Shoham. Eliciting properties of probability distribu- In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129-138,  tions. 2008.  Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions. Proceedings of the 10th ACM Conference on Electronic Commerce, pages 109-118, 2009.  In  Nicolas S. Lambert.  Elicitation and evaluation of statistical forecasts.  Preprint, Stan- ford Graduate School of Business. (This version has been superseded by a version dated June 2013), April 2012. URL http://www.stanford.edu/\u02dcnlambert/papers/ elicitation_ver2012_buggyproof.pdf.  John McCarthy. Measures of the value of information. Proceedings of the National Academy of  Sciences, 42:654-655, 1956.  Robert E. Megginson. An Introduction to Banach Space Theory. Springer, New York, 1998.  Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econo-  metrica, 55(4):819-847, July 1987.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. PhD thesis, University of  California, Berkeley, 1985.  Albrecht Pietsch. Eigenvalues and s-Numbers. Geest & Portig K.-G., Leipzig, 1987.  R. Tyrell Rockafellar. Coherent approaches to risk in optimization under uncertainty. Tutorials in  operations research, pages 38-61, 2007.  R. Tyrell Rockafellar and Stan Uryasev. The fundamental risk quadrangle in risk management, op- timization and statistical estimation. Surveys in Operations Research and Management Science, 18(1):33-53, 2013.  Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the American  Statistical Association, 66(336):783-801, 1971. ELICITATION AND IDENTIFICATION OF PROPERTIES  Mark J. Schervish. A general method for comparing probability assessors. The Annals of Statistics,  17(4):1856-1879, 1989.  Mark J. Schervish, Joseph B. Kadane, and Teddy Seidenfeld. Characterization of proper and strictly proper scoring rules for quantiles. Preprint, Carnegie Mellon University, March 2012. URL http://www.hss.cmu.edu/philosophy/seidenfeld/relating% 20to%20scoring%20rules/Scoring%20Rules%20for%20Quantiles.pdf.  Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, New York, 2008.  William Thompson. Eliciting production possibilities from a well-informed manager. Journal of  Economic Theory, 20:360-380, 1979.  Peter Tsyarmasto and Stan Uryasev. Advanced risk measures in estimation and classifica- In Proceedings of the International Workshop \u201cStochastic Programming for Implemen- tion. tation and Advanced Applications\u201d, pages 114-118, Vilnius, July 2012. URL http://www. moksloperiodika.lt/STOPROG_2012/abstract/114-118-Tsyar.pdf.  Johanna F. Ziegel. Coherence and elicitability. Mathematical Finance, 2014. To appear.  "}, "The sample complexity of agnostic learning under deterministic labels": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "The sample complexity of agnostic learning under deterministic labels", "abstract": "With the emergence of Machine Learning tools that allow handling data with a huge number of features, it becomes reasonable to assume that, over the full set of features, the true labeling is (almost) fully determined. That is, the labeling function is deterministic, but not necessarily a member of some known  hypothesis class. However, agnostic learning of deterministic labels has so far received little research attention. We investigate this setting and show that it displays a behavior that is quite different from that of the fundamental results of the common (PAC) learning setups. First, we show that the sample complexity of learning a binary hypothesis class (with respect to deterministic labeling functions) is not fully determined by the VC-dimension of the class. For any d, we present classes of VC-dimension d that are learnable from \\tilde O(d/\u03b5)-many samples and classes that require samples of size \u03a9(d/\u03b5^2). Furthermore, we show that in this setup, there are classes for which any proper learner has suboptimal sample complexity.  While the class can be learned with sample complexity \\tilde O(d/\u03b5), any \\emphproper (and therefore, any ERM) algorithm requires \u03a9(d/\u03b5^2) samples. We provide combinatorial characterizations of both phenomena, and further analyze the utility of unlabeled samples in this setting. Lastly, we discuss the error rates of nearest neighbor algorithms under deterministic labels and additional niceness-of-data assumptions.", "pdf_url": "http://proceedings.mlr.press/v35/ben-david14.pdf", "keywords": []}, "Density-preserving quantization with application to graph downsampling": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Density-preserving quantization with application to graph downsampling", "abstract": "We consider the problem of vector quantization of i.i.d. samples drawn from a density p on \\mathbbR^d. It is  desirable that the representatives selected by the quantization algorithm have the same distribution p as the original sample points. However, quantization algorithms based on Euclidean distance, such as k-means, do not have this property. We provide a solution to this problem that takes the unweighted k-nearest neighbor graph on the sample as input. In particular, it does not need to have access to the data points themselves. Our solution generates quantization centers that are \u201cevenly spaced\". We exploit this property to downsample geometric graphs and show that our method produces  sparse downsampled graphs.  Our algorithm is easy to implement, and we provide theoretical guarantees on the performance of the proposed algorithm.", "pdf_url": "http://proceedings.mlr.press/v35/alamgir14.pdf", "keywords": ["Vector quantization", "kNN graph", "sampling"], "reference": "ICML, 2012.  M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. In  N. Asgharbeygi and A. Maleki. Geodesic k-means clustering. In ICPR, 2008.  P.L. Bartlett, T. Linder, and G. Lugosi. The minimax distortion redundancy in empirical quantizer  design. IEEE Transactions on Information Theory, 44(5), 1998.  A.L. Besse. Einstein Manifolds. Classics in Mathematics. Springer, 1987.  G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints. Workshop  on Statistical Learning in Computer Vision, ECCV, pages 1-22, 2004.  V. de Silva and J.B. Tenenbaum. Sparse multidimensional scaling using landmark points. Technical  report, Stanford University, 2004.  tions, 39(11):1549-1558, 1991.  E.J. Delp and O.R. Mitchell. Moment preserving quantization. IEEE Transactions on Communica-  13  \u2212202468\u22123\u22122\u221210123\u22125051000.050.10.150.20.250.30.35  Original dataShortest pathEuclidean DENSITY-PRESERVING QUANTIZATION  (a) Original data (green), centroids based on the shortest path distance (red) and the Euclidean distance (black).  (b) The marginal distribution of samples and quantization centroids in the direction of x- axis, where quantization is with respect to Dsp or Euclidean distance.  matrices, one can apply multidimensional scaling (MDS) to find a distance preserving embedding of the points in a higher dimensional Euclidean space and use the Lloyd algorithm in the embedded space. At the end, they show how to do all the computations in the original space without paying the cost of the embedding step.  Acknowledgments  We thank Tam\u00b4as Linder for pointing out relevant references. This research is partly supported by the German Research Foundation via the Research Unit 1735 \u201cStructural Inference in Statistics: Adaptation and Efficiency\u201d and grant LU1718/1-1. G. Lugosi acknowledges support by the Spanish Ministry of Science and Technology grant MTM2012-37195.  References  ICML, 2012.  M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. In  N. Asgharbeygi and A. Maleki. Geodesic k-means clustering. In ICPR, 2008.  P.L. Bartlett, T. Linder, and G. Lugosi. The minimax distortion redundancy in empirical quantizer  design. IEEE Transactions on Information Theory, 44(5), 1998.  A.L. Besse. Einstein Manifolds. Classics in Mathematics. Springer, 1987.  G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints. Workshop  on Statistical Learning in Computer Vision, ECCV, pages 1-22, 2004.  V. de Silva and J.B. Tenenbaum. Sparse multidimensional scaling using landmark points. Technical  report, Stanford University, 2004.  tions, 39(11):1549-1558, 1991.  E.J. Delp and O.R. Mitchell. Moment preserving quantization. IEEE Transactions on Communica-\u2212202468\u22123\u22122\u221210123\u22125051000.050.10.150.20.250.30.35  Original dataShortest pathEuclidean ALAMGIR LUGOSI VON LUXBURG  B. Feil and A. Janos. Geodesic distance based fuzzy clustering. Lecture Notes in Computer Science,  Soft Computing in Industrial Applications, pages 50-59, 2007.  S. Graf and H. Luschgy. Foundations of quantization for probability distributions. Lecture notes in  mathematics. Springer-Verlag New York, Inc., 2000.  P. M. Gruber. Optimal configurations of finite sets in Riemannian 2-manifolds. Geometriae Dedi-  cata, 84(1-3):271-320, 2001.  497, 2004.  P. M. Gruber. Optimum quantization and its applications. Advances in Mathematics, 186(2):456 -  P. Heckbert. Color image quantization for frame buffer display. In SIGGRAPH, 1982.  A. Hegde, D. Erdogmus, T. Lehn-Schioler, Y. Rao, and J. Principe. Vector-Quantization by density matching in the minimum Kullback-Leibler divergence sense. In IEEE International Conference on Neural Networks, volume 1, pages 105-109, 2004.  M.M. Van Hulle. Faithful representations with topographic maps. Neural Networks, 12(6):803 -  823, 1999.  J. Kim, K. Shim, and S. Choi. Soft geodesic kernel k-means. In ICASSP, pages 429-432, 2007.  T. Leung and J. Malik. Representing and recognizing the visual appearance of materials using  three-dimensional textons. Int. J. Computer Vision, 43(1):29-44, 2001.  M. Li, J. Klejsa, and W. Bastiaan Kleijn. On distribution preserving quantization. CoRR,  S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):  P. Meinicke and H. Ritter. Quantizing density estimators. In Neural Information Processing Systems  H. Niederreiter. Random Number Generation and quasi-Monte Carlo Methods. Society for Indus-  trial and Applied Mathematics, Philadelphia, PA, USA, 1992.  M. Penrose. A strong law for the longest edge of the minimal spanning tree. The Annals of Proba-  bility, (1), 1999.  D. Pollard. Strong consistency of k-means clustering. The Annals of Statistics, 9(1):135-140, 1981.  D. Pollard. A central limit theorem for k-means clustering. The Annals of Probability, (4), 1982.  Sajama and A. Orlitsky. Estimating and computing density based distance metrics. In ICML, 2005.  S. Yakowitz, P. L\u2019Ecuyer, and F. V\u00b4azquez-Abad. Global stochastic optimization with low-dispersion  point sets. Operational Research, 48:939-950, 2000.  abs/1108.3728, 2011.  129-137, 1982.  (NIPS), 2001. DENSITY-PRESERVING QUANTIZATION  "}, "A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates", "abstract": "We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and provide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assumptions, our algorithm is information-theoretically optimal. Our results represent the first (and currently only known) tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample complexity.", "pdf_url": "http://proceedings.mlr.press/v35/chen14.pdf", "keywords": [], "reference": "Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012.  Martin Azizyan, Aarti Singh, and Larry Wasserman. Minimax theory for high-dimensional gaussian  mixtures with sparse mean separation. arXiv preprint arXiv:1306.2035, 2013.  13   MIXED REGRESSION  p\u22121 , \u03ba2  0 = \u03ba2 \u2212 (p \u2212 1)\u03c4 2, and (cid:15)j is the j-th standard basis in Rp. We verify that this where \u03c4 = 4\u03b4\u221a \u0398 indeed defines a \u03b4-packing of \u0398(\u03b3), and moreover satisfies (cid:107)\u03b2i \u2212 \u03b2i(cid:48)(cid:107)2 \u2264 16\u03b42 for all i (cid:54)= i(cid:48). To bound the mutual information, we observe that by independence between X and \u03b8\u2217, we have  I (\u03b8\u2217; X, y) \u2264  1 M 2  (cid:88)  D (Pi(cid:107)Pi(cid:48)) =  1 M  1\u2264i,i(cid:48)\u2264M  1\u2264i,i(cid:48)\u2264M  j=1  (cid:88)  n (cid:88)  (cid:16)  (cid:104)  D  EX  i,X (cid:107)P(j) P(j) i(cid:48),X  (cid:17)(cid:105)  ,  where P(j) i,X denotes the distribution of yj conditioned on X and \u03b8\u2217 = \u03b8i. The remaining and crucial step is to obtain sharp upper bounds on the above KL-divergence between two mixtures of one-dimensional Gaussian distributions. This requires some technical calculations, from which we obtain  (cid:16)  EX D  i,X (cid:107)P(j) P(j) i(cid:48),X  (cid:17)  \u2264  c(cid:48) (cid:107)\u03b2i \u2212 \u03b2i(cid:48)(cid:107)2 \u03ba2 \u03c34  .  We conclude that I(\u03b8\u2217; X, y) \u2264 1 rem 8. Theorem 7 and Parts 1, 3 of Theorem 8 are proved in a similar manner.  4 log M . Combining with (12) and (13) proves Part 2 of Theo-  5. Conclusion  This paper provides a computationally and statistically efficient algorithm for mixed regression with two components. To the best of our knowledge, the is the first efficient algorithm that can provide O(p) sample complexity guarantees. Under certain conditions, we prove matching lower bounds, thus demonstrating our algorithm achieves the minimax optimal rates. There are several interesting open questions that remain. Most immediate is the issue of understanding the degree to which the assumptions currently required for minimax optimality can be removed or relaxed. The extension to more than two components is important, though how to do this within the current framework is not obvious.  At its core, the approach here is a method of moments, as the convex optimization formulation produces an estimate of the cross moments, (\u03b2\u2217 1\u03b2\u2217(cid:62) 1 ). An interesting aspect of these results is the significant improvement in sample complexity guarantees this tailored approach brings, compared to a more generic implementation of the tensor machinery which requires use of third order moments. Given the statistical and also computational challenges related to third order tensors, understanding the connections more carefully seems to be an important future direction.  2 + \u03b2\u2217  2\u03b2\u2217(cid:62)  We thank Yuxin Chen for illuminating conversations on the topic. We acknowledge support from NSF Grants EECS-1056028, CNS-1302435, CCF-1116955, and the USDOT UTC - D-STOP Cen- ter at UT Austin.  Acknowledgments  References  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012.  Martin Azizyan, Aarti Singh, and Larry Wasserman. Minimax theory for high-dimensional gaussian  mixtures with sparse mean separation. arXiv preprint arXiv:1306.2035, 2013. CHEN YI CARAMANIS  Lucien Birg\u00b4e. Approximation dans les espaces m\u00b4etriques et th\u00b4eorie de l\u2019estimation. Z. Wahrsch.  verw. Gebiete, 65(2):181-237, 1983.  T Tony Cai and Anru Zhang. ROP: Matrix recovery via rank-one projections. arXiv preprint  arXiv:1310.5791, 2013.  Emmanuel Cand`es and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57 (4):2342-2359, 2011.  Arun Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. In  International Conference on Machine Learning (ICML), 2013.  Jiahua Chen. Optimal rate of convergence for finite mixture models. The Annals of Statistics, pages  221-233, 1995.  Yuxin Chen, Yuejie Chi, and Andrea Goldsmith. Exact and stable covariance estimation from  quadratic sampling via convex programming. arXiv preprint arXiv:1310.0807, 2013.  Thomas M Cover and Joy A Thomas. Elements of information theory. Wiley, 2012.  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1-38, 1977.  Ehsan Elhamifar and Ren\u00b4e Vidal. Sparse subspace clustering.  In Computer Vision and Pattern  Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2790-2797. IEEE, 2009.  Bettina Gr\u00a8un and Friedrich Leisch. Applications of finite mixtures of regression models. URL:  http://cran. r-project. org/web/packages/\ufb02exmix/vignettes/regression-examples.pdf, 2007.  Daniel Hsu and Sham M. Kakade. Learning gaussian mixture models: Moment methods and spec-  tral decompositions. CoRR, abs/1206.5766, 2012.  Geoffrey McLachlan and David Peel. Finite Mixture Models. Wiley series in probability and statistics: Applied probability and statistics. Wiley, 2004. ISBN 9780471654063. URL http: //books.google.com/books?id=7M5vK8OpXZ4C.  Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed Minimum-Rank Solutions of  Linear Matrix Equations via Nuclear Norm Minimization. SIAM Review, 52(471), 2010.  Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.  arXiv preprint arXiv:1306.2872, 2013.  Mahdi Soltanolkotabi, Ehsan Elhamifar, and Emmanuel Candes. Robust subspace clustering. arXiv  preprint arXiv:1301.2603, 2013.  Nicolas Stadler, Peter Buhlmann, and Sara Geer. L1-penalization for mixture regression models.  TEST, 19(2):209-256, 2010. ISSN 1133-0686. MIXED REGRESSION  Yuekai Sun, Stratis Ioannidis, and Andrea Montanari. Learning mixtures of linear classifiers. arXiv  preprint arXiv:1311.2547, 2013.  Mathematics, 12(4):389-434, 2012.  J.A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. Arxiv preprint  Kert Viele and Barbara Tong. Modeling with mixtures of linear regressions.  Statistics and ISSN 0960-3174. URL http://dx.doi.org/10.1023/A%  Springer, 2009.  arxiv:1011.3027, 2010.  Computing, 12(4), 2002. 3A1020779827503.  Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering.  In Proceedings of The 30th  International Conference on Machine Learning, pages 89-97, 2013.  CF Wu. On the convergence properties of the EM algorithm. The Annals of Statistics, 11(1):95-103,  Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The  Annals of Statistics, 27(5):1564-1599, 1999.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear  regression. Arxiv preprint arxiv:1310.3745, 2013.  Bin Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer,  1983.  1997. CHEN YI CARAMANIS  Supplemental Results  "}, "Efficiency of conformalized ridge regression": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Efficiency of conformalized ridge regression", "abstract": "Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied.", "pdf_url": "http://proceedings.mlr.press/v35/burnaev14.pdf", "keywords": ["Bayesian learning", "conformal prediction", "asymptotic analysis", "Bahadur representation"], "reference": "577-580, 1966.  We are grateful to Albert Shiryaev for inviting us in September 2013 to Kolmogorov\u2019s dacha in Komarovka, where this project was conceived, and to Glenn Shafer for his advice about terminol- ogy. Thanks to the reviewers for their comments. The first author has been partially supported by the Laboratory for Structural Methods of Data Analysis in Predictive Modeling, Moscow Institute of Physics and Technology, Russian Government grant (agreement 11.G34.31.0073). The second author has been partially supported by EPSRC (grant EP/K033344/1).  R. Raj Bahadur. A note on quantiles in large samples. Annals of Mathematical Statistics, 37:  Raymond J. Carroll. On the distribution of quantiles of residuals in a linear model. Technical Report Mimeo Series No. 1161, Department of Statistics, University of North Carolina at Chapel Hill, March 1978. Available from http://www.stat.ncsu.edu/information/library/mimeo.php.  Samprit Chatterjee and Ali S. Hadi. Sensitivity Analysis in Linear Regression. Wiley, New York,  1988.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzy\u02d9zak, and Harro Walk. A Distribution-Free Theory of  Nonparametric Regression. Springer, New York, 2002.  Harold V. Henderson and Shayle R. Searle. On deriving the inverse of a sum of matrices. SIAM  Review, 23:53-60, 1981.  12   BURNAEV VOVK  coefficients whose maximum is o(1) as n \u2192 \u221e (this uses the assumption 1 earlier).  n X (cid:48)  nXn \u2192 \u03a3 made  A more intuitive (but not necessarily simpler) proof can be obtained by noticing that \u02c6wn \u2212 w  and the residuals are asymptotically (precisely when a = 0) independent.  7. Conclusion  The results of this paper are asymptotic; it would be very interesting to obtain their non-asymptotic counterparts. In non-asymptotic settings, however, it is not always true that conformalized ridge re- gression loses little in efficiency as compared with the Bayesian prediction interval; this is illustrated in Vovk et al. (2005), Section 8.5, and illustrated and explained in Vovk et al. (2009). The main dif- ference is that CRR and Bayesian predictor start producing informative predictions after seeing a different number of observations. CRR, like any other conformal predictor (or any other method whose validity depends only on the IID assumption), starts producing informative predictions only after the number of observations exceeds the inverse significance level 1/(cid:15). After this theoretical lower bound is exceeded, however, the difference between CRR and Bayesian predictions quickly becomes very small.  Another interesting direction of further research is to extend our results to kernel ridge regres-  sion.  Acknowledgements  References  577-580, 1966.  We are grateful to Albert Shiryaev for inviting us in September 2013 to Kolmogorov\u2019s dacha in Komarovka, where this project was conceived, and to Glenn Shafer for his advice about terminol- ogy. Thanks to the reviewers for their comments. The first author has been partially supported by the Laboratory for Structural Methods of Data Analysis in Predictive Modeling, Moscow Institute of Physics and Technology, Russian Government grant (agreement 11.G34.31.0073). The second author has been partially supported by EPSRC (grant EP/K033344/1).  R. Raj Bahadur. A note on quantiles in large samples. Annals of Mathematical Statistics, 37:  Raymond J. Carroll. On the distribution of quantiles of residuals in a linear model. Technical Report Mimeo Series No. 1161, Department of Statistics, University of North Carolina at Chapel Hill, March 1978. Available from http://www.stat.ncsu.edu/information/library/mimeo.php.  Samprit Chatterjee and Ali S. Hadi. Sensitivity Analysis in Linear Regression. Wiley, New York,  1988.  L\u00b4aszl\u00b4o Gy\u00a8orfi, Michael Kohler, Adam Krzy\u02d9zak, and Harro Walk. A Distribution-Free Theory of  Nonparametric Regression. Springer, New York, 2002.  Harold V. Henderson and Shayle R. Searle. On deriving the inverse of a sum of matrices. SIAM  Review, 23:53-60, 1981. EFFICIENCY OF CONFORMALIZED RIDGE REGRESSION  Jing Lei and Larry Wasserman. Distribution free prediction bands for nonparametric regression.  Journal of the Royal Statistical Society B, 76:71-96, 2014.  Ronald H. Randles, Thomas P. Hettmansperger, and George Casella. Introduction to the Special  Issue: Nonparametric statistics. Statistical Science, 19:561, 2004.  George A. F. Seber and Alan J. Lee. Linear Regression Analysis. Wiley, Hoboken, NJ, second  edition, 2003.  349-376, 2013a.  Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Press, Cambridge, 1998.  Vladimir Vovk. Conditional validity of inductive conformal predictors. Machine Learning, 92:  Vladimir Vovk. Kernel ridge regression. In Bernhard Sch\u00a8olkopf, Zhiyuan Luo, and Vladimir Vovk, editors, Empirical Inference: Festschrift in Honour of Vladimir N. Vapnik, chapter 11, pages 105-116. Springer, Berlin, 2013b.  Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World.  Springer, New York, 2005.  Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. On-line predictive linear regression. An-  nals of Statistics, 37:1566-1590, 2009.  Larry Wasserman. Frasian inference. Statistical Science, 26:322-325, 2011.  "}, "Most Correlated Arms Identification": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Most Correlated Arms Identification", "abstract": "We study the problem of finding the most mutually correlated arms among many arms. We show that adaptive arms sampling strategies can have significant advantages over the non-adaptive uniform sampling strategy. Our proposed algorithms rely on a novel correlation estimator. The use of this accurate estimator allows us to get improved results for a wide range of problem instances.", "pdf_url": "http://proceedings.mlr.press/v35/liu14.pdf", "keywords": [], "reference": "40(1):412-435, 2012a.  E. Arias-Castro, S. Bubeck, and G. Lugosi. Detection of correlations. The Annals of Statistics,  E. Arias-Castro, S. Bubeck, and G. Lugosi. Detecting positive correlations in a multivariate sample.  arXiv preprint arXiv:1202.5536, 2012b.  J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Pro-  ceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.  S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Pro- ceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.  S. Bubeck, T. Wang, and N. Viswanathan. Multiple identifications in multi-armed bandits.  In  Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.  R.M. Castro, G. Lugosi, and P.-A. Savalle. Detection of correlations with adaptive sensing. arXiv  preprint arXiv:1311.5366, 2013.  E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for the multi- armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7: 1079-1105, 2006.  S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.  A.B. Tsybakov. Introduction to nonparametric estimation. Springer, 2009.  12   LIU BUBECK  way as seemingly suboptimal arms are rejected early, see Kalyanakrishnan et al. (2012) and Bubeck et al. (2013) for details. Unfortunately, the same trick can not be applied easily to the problem of most correlated arms identification. The main difficulty is that when an optimal arm is accepted early, the remaining optimal arms are not necessarily the most mutually correlated arms among all the remaining arms, thus making the identification of the remaining optimal arms difficult (if not impossible). To summarize, a novel algorithmic idea is needed to improve our upper bound. An- other interesting direction of further work is to prove a lower bound on the number of samples that any adaptive strategy must use.  References  40(1):412-435, 2012a.  E. Arias-Castro, S. Bubeck, and G. Lugosi. Detection of correlations. The Annals of Statistics,  E. Arias-Castro, S. Bubeck, and G. Lugosi. Detecting positive correlations in a multivariate sample.  arXiv preprint arXiv:1202.5536, 2012b.  J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identification in multi-armed bandits. In Pro-  ceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.  S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Pro- ceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.  S. Bubeck, T. Wang, and N. Viswanathan. Multiple identifications in multi-armed bandits.  In  Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.  R.M. Castro, G. Lugosi, and P.-A. Savalle. Detection of correlations with adaptive sensing. arXiv  preprint arXiv:1311.5366, 2013.  E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for the multi- armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7: 1079-1105, 2006.  S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.  A.B. Tsybakov. Introduction to nonparametric estimation. Springer, 2009. MOST CORRELATED ARMS IDENTIFICATION  "}, "Fast matrix completion without the condition number": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Fast matrix completion without the condition number", "abstract": "We give the first algorithm for Matrix Completion that achieves running time and sample complexity that is polynomial in the rank of the unknown target matrix, \\emphlinear in the dimension of the matrix, and \\emphlogarithmic in the condition number of the matrix.  To the best of our knowledge, all previous algorithms either incurred a quadratic dependence on the condition number of the unknown matrix or a quadratic dependence on the dimension of the matrix. Our algorithm is based on a novel extension of Alternating Minimization which we show has theoretical guarantees under standard assumptions even in the presence of noise.", "pdf_url": "http://proceedings.mlr.press/v35/hardt14a.pdf", "keywords": [], "reference": "Haim Avron, Satyen Kale, Shiva Prasad Kasiviswanathan, and Vikas Sindhwani. Efficient and In Proc. 29th ICML.  practical stochastic subgradient descent for nuclear norm regularization. ACM, 2012.  Robert M. Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neighbor-  hood interpolation weights. In ICDM, pages 43-52. IEEE Computer Society, 2007.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computional Mathematics, 9:717-772, December 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: near-optimal matrix com-  pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion using  alternating minimization. In Proc. ECML PKDD, pages 194-209. Springer, 2013.  Justin P. Haldar and Diego Hernando. Rank-constrained solutions to linear matrix equations using  powerfactorization. IEEE Signal Process. Lett., 16(7):584-587, 2009.  Moritz Hardt. Robust subspace iteration and privacy-preserving spectral analysis. arXiv, 1311:2495,  2013a.  arXiv, 1312.0925, 2013b.  Moritz Hardt. On the provable convergence of alternating minimization for matrix completion.  Elad Hazan and Satyen Kale. Projection-free online learning. In Proc. 29th ICML. ACM, 2012.  Cho-Jui Hsieh and Peder A. Olsen. Nuclear norm minimization via active subspace selection. In  Proc. 31st ICML. ACM, 2014.  Martin Jaggi and Marek Sulovsk\u00b4y. A simple algorithm for nuclear norm regularized problems. In  Proc. 27th ICML, pages 471-478. ACM, 2010.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value projection. In Proc. 24th Neural Information Processing Systems (NIPS), pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proc. 45th Symposium on Theory of Computing (STOC), pages 665-674. ACM, 2013.  Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization.  In  Proc. 26th ICML, page 58. ACM, 2009.  13   FAST MATRIX COMPLETION WITHOUT THE CONDITION NUMBER  We thank the Simons Institute for Theoretical Computer Science at Berkeley, where part of this work was done.  Acknowledgments  References  Haim Avron, Satyen Kale, Shiva Prasad Kasiviswanathan, and Vikas Sindhwani. Efficient and In Proc. 29th ICML.  practical stochastic subgradient descent for nuclear norm regularization. ACM, 2012.  Robert M. Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neighbor-  hood interpolation weights. In ICDM, pages 43-52. IEEE Computer Society, 2007.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computional Mathematics, 9:717-772, December 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: near-optimal matrix com-  pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion using  alternating minimization. In Proc. ECML PKDD, pages 194-209. Springer, 2013.  Justin P. Haldar and Diego Hernando. Rank-constrained solutions to linear matrix equations using  powerfactorization. IEEE Signal Process. Lett., 16(7):584-587, 2009.  Moritz Hardt. Robust subspace iteration and privacy-preserving spectral analysis. arXiv, 1311:2495,  2013a.  arXiv, 1312.0925, 2013b.  Moritz Hardt. On the provable convergence of alternating minimization for matrix completion.  Elad Hazan and Satyen Kale. Projection-free online learning. In Proc. 29th ICML. ACM, 2012.  Cho-Jui Hsieh and Peder A. Olsen. Nuclear norm minimization via active subspace selection. In  Proc. 31st ICML. ACM, 2014.  Martin Jaggi and Marek Sulovsk\u00b4y. A simple algorithm for nuclear norm regularized problems. In  Proc. 27th ICML, pages 471-478. ACM, 2010.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value projection. In Proc. 24th Neural Information Processing Systems (NIPS), pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proc. 45th Symposium on Theory of Computing (STOC), pages 665-674. ACM, 2013.  Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization.  In  Proc. 26th ICML, page 58. ACM, 2009. HARDT WOOTTERS  Raghunandan H. Keshavan. Efficient algorithms for collaborative filtering. PhD thesis, Stanford  University, 2012.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010a.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. Journal of Machine Learning Research, 11:2057-2078, 2010b.  Yehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for recom-  mender systems. IEEE Computer, 42(8):30-37, 2009.  Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287-2322, 2010.  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12:3413-3430, 2011.  Benjamin Recht and Christopher R\u00b4e. Parallel stochastic gradient algorithms for large-scale matrix  completion. Mathematical Programming Computation, 5(2):201-226, 2013.  Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471-501, 2010.  Gilbert W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press London, 1990.  G.W. Stewart. Matrix Algorithms. Volume II: Eigensystems. Society for Industrial and Applied  Mathematics, 2001.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  "}, "Learning Coverage Functions and Private Release of Marginals": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Learning Coverage Functions and Private Release of Marginals", "abstract": "We study the problem of approximating and learning coverage functions. A function c: 2^[n] \u2192\\mathbfR^+ is a coverage function, if there exists a universe U with non-negative weights w(u) for each u \u2208U and subsets A_1, A_2, \\ldots, A_n of U such that c(S) = \\sum_u \u2208\\cup_i \u2208S A_i w(u). Alternatively, coverage functions can be described as non-negative linear combinations of monotone disjunctions. They are a natural subclass of submodular functions and arise in a number of applications. We give an algorithm that for any \u03b3,\u03b4>0, given random and uniform examples of an unknown coverage function c, finds a function h that approximates c within factor 1+\u03b3on all but \u03b4-fraction of the points in time poly(n,1/\u03b3,1/\u03b4). This is the first fully-polynomial algorithm for learning an interesting class of functions in the demanding PMAC model of Balcan and Harvey (2011). Our algorithms are based on several new structural properties of coverage functions. Using the results in (Feldman and Kothari, 2014), we also show that coverage functions are learnable agnostically with excess \\ell_1-error \u03b5over all product and symmetric distributions in time n^\\log(1/\u03b5). In contrast, we show that, without assumptions on the distribution, learning coverage functions is at least as hard as learning polynomial-size disjoint DNF formulas, a class of functions for which the best known algorithm runs in time 2^\\tildeO(n^1/3) (Klivans and Servedio, 2004). As an application of our learning results, we give simple differentially-private algorithms for releasing monotone conjunction counting queries with low \\em average error. In particular, for any k \u2264n, we obtain private release of k-way marginals with average error \\bar\u03b1 in time n^O(\\log(1/\\bar\u03b1)).", "pdf_url": "http://proceedings.mlr.press/v35/feldman14a.pdf", "keywords": [], "reference": "D. J. Lehmann B. Lehmann and N. Nisan. Combinatorial auctions with decreasing marginal utilities.  Games and Economic Behavior, 55:1884-1899, 2006.  A. Badanidiyuru, S. Dobzinski, H. Fu, R. Kleinberg, N. Nisan, and T. Roughgarden. Sketching  valuation functions. In SODA, pages 1025-1035, 2012.  M.F. Balcan and N. Harvey. Submodular functions: Learnability, structure, and optimization. CoRR,  abs/1008.2159, 2012. Earlier version in proceedings of STOC 2011.  M.F. Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions. Journal  of Machine Learning Research - COLT Proceedings, 23:4.1-4.24, 2012.  B. Barak, K. Chaudhuri, C. Dwork, S. Kale, F. McSherry, and K. Talwar. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In PODS, pages 273-282, 2007.  E. Blais, R. O\u2019Donnell, and K. Wimmer. Polynomial regression under arbitrary product distributions.  In COLT, pages 193-204, 2008.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the sulq framework. In PODS,  pages 128-138, 2005.  S. Boucheron, G. Lugosi, and P. Massart. A sharp concentration inequality with applications. Random  Struct. Algorithms, 16(3):277-292, 2000.  M. Bun, J. Ullman, and S. P. Vadhan. Fingerprinting codes and the price of approximate differential  privacy. CoRR, abs/1311.3158, 2013.  D. Chakrabarty and Z. Huang. Testing coverage functions. In ICALP (1), pages 170-181, 2012.  K. Chandrasekaran, J. Thaler, J. Ullman, and A. Wan. Faster private release of marginals on small  databases. ITCS, 2014.  pages 1586-1592, 2012.  M. Cheraghchi, A. Klivans, P. Kothari, and H. Lee. Submodular functions are noise stable. In SODA,  G. Cornuejols, M. Fisher, and G. Nemhauser. Location of Bank Accounts to Optimize Float: An Analytic Study of Exact and Approximate Algorithms. Management Science, 23(8):789-810, 1977.  S. Dobzinski and M. Schapira. An improved approximation algorithm for combinatorial auctions  with submodular bidders. In SODA, pages 1064-1073, 2006.  S. Dughmi and J. Vondr\u00b4ak. Limitations of randomized mechanisms for combinatorial auctions. In  FOCS, pages 502-511, 2011.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  14   FELDMAN KOTHARI  References  D. J. Lehmann B. Lehmann and N. Nisan. Combinatorial auctions with decreasing marginal utilities.  Games and Economic Behavior, 55:1884-1899, 2006.  A. Badanidiyuru, S. Dobzinski, H. Fu, R. Kleinberg, N. Nisan, and T. Roughgarden. Sketching  valuation functions. In SODA, pages 1025-1035, 2012.  M.F. Balcan and N. Harvey. Submodular functions: Learnability, structure, and optimization. CoRR,  abs/1008.2159, 2012. Earlier version in proceedings of STOC 2011.  M.F. Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions. Journal  of Machine Learning Research - COLT Proceedings, 23:4.1-4.24, 2012.  B. Barak, K. Chaudhuri, C. Dwork, S. Kale, F. McSherry, and K. Talwar. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In PODS, pages 273-282, 2007.  E. Blais, R. O\u2019Donnell, and K. Wimmer. Polynomial regression under arbitrary product distributions.  In COLT, pages 193-204, 2008.  A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial  Intelligence, 97(1-2):245-271, 1997.  A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the sulq framework. In PODS,  pages 128-138, 2005.  S. Boucheron, G. Lugosi, and P. Massart. A sharp concentration inequality with applications. Random  Struct. Algorithms, 16(3):277-292, 2000.  M. Bun, J. Ullman, and S. P. Vadhan. Fingerprinting codes and the price of approximate differential  privacy. CoRR, abs/1311.3158, 2013.  D. Chakrabarty and Z. Huang. Testing coverage functions. In ICALP (1), pages 170-181, 2012.  K. Chandrasekaran, J. Thaler, J. Ullman, and A. Wan. Faster private release of marginals on small  databases. ITCS, 2014.  pages 1586-1592, 2012.  M. Cheraghchi, A. Klivans, P. Kothari, and H. Lee. Submodular functions are noise stable. In SODA,  G. Cornuejols, M. Fisher, and G. Nemhauser. Location of Bank Accounts to Optimize Float: An Analytic Study of Exact and Approximate Algorithms. Management Science, 23(8):789-810, 1977.  S. Dobzinski and M. Schapira. An improved approximation algorithm for combinatorial auctions  with submodular bidders. In SODA, pages 1064-1073, 2006.  S. Dughmi and J. Vondr\u00b4ak. Limitations of randomized mechanisms for combinatorial auctions. In  FOCS, pages 502-511, 2011.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006. LEARNING COVERAGE FUNCTIONS AND PRIVATE RELEASE OF MARGINALS  C. Dwork, A. Nikolov, and K. Talwar. Efficient algorithms for privately releasing marginals via  convex relaxations. CoRR, abs/1308.1385, 2013.  J. Edmonds. Matroids, submodular functions and certain polyhedra. Combinatorial Structures and  Their Applications, pages 69-87, 1970.  U. Feige. On maximizing welfare when utility functions are subadditive. In ACM STOC, pages  41-50, 2006.  V. Feldman. A complete characterization of statistical query learning with applications to evolvability.  Journal of Computer System Sciences, 78(5):1444-1459, 2012.  V. Feldman and P. Kothari. Learning coverage functions and private release of marginals. arXiv,  CoRR, abs/1304.2079, 2013.  CoRR, abs/1405.6791, 2014.  V. Feldman and P. Kothari. Agnostic learning of disjunctions on symmetric distributions. arXiv,  V. Feldman and J. Vondr\u00b4ak. Optimal bounds on approximation of submodular and xos functions by  juntas. In FOCS, pages 227-236, 2013.  V. Feldman, P. Kothari, and J. Vondr\u00b4ak. Representation, approximation and learning of submodular  functions using low-rank decision trees. In COLT, pages 30:711-740, 2013.  L. Fleischer, S. Fujishige, and S. Iwata. A combinatorial, strongly polynomial-time algorithm for  minimizing submodular functions. JACM, 48(4):761-777, 2001.  A. Frank. Matroids and submodular functions. Annotated Biblographies in Combinatorial Optimiza-  tion, pages 65-80, 1997.  M. Goemans and D. Williamson. Improved approximation algorithms for maximum cut and satisfia-  bility problems using semidefinite programming. J. ACM, 42(6):1115-1145, 1995.  C. Guestrin, A. Krause, and A. Singh. Near-optimal sensor placements in gaussian processes. In  ICML, pages 265-272, 2005.  A. Gupta, M. Hardt, A. Roth, and J. Ullman. Privately releasing conjunctions and the statistical  query barrier. In STOC. ACM, 2011.  M. Hardt, G. Rothblum, and R. Servedio. Private data release via learning thresholds. In SODA,  pages 168-187, 2012.  D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning  applications. Information and Computation, 100(1):78-150, 1992. ISSN 0890-5401.  R. K. Iyer and J. A. Bilmes. Submodular optimization with submodular cover and submodular  knapsack constraints. In NIPS, pages 2436-2444, 2013.  J. Jackson. An efficient membership-query algorithm for learning DNF with respect to the uniform  distribution. Journal of Computer and System Sciences, 55:414-440, 1997. FELDMAN KOTHARI  A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. SIAM J.  Comput., 37(6):1777-1805, 2008.  M. Kearns, M. Li, and L. Valiant. Learning boolean formulas. J. ACM, 41(6):1298-1328, November  M. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine Learning, 17  (2-3):115-141, 1994b.  A. Klivans and R. Servedio. Learning dnf in time 2\u02dco(n1/3). J. Comput. Syst. Sci., 68(2):303-318,  1994a.  2004.  A. Krause and C. Guestrin. Submodularity and its applications in optimized information gathering.  ACM TIST, 2(4):32, 2011.  A. Krause, C. Guestrin, A. Gupta, and J. Kleinberg. Near-optimal sensor placements: maximizing  information while minimizing communication cost. In IPSN, pages 2-10, 2006.  E. Kushilevitz and Y. Mansour. Learning decision trees using the Fourier spectrum. SIAM Journal  on Computing, 22(6):1331-1348, 1993.  L. Lov\u00b4asz. Submodular functions and convexity. Mathematical Programmming: The State of the Art,  pages 235-257, 1983.  SODA, 2012.  (1), pages 810-821, 2012.  400-416, 2011.  S. Raskhodnikova and G. Yaroslavtsev. Learning pseudo-boolean k-dnf and submodular functions.  J. Thaler, J. Ullman, and S. Vadhan. Faster algorithms for privately releasing marginals. In ICALP  J. Ullman and S. Vadhan. Pcps and the hardness of generating private synthetic data. In TCC, pages  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.  J. Vondr\u00b4ak. Optimal approximation for the submodular welfare problem in the value oracle model.  In STOC, pages 67-74, 2008.  Jan Vondr\u00b4ak. A note on concentration of submodular functions. CoRR, abs/1005.2791, 2010.  Wikipedia. Least absolute deviations, 2010. URL http://en.wikipedia.org/wiki/  Least_absolute_deviations.  L. Yang, A. Blum, and J. Carbonell. Learnability of DNF with representation-specific queries. ITCS,  2013. LEARNING COVERAGE FUNCTIONS AND PRIVATE RELEASE OF MARGINALS  "}, "Computational Limits for Matrix Completion": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Computational Limits for Matrix Completion", "abstract": "Matrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries. Important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random. Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if we make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank\u00a04 but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown  matrix is incoherent and are shown 90% of the entries. This result relies on the conjectured hardness of the 4-Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that \\mathrmP\\ne \\mathrmNP. Our results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems.", "pdf_url": "http://proceedings.mlr.press/v35/hardt14b.pdf", "keywords": ["Matrix Completion", "Computational Hardness", "Coloring"], "reference": "number. ACM, 2006.  Sanjeev Arora, Eden Chlamtac, and Moses Charikar. New approximation guarantee for chromatic In Proc. 38th Annual Symposium on Theory of Computing (STOC), pages 215\u2013224.  12   HARDT MEKA RAGHAVENDRA WEITZ  point because inner product constraints are invariant to rotations. For each variable, we constrain its basis to be a special rotation of the reference basis. The rotation is special in the sense that it is a set of identical rotations in k pairs of two-dimensional subspaces. Because a rotation in two dimensions has exactly two configurations, rotate clockwise or rotate counter-clockwise, there are only two possible rotations of the variable\u2019s basis. We interpret each of these rotations as setting the variable to +1 or \u22121.  For each clause in \u03a6, the clause gadget is a set of constraints that are intended to construct a vector whose ith coordinate is the value (either +1 or \u22121) of the ith variable appearing in \u03a6. Finally, we constrain that the sum of the elements of \u03a6 is exactly (k \u2212 2). This forces exactly one of the coordinates of the vector to be \u22121, so the variables must be set to a satisfying assignment.  It is not obvious how we are able to force such specific structure on the rotations in the variable gadget even when the ambient dimension gets as high as 4k \u2212 1. By constraining dot products of sums of basis vectors in the variable gadget we are able to resolve this problem. The full details and proof of Theorem 16 are located in the "}, "Robust Multi-objective Learning with Mentor Feedback": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Robust Multi-objective Learning with Mentor Feedback", "abstract": "We study decision making when each action is described by a set of objectives, all of which are to be maximized. During the training phase, we have access to the actions of an outside agent (\u201cmentor\u201d). In the test phase, our goal is to maximally improve upon the mentor\u2019s (unobserved) actions across all objectives. We present an algorithm with a vanishing regret compared with the optimal possible improvement, and show that our regret bound is the best possible. The bound is independent of the number of actions, and scales only as the logarithm of the number of objectives.", "pdf_url": "http://proceedings.mlr.press/v35/agarwal14b.pdf", "keywords": ["multi-objective learning", "apprenticeship learning", "random matrix games"], "reference": "ICML, 2004.  Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119\u2013139, August 1997.  Paul E. Green and V. Srinivasan. Conjoint analysis in marketing: New developments with implica-  tions for research and practice. Journal of Marketing, 54(4):3\u201319, October 1990.  Nathan D. Ratliff, J. Andrew Bagnell, and Martin Zinkevich. Maximum margin planning. In ICML,  pages 729\u2013736, 2006.  Umar Syed and Robert Schapire. A game-theoretic approach to apprenticeship learning. In Ad-  vances in Neural Information Processing Systems 20, NIPS 2007, pages 1449\u20131456, 2008.  Kenneth Train. Discrete Choice Methods with Simulation. Cambridge University Press, New York,  NY, 2nd edition, 2009.  Bin Yu. Assouad, Fano and Le Cam. Festschrift in Honor of L. Le Cam on his 70th Birthday, 1993.  Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy  inverse reinforcement learning. In AAAI, pages 1433\u20131438, 2008.  Appendix A. Application to apprenticeship learning  As noted earlier, our model is a generalization of the apprenticeship learning framework of Abbeel and Ng (2004) as subsequently extended by Syed and Schapire (2008). Here, we brie\ufb02y describe how their framework and algorithm are essentially a special case of ours.  In this setting, an agent called an apprentice is allowed to observe the actions of a so-called expert in a stochastic environment modeled as a Markov Decision Process (MDP). We assume that both the apprentice and the expert have access to the states, actions, and transition function of the MDP, as well as an assumed initial distribution over start states. However, the MDP has no explicit reward function. Rather, each state \u03c3 is associated with a vector of reward features, denoted \u03c6(\u03c3). We suppose that the \u201ctrue\u201d reward function is some convex combination q\u2217 of the reward features so that the (never directly observed) reward at state \u03c3 is q\u2217 \u00b7 \u03c6(\u03c3). Although q\u2217 is unknown, we assume that each of the reward features has known direction, that is, that larger values are always at least as desirable as lower values.  We suppose that the apprentice observes the expert following trajectories through the MDP. On each one of these, an initial state \u03c30 is chosen from the initial state distribution D0, then actions are selected by the expert causing the MDP to progress along a trajectory of states \u03c3 = (\u03c30, \u03c31, . . .) in the usual fashion. For simplicity, we assume these trajectories are in\ufb01nite, although, as with most of our assumptions, this certainly can be relaxed. Further, we assume that the expert\u2019s behavior induces a distribution D over such state sequences, and that these trajectories are sampled independently from D.  14   AGARWAL BADANIDIYURU DUD\u00b4IK SCHAPIRE SLIVKINS  References  ICML, 2004.  Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. J. Comput. Syst. Sci., 55(1):119-139, August 1997.  Paul E. Green and V. Srinivasan. Conjoint analysis in marketing: New developments with implica-  tions for research and practice. Journal of Marketing, 54(4):3-19, October 1990.  Nathan D. Ratliff, J. Andrew Bagnell, and Martin Zinkevich. Maximum margin planning. In ICML,  pages 729-736, 2006.  Umar Syed and Robert Schapire. A game-theoretic approach to apprenticeship learning. In Ad-  vances in Neural Information Processing Systems 20, NIPS 2007, pages 1449-1456, 2008.  Kenneth Train. Discrete Choice Methods with Simulation. Cambridge University Press, New York,  NY, 2nd edition, 2009.  Bin Yu. Assouad, Fano and Le Cam. Festschrift in Honor of L. Le Cam on his 70th Birthday, 1993.  Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy  inverse reinforcement learning. In AAAI, pages 1433-1438, 2008.  "}, "Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability", "abstract": "We give a robust version of the celebrated result of Kruskal on the uniqueness of tensor decompositions: given a tensor whose decomposition satisfies a robust form of Kruskal\u2019s rank condition, we prove that it is possible to approximately recover the decomposition if the tensor is known up to a sufficiently small (inverse polynomial) error. Kruskal\u2019s theorem has found many applications in proving the identifiability of parameters for various latent variable models and mixture models such as Hidden Markov models, topic models etc. Our robust version immediately implies identifiability using only polynomially many samples in many of these settings \u2013 an essential first step towards efficient learning algorithms. Our methods also apply to the \u201covercomplete\u201d case, which has proved challenging in many applications. Given the importance of Kruskal\u2019s theorem in the tensor literature, we expect that our robust version will have several applications beyond the settings we explore in this work.", "pdf_url": "http://proceedings.mlr.press/v35/bhaskara14a.pdf", "keywords": ["Kruskal uniqueness theorem", "tensor decomposition", "latent variable models"], "reference": "Elizabeth S Allman, Catherine Matias, and John A Rhodes. Identifiability of parameters in latent structure models with many observed variables. The Annals of Statistics, 37(6A):3099-3132, 2009.  Elizabeth S Allman, Sonia Petrovic, John A Rhodes, and Seth Sullivant.  Identifiability of two- tree mixtures for group-based models. Computational Biology and Bioinformatics, IEEE/ACM Transactions on, 8(3):710-722, 2011.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-  positions for learning latent variable models. arXiv preprint arXiv:1210.7559, 2012a.  Anima Anandkumar, Daniel Hsu, Furong Huang, and Sham Kakade. Learning mixtures of tree graphical models. In Advances in Neural Information Processing Systems 25, pages 1061-1069, 2012b.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012c.  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R. Voss. The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures. CoRR, abs/1311.2891, 2013.  Saugata Basu, Richard Pollack, and Marie-Franc\u00b8oise Roy. On the combinatorial and algebraic complexity of quantifier elimination. J. ACM, 43(6):1002-1045, November 1996. ISSN 0004- 5411. doi: 10.1145/235809.235813. URL http://doi.acm.org/10.1145/235809. 235813.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103-112. IEEE, 2010.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal- ysis of tensor decompositions. In Proceedings of the 46th annual ACM Symposium on Theory of Computing (STOC). ACM, 2014.  Joseph T Chang. Full reconstruction of markov models on evolutionary trees: identifiability and  consistency. Mathematical biosciences, 137(1):51-73, 1996.  L. Chiantini and G. Ottaviani. On generic identifiability of 3-tensors of small rank. SIAM Journal  on Matrix Analysis and Applications, 33(3):1018-1037, 2012. doi: 10.1137/110829180.  L. De Lathauwer, J. Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of  underdetermined mixtures. IEEE Trans. on Signal Processing, 55(6):2965-2973, 2007.  13   UNIQUENESS OF TENSOR DECOMPOSITIONS  We thank Ravi Kannan for valuable discussions about the algorithmic results in this work, and Daniel Hsu for helpful pointers to the literature. The third author would also like to thank Siddharth Gopal for some useful pointers about HMM models in speech and image recognition.  Acknowledgments  References  Elizabeth S Allman, Catherine Matias, and John A Rhodes. Identifiability of parameters in latent structure models with many observed variables. The Annals of Statistics, 37(6A):3099-3132, 2009.  Elizabeth S Allman, Sonia Petrovic, John A Rhodes, and Seth Sullivant.  Identifiability of two- tree mixtures for group-based models. Computational Biology and Bioinformatics, IEEE/ACM Transactions on, 8(3):710-722, 2011.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-  positions for learning latent variable models. arXiv preprint arXiv:1210.7559, 2012a.  Anima Anandkumar, Daniel Hsu, Furong Huang, and Sham Kakade. Learning mixtures of tree graphical models. In Advances in Neural Information Processing Systems 25, pages 1061-1069, 2012b.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012c.  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R. Voss. The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures. CoRR, abs/1311.2891, 2013.  Saugata Basu, Richard Pollack, and Marie-Franc\u00b8oise Roy. On the combinatorial and algebraic complexity of quantifier elimination. J. ACM, 43(6):1002-1045, November 1996. ISSN 0004- 5411. doi: 10.1145/235809.235813. URL http://doi.acm.org/10.1145/235809. 235813.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103-112. IEEE, 2010.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal- ysis of tensor decompositions. In Proceedings of the 46th annual ACM Symposium on Theory of Computing (STOC). ACM, 2014.  Joseph T Chang. Full reconstruction of markov models on evolutionary trees: identifiability and  consistency. Mathematical biosciences, 137(1):51-73, 1996.  L. Chiantini and G. Ottaviani. On generic identifiability of 3-tensors of small rank. SIAM Journal  on Matrix Analysis and Applications, 33(3):1018-1037, 2012. doi: 10.1137/110829180.  L. De Lathauwer, J. Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of  underdetermined mixtures. IEEE Trans. on Signal Processing, 55(6):2965-2973, 2007. BHASKARA CHARIKAR VIJAYARAGHAVAN  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca. In Proceedings of the 46th annual  ACM Symposium on Theory of Computing (STOC). ACM, 2014.  Nick Gravin, Jean Lasserre, Dmitrii V Pasechnik, and Sinai Robins. The inverse moment problem  for convex polytopes. Discrete & Computational Geometry, 48(3):596-621, 2012.  Richard A Harshman. Foundations of the parafac procedure: models and conditions for an explana-  tory multimodal factor analysis. 1970.  Tao Jiang and Nicholas D Sidiropoulos. Kruskal\u2019s permutation lemma and the identification of candecomp/parafac and bilinear models with constant modulus constraints. Signal Processing, IEEE Transactions on, 52(9):2625-2636, 2004.  Joseph B Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with appli- cation to arithmetic complexity and statistics. Linear algebra and its applications, 18(2):95-138, 1977.  J.M. Landsberg. Tensors:: Geometry and Applications. Graduate Studies in Mathematics Se- ries. American Mathematical Society, 2012. ISBN 9780821869079. URL http://books. google.com.au/books?id=JTjv3DTvxZIC.  S. Leurgans, R. Ross, and R. Abel. A decomposition for three-way arrays. SIAM Journal on Matrix  Analysis and Applications, 14(4):1064-1083, 1993.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93- 102. IEEE, 2010.  Elchanan Mossel and S\u00b4ebastien Roch. Learning nonsingular phylogenies and hidden markov mod-  els. The Annals of Applied Probability, pages 583-614, 2006.  Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions  of the Royal Society of London. A, 185:71-110, 1894.  John A Rhodes. A concise proof of kruskal\u2019s theorem on tensor decomposition. Linear Algebra  and Its Applications, 432(7):1818-1824, 2010.  John A Rhodes and Seth Sullivant. Identifiability of large phylogenetic mixture models. Bulletin of  mathematical biology, 74(1):212-231, 2012.  Nicholas D Sidiropoulos and Rasmus Bro. On the uniqueness of multilinear decomposition of  n-way arrays. Journal of chemometrics, 14(3):229-239, 2000.  Alwin Stegeman and Nicholas D Sidiropoulos. On kruskal\u2019s uniqueness condition for the cande-  comp/parafac decomposition. Linear Algebra and its applications, 420(2):540-552, 2007.  GM Tallis and P Chesson. Identifiability of mixtures. J. Austral. Math. Soc. Ser. A, 32(3):339-348,  Henry Teicher. Identifiability of mixtures. The annals of Mathematical statistics, 32(1):244-248,  1982.  1961. UNIQUENESS OF TENSOR DECOMPOSITIONS  Henry Teicher. Identifiability of mixtures of product measures. The Annals of Mathematical Statis-  tics, 38(4):1300-1302, 1967.  "}, "New Algorithms for Learning Incoherent and Overcomplete Dictionaries ": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries ", "abstract": "In \\em sparse recovery we are given a matrix A \u2208\\mathbbR^n\\times m (\u201cthe dictionary\u201d) and a vector of the form A X where X is \\em sparse, and the goal is to recover X. This is a central notion in signal processing, statistics and machine learning. But in applications such as \\em sparse coding, edge detection, compression and super resolution, the dictionary A is unknown and has to be learned from random examples of the form Y = AX where X is drawn from an appropriate distribution \u2014 this is the \\em dictionary learning problem. In most settings, A is \\em overcomplete: it has more columns than rows. This paper presents a polynomial-time algorithm for learning overcomplete dictionaries; the only previously known algorithm with provable guarantees is the recent work of Spielman et al. (2012) who who gave an algorithm for the undercomplete case, which is rarely the case in applications. Our algorithm applies to \\em incoherent dictionaries which have been a central object of study since they were introduced in seminal work of Donoho and Huo (1999). In particular, a dictionary is \u03bc-incoherent if each pair of columns has inner product at most \u03bc/ \\sqrtn. The algorithm makes natural stochastic assumptions about the unknown sparse vector X, which can contain k \u2264c \\min(\\sqrtn/\u03bc\\log n, m^1/2 - \u03b7) non-zero entries (for any \u03b7> 0). This is close to the best k allowable by the best sparse recovery algorithms  \\em even if one knows the dictionary A exactly. Moreover, both the running time and sample complexity depend on \\log 1/\u03b5, where \u03b5is the target accuracy, and so our algorithms converge very quickly to the true dictionary. Our algorithm can also tolerate substantial amounts of noise provided it is incoherent with respect to the dictionary (e.g., Gaussian). In the noisy setting, our running time and sample complexity depend polynomially on 1/\u03b5, and this is necessary.", "pdf_url": "http://proceedings.mlr.press/v35/arora14.pdf", "keywords": [], "reference": "A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning sparsely used over-  complete dictionaries via alternating minimization. In arxiv:1310.7991, 2013a.  A. Agarwal, A. Anandkumar, and P. Netrapalli. Exact recovery of sparsely used overcomplete  dictionaries. In arxiv:1309.1952, 2013b.  M. Aharon. Overcomplete dictionaries for sparse representation of signals. In PhD Thesis, 2006.  M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictio- naries for sparse representation. In IEEE Trans. on Signal Processing, pages 4311-4322, 2006.  N. Alon and D. Kleitman. Piercing convex sets and the hadwigder debrunner (p, q)-problem. In  Advances in Mathematics, pages 103-112, 1992.  A. Anandkumar, D. Foster, D. Hsu, S. Kakade, and Y. Liu. A spectral algorithm for latent dirichlet  allocation. In NIPS, pages 926-934, 2012.  S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative matrix factorization - prov-  ably. In STOC, pages 145-162, 2012a.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond svd. In FOCS, pages 1-10,  2012b.  2010.  1994.  S. Arora, R. Ge, S. Sachdeva, and G. Schoenebeck. Finding overlapping communities in social  networks: Towards a rigorous approach. In EC, 2012c.  M. Balcan, C. Borgs, M. Braverman, J. Chayes, and S-H Teng. Finding endogenously formed  communities. In SODA, 2013.  unpublished manuscript, 2014.  Boaz Barak, John Kelner, and David Steurer. Dictionary learning using sum-of-square hierarchy.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  E. Candes and T. Tao. Decoding by linear programming. In IEEE Trans. on Information Theory,  pages 4203-4215, 2005.  E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate mea-  surements. In Communications of Pure and Applied Math, pages 1207-1223, 2006.  P. Comon. Independent component analysis: A new concept? In Signal Processing, pages 287-314,  G. Davis, S. Mallat, and M. Avellaneda. Greedy adaptive approximations. In J. of Constructive  Approximation, pages 57-98, 1997.  D. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dictionaries  via (cid:96)1-minimization. In PNAS, pages 2197-2202, 2003.  14   ARORA GE MOITRA  References  A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning sparsely used over-  complete dictionaries via alternating minimization. In arxiv:1310.7991, 2013a.  A. Agarwal, A. Anandkumar, and P. Netrapalli. Exact recovery of sparsely used overcomplete  dictionaries. In arxiv:1309.1952, 2013b.  M. Aharon. Overcomplete dictionaries for sparse representation of signals. In PhD Thesis, 2006.  M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictio- naries for sparse representation. In IEEE Trans. on Signal Processing, pages 4311-4322, 2006.  N. Alon and D. Kleitman. Piercing convex sets and the hadwigder debrunner (p, q)-problem. In  Advances in Mathematics, pages 103-112, 1992.  A. Anandkumar, D. Foster, D. Hsu, S. Kakade, and Y. Liu. A spectral algorithm for latent dirichlet  allocation. In NIPS, pages 926-934, 2012.  S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative matrix factorization - prov-  ably. In STOC, pages 145-162, 2012a.  S. Arora, R. Ge, and A. Moitra. Learning topic models - going beyond svd. In FOCS, pages 1-10,  2012b.  2010.  1994.  S. Arora, R. Ge, S. Sachdeva, and G. Schoenebeck. Finding overlapping communities in social  networks: Towards a rigorous approach. In EC, 2012c.  M. Balcan, C. Borgs, M. Braverman, J. Chayes, and S-H Teng. Finding endogenously formed  communities. In SODA, 2013.  unpublished manuscript, 2014.  Boaz Barak, John Kelner, and David Steurer. Dictionary learning using sum-of-square hierarchy.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  E. Candes and T. Tao. Decoding by linear programming. In IEEE Trans. on Information Theory,  pages 4203-4215, 2005.  E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate mea-  surements. In Communications of Pure and Applied Math, pages 1207-1223, 2006.  P. Comon. Independent component analysis: A new concept? In Signal Processing, pages 287-314,  G. Davis, S. Mallat, and M. Avellaneda. Greedy adaptive approximations. In J. of Constructive  Approximation, pages 57-98, 1997.  D. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dictionaries  via (cid:96)1-minimization. In PNAS, pages 2197-2202, 2003. NEW ALGORITHMS FOR LEARNING INCOHERENT AND OVERCOMPLETE DICTIONARIES  D. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. In IEEE Trans. on  Information Theory, pages 2845-2862, 1999.  D. Donoho and P. Stark. Uncertainty principles and signal recovery. In SIAM J. on Appl. Math,  pages 906-931, 1999.  M. Elad. Sparse and redundant representations. In Springer, 2010.  M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned  dictionaries. In IEEE Trans. on Signal Processing, pages 3736-3745, 2006.  K. Engan, S. Aase, and J. Hakon-Husoy. Method of optimal directions for frame design. In ICASSP,  pages 2443-2446, 1999.  1996.  A. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In FOCS, pages 359-368,  Q. Geng, H. Wang, and J. Wright. On the local correctness of (cid:96)1-minimization for dictionary  learning. In arxiv:1101.5672, 2013.  A. Gilbert, S. Muthukrishnan, and M. Strauss. Approximation of functions over redundant dictio-  naries using coherence. In SODA, 2003.  G. Golub and C. van Loan. Matrix computations. In The Johns Hopkins University Press, 1996.  I. J. Goodfellow, A. Courville, and Y.Bengio. Large-scale feature learning with spike-and-slab  sparse coding. In ICML, pages 718-726, 2012.  N. Goyal, S. Vempala, and Y. Xiao. Fourier pca. In STOC, 2014.  R. Gribonval and M. Nielsen. Sparse representations in unions of bases. In IEEE Transactions on  Information Theory, pages 3320-3325, 2003.  D. Gross. Recovering low-rank matrices from few coefficients in any basis. In arxiv:0910.1879,  2009.  D. Hanson and F. Wright. A bound on tail probabilities for quadratic forms in independent random  variables. In Annals of Math. Stat., pages 1079-1083, 1971.  M. Hardt. On the provable convergence of alternating minimization for matrix completion.  In  arxiv:1312.0925, 2013.  R. Horn and C. Johnson. Matrix analysis. In Cambridge University Press, 1990.  P. Jain, P. Netrapalli, and S. Sanghavi. Low rank matrix completion using alternating minimization.  In STOC, pages 665-674, 2013.  K. Kavukcuoglu, M. Ranzato, and Y. LeCun. Fast inference in sparse coding algorithms with  applications to object recognition. In NYU Tech Report, 2008.  K. Kreutz-Delgado, J. Murray, K. Engan B. Rao, T. Lee, and T. Sejnowski. Dictionary learning  algorithms for sparse representation. In Neural Computation, 2003. ARORA GE MOITRA  L. De Lathauwer, J Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of  underdetermined mixtures. In IEEE Trans. on Signal Processing, pages 2965-2973, 2007.  H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. In NIPS, 2006.  M. Lewicki and T. Sejnowski. Learning overcomplete representations.  In Neural Computation,  pages 337-365, 2000.  J. Mairal, M. Leordeanu, F. Bach, M. Herbert, and J. Ponce. Discriminative sparse image models  for class-specific edge detection and image interpretation. In ECCV, 2008.  S. Mallat. A wavelet tour of signal processing. In Academic-Press, 1998.  J. Matousek. Lectures on discrete geometry. In Springer, 2002.  A. Moitra and G. Valiant. Setting the polynomial learnability of mixtures of gaussians. In FOCS,  pages 93-102, 2010.  B. Olshausen and B. Field. Sparse coding with an overcomplete basis set: A strategy employed by  v1? In Vision Research, pages 3331-3325, 1997.  M. Pontil, A. Argyriou, and T. Evgeniou. Multi-task feature learning. In NIPS, 2007.  M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In NIPS,  2007.  1999.  M. Rudelson. Random vectors in the isotropic position. In J. of Functional Analysis, pages 60-72,  D. Spielman, H. Wang, and J. Wright. Exact recovery of sparsely-used dictionaries. In Journal of  Machine Learning Research, 2012.  J. Tropp. Greed is good: Algorithmic results for sparse approximation. In IEEE Transactions on  Information Theory, pages 2231-2242, 2004.  J. Tropp, A. Gilbert, S. Muthukrishnan, and M. Strauss. Improved sparse approximation over quasi-  incoherent dictionaries. In IEEE International Conf. on Image Processing, 2003.  P. Wedin. Perturbation bounds in connection with singular value decompositions. In BIT, pages  99-111, 1972.  image patches. In CVPR, 2008.  J. Yang, J. Wright, T. Huong, and Y. Ma. Image super-resolution as sparse representation of raw  X. Yuan and T. Zhang. Truncated power method for sparse eigenvalue problems. In Journal of  Machine Learning Research, pages 899-925, 2013. NEW ALGORITHMS FOR LEARNING INCOHERENT AND OVERCOMPLETE DICTIONARIES  "}, "Online Linear Optimization via Smoothing": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Online Linear Optimization via Smoothing", "abstract": "We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between \u201cFollow the Regularized Leader\u201d and \u201cFollow the Perturbed Leader\u201d up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.", "pdf_url": "http://proceedings.mlr.press/v35/abernethy14.pdf", "keywords": [], "reference": "Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Amir Beck and Marc Teboulle. Smoothing and first order methods: A unified framework. SIAM  Journal on Optimization, 22(2):557-580, 2012.  Dimitri P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal of Optimization Theory and Applications, 12(2):218-231, 1973. ISSN 0022-3239. doi: 10.1007/BF00934819.  Shalabh Bhatnagar. Adaptive newton-based multivariate smoothed functional algorithms for simu-  lation optimization. ACM Transactions on Modeling and Computer Simulation, 2007.  Chris M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation,  7(1):108-116, January 1995. ISSN 0899-7667.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006. ISBN 978-0-521-84108-5.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Pro-  ceedings of Conference on Learning Theory (COLT), 2013.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. In Proceedings of Conference on Learning Theory (COLT), 2010.  John Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674-701, 2012. doi: 10.1137/110831659.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 - 139, 1997. ISSN 0022-0000. doi: http://dx.doi.org/10.1006/jcss.1997.1504.  Paul Glasserman. Gradient Estimation Via Perturbation Analysis. Kluwer international series in engineering and computer science: Discrete event dynamic systems. Springer, 1991. ISBN 9780792390954.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,  3:97-139, 1957.  April 1965.  John C. Harsanyi. Oddness of the number of equilibrium points: a new proof. International Journal  of Game Theory, 2(1):235-250, 1973.  James R. Harvey. Fractional moments of a quadratic form in noncentral normal random variables,  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv preprint, arXiv:1207.0580, 2012.  13   ONLINE LINEAR OPTIMIZATION VIA SMOOTHING  References  Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Amir Beck and Marc Teboulle. Smoothing and first order methods: A unified framework. SIAM  Journal on Optimization, 22(2):557-580, 2012.  Dimitri P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal of Optimization Theory and Applications, 12(2):218-231, 1973. ISSN 0022-3239. doi: 10.1007/BF00934819.  Shalabh Bhatnagar. Adaptive newton-based multivariate smoothed functional algorithms for simu-  lation optimization. ACM Transactions on Modeling and Computer Simulation, 2007.  Chris M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation,  7(1):108-116, January 1995. ISSN 0899-7667.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006. ISBN 978-0-521-84108-5.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Pro-  ceedings of Conference on Learning Theory (COLT), 2013.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. In Proceedings of Conference on Learning Theory (COLT), 2010.  John Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674-701, 2012. doi: 10.1137/110831659.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 - 139, 1997. ISSN 0022-0000. doi: http://dx.doi.org/10.1006/jcss.1997.1504.  Paul Glasserman. Gradient Estimation Via Perturbation Analysis. Kluwer international series in engineering and computer science: Discrete event dynamic systems. Springer, 1991. ISBN 9780792390954.  James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,  3:97-139, 1957.  April 1965.  John C. Harsanyi. Oddness of the number of equilibrium points: a new proof. International Journal  of Game Theory, 2(1):235-250, 1973.  James R. Harvey. Fractional moments of a quadratic form in noncentral normal random variables,  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv preprint, arXiv:1207.0580, 2012. ABERNETHY LEE SINHA TEWARI  Josef Hofbauer and William H. Sandholm. On the global convergence of stochastic fictitious play.  Econometrica, 70(6):2265-2294, 2002.  Adam T. Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  H. Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems  and l1 regularization. In AISTATS, pages 525-533, 2011.  Ilya S. Molchanov. Theory of random sets. Probability and its applications. Springer, New York,  2005. ISBN 1-85233-892-X.  2011.  Yurii Nesterov. Random gradient-free minimization of convex functions. ECORE Discussion Paper,  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize : From value to  algorithms. In Proceedings of Neural Information Processing Systems (NIPS), 2012.  R.T. Rockafellar. Convex Analysis. Convex Analysis. Princeton University Press, 1997.  ISBN  9780691015866.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, February 2012. ISSN 1935-8237.  Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In  Proceedings of Neural Information Processing Systems (NIPS), pages 2645-2653, 2011.  Tim van Erven, Wojciech Kotlowski, and Manfred K. Warmuth. Follow the leader with dropout  perturbations. In Proceedings of Conference on Learning Theory (COLT), 2014.  Stefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Pro-  ceedings of Neural Information Processing Systems (NIPS), 2013.  Manfred Warmuth. A perturbation that makes \u201cFollow the leader\u201d equivalent to \u201cRandom- ized weighted majority\u201d. http://classes.soe.ucsc.edu/cmps290c/Spring09/ lect/10/wmkalai-rewrite.pdf, 2009.  Farzad Yousean, Angelia Nedi\u00b4c, and Uday V. Shanbhag. Convex nondifferentiable stochastic opti- mization: A local randomized smoothing technique. In Proceedings of American Control Con- ference (ACC), 2010, pages 4875-4880, June 2010.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In  International Conference on Machine Learning (ICML), 2003. ONLINE LINEAR OPTIMIZATION VIA SMOOTHING  "}, "Learning Mixtures of Discrete Product Distributions using Spectral Decompositions": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Learning Mixtures of Discrete Product Distributions using Spectral Decompositions", "abstract": "We study the problem of learning a distribution from samples, when the underlying distribution is a mixture of product distributions over discrete domains. This problem is motivated by several practical applications such as  crowdsourcing, recommendation systems, and learning Boolean functions. The existing solutions either heavily rely on the fact that the number of mixtures is finite or have  sample/time complexity that is exponential in the number of mixtures. In this paper, we introduce a polynomial time/sample complexity  method for learning a mixture of r discrete product distributions over {1, 2, \u2026, \\ell}^n, for  general \\ell and r. We show that our approach is consistent and further provide finite sample guarantees. We use recently developed techniques from tensor decompositions for moment matching. A crucial step  in these approaches is to construct certain  tensors  with low-rank spectral decompositions. These tensors are typically estimated from the sample moments. The main challenge in learning mixtures of discrete product distributions is that  the corresponding low-rank tensors cannot be obtained directly from the sample moments. Instead, we need to estimate a low-rank matrix using only off-diagonal entries, and estimate a tensor using a few linear measurements. We give an alternating minimization based method to estimate the low-rank matrix, and  formulate the tensor estimation problem as a least-squares problem.", "pdf_url": "http://proceedings.mlr.press/v35/jain14.pdf", "keywords": [], "reference": "Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  Learning Theory, pages 458-469. Springer, 2005.  A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden  markov models. arXiv preprint arXiv:1203.0683, 2012a.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012b.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, pages 247-257, 2001.  Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models - going beyond SVD. In FOCS,  pages 1-10, 2012a.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown Gaus- sian noise, with implications for Gaussian mixtures and autoencoders. In NIPS, pages 2384-2392, 2012b.  K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and inde-  pendence. In COLT, pages 9-20, 2008.  Kamalika Chaudhuri, Eran Halperin, Satish Rao, and Shuheng Zhou. A rigorous analysis of popu-  lation stratification with limited data. In SODA, pages 1046-1055, 2007.  A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20-28, 1979.  J. Feldman, R. O\u2019Donnell, and R. A Servedio. Learning mixtures of product distributions over  discrete domains. SIAM Journal on Computing, 37(5):1536-1564, 2008.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions.  In COLT, pages  53-62, 1999.  A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators?: crowdsourcing abuse detection  in user-generated content. In EC, pages 167-176, 2011.  Navin Goyal and Luis Rademacher. Efficient learning of simplices. CoRR, abs/1211.2227, 2012.  Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion using alternating minimization. In Machine Learning and Knowledge Discovery in Databases, pages 194-209. Springer, 2013.  D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral  decompositions. In ITCS, pages 11-20, 2013.  D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models.  Journal of Computer and System Sciences, 78(5):1460-1480, 2012.  14   JAIN OH  References  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  Learning Theory, pages 458-469. Springer, 2005.  A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden  markov models. arXiv preprint arXiv:1203.0683, 2012a.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor de-  compositions for learning latent variable models. CoRR, abs/1210.7559, 2012b.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, pages 247-257, 2001.  Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models - going beyond SVD. In FOCS,  pages 1-10, 2012a.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown Gaus- sian noise, with implications for Gaussian mixtures and autoencoders. In NIPS, pages 2384-2392, 2012b.  K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and inde-  pendence. In COLT, pages 9-20, 2008.  Kamalika Chaudhuri, Eran Halperin, Satish Rao, and Shuheng Zhou. A rigorous analysis of popu-  lation stratification with limited data. In SODA, pages 1046-1055, 2007.  A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20-28, 1979.  J. Feldman, R. O\u2019Donnell, and R. A Servedio. Learning mixtures of product distributions over  discrete domains. SIAM Journal on Computing, 37(5):1536-1564, 2008.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions.  In COLT, pages  53-62, 1999.  A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators?: crowdsourcing abuse detection  in user-generated content. In EC, pages 167-176, 2011.  Navin Goyal and Luis Rademacher. Efficient learning of simplices. CoRR, abs/1211.2227, 2012.  Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion using alternating minimization. In Machine Learning and Knowledge Discovery in Databases, pages 194-209. Springer, 2013.  D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral  decompositions. In ITCS, pages 11-20, 2013.  D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models.  Journal of Computer and System Sciences, 78(5):1460-1480, 2012. LEARNING MIXTURES OF DISCRETE PRODUCT DISTRIBUTIONS USING SPECTRAL DECOMPOSITIONS  Siu L Hui and Xiao H Zhou. Evaluation of diagnostic tests without gold standards. Statistical  methods in medical research, 7(4):354-370, 1998.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat-  ing minimization. In STOC, pages 665-674, 2013.  D. R. Karger, S. Oh, and D. Shah. Budget-optimal task allocation for reliable crowdsourcing sys-  tems. arXiv preprint arXiv:1110.3564, 2011a.  D. R. Karger, S. Oh, and D. Shah. Budget-optimal crowdsourcing using low-rank matrix approxi-  mations. In Allerton, 2011b.  D. R. Karger, S. Oh, and D. Shah. Efficient crowdsourcing for multi-class labeling. In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems, pages 81-92, 2013.  M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. On the learnability of  discrete distributions. In STOC, pages 273-282, 1994.  Frank McSherry. Spectral partitioning of random graphs. In FOCS, pages 529-537, 2001.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians.  In FOCS, pages 93-102, 2010.  Yuval Rabani, Leonard J. Schulman, and Chaitanya Swamy. Learning mixtures of arbitrary distri-  butions over large discrete domains. CoRR, abs/1212.1527, 2012.  James Saunderson, Venkat Chandrasekaran, Pablo A. Parrilo, and Alan S. Willsky. Diagonal and low-rank matrix decompositions, correlation matrices, and ellipsoid fitting. SIAM J. Matrix Anal- ysis Applications, 33(4):1395-1416, 2012.  V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data  mining using multiple, noisy labelers. In KDD, pages 614-622, 2008.  P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi.  Inferring ground truth from subjective  labelling of venus images. In NIPS, pages 1085-1092, 1995.  S. Sridhar, S. Rao, and E. Halperin. An efficient and accurate graph-based approach to detect population substructure. In Research in Computational Molecular Biology, pages 503-517, 2007.  Dan-Cristian Tomozei and Laurent Massouli\u00b4e. Distributed user profiling via spectral methods. In  ACM SIGMETRICS Performance Evaluation Review, volume 38, pages 383-384, 2010.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  Syst. Sci., 68(4):841-860, 2004.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J. Comput. JAIN OH  "}, "Localized Complexities for Transductive Learning": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Localized Complexities for Transductive Learning", "abstract": "We show two novel concentration inequalities for suprema of empirical processes when sampling without replacement, which both take the variance of the functions into account. While these inequalities may potentially have broad applications in learning theory in general, we exemplify their significance by studying the transductive setting of learning theory. For which we provide the first excess risk bounds based on the localized complexity of the hypothesis class, which can yield fast rates of convergence also in the transductive learning setting. We give a preliminary analysis of the localized complexities for the prominent case of kernel classes.", "pdf_url": "http://proceedings.mlr.press/v35/tolstikhin14.pdf", "keywords": ["Statistical Learning", "Transductive Learning", "Fast Rates", "Localized Complexities", "Concentration Inequalities", "Empirical Processes", "Kernel Classes"], "reference": "R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applications to  markov chains. Electronic Journal of Probability, 34(13), 2008.  R. Bardenet and O.-A. Maillard. Concentration inequalities for sampling without replacement.  http://arxiv.org/abs/1309.4029, 2013.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statis-  tics, 33(4):14971537, 2005.  13   LOCALIZED COMPLEXITIES FOR TRANSDUCTIVE LEARNING  Remark 16 The question of transductive convergence rates is somewhat delicate, since all results stated here assume a fixed set XN , as re\ufb02ected for instance in the bound of Corollary 15 depending on the eigenvalues of the kernel Gram matrix of the set XN . In order to give a precise meaning to rates, one has to specify how XN evolves as N grows. A natural setting for this is Vapnik (1998)\u2019s second transductive setting where XN is i.i.d. from some generating distribution. In that case we think it is possible to adapt once again the results of Bartlett et al. (2005) in order to relate the quantities r\u2217 m(N ) to asymptotic counterparts as N \u2192 \u221e, though we do not pursue this avenue in the present work.  5. Conclusion  In this paper, we have considered the setting of transductive learning over a broad class of bounded and nonnegative loss functions. We provide excess risk bounds for the transductive learning setting based on the localized complexity of the hypothesis class, which hold under general assumptions on the loss function and the hypothesis class. When applied to kernel classes, the transductive excess risk bound can be formulated in terms of the tailsum of the eigenvalues of the kernels, similar to the best known estimates in inductive learning. The localized excess risk bound is achieved by proving two novel and very general concentration inequalities for suprema of empirical processes when sampling without replacement, which are of potential interest also in various other application areas in machine learning and learning theory, where they may serve as a fundamental mathematical tool. For instance, sampling without replacement is commonly employed in the Nystr\u00a8om method (Kumar et al., 2012), which is an efficient technique to generate low-rank matrix approximations in large-scale machine learning. Another potential application area of our novel concentration in- equalities could be the analysis of randomized sequential algorithms such as stochastic gradient descent and randomized coordinate descent, practical implementations of which often deploy sam- pling without replacement (Recht and Re, 2012). Very interesting also would be to explore whether the proposed techniques could be used to generalize matrix Bernstein inequalities (Tropp, 2012) to the case of sampling without replacement, which could be used to analyze matrix completion problems (Koltchinskii et al., 2011). The investigation of application areas beyond the transductive learning setting is, however, outside of the scope of the present paper.  The authors are thankful to Sergey Bobkov, Stanislav Minsker, and Mehryar Mohri for stimulating discussions and to the anonymous reviewers for their helpful comments. Marius Kloft acknowledges a postdoctoral fellowship by the German Research Foundation (DFG).  Acknowledgments  References  R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applications to  markov chains. Electronic Journal of Probability, 34(13), 2008.  R. Bardenet and O.-A. Maillard. Concentration inequalities for sampling without replacement.  http://arxiv.org/abs/1309.4029, 2013.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statis-  tics, 33(4):14971537, 2005. TOLSTIKHIN BLANCHARD KLOFT  P. L. Bartlett, S. Mendelson, and P. Phillips. On the optimality of sample-based estimates of the  expectation of the empirical minimizer. ESAIM: Probability and Statistics, 2010.  A. Blum and J. Langford. PAC-MDL bounds. In Proceedings of the International Conference on  Computational Learning Theory (COLT), 2003.  S. Bobkov. Concentration of normalized sums and a central limit theorem for noncorrelated random  variables. Annals of Probability, 32, 2004.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of  Independence. Oxford University Press, 2013.  O. Bousquet. A Bennett concentration inequality and its application to suprema of empirical pro-  cesses. C. R. Acad. Sci. Paris, Ser. I, 334:495-500, 2002a.  O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis  of Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002b.  C. Cortes and M. Mohri. On transductive regression. In Advances in Neural Information Processing  C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn., 20:273-297, September 1995.  Systems (NIPS), 2006.  ISSN 0885-6125.  C. Cortes, M. Mohri, D. Pechyony, and A. Rastogi. Stability analysis and learning bounds for  transductive regression algorithms. http://arxiv.org/abs/0904.0814, 2009.  P. Derbeko, R. El-Yaniv, and R. Meir. Explicit learning curves for transduction and application to clustering and compression algorithms. Journal of Artificial Intelligence Research, 22, 2004.  R. El-Yaniv and D. Pechyony. Stable transductive learning. In Proceedings of the International  Conference on Computational Learning Theory (COLT), 2006.  R. El-Yaniv and D. Pechyony. Transductive rademacher complexity and its applications. Journal of  Artificial Intelligence Research, 2009.  D. Gross and V. Nesme. Note on sampling without replacing from a fnite collection of matrices.  http://arxiv.org/abs/1001.2738v2, 2010.  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58(301):13-30, 1963.  T. Klein and E. Rio. Concentration around the mean for maxima of empirical processes. The Annals  of Probability, 33(3):10601077, 2005.  V. Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. The  Annals of Statistics, 34(6):25932656, 2006.  V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: \u00b4Ecole D \u00b4Et\u00b4e de Probabilit\u00b4es de Saint-Flour XXXVIII-2008. Ecole d\u2019\u00b4et\u00b4e de probabilit\u00b4es de Saint- Flour. Springer, 2011a. LOCALIZED COMPLEXITIES FOR TRANSDUCTIVE LEARNING  V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems.  \u00b4Ecole d\u2019\u00b4et\u00b4e de probabilit\u00b4es de Saint-Flour XXXVIII-2008. Springer Verlag, 2011b.  V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learn- In D. E. Gine and J.Wellner, editors, High Dimensional Probability, II, pages 443-457.  ing. Birkhauser, 1999.  V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. Annals of Statistics, 39:2302-2329, 2011. doi: 10.1214/ 11-AOS894.  S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the nystr&#246;m method. Journal  of Machine Learning Research, 13(1):981-1006, Apr. 2012.  P. Massart. Some applications of concentration inequalities to statistics. Ann. Fac. Sci. Toulouse  Math., 9(6):245-303, 2000.  2003.  S. Mendelson. On the performance of kernel classes. J. Mach. Learn. Res., 4:759-771, December  D. Pechyony. Theory and Practice of Transductive Learning. PhD thesis, Technion, 2008.  B. Recht and C. Re. Toward a noncommutative arithmetic-geometric mean inequality: Conjectures,  case-studies, and consequences. In COLT, 2012.  R. J. Ser\ufb02ing. Probability inequalities for the sum in sampling without replacement. The Annuals  of Statistics, 2(1):39-48, 1974.  I. Steinwart and A. Christmann. Support Vector Machines. Springer Publishing Company, Incorpo-  rated, 1st edition, 2008. ISBN 0387772413.  I. Steinwart, D. R. Hush, and C. Scovel. Learning from dependent observations. J. Multivariate  Analysis, 100(1):175-194, 2009.  M. Stone. Cross-validatory choice and assessment of statistical predictors (with discussion). Journal  of the Royal Statistical Society, B36:111-147, 1974.  M. Talagrand. New concentration inequalities in product spaces. Inventiones Mathematicae, 126,  J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  V. N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag New York,  1996.  Inc., 1982.  V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998. TOLSTIKHIN BLANCHARD KLOFT  "}, "On the Consistency of Output Code Based Learning Algorithms for Multiclass Learning Problems": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "On the Consistency of Output Code Based Learning Algorithms for Multiclass Learning Problems", "abstract": "A popular approach to solving multiclass learning problems is to reduce them to a set of binary classification problems through some output code matrix: the widely used one-vs-all and all-pairs methods, and the error-correcting output code methods of Dietterich and Bakiri (1995), can all be viewed as special cases of this approach. In this paper, we consider the question of statistical consistency of such methods. We focus on settings where the binary problems are solved by minimizing a binary surrogate loss, and derive general conditions on the binary surrogate loss under which the one-vs-all and all-pairs code matrices yield consistent algorithms with respect to the multiclass 0-1 loss. We then consider general multiclass learning problems defined by a general multiclass loss, and derive conditions on the output code matrix and binary surrogates under which the resulting algorithm is consistent with respect to the target multiclass loss. We also consider \\emphprobabilistic code matrices, where one reduces a multiclass problem to a set of \\emphclass probability labeled binary problems, and show that these can yield benefits in the sense of requiring a smaller number of binary problems to achieve overall consistency. Our analysis makes interesting connections with the theory of proper composite losses (Buja et al., 2005; Reid and Williamson, 2010); these play a role in constructing the right \u2018decoding\u2019 for converting the predictions on the binary problems to the final multiclass prediction. To our knowledge, this is the first work that comprehensively studies consistency properties of output code based methods for multiclass learning.", "pdf_url": "http://proceedings.mlr.press/v35/ramaswamy14.pdf", "keywords": ["Multiclass learning", "output codes", "consistency", "one-versus-all", "all-pairs", "error-correcting output codes", "proper composite losses"], "reference": "Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying  approach for margin classifiers. Journal of Machine Learning Research, 1:113-141, 2000.  Peter L. Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classification and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  David Buffoni, Cl\u00b4ement Calauz`enes, Patrick Gallinari, and Nicolas Usunier. Learning scoring func- tions with order-preserving losses and standardized supervision. In International Conference on Machine Learning, 2011.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation:  Structure and applications. Technical report, University of Pennsylvania, November 2005.  Cl\u00b4ement Calauz`enes, Nicolas Usunier, and Patrick Gallinari. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems 25, pages 197-205. 2012.  David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE Trans-  actions on Information Theory, 54(11):5140-5154, 2008.  Koby Crammer and Yoram Singer. On the learnability and design of output codes for multiclass  problems. Machine Learning, 47:201-233, 2002.  Ofer Dekel and Yoram Singer. Multiclass learning by probabilistic embeddings. In Neural Infor-  mation Processing Systems, 2002.  Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting  output codes. Journal of Artificial Intelligence Research, 2:263-286, 1995.  John Duchi, Lester Mackey, and Michael Jordan. On the consistency of ranking algorithms. In  International Conference on Machine Learning, 2010.  Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Conference on Learning  Theory, 2011.  Venkatesan Guruswami and Amit Sahai. Multiclass learning, boosting and error-correcting codes.  In Conference on Learning Theory, 1999.  Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. Statistical consistency of ranking meth- In Advances in Neural Information Processing  ods in a rank-differentiable probability space. Systems 25, pages 1241-1249. 2012.  John Langford and Alina Beygelzimer. Sensitive error correcting output codes. In Conference on  Learning Theory, 2005.  Yoonkyung Lee, Yi Lin, and Grace Wahba. Multicategory support vector machines: Theory and ap- plication to the classification of microarray data. Journal of the American Statistical Association, 99(465):67-81, 2004.  14   RAMASWAMY BABU AGARWAL WILLIAMSON  References  Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying  approach for margin classifiers. Journal of Machine Learning Research, 1:113-141, 2000.  Peter L. Bartlett, Michael Jordan, and Jon McAuliffe. Convexity, classification and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  David Buffoni, Cl\u00b4ement Calauz`enes, Patrick Gallinari, and Nicolas Usunier. Learning scoring func- tions with order-preserving losses and standardized supervision. In International Conference on Machine Learning, 2011.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation:  Structure and applications. Technical report, University of Pennsylvania, November 2005.  Cl\u00b4ement Calauz`enes, Nicolas Usunier, and Patrick Gallinari. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems 25, pages 197-205. 2012.  David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE Trans-  actions on Information Theory, 54(11):5140-5154, 2008.  Koby Crammer and Yoram Singer. On the learnability and design of output codes for multiclass  problems. Machine Learning, 47:201-233, 2002.  Ofer Dekel and Yoram Singer. Multiclass learning by probabilistic embeddings. In Neural Infor-  mation Processing Systems, 2002.  Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting  output codes. Journal of Artificial Intelligence Research, 2:263-286, 1995.  John Duchi, Lester Mackey, and Michael Jordan. On the consistency of ranking algorithms. In  International Conference on Machine Learning, 2010.  Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Conference on Learning  Theory, 2011.  Venkatesan Guruswami and Amit Sahai. Multiclass learning, boosting and error-correcting codes.  In Conference on Learning Theory, 1999.  Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Tie-Yan Liu. Statistical consistency of ranking meth- In Advances in Neural Information Processing  ods in a rank-differentiable probability space. Systems 25, pages 1241-1249. 2012.  John Langford and Alina Beygelzimer. Sensitive error correcting output codes. In Conference on  Learning Theory, 2005.  Yoonkyung Lee, Yi Lin, and Grace Wahba. Multicategory support vector machines: Theory and ap- plication to the classification of microarray data. Journal of the American Statistical Association, 99(465):67-81, 2004. CONSISTENCY OF OUTPUT CODE BASED LEARNING ALGORITHMS  Bernardo \u00b4A. Pires, Csaba Szepesvari, and Mohammad Ghavamzadeh. Cost-sensitive multiclass  classification risk bounds. In International Conference on Machine Learning, 2013.  Harish G. Ramaswamy and Shivani Agarwal. Classification calibration dimension for general mul- In Advances in Neural Information Processing Systems 25, pages 2087-2095.  ticlass losses. 2012.  Harish G. Ramaswamy, Shivani Agarwal, and Ambuj Tewari. Convex calibrated surrogates for low- rank loss matrices with applications to subset ranking losses. In Advances in Neural Information Processing Systems 26. 2013.  Pradeep Ravikumar, Ambuj Tewari, and Eunho Yang. On NDCG consistency of listwise ranking  methods. In International Conference on Artificial Intelligence and Statistics, 2011.  Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning  Research, 11:2387-2422, 2010.  Robert E. Schapire. Using output codes to boost multiclass learning problems. In International  Conference on Machine Learning, 1997.  Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated pre-  dictions. Machine Learning, 37(3):297-336, 1999.  Terrence J. Sejnowski and Charles R. Rosenberg. Parallel networks that learn to pronounce English  text. Complex Systems, 1:145-168, 1987.  Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approxima-  tion, 26:225-287, 2007.  Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classification methods. Journal  of Machine Learning Research, 8:1007-1025, 2007.  Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to  rank: Theory and algorithm. In International Conference on Machine Learning, 2008.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. Annals of Statistics, 32(1):56-134, 2004a.  Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Jour-  nal of Machine Learning Research, 5:1225-1251, 2004b.  "}, "Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results", "abstract": "The classical setting of community detection consists of networks exhibiting a clustered structure. To more accurately model real systems we consider a class of networks (i) whose edges may carry labels and (ii) which may lack a clustered structure. Specifically we assume that nodes possess latent attributes drawn from a general compact space and edges between two nodes are randomly generated and labeled according to some unknown distribution as a function of their latent attributes. Our goal is then to infer the edge label distributions from a partially observed network. We propose a computationally efficient spectral algorithm and show it allows for asymptotically correct inference when the average node degree could be as low as logarithmic in the total number of nodes. Conversely, if the average node degree is below a specific constant threshold, we show that no algorithm can achieve better inference than guessing without using the observations. As a byproduct of our analysis, we show that our model provides a general procedure to construct random graph models with a spectrum asymptotic to a pre-specified eigenvalue distribution such as a power-law distribution.", "pdf_url": "http://proceedings.mlr.press/v35/xu14.pdf", "keywords": ["Community Detection", "Stochastic Blockmodel", "Spectral Methods", "Galton-Watson Tree"], "reference": "Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed membership  stochastic blockmodels. Journal of Machine Learning Research, 9:1981-2014, 2008.  Edoardo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation of a graphon: Theory and consistent estimation. In Advances in Neural Information Processing Systems 26, pages 692-700, 2013.  13   EDGE LABEL INFERENCE IN GENERALIZED STOCHASTIC BLOCK MODELS  The attribute of root \u03c1 is first chosen uniformly at random from X . Then, for each child node, independently of everything else, it has the same attribute with its parent with probability 1 \u2212 (r \u2212 1)(cid:15)((cid:96)) and one of r \u2212 1 different attributes with probability (cid:15)((cid:96)), where  (cid:15)((cid:96)) =  b\u03bd((cid:96)) a\u00b5((cid:96)) + (r \u2212 1)b\u03bd((cid:96))  .  (19)  Recall that GR denote the neighborhood of \u03c1 in G within distance R and \u2202GR denote the nodes at the boundary of GR. Let TR denote the tree T up to depth R and \u2202TR denote the set of leaf nodes of TR. The following lemma similar to coupling lemmas in Mossel et al. (2012) and Lelarge et al. (2013) shows that GR can be coupled with the labeled Galton-Watson tree TR.  Lemma 6 Let R = \u03b8 log n for some small enough constant \u03b8 > 0, then there exists a coupling such that a.a.s. (GR, \u03c3GR) = (TR, \u03c3TR), where \u03c3GR denote the node attributes on the subgraph GR.  For the labeled Galton-Watson tree, we show that if \u03c9 < \u03c90, then the attributes of leaf nodes  are asymptotically independent with the attribute of root.  Lemma 7 Consider a labeled Galton-Waltson tree T with \u03c9 < \u03c90. Then as R \u2192 \u221e,  \u2200x \u2208 {1, . . . , r}, P(\u03c3\u03c1 = x|T , \u03c3\u2202TR) \u2192  a.a.s.  1 r  By exploiting Lemma 6 and Lemma 7, we give our proof of Theorem 3. By symmetry, P[\u03c3\u03c1 = x|G, \u03c3v = y] = P[\u03c3\u03c1 = x(cid:48)|G, \u03c3v = y] for x, x(cid:48) (cid:54)= y and x (cid:54)= x(cid:48). Therefore, we only need to show that P[\u03c3\u03c1 = y|G, \u03c3v = y] \u223c 1/r for any y \u2208 X and it further reduces to showing that  P[\u03c3\u03c1 = y|G, \u03c3v = y, \u03c3\u2202GR] \u223c 1/r.  (20)  Let R = \u03b8 log n be as in Lemma 6 such that GR = o( n) and thus v /\u2208 GR a.a.s.. Lemma 4.7 in Mossel et al. (2012) shows that \u03c3\u03c1 is asymptotically independent with \u03c3v conditional on \u03c3\u2202GR. Hence, P[\u03c3\u03c1 = y|G, \u03c3v = y, \u03c3\u2202GR] \u223c P[\u03c3\u03c1 = y|G, \u03c3\u2202GR]. Also, note that P(\u03c3\u03c1 = y|G, \u03c3\u2202GR) = P(\u03c3\u03c1 = y|GR, \u03c3\u2202GR). Lemma 6 implies that P(\u03c3\u03c1 = y|GR, \u03c3GR) \u223c P(\u03c3\u03c1 = y|TR, \u03c3\u2202TR), and by Lemma 7, P(\u03c3\u03c1 = y|TR, \u03c3\u2202TR) \u223c 1  r . Therefore, equation (20) holds.  \u221a  M.L acknowledges the support of the French Agence Nationale de la Recherche (ANR) under refer- ence ANR-11-JS02-005-01 (GAP project). J. X. acknowledges the support of NSF ECCS 10-28464.  Acknowledgments  References  Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed membership  stochastic blockmodels. Journal of Machine Learning Research, 9:1981-2014, 2008.  Edoardo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation of a graphon: Theory and consistent estimation. In Advances in Neural Information Processing Systems 26, pages 692-700, 2013. XU MASSOULI \u00b4E LELARGE  Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham Kakade. A tensor spectral approach to  learning mixed membership community models. In COLT, pages 867-881, 2013.  Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and Newman-Girvan  and other modularities. Proceedings of the National Academy of Sciences, 2009.  B\u00b4ela Bollob\u00b4as, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random  graphs. Random Struct. Algorithms, 31(1):3-122, August 2007.  Sourav Chatterjee. Matrix estimation by universal singular value thresholding. arxiv:1212.1247,  2012.  Kamalika Chaudhuri, Fan Chung Graham, and Alexander Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. Journal of Machine Learning Research, 23:35.1-35.23, 2012.  Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and subma- trix localization with a growing number of clusters and submatrices. arxiv:1402.1267, 2014.  Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Comb. Probab. Comput.,  19(2):227-284, 2010.  Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM Journal  on Numerical Analysis, 7(1):pp. 1-46, 1970.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physics Review E, 84:066106, 2011.  Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs. Random Struct.  Algorithms, 27(2):251-275, Sept. 2005.  Santo Fortunato. Community detection in graphs. arXiv:0906.0612, 2010.  Mark S. Handcock, Adrian E. Raftery, and Jeremy M. Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301- 354, 2007.  Simon Heimlicher, Marc Lelarge, and Laurent Massouli\u00b4e. Community detection in the labelled  stochastic block model. arXiv:1209.2910, 2012.  Peter D. Hoff, Adrian E. Raftery, and Mark S. Handcock. Latent space approaches to social network  analysis. Journal of the American Statistical Association, 97:1090+, December 2002.  Tosio Kato. Perturbation Theory for Linear Operators. Springer, Berlin, 1966.  Vladimir I. Koltchinskii. Asymptotics of spectral projections of some random matrices approximat-  ing integral operators. Progress in Probability, 1998.  Marc Lelarge, Laurent Massouli\u00b4e, and Jiaming Xu. Reconstruction in the labeled stochastic block  model. In Information Theory Workshop, Sept. 2013. EDGE LABEL INFERENCE IN GENERALIZED STOCHASTIC BLOCK MODELS  Laurent Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property. In STOC 2014: 46th Annual Symposium on the Theory of Computing, pages 1-10, United States, 2014.  Frank McSherry. Spectral partitioning of random graphs. In 42nd IEEE Symposium on Foundations  of Computer Science, pages 529 - 537, Oct. 2001.  Elchanan Mossel. Survey - information \ufb02ows on trees. DIMACS series in discrete mathematics and  theoretical computer science, pages 155-170, 2004.  Elchanan Mossel, Joe Neeman, and Allan Sly. Stochastic block models and reconstruction.  arXiv:1202.1499, 2012.  arxiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional stochastic  blockmodel. The Annals of Statistics, 39(4):1878-1915, 2011.  Dan-Cristian Tomozei and Laurent Massouli\u00b4e. Distributed user profiling via spectral methods. SIG-  METRICS Perform. Eval. Rev., 38(1):383-384, June 2010.  Ulrike von Luxburg, Olivier Bousquet, and Mikhail Belkin. On the convergence of spectral cluster-  ing on random samples: the normalized case. NIPS, 2005.  "}, "Lower Bounds on the Performance of Polynomial-time Algorithms for Sparse Linear Regression": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Lower Bounds on the Performance of Polynomial-time Algorithms for Sparse Linear Regression", "abstract": "Under a standard assumption in complexity theory (NP not in P/poly), we demonstrate a gap between the minimax prediction risk for sparse linear regression that can be achieved by polynomial-time algorithms, and that achieved by optimal algorithms.  In particular, when the design matrix is ill-conditioned, the minimax prediction loss achievable by polynomial-time algorithms can be substantially greater than that of an optimal algorithm.  This result is the first known gap between polynomial and optimal algorithms for sparse linear regression, and does not depend on conjectures in average-case complexity.", "pdf_url": "http://proceedings.mlr.press/v35/zhang14.pdf", "keywords": [], "reference": "A. A. Amini and M. J. Wainwright. High-dimensional analysis of semidefinite relaxations for sparse  principal component analysis. Annals of Statistics, 5:2877-2921, 2009.  Sanjeev Arora and Boaz Barak. Computational Complexity: A Modern Approach. Cambridge  University Press, 2009.  Q. Berthet and P. Rigollet. Optimal detection of sparse principal components in high dimensions.  Technical report, Princeton University, 2012. arxiv1202.5070.  Q. Berthet and P. Rigollet. Computational lower bounds for sparse PCA. Technical report, Princeton  University, April 2013. arxiv1304.0828.  Peter J Bickel, Ya\u2019Acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of Lasso and  Dantzig selector. Annals of Statistics, 37(4):1705-1732, 2009.  F. Bunea, A. Tsybakov, and M. Wegkamp. Aggregation for Gaussian regression. Annals of Statis-  tics, 35(4):1674-1697, 2007.  Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much  larger than n. Annals of Statistics, 35(6):2313-2351, 2007.  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse  problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis  pursuit. SIAM Journal on Scientific Computing, 20(1):33-61, 1998.  R. Foygel and N. Srebro. Fast rate and optimistic rate for (cid:96)1-regularized regression. Technical  report, Toyoto Technological Institute, 2011. arXiv:1108.037v1.  R. Krauthgamer, B. Nadler, and D. Vilenchik. Do semidefinite relaxations really solve sparse PCA?  Technical report, Weizmann Institute of Science, June 2013. arXiv:1306.3690v1.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. arXiv  preprint arXiv:1309.5914, 2013.  Nicolai Meinshausen and Bin Yu.  Lasso-type recovery of sparse representations for high-  dimensional data. Annals of Statistics, 37(1):246-270, 2009.  Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Com-  puting, 24(2):227-234, 1995.  S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high- dimensional analysis of M -estimators with decomposable regularizers. Statistical Science, 27 (4):538-557, 2012.  S. Oymak, A. Jalali, M. Fazel, Y. C. Eldar, and B. Hassibi. Simultaneously structured models with applications to sparse and low-rank matrices. Technical report, Caltech, 2012. arxiv1212.3753.  14   ZHANG WAINWRIGHT JORDAN  References  A. A. Amini and M. J. Wainwright. High-dimensional analysis of semidefinite relaxations for sparse  principal component analysis. Annals of Statistics, 5:2877-2921, 2009.  Sanjeev Arora and Boaz Barak. Computational Complexity: A Modern Approach. Cambridge  University Press, 2009.  Q. Berthet and P. Rigollet. Optimal detection of sparse principal components in high dimensions.  Technical report, Princeton University, 2012. arxiv1202.5070.  Q. Berthet and P. Rigollet. Computational lower bounds for sparse PCA. Technical report, Princeton  University, April 2013. arxiv1304.0828.  Peter J Bickel, Ya\u2019Acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of Lasso and  Dantzig selector. Annals of Statistics, 37(4):1705-1732, 2009.  F. Bunea, A. Tsybakov, and M. Wegkamp. Aggregation for Gaussian regression. Annals of Statis-  tics, 35(4):1674-1697, 2007.  Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much  larger than n. Annals of Statistics, 35(6):2313-2351, 2007.  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse  problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.  Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis  pursuit. SIAM Journal on Scientific Computing, 20(1):33-61, 1998.  R. Foygel and N. Srebro. Fast rate and optimistic rate for (cid:96)1-regularized regression. Technical  report, Toyoto Technological Institute, 2011. arXiv:1108.037v1.  R. Krauthgamer, B. Nadler, and D. Vilenchik. Do semidefinite relaxations really solve sparse PCA?  Technical report, Weizmann Institute of Science, June 2013. arXiv:1306.3690v1.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. arXiv  preprint arXiv:1309.5914, 2013.  Nicolai Meinshausen and Bin Yu.  Lasso-type recovery of sparse representations for high-  dimensional data. Annals of Statistics, 37(1):246-270, 2009.  Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Com-  puting, 24(2):227-234, 1995.  S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high- dimensional analysis of M -estimators with decomposable regularizers. Statistical Science, 27 (4):538-557, 2012.  S. Oymak, A. Jalali, M. Fazel, Y. C. Eldar, and B. Hassibi. Simultaneously structured models with applications to sparse and low-rank matrices. Technical report, Caltech, 2012. arxiv1212.3753. LOWER BOUNDS ON THE PERFORMANCE OF POLYNOMIAL-TIMEALGORITHMS FOR SPARSE LINEAR REGRESSION  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated  Gaussian designs. Journal of Machine Learning Research, 99:2241-2259, 2010.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high- dimensional linear regression over (cid:96)q-balls. IEEE Transactions on Information Theory, 57(10): 6976-6994, 2011.  Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu-  lar values. arXiv:1003.2990, 2010.  Society, Series B, 58:267-288, 1996.  Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical  Sara A Van De Geer and Peter B\u00a8uhlmann. On the conditions used to prove oracle results for the  Lasso. Electronic Journal of Statistics, 3:1360-1392, 2009. ZHANG WAINWRIGHT JORDAN  "}, "Follow the Leader with Dropout Perturbations": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Follow the Leader with Dropout Perturbations", "abstract": "We consider online prediction with expert advice. Over the course of many trials, the goal of the learning algorithm is to achieve small additional loss (i.e. regret) compared to the loss of the best from a set of K experts. The two most popular algorithms are Hedge/Weighted Majority and Follow the Perturbed Leader (FPL). The latter algorithm first perturbs the loss of each expert by independent additive noise drawn from a fixed distribution, and then predicts with the expert of minimum perturbed loss (\u201cthe leader\u201d) where ties are broken uniformly at random. To achieve the optimal worst-case regret as a function of the loss L^* of the best expert in hindsight, the two types of algorithms need to tune their learning rate or noise magnitude, respectively, as a function of L^*. Instead of perturbing the losses of the experts with additive noise, we randomly set them to 0 or 1 before selecting the leader. We show that our perturbations are an instance of dropout \u2014 because experts may be interpreted as features \u2014 although for non-binary losses the dropout probability needs to be made dependent on the losses to get good regret bounds. We show that this simple, tuning-free version of the FPL algorithm achieves two feats: optimal worst-case O(\\sqrtL^* \\ln K + \\ln K) regret as a function of L^*, and optimal O(\\ln K) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a gap between the expected loss of the best expert and all others. A number of recent algorithms from the Hedge family (AdaHedge and FlipFlop) also achieve this, but they employ sophisticated tuning regimes. The dropout perturbation of the losses of the experts result in different noise distributions for each expert (because they depend on the expert\u2019s total loss) and curiously enough no additional tuning is needed: the choice of dropout probability only affects the constants.", "pdf_url": "http://proceedings.mlr.press/v35/vanerven14.pdf", "keywords": ["Online learning", "regret bounds", "expert setting", "Follow the Leader", "Follow the Perturbed Leader", "Dropout"], "reference": "Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Neural Information Processing Systems (NIPS), pages 1-9, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64(1):48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds IEEE Transactions on Neural  for on-line prediction of linear functions by gradient descent. Networks, 7(2):604-619, May 1996.  Nicol`o Cesa-Bianchi, Yaov Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research. To appear., 2014.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Con-  ference on Learning Theory (COLT), pages 460-473, 2013.  13   DROPOUT PERTURBATIONS  4. Constant Regret on IID Losses  In this section we show that our BDP algorithm has optimal O(ln K) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a fixed gap \u03b3 between the expected loss of the best expert and all others.  Theorem 4.1 Let \u03b3 \u2208 (0, 1] and \u03b4 \u2208 (0, 1] be constants, and let k\u2217 be a fixed expert. Suppose the loss vectors (cid:96)t are independent random variables such that the expected differences in loss satisfy  Then, with probability at least 1 \u2212 \u03b4, the regret of the BDP algorithm is bounded by a constant:  E[(cid:96)t,k \u2212 (cid:96)t,k\u2217] \u2265 \u03b3  for all t.  min k(cid:54)=k\u2217  RT \u2264  8 (1 \u2212 \u03b1)2\u03b32 ln  8K (1 \u2212 \u03b1)2\u03b32\u03b4  + 3.  (4.1)  (4.2)  As discussed in the proof in Section B.1, it is possible to improve the dependence on \u03b3 at the  cost of getting a more complicated expression.  Tim van Erven was supported by NWO Rubicon grant 680-50-1112, Wojciech Kot\u0142owski by the Foundation for Polish Science under the Homing Plus Program, and Manfred K. Warmuth by NSF grant IIS-1118028.  Acknowledgments  References  Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries.  In  Neural Information Processing Systems (NIPS), pages 1-9, 2010.  Peter Auer, Nicol`o Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning  algorithms. Journal of Computer and System Sciences, 64(1):48-75, 2002.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University  Press, 2006.  Nicol`o Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds IEEE Transactions on Neural  for on-line prediction of linear functions by gradient descent. Networks, 7(2):604-619, May 1996.  Nicol`o Cesa-Bianchi, Yaov Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  Steven de Rooij, Tim van Erven, Peter D. Gr\u00a8unwald, and Wouter M. Koolen. Follow the leader if  you can, hedge if you must. Journal of Machine Learning Research. To appear., 2014.  Luc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Con-  ference on Learning Theory (COLT), pages 460-473, 2013. VAN ERVEN KOT\u0141OWSKI WARMUTH  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55:119-139, 1997.  James Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of  Games, 3:97-139, 1957.  Elad Hazan, Satyen Kale, and Manfred K. Warmuth. On-line variance minimization in O(n2) per  trial? In Conference on Learning Theory (COLT), pages 314-315, 2010.  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. CoRR,  dinov. abs/1207.0580, 2012.  Nie Jiazhong, Wojciech Kot\u0142owski, and Manfred K. Warmuth. On-line PCA with optimal regrets.  In Algorithmic Learning Theory (ALT), pages 98-112, 2013.  Adam Kalai. A perturbation that makes Follow the Leader equivalent to Randomized Weighted  Majority. Private communication, December 2005.  Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of  Computer and System Sciences, 71(3):291-307, 2005.  Jyrki Kivinen and Manfred K. Warmuth. Additive versus Exponentiated Gradient updates for linear  prediction. Information and Computation, 132(1):1-64, 1997.  Wouter M. Koolen and Manfred K. Warmuth. Hedging structured concepts. In 23rd Annual Con-  ference on Learning Theory - COLT 2010, pages 93-104. Omnipress, June 2010.  Dima Kuzmin and Manfred K. Warmuth. Optimum follow the leader algorithm. In Conference on  Learning Theory (COLT), pages 684-686, 2005. Open problem.  Nick Littlestone and Manfred K. Warmuth. The Weighted Majority algorithm. Information and  Computation, 108(2):212-261, 1994.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Ma-  chine Learning Research, 4:773-818, 2003.  Tim van Erven, Peter D. Gr\u00a8unwald, Wouter Koolen, and Steven de Rooij. Adaptive hedge.  In  Neural Information Processing Systems (NIPS), pages 1656-1664, 2011.  Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences,  56(2):153-173, 1998.  Stefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Neural  Information Processing Systems (NIPS), pages 351-359, 2013.  Sida I. Wang and Christopher D. Manning. Fast dropout training. In International Conference on  Machine Learning (ICML), pages 118-126, 2013. DROPOUT PERTURBATIONS  Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. Machine Learning, 87(1):  1-32, 2011.  Additional Material for \u201cFollow the Leader with Dropout Perturbations\u201d  "}, "Lipschitz Bandits: Regret Lower Bound and Optimal Algorithms": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Lipschitz Bandits: Regret Lower Bound and Optimal Algorithms", "abstract": "We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove  that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.", "pdf_url": "http://proceedings.mlr.press/v35/magureanu14.pdf", "keywords": [], "reference": "1926-1951, November 1995.  R. Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33(6):  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Bubeck, R. Munos, G. Stoltz, and C Szepesv\u00b4ari. Online optimization in x-armed bandits. In  Advances in Neural Information Processing Systems 22, 2008.  O. Capp\u00b4e, A. Garivier, O. Maillard, R. Munos, and G. Stoltz. Kullback-leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):516-541, June 2013.  R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In  Proc. of ICML, 2014a.  R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.  Technical Report, people.kth.se/\u02dcalepro/pdf/tr-icml2014.pdf, 2014b.  V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In  Proc. of Conference On Learning Theory (COLT), pages 355-366, 2008.  A. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA), pages 385-394, 2005.  A. Garivier.  Informational confidence bounds for self-normalized averages and applications.  In  Information Theory Workshop, 2013.  A. Garivier and O. Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and beyond. In  Proc. of Conference On Learning Theory (COLT), 2011.  T. L. Graves and T. L. Lai. Asymptotically efficient adaptive choice of control laws in controlled  markov chains. SIAM J. Control and Optimization, 35(3):715-743, 1997.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Proc. of the confer-  ence on Neural Information Processing Systems (NIPS), 2004.  R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proc. of the 40th  annual ACM Symposium on Theory of Computing (STOC), pages 681-690, 2008.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statis-  tics, 15(3):1091-1114, 09 1987.  Mathematics, 6(1):4-2, 1985.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  A. M\u00a8uller and D. Stoyan. Comparison Methods for Stochastic Models and Risks. Wiley, 2002.  A. Slivkins. Contextual bandits with similarity information. In Proc. of Conference On Learning  Theory (COLT), pages 679-702, 2011.  13   LIPSCHITZ BANDITS  References  1926-1951, November 1995.  R. Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33(6):  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of the multiarmed bandit problem.  Machine Learning, 47(2-3):235-256, 2002.  S. Bubeck, R. Munos, G. Stoltz, and C Szepesv\u00b4ari. Online optimization in x-armed bandits. In  Advances in Neural Information Processing Systems 22, 2008.  O. Capp\u00b4e, A. Garivier, O. Maillard, R. Munos, and G. Stoltz. Kullback-leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):516-541, June 2013.  R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In  Proc. of ICML, 2014a.  R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.  Technical Report, people.kth.se/\u02dcalepro/pdf/tr-icml2014.pdf, 2014b.  V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In  Proc. of Conference On Learning Theory (COLT), pages 355-366, 2008.  A. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA), pages 385-394, 2005.  A. Garivier.  Informational confidence bounds for self-normalized averages and applications.  In  Information Theory Workshop, 2013.  A. Garivier and O. Capp\u00b4e. The KL-UCB algorithm for bounded stochastic bandits and beyond. In  Proc. of Conference On Learning Theory (COLT), 2011.  T. L. Graves and T. L. Lai. Asymptotically efficient adaptive choice of control laws in controlled  markov chains. SIAM J. Control and Optimization, 35(3):715-743, 1997.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Proc. of the confer-  ence on Neural Information Processing Systems (NIPS), 2004.  R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proc. of the 40th  annual ACM Symposium on Theory of Computing (STOC), pages 681-690, 2008.  T. L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statis-  tics, 15(3):1091-1114, 09 1987.  Mathematics, 6(1):4-2, 1985.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  A. M\u00a8uller and D. Stoyan. Comparison Methods for Stochastic Models and Risks. Wiley, 2002.  A. Slivkins. Contextual bandits with similarity information. In Proc. of Conference On Learning  Theory (COLT), pages 679-702, 2011. MAGUREANU COMBES PROUTIERE  R. Wets. On the continuity of the value of a linear program and of related polyhedral-valued multi-  functions. Mathematical Programming Study, 1985.  "}, "Sample Complexity Bounds on Differentially Private Learning via Communication Complexity": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Sample Complexity Bounds on Differentially Private Learning via Communication Complexity", "abstract": "In this work we analyze the sample complexity of classification by differentially private algorithms. Differential privacy is a strong and well-studied notion of privacy introduced by Dwork et al. (2006) that ensures that the output of an algorithm leaks little information about the data point provided by any of the participating individuals. Sample complexity of private PAC and agnostic learning was studied in a number of prior works starting with (Kasiviswanathan et al., 2008) but a number of basic questions still remain open (Beimel et al. 2010; Chaudhuri and Hsu, 2011; Beimel et al., 2013a,b). Our main contribution is an equivalence between the sample complexity of differentially-private learning of a concept class C (or \\mathrmSCDP(C)) and the randomized one-way communication complexity of the evaluation problem for concepts from C. Using this equivalence we prove the following bounds: \\beginitemize \\item \\mathrmSCDP(C) = \u03a9(\\mathrmLDim(C)), where \\mathrmLDim(C) is the Littlestone\u2019s (1987) dimension characterizing the number of mistakes in the online-mistake-bound learning model. This result implies that \\mathrmSCDP(C) is different from the VC-dimension of C, resolving one of the main open questions from prior work. \\item For any t, there exists a class C such that \\mathrmLDim(C)=2 but \\mathrmSCDP(C) \u2265t. \\item For any t, there exists a class C such that the sample complexity of (pure) \u03b1-differentially private PAC learning is \u03a9(t/\u03b1) but the sample complexity of the relaxed (\u03b1,\u03b2)-differentially private PAC learning is O(\\log(1/\u03b2)/\u03b1). This resolves an open problem from (Beimel et al., 2013b). \\enditemize We also obtain simpler proofs for a number of known related results. Our equivalence builds on a characterization of sample complexity by Beimel et al., (2013a) and our bounds rely on a number of known results from communication complexity.", "pdf_url": "http://proceedings.mlr.press/v35/feldman14b.pdf", "keywords": [], "reference": "Scott Aaronson. Limitations of quantum advice and one-way communication. In IEEE Conference  on Computational Complexity, pages 320-332, 2004.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  Khanh Do Ba, Piotr Indyk, Eric Price, and David P. Woodruff. Lower bounds for sparse recovery.  In SODA, pages 1190-1197, 2010.  13   SAMPLE COMPLEXITY BOUNDS ON DIFFERENTIALLY PRIVATE LEARNING VIA COMMUNICATION COMPLEXITY  1. We see two positively labeled points and can recover the hidden concept.  2. We see only one positively labeled point, in which case we can safely output just a point  function that is positive on this point.  3. We see no positively labeled points, in which case we can safely output the all zero function.  We define a \u201cbasic learner\u201d that takes O(1) samples and outputs a concept according to the above rule.  If indeed we are in one of the above cases, we can then use the \u201cpropose-test-release\u201d paradigm (Dwork and Lei, 2009) to release the hidden concept: we run the learner many times and hope that in almost every execution it will output the exact same hypothesis. If this is the case we can release this unique hypothesis as follows: compute the number of samples that must be modified in order to change the majority hypothesis, add noise to make this number differentially private, and if it exceeds some appropriate threshold output the hypothesis, otherwise output the constant zero hypothesis.  There is a technical detail to overcome: it may be the case that the input distribution does not fall into any of the above cases, but lands \u201cbetween\u201d two of them, in which case the basic learner will oscillate between, say, outputting a line or outputting a point function. To handle this case, we randomize the number of samples we feed to the basic learner, and show that with high probability we pick a number such that we land firmly in one of the three good cases.  Finally, randomizing the number of samples leads to constant sample complexity but the depen- dence on confidence \u03b4 ends up being bad. We boost the confidence by running the poor-sample- complexity learner many times and sampling a single output using the exponential mechanism of McSherry and Talwar (2007).  Acknowledgements  We are grateful to Kobbi Nissim for first drawing our attention to the intriguing problem of under- standing the relationship between probabilistic representation dimension and VC dimension, and for valuable discussions regarding the sample complexity of privately learning threshold functions. We thank Nina Balcan and Avrim Blum who brought up the relationship of our bounds for intervals in Section 3.1 to those based on Littlestone\u2019s dimension. Their insightful comments and questions have lead to our result in Theorem 2. We also thank Sasha Rakhlin and Sasha Sherstov for useful suggestions and references.  D.X. was supported in part by the French ANR Blanc program under contract ANR-12-BS02- 005 (RDAM project), by NSF grant CNS-1237235, a gift from Google, Inc., and a Simons Investi- gator grant to Salil Vadhan.  References  Scott Aaronson. Limitations of quantum advice and one-way communication. In IEEE Conference  on Computational Complexity, pages 320-332, 2004.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  Khanh Do Ba, Piotr Indyk, Eric Price, and David P. Woodruff. Lower bounds for sparse recovery.  In SODA, pages 1190-1197, 2010. FELDMAN XIAO  Ziv Bar-Yossef, T. S. Jayram, Robert Krauthgamer, and Ravi Kumar. The sketching complexity of  pattern matching. In APPROX-RANDOM, pages 261-272, 2004.  Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity  for private learning and private data release. In TCC, pages 437-454, 2010.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private  learners. In ITCS, pages 97-110, 2013a.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approx-  imate differential privacy. In APPROX-RANDOM, pages 363-378, 2013b.  Gyora M. Benedek and Alon Itai. Learnability with respect to fixed distributions. Theoretical  Computer Science, 86(2):377 - 389, 1991.  A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SuLQ framework.  In  Proceedings of PODS, pages 128-138, 2005.  Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to noninteractive database  privacy. J. ACM, 60(2):12, 2013.  In COLT, pages 155-186, 2011.  tion. In ICML, 2012.  Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning.  Kamalika Chaudhuri and Daniel Hsu. Convergence rates for differentially private statistical estima-  John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local privacy and statistical minimax  rates. In FOCS, pages 429-438, 2013a.  John C. Duchi, Martin J. Wainwright, and Michael I. Jordan. Local privacy and minimax bounds:  Sharp rates for probability estimation. In NIPS, pages 1529-1537, 2013b.  C. Dwork and A. Roth. The Algorithmic Foundations of Differential Privacy (preprint). 2014.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  Cynthia Dwork. Differential privacy. In ICALP (2), pages 1-12, 2006.  Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC, pages 371-380,  2009.  Cynthia Dwork and Adam Smith. Differential privacy for statistics: What we know and what we  want to learn. Journal of Privacy and Confidentiality, 1(2):135-154, 2009.  Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via  communication complexity. CoRR, abs/1402.6278, 2014.  A. Gupta, M. Hardt, A. Roth, and J. Ullman. Privately releasing conjunctions and the statistical  query barrier. In STOC, pages 803-812, 2011. SAMPLE COMPLEXITY BOUNDS ON DIFFERENTIALLY PRIVATE LEARNING VIA COMMUNICATION COMPLEXITY  D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning  applications. Information and Computation, 100(1):78-150, 1992. ISSN 0890-5401.  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, June 2011.  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  M. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine Learning, 17  983-1006, 1998.  (2-3):115-141, 1994.  Ilan Kremer, Noam Nisan, and Dana Ron. On randomized one-round communication complexity.  Computational Complexity, 8(1):21-49, 1999.  N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algo-  rithm. Machine Learning, 2:285-318, 1987.  W. Maass and G. Turan. How fast can a threshold gate learn?, pages 381-414. MIT Press, 1994.  Wolfgang Maass and Gy\u00a8orgy Tur\u00b4an. Algorithms and lower bounds for on-line learning of geomet-  rical concepts. Machine Learning, 14(1):251-269, 1994.  Andrew McGregor, Ilya Mironov, Toniann Pitassi, Omer Reingold, Kunal Talwar, and Salil P. Vad-  han. The limits of two-party differential privacy. In FOCS, pages 81-90, 2010.  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS, pages  94-103, 2007.  Peter Bro Miltersen, Noam Nisan, Shmuel Safra, and Avi Wigderson. On data structures and asym-  metric communication complexity. J. Comput. Syst. Sci., 57(1):37-49, 1998.  S. Muroga. Threshold logic and its applications. Wiley-Interscience, New York, 1971.  Ashwin Nayak. Optimal lower bounds for quantum automata and random access codes. In FOCS,  Ilan Newman. Private vs. common random bits in communication complexity. Inf. Process. Lett.,  Anand D. Sarwate and Kamalika Chaudhuri. Signal processing and machine learning with differ- ential privacy: Algorithms and challenges for continuous data. IEEE Signal Process. Mag., 30 (5):86-94, 2013.  Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In STOC,  Jonathan Ullman. Answering n2+O(1) counting queries with differential privacy is hard. In STOC,  pages 369-377, 1999.  39(2):67-71, 1991.  pages 813-822, 2011.  pages 361-370, 2013.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. FELDMAN XIAO  Andrew Yao. Probabilistic computations: Toward a unified measure of complexity. In Proceedings  of the 18th Annual Symposium on Foundations of Computer Science, pages 222-227, 1977.  Shengyu Zhang. On the power of lower bound methods for one-way quantum communication  complexity. In ICALP (1), pages 49-60, 2011.  "}, "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations", "abstract": "We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained.  We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries.  Moreover, using our tools, we develop an algorithm that provides a regret bound of O(U \\sqrtT \\log( U \\sqrtT \\log^2 T +1)), where U is the L_2 norm of an arbitrary comparator and both T and U are unknown to the player. This bound is optimal up to \\sqrt\\log \\log T terms.  When T is known, we derive an algorithm with an optimal regret bound (up to constant factors).  For both the known and unknown T case, a Normal approximation to the conditional value of the game proves to be the key analysis tool.", "pdf_url": "http://proceedings.mlr.press/v35/mcmahan14.pdf", "keywords": ["Online learning", "minimax analysis", "online convex optimization"], "reference": "J. Abernethy and M.K. Warmuth. Repeated games against budgeted adversaries. Advances in Neural Infor-  mation Processing Systems, 22, 2010.  J. Abernethy, J. Langford, and M. K. Warmuth. Continuous experts and the Binning algorithm.  In Pro- ceedings of the 19th Annual Conference on Learning Theory (COLT06), pages 544-558. Springer, June 2007.  J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for  online convex games. In COLT, 2008a.  J. Abernethy, M. K. Warmuth, and J. Yellin. Optimal strategies from random walks. In Proceedings of the  21st Annual Conference on Learning Theory (COLT 08), pages 437-445, July 2008b.  J. V. Baxley. Euler\u2019s constant, Taylor\u2019s formula, and slowly converging series. Mathematics Magazine, 65  (5):302-313, 1992.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.  K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 297-305. 2009.  C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265-299, 2003.  A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for linear discriminant updates.  Machine Learning, 43(3):173-210, 2001.  J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Machine  Learning, 45(3):301-329, 2001.  NIPS, 2013.  H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In  F. Orabona. Dimension-free exponentiated gradient. In NIPS, 2013.  F. Orabona, K. Crammer, and N. Cesa-Bianchi. A generalized online mirror descent with applications to  classification and regression, 2013. arXiv:1304.2994.  A. Rakhlin. Lecture notes on online learning. Technical report, 2009.  A. Rakhlin, O. Shamir, and K. Sridharan. Localization and adaptation in online learning. In AISTATS, 2013.  S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms. In NIPS, 2012.  S. Shalev-Shwartz. Online learning: Theory, algorithms, and applications. Technical report, The Hebrew  University, 2007. PhD thesis.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine  Learning, 2012.  NIPS, 2012.  M. Streeter and H. B. McMahan. No-regret algorithms for unconstrained online convex optimization. In  L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In NIPS, 2009.  14   MCMAHAN ORABONA  We thank Jacob Abernethy for many useful conversations about this work.  Acknowledgments  References  J. Abernethy and M.K. Warmuth. Repeated games against budgeted adversaries. Advances in Neural Infor-  mation Processing Systems, 22, 2010.  J. Abernethy, J. Langford, and M. K. Warmuth. Continuous experts and the Binning algorithm.  In Pro- ceedings of the 19th Annual Conference on Learning Theory (COLT06), pages 544-558. Springer, June 2007.  J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for  online convex games. In COLT, 2008a.  J. Abernethy, M. K. Warmuth, and J. Yellin. Optimal strategies from random walks. In Proceedings of the  21st Annual Conference on Learning Theory (COLT 08), pages 437-445, July 2008b.  J. V. Baxley. Euler\u2019s constant, Taylor\u2019s formula, and slowly converging series. Mathematics Magazine, 65  (5):302-313, 1992.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.  K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 297-305. 2009.  C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265-299, 2003.  A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for linear discriminant updates.  Machine Learning, 43(3):173-210, 2001.  J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Machine  Learning, 45(3):301-329, 2001.  NIPS, 2013.  H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In  F. Orabona. Dimension-free exponentiated gradient. In NIPS, 2013.  F. Orabona, K. Crammer, and N. Cesa-Bianchi. A generalized online mirror descent with applications to  classification and regression, 2013. arXiv:1304.2994.  A. Rakhlin. Lecture notes on online learning. Technical report, 2009.  A. Rakhlin, O. Shamir, and K. Sridharan. Localization and adaptation in online learning. In AISTATS, 2013.  S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms. In NIPS, 2012.  S. Shalev-Shwartz. Online learning: Theory, algorithms, and applications. Technical report, The Hebrew  University, 2007. PhD thesis.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine  Learning, 2012.  NIPS, 2012.  M. Streeter and H. B. McMahan. No-regret algorithms for unconstrained online convex optimization. In  L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In NIPS, 2009. UNCONSTRAINED ONLINE LINEAR LEARNING IN HILBERT SPACES  "}, "Principal Component Analysis and Higher Correlations for Distributed Data": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Principal Component Analysis and Higher Correlations for Distributed Data", "abstract": "We consider algorithmic problems in the setting in which the input data has been partitioned arbitrarily on many servers. The goal is to compute a function of all the data, and the bottleneck is the communication used by the algorithm. We present algorithms for two illustrative problems on massive data sets: (1) computing a low-rank approximation of a matrix A=A^1 + A^2 + \\ldots + A^s, with matrix A^t stored on server t and (2) computing a function of a vector a_1 + a_2 + \\ldots + a_s, where server t has the vector a_t; this includes the well-studied special case of computing frequency moments and separable functions, as well as higher-order correlations such as the number of subgraphs of a specified type occurring in a graph. For both problems we give algorithms with nearly optimal communication, and in particular the only dependence on n, the size of the data, is in the number of bits needed to represent indices and words (O(\\log n)).", "pdf_url": "http://proceedings.mlr.press/v35/kannan14.pdf", "keywords": [], "reference": "Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary  coins. J. Comput. Syst. Sci., 66(4):671-687, 2003.  Nir Ailon and Bernard Chazelle. The fast johnson-lindenstrauss transform and approximate nearest  neighbors. SIAM J. Comput., 39(1):302-322, 2009.  Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the fre-  quency moments. J. Comput. Syst. Sci., 58(1):137-147, 1999.  Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast random sampling algorithm for sparsifying  matrices. In APPROX-RANDOM, pages 272-279, 2006.  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and  random projection. In FOCS, pages 616-623, 1999.  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and  random projection. Machine Learning, 63(2):161-182, 2006.  Haim Avron, Huy L. Nguyen, and David P. Woodruff. Subspace embeddings for the polynomial  kernel, 2014.  Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, and David P. Woodruff. Fast and  communication efficient algorithms for distributed pca. Manuscript, 2014.  Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statistics approach  to data stream and communication complexity. J. Comput. Syst. Sci., 68(4):702-732, 2004.  Christos Boutsidis and David P. Woodruff. Optimal cur matrix factorizations. In STOC, 2014.  Mark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, and Vinod Vaikuntanathan. Tight  bounds for set disjointness in the message passing model. CoRR, abs/1305.4696, 2013.  13   ALGORITHMS FOR DISTRIBUTED DATA  This is \u02dcO(sk\u22121/\u03b53) for k \u2265 4.  We have given the proof already of all assertions except the number of rounds. For the number of rounds, the most crucial point is that though the algorithm as stated requires O((ln s)c) rounds, we can instead deal with all \u03b2 simultaneously. CP just picks the T for all of them at once and sends them accross. Also, we just make sure that CP communicates all choices of t for each i all in one round. Also, note that the sk\u22122 sampling and checking if the first \u02dcA > \u2126(sB) can all be done in O(1) rounds, so also the sk\u22121 sampling. Then the crude estimation of \u02dc\u03c1i can be done in one O(1) rounds followed by the finer sampling in O(1) rounds.  Acknowledgements. We are grateful to Dick Karp and Jelani Nelson for helpful discussions, as well as the Simons Institute at Berkeley. Santosh Vempala was supported in part by NSF award CCF-1217793 and David Woodruff by the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750- 12-C0323.  References  Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary  coins. J. Comput. Syst. Sci., 66(4):671-687, 2003.  Nir Ailon and Bernard Chazelle. The fast johnson-lindenstrauss transform and approximate nearest  neighbors. SIAM J. Comput., 39(1):302-322, 2009.  Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the fre-  quency moments. J. Comput. Syst. Sci., 58(1):137-147, 1999.  Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast random sampling algorithm for sparsifying  matrices. In APPROX-RANDOM, pages 272-279, 2006.  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and  random projection. In FOCS, pages 616-623, 1999.  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and  random projection. Machine Learning, 63(2):161-182, 2006.  Haim Avron, Huy L. Nguyen, and David P. Woodruff. Subspace embeddings for the polynomial  kernel, 2014.  Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, and David P. Woodruff. Fast and  communication efficient algorithms for distributed pca. Manuscript, 2014.  Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statistics approach  to data stream and communication complexity. J. Comput. Syst. Sci., 68(4):702-732, 2004.  Christos Boutsidis and David P. Woodruff. Optimal cur matrix factorizations. In STOC, 2014.  Mark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, and Vinod Vaikuntanathan. Tight  bounds for set disjointness in the message passing model. CoRR, abs/1305.4696, 2013. KANNAN VEMPALA WOODRUFF  Amit Chakrabarti, Subhash Khot, and Xiaodong Sun. Near-optimal lower bounds on the multi- In IEEE Conference on Computational  party communication complexity of set disjointness. Complexity, pages 107-117, 2003.  Kenneth L. Clarkson and David P. Woodruff. Numerical linear algebra in the streaming model. In  STOC, pages 205-214, 2009.  sparsity time. STOC, 2013.  Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input  Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs. Random Struct.  Algorithms, 27(2):251-275, 2005.  Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size  coresets for k-means, pca and projective clustering. In SODA, pages 1434-1453, 2013.  Mina Ghashami and Jeff M. Phillips. Relative errors for deterministic low-rank matrix approxima-  tions. CoRR, abs/1307.7454, 2013.  Andre Gronemeier. Asymptotically optimal lower bounds on the nih-multi-party information com-  plexity of the and-function and disjointness. In STACS, pages 505-516, 2009.  T. S. Jayram. Hellinger strikes back: A note on the multi-party information complexity of and. In  APPROX-RANDOM, pages 562-573, 2009.  Daniel M. Kane, Raghu Meka, and Jelani Nelson. Almost optimal explicit johnson-lindenstrauss  families. In APPROX-RANDOM, pages 628-639, 2011.  Yingyu Liang, Maria-Florina Balcan, and Vandana Kanchanapally. Distributed pca and k-means  clustering. In The Big Learning Workshop at NIPS, 2013.  Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity  time and applications to robust linear regression. In STOC, pages 91-100, 2013.  Jelani Nelson and Huy L. Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser  subspace embeddings. CoRR, abs/1211.1002, 2012.  Jeff M. Phillips, Elad Verbin, and Qin Zhang. Lower bounds for number-in-hand multiparty com-  munication complexity, made easy. In SODA, pages 486-501, 2012.  David P. Woodruff and Qin Zhang. Tight bounds for distributed functional monitoring. In STOC,  pages 941-960, 2012.  4. "}, "Compressed Counting Meets Compressed Sensing": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Compressed Counting Meets Compressed Sensing", "abstract": "Compressed sensing (sparse signal recovery) has been a popular and important research topic in recent years. By observing that natural signals  (e.g., images or network data) are often nonnegative, we propose a framework for nonnegative signal recovery using \\em Compressed Counting (CC). CC is a technique built on  \\em maximally-skewed \u03b1-stable random projections originally developed for data stream computations (e.g., entropy estimations).  Our recovery procedure is computationally efficient in that it requires only one linear scan of the coordinates. In our settings, the signal \\mathbfx\u2208\\mathbbR^N is assumed to be nonnegative, i.e., x_i\u22650, \u2200i. We prove that, when \u03b1\u2208(0, 0.5], it suffices to use  M=(C_\u03b1+o(1)) \u03b5^-\u03b1 \\left(\\sum_i=1^N x_i^\u03b1\\right)\\log N/\u03b4measurements so that, with probability 1-\u03b4, all coordinates will be recovered within \u03b5additive precision, in one scan of the coordinates. The constant C_\u03b1=1 when \u03b1\\rightarrow0 and C_\u03b1=\\pi/2 when \u03b1=0.5. In particular, when \u03b1\\rightarrow0, the required number of measurements is essentially M=K\\log N/\u03b4, where K = \\sum_i=1^N 1{x_i\u22600} is the number of nonzero coordinates of the signal.", "pdf_url": "http://proceedings.mlr.press/v35/li14.pdf", "keywords": [], "reference": "Emmanuel Cand`es and Justin Romberg. l1-magic: Reocvery of sparse signals via convex program-  ming. Technical report, Calinfornia Institute of Technology, 2005.  Emmanuel Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52 (2):489-509, 2006.  John M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random  variables. Journal of the American Statistical Association, 71(354):340-344, 1976.  Scott Shaobing Chen, David L. Donoho, Michael, and A. Saunders. Atomic decomposition by basis  pursuit. SIAM Journal on Scientific Computing, 20:33-61, 1998.  Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch  and its applications. Journal of Algorithm, 55(1):58-75, 2005.  Daivd L. Donoho and Xiaoming Huo. Uncertainty principles and ideal atomic decomposition.  Information Theory, IEEE Transactions on, 40(7):2845-2862, nov. 2001.  David L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289-1306, 2006.  David L. Donoho and Philip B. Stark. Uncertainty principles and signal recovery. SIAM Journal of  Applied Mathematics, 49(3):906-931, 1989.  David L. Donoho and Jared Tanner. Counting faces of randomly projected polytopes when the projection radically lowers dimension. Journal of the American Mathematical Society, 22(1), jan. 2009.  Laura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical ap- proaches to DDoS attack detection and response. In DARPA Information Survivability Conference and Exposition, pages 303-314, 2003.  William Feller. An Introduction to Probability Theory and Its Applications (Volume II). John Wiley  & Sons, New York, NY, second edition, 1971.  A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proc. of the IEEE, 98(6):937 -947,  june 2010.  Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream compu-  tation. Journal of ACM, 53(3):307-323, 2006.  Ping Li. Very sparse stable random projections for dimension reduction in l\u03b1 (0 < \u03b1  2) norm. In  \u2264  KDD, San Jose, CA, 2007.  Ping Li. Estimators and tail bounds for dimension reduction in l\u03b1 (0 < \u03b1  2) using stable random  projections. In SODA, pages 10 - 19, San Francisco, CA, 2008.  \u2264  Ping Li. Compressed counting. In SODA, New York, NY, 2009a.  Ping Li. Improving compressed counting. In UAI, Montreal, CA, 2009b.  13   COMPRESSED COUNTING MEETS COMPRESSED SENSING  References  Emmanuel Cand`es and Justin Romberg. l1-magic: Reocvery of sparse signals via convex program-  ming. Technical report, Calinfornia Institute of Technology, 2005.  Emmanuel Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52 (2):489-509, 2006.  John M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random  variables. Journal of the American Statistical Association, 71(354):340-344, 1976.  Scott Shaobing Chen, David L. Donoho, Michael, and A. Saunders. Atomic decomposition by basis  pursuit. SIAM Journal on Scientific Computing, 20:33-61, 1998.  Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch  and its applications. Journal of Algorithm, 55(1):58-75, 2005.  Daivd L. Donoho and Xiaoming Huo. Uncertainty principles and ideal atomic decomposition.  Information Theory, IEEE Transactions on, 40(7):2845-2862, nov. 2001.  David L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289-1306, 2006.  David L. Donoho and Philip B. Stark. Uncertainty principles and signal recovery. SIAM Journal of  Applied Mathematics, 49(3):906-931, 1989.  David L. Donoho and Jared Tanner. Counting faces of randomly projected polytopes when the projection radically lowers dimension. Journal of the American Mathematical Society, 22(1), jan. 2009.  Laura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical ap- proaches to DDoS attack detection and response. In DARPA Information Survivability Conference and Exposition, pages 303-314, 2003.  William Feller. An Introduction to Probability Theory and Its Applications (Volume II). John Wiley  & Sons, New York, NY, second edition, 1971.  A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proc. of the IEEE, 98(6):937 -947,  june 2010.  Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream compu-  tation. Journal of ACM, 53(3):307-323, 2006.  Ping Li. Very sparse stable random projections for dimension reduction in l\u03b1 (0 < \u03b1  2) norm. In  \u2264  KDD, San Jose, CA, 2007.  Ping Li. Estimators and tail bounds for dimension reduction in l\u03b1 (0 < \u03b1  2) using stable random  projections. In SODA, pages 10 - 19, San Francisco, CA, 2008.  \u2264  Ping Li. Compressed counting. In SODA, New York, NY, 2009a.  Ping Li. Improving compressed counting. In UAI, Montreal, CA, 2009b. LI ZHANG ZHANG  2013.  Technical report, 2013.  Globecom, 2012.  Ping Li and Cun-Hui Zhang. A new algorithm for compressed counting with applications in shannon  entropy estimation in dynamic data. In COLT, 2011.  Ping Li and Cun-Hui Zhang. Entropy estimations using correlated symmetric stable random pro-  jections. In NIPS, Lake Tahoe, NV, 2012.  Ping Li and Cun-Hui Zhang. Exact sparse recovery with L0 projections. In KDD, pages 302-310,  Ping Li, Cun-Hui Zhang, and Tong Zhang. Sparse recovery with very sparse compressed counting.  Tsung-Han Lin and H. T. Kung. Compressive sensing medium access control for wireless lans. In  S.G. Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. Signal Pro-  cessing, IEEE Transactions on, 41(12):3397 -3415, 1993.  S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoret-  ical Computer Science, 1:117-236, 2 2005.  Noam Nisan. Pseudorandom generators for space-bounded computations. In Proceedings of the twenty-second annual ACM symposium on Theory of computing, STOC, pages 204-212, 1990.  Y.C. Pati, R. Rezaiifar, and P. S. Krishnaprasad. Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. In Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, pages 40-44 vol.1, Nov 1993.  J.A. Tropp. Greed is good: algorithmic results for sparse approximation. Information Theory, IEEE  Transactions on, 50(10):2231 - 2242, oct. 2004.  Ewout van den Berg and Michael P. Friedlander. Probing the pareto frontier for basis pursuit solu-  tions. SIAM J. Sci. Comput., 31(2):890-912, 2008.  Jun Wang, Haitham Hassanieh, Dina Katabi, and Piotr Indyk. Efficient and reliable low-power  backscatter networks. In SIGCOMM, pages 61-72, Helsinki, Finland, 2012a.  Meng Wang, Weiyu Xu, Enrique Mallada, and Ao Tang. Sparse recovery with graph constraints:  Fundamental limits and measurement construction. In Infomcom, 2012b.  Tong Zhang. Sparse recovery with orthogonal matching pursuit under rip.  Information Theory,  IEEE Transactions on, 57(9):6215 -6221, sept. 2011.  Haiquan (Chuck) Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and Jun Xu. A data streaming algorithm for estimating entropies of od \ufb02ows. In IMC, San Diego, CA, 2007.  Vladimir M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society,  Providence, RI, 1986. COMPRESSED COUNTING MEETS COMPRESSED SENSING  "}, "The Geometry of Losses": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "The Geometry of Losses", "abstract": "Loss functions are central to machine learning because they are the means by which the quality of a prediction is evaluated. Any loss that is not proper, or can not be transformed to be proper via a link function is inadmissible. All admissible losses for n-class problems can be obtained in terms of a convex body in \\mathbbR^n.  We show this explicitly and show how some existing results simplify when viewed from this perspective.  This  allows the development of a rich algebra of losses induced by binary operations on convex bodies (that return a convex body).  Furthermore it allows us to define an \u201cinverse loss\u201d which provides a universal \u201csubstitution function\u201d for the Aggregating Algorithm.  In doing so we show a formal connection between proper losses and norms.", "pdf_url": "http://proceedings.mlr.press/v35/williamson14.pdf", "keywords": ["convex bodies", "support functions", "gauges", "polars", "proper losses", "distorted probabilities", "inverse losses", "entropies", "norms", "Bregman divergences", "aggregating algorithm", "substitution functions"], "reference": "Jean-Pierre Aubin and H\u00b4el`ene Frankowska. Set-Valued Analysis. Birkh\u00a8auser, 1990.  Abdessamad Barbara and Jean-Pierre Crouzeix. Concave gauge functions and applications. ZOR -  Mathematical Methods of Operations Research, 40:43-74, 1994.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estima- tion and classification: Structure and applications. Technical report, University of Pennsylvania, November 2005.  Alain Chateauneuf. Decomposable capacities, distorted probabilities and concave capacities. Math-  ematical Social Sciences, 31:19-37, 1996.  Charles W. Cobb and Paul H. Douglas. A theory of production. The American Economic Review,  18(1):139-165, March 1928.  Richard Cornes. Duality and Modern Economics. Cambridge University Press, 1992.  A. Philip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Math-  ematics, 59(1):77-93, March 2007.  A. Philip Dawid and Steffen L. Lauritzen. The geometry of decision theory. In Proceedings of the 2nd International Symposium on Information Geometry and its Applications, pages 22-28, 2006.  Michael P. Drazin. Pseudo-inverses in associative rings and semigroups. The American Mathemat-  ical Monthly, 65(7):506-514, August 1958.  Rolf F\u00a8are and Daniel Primont. The unification of Ronald W. Shephard\u2019s duality theory. Journal of  Economics (Zeitschrift f\u00a8ur National\u00a8okonomie), 60(2):199-207, 1994.  Rolf F\u00a8are and Daniel Primont. Multi-output Production and Duality: Theory and Applications.  Kluwer Academic Publishers, 1995.  Edward Furman and Ri\u02c7cardas Zitikis. Weighted pricing functionals with applications to insurance:  An overview. North American Actuarial Journal, 13(4):483-496, 2009.  Dario Garcia-Garcia and Robert C. Williamson. Divergences and Risks for Multiclass Experiments.  In Conference on Learning Theory (JMLR: W&CP), volume 23, pages 28.1-28.20, 2012.  Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical  Association, 106(494):746-762, 2011.  Tilmann Gneiting and Matthias Katzfuss. Probabilistic forecasting. Annual Review of Statistics and  its Applications, 1:125-151, 2014.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, March 2007.  David J. Hand. Deconstructing Statistical Questions. Journal of the Royal Statistical Society. Series  A (Statistics in Society), 157(3):317-356, 1994.  14   WILLIAMSON  References  Jean-Pierre Aubin and H\u00b4el`ene Frankowska. Set-Valued Analysis. Birkh\u00a8auser, 1990.  Abdessamad Barbara and Jean-Pierre Crouzeix. Concave gauge functions and applications. ZOR -  Mathematical Methods of Operations Research, 40:43-74, 1994.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estima- tion and classification: Structure and applications. Technical report, University of Pennsylvania, November 2005.  Alain Chateauneuf. Decomposable capacities, distorted probabilities and concave capacities. Math-  ematical Social Sciences, 31:19-37, 1996.  Charles W. Cobb and Paul H. Douglas. A theory of production. The American Economic Review,  18(1):139-165, March 1928.  Richard Cornes. Duality and Modern Economics. Cambridge University Press, 1992.  A. Philip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Math-  ematics, 59(1):77-93, March 2007.  A. Philip Dawid and Steffen L. Lauritzen. The geometry of decision theory. In Proceedings of the 2nd International Symposium on Information Geometry and its Applications, pages 22-28, 2006.  Michael P. Drazin. Pseudo-inverses in associative rings and semigroups. The American Mathemat-  ical Monthly, 65(7):506-514, August 1958.  Rolf F\u00a8are and Daniel Primont. The unification of Ronald W. Shephard\u2019s duality theory. Journal of  Economics (Zeitschrift f\u00a8ur National\u00a8okonomie), 60(2):199-207, 1994.  Rolf F\u00a8are and Daniel Primont. Multi-output Production and Duality: Theory and Applications.  Kluwer Academic Publishers, 1995.  Edward Furman and Ri\u02c7cardas Zitikis. Weighted pricing functionals with applications to insurance:  An overview. North American Actuarial Journal, 13(4):483-496, 2009.  Dario Garcia-Garcia and Robert C. Williamson. Divergences and Risks for Multiclass Experiments.  In Conference on Learning Theory (JMLR: W&CP), volume 23, pages 28.1-28.20, 2012.  Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical  Association, 106(494):746-762, 2011.  Tilmann Gneiting and Matthias Katzfuss. Probabilistic forecasting. Annual Review of Statistics and  its Applications, 1:125-151, 2014.  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, March 2007.  David J. Hand. Deconstructing Statistical Questions. Journal of the Royal Statistical Society. Series  A (Statistics in Society), 157(3):317-356, 1994. THE GEOMETRY OF LOSSES  Berlin, 2001.  1965.  1972.  David J. Hand and Veronica Vinciotti. Local Versus Global Models for Classification Problems:  Fitting Models Where it Matters. The American Statistician, 57(2):124-131, 2003.  Giora Hanoch. Symmetric duality and polar production functions. In Melvyn Fuss and Daniel Mc- Fadden, editors, Production Economics: A Dual Approach to Theory and Applications, volume 1, pages 111-132. North-Holland, 1978.  Georg Hasenkamp and J\u00a8urgen Schrader. Dual polar price and quantity aggregation. Zeitschrift f\u00a8ur  National\u00a8okonomie, 38(3-4):305-322, 1978.  Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive definite kernels on probability measures. Technical Report 126, Max-Planck-Institut f\u00a8ur biologische Kybernetik, July 2004.  Arlo D. Hendrickson and Robert J. Buehler. Proper scores for probability forecasters. The Annals  of Mathematical Statisitics, 42(6):1916-1921, 1971.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis. Springer,  Hendrik S. Houthhakker. A note on self-dual preferences. Econometrica, 33(4):797-801, October  Patrick Hummel and R. Preston McAfee. Loss functions for predicted click-through rates in auctions  for online advertising. Preprint, Google Inc., October 2013.  Stephen E. Jacobsen. On Shephard\u2019s duality theorem. Journal of Economic Theory, 4:458-464,  John McCarthy. Measures of the value of information. Proceedings of the National Academy of  Sciences, 42:654-655, 1956.  Daniel McFadden. Cost, revenue, and profit functions.  In Melvyn Fuss and Daniel McFadden, editors, Production Economics: A Dual Approach to Theory and Applications, volume 1, pages 1-110. North-Holland, 1978.  Edgar C. Merkle and Mark Steyvers. Choosing a strictly proper scoring rule. Preprint, Department  of Psychological Sciences, University of Missouri, Columbia, Missouri, June 2013.  Walter Meyer and David C. Kay. A convexity structure admits but one real linearization of dimen- sion greater than one. Journal of the London Mathematical Society, 7:124-130, 1973. Series 2.  Allan H. Murphy and Carl-Axel S. Sta\u00a8el von Holstein. A geometrical framework for the ranked  probability score. Monthly Weather Review, 103(1):16-20, 1975.  Jean-Paul Penot. The bearing of duality on microeconomics. In Advances in Mathematical Eco-  nomics, pages 113-139. Springer, 2005.  Jean-Paul Penot and Constantin Z\u02d8alinescu. Harmonic sum and duality. Journal of Convex Analysis,  7(1):95-113, 2000. WILLIAMSON  Georg Ch. P\ufb02ug and Werner R\u00a8omisch. Modeling, Measuring and Managing Risk. World Scientific,  2007.  Friedrich Pukelsheim. On information functions and their polars. Journal of Optimization Theory  and Applications, 41(4):533-546, 1983.  R. Mark Reesor and Don L. McLeish. Risk, entropy, and the transformation of distributions. Bank  of Canada Working Paper 2002-11, April 2002.  Mark D. Reid and Robert C. Williamson. Information, divergence and risk for binary experiments.  Journal of Machine Learning Research, 12:731-817, March 2011.  R. Tyrell Rockafellar. Convex Analysis. Princeton University Press, 1970.  R. Tyrrell Rockafellar. Monotone Processes of Convex and Concave Type, volume 77 of Memoirs  of the American Mathematical Society. 1967.  Michael Edward Ruberry. Prediction Markets: Theory and Applications. PhD thesis, School of  Engineering and Applied Sciences, Harvard University, November 2013.  Paul A. Samuelson. Using full duality to show that simultaneously additive direct and indirect utilities implies unitary price elasticity of demand. Econometrica, 33(4):781-796, October 1965.  Ryuzo Sato. Self-dual preferences. Econometrica, 44(5):1017-1032, September 1976.  Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the American  Statistical Association, 66(336):783-801, 1971.  Rolf Schneider. Convex Bodies: The Brunn-Minkowski Theory. Cambridge University Press, 2014.  Second expanded edition.  Alberto Seeger. Direct and inverse addition in convex analysis and applications. Journal of Mathe-  matical Analysis and Applications, 148:317-349, 1990.  Alberto Seeger and Michel Volle. On a convolution operation obtained by adding level sets: classical  and new results. Recherche op\u00b4erationnelle/Operations Research, 29(2):131-154, 1995.  Ronald W. Shephard. Cost and Production Functions. Princeton University Press, 1953.  Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics, 8:171-176, 1958.  Carl-Axel S. Sta\u00a8el von Holstein. Assessment and evaluation of subjective probability distributions. PhD thesis, The Economic Research Institute, Stockholm School of Economics, September 1970.  Carl-Axel S. Sta\u00a8el von Holstein and Allan H. Murphy. The family of quadratic scoring rules.  Monthly Weather Review, 106(7):917-924, July 1978.  Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, New York, 2008.  Anthony C. Thompson. Minkowski Geometry. Cambridge University Press, 1996.  Hal R. Varian. Microeconomic Analysis. W.W. Norton and Company, 1978. THE GEOMETRY OF LOSSES  Elodie Vernet, Robert C. Williamson, and Mark D. Reid. Composite multiclass losses. Submitted to Journal of Machine Learning Research, 42 pages., June 2012. URL http://users.cecs. anu.edu.au/\u02dcwilliams/papers/P189.pdf.  Volodya Vovk. Aggregating strategies. In Proceedings of the Third Annual Workshop on Computa-  tional Learning Theory (COLT), pages 371-383, 1990.  Volodya Vovk. A game of prediction with expert advice.  In Proceedings of the Eighth Annual  Conference on Computational Learning Theory, pages 51-60. ACM, 1995.  Volodya Vovk. Competitive on-line statistics.  International Statistical Review, 69(2):213-248,  2001.  Robert C. Williamson. Loss functions. In Bernhard Sch\u00a8olkopf, Zhiyuan Luo, and Vladimir Vovk, editors, Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik, pages 71-80. Springer, 2013.  Eugene M. Zaustinsky. Spaces with non-symmetric distance. Memoirs of the American Mathemat-  ical Society, 34:1-91, 1959.  Fedor Zhdanov. Theory and Applications of Competitive Prediction. PhD thesis, Department of  Computer Science, Royal Holloway, University of London, 2011.  Constantin Z\u02d8alinescu. Relations between the convexity of a set and the differentiability of its support  function. arXiv:1301.0810v1, January 2013.  "}, "Resourceful Contextual Bandits": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Resourceful Contextual Bandits", "abstract": "We study contextual bandits with ancillary constraints on resources, which are common in real-world applications such as choosing ads or dynamic pricing of items.  We design the first algorithm for solving these problems that improves over a trivial reduction to the non-contextual case. We consider very general settings for both contextual bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits with knapsacks,  Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties.", "pdf_url": "http://proceedings.mlr.press/v35/badanidiyuru14.pdf", "keywords": [], "reference": "Alekh Agarwal, Miroslav Dudik, Satyen Kale, and John Langford. Contextual bandit learning under the realizability assumption. In 15th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS), 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In 31st Intl. Conf. on Machine Learning (ICML), 2013.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In  15th, 2014.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. of Machine Learning  Research (JMLR), 3:397-422, 2002. Preliminary version in 41st IEEE FOCS, 2000.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi- armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002. Preliminary version in 36th IEEE FOCS, 1995.  Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, and Aleksandrs Slivkins. Dynamic pricing  with limited supply. In 13th ACM Conf. on Electronic Commerce (EC), 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted price mechanisms for online procurement. In 13th ACM Conf. on Electronic Commerce (EC), pages 128-145, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  In 54th IEEE Symp. on Foundations of Computer Science (FOCS), 2013a.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  A technical report on arxiv.org., May 2013b.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk  bounds and near-optimal algorithms. Operations Research, 57:1407-1420, 2009.  Omar Besbes and Assaf J. Zeevi. Blind network revenue management. Operations Research, 60  (6):1537-1550, 2012.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Efficient optimal leanring for contextual bandits. In 14th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS), 2011.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-  armed Bandit Problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In 25th Conf. on Learning Theory (COLT), 2012.  14   BADANIDIYURU LANGFORD SLIVKINS  References  Alekh Agarwal, Miroslav Dudik, Satyen Kale, and John Langford. Contextual bandit learning under the realizability assumption. In 15th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS), 2012.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In 31st Intl. Conf. on Machine Learning (ICML), 2013.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In  15th, 2014.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. of Machine Learning  Research (JMLR), 3:397-422, 2002. Preliminary version in 41st IEEE FOCS, 2000.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi- armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002. Preliminary version in 36th IEEE FOCS, 1995.  Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, and Aleksandrs Slivkins. Dynamic pricing  with limited supply. In 13th ACM Conf. on Electronic Commerce (EC), 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted price mechanisms for online procurement. In 13th ACM Conf. on Electronic Commerce (EC), pages 128-145, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  In 54th IEEE Symp. on Foundations of Computer Science (FOCS), 2013a.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  A technical report on arxiv.org., May 2013b.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk  bounds and near-optimal algorithms. Operations Research, 57:1407-1420, 2009.  Omar Besbes and Assaf J. Zeevi. Blind network revenue management. Operations Research, 60  (6):1537-1550, 2012.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Efficient optimal leanring for contextual bandits. In 14th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS), 2011.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-  armed Bandit Problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial  bandits. In 25th Conf. on Learning Theory (COLT), 2012. RESOURCEFUL CONTEXTUAL BANDITS  Nikhil Devanur and Vijay Vazirani. The spending constraint model for market equilibrium: Algo- rithmic, existence and uniqueness results. In 36th ACM Symp. on Theory of Computing (STOC), 2004.  Nikhil R. Devanur and Thomas P. Hayes. The AdWords problem: Online keyword matching with budgeted bidders under random permutations. In 10th ACM Conf. on Electronic Commerce (EC), pages 71-78, 2009.  Nikhil R. Devanur, Kamal Jain, Balasubramanian Sivan, and Christopher A. Wilkens. Near optimal online algorithms and fast approximation algorithms for resource allocation problems. In 12th ACM Conf. on Electronic Commerce (EC), pages 29-38, 2011.  Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal leanring for contextual bandits. In 27th Conf. on Uncertainty in Artificial Intelligence (UAI), 2011.  D. A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3:100-118, 1975.  John Gittins, Kevin Glazebrook, and Richard Weber. Multi-Armed Bandit Allocation Indices. John  Wiley & Sons, 2011.  Sudipta Guha and Kamesh Munagala. Multi-armed Bandits with Metric Switching Costs. In 36th Intl. Colloquium on Automata, Languages and Programming (ICALP), pages 496-507, 2007.  Sudipta Guha, Kamesh Munagala, and Peng Shi. Approximation algorithms for restless bandit problems., 2010. Combined final version of papers in IEEE FOCS 2007 and ACM-SIAM SODA 2009.  Anupam Gupta, Ravishankar Krishnaswamy, Marco Molinaro, and R. Ravi. Approximation algo- rithms for correlated knapsacks and non-martingale bandits. In 52nd IEEE Symp. on Foundations of Computer Science (FOCS), pages 827-836, 2011.  Andr\u00b4as Gy\u00a8orgy, Levente Kocsis, Ivett Szab\u00b4o, and Csaba Szepesv\u00b4ari. Continuous time associative bandit problems. In 20th Intl. Joint Conf. on Artificial Intelligence (IJCAI), pages 830-835, 2007.  Robert Kleinberg and Aleksandrs Slivkins. Sharp Dichotomies for Regret Minimization in Metric  Spaces. In 21st ACM-SIAM Symp. on Discrete Algorithms (SODA), 2010.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-Armed Bandits in Metric Spaces. In  40th ACM Symp. on Theory of Computing (STOC), pages 681-690, 2008.  John Langford and Tong Zhang. The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits.  In 21st Advances in Neural Information Processing Systems (NIPS), 2007.  Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert  advice. In 22nd Conf. on Learning Theory (COLT), 2009.  Adish Singla and Andreas Krause. Truthful incentives in crowdsourcing tasks using regret mini- mization mechanisms. In 22nd Intl. World Wide Web Conf. (WWW), pages 1167-1178, 2013.  Maurice Sion. On general minimax theorems. Pac. J. Math., 8:171176, 1958. BADANIDIYURU LANGFORD SLIVKINS  Aleksandrs Slivkins. Contextual Bandits with Similarity Information. In 24th Conf. on Learning  Theory (COLT), 2011. To appear in J. of Machine Learning Research (JMLR), 2014.  Aleksandrs Slivkins. Dynamic ad allocation: Bandits with budgets. A technical report on  arxiv.org/abs/1306.0155, June 2013.  Aleksandrs Slivkins and Jennifer Wortman Vaughan. Online decision making in crowdsourcing markets: Theoretical challenges. SIGecom Exchanges, 12(2), December 2013. Position Paper and survey.  William R. Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3-4):285294, 1933.  Long Tran-Thanh, Archie Chapman, Enrique Munoz de Cote, Alex Rogers, and Nicholas R. Jen- nings. (cid:15)-first policies for budget-limited multi-armed bandits. In 24th AAAI Conference on Arti- ficial Intelligence (AAAI), pages 1211-1216, 2010.  Long Tran-Thanh, Archie Chapman, Alex Rogers, and Nicholas R. Jennings. Knapsack based optimal policies for budget-limited multi-armed bandits. In 26th AAAI Conference on Artificial Intelligence (AAAI), pages 1134-1140, 2012.  "}, "The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures", "abstract": "In this paper we show that very large mixtures of Gaussians are efficiently learnable in high dimension. More precisely, we prove that a mixture with known identical covariance matrices whose number of components is a polynomial of any fixed degree in the dimension n is polynomially learnable as long as a certain non-degeneracy condition on the means is satisfied. It turns out that this condition is generic in the sense of smoothed complexity, as soon as the dimensionality of the space is high enough. Moreover, we prove that no such condition can possibly exist in low dimension and the problem of learning the parameters is generically hard.  In contrast, much of the existing work on Gaussian Mixtures relies on low-dimensional projections and thus hits an artificial barrier. Our main result on mixture recovery relies on a new \u201cPoissonization\"-based technique, which transforms a mixture of Gaussians to a linear map of a product distribution. The problem of learning this map can be efficiently solved using some recent results on tensor decompositions and Independent Component Analysis (ICA), thus giving an  algorithm for recovering the mixture. In addition, we combine our low-dimensional hardness results for Gaussian mixtures with  Poissonization to show how to embed difficult instances of low-dimensional Gaussian mixtures into the ICA setting, thus establishing exponential information-theoretic lower bounds for underdetermined ICA in low dimension. To the best of our knowledge, this is the first such result  in the literature. In addition to contributing to  the problem of Gaussian mixture learning, we believe that this work is among the first steps toward better understanding the rare phenomenon of the \u201cblessing of dimensionality\" in the computational aspects of statistical inference.", "pdf_url": "http://proceedings.mlr.press/v35/anderson14.pdf", "keywords": ["Gaussian mixture models", "tensor methods", "blessing of dimensionality", "smoothed analysis", "Independent Component Analysis"], "reference": "D. Achlioptas and F. McSherry. On spectral learning of mixture of distributions. In The 18th Annual  Conference on Learning Theory, 2005.  L. Albera, A. Ferreol, P. Comon, and P. Chevalier. Blind Identification of Overcomplete MixturEs of  sources (BIOME). Lin. Algebra Appl., 391:1-30, 2004.  Noga Alon and Joel H Spencer. The probabilistic method. Wiley, 2004.  S. Arora and R. Kannan. Learning Mixtures of Arbitrary Gaussians. In 33rd ACM Symposium on  Theory of Computing, 2001.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown In NIPS, pages  Gaussian noise, with implications for Gaussian mixtures and autoencoders. 2384-2392, 2012.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, pages  103-112. IEEE Computer Society, 2010. ISBN 978-0-7695-4244-7.  Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of  Gaussian noise. In JMLR W&CP, volume 30: COLT, pages 270-287, 2013.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis  of tensor decompositions. CoRR, abs/1311.3651v4, 2014.  Anthony Carbery and James Wright. Distributional and Lq norm inequalities for polynomials over  convex bodies in Rn. Mathematical Research Letters, 8:233-248, 2001.  J-F Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. Blind identification of more sources than sensors. In Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on, pages 3109-3112. IEEE, 1991.  J.-F. Cardoso and A. Souloumiac. Blind beamforming for non-gaussian signals. In Radar and Signal  Processing, IEE Proceedings F, volume 140, pages 362-370, 1993.  Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press,  2010.  Science, 1999.  A. Dasgupta. Probability for Statistics and Machine Learning. Springer, 2011.  S. Dasgupta. Learning Mixture of Gaussians. In 40th Annual Symposium on Foundations of Computer  S. Dasgupta and L. Schulman. A Two Round Variant of EM for Gaussian Mixtures.  In 16th  Conference on Uncertainty in Artificial Intelligence, 2000.  J. Feldman, R. A. Servedio, and R. O\u2019Donnell. PAC Learning Axis Aligned Mixtures of Gaussians with No Separation Assumption. In The 19th Annual Conference on Learning Theory, 2006.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA. CoRR, http://arxiv.org/abs/1306.5825,  2013.  14   ANDERSON BELKIN GOYAL RADEMACHER VOSS  References  D. Achlioptas and F. McSherry. On spectral learning of mixture of distributions. In The 18th Annual  Conference on Learning Theory, 2005.  L. Albera, A. Ferreol, P. Comon, and P. Chevalier. Blind Identification of Overcomplete MixturEs of  sources (BIOME). Lin. Algebra Appl., 391:1-30, 2004.  Noga Alon and Joel H Spencer. The probabilistic method. Wiley, 2004.  S. Arora and R. Kannan. Learning Mixtures of Arbitrary Gaussians. In 33rd ACM Symposium on  Theory of Computing, 2001.  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown In NIPS, pages  Gaussian noise, with implications for Gaussian mixtures and autoencoders. 2384-2392, 2012.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, pages  103-112. IEEE Computer Society, 2010. ISBN 978-0-7695-4244-7.  Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of  Gaussian noise. In JMLR W&CP, volume 30: COLT, pages 270-287, 2013.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis  of tensor decompositions. CoRR, abs/1311.3651v4, 2014.  Anthony Carbery and James Wright. Distributional and Lq norm inequalities for polynomials over  convex bodies in Rn. Mathematical Research Letters, 8:233-248, 2001.  J-F Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. Blind identification of more sources than sensors. In Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on, pages 3109-3112. IEEE, 1991.  J.-F. Cardoso and A. Souloumiac. Blind beamforming for non-gaussian signals. In Radar and Signal  Processing, IEE Proceedings F, volume 140, pages 362-370, 1993.  Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press,  2010.  Science, 1999.  A. Dasgupta. Probability for Statistics and Machine Learning. Springer, 2011.  S. Dasgupta. Learning Mixture of Gaussians. In 40th Annual Symposium on Foundations of Computer  S. Dasgupta and L. Schulman. A Two Round Variant of EM for Gaussian Mixtures.  In 16th  Conference on Uncertainty in Artificial Intelligence, 2000.  J. Feldman, R. A. Servedio, and R. O\u2019Donnell. PAC Learning Axis Aligned Mixtures of Gaussians with No Separation Assumption. In The 19th Annual Conference on Learning Theory, 2006.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA. CoRR, http://arxiv.org/abs/1306.5825,  2013. THE BLESSING OF DIMENSIONALITY FOR LEARNING LARGE GAUSSIAN MIXTURES  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods and  spectral decompositions. In ITCS, pages 11-20, 2013.  Maurice Kendall, Alan Stuart, and J. Keith Ord. Kendall\u2019s advanced theory of statistics. Vol. 1.  Halsted Press, sixth edition, 1994. Distribution theory.  A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 51st  Annual IEEE Symposium on Foundations of Computer Science (FOCS 2010), 2010.  Elchanan Mossel, Ryan O\u2019Donnell, and Krzysztof Oleszkiewicz. Noise stability of functions with  low in\ufb02uences: Invariance and optimality. Annals of Math., 171:295-341, 2010.  Ole A Nielsen. An Introduction to Integration Theory and Measure Theory. Wiley, 1997.  B.C. Rennie and A.J. Dobson. On Stirling numbers of the second kind.  Journal of Com- binatorial Theory, 7(2):116 - 121, 1969. doi: http://dx.doi.org/10. 1016/S0021-9800(69)80045-1. URL http://www.sciencedirect.com/science/ article/pii/S0021980069800451.  ISSN 0021-9800.  Christian Rieger and Barbara Zwicknagl. Sampling inequalities for infinitely smooth functions, with applications to interpolation and machine learning. Advances in Computational Mathematics, 32 (1):103-129, 2010.  John Riordan. Moment recurrence relations for binomial, poisson and hypergeometric frequency  distributions. Annals of Mathematical Statistics, 8:103-111, 1937.  Halsey Lawrence Royden, Patrick Fitzpatrick, and Prentice Hall. Real analysis, volume 4. Prentice  Hall New York, 1988.  Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.  Comm. Pure Appl. Math., 62(12):1707-1739, 2009.  Paul Valiant and Gregory Valiant. Estimating the unseen: Improved estimators for entropy and other  properties. In NIPS, pages 2157-2165, 2013.  S. Vempala and G. Wang. A Spectral Algorithm for Learning Mixtures of Distributions. In 43rd  Annual Symposium on Foundations of Computer Science, 2002.  Holger Wendland. Scattered data approximation, volume 17. Cambridge University Press Cambridge,  A. Winkelbauer. Moments and Absolute Moments of the Normal Distribution. ArXiv e-prints,  2005.  September 2012.  "}, "Near-Optimal Herding": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Near-Optimal Herding", "abstract": "Herding is an algorithm of recent interest in the machine learning community, motivated by inference in Markov random fields. It solves the following sampling problem: given a set \\mathcalX \u2282\\mathbbR^d with mean \u03bc, construct an infinite sequence of points from \\mathcalX such that, for every t \u22651, the mean of the first t points in that sequence lies within Euclidean distance O(1/t) of \u03bc. The classic Perceptron boundedness theorem implies that such a result actually holds for a wide class of algorithms, although the factors suppressed by the O(1/t) notation are exponential in d. Thus, to establish a non-trivial result for the sampling problem, one must carefully analyze the factors suppressed by the O(1/t) error bound. This paper studies the best error that can be achieved for the sampling problem. Known analysis of the Herding algorithm give an error bound that depends on geometric properties of \\mathcalX but, even under favorable conditions, this bound depends linearly on d. We present a new polynomial-time algorithm that solves the sampling problem with error O\\left(\\sqrtd \\log^2.5|\\mathcalX| / t \\right) assuming that \\mathcalX is finite. Our algorithm is based on recent algorithmic results in \\textitdiscrepancy theory. We also show that any algorithm for the sampling problem must have error \u03a9( \\sqrtd / t ). This implies that our algorithm is optimal to within logarithmic factors.", "pdf_url": "http://proceedings.mlr.press/v35/harvey14.pdf", "keywords": ["Herding", "discrepancy theory", "deterministic sampling methods"], "reference": "N. Alon and J. Spencer. The Probabilistic Method. John Wiley,, 2000.  I. Alth\u00a8ofer. On sparse approximations to randomized strategies and convex combinations. Linear  Algebra and its Applications, 199:339-355, 1994.  Edoardo Amaldi and Raphael Hauser. Boundedness theorems for the relaxation method. Mathe-  matics of Operations Research, 30:939-955, 2005.  F. Bach, S. Lacoste-Julien, and G. Obozinsk. On the equivalence between herding and conditional  gradient algorithms. In In Proc. ICML, 2012.  K. Ball. An elementary introduction to modern convex geometry. Random Structures and Algo-  rithms, 31, 1997.  W. Banaszczyk. Balancing vectors and Gaussian measures of n-dimensional convex bodies. Ran-  dom Structures and Algorithms, 12(4):351-360, 1998.  W. Banaszczyk. On series of signed vectors and their rearrangements. Random Structures and  Algorithms, 40:301-316, 2012.  N. Bansal. Constructive algorithms for discrepancy minimization. In FOCS, 2010.  N. Bansal and J. Spencer. Deterministic discrepancy minimization. Algorithmica, 67(4):451-471,  I. Ba\u00b4ra\u00b4ny. On the power of linear dependencies. Building Bridges, Bolyai Society Mathematical  Viktor Bergstr\u00a8om. Zwei s\u00a8atze \u00a8uber ebene vektorpolygone. Abh. Math. Sem. Univ. Hamburg, 8:  2013.  Studies, 19:31-45, 2008.  148-152, 1931.  H. D. Block and S. A. Levin. On the boundedness of an iterative procedure for solving a system of  linear inequalities. Proceedings of the American Mathematical Society, 26:229-235, 1970.  L. Bornn, Y. Chen, N. de Freitas, M. Eskelin, J. Fang, and M. Welling. Herded Gibbs sampling. In  International Conference on Learning Representations, 2013.  Bernard Chazelle. The Discrepancy Method: Randomness and Complexity. Cambridge University  Press, 2000.  S. Chen, J. Dick, and A. B. Owen. Consistency of Markov chain quasi-monte carlo on continuous  state spaces. Annals of Statistics, 39:673-701, 2011.  Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In In Proc. UAI, 2010.  A. Gelfand, Y. Chen, L. van der Maaten, and M. Welling. On herding and the perceptron cycling  theorem. In In Proc. NIPS, 2010.  Alexander E. Holroyd and James Propp. Rotor walks and Markov chains. Algorithmic Probability  and Combinatorics, 520:105-126, 2010.  14   HARVEY SAMADI  References  N. Alon and J. Spencer. The Probabilistic Method. John Wiley,, 2000.  I. Alth\u00a8ofer. On sparse approximations to randomized strategies and convex combinations. Linear  Algebra and its Applications, 199:339-355, 1994.  Edoardo Amaldi and Raphael Hauser. Boundedness theorems for the relaxation method. Mathe-  matics of Operations Research, 30:939-955, 2005.  F. Bach, S. Lacoste-Julien, and G. Obozinsk. On the equivalence between herding and conditional  gradient algorithms. In In Proc. ICML, 2012.  K. Ball. An elementary introduction to modern convex geometry. Random Structures and Algo-  rithms, 31, 1997.  W. Banaszczyk. Balancing vectors and Gaussian measures of n-dimensional convex bodies. Ran-  dom Structures and Algorithms, 12(4):351-360, 1998.  W. Banaszczyk. On series of signed vectors and their rearrangements. Random Structures and  Algorithms, 40:301-316, 2012.  N. Bansal. Constructive algorithms for discrepancy minimization. In FOCS, 2010.  N. Bansal and J. Spencer. Deterministic discrepancy minimization. Algorithmica, 67(4):451-471,  I. Ba\u00b4ra\u00b4ny. On the power of linear dependencies. Building Bridges, Bolyai Society Mathematical  Viktor Bergstr\u00a8om. Zwei s\u00a8atze \u00a8uber ebene vektorpolygone. Abh. Math. Sem. Univ. Hamburg, 8:  2013.  Studies, 19:31-45, 2008.  148-152, 1931.  H. D. Block and S. A. Levin. On the boundedness of an iterative procedure for solving a system of  linear inequalities. Proceedings of the American Mathematical Society, 26:229-235, 1970.  L. Bornn, Y. Chen, N. de Freitas, M. Eskelin, J. Fang, and M. Welling. Herded Gibbs sampling. In  International Conference on Learning Representations, 2013.  Bernard Chazelle. The Discrepancy Method: Randomness and Complexity. Cambridge University  Press, 2000.  S. Chen, J. Dick, and A. B. Owen. Consistency of Markov chain quasi-monte carlo on continuous  state spaces. Annals of Statistics, 39:673-701, 2011.  Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In In Proc. UAI, 2010.  A. Gelfand, Y. Chen, L. van der Maaten, and M. Welling. On herding and the perceptron cycling  theorem. In In Proc. NIPS, 2010.  Alexander E. Holroyd and James Propp. Rotor walks and Markov chains. Algorithmic Probability  and Combinatorics, 520:105-126, 2010. NEAR-OPTIMAL HERDING  Ferenc Husz\u00b4ar and David Duvenaud. Optimally-weighted herding is Bayesian quadrature. In In  S. Lovett and R. Meka. Constructive discrepancy minimization by walking on the edges. In FOCS,  Proc. UAI, 2012.  2012.  Jiri Matousek. Geometric Discrepancy: An Illustrated Guide. Springer, 1999.  J. Spencer. Six standard deviations suffice. Trans. Amer. Math. Soc., 289:679-706, 1985.  E. Steinitz. Bedingt konvergente reihen und konvexe systeme. J. Reine Ang. Mathematik, 143:  128-175, 1913.  T. Rothvoss. Constructive discrepancy minimization for convex sets. CoRR, abs/1404.0339, 2014.  R. Vershynin. Lecture notes in geometric functional analysis, 2009. manuscript.  M. Welling. Herding dynamical weights to learn. In Proceedings of the 26th International Confer-  ence on Machine Learning (ICML), 2009.  "}, "Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians", "abstract": "We provide an algorithm for properly learning mixtures of two single-dimensional Gaussians without any separability assumptions. Given \\tildeO(1/\\varepsilon^2) samples from an unknown mixture, our algorithm outputs a mixture that is \\varepsilon-close in total variation distance, in time \\tildeO(1/\\varepsilon^5). Our sample complexity is optimal up to logarithmic factors, and significantly improves upon both Kalai et al., whose algorithm has a prohibitive dependence on 1/\\varepsilon, and Feldman et al., whose algorithm requires bounds on the mixture parameters and depends pseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm for selecting a good candidate distribution from among competing hypotheses. Namely, given a collection of N hypotheses containing at least one candidate that is \\varepsilon-close to an unknown distribution, our algorithm outputs a candidate which is O(\\varepsilon)-close to the  distribution. The algorithm requires O(\\logN/\\varepsilon^2) samples from the unknown distribution and O(N \\log N/\\varepsilon^2) time, which improves previous such results (such as the Scheff\u00e9 estimator) from a quadratic dependence of the running time on N to quasilinear. Given the wide use of such results for the purpose of hypothesis selection, our improved algorithm implies immediate improvements to any such use.", "pdf_url": "http://proceedings.mlr.press/v35/daskalakis14.pdf", "keywords": ["Gaussian mixture models", "proper learning", "robust statistics", "hypothesis selection"], "reference": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theerta Suresh. Near-optimal-  sample estimators for spherical Gaussian mixtures. Online manuscript, 2014a.  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In Proceedings of the 2014 IEEE International Symposium on Information Theory, ISIT \u201914, Washington, DC, USA, 2014b. IEEE Computer Society.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In Proceedings of the 18th Annual Conference on Learning Theory, COLT \u201905, pages 458-469. Springer, 2005.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd Annual ACM Symposium on the Theory of Computing, STOC \u201901, pages 247-257, New York, NY, USA, 2001. ACM.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 103-112, Washington, DC, USA, 2010. IEEE Computer Society.  G. E. P. Box and Mervin E. Muller. A note on the generation of random normal deviates. The  Annals of Mathematical Statistics, 29(2):610-611, June 1958.  Spencer Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201908, pages 551-560, Washington, DC, USA, 2008. IEEE Computer Society.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Learning mixtures of struc- tured distributions over discrete domains. In Proceedings of the 24th Annual ACM-SIAM Sym- posium on Discrete Algorithms, SODA \u201913, pages 1380-1394, Philadelphia, PA, USA, 2013. SIAM.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing, STOC \u201914, New York, NY, USA, 2014. ACM.  Sanjoy Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual IEEE Sym- posium on Foundations of Computer Science, FOCS \u201999, pages 634-644, Washington, DC, USA, 1999. IEEE Computer Society.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning Poisson binomial distributions. In Proceedings of the 44th Annual ACM Symposium on the Theory of Computing, STOC \u201912, pages 709-728, New York, NY, USA, 2012. ACM.  Constantinos Daskalakis, Ilias Diakonikolas, Ryan O\u2019Donnell, Rocco A. Servedio, and Li Yang Tan. Learning sums of independent integer random variables. In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201913, pages 217-226, Washington, DC, USA, 2013. IEEE Computer Society.  14   DASKALAKIS KAMATH  References  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theerta Suresh. Near-optimal-  sample estimators for spherical Gaussian mixtures. Online manuscript, 2014a.  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In Proceedings of the 2014 IEEE International Symposium on Information Theory, ISIT \u201914, Washington, DC, USA, 2014b. IEEE Computer Society.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In Proceedings of the 18th Annual Conference on Learning Theory, COLT \u201905, pages 458-469. Springer, 2005.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd Annual ACM Symposium on the Theory of Computing, STOC \u201901, pages 247-257, New York, NY, USA, 2001. ACM.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 103-112, Washington, DC, USA, 2010. IEEE Computer Society.  G. E. P. Box and Mervin E. Muller. A note on the generation of random normal deviates. The  Annals of Mathematical Statistics, 29(2):610-611, June 1958.  Spencer Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201908, pages 551-560, Washington, DC, USA, 2008. IEEE Computer Society.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Learning mixtures of struc- tured distributions over discrete domains. In Proceedings of the 24th Annual ACM-SIAM Sym- posium on Discrete Algorithms, SODA \u201913, pages 1380-1394, Philadelphia, PA, USA, 2013. SIAM.  Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing, STOC \u201914, New York, NY, USA, 2014. ACM.  Sanjoy Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual IEEE Sym- posium on Foundations of Computer Science, FOCS \u201999, pages 634-644, Washington, DC, USA, 1999. IEEE Computer Society.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning Poisson binomial distributions. In Proceedings of the 44th Annual ACM Symposium on the Theory of Computing, STOC \u201912, pages 709-728, New York, NY, USA, 2012. ACM.  Constantinos Daskalakis, Ilias Diakonikolas, Ryan O\u2019Donnell, Rocco A. Servedio, and Li Yang Tan. Learning sums of independent integer random variables. In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201913, pages 217-226, Washington, DC, USA, 2013. IEEE Computer Society. FASTER AND SAMPLE NEAR-OPTIMAL ALGORITHMS FOR PROPER LEARNING MIXTURES OF GAUSSIANS  A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1-38, 1977.  Luc Devroye and G\u00b4abor Lugosi. A universally acceptable smoothing factor for kernel density  estimation. The Annals of Statistics, 24:2499-2512, 1996.  Luc Devroye and G\u00b4abor Lugosi. Nonasymptotic universal smoothing factors, kernel complexity  and yatracos classes. The Annals of Statistics, 25:2626-2637, 1997.  Luc Devroye and G\u00b4abor Lugosi. Combinatorial methods in density estimation. Springer, 2001.  A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics, 27 (3):642-669, 09 1956.  Jon Feldman, Ryan O\u2019Donnell, and Rocco A. Servedio. PAC learning axis-aligned mixtures of In Proceedings of the 19th Annual Conference on  Gaussians with no separation assumption. Learning Theory, COLT \u201906, pages 20-34, Berlin, Heidelberg, 2006. Springer-Verlag.  Alison L. Gibbs and Francis E. Su. On choosing and bounding probability metrics. International  Statistical Review, 70(3):419-435, December 2002.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 11-20, New York, NY, USA, 2013. ACM.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two Gaussians. In Proceedings of the 42nd Annual ACM Symposium on the Theory of Computing, STOC \u201910, pages 553-562, New York, NY, USA, 2010. ACM.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda In Proceedings of the 26th Annual ACM Sellie. On the learnability of discrete distributions. Symposium on the Theory of Computing, STOC \u201994, pages 273-282, New York, NY, USA, 1994. ACM.  P. Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. The Annals of Proba-  bility, 18(3):1269-1283, 07 1990.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 93-102, Washington, DC, USA, 2010. IEEE Computer Society.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201902, pages 113-123, Washington, DC, USA, 2002. IEEE Computer Society.  Yannis G. Yatracos. Rates of convergence of minimum distance estimators and Kolmogorov\u2019s  entropy. The Annals of Statistics, 13(2):768-774, 1985. DASKALAKIS KAMATH  "}, "Online Learning with Composite Loss Functions": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Online Learning with Composite Loss Functions", "abstract": "We study a new class of online learning problems where each of the online algorithm\u2019s actions is assigned an adversarial value, and the loss of the algorithm at each step is a known and deterministic function of the values assigned to its recent actions. This class includes problems where the algorithm\u2019s loss is the \\emphminimum over the recent adversarial values, the \\emphmaximum over the recent values, or a \\emphlinear combination of the recent values. We analyze the minimax regret of this class of problems when the algorithm receives bandit feedback, and prove that when the    \\emphminimum or \\emphmaximum functions are used, the minimax regret is \\widetilde \u03a9(T^2/3) (so called \\emphhard online learning problems), and when a linear function is used, the minimax regret is \\widetilde O(\\sqrtT) (so called \\empheasy learning problems). Previously, the only online learning problem that was known to be provably hard was the multi-armed bandit with switching costs.", "pdf_url": "http://proceedings.mlr.press/v35/dekel14.pdf", "keywords": [], "reference": "A. Antos, G. Bart\u00b4ok, D. P\u00b4al, and C. Szepesv\u00b4ari. Toward a classification of finite partial-monitoring  games. Theoretical Computer Science, 2012.  R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In Proceedings of the Twenty-Ninth International Conference on Machine Learning, 2012.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. The nonstochastic multiarmed bandit prob-  lem. SIAM Journal on Computing, 32(1):48-77, 2002.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How  to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013.  O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. arXiv preprint  arXiv:1310.2997. (STOC 2014, to appear), 2013.  Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an appli-  cation to boosting. Journal of computer and System Sciences, 55(1):119-139, 1997.  S. Geulen, B. V\u00a8ocking, and M. Winkler. Regret minimization for online buffering problems us- ing the weighted majority algorithm. In Proceedings of the 23rd International Conference on Learning Theory, pages 132-143, 2010.  A. Gyorgy and G. Neu. Near-optimal rates for limited-delay universal lossy source coding.  In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2218- 2222. IEEE, 2011.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71:291-307, 2005.  N. Littlestone and M.K. Warmuth. The weighted majority algorithm. Information and Computation,  108:212-261, 1994.  A. Yao. Probabilistic computations: Toward a unified measure of complexity. In Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS), pages 222-227, 1977.  13   ONLINE LEARNING WITH COMPOSITE LOSS FUNCTIONS  References  A. Antos, G. Bart\u00b4ok, D. P\u00b4al, and C. Szepesv\u00b4ari. Toward a classification of finite partial-monitoring  games. Theoretical Computer Science, 2012.  R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. In Proceedings of the Twenty-Ninth International Conference on Machine Learning, 2012.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. The nonstochastic multiarmed bandit prob-  lem. SIAM Journal on Computing, 32(1):48-77, 2002.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How  to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  N. Cesa-Bianchi, O. Dekel, and O. Shamir. Online learning with switching costs and other adaptive  adversaries. In Advances in Neural Information Processing Systems 26, 2013.  O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. arXiv preprint  arXiv:1310.2997. (STOC 2014, to appear), 2013.  Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an appli-  cation to boosting. Journal of computer and System Sciences, 55(1):119-139, 1997.  S. Geulen, B. V\u00a8ocking, and M. Winkler. Regret minimization for online buffering problems us- ing the weighted majority algorithm. In Proceedings of the 23rd International Conference on Learning Theory, pages 132-143, 2010.  A. Gyorgy and G. Neu. Near-optimal rates for limited-delay universal lossy source coding.  In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2218- 2222. IEEE, 2011.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer  and System Sciences, 71:291-307, 2005.  N. Littlestone and M.K. Warmuth. The weighted majority algorithm. Information and Computation,  108:212-261, 1994.  A. Yao. Probabilistic computations: Toward a unified measure of complexity. In Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS), pages 222-227, 1977. DEKEL DING KOREN PERES  "}, "Online Non-Parametric Regression": {"volumn": "v35", "url": "http://proceedings.mlr.press/v35/", "header": "Online Non-Parametric Regression", "abstract": "We establish optimal rates for online regression for arbitrary classes of regression functions in terms of the sequential entropy introduced in (Rakhlin et al., 2010). The optimal rates are shown to exhibit a phase transition analogous to the i.i.d./statistical learning case, studied in  (Rakhlin et al., 2014b). In the frequently encountered situation when sequential entropy and i.i.d. empirical entropy match, our results point to the interesting phenomenon that the rates for statistical learning with squared loss and online nonparametric regression are the same. In addition to a non-algorithmic study of minimax regret, we exhibit a generic forecaster that enjoys the established optimal rates. We also provide a recipe for designing online regression algorithms that can be computationally efficient. We illustrate the techniques by deriving existing and new forecasters for the case of finite experts and for online linear regression.", "pdf_url": "http://proceedings.mlr.press/v35/rakhlin14.pdf", "keywords": [], "reference": "tics, 37(4):1591-1646, 2009.  We gratefully acknowledge the support of NSF under grants CAREER DMS-0954737 and CCF- 1116928, as well as Dean\u2019s Research Fund.  J.Y. Audibert. Fast learning rates in statistical inference through aggregation. The Annals of Statis-  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms.  Journal of Computer and System Sciences, 64(1):48-75, 2002.  K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the  exponential family of distributions. Machine Learning, 43(3):211-246, June 2001.  N. Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. Journal of  Computer and System Sciences, 59(3):392-411, 1999.  N. Cesa-Bianchi and G. Lugosi. Minimax regret under log loss for general classes of experts. In Proceedings of the Twelfth annual conference on computational learning theory, pages 12-18. ACM, 1999.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How  to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  D. P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084-1090, 1991.  S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. Journal  of Machine Learning Research, 14:729-769, 2013.  S. Gerchinovitz and J. Yu. Adaptive and optimal online linear regression on (cid:96)1-balls. Theoretical  Computer Science, 2013.  E. Hazan and N. Megiddo. Online learning with prior knowledge. In Learning Theory, volume  4539 of Lecture Notes in Computer Science, pages 499-513. 2007.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Inf. Comput., 132(1):1-63, 1997.  S. Mendelson. Learning without Concentration. ArXiv e-prints, January 2014.  A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial param- eters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010. URL http://books.nips.cc/papers/files/nips23/NIPS2010_1269. pdf.  13   ONLINE NONPARAMETRIC REGRESSION  Acknowledgements  References  tics, 37(4):1591-1646, 2009.  We gratefully acknowledge the support of NSF under grants CAREER DMS-0954737 and CCF- 1116928, as well as Dean\u2019s Research Fund.  J.Y. Audibert. Fast learning rates in statistical inference through aggregation. The Annals of Statis-  P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident on-line learning algorithms.  Journal of Computer and System Sciences, 64(1):48-75, 2002.  K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the  exponential family of distributions. Machine Learning, 43(3):211-246, June 2001.  N. Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. Journal of  Computer and System Sciences, 59(3):392-411, 1999.  N. Cesa-Bianchi and G. Lugosi. Minimax regret under log loss for general classes of experts. In Proceedings of the Twelfth annual conference on computational learning theory, pages 12-18. ACM, 1999.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  2006.  N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How  to use expert advice. Journal of the ACM, 44(3):427-485, 1997.  D. P. Foster. Prediction in the worst case. Annals of Statistics, 19(2):1084-1090, 1991.  S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. Journal  of Machine Learning Research, 14:729-769, 2013.  S. Gerchinovitz and J. Yu. Adaptive and optimal online linear regression on (cid:96)1-balls. Theoretical  Computer Science, 2013.  E. Hazan and N. Megiddo. Online learning with prior knowledge. In Learning Theory, volume  4539 of Lecture Notes in Computer Science, pages 499-513. 2007.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Inf. Comput., 132(1):1-63, 1997.  S. Mendelson. Learning without Concentration. ArXiv e-prints, January 2014.  A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial param- eters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010. URL http://books.nips.cc/papers/files/nips23/NIPS2010_1269. pdf. RAKHLIN SRIDHARAN  A. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms.  In  Advances in Neural Information Processing Systems 25, pages 2150-2158, 2012.  A. Rakhlin, K. Sridharan, and A. Tewari. Sequential complexities and uniform martingale laws of  large numbers. Probability Theory and Related Fields, February 2014a.  A. Rakhlin, K. Sridharan, and A. Tsybakov. Entropy, minimax regret and minimax risk. In submis-  sion, 2014b.  University, 2007.  S. Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew  N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates.  In Advances in  Neural Information Processing Systems, pages 2199-2207, 2010.  V. Vovk. Competitive on-line linear regression. In NIPS \u201997: Proceedings of the 1997 conference on Advances in neural information processing systems 10, pages 364-370, Cambridge, MA, USA, 1998. MIT Press.  V. Vovk. Competitive on-line statistics. International Statistical Review, 69:213-248, 2001.  V. Vovk. Metric entropy in competitive on-line prediction. CoRR, abs/cs/0609045, 2006a.  V. Vovk. On-line regression competitive with reproducing kernel hilbert spaces.  In Theory and  Applications of Models of Computation, pages 452-463. Springer, 2006b.  V. Vovk. Competing with wild prediction rules. Machine Learning, 69(2):193-212, 12 2007. ONLINE NONPARAMETRIC REGRESSION  "}}