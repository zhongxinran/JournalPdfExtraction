{"Preface: Conference on Learning Theory (COLT), 2017": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Preface: Conference on Learning Theory (COLT), 2017", "abstract": "", "pdf_url": "http://proceedings.mlr.press/v65/kale17a/kale17a.pdf", "keywords": []}, "Open Problem: First-Order Regret Bounds for Contextual Bandits": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Open Problem: First-Order Regret Bounds for Contextual Bandits", "abstract": "We describe two open problems related to first order regret bounds for contextual bandits. The first asks for an algorithm with a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{L_\u22c6}K \\ln N)$ where there are $K$ actions, $N$ policies, and $L_\u22c6$ is the cumulative loss of the best policy. The second asks for an optimization-oracle-efficient algorithm with regret $\\tilde{\\mathcal{O}}(L_\u22c6^{2/3}poly(K, \\ln(N/\u03b4)))$.  We describe some positive results, such as an inefficient algorithm for the second problem, and some partial negative results.", "pdf_url": "http://proceedings.mlr.press/v65/agarwal17a/agarwal17a.pdf", "keywords": ["contextual bandits", "first-order regret bounds", "oracle-efficient algorithms"], "reference": "A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A fast and simple  algorithm for contextual bandits. In ICML, 2014.  A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas,  S. Sen, and A. Slivkins. A multiworld testing decision service. arXiv:1606.03966, 2016.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and G. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of un-  bounded losses under partial monitoring. In ALT, 2006.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem.  SIAM Journal on Computing, 2002.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient optimal  learning for contextual bandits. In UAI, 2011.  D. Foster, Z. Li, T. Lykouris, K. Sridharan, and E. Tardos. Learning in games: Robustness of fast convergence.  In NIPS, 2016.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to  boosting. Journal of Computer and System Sciences, 1997.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In STOC, 2016.  J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In  G. Neu. First-order regret bounds for combinatorial semi-bandits. In COLT, 2015.  A. Rakhlin and K. Sridharan. Bistro: An efficient relaxation-based method for contextual bandits. In ICML,  V. Syrgkanis, A. Krishnamurthy, and R. E. Schapire. Efficient algorithms for adversarial contextual learning.  V. Syrgkanis, H. Luo, A. Krishnamurthy, and R. E. Schapire.  Improved regret bounds for oracle-based  adversarial contextual bandits. In NIPS, 2016b.  NIPS, 2008.  2016.  In ICML, 2016a.  4   AGARWAL KRISHNAMURTHY LANGFORD LUO SCHAPIRE  proportional to 1{p\u00b5 0} so that policies that are observed to make a mistake are eliminated. Now one can verify that  t (a|xt); finally update \u03a0t = {\u03c0 \u2208 \u03a0t\u22121 : \u03c0(xt) (cid:54)= at \u2228 (cid:96)t(at) =  t (a|xt) \u2265 \u03b3}p\u00b5  pt =  1\u03c0i +  1 2  1 2(|\u03a0t\u22121| \u2212 2)  (cid:88)  1\u03c0j ,  j(cid:54)=0,i \u03c0j \u2208\u03a0t\u22121  for any i (cid:54)= 0 s.t. \u03c0i \u2208 \u03a0t\u22121 satisfies the constraint (1\u03c0 denotes the point mass at \u03c0). However, since p\u00b5 t (ab|xi) = 1/2, no matter what \u03b3 is on each round the algorithm either makes a mistake with probability at least 1/(2 T mistakes to eliminate all bad policies. Therefore the \u221a T ). Several variants of this algorithm were also considered and all of them expected regret is \u2126( \u221a were shown to have \u2126(  T ) regret, indicating that some very different techniques are needed.  T ), or has made  \u221a  \u221a  Prize We are offering a prize of $250 for positive or negative resolutions to (Q1) or (Q2).  Acknowledgements We thank Gergely Neu for formative discussions about these problems.  References  A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A fast and simple  algorithm for contextual bandits. In ICML, 2014.  A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas,  S. Sen, and A. Slivkins. A multiworld testing decision service. arXiv:1606.03966, 2016.  C. Allenberg, P. Auer, L. Gy\u00a8orfi, and G. Ottucs\u00b4ak. Hannan consistency in on-line learning in case of un-  bounded losses under partial monitoring. In ALT, 2006.  P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem.  SIAM Journal on Computing, 2002.  M. Dud\u00b4\u0131k, D. Hsu, S. Kale, N. Karampatziakis, J. Langford, L. Reyzin, and T. Zhang. Efficient optimal  learning for contextual bandits. In UAI, 2011.  D. Foster, Z. Li, T. Lykouris, K. Sridharan, and E. Tardos. Learning in games: Robustness of fast convergence.  In NIPS, 2016.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to  boosting. Journal of Computer and System Sciences, 1997.  Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In STOC, 2016.  J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In  G. Neu. First-order regret bounds for combinatorial semi-bandits. In COLT, 2015.  A. Rakhlin and K. Sridharan. Bistro: An efficient relaxation-based method for contextual bandits. In ICML,  V. Syrgkanis, A. Krishnamurthy, and R. E. Schapire. Efficient algorithms for adversarial contextual learning.  V. Syrgkanis, H. Luo, A. Krishnamurthy, and R. E. Schapire.  Improved regret bounds for oracle-based  adversarial contextual bandits. In NIPS, 2016b.  NIPS, 2008.  2016.  In ICML, 2016a. "}, "Open Problem: Meeting Times for Learning Random Automata": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Open Problem: Meeting Times for Learning Random Automata", "abstract": "Learning automata is a foundational problem in computational learning theory. However, even efficiently learning random DFAs is hard. A natural restriction of this problem is to consider learning random DFAs under the uniform distribution. To date, this problem has no non-trivial lower bounds nor algorithms faster than brute force. In this note, we propose a method to find faster algorithms for this problem. We reduce the learning problem to a conjecture about meeting times of random walks over random DFAs, which may be of independent interest to prove.", "pdf_url": "http://proceedings.mlr.press/v65/fish17a/fish17a.pdf", "keywords": ["automata theory", "PAC learning", "uniform distribution", "social choice theory"], "reference": "106, 1987.  David J Aldous. Meeting times for independent markov chains. Stochastic Processes and their  applications, 38(2):185-193, 1991.  Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87-  Dana Angluin and Dongqu Chen. Learning a random DFA from uniform strings and state infor- mation. In Algorithmic Learning Theory - 26th International Conference, ALT 2015, Banff, AB, Canada, October 4-6, 2015, Proceedings, pages 119-133, 2015.  Dana Angluin, Leonor Becerra-Bonache, Adrian-Horia Dediu, and Lev Reyzin. Learning finite automata using label queries. In Algorithmic Learning Theory, 20th International Conference, ALT 2009, Porto, Portugal, October 3-5, 2009. Proceedings, pages 171-185, 2009.  Dana Angluin, David Eisenstat, Leonid Kontorovich, and Lev Reyzin. Lower bounds on learning random structures with statistical queries. In Algorithmic Learning Theory, 21st International Conference, ALT 2010, Canberra, Australia, October 6-8, 2010. Proceedings, pages 194-208, 2010.  Michael Ben-Or and Nathan Linial. Collective coin \ufb02ipping, robust voting schemes and minima of banzhaf values. In 26th Annual Symposium on Foundations of Computer Science, Portland, Oregon, USA, 21-23 October 1985, pages 408-416, 1985.  J\u00b4an \u02c7Cern`y. Pozn\u00b4amka k homog\u00b4ennym experimentom s kone\u02c7cn`ymi automatmi. Matematicko-  fyzik\u00b4alny \u02c7casopis, 14(3):208-216, 1964.  Colin Cooper, Alan M. Frieze, and Tomasz Radzik. Multiple random walks in random regular  graphs. SIAM J. Discrete Math., 23(4):1738-1761, 2009.  Colin Cooper, Robert Els\u00a8asser, Hirotaka Ono, and Tomasz Radzik. Coalescing random walks and  voting on connected graphs. SIAM J. Discrete Math., 27(4):1748-1758, 2013.  Varun Kanade, Frederik Mallmann-Trenn, and Thomas Sauerwald. On coalescence time in graphs-  when is coalescing as fast as meeting? CoRR, abs/1611.02460, 2016.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform, and  learnability. J. ACM, 40(3):607-620, 1993.  Ryan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.  Leonard Pitt and Manfred K. Warmuth. Prediction-preserving reducibility. J. Comput. Syst. Sci., 41  (3):430-467, 1990.  Evgeny S. Skvortsov and Yulia Zaks. Synchronizing random automata. Discrete Mathematics &  Theoretical Computer Science, 12(4):95-108, 2010.  4   FISH REYZIN  References  106, 1987.  David J Aldous. Meeting times for independent markov chains. Stochastic Processes and their  applications, 38(2):185-193, 1991.  Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87-  Dana Angluin and Dongqu Chen. Learning a random DFA from uniform strings and state infor- mation. In Algorithmic Learning Theory - 26th International Conference, ALT 2015, Banff, AB, Canada, October 4-6, 2015, Proceedings, pages 119-133, 2015.  Dana Angluin, Leonor Becerra-Bonache, Adrian-Horia Dediu, and Lev Reyzin. Learning finite automata using label queries. In Algorithmic Learning Theory, 20th International Conference, ALT 2009, Porto, Portugal, October 3-5, 2009. Proceedings, pages 171-185, 2009.  Dana Angluin, David Eisenstat, Leonid Kontorovich, and Lev Reyzin. Lower bounds on learning random structures with statistical queries. In Algorithmic Learning Theory, 21st International Conference, ALT 2010, Canberra, Australia, October 6-8, 2010. Proceedings, pages 194-208, 2010.  Michael Ben-Or and Nathan Linial. Collective coin \ufb02ipping, robust voting schemes and minima of banzhaf values. In 26th Annual Symposium on Foundations of Computer Science, Portland, Oregon, USA, 21-23 October 1985, pages 408-416, 1985.  J\u00b4an \u02c7Cern`y. Pozn\u00b4amka k homog\u00b4ennym experimentom s kone\u02c7cn`ymi automatmi. Matematicko-  fyzik\u00b4alny \u02c7casopis, 14(3):208-216, 1964.  Colin Cooper, Alan M. Frieze, and Tomasz Radzik. Multiple random walks in random regular  graphs. SIAM J. Discrete Math., 23(4):1738-1761, 2009.  Colin Cooper, Robert Els\u00a8asser, Hirotaka Ono, and Tomasz Radzik. Coalescing random walks and  voting on connected graphs. SIAM J. Discrete Math., 27(4):1748-1758, 2013.  Varun Kanade, Frederik Mallmann-Trenn, and Thomas Sauerwald. On coalescence time in graphs-  when is coalescing as fast as meeting? CoRR, abs/1611.02460, 2016.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform, and  learnability. J. ACM, 40(3):607-620, 1993.  Ryan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.  Leonard Pitt and Manfred K. Warmuth. Prediction-preserving reducibility. J. Comput. Syst. Sci., 41  (3):430-467, 1990.  Evgeny S. Skvortsov and Yulia Zaks. Synchronizing random automata. Discrete Mathematics &  Theoretical Computer Science, 12(4):95-108, 2010. "}, "Corralling a Band of Bandit Algorithms": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Corralling a Band of Bandit Algorithms", "abstract": "We study the problem of combining multiple bandit algorithms (that is, online learning algorithms with partial feedback) with the goal of creating a master algorithm that performs almost as well as the best base algorithm \\it if it were to be run on its own.  The main challenge is that when run with a master, base algorithms unavoidably receive much less feedback and it is thus critical that the master not starve a base algorithm that might perform uncompetitively initially but would eventually outperform others if given enough feedback.  We address this difficulty by devising a version of Online Mirror Descent with a special mirror map together with a sophisticated learning rate scheme. We show that this approach manages to achieve a more delicate balance between exploiting and exploring base algorithms than previous works yielding superior regret bounds. Our results are applicable to many settings, such as multi-armed bandits, contextual bandits, and convex bandits. As examples, we present two main applications. The first is to create an algorithm that enjoys worst-case robustness while at the same time performing much better when the environment is relatively easy.  The second is to create an algorithm that works simultaneously under different assumptions of the environment, such as different priors or different loss structures.", "pdf_url": "http://proceedings.mlr.press/v65/agarwal17b/agarwal17b.pdf", "keywords": ["bandits", "ensemble", "adaptive algorithms"], "reference": "Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and  application to sparse stochastic bandits. In AISTATS, 2012.  13   CORRALLING A BAND OF BANDIT ALGORITHMS  5.2.2. OTHER EXAMPLES  We brie\ufb02y mention some other examples without giving details. For contextual bandits, if we have different ways to represent the contexts, then each base algorithm can be any existing contextual bandit algorithm with a specific context representation and policy space. The master can then have good performance as long as one of these representations captures the problem well.  For stochastic linear bandits, \u0398 \u2282 Rd is a compact convex set and ft(\u03b8, x) = (cid:104)\u03b8, c\u2217(cid:105) + \u03bet where c\u2217 \u2208 Rd is fixed and unknown, and \u03bet is some zero-mean noise. Previous works have studied cases where c\u2217 is assumed to admit some special structures, such as sparsity, group-sparsity and so on (see for example (Abbasi-Yadkori et al., 2012; Carpentier and Munos, 2012; Johnson et al., 2016)). One can then run CORRAL with different base algorithms assuming different structures of c\u2217. Another related problem is generalized linear bandits, where ft(\u03b8, x) = \u03c3((cid:104)\u03b8, c\u2217(cid:105)) + \u03bet for some link function \u03c3 (such as the logistic function, exponential function and so on, see (Filippi et al., 2010)). It is clear that one can run CORRAL with different base algorithms using different link functions to capture more possibilities of the environments. In all these cases, the number of base algorithms is relatively small.  6. Conclusion and Open Problems  In this work, we presented a master algorithm which can combine a set of base algorithms and per- form as well as the best of them in a very strong sense in the bandit setting. Two major applications of our approach were presented to illustrate how this master algorithm can be used to create more adaptive bandit algorithms in a black-box fashion.  There are two major open problems left in this direction. One is to improve the results of Theorem 7 so that the master can basically inherit the same regret bounds of all the base algorithms, i.e., Eq. (1) holds simultaneously for all base algorithms satisfying stability condition with \u03b1i < 1. Note that this is in general impossible (see (Lattimore and Szepesv\u00b4ari, 2016) for a lower bound in a special case), but it is not clear whether it is possible if we only care about the scaling with T while allowing worse dependence on other parameters. The current approach fails to achieve this mainly because each of these bounds requires a different tuning of the same learning rate \u03b7.  Another open problem is to improve the dependence on M , the number of base algorithms, from polynomial to logarithmic while keeping the same dependence on other parameters (or prove its impossibility). Logarithmic dependence on M can be achieved by using EXP4 as the master, but as was earlier discussed, this leads to poor dependence on other parameters.  The authors would like to thank John Langford for posing the question initially that stimulated this research. Most of the work was completed when Behnam Neyshabur was an intern at Microsoft Research.  Acknowledgments  References  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and  application to sparse stochastic bandits. In AISTATS, 2012. AGARWAL LUO NEYSHABUR SCHAPIRE  Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Interior-point methods for full-information and bandit online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.  Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smooth-  ness. In Advances in Neural Information Processing Systems, 2015.  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming In Proceedings of the 31st  the monster: A fast and simple algorithm for contextual bandits. International Conference on Machine Learning, 2014.  Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11(Oct):2785-2836, 2010.  Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine  Learning Research, 3(Nov):397-422, 2002.  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the 29th Annual Conference on Learning Theory (COLT), 2016.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002b.  Sbastien Bubeck and Nicol Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. ISSN 1935-8237. doi: 10.1561/2200000024.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex optimization. In Proceedings of the 29th Annual Conference on Learning Theory (COLT), 2016.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial  bandits. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), volume 23, 2012.  S\u00b4ebastien Bubeck, Ofer Dekel, Tomer Koren, and Yuval Peres. Bandit convex optimization:  T In Proceedings of the 28th Annual Conference on Learning Theory  regret in one dimension. (COLT), 2015.  \u221a  S\u00b4ebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016. CORRALLING A BAND OF BANDIT ALGORITHMS  Alexandra Carpentier and R\u00b4emi Munos. Bandit theory meets compressed sensing for high dimen-  sional stochastic linear bandit. In AISTATS, 2012.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, May 1997.  Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff  functions. In AISTATS, volume 15, pages 208-214, 2011.  Uriel Feige, Tomer Koren, and Moshe Tennenholtz. Chasing ghosts: competing with stateful poli-  cies. In Foundations of Computer Science (FOCS), pages 100-109. IEEE, 2014.  Sarah Filippi, Olivier Cappe, Aur\u00b4elien Garivier, and Csaba Szepesv\u00b4ari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.  Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth an- nual ACM-SIAM symposium on Discrete algorithms, pages 385-394. Society for Industrial and Applied Mathematics, 2005.  Dylan Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In Advances in Neural Information Processing Systems, 2016.  Nicholas Johnson, Vidyashankar Sivakumar, and Arindam Banerjee. Structured stochastic linear  bandits. arXiv preprint arXiv:1606.05693, 2016.  Emilie Kaufmann, Nathaniel Korda, and R\u00b4emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In International Conference on Algorithmic Learning Theory, pages 199-213. Springer, 2012.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side  information. In Advances in neural information processing systems, pages 817-824, 2008.  Tor  Lattimore bandits.  Szepesv\u00b4ari. ear at posts lower-bounds-for-stochastic-linear-bandits, 2016.  and Csaba Blog  lin- http://banditalgs.com/2016/10/20/  stochastic  bounds  Lower  for  Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661-670. ACM, 2010.  Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. In Foundations of  Computer Science, 1989., 30th Annual Symposium on, pages 256-261. IEEE, 1989.  Odalric-Ambrym Maillard and R\u00b4emi Munos. Adaptive bandits: Towards the best history-dependent  strategy. In AISTATS, pages 570-578, 2011. AGARWAL LUO NEYSHABUR SCHAPIRE  Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-  ming, volume 13. Siam, 1994.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceed-  ings of the 26th Annual Conference on Learning Theory (COLT), pages 993-1019, 2013.  Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contex- tual bandits. In Proceedings of the 33rd International Conference on Machine Learning, 2016.  Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling.  Journal of Machine Learning Research, 2014.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. In Advances in Neural Information Pro- cessing Systems, 2016.  William R Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.  "}, "Learning with Limited Rounds of Adaptivity: Coin Tossing, Multi-Armed Bandits, and Ranking from Pairwise Comparisons": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Learning with Limited Rounds of Adaptivity: Coin Tossing, Multi-Armed Bandits, and Ranking from Pairwise Comparisons", "abstract": "In many learning settings, active/adaptive querying is possible, but the number of rounds of adaptivity is limited. We study the relationship between query complexity and adaptivity in identifying the $k$ most biased coins among a set of $n$ coins with unknown biases. This problem is a common abstraction of many well-studied problems, including the problem of identifying the $k$ best arms in a stochastic multi-armed bandit, and the problem of top-$k$ ranking from pairwise comparisons. An $r$-round adaptive algorithm for the $k$ most biased coins problem specifies in each round the set of coin tosses to be performed based on the observed outcomes in earlier rounds, and outputs the set of $k$ most biased coins at the end of $r$ rounds. When $r=1$, the algorithm is known as \\em non-adaptive; when $r$ is unbounded, the algorithm is known as \\em fully adaptive. While the power of adaptivity in reducing query complexity is well known, full adaptivity requires repeated interaction with the coin tossing (feedback generation) mechanism, and is highly sequential, since the set of coins to be tossed in each round can only be determined after we have observed the outcomes of the coin tosses from the previous round. In contrast, algorithms with only few rounds of adaptivity require fewer rounds of interaction with the feedback generation mechanism, and offer the benefits of parallelism in algorithmic decision-making. Motivated by these considerations, we consider the question of how much adaptivity is needed to realize the optimal worst case query complexity for identifying the $k$ most biased coins. Given any positive integer $r$, we derive essentially matching upper and lower bounds on the query complexity of $r$-round algorithms. We then show that $\u0398(\\log^*n)$ rounds are both necessary and sufficient for achieving the optimal worst case query complexity for identifying the $k$ most biased coins. In particular, our algorithm achieves the optimal query complexity in at most $\\log^*n$ rounds, which implies that on any realistic input, $5$ parallel rounds of exploration suffice to achieve the optimal worst-case sample complexity. The best known algorithm prior to our work required $\u0398(\\log n)$ rounds to achieve the optimal worst case query complexity even for the special case of $k=1$.", "pdf_url": "http://proceedings.mlr.press/v65/agarwal17c/agarwal17c.pdf", "keywords": ["Most biased coins", "Best arms identification", "Limited adaptivity", "Multi-armed bandits", "Ranking from pairwise comparisons", "Top-k ranking", "Active learning", "Adaptivity"], "reference": "Miklos Ajtai, J\u00b4anos Komlos, William L Steiger, and Endre Szemer\u00b4edi. Deterministic selection in  o(log log n) parallel time. In STOC, 1986.  Noga Alon and Yossi Azar. Sorting, approximate sorting, and searching in rounds. SIAM J. Discrete  Math., 1(3):269-280, 1988.  streams. In SODA, 2017.  COLT, 2010.  Sepehr Assadi, Sanjeev Khanna, and Yang Li. On estimating maximum matching size in graph  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best Arm Identification in Multi-Armed Bandits. In  B\u00b4ela Bollob\u00b4as and Graham Brightwell. Parallel selection with high probability. SIAM Journal on  Discrete Mathematics, 3(1):21-31, 1990.  B\u00b4ela Bollob\u00b4as and Andrew Thomason. Parallel sorting. Discrete Applied Mathematics, 6(1):1-11,  1983.  Mark Braverman, Jieming Mao, and S. Matthew Weinberg. Parallel Algorithms for Select and  Partition with Noisy Comparisons. In STOC, 2016.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In ICML, 2013.  R\u00b4obert Busa-Fekete, B. Szorenyi, Weiwei Cheng, Paul Weng, and E. Hullermeier. Top-k selection  based on adaptive sampling of noisy preferences. In ICML, 2013.  Karthekeyan Chandrasekaran and Richard Karp. Finding a most biased coin with fewest \ufb02ips. In  Journal of Machine Learning Research, volume 35, pages 394-407, 2014.  Lijie Chen and Jian Li. On the Optimal Sample Complexity for Best Arm Identification. arXiv  preprint arXiv:1511.03774, 2015. URL http://arxiv.org/abs/1511.03774.  Lijie Chen, Jian Li, and Mingda Qiao. Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection. arXiv preprint arXiv:1702.03605, 2017. URL https://arxiv.org/ abs/1702.03605.  Yuxin Chen and Changho Suh. Spectral MLE: Top-k rank aggregation from pairwise comparisons.  In ICML, 2015.  Herman Chernoff. Sequential analysis and optimal design. SIAM, 1972.  Richard Cole. Parallel merge sort. SIAM J. Comput., 17(4):770-785, 1988.  Thomas M. Cover and Joy A. Thomas. Elements of information theory (2. ed.). Wiley, 2006. ISBN  978-0-471-24195-9.  Susan Davidson, Sanjeev Khanna, Tova Milo, and Sudeepa Roy. Top-k and clustering with noisy  comparisons. ACM Transactions on Database Systems (TODS), 39(4):35, 2014.  15   LEARNING WITH LIMITED ROUNDS OF ADAPTIVITY  References  Miklos Ajtai, J\u00b4anos Komlos, William L Steiger, and Endre Szemer\u00b4edi. Deterministic selection in  o(log log n) parallel time. In STOC, 1986.  Noga Alon and Yossi Azar. Sorting, approximate sorting, and searching in rounds. SIAM J. Discrete  Math., 1(3):269-280, 1988.  streams. In SODA, 2017.  COLT, 2010.  Sepehr Assadi, Sanjeev Khanna, and Yang Li. On estimating maximum matching size in graph  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best Arm Identification in Multi-Armed Bandits. In  B\u00b4ela Bollob\u00b4as and Graham Brightwell. Parallel selection with high probability. SIAM Journal on  Discrete Mathematics, 3(1):21-31, 1990.  B\u00b4ela Bollob\u00b4as and Andrew Thomason. Parallel sorting. Discrete Applied Mathematics, 6(1):1-11,  1983.  Mark Braverman, Jieming Mao, and S. Matthew Weinberg. Parallel Algorithms for Select and  Partition with Noisy Comparisons. In STOC, 2016.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In ICML, 2013.  R\u00b4obert Busa-Fekete, B. Szorenyi, Weiwei Cheng, Paul Weng, and E. Hullermeier. Top-k selection  based on adaptive sampling of noisy preferences. In ICML, 2013.  Karthekeyan Chandrasekaran and Richard Karp. Finding a most biased coin with fewest \ufb02ips. In  Journal of Machine Learning Research, volume 35, pages 394-407, 2014.  Lijie Chen and Jian Li. On the Optimal Sample Complexity for Best Arm Identification. arXiv  preprint arXiv:1511.03774, 2015. URL http://arxiv.org/abs/1511.03774.  Lijie Chen, Jian Li, and Mingda Qiao. Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection. arXiv preprint arXiv:1702.03605, 2017. URL https://arxiv.org/ abs/1702.03605.  Yuxin Chen and Changho Suh. Spectral MLE: Top-k rank aggregation from pairwise comparisons.  In ICML, 2015.  Herman Chernoff. Sequential analysis and optimal design. SIAM, 1972.  Richard Cole. Parallel merge sort. SIAM J. Comput., 17(4):770-785, 1988.  Thomas M. Cover and Joy A. Thomas. Elements of information theory (2. ed.). Wiley, 2006. ISBN  978-0-471-24195-9.  Susan Davidson, Sanjeev Khanna, Tova Milo, and Sudeepa Roy. Top-k and clustering with noisy  comparisons. ACM Transactions on Database Systems (TODS), 39(4):35, 2014. AGARWAL AGARWAL ASSADI KHANNA  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC Bounds for Multi-Armed Bandit and  Markov Decision Processes. In COLT, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7:1079-1105, 2006.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with Noisy Information.  SIAM Journal on Computing, 23(5):1001-1018, 1994. doi: 10.1137/S0097539791195877.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A  unified approach to fixed budget and fixed confidence. In NIPS, 2012.  Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. Interna-  tional statistical review, 70(3):419-435, 2002.  David F Gleich and Lek-heng Lim. Rank aggregation via nuclear norm minimization. In KDD,  pages 60-68, 2011.  Reinhard Heckel, Nihar B. Shah, Kannan Ramchandran, and Martin J Wainwright. Active Rank- ing from Pairwise Comparisons and when Parametric Assumptions Dont Help. arXiv preprint arXiv:1606.08842, 2016. URL http://arxiv.org/abs/1606.08842.  Eshcar Hillel, Zohar Shay Karnin, Tomer Koren, Ronny Lempel, and Oren Somekh. Distributed  exploration in multi-armed bandits. In NIPS, 2013.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On Finding the Largest Mean Among Many. arXiv preprint arXiv:1306.3917v1, 2013. URL http://arxiv.org/ abs/1306.3917.  Kevin Jamieson, Daniel Haas, and Ben Recht. The Power of Adaptivity in Identifying Statistical  Alternatives. In NIPS, 2016.  2011.  Kevin G Jamieson and Robert D. Nowak. Active Ranking using Pairwise Comparisons. In NIPS,  Minje Jang, Sunghyun Kim, Changho Suh, and Sewoong Oh. Top-k Ranking from Pairwise Com- parisons: When Spectral Ranking is Optimal. arXiv preprint arXiv:1603.04153, 2016. URL http://arxiv.org/abs/1603.04153.  Kwang-Sung Jun, Kevin Jamieson, Robert Nowak, and Xiaojin Zhu. Top Arm Identification in  Multi-Armed Bandits with Batch Arm Pulls. In AISTATS, 2016.  Shivaram Kalyanakrishnan and Peter Stone. Efficient Selection of Multiple Bandit Arms: Theory  and Practice. In ICML, 2010.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC Subset Selection in  Stochastic Multi-armed Bandits. In ICML, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost Optimal Exploration in Multi-Armed  Bandits. In ICML, 2013. LEARNING WITH LIMITED ROUNDS OF ADAPTIVITY  Richard M. Karp and Robert Kleinberg. Noisy binary search and its applications. In SODA, 2007.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aurlien Garivier. On the Complexity of Best-Arm Identifi- cation in Multi-Armed Bandit Models. Journal of Machine Learning Research, 17:1-42, 2016.  Matthew L Malloy, Gongguo Tang, and Robert D. Nowak. Quickest search for a rare distribution.  In Information Sciences and Systems (CISS). IEEE, 2012.  Shie Mannor and John N Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed  Bandit Problem. Journal of Machine Learning Research, 5:623-648, 2004.  Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons.  In NIPS, 2012.  lems. In COLT, 2015.  Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit prob-  Nicholas Pippenger. Sorting and selecting in rounds. SIAM J. Comput., 16(6):1032-1038, 1987.  Arun Rajkumar and Shivani Agarwal. A statistical convergence perspective of algorithms for rank  aggregation from pairwise data. In ICML, 2014.  Nihar B. Shah and Martin J. Wainwright. Simple, Robust and Optimal Ranking from Pairwise Comparisons. arXiv preprint arXiv:1512.08949, 2015. URL http://arxiv.org/abs/ 1512.08949.  Leslie G. Valiant. Parallelism in comparison problems. SIAM Journal on Computing, 4(3):348-355,  1975. AGARWAL AGARWAL ASSADI KHANNA  "}, "Thompson Sampling for the MNL-Bandit": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Thompson Sampling for the MNL-Bandit", "abstract": "We consider a sequential subset selection problem under parameter uncertainty, where at each time step, the decision maker selects a subset of cardinality $K$ from $N$ possible  items (arms), and observes a (bandit) feedback in the form of the index of one of the items in said subset, or none. Each item in the index set is ascribed a certain value (reward), and the feedback is governed by a Multinomial Logit (MNL) choice model whose parameters are a priori unknown.  The objective of the decision maker is to maximize the expected cumulative rewards over a finite horizon $T$, or alternatively, minimize the regret relative to an oracle that knows the MNL parameters.  We refer to this as the MNL-Bandit problem. This problem is representative of a larger family of exploration-exploitation problems that involve a combinatorial objective, and arise in several important application domains. We present an approach to adapt Thompson Sampling to this problem and show that it achieves near-optimal regret as well as attractive numerical performance.", "pdf_url": "http://proceedings.mlr.press/v65/agrawal17a/agrawal17a.pdf", "keywords": [], "reference": "S. Agrawal, V. Avadhanula, V. Goyal, and A. Zeevi. 2016. A Near-Optimal Exploration-Exploitation Ap- proach for Assortment Selection. Proceedings of the 2016 ACM Conference on Economics and Computa- tion (EC), 599-600.  P. Auer. 2003. Using Confidence Bounds for Exploitation-exploration Trade-offs. Journal of Machine Learn-  ing Research 3 , 397-422.  Machine Learning 47 , 235-256.  P. Auer, N. Cesa-Bianchi, and P. Fischer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem.  A. Gopalan, S. Mannor, and Y. Mansour. 2014. Thompson Sampling for Complex Online Problems.. In Pro- ceedings of the 31st International Conference on International Conference on Machine Learning (ICML), Vol. 32. 100-108.  B. C. May, N. Korda, A. Lee, and D. S. Leslie. 2012. Optimistic Bayesian sampling in contextual-bandit  problems. Journal of Machine Learning Research 13, 2069-2106.  C. Oliver and L. Li. 2011. An Empirical Evaluation of Thompson Sampling. In Advances in Neural Infor-  mation Processing Systems (NIPS) 24 , 2249-2257.  P. Rusmevichientong, Z. M. Shen, and D.B. Shmoys. 2010. Dynamic assortment optimization with a multi-  nomial logit choice model and capacity constraint. Operations Research 58 (6), 1666-1680.  D. Saur\u00b4e and A. Zeevi. 2013. Optimal Dynamic Assortment Planning with Demand Learning. Manufacturing  & Service Operations Management 15 (3), 387-404.  3   THOMPSON SAMPLING FOR THE MNL-BANDIT  References  S. Agrawal, V. Avadhanula, V. Goyal, and A. Zeevi. 2016. A Near-Optimal Exploration-Exploitation Ap- proach for Assortment Selection. Proceedings of the 2016 ACM Conference on Economics and Computa- tion (EC), 599-600.  P. Auer. 2003. Using Confidence Bounds for Exploitation-exploration Trade-offs. Journal of Machine Learn-  ing Research 3 , 397-422.  Machine Learning 47 , 235-256.  P. Auer, N. Cesa-Bianchi, and P. Fischer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem.  A. Gopalan, S. Mannor, and Y. Mansour. 2014. Thompson Sampling for Complex Online Problems.. In Pro- ceedings of the 31st International Conference on International Conference on Machine Learning (ICML), Vol. 32. 100-108.  B. C. May, N. Korda, A. Lee, and D. S. Leslie. 2012. Optimistic Bayesian sampling in contextual-bandit  problems. Journal of Machine Learning Research 13, 2069-2106.  C. Oliver and L. Li. 2011. An Empirical Evaluation of Thompson Sampling. In Advances in Neural Infor-  mation Processing Systems (NIPS) 24 , 2249-2257.  P. Rusmevichientong, Z. M. Shen, and D.B. Shmoys. 2010. Dynamic assortment optimization with a multi-  nomial logit choice model and capacity constraint. Operations Research 58 (6), 1666-1680.  D. Saur\u00b4e and A. Zeevi. 2013. Optimal Dynamic Assortment Planning with Demand Learning. Manufacturing  & Service Operations Management 15 (3), 387-404. "}, "Homotopy Analysis for Tensor PCA": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Homotopy Analysis for Tensor PCA", "abstract": "Developing efficient and guaranteed nonconvex algorithms has been an important challenge in modern machine learning. Algorithms with good empirical performance such as stochastic gradient descent often lack theoretical guarantees. In this paper, we analyze the class of homotopy or continuation methods for global optimization of nonconvex functions. These  methods start from an objective function that is efficient to optimize (e.g. convex), and progressively modify it to obtain the required objective, and the solutions are passed along the homotopy path. For the challenging problem of tensor PCA, we prove global convergence of the homotopy method  in the \u201chigh noise\u201d regime.   The signal-to-noise requirement for our algorithm is tight in the sense that it matches the recovery guarantee for the \\em best degree-$4$ sum-of-squares algorithm. In addition, we prove a phase transition along the homotopy path for  tensor PCA. This allows us to simplify the homotopy method to a local search algorithm, viz., tensor power iterations, with a specific initialization and a noise injection procedure, while retaining the theoretical guarantees.", "pdf_url": "http://proceedings.mlr.press/v65/anandkumar17a/anandkumar17a.pdf", "keywords": ["Tensor PCA", "homotopy", "continuation", "Gaussian smoothing", "nonconvex optimization", "global optimization"], "reference": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. In Proceedings of The 27th Conference on  Learning sparsely used overcomplete dictionaries. Learning Theory, pages 123-137, 2014.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Learning overcomplete latent variable models through tensor methods. In Proceedings of The 28th Conference on Learning Theory, pages 36-112, 2015.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-  plete dictionaries. In COLT, pages 779-806, 2014.  Quentin Berthet, Philippe Rigollet, et al. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013.  Alex Bloemendal and B\u00b4alint Vir\u00b4ag. Limits of spiked random matrices i. Probability Theory and  Related Fields, 156(3-4):795-825, 2013.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The  loss surfaces of multilayer networks. In AISTATS, 2015.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com- pressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914-18919, 2009.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv  preprint arXiv:1605.07272, 2016.  Elad Hazan, Kfir Y Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-  convex problems. In International Conference on Machine Learning (ICML), 2016.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via sum- of-square proofs. In Proceedings of The 28th Conference on Learning Theory, COLT, pages 3-6, 2015.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alter- In Proceedings of the forty-fifth annual ACM symposium on Theory of  nating minimization. computing, pages 665-674. ACM, 2013.  14   ANANDKUMAR DENG GE MOBAHI  References  Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. In Proceedings of The 27th Conference on  Learning sparsely used overcomplete dictionaries. Learning Theory, pages 123-137, 2014.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Learning overcomplete latent variable models through tensor methods. In Proceedings of The 28th Conference on Learning Theory, pages 36-112, 2015.  Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-  plete dictionaries. In COLT, pages 779-806, 2014.  Quentin Berthet, Philippe Rigollet, et al. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013.  Alex Bloemendal and B\u00b4alint Vir\u00b4ag. Limits of spiked random matrices i. Probability Theory and  Related Fields, 156(3-4):795-825, 2013.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The  loss surfaces of multilayer networks. In AISTATS, 2015.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com- pressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914-18919, 2009.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv  preprint arXiv:1605.07272, 2016.  Elad Hazan, Kfir Y Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-  convex problems. In International Conference on Machine Learning (ICML), 2016.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM  (JACM), 60(6):45, 2013.  Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via sum- of-square proofs. In Proceedings of The 28th Conference on Learning Theory, COLT, pages 3-6, 2015.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alter- In Proceedings of the forty-fifth annual ACM symposium on Theory of  nating minimization. computing, pages 665-674. ACM, 2013. HOMOTOPY ANALYSIS FOR TENSOR PCA  Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference, page iat004, 2013.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000.  Michel Ledoux. Deviation inequalities on largest eigenvalues. In Geometric aspects of functional  analysis, pages 167-219. Springer, 2007.  Hossein Mobahi. Closed form for some gaussian convolutions. arXiv:1602.05610, 2016.  Hossein Mobahi and John W Fisher III. A theoretical analysis of optimization by gaussian contin-  uation. In AAAI, pages 1205-1211. Citeseer, 2015a.  Hossein Mobahi and John W Fisher III. On the link between gaussian homotopy continuation and In International Workshop on Energy Minimization Methods in Computer  convex envelopes. Vision and Pattern Recognition, pages 43-56. Springer, 2015b.  Praneeth Netrapalli, UN Niranjan, Sujay Sanghavi, Animashree Anandkumar, and Prateek Jain. Non-convex robust pca. In Advances in Neural Information Processing Systems, pages 1107- 1115, 2014.  Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and arXiv preprint  sub-optimality of pca for spiked random matrices and synchronization. arXiv:1609.05573, 2016.  Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Advances in Neural  Information Processing Systems, pages 2897-2905, 2014.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. In Sampling Theory and Applications (SampTA), 2015 International Conference on, pages 407-410. IEEE, 2015.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Arxiv, 1602.06664,  Ryota Tomioka and Taiji Suzuki. Spectral norm of random tensors. arXiv preprint arXiv:1407.1870,  2016.  2014.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Solving a mixture of many random linear equations by tensor decomposition and alternating minimization. Arxiv, abs/1608.05749, 2016. ANANDKUMAR DENG GE MOBAHI  "}, "Correspondence retrieval": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Correspondence retrieval", "abstract": "This article studies the correspondence retrieval problem: a set of $k$ distinct but unknown points $\\mathbf{x}_1, \\mathbf{x}_2, \\dotsc, \\mathbf{x}_k \u2208\\mathbb{R}^d$ are to be recovered from the unordered collection of projection values $\u27e8\\mathbf{w}_i,\\mathbf{x}_1 \u27e9, \u27e8\\mathbf{w}_i,\\mathbf{x}_2 \u27e9, \\dotsc, \u27e8\\mathbf{w}_i,\\mathbf{x}_k \u27e9$ onto $n$ known measurement vectors $\\mathbf{w}_1, \\mathbf{w}_2, \\dotsc, \\mathbf{w}_n$.  Importantly, the correspondence of the $k$ projections ${\u27e8\\mathbf{w}_i,\\mathbf{x}_j \u27e9}_j=1^k$ across different measurements is unknown.  A special case of this problem is the well-studied problem of (real-valued) phase retrieval.  In the case of independent standard Gaussian measurement vectors, the main algorithm proposed in this work requires $n = d+1$ measurements to correctly return the $k$ unknown points with high probability.  This number of measurements is optimal, and it is smaller than the number of measurements required for a stronger \u201cfor all\u201d guarantee even in the phase retrieval setting.  The algorithm is based on reductions to the Shortest Vector Problem on certain random lattices, and employs the Lenstra, Lenstra, and Lov\u00e1sz (1982) basis reduction algorithm in a manner similar to the Lagarias & Odlyzko (1985) algorithm for solving random instances of Subset Sum.  Another algorithm, essentially due to Yi, Caramanis, & Sanghavi (2016), based on higher-order moments and tensor decompositions is shown to work in a setting where the projection values are corrupted by additive Gaussian noise, but it requires a significantly larger number of measurements.", "pdf_url": "http://proceedings.mlr.press/v65/andoni17a/andoni17a.pdf", "keywords": [], "reference": "Mikl\u00b4os Ajtai. Generating hard instances of lattice problems. In Proceedings of the twenty-eighth  annual ACM symposium on Theory of computing, pages 99-108. ACM, 1996.  Boris Alexeev, Afonso S Bandeira, Matthew Fickus, and Dustin G Mixon. Phase retrieval with  polarization. SIAM Journal on Imaging Sciences, 7(1):35-66, 2014.  Joel Alwen, Stephan Krenn, Krzysztof Pietrzak, and Daniel Wichs. Learning with rounding, revis- ited: New reduction, properties and applications. Cryptology ePrint Archive, Report 2013/098, 2013.  Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Radu Balan, Pete Casazza, and Dan Edidin. On signal reconstruction without phase. Applied and  Computational Harmonic Analysis, 20(3):345-356, 2006.  Emmanuel J Cand`es and Xiaodong Li. Solving quadratic equations via phaselift when there are about as many equations as unknowns. Foundations of Computational Mathematics, 14(5):1017- 1026, 2014.  Emmanuel J Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):1241-1274, 2013.  Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval  via matrix completion. SIAM review, 57(2):225-251, 2015a.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval from coded diffrac-  tion patterns. Applied and Computational Harmonic Analysis, 39(2):277-299, 2015b.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015c.  Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739-747, 2015.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Alan Edelman. Eigenvalues and condition numbers of random matrices. SIAM Journal on Matrix  Analysis and Applications, 9(4):543-560, 1988.  Yonina C Eldar and Shahar Mendelson. Phase retrieval: Stability and recovery guarantees. Applied  and Computational Harmonic Analysis, 36(3):473-494, 2014.  Bing Gao and Zhiqiang Xu. Gauss-newton method for phase retrieval. arXiv preprint, 2016.  13   CORRESPONDENCE RETRIEVAL  References  Mikl\u00b4os Ajtai. Generating hard instances of lattice problems. In Proceedings of the twenty-eighth  annual ACM symposium on Theory of computing, pages 99-108. ACM, 1996.  Boris Alexeev, Afonso S Bandeira, Matthew Fickus, and Dustin G Mixon. Phase retrieval with  polarization. SIAM Journal on Imaging Sciences, 7(1):35-66, 2014.  Joel Alwen, Stephan Krenn, Krzysztof Pietrzak, and Daniel Wichs. Learning with rounding, revis- ited: New reduction, properties and applications. Cryptology ePrint Archive, Report 2013/098, 2013.  Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Radu Balan, Pete Casazza, and Dan Edidin. On signal reconstruction without phase. Applied and  Computational Harmonic Analysis, 20(3):345-356, 2006.  Emmanuel J Cand`es and Xiaodong Li. Solving quadratic equations via phaselift when there are about as many equations as unknowns. Foundations of Computational Mathematics, 14(5):1017- 1026, 2014.  Emmanuel J Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):1241-1274, 2013.  Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval  via matrix completion. SIAM review, 57(2):225-251, 2015a.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval from coded diffrac-  tion patterns. Applied and Computational Harmonic Analysis, 39(2):277-299, 2015b.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015c.  Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages 739-747, 2015.  Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach  spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.  Alan Edelman. Eigenvalues and condition numbers of random matrices. SIAM Journal on Matrix  Analysis and Applications, 9(4):543-560, 1988.  Yonina C Eldar and Shahar Mendelson. Phase retrieval: Stability and recovery guarantees. Applied  and Computational Harmonic Analysis, 36(3):473-494, 2014.  Bing Gao and Zhiqiang Xu. Gauss-newton method for phase retrieval. arXiv preprint, 2016. ANDONI HSU SHI SUN  Michael R Gary and David S Johnson. Computers and Intractability: A Guide to the Theory of  NP-completeness. WH Freeman and Company, New York, 1979.  Anna C Gilbert, Martin J Strauss, Joel A Tropp, and Roman Vershynin. One sketch for all: fast algorithms for compressed sensing. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 237-246. ACM, 2007.  Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge University Press, 1985.  Kishore Jaganathan, Yonina C Eldar, and Babak Hassibi. Phase retrieval: An overview of recent  developments. arXiv preprint arXiv:1510.07713, 2015.  Ritesh Kolte and Ayfer \u00a8Ozg\u00a8ur. Phase retrieval via incremental truncated wirtinger \ufb02ow. arXiv  preprint arXiv:1606.03196, 2016.  Richard Kueng, Holger Rauhut, and Ulrich Terstiege. Low rank matrix recovery from rank one  measurements. Applied and Computational Harmonic Analysis, 42(1):88-116, 2017.  Jeffrey C Lagarias and Andrew M Odlyzko. Solving low-density subset sum problems. Journal of  the ACM, 32(1):229-246, 1985.  Arjen Klaas Lenstra, Hendrik Willem Lenstra, and L\u00b4aszl\u00b4o Lov\u00b4asz. Factoring polynomials with  rational coefficients. Mathematische Annalen, 261(4):515-534, 1982.  Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-  tion. In Advances in Neural Information Processing Systems, pages 2796-2804, 2013.  Sujay Sanghavi, Rachel Ward, and Chris D White. The local convexity of solving systems of  quadratic equations. Results in Mathematics, pages 1-40, 2016.  Yoav Shechtman, Yonina C Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and Mordechai Segev. Phase retrieval with application to optical imaging: a contemporary overview. IEEE Signal Processing Magazine, 32(3):87-109, 2015.  Damien Stehl\u00b4e. Floating-Point LLL: Theoretical and Practical Aspects, pages 179-213. Springer doi: 10.1007/  ISBN 978-3-642-02295-1.  Berlin Heidelberg, Berlin, Heidelberg, 2010. 978-3-642-02295-1 5.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory  (ISIT), 2016 IEEE International Symposium on, pages 2379-2383. IEEE, 2016.  Ir`ene Waldspurger, Alexandre dAspremont, and St\u00b4ephane Mallat. Phase recovery, maxcut and com-  plex semidefinite programming. Mathematical Programming, 149(1-2):47-81, 2015.  Gang Wang, Georgios B Giannakis, and Yonina C Eldar. Solving systems of random quadratic  equations via truncated amplitude \ufb02ow. arXiv preprint arXiv:1605.08285, 2016.  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear  regression. In ICML, pages 613-621, 2014. CORRESPONDENCE RETRIEVAL  Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.  Solving a mixture of many ran- dom linear equations by tensor decomposition and alternating minimization. arXiv preprint arXiv:1608.05749, 2016.  Huishuai Zhang and Yingbin Liang. Reshaped wirtinger \ufb02ow for solving quadratic system of equa-  tions. In Advances in Neural Information Processing Systems, pages 2622-2630, 2016.  "}, "Efficient PAC Learning from the Crowd": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Efficient PAC Learning from the Crowd", "abstract": "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example. In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the \\em realizable setting where there exists a true target function in $\\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are \\emphperfect, and the rest  behave arbitrarily, we show that any $\\mathcal{F}$ that can be efficiently learned in the traditional \\em realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are \\emphgood but not perfect. We show that we can identify  all good labelers, when at least the majority of labelers are good.", "pdf_url": "http://proceedings.mlr.press/v65/awasthi17a/awasthi17a.pdf", "keywords": ["PAC Learning", "Crowdsourcing", "Boosting", "Learning with Noise"], "reference": "Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of lin- ear separators under bounded noise. In Proceedings of the 28th Conference on Computational Learning Theory (COLT), pages 167-190, 2015.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning and 1-bit compressed sensing under asymmetric noise. In Proceedings of the 29th Conference on Compu- tational Learning Theory (COLT), pages 152-192, 2016.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted price mechanisms for online procurement. In Proceedings of the 13th ACM Conference on Eco- nomics and Computation (EC), pages 128-145. ACM, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks: Dynamic procurement for crowdsourcing. In The 3rd Workshop on Social Computing and User Generated Content, co-located with ACM EC, 2013.  17   EFFICIENT PAC LEARNING FROM THE CROWD  i, j \u2208 [n],  (cid:12) (cid:12) (cid:12) (cid:12)  DISAGREE(i, j) \u2212 Pr  (cid:12) (cid:12) [gi(x) (cid:54)= gj(x)] (cid:12) (cid:12)  <  (cid:15) 2  .  x\u223cD|X  Therefore, for any pair of good labelers i and j tested by the algorithm, DISAGREE(i, j) < 2.5(cid:15), and for any pair of labelers i and j that one is good and the other is bad, DISAGREE(i, j) \u2265 2.5(cid:15). Therefore, the connected components of such a graph only include labelers from a single commu- nity.  Next, we show that at step 2 of Algorithm 4 with probability 1 \u2212 \u03b4 there exists at least one  connected component of size n/4 of good labelers.  To see this we first prove that for any two good labelers i and j, the probability of (i, j) existing is at least \u0398(1/n). Let Vg be the set of nodes corresponding to good labelers. For i, j \u2208 Vg, we have  Pr[(i, j) \u2208 G] = 1 \u2212  1 \u2212  (cid:18)  (cid:19)4 ln(2)n  1 n2  \u2248  4 ln(2) n  \u2265  2 ln(2) |Vg|  .  By the properties of random graphs, with very high probability there is a component of size \u03b2|Vg| in a random graph whose edges exists with probability c/|Vg|, for \u03b2 + e\u2212\u03b2c = 1 (Janson et al., 2011). Therefore, with probability 1 \u2212 \u03b4, there is a component of size |Vg|/2 > n/4 over the vertices in Vg. Finally, at step 3 the algorithm considers smaller connected components and tests whether they join any of the bigger components, by measuring the disagreement of two arbitrary labelers from these components.,At this point, all good labelers form one single connected component of size > n  2 . So, the algorithm succeeds in identifying all good labelers. Next, we brie\ufb02y discuss the expected load per labeler in Algorithm 4. Each labeler participates (cid:15) ln(n/\u03b4)) queries. So, in  in O(1) pairs of disagreement tests in expectation, each requiring O( 1 expectation each labeler labels O( 1  (cid:15) ln(n/\u03b4)) instances.  References  Martin Anthony and Peter L Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of lin- ear separators under bounded noise. In Proceedings of the 28th Conference on Computational Learning Theory (COLT), pages 167-190, 2015.  Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning and 1-bit compressed sensing under asymmetric noise. In Proceedings of the 29th Conference on Compu- tational Learning Theory (COLT), pages 152-192, 2016.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted price mechanisms for online procurement. In Proceedings of the 13th ACM Conference on Eco- nomics and Computation (EC), pages 128-145. ACM, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks: Dynamic procurement for crowdsourcing. In The 3rd Workshop on Social Computing and User Generated Content, co-located with ACM EC, 2013. AWASTHI BLUM HAGHTALAB MANSOUR  Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. Journal of  Computer and System Sciences, 75(1):78-89, 2009.  St\u00b4ephane Boucheron, Olivier Bousquet, and G\u00b4abor Lugosi. Theory of classification: A survey of  some recent advances. ESAIM: Probability and Statistics, 9:323-375, 2005.  David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Ma-  chine learning, 15(2):201-221, 1994.  Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Proceedings of the 19th Annual Conference on Neural Information Processing Systems (NIPS), pages 235-242, 2005.  Ofer Dekel and Ohad Shamir. Vox populi: Collecting high-quality labels from a crowd. In Pro- ceedings of the 22nd Conference on Computational Learning Theory (COLT), pages 377-386, 2009.  Willliam Feller. An introduction to probability theory and its applications, volume 2. John Wiley  & Sons, 2008.  Yoav Freund. Boosting a weak learning algorithm by majority. In Proceedings of the 22nd Confer-  ence on Computational Learning Theory (COLT), volume 90, pages 202-216, 1990.  Yoav Freund and Robert E Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting. In European conference on computational learning theory, pages 23-37. Springer, 1995.  Steve Hanneke. Rates of convergence in active learning. The Annals of Statistics, 39(1):333-361,  2011.  Chien-Ju Ho, Shahin Jabbari, and Jennifer Wortman Vaughan. Adaptive task assignment for crowd- sourced classification. Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.  Panagiotis G Ipeirotis, Foster Provost, and Jing Wang. Quality management on amazon mechanical turk. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 64-67. ACM, 2010.  Svante Janson, Tomasz Luczak, and Andrzej Rucinski. Random graphs, volume 45. John Wiley &  Sons, 2011.  David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing sys- tems. In Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS), pages 1953-1961, 2011.  David R Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable  crowdsourcing systems. Operations Research, 62(1):1-24, 2014.  Aniket Kittur, Ed H Chi, and Bongwon Suh. Crowdsourcing user studies with mechanical turk. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 453-456. ACM, 2008. EFFICIENT PAC LEARNING FROM THE CROWD  Vladimir Koltchinskii. Rademacher complexities and bounding the excess risk in active learning.  Journal of Machine Learning Research, 11:2457-2485, 2010.  Ronald L Rivest and Robert Sloan. A formal model of hierarchical concept-learning. Information  and Computation, 114(1):88-114, 1994.  Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197-227, 1990.  Adish Singla and Andreas Krause. Truthful incentives in crowdsourcing tasks using regret mini- mization mechanisms. In Proceedings of the 22nd international conference on World Wide Web, pages 1167-1178. ACM, 2013.  Aleksandrs Slivkins and Jennifer Wortman Vaughan. Online decision making in crowdsourcing  markets: Theoretical challenges. ACM SIGecom Exchanges, 12(2):4-23, 2014.  Jacob Steinhardt, Gregory Valiant, and Moses Charikar. Avoiding imposters and delinquents: Ad- versarial crowdsourcing and peer prediction. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS), pages 4439-4447, 2016.  Long Tran-Thanh, Sebastian Stein, Alex Rogers, and Nicholas R Jennings. Efficient crowdsourcing of unknown experts using bounded multi-armed bandits. Artificial Intelligence, 214:89-111, 2014.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a high-quality workforce with me- chanical turk. Presented at the NIPS Workshop on Computational Social Science and the Wisdom of Crowds, pages 1-5, 2010.  Songbai Yan, Kamalika Chaudhuri, and Tara Javidi. Active learning from imperfect labelers. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS), pages 2128-2136, 2016.  Chicheng Zhang and Kamalika Chaudhuri. Active learning from weak and strong labelers.  In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 703-711, 2015. AWASTHI BLUM HAGHTALAB MANSOUR  "}, "The Price of Selection in Differential Privacy": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "The Price of Selection in Differential Privacy", "abstract": "In the differentially private top-$k$ selection problem, we are given a dataset $X \u2208\\pmo^n \\times d$, in which each row belongs to an individual and each column corresponds to some binary attribute, and our goal is to find a set of $k \u226ad$ columns whose means are approximately as large as possible.  Differential privacy requires that our choice of these $k$ columns does not depend too much on any on individual\u2019s dataset.  This problem can be solved using the well known exponential mechanism and composition properties of differential privacy.  In the high-accuracy regime, where we require the error of the selection procedure to be to be smaller than the so-called sampling error $\u03b1\u2248\\sqrt\\ln(d)/n$, this procedure succeeds given a dataset of size $n \u2273k \\ln(d)$. We prove a matching lower bound, showing that a dataset of size $n \u2273k \\ln(d)$ is necessary for private top-$k$ selection in this high-accuracy regime.  Our lower bound shows that selecting the $k$ largest columns requires more data than simply estimating the value of those $k$ columns, which can be done using a dataset of size just $n \u2273k$.", "pdf_url": "http://proceedings.mlr.press/v65/bafna17a/bafna17a.pdf", "keywords": ["Differential privacy", "lower bounds", "variable selection"], "reference": "We are grateful to Adam Smith and Thomas Steinke for many helpful discussions about tracing attacks and private top-k selection.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approxi-  mate differential privacy. In RANDOM, 2013.  Raghav Bhaskar, Srivatsan Laxman, Adam Smith, and Abhradeep Thakurta. Discovering frequent patterns in sensitive data. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 503-512. ACM, 2010.  Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the SuLQ  framework. In PODS, 2005.  Dan Boneh and James Shaw. Collusion-secure fingerprinting for digital data. IEEE Transactions on  Information Theory, 44(5):1897-1905, 1998.  13   which means that,  E[xa  (cid:12) (cid:12) B] \u2265 \u03bb = E[xa].  Since P [xa = \u22121] = (1 \u2212 E[xa])/2, we get that, P (cid:2)xa = \u22121 (cid:12) this back into (4), we get that the variables are positively correlated. (cid:105)  We have that, E  (cid:104)(cid:80)  i\u2208R,j\u2208v xij  = |R|k\u03bb, and since Hoeffding\u2019s inequality applies equally well  (cid:12) B(cid:3) \u2264 P [xa = \u22121]. Substituting  to positively-correlated random variables Panconesi and Srinivasan (1997), we also have,  P (cid:101)XS  (cid:88)  i\u2208R,j\u2208v  xij \u2264 |R|\u03c4c  \uf8f9  \uf8fb  \uf8ee  \uf8f0  \uf8ee  \uf8f0  \u2264 P (cid:101)XS  (cid:88)  i\u2208R,j\u2208V \uf8eb  (cid:16)  \u2264 exp  \uf8ec \uf8ed\u2212  |R|(cid:112)2k ln(1/\u03c1)  (cid:17)2  2|R|k  \uf8f6 \uf8f8 = \u03c1|R|. \uf8f7  \uf8f9 xij < |R|k\u03bb \u2212 |R|(cid:112)2k ln(1/\u03c1) \uf8fb  Substituting this in equation 2, we get that, P (cid:101)XS  (cid:86) i\u2208R our definition of the distributions, XS, (cid:101)XS, we have, P XS  Ei  \u2264 \u03c1|R|. Finally, we use the fact that, by  (cid:2)(cid:86)  i\u2208R Ei  (cid:3) \u2264 P (cid:101)XS  (cid:2)(cid:86)  i\u2208R Ei  (cid:3) \u2264 \u03c1|R|. This  (cid:20)  (cid:21)  completes the proof.  Acknowledgments  References  We are grateful to Adam Smith and Thomas Steinke for many helpful discussions about tracing attacks and private top-k selection.  Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 1046-1059. ACM, 2016.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approxi-  mate differential privacy. In RANDOM, 2013.  Raghav Bhaskar, Srivatsan Laxman, Adam Smith, and Abhradeep Thakurta. Discovering frequent patterns in sensitive data. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 503-512. ACM, 2010.  Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the SuLQ  framework. In PODS, 2005.  Dan Boneh and James Shaw. Collusion-secure fingerprinting for digital data. IEEE Transactions on  Information Theory, 44(5):1897-1905, 1998. MITALI.BAFNA@GMAIL.COM  JULLMAN@CCS.NEU.EDU  Mark Bun, Jonathan Ullman, and Salil P. Vadhan. Fingerprinting codes and the price of approximate  differential privacy. In STOC, 2014.  Kamalika Chaudhuri, Daniel J Hsu, and Shuang Song. The large margin mechanism for differentially private maximization. In Advances in Neural Information Processing Systems, pages 1287-1295, 2014.  Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In PODS, 2003.  Cynthia Dwork and Jing Lei. Differential privacy and robust statistics.  In Proceedings of the  forty-first annual ACM symposium on Theory of computing, pages 371-380. ACM, 2009.  Cynthia Dwork and Kobbi Nissim. Privacy-preserving datamining on vertically partitioned databases.  In CRYPTO, 2004.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in  private data analysis. In TCC, 2006.  Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 715-724. ACM, 2010a.  Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In FOCS.  IEEE, 2010b.  Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations  and Trends R(cid:13) in Theoretical Computer Science, 9(3-4):211-407, 2014.  Cynthia Dwork, Adam D. Smith, Thomas Steinke, Jonathan Ullman, and Salil P. Vadhan. Robust  traceability from trace amounts. In FOCS, 2015a.  Cynthia Dwork, Weijie Su, and Li Zhang. Private false discovery rate control. arXiv preprint  arXiv:1511.03803, 2015b.  Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! A survey of attacks  on private data. Annual Review of Statistics and Its Application, 2017.  Anupam Gupta, Katrina Ligett, Frank McSherry, Aaron Roth, and Kunal Talwar. Differentially private combinatorial optimization. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 1106-1125. Society for Industrial and Applied Mathematics, 2010.  Anupam Gupta, Aaron Roth, and Jonathan Ullman. Iterative constructions and private data release.  In TCC, 2012.  data analysis. In FOCS, 2010.  Moritz Hardt and Guy N. Rothblum. A multiplicative weights mechanism for privacy-preserving  Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig. Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLoS genetics, 4(8):e1000167, 2008. Prateek Jain and Abhradeep Thakurta. Mirror descent based database privacy. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 579-590. Springer, 2012.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.  Springer Science & Business Media, 2013.  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Foundations of Computer Science, 2007. FOCS\u201907. 48th Annual IEEE Symposium on, pages 94-103. IEEE, 2007.  Alessandro Panconesi and Aravind Srinivasan. Randomized distributed edge coloring via an extension  of the chernoff-hoeffding bounds. SIAM Journal on Computing, 26(2):350-368, 1997.  Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism. In STOC, pages  765-774. ACM, June 5-8 2010.  Sriram Sankararaman, Guillaume Obozinski, Michael I Jordan, and Eran Halperin. Genomic privacy  and limits of individual detection in a pool. Nature genetics, 41(9):965-967, 2009.  Adam Smith and Abhradeep Thakurta. Differentially private model selection via stability arguments  and the robustness of the lasso. J Mach Learn Res Proc Track, 30:819-850, 2013.  Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. Journal  of Privacy and Confidentiality, 7, 2017a.  Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially private selection. arXiv  preprint arXiv:1704.03024, 2017b.  G\u00e1bor Tardos. Optimal probabilistic fingerprint codes. Journal of the ACM (JACM), 55(2):10, 2008.  Jonathan Ullman. Answering n2+o(1) counting queries with differential privacy is hard. In STOC,  2013.  Jonathan Ullman. Private multiplicative weights beyond linear queries. In PODS, 2015.  "}, "Computationally Efficient Robust Sparse Estimation in High Dimensions": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Computationally Efficient Robust Sparse Estimation in High Dimensions", "abstract": "Many conventional statistical procedures are extremely sensitive to seemingly minor deviations from modeling assumptions. This problem is exacerbated in modern high-dimensional settings, where the problem dimension can grow with and possibly exceed the sample size.  We consider the problem of robust estimation of sparse functionals, and  provide a computationally and statistically efficient algorithm in the high-dimensional setting. Our theory identifies a unified set of deterministic conditions under which our algorithm guarantees accurate recovery.  By further establishing that these deterministic conditions hold with high-probability for a wide range of statistical models, our theory applies to many problems of considerable interest including sparse mean and covariance estimation; sparse linear regression; and sparse generalized linear models. In certain settings, such as the detection and estimation of sparse principal components in  the spiked covariance model, our general theory does not yield optimal sample complexity, and we provide a novel algorithm based on the same intuition which is able to take advantage of further structure of the problem to achieve nearly optimal rates.", "pdf_url": "http://proceedings.mlr.press/v65/balakrishnan17a/balakrishnan17a.pdf", "keywords": ["Robustness", "sparsity", "linear regression", "covariance estimation", "sparse principal components analysis (PCA)", "generalized linear models", "logistic regression"], "reference": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density esti- mation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1278-1289. SIAM, 2017.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013a.  Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse pca. arXiv preprint  arXiv:1304.0828, 2013b.  Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Jacob Bien and Robert J Tibshirani. Sparse estimation of a covariance matrix. Biometrika, 98(4):  807, 2011.  13   COMPUTATIONALLY EFFICIENT ROBUST SPARSE ESTIMATION IN HIGH DIMENSIONS  Algorithm 4 Robust estimation of the top principal component  1: Input: samples x1, . . . , xn, error rate (cid:15), failure probability \u03b4, signal to noise ratio \u03c1 2: Let w\u2217, A\u2217 be the solution to  argminw\u2208Sn,(cid:15),A\u2208Xk  wi(xixT  i \u2212 I) \u2212 \u03c1A  (22)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  n (cid:88)  i=1  \u2217  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  W2k  3: Let u be the top eigenector of A\u2217 4: return Pk(v)  6. Conclusion and Future Directions  In this paper we propose a computationally tractable robust algorithm for sparse high-dimensional statistical estimation problems. We develop a general result, which we then specialize to obtain corollaries for sparse mean/covariance estimation, sparse linear regression and sparse generalized linear models. In each of these problems, we obtain near optimal dependency on the contamination parameter, and sample complexities that depend only logarithmically on the ambient dimension.  Future directions of research include developing faster alternatives to the ellipsoid algorithm, to further relax the assumptions in various settings, and finally to close the gap in sample complexity to statistically optimal, albeit computationally intractable procedures (Chen et al., 2015, 2016).  Acknowledgments  S.B. was supported by the NSF grant DMS-1713003. S.S.D was supported by ARPA-E under grant in the the Terra program (Transportation Energy Resources from Renewable Agriculture) with title Automated TERRA Phenotyping System for Genetic Improvement of Energy Crops. J.L. was supported by NSF grant CCF-1217921, DOE grant DE-SC0008923, NSF CAREER Award CCF- 145326, and a NSF Graduate Research Fellowship. The authors would like to thank Michael Cohen, Ilias Diakonikolas, Ankur Moitra, Ludwig Schmidt and Martin Wainwright for useful discussions. A.S. was supported by the NSF CAREER grant IIS-1252412 and CCF-1563918.  References  Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density esti- mation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1278-1289. SIAM, 2017.  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. The Annals of Statistics, 41(4):1780-1815, 2013a.  Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse pca. arXiv preprint  arXiv:1304.0828, 2013b.  Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding.  In  Advances in Neural Information Processing Systems, pages 721-729, 2015.  Jacob Bien and Robert J Tibshirani. Sparse estimation of a covariance matrix. Biometrika, 98(4):  807, 2011. BALAKRISHNAN DU LI SINGH  Siu-On Chan, Ilias Diakonikolas, Xiaorui Sun, and Rocco A Servedio. Learning mixtures of struc- In Proceedings of the Twenty-Fourth Annual ACM-  tured distributions over discrete domains. SIAM Symposium on Discrete Algorithms, pages 1380-1394. SIAM, 2013.  Siu On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Near-optimal density esti- mation in near-linear time using variable-width histograms. In Advances in Neural Information Processing Systems, pages 1844-1852, 2014.  Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. arXiv  preprint arXiv:1611.02315, 2016.  Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance matrix estimation via matrix depth.  arXiv preprint arXiv:1506.00691, 2015.  Mengjie Chen, Chao Gao, and Zhao Ren. A general decision theory for huber\u2019s epsilon-  contamination model. Electronic Journal of Statistics, 10(2):3752-3774, 2016.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A Servedio. Learning k-modal distribu- tions via testing. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 1371-1385. Society for Industrial and Applied Mathematics, 2012.  Alexandre d\u2019Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert RG Lanckriet. A direct formulation for sparse pca using semidefinite programming. SIAM review, 49(3):434-448, 2007.  Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. arXiv preprint arXiv:1604.06443, 2016a.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Efficient robust proper learning of log-  concave distributions. arXiv preprint arXiv:1606.03077, 2016b.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart.  Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. arXiv preprint arXiv:1611.03473, 2016c.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. Geometric algorithms and combinato-  rial optimization, volume 2. Springer Science & Business Media, 2012.  Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics:  the approach based on in\ufb02uence functions, volume 114. John Wiley & Sons, 2011.  Cecil Hastings Jr, Frederick Mosteller, John W Tukey, and Charles P Winsor. Low moments for small samples: a comparative study of order statistics. The Annals of Mathematical Statistics, pages 413-426, 1947.  Peter J Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics,  35(1):73-101, 1964.  36(6):1753-1758, 1965.  Peter J Huber. A robust version of the probability ratio test. The Annals of Mathematical Statistics, COMPUTATIONALLY EFFICIENT ROBUST SPARSE ESTIMATION IN HIGH DIMENSIONS  Peter J Huber. Robust statistics. Springer, 2011.  Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In Advances in Neural Information Processing Systems, pages 685-693, 2014.  Iain M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis.  Ann. Statist., 29(2):295-327, 04 2001.  Michael Kearns. Efficient noise-tolerant learning from statistical queries. J. ACM, 45(6):983-1006,  1998.  Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance.  arXiv preprint arXiv:1604.06968, 2016.  Malik Magdon-Ismail. Np-hardness and inapproximability of sparse pca.  arXiv preprint  arXiv:1502.05675, 2015.  Michael L Overton and Robert S Womersley. On the sum of the largest eigenvalues of a symmetric  matrix. SIAM Journal on Matrix Analysis and Applications, 13(1):41-45, 1992.  Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and arXiv preprint  sub-optimality of pca for spiked random matrices and synchronization. arXiv:1609.05573, 2016.  Charles Stein. Dependent random variables. 1971.  Andreas M Tillmann and Marc E Pfetsch. The computational complexity of the restricted isometry property, the nullspace property, and related concepts in compressed sensing. IEEE Transactions on Information Theory, 60(2):1248-1259, 2014.  John W Tukey. Mathematics and the picturing of data. In Proceedings of the international congress  of mathematicians, volume 2, pages 523-531, 1975.  Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe. Fantope projection and selection: A near- optimal convex relaxation of sparse pca. In Advances in neural information processing systems, pages 2670-2678, 2013.  Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896-1930, 2016.  Yannis G Yatracos. Rates of convergence of minimum distance estimators and kolmogorov\u2019s en-  tropy. The Annals of Statistics, 13(2):768-774, 1985. BALAKRISHNAN DU LI SINGH  "}, "Learning-Theoretic Foundations of Algorithm Configuration for Combinatorial Partitioning Problems": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Learning-Theoretic Foundations of Algorithm Configuration for Combinatorial Partitioning Problems", "abstract": "Max-cut, clustering, and many other partitioning problems that are of significant importance to machine learning and other scientific fields are NP-hard, a reality that has motivated researchers to develop a wealth of approximation algorithms and heuristics. Although the best algorithm to use typically depends on the specific application domain, a worst-case analysis is often used to compare algorithms. This may be misleading if worst-case instances occur infrequently, and thus there is a demand for optimization methods which return the algorithm configuration best suited for the given application\u2019s typical inputs. Recently, Gupta and Roughgarden introduced the first learning-theoretic framework to rigorously study this problem, using it to analyze classes of greedy heuristics, parameter tuning in gradient descent, and other problems. We study this algorithm configuration problem for clustering, max-cut, and other partitioning problems, such as integer quadratic programming, by designing computationally efficient and sample efficient learning algorithms which receive samples from an application-specific distribution over problem instances and learn a partitioning algorithm with high expected performance. Our algorithms learn over common integer quadratic programming and clustering algorithm families: SDP rounding algorithms and agglomerative clustering algorithms with dynamic programming. For our sample complexity analysis, we provide tight bounds on the pseudodimension of these algorithm classes,  and show that surprisingly, even for classes of algorithms parameterized by a single parameter, the pseudo-dimension is superconstant. In this way, our work both contributes to the foundations of algorithm configuration and pushes the boundaries of learning theory, since the algorithm classes we analyze consist of multi-stage optimization procedures and are significantly more complex than classes typically studied in learning theory.", "pdf_url": "http://proceedings.mlr.press/v65/balcan17a/balcan17a.pdf", "keywords": ["algorithm configuration", "integer quadratic programming", "max-cut", "clustering", "center based objectives", "computationally efficient and sample efficient meta-algorithms"], "reference": "Noga Alon, Konstantin Makarychev, Yury Makarychev, and Assaf Naor. Quadratic forms on graphs.  Inventiones mathematicae, 163(3):499-522, 2006.  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge  University Press, 2009.  Pranjal Awasthi, Avrim Blum, and Or Sheffet. Center-based clustering under perturbation stability.  Information Processing Letters, 112(1):49-54, 2012.  Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. Local algorithms for interactive clustering. In Proceedings of the International Conference on Machine Learning (ICML), pages 550-558, 2014.  Maria-Florina Balcan and Yingyu Liang. Clustering under perturbation resilience. SIAM Journal  on Computing, 45(1):102-155, 2016.  Maria-Florina Balcan, Nika Haghtalab, and Colin White. k-center clustering under perturbation resilience. In Proceedings of the Annual International Colloquium on Automata, Languages, and Programming (ICALP), 2016.  Afonso S. Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Proceedings of the Conference on Learning Theory (COLT), pages 361-382, 2016.  MohammadHossein Bateni, Aditya Bhaskara, Silvio Lattanzi, and Vahab Mirrokni. Distributed balanced clustering via mapping coresets. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 2591-2599, 2014.  Ahron Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algo-  rithms, and engineering applications, volume 2. Siam, 2001.  William Brendel and Sinisa Todorovic. Segmentation as maximum-weight independent set.  In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 307-315, 2010.  17   LEARNING-THEORETIC FOUNDATIONS OF ALGORITHM CONFIGURATION  the exact empirical cost of an algorithm on the training set. Could we still make strong, provable guarantees for application-specific algorithm configuration in this scenario? This work also leaves open the potential for data-dependent bounds over well-behaved distributions, such as those over clustering instances satisfying some form of stability, be it approximation stability, perturbation resilience, or so on.  Acknowledgments. This work was supported in part by grants NSF-CCF 1535967, NSF CCF- 1422910, NSF IIS-1618714, a Sloan Fellowship, a Microsoft Research Fellowship, a NSF Graduate Research Fellowship, a Microsoft Research Women\u2019s Fellowship, and a National Defense Science and Engineering Graduate (NDSEG) fellowship.  We thank Sanjoy Dasgupta, Anupam Gupta, Carl Kingsford, and Ryan O\u2019Donnell for useful  discussions.  References  Noga Alon, Konstantin Makarychev, Yury Makarychev, and Assaf Naor. Quadratic forms on graphs.  Inventiones mathematicae, 163(3):499-522, 2006.  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge  University Press, 2009.  Pranjal Awasthi, Avrim Blum, and Or Sheffet. Center-based clustering under perturbation stability.  Information Processing Letters, 112(1):49-54, 2012.  Pranjal Awasthi, Maria-Florina Balcan, and Konstantin Voevodski. Local algorithms for interactive clustering. In Proceedings of the International Conference on Machine Learning (ICML), pages 550-558, 2014.  Maria-Florina Balcan and Yingyu Liang. Clustering under perturbation resilience. SIAM Journal  on Computing, 45(1):102-155, 2016.  Maria-Florina Balcan, Nika Haghtalab, and Colin White. k-center clustering under perturbation resilience. In Proceedings of the Annual International Colloquium on Automata, Languages, and Programming (ICALP), 2016.  Afonso S. Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In Proceedings of the Conference on Learning Theory (COLT), pages 361-382, 2016.  MohammadHossein Bateni, Aditya Bhaskara, Silvio Lattanzi, and Vahab Mirrokni. Distributed balanced clustering via mapping coresets. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 2591-2599, 2014.  Ahron Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algo-  rithms, and engineering applications, volume 2. Siam, 2001.  William Brendel and Sinisa Todorovic. Segmentation as maximum-weight independent set.  In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 307-315, 2010. BALCAN NAGARAJAN VITERCIK WHITE  Fazli Can. Incremental clustering for dynamic information processing. ACM Transactions on In-  formation Systems (TOIS), 11(2):143-164, 1993.  Yves Caseau, Franc\u00b8ois Laburthe, and Glenn Silverstein. A meta-heuristic factory for vehicle routing problems. In International Conference on Principles and Practice of Constraint Programming (CP), pages 144-158. Springer, 1999.  Moses Charikar and Anthony Wirth. Maximizing quadratic programs: extending Grothendieck\u2019s in- equality. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), pages 54-60, 2004.  Moses Charikar, Chandra Chekuri, Tom\u00b4as Feder, and Rajeev Motwani. Incremental clustering and dynamic information retrieval. In Proceedings of the Annual Symposium on Theory of Computing (STOC), pages 626-635, 1997.  Timothee Cour, Praveen Srinivasan, and Jianbo Shi. Balanced graph matching. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 313-320, 2006.  Jim Demmel, Jack Dongarra, Victor Eijkhout, Erika Fuentes, Antoine Petitet, Rich Vuduc, R Clint Whaley, and Katherine Yelick. Self-adapting linear algebra algorithms and software. Proceedings of the IEEE, 93(2):293-312, 2005.  R.M Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes.  Journal of Functional Analysis, 1(3):290 - 330, 1967.  Uriel Feige and Michael Langberg. The RPR2 rounding technique for semidefinite programs. Jour-  nal of Algorithms, 60(1):1-23, 2006.  Darya Filippova, Aashish Gadani, and Carl Kingsford. Coral: an integrated suite of visualizations  for comparing clusterings. BMC bioinformatics, 13(1):276, 2012.  Roy Frostig, Sida Wang, Percy S Liang, and Christopher D Manning. Simple MAP inference via low-rank relaxations. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 3077-3085, 2014.  Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42 (6):1115-1145, 1995.  Anna Grosswendt and Heiko Roeglin. Improved analysis of complete linkage clustering. In Euro-  pean Symposium of Algorithms, volume 23, pages 656-667. Springer, 2015.  Anupam Gupta and Kanat Tangwongsan. Simpler analyses of local search algorithms for facility  location. arXiv preprint arXiv:0809.2554, 2008.  Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science (ITCS), pages 123-134, 2016. LEARNING-THEORETIC FOUNDATIONS OF ALGORITHM CONFIGURATION  Qixing Huang, Yuxin Chen, and Leonidas Guibas. Scalable semidefinite relaxation for maximum In Proceedings of the International Conference on Machine Learning  a posterior estimation. (ICML), pages 64-72, 2014.  Fredrik D Johansson, Ankani Chattoraj, Chiranjib Bhattacharyya, and Devdatt Dubhashi. Weighted theta functions and embeddings with applications to max-cut, clustering and summarization. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 1018-1026, 2015.  Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O\u2019Donnell. Optimal inapproximability results for MAX-CUT and other 2-variable CSPs? SIAM Journal on Computing, 37(1):319-357, 2007.  Kevin Leyton-Brown, Eugene Nudelman, and Yoav Shoham. Empirical hardness models: Method- ology and a case study on combinatorial auctions. Journal of the ACM (JACM), 56(4):22, 2009.  Marina Meil\u02d8a. Comparing clusterings: an information based distance. Journal of multivariate  analysis, 98(5):873-895, 2007.  Ryan O\u2019Donnell and Yi Wu. An optimal SDP algorithm for max-cut, and equally optimal long code tests. In Proceedings of the Annual Symposium on Theory of Computing (STOC), pages 335-344, 2008.  David Pollard. Convergence of stochastic processes. Springer-Verlag, 1984.  David Pollard. Empirical processes. Institute of Mathematical Statistics, 1990.  John R Rice. The algorithm selection problem. Advances in computers, 15:65-118, 1976.  Andrej Risteski and Yuanzhi Li. Approximate maximum entropy principles via Goemans- In Proceedings of the Annual  Williamson with applications to provable variational methods. Conference on Neural Information Processing Systems (NIPS), pages 4628-4636, 2016.  Mehreen Saeed, Onaiza Maqbool, Haroon Atique Babri, Syed Zahoor Hassan, and S Mansoor Sarwar. Software clustering techniques and the use of combined algorithm. In Proceedings of the European Conference on Software Maintenance and Reengineering, pages 301-306. IEEE, 2003.  Sagi Snir and Satish Rao. Using max cut to enhance rooted trees consistency. IEEE/ACM Transac-  tions on Computational Biology and Bioinformatics (TCBB), 3(4):323-333, 2006.  Timo Tossavainen. On the zeros of finite sums of exponential functions. Australian Mathematical  Society Gazette, 33(1):47-50, 2006.  Vijay V Vazirani. Approximation algorithms. Springer Science & Business Media, 2013.  Jun Wang, Tony Jebara, and Shih-Fu Chang. Semi-supervised learning using greedy max-cut. Jour-  nal of Machine Learning Research, 14(Mar):771-800, 2013. BALCAN NAGARAJAN VITERCIK WHITE  James R White, Saket Navlakha, Niranjan Nagarajan, Mohammad-Reza Ghodsi, Carl Kingsford, and Mihai Pop. Alignment and clustering of phylogenetic markers-implications for microbial diversity studies. BMC bioinformatics, 11(1):152, 2010.  David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge  University press, 2011.  Lin Xu, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. SATzilla: portfolio-based algo- rithm selection for SAT. Journal of Artificial Intelligence Research, 32:565-606, June 2008.  Chihiro Yoshimura, Masanao Yamaoka, Masato Hayashi, Takuya Okuyama, Hidetaka Aoki, Ken- ichi Kawarabayashi, and Hiroyuki Mizuno. Uncertain behaviours of integrated circuits improve computational performance. Scientific reports, 5, 2015.  Mingjun Zhong, Nigel Goddard, and Charles Sutton. Signal aggregate constraints in additive facto- rial HMMs, with application to energy disaggregation. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 3590-3598, 2014.  Uri Zwick. Outward rotations: a tool for rounding solutions of semidefinite programming relax- ations, with applications to max cut and other problems. In Proceedings of the Annual Symposium on Theory of Computing (STOC), pages 679-687, 1999.  "}, "The Sample Complexity of Optimizing a Convex Function": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "The Sample Complexity of Optimizing a Convex Function", "abstract": "In this paper we study optimization from samples of convex functions. There are many scenarios in which we do not know the function we wish to optimize but can learn it from data.  In such cases,  we are interested in bounding the number of samples required to optimize the function.   Our main result shows that in general,  the number of samples required to obtain a non-trivial approximation to the optimum of a convex function is exponential in its dimension, even when the function is PAC-learnable. We also obtain strong lower bounds for strongly convex and Lipschitz continuous functions. On the positive side, we show that there are interesting classes of functions and distributions for which the sample complexity is polynomial in the dimension of the function.", "pdf_url": "http://proceedings.mlr.press/v65/balkanski17a/balkanski17a.pdf", "keywords": ["Convex optimization", "PAC learning", "sample complexity"], "reference": "Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with  multi-point bandit feedback. In COLT, pages 28-40. Citeseer, 2010.  Maria-Florina Balcan and Nicholas JA Harvey. Learning submodular functions. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 793-802. ACM, 2011.  Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The power of optimization from samples. In  Advances in Neural Information Processing Systems, pages 4017-4025, 2016.  Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The limitations of optimization from samples. Proceedings of the Forty-Ninth Annual ACM on Symposium on Theory of Computing, 2017.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the In local minima via simulated annealing: Optimization of approximately convex functions. COLT, pages 240-265, 2015.  Michael Ben-Or and Prasoon Tiwari. A deterministic algorithm for sparse multivariate polynomial interpolation. In Proceedings of the twentieth annual ACM symposium on Theory of computing, pages 301-309. ACM, 1988.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. CoRR, abs/1507.06580, 2015.  Carl De Boor and Amos Ron. On multivariate polynomial interpolation. Constructive Approxima-  tion, 6(3):287-302, 1990.  Alina Ene, Jan Vondr\u00b4ak, and Yi Wu. Local distribution and the symmetry gap: Approximability In Proceedings of the Twenty-Fourth Annual ACM-SIAM  of multiway partitioning problems. Symposium on Discrete Algorithms, pages 306-325. SIAM, 2013.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems, pages 3576-3584, 2016.  Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals.  In COLT, pages 679-702, 2014.  Vitaly Feldman, Pravesh Kothari, and Jan Vondr\u00b4ak. Representation, approximation and learning of  submodular functions using low-rank decision trees. In COLT, pages 711-740, 2013.  Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth an- nual ACM-SIAM symposium on Discrete algorithms, pages 385-394. Society for Industrial and Applied Mathematics, 2005.  Mariano Gasca. Multivariate polynomial interpolation.  In Computation of curves and surfaces,  pages 215-236. Springer, 1990.  arXiv:1601.03095, 2016.  Avinatan Hassidim and Yaron Singer. Submodular optimization under noise. arXiv preprint  13   THE SAMPLE COMPLEXITY OF OPTIMIZING A CONVEX FUNCTION  References  Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with  multi-point bandit feedback. In COLT, pages 28-40. Citeseer, 2010.  Maria-Florina Balcan and Nicholas JA Harvey. Learning submodular functions. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 793-802. ACM, 2011.  Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The power of optimization from samples. In  Advances in Neural Information Processing Systems, pages 4017-4025, 2016.  Eric Balkanski, Aviad Rubinstein, and Yaron Singer. The limitations of optimization from samples. Proceedings of the Forty-Ninth Annual ACM on Symposium on Theory of Computing, 2017.  Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the In local minima via simulated annealing: Optimization of approximately convex functions. COLT, pages 240-265, 2015.  Michael Ben-Or and Prasoon Tiwari. A deterministic algorithm for sparse multivariate polynomial interpolation. In Proceedings of the twentieth annual ACM symposium on Theory of computing, pages 301-309. ACM, 1988.  S\u00b4ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex  optimization. CoRR, abs/1507.06580, 2015.  Carl De Boor and Amos Ron. On multivariate polynomial interpolation. Constructive Approxima-  tion, 6(3):287-302, 1990.  Alina Ene, Jan Vondr\u00b4ak, and Yi Wu. Local distribution and the symmetry gap: Approximability In Proceedings of the Twenty-Fourth Annual ACM-SIAM  of multiway partitioning problems. Symposium on Discrete Algorithms, pages 306-325. SIAM, 2013.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. In Advances in Neural Information Processing Systems, pages 3576-3584, 2016.  Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals.  In COLT, pages 679-702, 2014.  Vitaly Feldman, Pravesh Kothari, and Jan Vondr\u00b4ak. Representation, approximation and learning of  submodular functions using low-rank decision trees. In COLT, pages 711-740, 2013.  Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth an- nual ACM-SIAM symposium on Discrete algorithms, pages 385-394. Society for Industrial and Applied Mathematics, 2005.  Mariano Gasca. Multivariate polynomial interpolation.  In Computation of curves and surfaces,  pages 215-236. Springer, 1990.  arXiv:1601.03095, 2016.  Avinatan Hassidim and Yaron Singer. Submodular optimization under noise. arXiv preprint BALKANSKI SINGER  Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,  2(3-4):157-325, 2015.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Vahab Mirrokni, Michael Schapira, and Jan Vondr\u00b4ak. Tight information-theoretic lower bounds for welfare maximization in combinatorial auctions. In Proceedings of the 9th ACM conference on Electronic commerce, pages 70-77. ACM, 2008.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge university press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In COLT, 2009.  Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and  Trends R(cid:13) in Machine Learning, 4(2):107-194, 2012.  Yaron Singer and Jan Vondr\u00b4ak. Information-theoretic lower bounds for convex optimization with erroneous oracles. In Advances in Neural Information Processing Systems, pages 3204-3212, 2015.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Jan Vondr\u00b4ak. Symmetry and approximability of submodular maximization problems. SIAM Journal  on Computing, 42(1):265-304, 2013. THE SAMPLE COMPLEXITY OF OPTIMIZING A CONVEX FUNCTION  "}, "Efficient Co-Training of Linear Separators under Weak Dependence": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Efficient Co-Training of Linear Separators under Weak Dependence", "abstract": "We develop the first polynomial-time algorithm for co-training of homogeneous linear separators under \\em weak dependence, a relaxation of the condition of independence given the label. Our algorithm learns from purely unlabeled data, except for a single labeled example to break symmetry of the two classes, and works for any data distribution having an inverse-polynomial margin and with center of mass at the origin.", "pdf_url": "http://proceedings.mlr.press/v65/blum17a/blum17a.pdf", "keywords": ["co-training", "unsupervised learning", "linear classifier"], "reference": "S. Abney. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Com-  putational Linguistics (ACL), pages 360-367, 2002.  M.-F. Balcan and A. Blum. A discriminative model for semi-supervised learning. JACM, 57(3),  2010. Article 19.  A. Blum and T. M. Mitchell. Combining labeled and unlabeled data with co-training. In Proc. 11th  Annual Conf. Computational Learning Theory, pages 92-100, 1998.  Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, and Karthik Sridharan. Multi-view cluster- ing via canonical correlation analysis. In Proceedings of the 26th Annual International Confer- ence on Machine Learning, ICML \u201909, pages 129-136, 2009. ISBN 978-1-60558-516-1.  Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for co-  training. In NIPS, volume 1, pages 375-382, 2001.  Ngoc Quynh Do Thi, Steven Bethard, and Marie-Francine Moens. Facing the most difficult case of semantic role labeling: A collaboration of word embeddings and co-training. In Proceedings of the 26th International Conference on Computational Linguistics. ACL, 2016.  Svetlana Kiritchenko and Stan Matwin. Email classification with co-training. In Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research, CASCON \u201911, pages 301-312, 2011.  Abhishek Kumar and Hal Daum\u00b4e. A co-training approach for multi-view spectral clustering. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 393- 400, 2011.  Anat Levin, Paul A Viola, and Yoav Freund. Unsupervised improvement of visual detectors using  co-training. In ICCV, pages 626-633, 2003.  Weifeng Liu, Yang Li, Xu Lin, Dacheng Tao, and Yanjiang Wang. Hessian-regularized co-training  for social activity recognition. PloS one, 9(9):e108474, 2014.  Robert H. Sloan. Four types of noise in data for PAC learning. Information Processing Letters, 54  (3):157-162, 1995.  Xiaojun Wan. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-volume 1, pages 235-243. Association for Computational Linguistics, 2009.  15   EFFICIENT CO-TRAINING OF LINEAR SEPARATORS UNDER WEAK DEPENDENCE  References  S. Abney. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Com-  putational Linguistics (ACL), pages 360-367, 2002.  M.-F. Balcan and A. Blum. A discriminative model for semi-supervised learning. JACM, 57(3),  2010. Article 19.  A. Blum and T. M. Mitchell. Combining labeled and unlabeled data with co-training. In Proc. 11th  Annual Conf. Computational Learning Theory, pages 92-100, 1998.  Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, and Karthik Sridharan. Multi-view cluster- ing via canonical correlation analysis. In Proceedings of the 26th Annual International Confer- ence on Machine Learning, ICML \u201909, pages 129-136, 2009. ISBN 978-1-60558-516-1.  Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for co-  training. In NIPS, volume 1, pages 375-382, 2001.  Ngoc Quynh Do Thi, Steven Bethard, and Marie-Francine Moens. Facing the most difficult case of semantic role labeling: A collaboration of word embeddings and co-training. In Proceedings of the 26th International Conference on Computational Linguistics. ACL, 2016.  Svetlana Kiritchenko and Stan Matwin. Email classification with co-training. In Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research, CASCON \u201911, pages 301-312, 2011.  Abhishek Kumar and Hal Daum\u00b4e. A co-training approach for multi-view spectral clustering. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 393- 400, 2011.  Anat Levin, Paul A Viola, and Yoav Freund. Unsupervised improvement of visual detectors using  co-training. In ICCV, pages 626-633, 2003.  Weifeng Liu, Yang Li, Xu Lin, Dacheng Tao, and Yanjiang Wang. Hessian-regularized co-training  for social activity recognition. PloS one, 9(9):e108474, 2014.  Robert H. Sloan. Four types of noise in data for PAC learning. Information Processing Letters, 54  (3):157-162, 1995.  Xiaojun Wan. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-volume 1, pages 235-243. Association for Computational Linguistics, 2009. BLUM MANSOUR  "}, "Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo", "abstract": "This paper presents a detailed theoretical analysis of the Langevin Monte Carlo sampling algorithm recently introduced in Durmus et al. (Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau, 2016) when applied to log-concave probability distributions that are restricted to a convex body $K$. This method relies on a regularisation procedure involving the Moreau-Yosida envelope of the indicator function associated with $K$. Explicit convergence bounds in total variation norm and in Wasserstein distance of order $1$ are established. In particular, we show that the complexity of this algorithm given a first order oracle is polynomial in the dimension of the state space. Finally, some numerical experiments are presented to compare our method with competing MCMC approaches from the literature.", "pdf_url": "http://proceedings.mlr.press/v65/brosse17a/brosse17a.pdf", "keywords": ["Markov chain Monte Carlo methods", "Langevin Algorithm", "Bayesian inference", "convex body"], "reference": "The authors wish to express their thanks to the anonymous referees for several helpful remarks, in particular concerning a simplified proof of Proposition 4.  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine  Learning research, 3(Jan):993-1022, 2003.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte carlo. In Proceedings of the 28th International Conference on Neural Information Pro- cessing Systems, NIPS\u201915, pages 1243-1251, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969378.  Gilles Celeux, Mohammed El Anbari, Jean-Michel Marin, and Christian P. Robert. Regularization in regression: Comparing bayesian and frequentist methods in a poorly informative situation. Bayesian Anal., 7(2):477-502, 06 2012. doi: 10.1214/12-BA716. URL http://dx.doi. org/10.1214/12-BA716.  21   SAMPLING FROM A LOG-CONCAVE DISTRIBUTION WITH COMPACT SUPPORT  Using (cid:0)d i  (cid:1)\u0393((3 + i)/2) \u2264 (  \u03c0/2)di for i \u2208 {0, . . . , d}, we have for \u03bb \u2208 (cid:0)0, 16\u22121r2d\u22122(cid:3),  \u221a  \u221a  \u221a  C2 \u2264 2  \u03c0\u03bbe4\u03bb(\u22062/r)2  \u2264 4  \u03c0\u03bbe4\u03bb(\u22062/r)2  .  \u221a  d (cid:88)  (cid:32)(cid:33)i  \u03bbd r  i=0  D defined in (47) is upper bounded by RB where B is defined in Section 6.3-c). Combining the bounds on C1, C2, D gives the result.  6.5. Proof of Proposition 7 Assume that \u03b3 \u2208 (cid:0)0, (m + L)\u22121(cid:1). (Durmus and Moulines, 2016, Theorem 5) gives for all n \u2208 N(cid:63):  W 2  2 (\u03b4xRn  \u03b3 , \u03c0\u03bb) \u2264 2 (1 \u2212 (\u03ba\u03b3)/2)n (cid:110)  (cid:107)x \u2212 x(cid:63)(cid:107)2 + d/m  + u(\u03b3) ,  (cid:111)  where,  u(\u03b3) = 2\u03ba\u22121L2d\u03b3(\u03ba\u22121 + \u03b3)  2 +  (cid:18)  L2\u03b3 m  +  (cid:19)  .  L2\u03b32 6  Noting that \u03ba\u03b3 \u2264 1 and L2\u03b32 \u2264 1, it is then sufficient for \u03b3, n to satisfy,  4\u03ba\u22122L2d\u03b3 2 (1 \u2212 (\u03ba\u03b3)/2)n (cid:110)  (cid:18)  +  2 +  L2\u03b3 1 6 m (cid:107)x \u2212 x(cid:63)(cid:107)2 + d/m  (cid:19)  (cid:111)  \u2264 \u03b52/2 ,  \u2264 \u03b52/2 ,  which concludes the proof.  Acknowledgments  References  The authors wish to express their thanks to the anonymous referees for several helpful remarks, in particular concerning a simplified proof of Proposition 4.  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine  Learning research, 3(Jan):993-1022, 2003.  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte carlo. In Proceedings of the 28th International Conference on Neural Information Pro- cessing Systems, NIPS\u201915, pages 1243-1251, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969378.  Gilles Celeux, Mohammed El Anbari, Jean-Michel Marin, and Christian P. Robert. Regularization in regression: Comparing bayesian and frequentist methods in a poorly informative situation. Bayesian Anal., 7(2):477-502, 06 2012. doi: 10.1214/12-BA716. URL http://dx.doi. org/10.1214/12-BA716. BROSSE DURMUS MOULINES PEREYRA  Ming-Hui Chen, Qi-Man Shao, and Joseph G Ibrahim. Monte Carlo methods in Bayesian compu-  tation. Springer Science & Business Media, 2012.  Ben Cousins and Santosh Vempala.  the volume of convex bodies, Jun 2015. URL http://fr.mathworks.com/matlabcentral/fileexchange/ 43596-volume-computation-of-convex-bodies.  Computation of  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.  A. Durmus and E. Moulines. Non-asymptotic convergence analysis for the Unadjusted Langevin  Algorithm. ArXiv e-prints, July 2015. Accepted for publication in Ann. Appl. Probab.  A. Durmus and E. Moulines. High-dimensional Bayesian inference via the Unadjusted Langevin  Algorithm. ArXiv e-prints, May 2016.  A. Durmus, E. Moulines, and M. Pereyra. Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau. ArXiv e-prints, December 2016. Accepted for publication in SIAM J. Imaging Sciences.  Martin Dyer and Alan Frieze. Computing the volume of convex bodies: a case where randomness  provably helps. Probabilistic combinatorics and its applications, 44:123-170, 1991.  A. E. Gelfand, A. F. Smith, and T.-M. Lee. Bayesian analysis of constrained parameter and truncated data problems using gibbs sampling. Journal of the American Statistical Association, 87(418): 523-532, 1992.  Valen E Johnson and James H Albert. Ordinal data modeling. Springer Science & Business Media,  2006.  Press, 1997.  J\u00a8urgen Kampf. On weighted parallel volumes. Beitr\u00a8age Algebra Geom, 50(2):495-519, 2009.  Daniel A Klain and Gian-Carlo Rota. Introduction to geometric probability. Cambridge University  John P Klein and Melvin L Moeschberger. Survival analysis: techniques for censored and truncated  data. Springer Science & Business Media, 2005.  S. Lan and B. Shahbaba. Sampling constrained probability distributions using Spherical Augmen-  tation. ArXiv e-prints, June 2015.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. Hit-and-run from a corner. SIAM Journal on Computing, 35 (4):985-1005, 2006. doi: 10.1137/S009753970544727X. URL http://dx.doi.org/10. 1137/S009753970544727X.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling al- ISSN 1042-9832. doi:  gorithms. Random Struct. Algorithms, 30(3):307-358, May 2007. 10.1002/rsa.v30:3. URL http://dx.doi.org/10.1002/rsa.v30:3. SAMPLING FROM A LOG-CONCAVE DISTRIBUTION WITH COMPACT SUPPORT  Balasubramanian Narasimhan and Steven G. Johnson. cubature: Adaptive Multivariate Integration over Hypercubes, 2016. URL https://CRAN.R-project.org/package=cubature. R package version 1.3-6.  John Paisley, David M Blei, and Michael I Jordan. Bayesian nonnegative matrix factorization In Handbook of Mixed Membership Models and Their  with stochastic variational inference. Applications, pages 205-224. Chapman and Hall/CRC, 2014.  Ari Pakman and Liam Paninski. Exact hamiltonian monte carlo for truncated multivariate gaussians.  Journal of Computational and Graphical Statistics, 23(2):518-542, 2014.  G. Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180:378-384, 1981.  T. Park and G. Casella. The Bayesian lasso. J. Amer. Statist. Assoc., 103(482):681-686, 2008. ISSN 0162-1459. doi: 10.1198/016214508000000337. URL http://dx.doi.org/10. 1198/016214508000000337.  G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their dis- crete approximations. Bernoulli, 2(4):341-363, 1996. ISSN 1350-7265. doi: 10.2307/3318418. URL http://dx.doi.org/10.2307/3318418.  R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer- Verlag, Berlin, 1998. ISBN 3-540-62772-3. doi: 10.1007/978-3-642-02431-3. URL http: //dx.doi.org/10.1007/978-3-642-02431-3.  Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.  Gabriel Rodriguez-Yam, Richard A Davis, and Louis L Scharf. Efficient gibbs sampling of truncated multivariate normal with application to constrained linear regression. Unpublished manuscript, 2004.  Rolf Schneider. Convex bodies: the Brunn-Minkowski theory. Number 151. Cambridge University  Press, 2013.  C. Villani. Optimal transport : old and new. Grundlehren der mathematischen Wissenschaften. ISBN 978-3-540-71049-3. URL http://opac.inria.fr/  Springer, Berlin, 2009. record=b1129524.  "}, "Rates of estimation for determinantal point processes": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Rates of estimation for determinantal point processes", "abstract": "Determinantal point processes (DPPs) have wide-ranging applications in machine learning, where they are used to enforce the notion of diversity in subset selection problems. Many estimators have been proposed, but surprisingly the basic properties of the maximum likelihood estimator (MLE) have received little attention. In this paper, we study the local geometry of the expected log-likelihood function to prove several rates of convergence for the MLE. We also give a complete characterization of the case where the MLE converges at a parametric rate. Even in the latter case, we also exhibit a potential curse of dimensionality where the asymptotic variance of the MLE is exponentially large in the dimension of the problem.", "pdf_url": "http://proceedings.mlr.press/v65/brunel17a/brunel17a.pdf", "keywords": ["Determinantal point processes", "statistical estimation", "maximum likelihood", "L-ensembles"], "reference": "Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, and Benjamin Taskar. Learning the parameters of determinantal point process kernels. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1224-1232, 2014.  Jennifer Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for In Proceedings of the 27th International Conference learning determinantal point processes. on Neural Information Processing Systems, NIPS\u201914, pages 3149-3157, Cambridge, MA, USA, 2014. MIT Press.  A. Kulesza. Learning with determinantal point processes. PhD thesis, University of Pennsylvania,  2012.  Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publish-  ers Inc., Hanover, MA, USA, 2012. ISBN 1601986289, 9781601986283.  Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2389-2397, 2015.  3   MLE OF DPPS  4. Maximum likelihood estimation  Let Z1, . . . , Zn be n independent copies of Z \u223c DPP(L\u2217), where the kernel L\u2217 is unknown. We assume that L\u2217 is positive definite. Let \u02c6L be the maximum likelihood estimator (MLE) of L\u2217 (unique up to a \ufb02ip of the sign of its columns and its rows).  We measure the performance of the MLE using the loss (cid:96) defined by  (cid:96)( \u02c6L, L\u2217) = min D\u2208D  (cid:107) \u02c6L \u2212 DL\u2217D(cid:107)F  where (cid:107) \u00b7 (cid:107)F denotes the Frobenius norm.  Our statistical results establish asymptotic properties of the MLE. We use OIP for big-O notation in probability. For L \u2208 IRN \u00d7N and J, J (cid:48) \u2286 [N ], we denote by LJ,J (cid:48) the submatrix of L obtained by keeping the rows indexed in J and the columns indexed in J (cid:48).  Theorem 3  \u2022 (cid:96)( \u02c6L, L\u2217) \u2212\u2212\u2212\u2192 n\u2192\u221e  0 ,  in probability.  \u2022 If L\u2217 is irreducible, then, \u02dcL is asymptotically normal. In particular, (cid:96)( \u02c6L, L\u2217) = OIP(n\u22121/2).  \u2022 If L\u2217 is block diagonal, then, for any pair of distinct blocks J, J (cid:48) of L\u2217,  (cid:107) \u02c6LJ,J (cid:48) \u2212 DL\u2217  J,J (cid:48)D(cid:107)F = OIP(n\u22121/4)  (cid:107) \u02c6LJ \u2212 DJ L\u2217  J DJ (cid:107)F = OIP(n\u22121/2).  min D\u2208D  min D\u2208D  (4.1)  (4.2)  and  References  Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, and Benjamin Taskar. Learning the parameters of determinantal point process kernels. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1224-1232, 2014.  Jennifer Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for In Proceedings of the 27th International Conference learning determinantal point processes. on Neural Information Processing Systems, NIPS\u201914, pages 3149-3157, Cambridge, MA, USA, 2014. MIT Press.  A. Kulesza. Learning with determinantal point processes. PhD thesis, University of Pennsylvania,  2012.  Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Now Publish-  ers Inc., Hanover, MA, USA, 2012. ISBN 1601986289, 9781601986283.  Zelda Mariet and Suvrit Sra. Fixed-point algorithms for learning determinantal point processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2389-2397, 2015. "}, "Learning Disjunctions of Predicates": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Learning Disjunctions of Predicates", "abstract": "Let $\\mathcal F$ be a set of boolean functions. We give an algorithm for learning $\\mathcal F_\u2228:={\\vee_f\u2208Sf | S\u2286\\mathcal {F}}$ from membership queries. Our algorithm asks at most $|\\mathcal {F}|\u22c5\\rm OPT(\\mathcal {F}_\u2228)$ membership queries where $\\rm OPT(\\mathcal{F}_\u2228)$ is the minimum worst case number of membership queries for learning $\\mathcal{F}_\u2228$. When $\\mathcal{F}$ is a set of halfspaces over a constant dimension space or a set of variable inequalities, our algorithm runs in polynomial time. The problem we address has a practical importance in the field of program synthesis, where the goal is to synthesize a program meeting some requirements. Program synthesis has become popular especially in settings aimed to help end users. In such settings, the requirements are not provided upfront and the synthesizer can only learn them by posing membership queries to the end user. Our work completes such synthesizers with the ability to learn the exact requirements while bounding the number of membership queries.", "pdf_url": "http://proceedings.mlr.press/v65/bshouty17a/bshouty17a.pdf", "keywords": [], "reference": "Hasan Abasi, Ali Z. Abdi, and Nader H. Bshouty. Learning boolean halfspaces with small weights from membership queries. In Algorithmic Learning Theory: 25th International Conference, ALT \u201914, 2014.  Elias Abboud, Nader Agha, Nader H. Bshouty, Nizar Radwan, and Fathi Saleh. Learning threshold functions with small weights using membership queries. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pages 318-322, 1999.  Vicente Acua, Etienne Birmel, Ludovic Cottret, Pierluigi Crescenzi, Fabien Jourdan, Vincent La- croix, Alberto Marchetti-Spaccamela, Andrea Marino, Paulo Vieira Milreu, Marie-France Sagot, and Leen Stougie. Telling stories: Enumerating maximal directed acyclic graphs with a constrai- ned set of sources and targets. Theoretical Computer Science, 457:1 - 9, 2012.  Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid. Recursive program synthesis. In Compu-  ter Aided Verification - 25th International Conference, CAV \u201913, pages 934-950, 2013.  Noga Alon, Richard Beigel, Simon Kasif, Steven Rudich, and Benny Sudakov. Learning a hidden matching. In Proceedings of the 43rd Symposium on Foundations of Computer Science, FOCS \u201902, 2002.  Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax- guided synthesis. In Formal Methods in Computer-Aided Design, FMCAD \u201913, pages 1-8, 2013.  AmiBroker. https://www.amibroker.com/.  Saswat Anand, Wei-Ngan Chin, and Siau-Cheng Khoo. Charting patterns on price history.  In Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming (ICFP \u201901), pages 134-145, 2001.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Dana Angluin and Jiang Chen. Learning a hidden graph using queries per edge. Journal of Com-  puter and System Sciences, 74(4):546 - 556, 2008. Carl Smith Memorial Issue.  Sriram Karthik Badam, Jieqiong Zhao, Shivalik Sen, Niklas Elmqvist, and David S. Ebert. Time- fork: Interactive prediction of time series. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5409-5420, 2016.  17   LEARNING DISJUNCTIONS OF PREDICATES  synthesizer that learns time-series patterns in polynomial time and outputs an executable program, while interacting with the end user through visual charts.  Acknowledgements The research leading to these results has received funding from the European Union\u2019s - Seventh Framework Programme (FP7) under grant agreement no 615688-ERC-COG- PRIME.  References  Hasan Abasi, Ali Z. Abdi, and Nader H. Bshouty. Learning boolean halfspaces with small weights from membership queries. In Algorithmic Learning Theory: 25th International Conference, ALT \u201914, 2014.  Elias Abboud, Nader Agha, Nader H. Bshouty, Nizar Radwan, and Fathi Saleh. Learning threshold functions with small weights using membership queries. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pages 318-322, 1999.  Vicente Acua, Etienne Birmel, Ludovic Cottret, Pierluigi Crescenzi, Fabien Jourdan, Vincent La- croix, Alberto Marchetti-Spaccamela, Andrea Marino, Paulo Vieira Milreu, Marie-France Sagot, and Leen Stougie. Telling stories: Enumerating maximal directed acyclic graphs with a constrai- ned set of sources and targets. Theoretical Computer Science, 457:1 - 9, 2012.  Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid. Recursive program synthesis. In Compu-  ter Aided Verification - 25th International Conference, CAV \u201913, pages 934-950, 2013.  Noga Alon, Richard Beigel, Simon Kasif, Steven Rudich, and Benny Sudakov. Learning a hidden matching. In Proceedings of the 43rd Symposium on Foundations of Computer Science, FOCS \u201902, 2002.  Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax- guided synthesis. In Formal Methods in Computer-Aided Design, FMCAD \u201913, pages 1-8, 2013.  AmiBroker. https://www.amibroker.com/.  Saswat Anand, Wei-Ngan Chin, and Siau-Cheng Khoo. Charting patterns on price history.  In Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming (ICFP \u201901), pages 134-145, 2001.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Dana Angluin and Jiang Chen. Learning a hidden graph using queries per edge. Journal of Com-  puter and System Sciences, 74(4):546 - 556, 2008. Carl Smith Memorial Issue.  Sriram Karthik Badam, Jieqiong Zhao, Shivalik Sen, Niklas Elmqvist, and David S. Ebert. Time- fork: Interactive prediction of time series. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5409-5420, 2016. BSHOUTY DRACHSLER-COHEN VECHEV YAHAV  Mike Barnett, Badrish Chandramouli, Robert DeLine, Steven Drucker, Danyel Fisher, Jonathan Goldstein, Patrick Morrison, and John Platt. Stat!: An interactive analytics environment for big data. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD \u201913, pages 1013-1016, 2013.  Daniel W. Barowy, Sumit Gulwani, Ted Hart, and Benjamin Zorn. Flashrelate: Extracting relati- onal data from semi-structured spreadsheets using examples. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI \u201915, pages 218-228, 2015.  E. Biglieri and L. Gyrfi. Multiple Access Channels: Theory and Practice. IOS Press, 2007.  Annalisa De Bonis, Leszek Gasieniec, and Ugo Vaccaro. Optimal two-stage algorithms for group  testing problems. volume 34, pages 1253-1270, 2005.  Michele Borassi, Pierluigi Crescenzi, Vincent Lacroix, Andrea Marino, Marie-France Sagot, and Paulo Vieira Milreu. Telling stories fast. In Vincenzo Bonifaci, Camil Demetrescu, and Alberto Marchetti-Spaccamela, editors, Experimental Algorithms: 12th International Symposium, SEA \u201913, pages 200-211, 2013.  James Bornholt, Emina Torlak, Dan Grossman, and Luis Ceze. Optimizing synthesis with metas- ketches. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201916, pages 775-788, 2016.  Lars Brenna, Alan Demers, Johannes Gehrke, Mingsheng Hong, Joel Ossher, Biswanath Panda, Mi- rek Riedewald, Mohit Thatte, and Walker White. Cayuga: A high-performance event processing engine. In Proceedings of the ACM SIGMOD International Conference on Management of Data, pages 1100-1102, 2007.  Thomas N. Bulkowski. Encyclopedia of Chart Patterns. Wiley, 2nd edition, 2005.  Badrish Chandramouli, Jonathan Goldstein, and David Maier. High-performance dynamic pattern  matching over disordered streams. In PVLDB, volume 3, pages 220-231, 2010.  Mooi Choo Chuah and Fen Fu. Ecg anomaly detection via time series analysis. In Frontiers of High Performance Computing and Networking ISPA 2007 Workshops: ISPA 2007 International Workshops SSDSN, UPWN, WISH, SGC, ParDMCom, HiPCoMB, and IST-AWSN, pages 123- 135, 2007.  Ferdinando Cicalese. Group testing. In Fault-Tolerant Search Algorithms, pages 139-173. Springer,  2013.  Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and Charles E. Leiserson. Introduction to  Algorithms. McGraw-Hill Higher Education, 2nd edition, 2001.  Anish Das Sarma, Aditya Parameswaran, Hector Garcia-Molina, and Jennifer Widom. Synthesizing view definitions from data. In Database Theory - ICDT \u201910, 13th International Conference, pages 89-103, 2010. LEARNING DISJUNCTIONS OF PREDICATES  Robert Dorfman. The detection of defective members of large populations. The Annals of Mathe-  matical Statistics, 14(4):436-440, 1943.  D. Du and F. Hwang. Combinatorial Group Testing and Its Applications. Applied Mathematics.  World Scientific, 2000.  D. Du and F. Hwang. Pooling Designs and Nonadaptive Group Testing: Important Tools for DNA  Sequencing. Series on applied mathematics. World Scientific, 2006.  Vladimir Grebinski and Gregory Kucherov. Reconstructing a hamiltonian cycle by querying the graph: Application to DNA physical mapping. Discrete Appl. Math., 88(1-3):147-165, Novem- ber 1998.  Sumit Gulwani. Dimensions in program synthesis. In Proceedings of the 12th International ACM SIGPLAN Conference on Principles and Practice of Declarative Programming, pages 13-24, 2010.  Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In Pro- ceedings of the 38th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan- guages, POPL \u201911, pages 317-330, 2011.  Sumit Gulwani, William R. Harris, and Rishabh Singh. Spreadsheet data manipulation using exam-  ples. Commun. ACM, 55(8):97-105, 2012.  William R. Harris and Sumit Gulwani. Spreadsheet table transformations from examples. In Pro- ceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Imple- mentation, PLDI \u201911, pages 317-328, 2011.  Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Pro- ceedings of the Eighth Annual Conference on Computational Learning Theory, COLT \u201995, pages 108-117, 1995.  M. Hirzel, H. Andrade, B. Gedik, G. Jacques-Silva, R. Khandekar, V. Kumar, M. Mendell, H. Nas- gaard, S. Schneider, R. Soul\u00b4e, and K.-L. Wu. IBM streams processing language: Analyzing big data in motion. IBM J. Res. Dev., 57(3-4), 2013.  Susmit Jha, Sumit Gulwani, Sanjit A. Seshia, and Ashish Tiwari. Oracle-guided component-based program synthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1, ICSE \u201910, pages 215-224, 2010.  Donald E. Knuth. The Art of Computer Programming, Volume 1 (3rd Ed.): Fundamental Algo-  rithms. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA, 1997.  Tessa A. Lau, Steven A. Wolfman, Pedro Domingos, and Daniel S. Weld. Programming by demon-  stration using version space algebra. Machine Learning, 53(1-2):111-156, 2003.  Vu Le and Sumit Gulwani. Flashextract: A framework for data extraction by examples. In ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI \u201914, pages 542-553, 2014. BSHOUTY DRACHSLER-COHEN VECHEV YAHAV  Aditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. A ma- chine learning framework for programming by example. In Proceedings of the 30th International Conference on Machine Learning, ICML \u201913, pages 187-195, 2013.  A. Morales-Esteban, F. Martnez-lvarez, A. Troncoso, J.L. Justo, and C. Rubio-Escudero. Pattern recognition to forecast seismic time series. Expert Systems with Applications, 37(12):8333 - 8342, 2010.  Hung Q Ngo and Ding-Zhu Du. A survey on combinatorial group testing algorithms with ap- plications to DNA library screening. DIMACS Series in Discrete Mathematics and Theoretical Computer Science, 2000.  Andrzej Pelc. Searching games with errors\u2014fifty years of coping with liars. Theor. Comput. Sci.,  270(1-2):71-109, January 2002.  Oleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program synthe- sis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA \u201915, pages 107-126, 2015.  Rishabh Singh and Sumit Gulwani. Learning semantic string transformations from examples.  PVLDB, 5(8):740-751, 2012.  Rishabh Singh and Armando Solar-Lezama. Synthesizing data structure manipulations from story- boards. In SIGSOFT/FSE\u201911 19th ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE-19) and ESEC\u201911: 13th European Software Engineering Conference (ESEC- 13), pages 289-299, 2011.  Armando Solar-Lezama. Program synthesis by sketching. ProQuest, 2008.  Armando Solar-Lezama, Christopher Grant Jones, and Rastislav Bodik. Sketching concurrent data structures. In Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation, pages 136-148, 2008.  Mandana Vaziri, Olivier Tardieu, Rodric Rabbah, Philippe Suter, and Martin Hirzel. Stream pro- cessing with a spreadsheet. In ECOOP 2014 - Object-Oriented Programming - 28th European Conference, pages 360-384. 2014.  Kunihiro Wasa. Enumeration of enumeration algorithms. CoRR, abs/1605.05102, 2016.  Eugene Wu, Yanlei Diao, and Shariq Rizvi. High-performance complex event processing over In Proceedings of the ACM SIGMOD International Conference on Management of  streams. Data, pages 407-418, 2006.  Kuat Yessenov, Shubham Tulsiani, Aditya Krishna Menon, Robert C. Miller, Sumit Gulwani, But- ler W. Lampson, and Adam Kalai. A colorful approach to text processing by example. In The 26th Annual ACM Symposium on User Interface Software and Technology, UIST\u201913, pages 495-504, 2013.  Sai Zhang and Yuyin Sun. Automatically synthesizing sql queries from input-output examples. In 2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE \u201913, pages 224-234, 2013. LEARNING DISJUNCTIONS OF PREDICATES  N. Yu. Zolotykh and V. N. Shevchenko. Deciphering threshold functions of k-valued logic.  In Discrete Analysis and Operations Research. Novosibirsk 2(3), pp. 18. English translation: Kors- hunov, A. D. (ed.): Operations Research and Discrete Analysis. Kluwer Ac. Publ. Netherlands. (1997), 1995.  "}, "Testing Bayesian Networks": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Testing Bayesian Networks", "abstract": "This work initiates a systematic investigation of testing \\em high-dimensional structured distributions by focusing on testing \\em Bayesian networks \u2013 the prototypical family of directed graphical models. A Bayesian network is defined by a directed acyclic graph, where we associate a random variable with each node. The value at any particular node is conditionally independent of all the other non-descendant nodes once its parents are fixed. Specifically, we study the properties of identity testing and closeness testing of Bayesian networks. Our main contribution is the first non-trivial efficient testing algorithms for these problems and corresponding information-theoretic lower bounds. For a wide range of parameter settings, our testing algorithms have sample complexity \\em sublinear in the dimension and are sample-optimal, up to constant factors.", "pdf_url": "http://proceedings.mlr.press/v65/canonne17a/canonne17a.pdf", "keywords": ["distribution testing", "property testing", "Bayesian networks", "graphical models"], "reference": "P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.  J. Mach. Learn. Res., 7:1743-1788, 2006.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing. Journal of  Machine Learning Research - Proceedings Track, 19:47-68, 2011.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-linear  time. CoRR, abs/1506.00671, 2015b.  M. Adamaszek, A. Czumaj, and C. Sohler. Testing monotone continuous distributions on high- dimensional real cubes. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, pages 56-65, 2010.  A. Almudevar. A hypothesis test for equality of bayesian network models. EURASIP J. Bioinformatics  and Systems Biology, 2010, 2010.  NIPS, pages 1061-1069, 2012.  A. Anandkumar, D. J. Hsu, F. Huang, and S. Kakade. Learning mixtures of tree graphical models. In  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  Z. Bai and H. Saranadasa. Effect of high dimension: by an example of a two sample problem. Statist.  Sinica,, 6:311-329, 1996.  T. Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001.  62   CANONNE DIAKONIKOLAS KANE STEWART  But because of the d-way probability similarities, the terms Pr[ PSi = x ] and Pr[ QSi = x ] terms are very close, within an additive poly((cid:15)/n).  (Here we use the extra assumption that P and Q use the same ordering.) Denote by Ti the parents of i under the topology of Q. Then H(Qi | Q1,...,i\u22121 = y) depends only on the values of the coordinates in Ti. Thus the last part of the sum is a sum over z of Pr[ QTi = z ] H(Qi | QTi = z) and Pr[ PTi = z ] H(Qi | QTi = z), which are also close by a similar argument. Thus, EP1,...,Pi\u22121[D(Qi | PSi(cid:107)Qi | P1, . . . , Pi\u22121)] = I (Qi; Q1, . . . , Qi\u22121 | QSi)+poly(cid:0) (cid:15) n  (cid:1) = poly(cid:0) (cid:15) n  (cid:1).  This implies that P, Q are close in KL divergence, and therefore in L1.  The second part of the theorem, asserting the existence of a closeness testing algorithm with optimal dependence on d, will be very similar. Indeed, by the proof above it suffices to check that the restrictions of P and Q to any set of (d + 3)-coordinates are poly((cid:15)/n)-close. Using known results Chan et al. (2014c), this can be done for any specific collection of d + 3 coordinates with N samples in poly(N ) time, and high probability of success, implying the second part of the theorem.  References  P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.  J. Mach. Learn. Res., 7:1743-1788, 2006.  J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan. Competitive closeness testing. Journal of  Machine Learning Research - Proceedings Track, 19:47-68, 2011.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-linear  time. CoRR, abs/1506.00671, 2015b.  M. Adamaszek, A. Czumaj, and C. Sohler. Testing monotone continuous distributions on high- dimensional real cubes. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, pages 56-65, 2010.  A. Almudevar. A hypothesis test for equality of bayesian network models. EURASIP J. Bioinformatics  and Systems Biology, 2010, 2010.  NIPS, pages 1061-1069, 2012.  A. Anandkumar, D. J. Hsu, F. Huang, and S. Kakade. Learning mixtures of tree graphical models. In  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  Z. Bai and H. Saranadasa. Effect of high dimension: by an example of a two sample problem. Statist.  Sinica,, 6:311-329, 1996.  T. Batu. Testing Properties of Distributions. PhD thesis, Cornell University, 2001. TESTING BAYESIAN NETWORKS  T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In IEEE Symposium on Foundations of Computer Science, pages 259-269, 2000. URL citeseer.ist.psu.edu/batu00testing.html.  T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White. Testing random variables for independence and identity. In Proc. 42nd IEEE Symposium on Foundations of Computer Science, pages 442-451, 2001.  T. Batu, S. Dasgupta, R. Kumar, and R. Rubinfeld. The complexity of approximating entropy. In  ACM Symposium on Theory of Computing, pages 678-687, 2002.  T. Batu, R. Kumar, and R. Rubinfeld. Sublinear algorithms for testing monotone and unimodal  distributions. In ACM Symposium on Theory of Computing, pages 381-390, 2004.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  2010.  A. Bhattacharyya, E. Fischer, R. Rubinfeld, and P. Valiant. Testing monotonicity of distributions  over general partial orders. In ICS, pages 239-252, 2011.  P. J. Bickel. A distribution free version of the smirnov two sample test in the p-variate case. Ann.  Math. Statist., 40(1):1-23, 02 1969.  T. T. Cai and Z. Ma. Optimal hypothesis testing for high dimensional covariance matrices. Bernoulli,  19(5B):2359-2388, 2013.  C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016, pages 25:1-25:14, 2016.  C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Electronic  Colloquium on Computational Complexity (ECCC), 22:63, 2015.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S. Chan, I. Diakonikolas, P. Valiant, and G. Valiant. Optimal algorithms for testing closeness of  discrete distributions. In SODA, pages 1193-1203, 2014c.  S. X. Chen and Y. L. Qin. A two-sample test for high-dimensional data with applications to gene-set  testing. Ann. Statist., 38(2):808-835, 04 2010.  J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning bayesian networks from data: An  information-theory based approach. Artificial Intelligence, 137(1):43 - 90, 2002. CANONNE DIAKONIKOLAS KANE STEWART  D. M. Chickering. Learning equivalence classes of bayesian-network structures. J. Mach. Learn.  Res., 2:445-498, 2002.  C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE  Trans. Inf. Theor., 14(3):462-467, 1968.  M. Cryan, L. Goldberg, and P. Goldberg. Evolutionary trees can be learned in polynomial time in the  two state general Markov model. SIAM Journal on Computing, 31(2):375-397, 2002.  R. Daly, Q. Shen, and S. Aitken. Learning bayesian networks: approaches and issues. The Knowledge  Engineering Review, 26:99-157, 2011. ISSN 1469-8005.  S. Dasgupta. The sample complexity of learning fixed-structure bayesian networks. Machine  Learning, 29(2-3):165-180, 1997.  S. Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual Symposium on  Foundations of Computer Science, pages 634-644, 1999.  C. Daskalakis and Q. Pan. Square hellinger subadditivity for bayesian networks and its applications  to identity testing. CoRR, abs/1612.03164, 2016.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of  Independent Integer Random Variables. In FOCS, pages 217-226, 2013a.  C. Daskalakis, I. Diakonikolas, R. Servedio, G. Valiant, and P. Valiant. Testing k-modal distributions:  Optimal algorithms via reductions. In SODA, pages 1833-1852, 2013b.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and its  applications. In Proceedings of STOC\u201916, 2016a.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing ising models. CoRR,  abs/1612.03147, 2016b. URL http://arxiv.org/abs/1612.03147.  A. De, I. Diakonikolas, and R. Servedio. Learning from satisfying assignments. In Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pages 478-497, 2015.  I. Diakonikolas and D. M. Kane. A new approach for testing properties of discrete distributions. In  FOCS, pages 685-694, 2016. Full version available at abs/1601.05557.  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Testing Identity of Structured Distributions. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, 2015a. TESTING BAYESIAN NETWORKS  I. Diakonikolas, D. M. Kane, and V. Nikishkin. Optimal algorithms and lower bounds for testing closeness of structured distributions. In 56th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2015, 2015b.  I. Diakonikolas, D. M. Kane, and A. Stewart. Nearly optimal learning and sparse covers for sums of  independent integer random variables. CoRR, abs/1505.00662, 2015c.  I. Diakonikolas, T. Gouleakis, J. Peebles, and E. Price. Collision-based testers are optimal for uniformity and closeness. Electronic Colloquium on Computational Complexity (ECCC), 23:178, 2016a.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distributions and its algorithmic applications. In Proceedings of STOC\u201916, 2016b. Available at https://arxiv.org/abs/1511.03592.  I. Diakonikolas, D. M. Kane, and A. Stewart. Robust learning of fixed-structure bayesian networks.  CoRR, abs/1606.07384, 2016c.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In Proceedings of the  12th Annual COLT, pages 183-192, 1999.  N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence, UAI\u201996, pages 274-282, 1996.  N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine Learning, 29  (2):131-163, 1997.  N. Friedman, M. Linial, and I. Nachman. Using bayesian networks to analyze expression data.  Journal of Computational Biology, 7:601-620, 2000.  O. Goldreich and D. Ron. On testing expansion in bounded-degree graphs. Technical Report  TR00-020, Electronic Colloquium in Computational Complexity, 2000.  M. Gonen, P. H. Westfall, and W. O. Johnson. Bayesian multiple testing for two-sample multivariate  endpoints. Biometrics, 59(1):76-82, 2003. ISSN 1541-0420.  M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, pages 753-760, 2015.  H. Hotelling. The generalization of student\u2019s ratio. Ann. Math. Statist., 2(3):360-378, 08 1931.  P. Indyk, R. Levi, and R. Rubinfeld. Approximating and Testing k-Histogram Distributions in  Sub-linear Time. In PODS, pages 15-22, 2012.  A. Javanmard and A. Montanari. Confidence intervals and hypothesis testing for high-dimensional  regression. J. Mach. Learn. Res., 15(1):2869-2909, 2014.  F. V. Jensen and T. D. Nielsen. Bayesian Networks and Decision Graphs. Springer Publishing  Company, Incorporated, 2nd edition, 2007. CANONNE DIAKONIKOLAS KANE STEWART  D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive  Computation and Machine Learning. The MIT Press, 2009.  E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Texts in Statistics. Springer,  R. Levi, D. Ron, and R. Rubinfeld. Testing properties of collections of distributions. In ICS, pages  2005.  179-194, 2011.  R. Y. Liu and K. Singh. A quality index based on data depth and multivariate rank tests. Journal of the American Statistical Association, 88(421):252-260, 1993. ISSN 01621459. URL http: //www.jstor.org/stable/2290720.  P. L. Loh and M. J. Wainwright. Structure estimation for discrete graphical models: Generalized  covariance matrices and their inverses. In NIPS, pages 2096-2104, 2012.  D. Margaritis. Learning Bayesian Network Model Structure From Data. PhD thesis, CMU, 2003.  A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS,  pages 93-102, 2010.  E. Mossel and S. Roch. Learning nonsingular phylogenies and Hidden Markov Models. In To appear  in Proceedings of the 37th Annual Symposium on Theory of Computing (STOC), 2005.  R. E. Neapolitan. Learning Bayesian Networks. Prentice-Hall, Inc., 2003.  J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypothe- ses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933. doi: 10.1098/rsta. 1933.0009. URL http://rsta.royalsocietypublishing.org/content/231/ 694-706/289.short.  H.-T. Nguyen, P. Leray, and G. Ramstein. Multiple Hypothesis Testing and Quasi Essential Graph for Comparing Two Sets of Bayesian Networks, pages 176-185. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.  L. Paninski. A coincidence-based test for uniformity given very sparsely-sampled discrete data.  IEEE Transactions on Information Theory, 54:4750-4755, 2008.  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan  Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.  D. Pollard.  Asymptopia.  Asymptopia, 2003. Manuscript.  http://www.stat.yale.edu/\u02dcpollard/Books/  Y. Polyanskiy and Y. Wu. Strong data-processing inequalities for channels and Bayesian networks.  ArXiv e-prints, August 2015.  Y. Rahmatallah, F. Emmert-Streib, and G. Glazko. Gene Sets Net Correlations Analysis (GSNCA): a multivariate differential coexpression test for gene sets. Bioinformatics, 30(3):360-368, 2014. TESTING BAYESIAN NETWORKS  A. Ramdas, D. Isenberg, A. Singh, and L. A. Wasserman. Minimax lower bounds for linear independence testing. In IEEE International Symposium on Information Theory, ISIT 2016, pages 965-969, 2016.  R. Rubinfeld. Taming big probability distributions. XRDS, 19(1):24-28, 2012.  R. Rubinfeld and R. Servedio. Testing monotone high-dimensional distributions. In Proc. 37th  Annual ACM Symposium on Theory of Computing (STOC), pages 147-156, 2005.  N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical  models in high dimensions. IEEE Trans. Information Theory, 58(7):4117-4134, 2012.  D. M. Sobel and T. Kushnir. Interventions do not solely benefit causal learning: Being told what to do results in worse learning than doing it yourself. In Proceedings of the twenty-fifth annual meeting of the Cognitive Science Society, pages 1100-1105, 2003.  M. S. Srivastava and M. Du. A test for the mean vector with fewer observations than the dimension.  Journal of Multivariate Analysis, 99(3):386 - 402, 2008.  N. St\u00a8adler and S. Mukherjee. Multivariate gene-set testing based on graphical models. Biostatistics,  16(1):47-59, 2015.  G. Valiant and P. Valiant. Estimating the unseen: an n/ log(n)-sample estimator for entropy and  support size, shown optimal via new CLTs. In STOC, pages 685-694, 2011.  G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity testing. In  FOCS, 2014.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 113-122, 2002.  T. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence, UAI \u201990, pages 255-270, 1991.  M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.  Found. Trends Mach. Learn., 1(1-2):1-305, 2008.  M. J. Wainwright, P. Ravikumar, and J. D. Lafferty. High-dimensional graphical model selection  using (cid:96)1-regularized logistic regression. In NIPS, pages 1465-1472, 2006.  L. Weiss. Two-sample tests for multivariate distributions. Ann. Math. Statist., 31(1):159-164, 03  1960.  W. Yin, S. Garimalla, A. Moreno, M. R. Galinski, and M. P. Styczynski. A tree-like bayesian structure learning algorithm for small-sample datasets from complex biological model systems. BMC Systems Biology, 9(1):1-18, 2015. CANONNE DIAKONIKOLAS KANE STEWART  APPENDIX  "}, "Multi-Observation Elicitation": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Multi-Observation Elicitation", "abstract": "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts.", "pdf_url": "http://proceedings.mlr.press/v65/casalaina-martin17a/casalaina-martin17a.pdf", "keywords": ["Property elicitation", "loss functions", "empirical risk minimization"], "reference": "Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elici-  tation. In JMLR Workshop and Conference Proceedings, volume 40, pages 1-19, 2015.  M. F. Atiyah and I. G. Macdonald. Introduction to commutative algebra. Addison-Wesley Publish-  ing Co., Reading, Mass.-London-Don Mills, Ont., 1969.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Hans-Georg Beyer and Bernhard Sendhoff. Robust optimization-a comprehensive survey. Com-  puter methods in applied mechanics and engineering, 196(33):3190-3218, 2007.  Jacek Bochnak, Michel Coste, and Marie-Fran\u00e7oise Roy. Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Springer-Verlag, Berlin, 1998. ISBN 3-540-64663-9. Translated from the 1987 French original, Revised by the authors.  Andrew Browder. Mathematical analysis: An introduction. Undergraduate Texts in Mathematics.  Springer-Verlag, New York, 1996. ISBN 0-387-94614-4.  14   CASALAINA-MARTIN FRONGILLO MORGAN WAGGONER  Figure 5: The mean squared error of the two regression strategies for estimating Var(y|x), where x \u223c Unif(0, 1) and y \u223c a sin(4\u03c0x) + N (0, 1), for a = 1 (left) and a = 10 (right). The single-observation loss function approach fails because it tries to fit to the complex underlying model of y|x, while the two- observation loss approach is able to directly model the simple relationship between Var(y) and x.  We thank Karthik Kannan for contributing the upper bound for central moments. Sebastian Casalaina- Martin was partially supported by NSA grant H98230-16-1-0053. Tom Morgan was funded in part by NSF grants CCF-1320231 and CNS-1228598. Bo Waggoner is supported by the Warren Center for Network and Data Sciences at the University of Pennsylvania.  Acknowledgments  References  Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elici-  tation. In JMLR Workshop and Conference Proceedings, volume 40, pages 1-19, 2015.  M. F. Atiyah and I. G. Macdonald. Introduction to commutative algebra. Addison-Wesley Publish-  ing Co., Reading, Mass.-London-Don Mills, Ont., 1969.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Hans-Georg Beyer and Bernhard Sendhoff. Robust optimization-a comprehensive survey. Com-  puter methods in applied mechanics and engineering, 196(33):3190-3218, 2007.  Jacek Bochnak, Michel Coste, and Marie-Fran\u00e7oise Roy. Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Springer-Verlag, Berlin, 1998. ISBN 3-540-64663-9. Translated from the 1987 French original, Revised by the authors.  Andrew Browder. Mathematical analysis: An introduction. Undergraduate Texts in Mathematics.  Springer-Verlag, New York, 1996. ISBN 0-387-94614-4. MULTI-OBSERVATION ELICITATION  David A. Cox, John Little, and Donal O\u2019Shea. Ideals, varieties, and algorithms. Undergraduate Texts in Mathematics. Springer, Cham, fourth edition, 2015. ISBN 978-3-319-16720-6; 978-3- 319-16721-3. An introduction to computational algebraic geometry and commutative algebra.  David R Cox and Peter AW Lewis. The statistical analysis of series of events. Monographs on  Applied Probability and Statistics, 1966.  Ronald A. Fisher, A. Steven Corbet, and Carrington B. Williams. The relation between the number of species and the number of individuals in a random sample of an animal population. The Journal of Animal Ecology, 12(1):42-58, 1943.  Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis. In Web  and Internet Economics, pages 354-370. Springer, 2014.  Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation.  In Proceedings of the 28th  Conference on Learning Theory, pages 1-18, 2015a.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity and Conditional Elicitation. arXiv  preprint arXiv:1506.07212, 2015b.  Processing Systems 29, 2015c.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity. In Advances in Neural Information  Rafael M. Frongillo, Yiling Chen, and Ian A. Kash. Elicitation for Aggregation. Proceedings of the  29th AAAI Conference on Artificial Intelligence, 2015.  William Fulton. Intersection theory, volume 2 of Ergebnisse der Mathematik und ihrer Grenzgebi- ete. 3. Folge. A Series of Modern Surveys in Mathematics [Results in Mathematics and Related Areas. 3rd Series. A Series of Modern Surveys in Mathematics]. Springer-Verlag, Berlin, second edition, 1998. ISBN 3-540-62046-X; 0-387-98549-2.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Associa-  tion, 106(494):746-762, 2011.  Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1735-1742. IEEE, 2006.  Nicolas S. Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions. In  Proceedings of the 10th ACM Conference on Electronic Commerce, pages 109-118, 2009.  Nicolas S. Lambert, David M. Pennock, and Yoav Shoham. Eliciting properties of probability distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129- 138, 2008.  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California,  Berkeley, 1985. CASALAINA-MARTIN FRONGILLO MORGAN WAGGONER  Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face In Proceedings of the IEEE Conference on Computer Vision and  recognition and clustering. Pattern Recognition, pages 815-823, 2015.  William F Sharpe. Mutual fund performance. Journal of Business, 39(1):119-138, 1966.  Ingo Steinwart, Chlo\u00e9 Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties. In Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014.  Evgeniya Ustinova and Victor Lempitsky. Learning deep embeddings with histogram loss.  In  Advances in Neural Information Processing Systems, pages 4170-4178, 2016. "}, "Algorithmic Chaining and the Role of Partial Feedback in Online Nonparametric Learning": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Algorithmic Chaining and the Role of Partial Feedback in Online Nonparametric Learning", "abstract": "We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we design the first explicit algorithm achieving the minimax regret rate (up to log factors). In a partial feedback model motivated by second-price auctions, we obtain algorithms for Lipschitz and semi-Lipschitz losses with regret bounds improving on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.", "pdf_url": "http://proceedings.mlr.press/v65/cesa-bianchi17a/cesa-bianchi17a.pdf", "keywords": ["online learning", "nonparametric", "chaining", "bandits"], "reference": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning (ICML), 2014.  Noga Alon, Nicol\u00f2 Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. CoRR, abs/1409.8428, 2014.  Noga Alon, Nicol\u00f2 Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback  graphs: Beyond bandits. In COLT, pages 23-35, 2015.  Jean-Yves Audibert and S\u00e9bastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11(Oct):2785-2836, 2010.  Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesv\u00e1ri. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011a.  S\u00e9bastien Bubeck, Gilles Stoltz, and Jia Yuan Yu. Lipschitz bandits without the Lipschitz constant. In International Conference on Algorithmic Learning Theory, pages 144-158. Springer, 2011b.  S\u00e9bastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016.  Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. On prediction of individual sequences. The Annals of  Statistics, 27(6):1865-1895, 1999.  Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicol\u00f2 Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for reserve prices  in second-price auctions. IEEE Transactions on Information Theory, 61(1):549-564, 2015.  Nicol\u00f2 Cesa-Bianchi, Pierre Gaillard, Claudio Gentile, and S\u00e9bastien Gerchinovitz. Algorithmic Chaining and the Role of Partial Feedback in Online Nonparametric Learning. Preprint, February 2017. URL https://arxiv.org/abs/1702.08211.  Emile Contal and Nicolas Vayatis. Stochastic process bandits: Upper confidence bounds algorithms  via generic chaining. arXiv preprint arXiv:1602.04976, 2016.  Emile Contal, C\u00e9dric Malherbe, and Nicolas Vayatis. Optimization for gaussian processes via  chaining. arXiv preprint arXiv:1510.05576, 2015.  15   ALGORITHMIC CHAINING AND THE ROLE OF PARTIAL FEEDBACK IN ONLINE NONPARAMETRIC LEARNING  References  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning (ICML), 2014.  Noga Alon, Nicol\u00f2 Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. CoRR, abs/1409.8428, 2014.  Noga Alon, Nicol\u00f2 Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback  graphs: Beyond bandits. In COLT, pages 23-35, 2015.  Jean-Yves Audibert and S\u00e9bastien Bubeck. Regret bounds and minimax policies under partial  monitoring. Journal of Machine Learning Research, 11(Oct):2785-2836, 2010.  Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesv\u00e1ri. X-armed bandits. Journal of  Machine Learning Research, 12(May):1655-1695, 2011a.  S\u00e9bastien Bubeck, Gilles Stoltz, and Jia Yuan Yu. Lipschitz bandits without the Lipschitz constant. In International Conference on Algorithmic Learning Theory, pages 144-158. Springer, 2011b.  S\u00e9bastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex opti-  mization. arXiv preprint arXiv:1607.03084, 2016.  Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. On prediction of individual sequences. The Annals of  Statistics, 27(6):1865-1895, 1999.  Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicol\u00f2 Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for reserve prices  in second-price auctions. IEEE Transactions on Information Theory, 61(1):549-564, 2015.  Nicol\u00f2 Cesa-Bianchi, Pierre Gaillard, Claudio Gentile, and S\u00e9bastien Gerchinovitz. Algorithmic Chaining and the Role of Partial Feedback in Online Nonparametric Learning. Preprint, February 2017. URL https://arxiv.org/abs/1702.08211.  Emile Contal and Nicolas Vayatis. Stochastic process bandits: Upper confidence bounds algorithms  via generic chaining. arXiv preprint arXiv:1602.04976, 2016.  Emile Contal, C\u00e9dric Malherbe, and Nicolas Vayatis. Optimization for gaussian processes via  chaining. arXiv preprint arXiv:1510.05576, 2015. CESA-BIANCHI GAILLARD GENTILE GERCHINOVITZ  Rocco De Rosa, Francesco Orabona, and Nicol\u00f2 Cesa-Bianchi. The ABACOC algorithm: A novel approach for nonparametric classification of data streams. In Proceedings of the IEEE Interna- tional Conference on Data Mining (ICDM), pages 733-738, 2015.  Richard M Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian  processes. Journal of Functional Analysis, 1(3):290-330, 1967.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  Pierre Gaillard and Sebastien Gerchinovitz. A chaining algorithm for online nonparametric regres- sion. In Proceedings of COLT\u201915, volume 40, pages 764-796. JMLR: Workshop and Conference Proceedings, 2015.  S\u00e9bastien Gerchinovitz and Tor Lattimore. Refined lower bounds for adversarial bandits. In Ad-  vances in Neural Information Processing Systems 29 (NIPS\u201916), pages 1198-1206, 2016.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization,  2(3-4):157-325, 2015.  Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In International Confer-  ence on Computational Learning Theory (COLT\u201907), pages 499-513. 2007.  Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem.  In NIPS, vol-  ume 17, pages 697-704, 2004.  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681-690. ACM, 2008.  Tyler Lu, D\u00e1vid P\u00e1l, and Martin P\u00e1l. Contextual multi-armed bandits. In AISTATS, pages 485-492,  2010.  Odalric-Ambrym Maillard and R\u00e9mi Munos. Adaptive bandits: Towards the best history-dependent  strategy. In AISTATS, pages 570-578, 2011.  Michael Ostrovsky and Michael Schwarz. Reserve prices in Internet advertising auctions: a field  experiment. In ACM Conference on Electronic Commerce, pages 59-60, 2011.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss func-  tions. CoRR, abs/1501.06598, 2015.  Alexander Rakhlin and Karthik Sridharan. Bistro: an efficient relaxation-based method for con- textual bandits. In Proceedings of the Twentieth International Conference on Machine Learning (ICML), 2016.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complex-  ities. Journal of Machine Learning Research, 16:155-186, 2015.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011. ALGORITHMIC CHAINING AND THE ROLE OF PARTIAL FEEDBACK IN ONLINE NONPARAMETRIC LEARNING  Aleksandrs Slivkins. Contextual bandits with similarity information. Journal of Machine Learning  Research, 15(1):2533-2568, 2014.  Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. Efficient algorithms for adversar- ial contextual learning. In Proceedings of the Twentieth International Conference on Machine Learning (ICML), 2016.  Vladimir Vovk. Competing with wild prediction rules. Machine Learning, 69(2-3):193-212, 2007. "}, "Nearly Optimal Sampling Algorithms for Combinatorial Pure Exploration": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Nearly Optimal Sampling Algorithms for Combinatorial Pure Exploration", "abstract": "We study the combinatorial pure exploration problem \\textscBest-Set in a stochastic multi-armed bandit game. In an \\textscBest-Set instance, we are given $n$ stochastic arms with unknown reward distributions, as well as a family $\\mathcal{F}$ of feasible subsets over the arms. Let the weight of an arm be the mean of its reward distribution. Our goal is to identify the feasible subset in $\\mathcal{F}$ with the maximum total weight, using as few samples as possible. The problem generalizes the classical best arm identification problem and the top-$k$ arm identification problem, both of which have attracted significant attention in recent years. We provide a novel \\textitinstance-wise lower bound for the sample complexity of the problem, as well as a nontrivial sampling algorithm, matching the lower bound up to a factor of $\\ln|\\mathcal{F}|$. For an important class of combinatorial families (including spanning trees, matchings, and path constraints), we also provide polynomial time implementation of the sampling algorithm, using the equivalence of separation and optimization for convex program, and the notion of approximate Pareto curves in multi-objective optimization (note that $|\\mathcal{F}|$ can be exponential in $n$). We also show that the $\\ln|\\mathcal{F}|$ factor is inevitable in general, through a nontrivial lower bound construction utilizing a combinatorial structure resembling the Nisan-Wigderson design. Our results significantly improve several previous results for several important combinatorial constraints, and provide a tighter understanding of the general \\textscBest-Set problem. We further introduce an even more general problem, formulated in geometric terms. We are given $n$ Gaussian arms with unknown means and unit variance. Consider the $n$-dimensional Euclidean space $\\mathbb{R}^n$, and a collection $\\mathcal{O}$ of disjoint subsets. Our goal is to determine the subset in $\\mathcal{O}$ that contains the mean profile (which is the $n$-dimensional vector of the means), using as few samples as possible. The problem generalizes most pure exploration bandit problems studied in the literature. We provide the first nearly optimal sample complexity upper and lower bounds for the problem.", "pdf_url": "http://proceedings.mlr.press/v65/chen17a/chen17a.pdf", "keywords": [], "reference": "Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 41-53, 2010.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, pages 258-265, 2013.  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification bandit problem. In Proceedings of the 29th Conference on Learning Theory, 2016.  Robert D Carr, Lisa K Fleischer, Vitus J Leung, and Cynthia A Phillips. Strengthening integrality In Proceedings of the eleventh gaps for capacitated network design and covering problems. annual ACM-SIAM symposium on Discrete algorithms, pages 106-115. Society for Industrial and Applied Mathematics, 2000.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  arXiv:1511.03774, 2015.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Lijie Chen and Jian Li. Open problem: Best arm identification: Almost instance-wise optimality and the gap entropy conjecture. In 29th Annual Conference on Learning Theory, pages 1643-1646, 2016.  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid  constraints. In 29th Annual Conference on Learning Theory, pages 647-669, 2016a.  37   NEARLY OPTIMAL SAMPLING ALGORITHMS FOR COMBINATORIAL PURE EXPLORATION  samples on any instance I = (S, O) in expectation, where  \u2206 = inf  \u03bd\u2208Alt(O)  (cid:107)\u00b5 \u2212 \u03bd(cid:107)2  is defined as the minimum Euclidean distance between the mean profile \u00b5 and an alternative mean profile \u03bd \u2208 Alt(O) with an answer other than O.  Proof By Lemmas 6.2, 6.3 and 6.5, LPSample is a (\u03b40, \u03b4, A, B)-correct algorithm for GENERAL- SAMP (as per Definition 4.7), where E0 = E good \u2229 E good, E1 = E good, \u03b40 = 0.01, A = Low(I) and B = Low(I)(n3 +n ln \u2206\u22121). Lemma 4.8 implies that there is a \u03b4-correct algorithm for GENERAL- SAMP with expected sample complexityO(A ln \u03b4\u22121 + B) = O (cid:0)Low(I)(ln \u03b4\u22121 + n3 + n ln \u2206\u22121)(cid:1) .  References  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 41-53, 2010.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi- armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. In International Conference on Machine Learning, pages 258-265, 2013.  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification bandit problem. In Proceedings of the 29th Conference on Learning Theory, 2016.  Robert D Carr, Lisa K Fleischer, Vitus J Leung, and Cynthia A Phillips. Strengthening integrality In Proceedings of the eleventh gaps for capacitated network design and covering problems. annual ACM-SIAM symposium on Discrete algorithms, pages 106-115. Society for Industrial and Applied Mathematics, 2000.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  arXiv:1511.03774, 2015.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Lijie Chen and Jian Li. Open problem: Best arm identification: Almost instance-wise optimality and the gap entropy conjecture. In 29th Annual Conference on Learning Theory, pages 1643-1646, 2016.  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid  constraints. In 29th Annual Conference on Learning Theory, pages 647-669, 2016a. CHEN GUPTA LI QIAO WANG  Lijie Chen, Jian Li, and Mingda Qiao. Towards instance optimal bounds for best arm identification.  arXiv preprint arXiv:1608.06031, 2016b.  Lijie Chen, Jian Li, and Mingda Qiao. Nearly instance optimal sample complexity bounds for top-k  arm selection. In Artificial Intelligence and Statistics, pages 101-110, 2017.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- In Advances in Neural Information Processing Systems, pages  ration of multi-armed bandits. 379-387, 2014.  Herman Chernoff. Sequential design of experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  VP Draglia, Alexander G Tartakovsky, and Venugopal V Veeravalli. Multihypothesis sequential probability ratio tests. i. asymptotic optimality. IEEE Transactions on Information Theory, 45(7): 2448-2461, 1999.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov decision processes. In International Conference on Computational Learning Theory, pages 255- 270. Springer, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):1079-1105, 2006.  RH Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals of  Mathematical Statistics, pages 36-72, 1964.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and S\u00b4ebastien Bubeck. Multi- In Advances in Neural Information Processing Systems, pages  bandit best arm identification. 2222-2230, 2011.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Pro- cessing Systems, pages 3212-3220, 2012.  Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett. Improved learning complexity in combinatorial pure exploration bandits. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 1004-1012, 2016.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In  29th Annual Conference on Learning Theory, pages 998-1027, 2016.  Aur\u00b4elien Garivier, Emilie Kaufmann, and Wouter M Koolen. Maximin action identification: A new bandit framework for games. In 29th Annual Conference on Learning Theory, pages 1028-1050, 2016.  Bhaskar Kumar Ghosh and Bhaskar Kumar Ghosh. Sequential tests of statistical hypotheses. JS-  TOR, 1970. NEARLY OPTIMAL SAMPLING ALGORITHMS FOR COMBINATORIAL PURE EXPLORATION  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck. lil\u2019ucb: An optimal ex- ploration algorithm for multi-armed bandits. In Proceedings of The 27th Conference on Learning Theory, pages 423-439, 2014.  Shivaram Kalyanakrishnan and Peter Stone. Efficient selection of multiple bandit arms: Theory and practice. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 511-518, 2010.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of The 30th International Conference on Machine Learning, pages 1238-1246, 2013.  Zohar S Karnin. Verification based solution for structured mab problems. In Advances in Neural  Information Processing Systems, pages 145-153, 2016.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. The Journal of Machine Learning Research, 2015.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000.  Andrea Locatelli, Maurilio Gutzeit, and Alexandra Carpentier. An optimal algorithm for the thresh- olding bandit problem. In Proceedings of The 33rd International Conference on Machine Learn- ing, pages 1690-1698, 2016.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. Journal of Machine Learning Research, 5(Jun):623-648, 2004.  R\u00b4emi Munos et al. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. Foundations and Trends R(cid:13) in Machine Learning, 7(1):1-129, 2014.  Mohammad Naghshvar, Tara Javidi, et al. Active sequential hypothesis testing. The Annals of  Statistics, 41(6):2703-2738, 2013.  ences, 49(2):149-167, 1994.  Noam Nisan and Avi Wigderson. Hardness vs randomness. Journal of computer and System Sci-  Christos H Papadimitriou and Mihalis Yannakakis. On the approximability of trade-offs and optimal access of web sources. In Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on, pages 86-92. IEEE, 2000.  Alexander Schrijver. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer  Science & Business Media, 2002. CHEN GUPTA LI QIAO WANG  Abraham Wald. Sequential tests of statistical hypotheses. The Annals of Mathematical Statistics,  16(2):117-186, 1945.  Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identification with applications to crowdsourcing. In Proceedings of The 31st International Conference on Machine Learning, pages 217-225, 2014. NEARLY OPTIMAL SAMPLING ALGORITHMS FOR COMBINATORIAL PURE EXPLORATION  Organization of the "}, "Towards Instance Optimal Bounds for Best Arm Identification": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Towards Instance Optimal Bounds for Best Arm Identification", "abstract": "In the classical best arm identification (Best-$1$-Arm) problem, we are given $n$ stochastic bandit arms, each associated with a reward distribution with an unknown mean. Upon each play of an arm, we can get a reward sampled i.i.d. from its reward distribution. We would like to identify the arm with the largest mean with probability at least $1-\u03b4$, using as few samples as possible. The problem has a long history and understanding its sample complexity has attracted significant attention since the last decade. However, the optimal sample complexity of the problem is still unknown. Recently, Chen and Li (2016) made an interesting conjecture, called gap-entropy conjecture, concerning the instance optimal sample complexity of Best-$1$-Arm. Given a Best-$1$-Arm instance $I$ (i.e., a set of arms), let $\\mu_[i]$ denote the $i$th largest mean and $\\Delta_[i]=\\mu_[1]-\\mu_[i]$ denote the corresponding gap. $H(I)=\\sum_i=2^n\\Delta_[i]^-2$ denotes the complexity of the instance. The gap-entropy conjecture states that for any instance $I$, $\u03a9\\left(H(I)\u22c5\\left(\\ln\u03b4^-1 + \\mathsf{Ent}(I)\\right)\\right)$ is an instance lower bound, where $\\mathsf{Ent}(I)$ is an entropy-like term determined by the gaps, and there is a $\u03b4$-correct algorithm for Best-$1$-Arm with sample complexity $O\\left(H(I)\u22c5\\left(\\ln\u03b4^-1 + \\mathsf{Ent}(I)\\right)+\\Delta_[2]^-2\\ln\\ln\\Delta_[2]^-1\\right)$. We note that $\u0398\\left(\\Delta_[2]^-2\\ln\\ln\\Delta_[2]^-1\\right)$ is necessary and sufficient to solve the two-arm instance with the best and second best arms. If the conjecture is true, we would have a complete understanding of the instance-wise sample complexity of Best-$1$-Arm (up to constant factors). In this paper, we make significant progress towards a complete resolution of the gap-entropy conjecture. For the upper bound, we provide a highly nontrivial algorithm which requires \\[O\\left(H(I)\u22c5\\left(\\ln\u03b4^-1 + \\mathsf{Ent}(I)\\right)+\\Delta_[2]^-2\\ln\\ln\\Delta_[2]^-1\\mathrmpolylog(n,\u03b4^-1)\\right)\\]samples in expectation for any instance $I$. For the lower bound, we show that for any Gaussian Best-$1$-Arm instance with gaps of the form $2^-k$, any $\u03b4$-correct monotone algorithm requires at least \\[\u03a9\\left(H(I)\u22c5\\left(\\ln\u03b4^-1 + \\mathsf{Ent}(I)\\right)\\right)\\]samples in expectation. Here, a monotone algorithm is one which uses no more samples (in expectation) on $I\u2019$ than on $I$, if $I\u2019$ is a sub-instance of $I$ obtained by removing some sub-optimal arms.", "pdf_url": "http://proceedings.mlr.press/v65/chen17b/chen17b.pdf", "keywords": ["best arm identification", "instance optimality", "gap-entropy"], "reference": "Peyman Afshani, J\u00b4er\u00b4emy Barbay, and Timothy M Chan. Instance-optimal geometric algorithms. In Foundations of Computer Science, 2009. FOCS\u201909. 50th Annual IEEE Symposium on, pages 129\u2013138. IEEE, 2009.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identi\ufb01cation in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13\u2013p, 2010.  Robert E Bechhofer. A single-sample multiple decision procedure for ranking means of normal populations with known variances. The Annals of Mathematical Statistics, pages 16\u201339, 1954.  16   CHEN LI QIAO  We prove in "}, "Thresholding Based Outlier Robust PCA": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Thresholding Based Outlier Robust PCA", "abstract": "We consider the problem of outlier robust PCA (\\textbfOR-PCA) where the goal is to recover principal directions despite the presence of outlier data points. That is, given a data matrix $M^*$, where $(1-\u03b1)$ fraction of the points are noisy samples from a low-dimensional subspace while $\u03b1$ fraction of the points can be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for \\textbfOR-PCA\u00a0have serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime quadratic in dimension, rendering them impractical for large scale applications. In this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity at most linear in the data size. Moreover, the fraction of outliers, $\u03b1$, that our method can handle is tight up to constants while providing nearly optimal computational complexity for a general noise setting. For the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian noise, we show that a modification of our thresholding based method leads to significant improvement in recovery error (of the subspace) even in the presence of a large fraction of outliers.", "pdf_url": "http://proceedings.mlr.press/v65/cherapanamjeri17a/cherapanamjeri17a.pdf", "keywords": [], "reference": "Rajendra Bhatia. Matrix Analysis. Springer, 1997.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford university press, 2013.  Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  J. ACM, 58(3):11, 2011.  Yudong Chen, Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs. IEEE Trans. Information Theory, 62(1):503-526, 2016. doi: 10.1109/TIT.2015.2499247. URL http://dx.doi.org/ 10.1109/TIT.2015.2499247.  Yeshwanth Cherapanamjeri, Kartik Gupta, and Prateek Jain. Nearly-optimal robust matrix comple-  tion. CoRR, abs/1606.07315, 2016. URL http://arxiv.org/abs/1606.07315.  Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input In Symposium on Theory of Computing Conference, STOC\u201913, Palo Alto, CA, sparsity time. USA, June 1-4, 2013, pages 81-90, 2013. doi: 10.1145/2488608.2488620. URL http:// doi.acm.org/10.1145/2488608.2488620.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Jiashi Feng, Huan Xu, and Shuicheng Yan. Robust PCA in high-dimension: A deterministic ap- proach. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012. URL http://icml.cc/2012/ papers/136.pdf.  Jiashi Feng, Huan Xu, and Shuicheng Yan. Online robust PCA via stochastic optimization.  In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 404-412, 2013. URL http://papers.nips.cc/ paper/5131-online-robust-pca-via-stochastic-optimization.  17   THRESHOLDING BASED EFFICIENT OUTLIER ROBUST PCA  principal directions in operator norm. A challenging and important open problem is if the principal directions can be estimated in operator norm even in the presence of outliers.  \u221a  Similarly, for Gaussian noise, where each entry has variance \u03c32, our result obtains an error bound of O(\u03c3 n) which is significantly better than the Frobenius norm bound we get for arbitrary noise. Unfortunately, our algorithm for the Gaussian setting is nearly a factor of n slower than that for vanilla PCA. In order for this to be practical, it is very important to design an algorithm for this setting with nearly the same runtime as that of vanilla PCA.  References  Rajendra Bhatia. Matrix Analysis. Springer, 1997.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford university press, 2013.  Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?  J. ACM, 58(3):11, 2011.  Yudong Chen, Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs. IEEE Trans. Information Theory, 62(1):503-526, 2016. doi: 10.1109/TIT.2015.2499247. URL http://dx.doi.org/ 10.1109/TIT.2015.2499247.  Yeshwanth Cherapanamjeri, Kartik Gupta, and Prateek Jain. Nearly-optimal robust matrix comple-  tion. CoRR, abs/1606.07315, 2016. URL http://arxiv.org/abs/1606.07315.  Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input In Symposium on Theory of Computing Conference, STOC\u201913, Palo Alto, CA, sparsity time. USA, June 1-4, 2013, pages 81-90, 2013. doi: 10.1145/2488608.2488620. URL http:// doi.acm.org/10.1145/2488608.2488620.  Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. In IEEE 57th Robust estimators in high dimensions without the computational intractability. Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 655-664, 2016. doi: 10.1109/FOCS.2016.85. URL https://doi.org/10.1109/FOCS.2016.85.  Jiashi Feng, Huan Xu, and Shuicheng Yan. Robust PCA in high-dimension: A deterministic ap- proach. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012. URL http://icml.cc/2012/ papers/136.pdf.  Jiashi Feng, Huan Xu, and Shuicheng Yan. Online robust PCA via stochastic optimization.  In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 404-412, 2013. URL http://papers.nips.cc/ paper/5131-online-robust-pca-via-stochastic-optimization. CHERAPANAMJERI JAIN NETRAPALLI  Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University  Press, 3rd edition, 1996.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000.  Praneeth Netrapalli, Niranjan U N, Sujay Sanghavi, Animashree Anandkumar, and Prateek Jain. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and Non-convex robust pca. K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1107-1115. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/ 5430-non-convex-robust-pca.pdf.  Roman Vershynin.  Introduction to the non-asymptotic analysis of random matrices. CoRR, ab-  s/1011.3027, 2010. URL http://arxiv.org/abs/1011.3027.  Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit.  IEEE Trans. Information Theory, 58(5):3047-3064, 2012a. doi: 10.1109/TIT.2011.2173156. URL http://dx.doi.org/10.1109/TIT.2011.2173156.  Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust pca via outlier pursuit.  IEEE  Transactions on Information Theory, 58(5):3047-3064, 2012b.  Huan Xu, Constantine Caramanis, and Shie Mannor. Outlier-robust PCA: the high-dimensional case. IEEE Trans. Information Theory, 59(1):546-572, 2013. doi: 10.1109/TIT.2012.2212415. URL http://dx.doi.org/10.1109/TIT.2012.2212415.  Wenzhuo Yang and Huan Xu. A unified framework for outlier-robust pca-like algorithms. In Pro- ceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 484-493, 2015. URL http://jmlr.org/proceedings/papers/ v37/yangc15.html.  Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis. Fast algorithms for ro- In Advances in Neural Information Processing Systems 29: bust PCA via gradient descent. Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 4152-4160, 2016. URL http://papers.nips.cc/paper/ 6445-fast-algorithms-for-robust-pca-via-gradient-descent.  Hongyang Zhang, Zhouchen Lin, and Chao Zhang. Completing low-rank matrices with corrupted samples from few coefficients in general basis. IEEE Trans. Information Theory, 62(8):4748- 4768, 2016. doi: 10.1109/TIT.2016.2573311. URL http://dx.doi.org/10.1109/TIT. 2016.2573311.  "}, "Tight Bounds for Bandit Combinatorial Optimization": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Tight Bounds for Bandit Combinatorial Optimization", "abstract": "We revisit the study of optimal regret rates in bandit combinatorial optimization\u2014a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as $\\widetilde\u0398(k^3/2\\sqrt{d}T)$ where $d$ is the dimension of the problem and $k$ is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form $\\widetilde\u0398(k\\sqrt{d}T)$. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "pdf_url": "http://proceedings.mlr.press/v65/cohen17a/cohen17a.pdf", "keywords": [], "reference": "Jacob Duncan Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An e\ufb03cient algorithm for bandit linear optimization. In 21st Annual Conference on Learning Theory, 2008.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial  optimization. Mathematics of Operations Research, 39(1):31-45, 2013.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Baruch Awerbuch and Robert D Kleinberg. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 45-53. ACM, 2004.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1): 1-122, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies  for online linear optimization with bandit feedback. In COLT, volume 23, 2012.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and  System Sciences, 78(5):1404-1422, 2012.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Andr\u00b4as Gy\u00a8orgy, Tam\u00b4as Linder, G\u00b4abor Lugosi, and Gy\u00a8orgy Ottucs\u00b4ak. The on-line shortest path problem under partial monitoring. Journal of Machine Learning Research, 8(Oct): 2369-2403, 2007.  Elad Hazan and Zohar Karnin. Volumetric spanners: An e\ufb03cient exploration basis for  learning. Journal of Machine Learning Research, 17:1-34, 2016.  David P Helmbold and Manfred K Warmuth. Learning permutations with exponential  weights. Journal of Machine Learning Research, 10(Jul):1705-1736, 2009.  Adam Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision problems. Jour-  nal of Computer and System Sciences, 71(3):291-307, 2005.  Satyen Kale, Lev Reyzin, and Robert E Schapire. Non-stochastic bandit slate problems. In  Advances in Neural Information Processing Systems, pages 1054-1062, 2010.  H Brendan McMahan and Avrim Blum. Online geometric optimization in the bandit setting against an adaptive adversary. In International Conference on Computational Learning Theory, pages 109-123. Springer, 2004.  13   Tight Bounds for Bandit Combinatorial Optimization  References  Jacob Duncan Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An e\ufb03cient algorithm for bandit linear optimization. In 21st Annual Conference on Learning Theory, 2008.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial  optimization. Mathematics of Operations Research, 39(1):31-45, 2013.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.  Baruch Awerbuch and Robert D Kleinberg. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 45-53. ACM, 2004.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1): 1-122, 2012.  S\u00b4ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies  for online linear optimization with bandit feedback. In COLT, volume 23, 2012.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and  System Sciences, 78(5):1404-1422, 2012.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2008.  Andr\u00b4as Gy\u00a8orgy, Tam\u00b4as Linder, G\u00b4abor Lugosi, and Gy\u00a8orgy Ottucs\u00b4ak. The on-line shortest path problem under partial monitoring. Journal of Machine Learning Research, 8(Oct): 2369-2403, 2007.  Elad Hazan and Zohar Karnin. Volumetric spanners: An e\ufb03cient exploration basis for  learning. Journal of Machine Learning Research, 17:1-34, 2016.  David P Helmbold and Manfred K Warmuth. Learning permutations with exponential  weights. Journal of Machine Learning Research, 10(Jul):1705-1736, 2009.  Adam Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision problems. Jour-  nal of Computer and System Sciences, 71(3):291-307, 2005.  Satyen Kale, Lev Reyzin, and Robert E Schapire. Non-stochastic bandit slate problems. In  Advances in Neural Information Processing Systems, pages 1054-1062, 2010.  H Brendan McMahan and Avrim Blum. Online geometric optimization in the bandit setting against an adaptive adversary. In International Conference on Computational Learning Theory, pages 109-123. Springer, 2004. Cohen Hazan Koren  Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Proceedings of  The 28th Conference on Learning Theory, pages 1360-1375, 2015.  Gergely Neu and G\u00b4abor Bart\u00b4ok. Importance weighting without importance weights: An e\ufb03cient algorithm for combinatorial semi-bandits. Journal of Machine Learning Research, 17(154):1-21, 2016.  Ohad Shamir. On the complexity of bandit linear optimization. In Proceedings of The 28th  Conference on Learning Theory, pages 1523-1551, 2015.  Eiji Takimoto and Manfred K Warmuth. Path kernels and multiplicative updates. Journal  of Machine Learning Research, 4(Oct):773-818, 2003. "}, "Online Learning Without Prior Information": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Online Learning Without Prior Information", "abstract": "The vast majority of optimization and online learning algorithms today require some prior information about the data (often in the form of bounds on gradients or on the optimal parameter value). When this information is not available, these algorithms require laborious manual tuning of various hyperparameters, motivating the search for algorithms that can adapt to the data with no prior information. We  describe a frontier of new lower bounds on the performance of such algorithms, reflecting a tradeoff between a term that depends on the optimal parameter value and a term that depends on the gradients\u2019 rate of growth. Further, we construct a family of algorithms whose performance matches any desired point on this frontier, which no previous algorithm reaches.", "pdf_url": "http://proceedings.mlr.press/v65/cutkosky17a/cutkosky17a.pdf", "keywords": [], "reference": "Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the nineteenth annual conference on computa- tional learning theory, 2008.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and  losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimiza-  tion. In Conference on Learning Theory (COLT), 2010.  Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization.  In Advances in Neural Information Processing Systems, pages 2724-2732, 2013.  Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimiza-  tion. In Advances in neural information processing systems, pages 2402-2410, 2012.  H. Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. arXiv preprint  arXiv:1403.3465, 2014.  12   CUTKOSKY BOAHEN  7. Conclusions  We have presented a frontier of lower bounds on the worst-case regret of any online convex opti- mization algorithm without prior information. This frontier demonstrates a fundamental trade-off at work between kuLmax log\u03b3(T u + 1) and exp  terms. We also present  maxt  (cid:17) 1  (cid:20)(cid:16)  2\u03b3\u22121  (cid:21)  Lt \u03b3k2Lt\u22121  some easy-to-use theorems that allow us to construct algorithms that match our lower bound for any chosen k and \u03b3. Note that by virtue of not requiring prior information, our algorithms are nearly hyperparameter-free. They only require the essentially unavoidable trade-off parameters k and \u03b3. Since our analysis does not make assumptions about the loss functions or comparison point u, the parameters k and \u03b3 can be freely chosen by the user. Unlike other algorithms that require (cid:107)u(cid:107) or Lmax, there are no unknown constraints on these parameters.  Our results also open a new perspective on optimization algorithms by casting using prior in- formation as a tool to avoid the exponential penalty. Previous algorithms that require bounds on the diameter of W or Lmax can be viewed as addressing this issue. We show that it also possible to avoid the exponential penalty by using a known bound on maxt Lt/Lt\u22121, leading to a regret of \u02dcO((cid:107)u(cid:107)Lmax  (cid:112)T maxt Lt/Lt\u22121).  Although we answer some important questions, there is still much to do in online learning without prior information. For example, it is possible to obtain O((cid:107)u(cid:107)2Lmax T ) regret without prior information (Orabona and P\u00b4al, 2016b), so it should be possible to extend our lower-bound frontier beyond (cid:107)u(cid:107) log((cid:107)u(cid:107)). Further, it would be valuable to further characterize the conditions for which the adversary can guarantee regret that is exponential in T . We showed that one such condition is that there must be a large jump in the value of Lt, but there may very well be others. Fully characterizing these conditions should allow us design algorithms that smoothly interpolate between \u201cnice\u201d environments that do not satisfy the conditions and fully adversarial ones that do.  \u221a  Finally, while our analysis allows for the use of arbitrary norms, we focus our examples on the L2 norm. It may be interesting to design adaptive regularizers with respect to a more diverse set of norms, or to extend our theory to encompass time-changing norms.  References  Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the nineteenth annual conference on computa- tional learning theory, 2008.  Ashok Cutkosky and Kwabena A Boahen. Online convex optimization with unconstrained domains and  losses. In Advances in Neural Information Processing Systems 29, pages 748-756, 2016.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimiza-  tion. In Conference on Learning Theory (COLT), 2010.  Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization.  In Advances in Neural Information Processing Systems, pages 2724-2732, 2013.  Brendan McMahan and Matthew Streeter. No-regret algorithms for unconstrained online convex optimiza-  tion. In Advances in neural information processing systems, pages 2402-2410, 2012.  H. Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. arXiv preprint  arXiv:1403.3465, 2014. ONLINE LEARNING WITHOUT PRIOR INFORMATION  H Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Min- imax algorithms and normal approximations. In Conference on Learning Theory (COLT), pages 1020- 1039, 2014.  H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization.  In Conference on Learning Theory (COLT), 2010.  Francesco Orabona. Dimension-free exponentiated gradient. In Advances in Neural Information Processing  Systems, pages 1806-1814, 2013.  Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochastic learn-  ing. In Advances in Neural Information Processing Systems, pages 1116-1124, 2014.  Francesco Orabona and D\u00b4avid P\u00b4al. Coin betting and parameter-free online learning. In Advances in Neural  Information Processing Systems 29, pages 577-585, 2016a.  Francesco Orabona and D\u00b4avid P\u00b4al. Scale-free online learning. arXiv preprint arXiv:1601.01974, 2016b.  Francesco Orabona, Koby Crammer, and Nicolo Cesa-Bianchi. A generalized online mirror descent with  applications to classification and regression. Machine Learning, 99(3):411-435, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine  Learning, 4(2):107-194, 2011.  "}, "Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent", "abstract": "In this paper, we revisit the recently established theoretical guarantees for the convergence of the Langevin Monte Carlo algorithm of sampling from a smooth and (strongly) log-concave density. We improve the existing results when the convergence is measured in the Wasserstein distance and provide further insights on the very tight relations between, on the one hand, the Langevin Monte Carlo for sampling and, on the other hand, the gradient descent for optimization. Finally, we also establish guarantees for the convergence of a version of the Langevin Monte Carlo algorithm that is based on noisy evaluations of the gradient.", "pdf_url": "http://proceedings.mlr.press/v65/dalalyan17a/dalalyan17a.pdf", "keywords": ["Markov Chain Monte Carlo", "Approximate sampling", "Rates of convergence", "Langevin algorithm", "Gradient descent"], "reference": "Y. Atchad\u00b4e, G. Fort, E. Moulines, and P. Priouret. Adaptive Markov chain Monte Carlo: theory and methods. In Bayesian time series models, pages 32-51. Cambridge Univ. Press, Cambridge, 2011.  R. N. Bhattacharya. Criteria for recurrence and existence of invariant measures for multidimensional  diffusions. Ann. Probab., 6(4):541-553, 08 1978.  11   FURTHER ANALOGY BETWEEN SAMPLING AND OPTIMIZATION  In view of Fubini\u2019s theorem, we arrive at  (cid:90)  R  f (cid:48)(x)2 \u03c0(x) dx =  f (cid:48)(cid:48)(y) \u03c0(y) dy +  f (cid:48)(cid:48)(y) \u03c0(y) dy \u2264 M.  (cid:90) \u221e(cid:90) 0  \u2212\u221e  This completes the proof.  Proof [Proof of Lemma 3] Since the process L is stationary, V (a) has the same distribution as V (0). For this reason, it suffices to prove the claim of the lemma for a = 0 only. Using the Lipschitz continuity of f , we get  E[(cid:107)V (0)(cid:107)2  2] = E  (cid:0)\u2207f (Lt) \u2212 \u2207f (L0) dt(cid:1)(cid:13) 2 (cid:13) (cid:13) 2  (cid:105)  (cid:104)(cid:13) (cid:13) (cid:13) (cid:90) h(cid:90) h  0 E(cid:2)(cid:13)  (cid:90) h\u2264 h  (cid:13)\u2207f (Lt) \u2212 \u2207f (L0)(cid:13) 2 (cid:13) 2  (cid:3) dt  \u2264 hM 2  E(cid:2)(cid:13)  (cid:13)Lt \u2212 L0  (cid:13) 2 (cid:13) 2  (cid:3) dt.  Combining this inequality with the stationarity of Lt, we arrive at  (cid:16)  E[(cid:107)V (0)(cid:107)2 2]  (cid:17)1/2  \u2264  hM 2  \u2207f (Ls) ds +  2 W t  \u221a  (cid:19)1/2  (cid:13) 2 (cid:13) 2  (cid:3) dt  (cid:90) t(cid:90) h  0 (cid:90) hE(cid:2)(cid:13)  (cid:13) \u2212  E(cid:2)(cid:13) (cid:13)  (cid:90) t\u2264  hM 2  (cid:18)  (cid:18)  (cid:18)  \u2264  =  (cid:18) 1 3  \u2207f (Ls) ds(cid:13) 2 (cid:13) 2  (cid:3) dt  (cid:19)1/2  (cid:18)  (cid:90) h  (cid:19)1/2  +  2hpM 2  t dt  hM 2E(cid:2)(cid:13)  (cid:13)\u2207f (L0)(cid:13) 2 (cid:13) 2  (cid:3)  (cid:90) h  (cid:19)1/2  (cid:18)  t2 dt  +  2hpM 2  h4M 2E(cid:2)(cid:13)  (cid:13)\u2207f (L0)(cid:13) 2 (cid:13) 2  + (cid:0)h3M 2p(cid:1)1/2.(cid:3)  (cid:19)1/2  (cid:90) h  (cid:19)1/2t dtTo complete the proof, it suffices to apply Lemma 2.  The work of the author was partially supported by the grant Investissements d\u2019Avenir (ANR-11- IDEX-0003/Labex Ecodec/ANR-11-LABX-0047).  Acknowledgments  References  Y. Atchad\u00b4e, G. Fort, E. Moulines, and P. Priouret. Adaptive Markov chain Monte Carlo: theory and methods. In Bayesian time series models, pages 32-51. Cambridge Univ. Press, Cambridge, 2011.  R. N. Bhattacharya. Criteria for recurrence and existence of invariant measures for multidimensional  diffusions. Ann. Probab., 6(4):541-553, 08 1978. DALALYAN  S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge,  2004.  S. Bubeck, R. Eldan, and J. Lehec. Sampling from a log-concave distribution with Projected  Langevin Monte Carlo. ArXiv e-prints, July 2015.  A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. ArXiv e-prints, December 2014.  A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin Monte-  Carlo. J. Comput. System Sci., 78(5):1423-1443, 2012.  A. Durmus and E. Moulines. High-dimensional Bayesian inference via the Unadjusted Langevin  Algorithm. ArXiv e-prints, May 2016.  Alain Durmus, Eric Moulines, and Marcelo Pereyra. Sampling from convex non continuously differentiable functions, when Moreau meets Langevin. February 2016. URL https://hal. archives-ouvertes.fr/hal-01267115.  L. Lov\u00b4asz and S. Vempala. Hit-and-run from a corner. SIAM J. Comput., 35(4):985-1005 (elec-  tronic), 2006a.  L. Lov\u00b4asz and S. Vempala. Fast algorithms for logconcave functions: Sampling, rounding, inte- gration and optimization. In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006), 21-24 October 2006, Berkeley, California, USA, Proceedings, pages 57-68, 2006b.  Walter Rudin. Real and complex analysis. McGraw-Hill Book Co., New York, third edition, 1987. "}, "Depth Separation for Neural Networks": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Depth Separation for Neural Networks", "abstract": "Let $f:\\mathbb{S}^d-1\\times \\mathbb{S}^d-1\\to\\mathbb{S}$ be a function of the form $f(x,x\u2019) = g(\u27e8x,x\u2019\u27e9)$ for $g:[-1,1]\\to \\mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$\u2019s, such as $g(x)=\\sin(\\pi d^3x)$, the number of neurons must be $2^\u03a9\\left(d\\log(d)\\right)$. Furthermore, the result holds w.r.t. the uniform distribution on $\\mathbb{S}^d-1\\times \\mathbb{S}^d-1$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t. the uniform distribution on $\\mathbb{S}^d-1\\times \\mathbb{S}^d-1$.", "pdf_url": "http://proceedings.mlr.press/v65/daniely17a/daniely17a.pdf", "keywords": ["Neural Networks", "Depth Separation", "Uniform Distribution"], "reference": "K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An Intro-  duction, volume 2044. Springer, 2012.  Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. In 29th Annual Conference on Learning Theory, pages 698-728, 2016.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 1989.  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in  Neural Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In 29th  Annual Conference on Learning Theory, pages 907-940, 2016.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Sys- tems, pages 2877-2885, 2013.  6   DANIELY  weights bounded by 4. For each i \u2208 [d] we can compose the linear function (x, x(cid:48)) (cid:55)\u2192 xi + x(cid:48) i with Nsquare to get a depth-2 network Ni that calculates (xi+x(cid:48) i)2 (cid:15) 2dL and has the same width and weight bound as Nsquare. Summing the networks Ni and subtracting 1 results with a depth-2 network Ninner that calculates (cid:104)x, x(cid:48)(cid:105) with an error of (cid:15) and hidden layer weights bounded by 2, and prediction layer weights bounded by 4.  2L and has width 16d2L  with an error of(cid:15)  Now, again by lemma 5 there is a depth-2 network Nf that calculates f in [\u22121, 1], with an error of (cid:15) 2 , has width at most 2L (cid:15) , hidden layer weights bounded by 1 and prediction layer weights bounded by 2L, and is L-Lipschitz. Finally, consider the depth-3 network NF that is the composition of Ninner and Nf . NF has width at most 16d2L  (cid:15) weight bound of max(4, 2L), and it satisfies  |NF (x, x(cid:48)) \u2212 F (x, x(cid:48))| = |Nf (Ninner(x, x(cid:48))) \u2212 f ((cid:104)x, x(cid:48)(cid:105))|  \u2264 |Nf (Ninner(x, x(cid:48))) \u2212 Nf ((cid:104)x, x(cid:48)(cid:105))| + |Nf ((cid:104)x, x(cid:48)(cid:105)) \u2212 f ((cid:104)x, x(cid:48)(cid:105))| (cid:15) \u2264 L|Ninner(x, x(cid:48)) \u2212 (cid:104)x, x(cid:48)(cid:105)| + 2  \u2264 L  +  = (cid:15)  (cid:15) 2L  (cid:15) 2  (cid:3)  References  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An Intro-  duction, volume 2044. Springer, 2012.  Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. In 29th Annual Conference on Learning Theory, pages 698-728, 2016.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 1989.  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in  Neural Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In 29th  Annual Conference on Learning Theory, pages 907-940, 2016.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Sys- tems, pages 2877-2885, 2013. DEPTH SEPARATION FOR NEURAL NETWORKS  Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-  linear functions. arXiv preprint arXiv:1610.09887, 2016.  Matus Telgarsky. Representation benefits of deep feedforward networks. In COLT, 2016. "}, "Square Hellinger Subadditivity for Bayesian Networks and its Applications to Identity Testing": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Square Hellinger Subadditivity for Bayesian Networks and its Applications to Identity Testing", "abstract": "We show that the square Hellinger distance between two Bayesian networks on the same directed graph, $G$, is subadditive with respect to the neighborhoods of $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two Bayesian networks on the same DAG, our inequality states that the square Hellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the sum, $\\sum_v H^2(P_{v} \u222a\\Pi_v, Q_{v} \u222a\\Pi_v)$, of the square Hellinger distances between the marginals of $P$ and $Q$ on every node $v$ and its parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the conditionals but the marginals of $P$ and $Q$. We derive a similar inequality for more general Markov Random Fields. As an application of our inequality, we show that distinguishing whether two (unknown) Bayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy $P=Q$ vs $d_\\rm TV(P,Q)>\u03b5$ can be performed from $\\tilde{O}(|\u03a3|^3/4(d+1) \u22c5n/\u03b5^2)$ samples, where $d$ is the maximum in-degree of the DAG and $\u03a3$ the domain of each variable of the Bayesian networks. If $P$ and $Q$ are defined on potentially different and potentially unknown trees, the sample complexity becomes $\\tilde{O}(|\u03a3|^4.5 n/\u03b5^2)$. In both cases the dependence of the sample complexity on $n, \u03b5$ is optimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product distributions over ${0,1}^n$ and $Q$ is known, the sample complexity becomes $O(\\sqrt{n}/\u03b5^2)$, which is optimal up to constant factors.", "pdf_url": "http://proceedings.mlr.press/v65/daskalakis17a/daskalakis17a.pdf", "keywords": [], "reference": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In Proceedings of the 2014 IEEE  5   TESTING BAYESNETS  (pi\u2212qi)2 qi  Noticing that (cid:80) is an identical expression to the \u03c72 divergence applied to vectors i (p1, . . . , pn) and (q1, . . . , qn), we reduce the problem to a \u03c72-test, mimicking the approach of Acharya et al. (2015). We only need to be careful that (cid:80) i qi do not necessarily equal 1, but this does not create any issues.  i pi and (cid:80)  Learning vs Testing. A natural approach to testing the equality between two Bayes-nets P and Q is to first use samples from P and Q to learn Bayes nets \u02c6P and \u02c6Q that are respectively close to P and Q, then compare \u02c6P and \u02c6Q of\ufb02ine, i.e. without drawing further samples from P and Q. While this approach has been used successfully for single-dimensional hypothesis testing, see e.g. Acharya et al. (2015), it presents analytical and computational difficulties in the high-dimensional regime. While learning of Bayes nets has been a topic of intense research, including the celebrated Chow-Liu algorithm for tree-structured Bayes-nets Chow and Liu (1968), we are aware of no computationally efficient algorithms that operate with \u02dcO(n/(cid:15)2) samples without assumptions. In particular, using net-based techniques Devroye and Lugosi (2001); Daskalakis and Kamath (2014); Acharya et al. (2014), standard calculations show that any Bayes-net on n variables and maximum in-degree d can be learned from \u02dcO( n\u00b7|\u03a3|d ) samples, but this algorithm is highly-inefficient computationally (expo- (cid:15)2 nential in n). Our algorithms are both efficient, and beat the sample complexity of this inefficient algorithm. On the efficient algorithms front, we are only aware of efficient algorithms that provide guarantees when the number of samples is >> n\u00b7|\u03a3|d or that place assumptions on the parameters (cid:15)2 or the structure of the Bayes-net to be able to learn it (see e.g. Anandkumar et al. (2012); Bresler (2015) and their references), even when the structure is a tree Chow and Liu (1968). Our algorithms do not need any assumptions on the parameters or the structure of the Bayes-net.  Comparison to Canonne et al. (2016b). Testing problems on Bayes-nets similar to the ones we study here are also considered, independently and contemporaneously, in Canonne et al. (2016b) for binary alphabets. However, their emphasis is quite different. Instead of trying to formulate efficient sample-optimal algorithms that work for all cases, they try to identify assumptions under which testing can be done from sub-linear in the number of nodes, n, samples. For example, for the goodness-of-fit problem where the structure of the unknown distribution P is known to be the same as that of the known distribution Q, they require that, for all nodes v, the conditional probability of v taking any value, conditioning on any assignment to the parent nodes, be bounded away from 0 by at least \u2126(1/ n). They also assume that the parents of every node attain any configuration with probability at least \u2126(1/ n). (See Definition 5.1 and Theorem 5.2 in Canonne et al. (2016b).) In the case the structure of P is unknown, they further assume that its structure contains less edges than that of Q. Moreover, they need that Q satisfies a \u201c\u03b3-non-degeneracy\u201d condition, which, roughly speaking, prohibits Q from being close to any distribution that satisfies additional conditional inde- pendence beyond that already implied by Q\u2019s graphical structure. (See Definition 6.10 and Theorem 6.11 in Canonne et al. (2016b).) In contrast, we do not make any assumptions, and obtain optimal results for our unconditional testing questions. As we show the testing complexity depends linearly in n.  \u221a  \u221a  References  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In Proceedings of the 2014 IEEE DASKSLAKIS PAN  International Symposium on Information Theory, ISIT \u201914, pages 1682-1686, Washington, DC, USA, 2014. IEEE Computer Society.  Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties In Advances in Neural Information Processing Systems 28, NIPS \u201915, pages  of distributions. 3577-3598. Curran Associates, Inc., 2015.  Noga Alon, Alexandr Andoni, Tali Kaufman, Kevin Matulef, Ronitt Rubinfeld, and Ning Xie. Test- In Proceedings of the thirty-ninth annual ACM  ing k-wise and almost k-wise independence. symposium on Theory of computing (STOC), 2007.  Animashree Anandkumar, Vincent YF Tan, Furong Huang, and Alan S Willsky. High-dimensional structure estimation in ising models: Local separation criterion. The Annals of Statistics, pages 1346-1375, 2012.  Tugkan Batu, Eldar Fischer, Lance Fortnow, Ravi Kumar, Ronitt Rubinfeld, and Patrick White.  Testing random variables for independence and identity. In Proceedings of FOCS, 2001.  Tu\u02d8gkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the 36th Annual ACM Symposium on the Theory of Computing, STOC \u201904, New York, NY, USA, 2004. ACM.  Arnab Bhattacharyya, Eldar Fischer, Ronitt Rubinfeld, and Paul Valiant. Testing monotonicity of  distributions over general partial orders. In Innovations in Computer Science (ICS), 2011.  Guy Bresler. Efficiently learning ising models on arbitrary graphs. In Proceedings of the Forty- Seventh Annual ACM on Symposium on Theory of Computing, STOC \u201915, pages 771-782, New York, NY, USA, 2015. ACM.  Cl\u00b4ement Canonne, Ilias Diakonikolas, Themistoklis Gouleakis, and Ronitt Rubinfeld. Testing Shape Restrictions of Discrete Distributions. In the 33rd International Symposium on Theoretical As- pects of Computer Science (STACS), 2016a.  Clement Canonne, Ilias Diakonikolas, Daniel Kane, and Alistair Stewart. Testing bayesian net-  works. arXiv preprint arXiv:1612.03156, 2016b.  CK Chow and CN Liu. Approximating discrete probability distributions with dependence trees.  Information Theory, IEEE Transactions on, 14(3):462-467, 1968.  Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of Gaussians. In Proceedings of the 27th Annual Conference on Learn- ing Theory, COLT \u201914, pages 1183-1213, 2014.  Constantinos Daskalakis and Qinxuan Pan. Square Hellinger Subadditivity for Bayesian Networks and its Applications to Identity Testing. arXiv, 2016. http://arxiv.org/abs/1612. 03164.  Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising Models. arXiv,  2016. TESTING BAYESNETS  Luc Devroye and G\u00b4abor Lugosi. Combinatorial methods in density estimation. Springer, 2001.  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data.  Information Theory, IEEE Transactions on, 54(10):4750-4755, 2008.  Ronitt Rubinfeld and Ning Xie. Testing non-uniform k-wise independent distributions over prod- In the 37th International Colloquium on Automata, Languages and Programming  uct spaces. (ICALP), 2010.  Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity test- ing. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201914, pages 51-60, Washington, DC, USA, 2014. IEEE Computer Society. "}, "Ten Steps of EM Suffice for Mixtures of Two Gaussians": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Ten Steps of EM Suffice for Mixtures of Two Gaussians", "abstract": "The Expectation-Maximization (EM) algorithm is a widely used method for maximum likelihood estimation in models with latent variables. For estimating mixtures of Gaussians, its iteration can be viewed as a soft version of the k-means clustering algorithm. Despite its wide use and applications, there are essentially no known convergence guarantees for this method. We provide global convergence guarantees for  mixtures of two Gaussians with known covariance matrices. We show that the population version of EM, where the algorithm is given access to infinitely many samples from the mixture, converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that, in one dimension, ten steps of the EM algorithm initialized at infinity result in less than $1%$ error estimation of the means. In the finite sample regime, we show that, under a random initialization, $\\tilde{O}(d/\u03b5^2)$ samples suffice to compute the unknown vectors to within $\u03b5$ in Mahalanobis distance, where $d$ is the dimension. In particular, the error rate of the EM based estimator is $\\tilde{O}\\left(\\sqrt{d} \\over n\\right)$ where $n$ is the number of samples, which is optimal up to logarithmic factors.", "pdf_url": "http://proceedings.mlr.press/v65/daskalakis17b/daskalakis17b.pdf", "keywords": [], "reference": "Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  International Conference on Computational Learning Theory, pages 458-469. Springer, 2005.  5  Popula\u2019on\tEM\tPopula\u2019on\tEM\twith\t\tperturbed\tcenter\t\u03b5\u03c3\tEM\twith\t\tfinite\tsamples\t\u03b5\u03c3\t+\t\u03b5(\u03bc-\u03bb)\t\tPopula;on\tEM\twith\t\tperturbed\tcenter\t TEN STEPS OF EM SUFFICE  (a)  (b)  Figure 2: (a) Coupling correctly and incorrectly centered population EM updates. We show that, starting from the same iterate, the correctly and incorrectly centered population EM up- dates will land to close-by points. (b) Coupling incorrectly centered population EM and finite sample EM updates. We show that, starting from the same iterate, the incorrectly centered population EM update and the finite sample update land to close-by points.  scenarios despite the lack of theoretical guarantees. Recently, Balakrishnan et al. (2017) studied the convergence of EM in the case of an equal-weight mixture of two Gaussians with the same and known covariance matrix, showing local convergence guarantees. In particular, they show that when EM is initialized close enough to the actual parameters, then it converges. In this work, we revisit the same setting considered by Balakrishnan et al. (2017) but establish global convergence guarantees. We show that, for any initialization of the parameters, the EM algorithm converges geometrically to the true parameters. We also provide a simple and explicit formula for the rate of convergence.  Concurrent and independent work by Xu, Hsu and Maleki Xu et al. (2016) has also provided global and geometric convergence guarantees for the same setting, as well as a slightly more general setting where the mean of the mixture is unknown, but they do not provide explicit convergence rates. They also do not provide an analysis of the finite-sample regime.  We thank Sham Kakade for suggesting the problem to us, and for initial discussions. The au- thors were supported by NSF Awards CCF-0953960 (CAREER), CCF-1551875, CCF-1617730, and CCF-1650733, ONR Grant N00014-12-1-0999, a Microsoft Faculty Fellowship, and a Simons Graduate Fellowship.  Acknowledgments  References  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  International Conference on Computational Learning Theory, pages 458-469. Springer, 2005.Popula\u2019on\tEM\tPopula\u2019on\tEM\twith\t\tperturbed\tcenter\t\u03b5\u03c3\tEM\twith\t\tfinite\tsamples\t\u03b5\u03c3\t+\t\u03b5(\u03bc-\u03bb)\t\tPopula;on\tEM\twith\t\tperturbed\tcenter\t DASKALAKIS TZAMOS ZAMPETAKIS  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the  thirty-third annual ACM symposium on Theory of computing, pages 247-257. ACM, 2001.  Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the em algo- rithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77-120, 2017.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103-112. IEEE, 2010.  S Charles Brubaker and Santosh S Vempala. Isotropic PCA and affine-invariant clustering. In the  49th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2008.  Kamalika Chaudhuri and Satish Rao. Learning Mixtures of Product Distributions Using Correla- tions and Independence. In the 21st International Conference on Computational Learning Theory (COLT), 2008.  Kamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani. Learning mixtures of gaussians using  the k-means algorithm. arXiv preprint arXiv:0912.0086, 2009.  St\u00b4ephane Chr\u00b4etien and Alfred O Hero. On EM algorithms and their proximal generalizations.  ESAIM: Probability and Statistics, 12:308-326, 2008.  Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th  Annual Symposium on, pages 634-644. IEEE, 1999.  Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of em for mixtures of separated,  spherical gaussians. Journal of Machine Learning Research, 8(Feb):203-226, 2007.  Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for In Proceedings of The 27th Conference on Learning  proper learning mixtures of gaussians. Theory, pages 1183-1213, 2014.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten Steps of EM Suffice for  Mixtures of Two Gaussians. arXiv, 2016. http://arxiv.org/abs/1609.00368.  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society. Series B (methodological), pages 1-38, 1977.  Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen-  sions. In the 47th Annual ACM on Symposium on Theory of Computing (STOC), 2015.  Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 753-760. ACM, 2015.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In the 4th conference on Innovations in Theoretical Computer Science (ITCS), 2013. TEN STEPS OF EM SUFFICE  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553-562. ACM, 2010.  Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture models. In the 18th International Conference on Computational Learning Theory (COLT), 2005.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93- 102. IEEE, 2010.  Richard A Redner and Homer F Walker. Mixture densities, maximum likelihood and the EM algo-  rithm. SIAM review, 26(2):195-239, 1984.  Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal- sample estimators for spherical gaussian mixtures. In Advances in Neural Information Processing Systems, pages 1395-1403, 2014.  Paul Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathemat-  ics of Operations Research, 29(1):27-44, 2004.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of  Computer and System Sciences, 68(4):841-860, 2004.  CF Jeff Wu. On the convergence properties of the EM algorithm. The Annals of statistics, pages  95-103, 1983.  of two Gaussians. (NIPS), 2016.  Ji Xu, Daniel Hsu, and Arian Maleki. Global analysis of Expectation Maximization for mixtures In the 30th Annual Conference on Neural Information Processing Systems "}, "Learning Multivariate Log-concave Distributions": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Learning Multivariate Log-concave Distributions", "abstract": "We study the problem of estimating multivariate log-concave probability density functions. We prove the first sample complexity upper bound for learning log-concave densities on $\\mathbb{R}^d$, for all $d \u22651$. Prior to our work, no upper bound on the sample complexity of this learning problem was known for the case of $d>3$. In more detail, we give an estimator that, for any $d \\ge 1$ and $\u03b5>0$, draws $\\tilde{O}_d \\left( (1/\u03b5)^(d+5)/2 \\right)$ samples from an unknown target log-concave density on $R^d$, and outputs a hypothesis that (with high probability) is $\u03b5$-close to the target, in total variation distance. Our upper bound on the sample complexity comes close to the known lower bound of $\\Omega_d \\left( (1/\u03b5)^(d+1)/2 \\right)$ for this problem.", "pdf_url": "http://proceedings.mlr.press/v65/diakonikolas17a/diakonikolas17a.pdf", "keywords": ["density estimation", "log-concave densities", "VC inequality"], "reference": "2015a.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. In NIPS,  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015b.  13   LEARNING MULTIVARIATE LOG-CONCAVE DISTRIBUTIONS  and therefore,  n = O(d/(cid:15))(d+1)/2 log2(1/(cid:15)) .  For the claim comparing the variation distance to (cid:107) \u00b7 (cid:107)Ad,c(cid:15), note that, by Lemma 9, if c is chosen to be sufficiently small, there exist g, g(cid:48) \u2208 Cd,c(cid:15) so that dTV (f, g), dTV (f (cid:48), g(cid:48)) \u2264 (cid:15)/8. We then have that  dTV (f, f (cid:48)) \u2264 dTV (f, g) + dTV (f (cid:48), g(cid:48)) + dTV (g, g(cid:48))  \u2264 (cid:15)/4 + (cid:107)g \u2212 g(cid:48)(cid:107)Ad,c(cid:15) \u2264 (cid:15)/4 + (cid:107)f \u2212 f (cid:48)(cid:107)Ad,c(cid:15) + dTV (f, g) + dTV (f (cid:48), g(cid:48)) \u2264 (cid:107)f \u2212 f (cid:48)(cid:107)Ad,c(cid:15) + (cid:15)/2 .  This completes the proof of Lemma 13.  The proof of Proposition 7 and Theorem 3 is now complete.  4. Conclusions  In this paper, we gave the first sample complexity upper bound for learning log-concave densities on Rd. Our upper bound agrees with the previously known lower bound up to a multiplicative factor of \u02dcOd((cid:15)\u22122). No sample complexity upper bound was previously known for this problem for any d > 3.  Our result is a step towards understanding the learnability of log-concave densities in multiple dimensions. A number of interesting open problems remain. We outline the two immediate ones here:  \u2022 What is the optimal sample complexity of log-concave density estimation? It is a plausible (cid:0)(1/(cid:15))d/2+2(cid:1). We conjecture that the correct answer, under the total variation distance, is \u0398d believe that a more sophisticated version of our structural approximation results could give such an upper bound. On the other hand, it seems likely that an adaptation of the construction in Kim and Samworth (2016) could yield a matching lower bound.  \u2022 Is there a polynomial time algorithm (as a function of the sample complexity) to learn log- concave densities on Rd? The estimator underlying this work (Lemma 6) has been previously exploited Chan et al. (2013, 2014a); Acharya et al. (2017) to obtain computationally efficient learning algorithms for d = 1 - in fact, running in sample near-linear time Acharya et al. (2017). Obtaining a computationally efficient algorithm for the case of general dimension is a challenging and important open question.  References  2015a.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. In NIPS,  J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the 34th ACM Symposium on Principles of Database Systems, PODS 2015, pages 249-263, 2015b. DIAKONIKOLAS KANE STEWART  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly- In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete linear time. Algorithms, SODA 2017, pages 1278-1289, 2017. Available at https://arxiv.org/abs/1506.00671.  M. Y. An. Log-concave probability distributions: Theory and statistical testing. Technical Report  Economics Working Paper Archive at WUSTL, Washington University at St. Louis, 1995.  M. Bagnoli and T. Bergstrom. Log-concave probability and its applications. Economic Theory, ISSN 09382259. URL http://www.jstor.org/stable/  26(2):pp. 445-469, 2005. 25055959.  F. Balabdaoui and C. R. Doss.  Inference for a Mixture of Symmetric Distributions under Log- Concavity. To appear in Journal of Bernoulli. Available at http://arxiv.org/abs/1411.4708, 2014.  Y. Baraud and L. Birge. Rho-estimators for shape restricted density estimation. Stochastic Processes  and their Applications, 126(12):3888 - 3912, 2016.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  G. Biau and L. Devroye. On the risk of estimates for block decreasing densities. Journal of Multi-  variate Analysis, 86(1):143 - 165, 2003.  L. Birg\u00b4e. Estimating a density under order restrictions: Nonasymptotic minimax risk. Annals of  Statistics, 15(3):995-1012, 1987a.  L. Birg\u00b4e. On the risk of histograms for estimating decreasing densities. Annals of Statistics, 15(3):  1013-1022, 1987b.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  E. M. Bronstein. Approximation of convex sets by polytopes. Journal of Mathematical Sciences,  153(6):727-762, 2008.  H. D. Brunk. On the estimation of parameters restricted by inequalities. The Annals of Mathematical  Statistics, 29(2):pp. 437-454, 1958. ISSN 00034851.  C. L. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of  discrete distributions. In STACS, pages 25:1-25:14, 2016.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b. LEARNING MULTIVARIATE LOG-CONCAVE DISTRIBUTIONS  Y. Chen and R. J. Samworth. Smoothed log-concave maximum likelihood estimation with applica-  tions. Statist. Sinica, 23:1373-1398, 2013.  M. Cule, R. Samworth, and M. Stewart. Maximum likelihood estimation of a multi-dimensional  log-concave density. Journal of the Royal Statistical Society: Series B, 72:545-607, 2010.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of Inde-  pendent Integer Random Variables. In FOCS, pages 217-226, 2013.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and its applications. In Proceedings of the 48th Annual ACM Symposium on the Theory of Computing, STOC \u201916, 2016.  L. Devroye and L. Gy\u00a8orfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  1985.  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.  I. Diakonikolas, M. Hardt, and L. Schmidt. Differentially private learning of structured discrete  distributions. In NIPS, pages 2566-2574, 2015.  I. Diakonikolas, D. M. Kane, and A. Stewart. Optimal learning via the fourier transform In Proceedings of the 29th Confer- Full version available at  for sums of independent integer random variables. ence on Learning Theory, COLT 2016, pages 831-849, 2016a. https://arxiv.org/abs/1505.00662.  I. Diakonikolas, D. M. Kane, and A. Stewart. Properly learning poisson binomial distributions in almost polynomial time. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, pages 850-878, 2016b. Full version available at https://arxiv.org/abs/1511.04066.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distri-  butions and its algorithmic applications. In Proceedings of STOC\u201916, 2016c.  I. Diakonikolas, D. M. Kane, and A. Stewart. Efficient Robust Proper Learning of Log-concave  Distributions. Arxiv report, 2016d.  C. R. Doss and J. A. Wellner. Global rates of convergence of the mles of log-concave and s-concave  densities. Ann. Statist., 44(3):954-981, 06 2016.  L. Dumbgen and K. Rufibach. Maximum likelihood estimation of a log-concave density and its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40-68, 2009.  L. Dumbgen, R. Samworth, and D. Schuhmacher. Approximation by log-concave distributions, with applications to regression. Ann. Statist., 39(2):702-730, 04 2011. doi: 10.1214/10-AOS853. DIAKONIKOLAS KANE STEWART  A.-L. Foug`eres. Estimation de densit\u00b4es unimodales. Canadian Journal of Statistics, 25:375-387,  1997.  Y. Gordon, M. Meyer, and S. Reisner. Volume approximation of convex sets by polytopes - a  constructive method. Stud. Math., 111:81-95, 1994.  Y. Gordon, M. Meyer, and S. Reisner. Constructing a polytope to approximate a convex body.  Geometriae Dedicata, 57(2):217-222, 1995.  U. Grenander. On the theory of mortality measurement. Skand. Aktuarietidskr., 39:125-153, 1956.  P. Groeneboom. Estimating a monotone density. In Proc. of the Berkeley Conference in Honor of  Jerzy Neyman and Jack Kiefer, pages 539-555, 1985.  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  P. M. Gruber. Aspects of approximation of convex bodies. Handbook of Convex Geometry, 1993.  Q. Han and J. A. Wellner. Approximation and estimation of s-concave densities via renyi diver-  gences. Ann. Statist., 44(3):1332-1359, 06 2016.  D. L. Hanson and G. Pledger. Consistency in concave regression. The Annals of Statistics, 4(6):pp.  1038-1050, 1976. ISSN 00905364.  P. J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73-101, 03 1964.  H. K. Jankowski and J. A. Wellner. Estimation of a discrete monotone density. Electronic Journal  of Statistics, 3:1567-1605, 2009.  bridge, MA, 1994.  M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cam-  A. K. H. Kim and R. J. Samworth. Global rates of convergence in log-concave density estimation.  Ann. Statist., 44(6):2756-2779, 12 2016. Available at http://arxiv.org/abs/1404.2298.  R. Koenker and I. Mizera. Quasi-concave density estimation. Ann. Statist., 38(5):2998-3027, 2010.  L. Lov\u00b4asz and S. Vempala. The geometry of logconcave functions and sampling algorithms. Ran-  dom Structures and Algorithms, 30(3):307-358, 2007.  K. Pearson. Contributions to the mathematical theory of evolution. ii. skew variation in homoge- neous material. Philosophical Trans. of the Royal Society of London, 186:343-414, 1895. doi: 10.1098/rsta.1895.0010.  B.L.S. Prakasa Rao. Estimation of a unimodal density. Sankhya Ser. A, 31:23-36, 1969.  A. Saumard and J. A. Wellner. Log-concavity and strong log-concavity: A review. Statist. Surv., 8:  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  45-114, 2014.  1992. LEARNING MULTIVARIATE LOG-CONCAVE DISTRIBUTIONS  A. Seregin and J. A. Wellner. Nonparametric estimation of multivariate convex-transformed densi-  ties. Ann. Statist., 38(6):3751-3781, 12 2010.  B. W. Silverman. Density Estimation. Chapman and Hall, London, 1986.  R. P. Stanley. Log-concave and unimodal sequences in algebra, combinatorics, and geome- try. Annals of the New York Academy of Sciences, 576(1):500-535, 1989. ISSN 1749- 6632. doi: 10.1111/j.1749-6632.1989.tb16434.x. URL http://dx.doi.org/10.1111/ j.1749-6632.1989.tb16434.x.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incor-  porated, 2008.  G. Valiant and P. Valiant.  In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 142-155, 2016.  Instance optimal learning of discrete distributions.  A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer Series  in Statistics. Springer-Verlag, New York, 1996. With applications to statistics.  V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their probabilities. Theory Probab. Appl., 16:264-280, 1971.  G. Walther. 2009.  Inference and modeling with log-concave distributions. Stat. Science, 24:319-327,  E.J. Wegman. Maximum likelihood estimation of a unimodal density. I. and II. Ann. Math. Statist.,  41:457-471, 2169-2174, 1970.  J. A. Wellner. Nonparametric estimation of s-concave and log-concave densities: an alternative to maximum likelihood. Talk given at European Meeting of Statisticians, Amsterdam, 2015. Available at https://www.stat.washington.edu/jaw/RESEARCH/TALKS/EMS-2015.1-rev1.pdf. "}, "Generalization for Adaptively-chosen Estimators via Stable Median": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Generalization for Adaptively-chosen Estimators via Stable Median", "abstract": "Datasets are often reused to perform multiple statistical analyses in an adaptive way, in which each analysis may depend on the outcomes of previous analyses on the same dataset. Standard statistical guarantees do not account for these dependencies and little is known about how to provably avoid overfitting and false discovery in the adaptive setting. We consider a natural formalization of this problem in which the goal is to design an algorithm that, given a limited number of i.i.d.\u00a0samples from an unknown distribution, can answer adaptively-chosen queries about that distribution. We present an algorithm that estimates the expectations of $k$ arbitrary adaptively-chosen real-valued estimators using a number of samples that scales as $\\sqrt{k}$. The answers given by our algorithm are essentially as accurate as if fresh samples were used to evaluate each estimator. In contrast, prior work yields error guarantees that scale with the worst-case sensitivity of each estimator. We also give a version of our algorithm that can be used to verify answers to such queries where the sample complexity depends logarithmically on the number of queries $k$ (as in the reusable holdout technique). Our algorithm is based on a simple approximate median algorithm that satisfies the strong stability guarantees of differential privacy. Our techniques provide a new approach for analyzing the generalization guarantees of differentially private algorithms.", "pdf_url": "http://proceedings.mlr.press/v65/feldman17a/feldman17a.pdf", "keywords": [], "reference": "Raef Bassily and Yoav Freund.  Typicality-based stability and privacy.  CoRR,  abs/1604.03336, 2016. URL http://arxiv.org/abs/1604.03336.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016.  Mark Bun and Thomas Steinke. Concentrated di\ufb00erential privacy: Simplifications, exten- sions, and lower bounds. In Theory of Cryptography Conference, pages 635-658. Springer Berlin Heidelberg, 2016.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Di\ufb00erentially private release and learning of threshold functions. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 634-649. IEEE, 2015.  Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in di\ufb00erential privacy. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1306-1325. Society for Industrial and Applied Mathematics, 2017.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private  data analysis. In TCC, pages 265-284, 2006a.  C. Dwork, M. Naor, O. Reingold, G. Rothblum, and S. Vadhan. On the complexity of di\ufb00erentially private data release: e\ufb03cient algorithms and hardness results. In STOC, pages 381-390, 2009.  Cynthia Dwork and Jing Lei. Di\ufb00erential privacy and robust statistics. In STOC, pages  371-380, 2009.  Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Di\ufb00erential Privacy,  volume 9. 2014. URL http://dx.doi.org/10.1561/0400000042.  25   Generalization for Adaptively-chosen Estimators via Stable Median  For example we can use this algorithm to obtain a new algorithm for answering a large number of low-sensitivity queries (that is queries \u03c6 : X t \u2192 [\u22121, 1] such that \u2206(\u03c6) = 1/t). To answer queries with accuracy \u03b1 we can use t = 16/\u03b12 and set T that is the interval [\u22121, 1] discretized with step \u03b1/2. Thus the number of samples that our algorithm uses is n = O . For comparison, the best previously known algorithm for this problem uses  (cid:16)(cid:112)log |X | \u00b7 log(k/\u03b1) \u00b7 log3/2(1/\u03b2)/\u03b13(cid:17)  n = O  (cid:16)  log |X | \u00b7 log(k/\u03b1) \u00b7 log3/2(1/\u03b2)/\u03b14(cid:17)  (Bassily et al., 2016) (a di\ufb00erent bound is stated there in error). Although, as pointed out in the introduction, the setting in which each query is applied to the entire dataset is more general than ours.  References  Raef Bassily and Yoav Freund.  Typicality-based stability and privacy.  CoRR,  abs/1604.03336, 2016. URL http://arxiv.org/abs/1604.03336.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC, pages 1046-1059, 2016.  Mark Bun and Thomas Steinke. Concentrated di\ufb00erential privacy: Simplifications, exten- sions, and lower bounds. In Theory of Cryptography Conference, pages 635-658. Springer Berlin Heidelberg, 2016.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Di\ufb00erentially private release and learning of threshold functions. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 634-649. IEEE, 2015.  Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your mind: The price of online queries in di\ufb00erential privacy. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1306-1325. Society for Industrial and Applied Mathematics, 2017.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private  data analysis. In TCC, pages 265-284, 2006a.  C. Dwork, M. Naor, O. Reingold, G. Rothblum, and S. Vadhan. On the complexity of di\ufb00erentially private data release: e\ufb03cient algorithms and hardness results. In STOC, pages 381-390, 2009.  Cynthia Dwork and Jing Lei. Di\ufb00erential privacy and robust statistics. In STOC, pages  371-380, 2009.  Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Di\ufb00erential Privacy,  volume 9. 2014. URL http://dx.doi.org/10.1561/0400000042. Feldman Steinke  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 486-503. Springer Berlin Heidelberg, 2006b.  Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and di\ufb00erential privacy.  In FOCS, pages 51-60, 2010.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toni Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. In Advances in Neural Information Processing Systems, pages 2350-2358, 2015a.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349 (6248):636-638, 2015b. doi: 10.1126/science.aaa9375. URL http://www.sciencemag. org/content/349/6248/636.abstract.  Vitaly Feldman. Dealing with range anxiety in mean estimation via statistical queries.  arXiv, abs/1611.06475, 2016. URL http://arxiv.org/abs/1611.06475.  M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving  data analysis. In FOCS, pages 61-70, 2010.  M. Hardt and J. Ullman. Preventing false discovery in interactive data analysis is hard. In  FOCS, pages 454-463, 2014.  M. Kearns. E\ufb03cient noise-tolerant learning from statistical queries. Journal of the ACM,  45(6):983-1006, 1998.  pages 94-103, 2007.  Frank McSherry and Kunal Talwar. Mechanism design via di\ufb00erential privacy. In FOCS,  Kobbi Nissim and Or She\ufb00et. Topics in cryptography and privacy - di\ufb00erential privacy. hw 1. http://isites.harvard.edu/fs/docs/icb.topic1475289.files/hwk1-kobbi. pdf, 2014.  Kobbi Nissim and Uri Stemmer. Concentration bounds for high sensitivity functions through di\ufb00erential privacy. CoRR, abs/1703.01970, 2017. URL http://arxiv.org/abs/1703. 01970.  Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling  in private data analysis. In STOC, pages 75-84, 2007.  Sofya Raskhodnikova and Adam D. Smith. Algorithmic challenges in data privacy. hw 1.  http://www.cse.psu.edu/ads22/privacy598/handouts/hw1.pdf, 2010. Generalization for Adaptively-chosen Estimators via Stable Median  Ryan Rogers, Aaron Roth, Adam Smith, and Om Thakkar. Max-information, di\ufb00eren- tial privacy, and post-selection hypothesis testing. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 487-494. IEEE, 2016.  Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS, 2016.  Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 813-822. ACM, 2011.  Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of preventing false discovery. In COLT, pages 1588-1628, 2015. URL http://jmlr.org/ proceedings/papers/v40/Steinke15.html.  Thomas Steinke and Jonathan Ullman. Between pure and approximate di\ufb00erential privacy.  Journal of Privacy and Confidentiality, 2016.  Thomas Steinke and Jonathan Ullman. Subgaussian tail bounds via stability arguments.  CoRR, abs/1701.03493, 2017. URL http://arxiv.org/abs/1701.03493.  "}, "Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization", "abstract": "It is known that greedy methods perform well for maximizing \\textitmonotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity.  In this paper, we show\u2014arguably, surprisingly\u2014that invoking the classical greedy algorithm $O(\\sqrt{k})$-times leads to the (currently) fastest deterministic algorithm, called RepeatedGreedy, for maximizing a general submodular function subject to $k$-independent system constraints. RepeatedGreedy achieves $(1 + O(1/\\sqrt{k}))k$ approximation using $O(nr\\sqrt{k})$ function evaluations (here, $n$ and $r$ denote the size of the ground set and the maximum size of a feasible solution, respectively). We then show that by a careful sampling procedure, we can run the greedy algorithm only \\textitonce and obtain the (currently) fastest randomized algorithm, called SampleGreedy, for maximizing a submodular function subject to $k$-extendible system constraints (a subclass of $k$-independent system constrains). SampleGreedy achieves $(k + 3)$-approximation with only $O(nr/k)$ function evaluations. Finally, we derive an almost matching lower bound, and show that no polynomial time algorithm can have an approximation ratio smaller than $ k + 1/2 - \\varepsilon$. To further support our theoretical results, we compare the performance of RepeatedGreedy and SampleGreedy with prior art in a concrete application (movie recommendation). We consistently observe that while SampleGreedy achieves practically the same utility as the best baseline, it performs at least two orders of magnitude faster.", "pdf_url": "http://proceedings.mlr.press/v65/feldman17b/feldman17b.pdf", "keywords": ["Submodular maximization", "k-systems", "k-extendible systems", "approximation algorithms"], "reference": "tions. In SODA, 2014.  Ashwinkumar Badanidiyuru and Jan Vondr\u00b4ak. Fast algorithms for maximizing submodular func-  Niv Buchbinder and Moran Feldman. Constrained submodular maximization via a non-symmetric  technique. CoRR, abs/1611.03253, 2016a.  Niv Buchbinder and Moran Feldman. Deterministic algorithms for submodular maximization prob-  lems. In SODA, 2016b.  Niv Buchbinder, Moran Feldman, Joesph (Seffi) Naor, and Roy Schwartz. Submodular maximiza-  tion with cardinality constrains. In SODA, 2014.  Niv Buchbinder, Moran Feldman, and Roy Schwartz. Comparing apples and oranges: Query trade-  off in submodular maximization. In SODA, 2015.  N. Buchdiner, M. Feldman, N.S. Joseph, and R. Schwartz. A tight linear time (1/2)-approximatoin  for unconstrained submodular maximization. SIAM Journal on Computing, 2015.  Gruia C\u02d8alinescu, Chandra Chekuri, Martin P\u00b4al, and Jan Vondr\u00b4ak. Maximizing a monotone submod-  ular function subject to a matroid constraint. SIAM Journal on Computing, 2011.  E. Cand\u00b4es and B. Recht. Exact matrix completion via convex optimization.  In Foundations of  Computational Mathematics, 2008.  14   FELDMAN HARSHAW KARBASI  solution so we can observe this phenomenon. See Figure 2 for the movies recommended by the different algorithms. Because mg = 1, we are constrained here to have at most one movie from Adventure, Animation and Fantasy. As seen in Figure 2, FANTOM and SAMPLEGREEDY return maximum size solution sets that are both diverse and representative of these genres. On the other hand, Greedy gets stuck choosing a single movie that belongs to all three genres, thus, precluding any other choice of movie from the solution set.  Finally, we would like to make a short remark on the relation between our theoretical analy- sis and experimental results. In our analysis, we showed that SAMPLEGREEDY achieves a better expected approximation ratio than the worst-case approximation ratio of REPEATEDGREEDY and FANTOM. However, there are many problem instances which are \u201cnot hard\u201d, and do not make the algorithms exhibit their worst-case behavior. For such instances, REPEATEDGREEDY and FAN- TOM may out-perform SAMPLEGREEDY. Our experiments demonstrate that even in usual \u201cnot hard\u201d problem instances, SAMPLEGREEDY performs competitively at a fraction of the computa- tional cost, indicating its potential for real-world large scale applications.  Acknowledgments  This material is based upon work supported by the National Science Foundation Graduate Re- search Fellowship under Grant No. (DGE1122492), DARPA Young Faculty Award (D16AP00046), Simons-Berkeley Fellowship, and Israel Science Foundation (ISF grant number 1357/16). The au- thors would like to thank Eric Lindgren for kindly sharing processed movieLens data.  References  tions. In SODA, 2014.  Ashwinkumar Badanidiyuru and Jan Vondr\u00b4ak. Fast algorithms for maximizing submodular func-  Niv Buchbinder and Moran Feldman. Constrained submodular maximization via a non-symmetric  technique. CoRR, abs/1611.03253, 2016a.  Niv Buchbinder and Moran Feldman. Deterministic algorithms for submodular maximization prob-  lems. In SODA, 2016b.  Niv Buchbinder, Moran Feldman, Joesph (Seffi) Naor, and Roy Schwartz. Submodular maximiza-  tion with cardinality constrains. In SODA, 2014.  Niv Buchbinder, Moran Feldman, and Roy Schwartz. Comparing apples and oranges: Query trade-  off in submodular maximization. In SODA, 2015.  N. Buchdiner, M. Feldman, N.S. Joseph, and R. Schwartz. A tight linear time (1/2)-approximatoin  for unconstrained submodular maximization. SIAM Journal on Computing, 2015.  Gruia C\u02d8alinescu, Chandra Chekuri, Martin P\u00b4al, and Jan Vondr\u00b4ak. Maximizing a monotone submod-  ular function subject to a matroid constraint. SIAM Journal on Computing, 2011.  E. Cand\u00b4es and B. Recht. Exact matrix completion via convex optimization.  In Foundations of  Computational Mathematics, 2008. V. Chv\u00b4atal. The tail of the hypergeometric distribution. Discrete Mathematics, 25(3):285-287,  Jack Edmonds. Matroids and the greedy algorithm. Mathematical programming, 1971.  Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the  blogosphere. In KDD, 2009.  Alina Ene and Huy L. Nguyen. Constrained submodular maximization: Beyond 1/e. In FOCS,  1979.  2016.  Uriel Feige, Vahab S. Mirrokni, and Jan Vondr\u00b4ak. Maximizing non-monotone submodular func-  tions. SIAM Journal on Computing, 2011.  Moran Feldman, Joseph Naor, and Roy Schwartz. A unified continuous greedy algorithm for sub-  modular maximization. In FOCS, 2011a.  Moran Feldman, Joseph Naor, Roy Schwartz, and Justin Ward. Improved approximations for k-  exchange systems - (extended abstract). In ESA, 2011b.  M. Fisher, G. Nemhauser, and L. Wolsey. An analysis of approximations for maximizing submod-  ular set functions\u2014ii. Mathematical Programming, 8:73-87, 1978.  Satoru Fujishige. Submodular functions and optimization. Elsevier Science, 2nd edition, 2005.  Shayan Oveis Gharan and Jan Vondr\u00b4ak. Submodular maximization by simulated annealing.  In  SODA, 2011.  point processes. In NIPS, 2012.  and in\ufb02uence. In KDD, 2010.  processes. In ICML, 2005.  Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal MAP inference for determinantal  Manuel Gomez Rodriguez, Jure Leskovec, and Andreas Krause. Inferring networks of diffusion  Carlos Guestrin, Andreas Krause, and Ajit Paul Singh. Near-optimal sensor placements in gaussian  Anupam Gupta, Aaron Roth, Grant Schoenebeck, and Kunal Talwar. Constrained non-monotone  submodular maximization: Of\ufb02ine and secretary algorithms. In WINE, 2010.  Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Rexa Zadeh. Matrix completion and low-rank  svd via fast alternating least squares. In Journal of Machine Learning Research, 2015.  Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 1963.  T. A. Jenkyns. The efficacy of the \u201cgreedy\u201d algorithm. In South Eastern Conference on Combina-  torics, Graph Theory and Computing, 1976.  David Kempe, Jon Kleinberg, and \u00b4Eva Tardos. Maximizing the spread of in\ufb02uence through a social  network. In KDD, 2003. FELDMAN HARSHAW KARBASI  Katrin Kirchhoff and Jeff Bilmes. Submodularity for data selection in statistical machine translation.  In EMNLP, 2014.  Jon Lee, Maxim Sviridenko, and Jan Vondr\u00b4ak. Submodular maximization over multiple matroids  via generalized exchange properties. Mathematics of Operations Research, 2010.  Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne Van Briesen, and Na-  talie Glance. Cost-effective outbreak detection in networks. In KDD, 2007.  Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In ACL,  2011.  Erik M Lindgren, Shanshan Wu, and Alexandros G Dimakis. Sparse and greedy: Sparsifying submodular facility location problems. In NIPS Workshop on Optimization for Machine Learning, 2015.  Juli\u00b4an Mestre. Greedy in approximation algorithms. In ESA, 2006.  Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular  maximization: Identifying representative elements in massive data. In NIPS, 2013.  Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, and Andreas  Krause. Lazier than lazy greedy. In AAAI, 2015a.  Baharan Mirzasoleiman, Amin Karbasi, Ashwinkumar Badanidiyuru, and Andreas Krause. Dis-  tributed submodular cover: Succinctly summarizing massive data. In NIPS, 2015b.  Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, and Amin Karbasi. Fast constrained sub-  modular maximization: Personalized data summarization. In ICML, 2016a.  Baharan Mirzasoleiman, Morteza Zadimoghaddam, and Amin Karbasi. Fast distributed submod- ular cover: Public-private data summarization. In Advances in Neural Information Processing Systems, pages 3594-3602, 2016b.  G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a submod-  ular set function. Mathematics of Operations Research, 1978.  G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing  submodular set functions\u2014i. Mathematical Programming, 1978.  Colorado Reed and Zoubin Ghahramani. Scaling the indian buffet process via submodular maxi-  mization. In ICML, 2013.  Adish Singla, Ilija Bogunovic, G\u00b4abor Bart\u00b4ok, Amin Karbasi, and Andreas Krause. Near-optimally  teaching the crowd to classify. In ICML, 2014.  Ruben Sipos, Adith Swaminathan, Pannaga Shivaswamy, and Thorsten Joachims. Temporal corpus  summarization using submodular word coverage. In CIKM, 2012.  Matthew Skala. Hypergeometric tail inequalities: ending the insanity. CoRR, abs/1311.5939, 2013. Jan Vondr\u00b4ak. Symmetry and approximability of submodular maximization problems. SIAM Journal  on Computing, 2013.  Justin Ward. A (k+3)/2-approximation algorithm for monotone submodular k-set packing and gen-  eral k-exchange systems. In STACS, 2012.  "}, "A General Characterization of the Statistical Query Complexity": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "A General Characterization of the Statistical Query Complexity", "abstract": "Statistical query (SQ) algorithms  are algorithms that have access to an \\em SQ oracle for the input distribution $D$ instead of i.i.d.\u00a0samples from $D$. Given a query function $\u03c6:X \\to [-1,1]$, the oracle returns an estimate of $\\bf E_x\u223cD[\u03c6(x)]$ within some tolerance $\\tau_\u03c6$ that roughly corresponds to the number of samples. In this work we demonstrate that the complexity of solving an arbitrary statistical problem using SQ algorithms can be captured by a relatively simple notion of statistical dimension that we introduce. SQ algorithms capture a broad spectrum of algorithmic approaches used in theory and practice, most notably, convex optimization techniques. Hence our statistical dimension allows to investigate the power of a variety of algorithmic approaches by analyzing a single linear-algebraic parameter.  Such characterizations were investigated over the past 20 years in learning theory but prior characterizations are restricted to the much simpler setting of classification problems relative to a fixed distribution on the domain.  Our characterization is also the first to precisely characterize the necessary tolerance of queries.  We give applications of our techniques to two open problems in learning theory and to algorithms that are subject to memory and communication constraints.", "pdf_url": "http://proceedings.mlr.press/v65/feldman17c/feldman17c.pdf", "keywords": [], "reference": "Apple\u2019s \u201cdifferential privacy\u201d is about collecting your data - but not your data. https://www. wired.com/2016/06/apples-differential-privacy-collecting-data. Accessed: 2016-07-30.  Noga Alon, Shay Moran, and Amir Yehudayoff. Sign rank versus VC dimension. In COLT, pages 47-80, 2016. URL http://jmlr.org/proceedings/papers/v49/alon16.html.  D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343-370, 1988.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity  and privacy. In COLT, pages 26.1-26.22, 2012.  Maria-Florina Balcan and Vitaly Feldman. Statistical active learning algorithms for noise tolerance  and differential privacy. Algorithmica, 72(1):282-315, 2015.  J. Balc\u00b4azar, J. Castro, D. Guijarro, J. K\u00a8obler, and W. Lindner. A general dimension for query learning.  Journal of Computer and System Sciences, 73(6):924-940, 2007.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. CoRR, abs/1511.02513, 2015. URL http: //arxiv.org/abs/1511.02513.  Shai Ben-David and Eli Dichterman. Learning with restricted focus of attention. J. Comput. Syst.  Sci., 56(3):277-298, 1998.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253-262, 1994.  A. Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial time algorithm for learning noisy  linear threshold functions. Algorithmica, 22(1/2):35-52, 1997.  36   While we have described several techniques for simplifying the analysis of our statistical di- mensions, a lot more work remains in adapting and simplifying the dimensions to specific types of problems (e.g. convex optimization or Boolean constraint satisfaction). In particular, it is interesting to understand for which problems one can avoid the RKL(D)/\u03c4 2 overhead of our characterization. We also have relatively few analysis techniques for the norms of operators that emerge in the pro- cess. Finally, the SQ complexity of many concrete problems is still unknown (e.g. Sherstov, 2008; Feldman, 2014).  I thank Sasha Sherstov and Santosh Vempala for many insightful discussions related to this work. I am especially grateful to Justin Thaler for the discussions that stimulated the work on the results in Section 7.2.  Acknowledgments  References  Apple\u2019s \u201cdifferential privacy\u201d is about collecting your data - but not your data. https://www. wired.com/2016/06/apples-differential-privacy-collecting-data. Accessed: 2016-07-30.  Noga Alon, Shay Moran, and Amir Yehudayoff. Sign rank versus VC dimension. In COLT, pages 47-80, 2016. URL http://jmlr.org/proceedings/papers/v49/alon16.html.  D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343-370, 1988.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity  and privacy. In COLT, pages 26.1-26.22, 2012.  Maria-Florina Balcan and Vitaly Feldman. Statistical active learning algorithms for noise tolerance  and differential privacy. Algorithmica, 72(1):282-315, 2015.  J. Balc\u00b4azar, J. Castro, D. Guijarro, J. K\u00a8obler, and W. Lindner. A general dimension for query learning.  Journal of Computer and System Sciences, 73(6):924-940, 2007.  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. CoRR, abs/1511.02513, 2015. URL http: //arxiv.org/abs/1511.02513.  Shai Ben-David and Eli Dichterman. Learning with restricted focus of attention. J. Comput. Syst.  Sci., 56(3):277-298, 1998.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253-262, 1994.  A. Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial time algorithm for learning noisy  linear threshold functions. Algorithmica, 22(1/2):35-52, 1997. A GENERAL CHARACTERIZATION OF THE STATISTICAL QUERY COMPLEXITY  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statistical  query model. Journal of the ACM, 50(4):506-519, 2003.  A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SuLQ framework. In PODS,  pages 128-138, 2005.  Guy Bresler, David Gamarnik, and Devavrat Shah. Structure learning of antiferromagnetic ising  models. In NIPS, pages 2852-2860, 2014.  N. Bshouty and V. Feldman. On using extended statistical queries to avoid membership queries.  Journal of Machine Learning Research, 2:359-395, 2002.  C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun. Map-reduce for machine learning  on multicore. In NIPS, pages 281-288, 2006.  C. J. Clopper and E. S. Pearson. The Use of Confidence or Fiducial Limits Illustrated in the Case of  the Binomial. Biometrika, 26(4):404-413, 1934.  Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, and Karl Wimmer. Approximate  resilience, monotonicity, and the complexity of agnostic learning. In SODA, 2015.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. CoRR, abs/1611.03473, 2016. URL http://arxiv.org/abs/1611.03473.  I. Dinur and K. Nissim. Revealing information while preserving privacy. In PODS, pages 202-210,  2003.  J. Dunagan and S. Vempala. A simple polynomial-time rescaling algorithm for solving linear  programs. In STOC, pages 315-320, 2004.  John Dunagan and Santosh Vempala. A simple polynomial-time rescaling algorithm for solving  linear programs. Math. Program., 114(1):101-114, 2008.  C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data  analysis. In TCC, pages 265-284, 2006.  Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy, volume 9.  2014. URL http://dx.doi.org/10.1561/0400000042.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. CoRR, abs/1506, 2015. Extended abstract in NIPS 2015.  \u00b4Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggregatable privacy- preserving ordinal response. In ACM SIGSAC Conference on Computer and Communications Security, pages 1054-1067, 2014. V. Feldman. A complete characterization of statistical query learning with applications to evolvability.  Journal of Computer System Sciences, 78(5):1444-1459, 2012.  V. Feldman, H. Lee, and R. Servedio. Lower bounds and hardness amplification for learning shallow  monotone formulas. In COLT, volume 19, pages 273-292, 2011.  Vitaly Feldman. Open problem: The statistical query complexity of learning sparse halfspaces. In  COLT, pages 1283-1289, 2014.  Vitaly Feldman. Dealing with range anxiety in mean estimation via statistical queries. arXiv,  abs/1611.06475, 2016. URL http://arxiv.org/abs/1611.06475.  Vitaly Feldman.  Statistical query learning.  In Encyclopedia of Algorithms, pages 2090- 2095. 2017. Available at researcher.ibm.com/researcher/files/us-vitaly/ Kearns93-2017.pdf.  Vitaly Feldman and Badih Ghazi. On the power of learning from k-wise queries. In Innovations in  Theoretical Computer Science (ITCS), 2017.  Vitaly Feldman and Varun Kanade. Computational bounds on statistical query learning. In COLT,  pages 16.1-16.22, 2012.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algorithms and a lower bound for detecting planted cliques. arXiv, CoRR, abs/1201.1214, 2012. Extended abstract in STOC 2013.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability problems with planted solutions. CoRR, abs/1311.4821, 2013. Extended abstract in STOC 2015.  Vitaly Feldman, Cristobal Guzman, and Santosh Vempala. Statistical query algorithms for mean vector estimation and stochastic convex optimization. CoRR, abs/1512.09170, 2015. URL http://arxiv.org/abs/1512.09170. Extended abstract in SODA 2017.  J. Forster. A linear lower bound on the unbounded error probabilistic communication complexity.  Journal of Computer and System Sciences, 65(4):612-625, 2002.  A. Gupta, M. Hardt, A. Roth, and J. Ullman. Privately releasing conjunctions and the statistical  query barrier. In STOC, pages 803-812, 2011.  M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis.  In FOCS, pages 61-70, 2010.  pages 437-456, 2011.  M. Kallweit and H. Simon. A close look to margin complexity and related parameters. In COLT,  Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM J. Comput., 40(3):793-826, June 2011.  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  983-1006, 1998. A GENERAL CHARACTERIZATION OF THE STATISTICAL QUERY COMPLEXITY  M. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine Learning, 17  (2-3):115-141, 1994.  A. Klivans and A. Sherstov. Unconditional lower bounds for learning intersections of halfspaces.  Machine Learning, 69(2-3):97-114, 2007.  Adam R. Klivans and Alexander A. Sherstov. Lower bounds for agnostic learning via approximate  rank. Computational Complexity, 19(4):581-604, 2010.  N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm.  Machine Learning, 2:285-318, 1987.  Indrajit Roy, Srinath T. V. Setty, Ann Kilzer, Vitaly Shmatikov, and Emmett Witchel. Airavat:  Security and privacy for MapReduce. In NSDI, pages 297-312, 2010.  Shai Shalev-Shwartz. Online learning and online convex optimization. 4(2):107-194, 2012.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Alexander A. Sherstov. Halfspace matrices. Computational Complexity, 17(2):149-178, 2008.  H. Simon. Spectral norm in learning theory: Some selected topics. In Algorithmic Learning Theory,  pages 13-27, 2006.  H. Simon. A characterization of strong learnability in the statistical query model. In Symposium on  Theoretical Aspects of Computer Science, pages 393-404, 2007.  J. Steinhardt, G. Valiant, and S. Wager. Memory, communication, and statistical queries. In COLT,  pages 1490-1516, 2016.  Jacob Steinhardt and John C. Duchi. Minimax rates for memory-bounded sparse linear regression. In COLT, pages 1564-1587, 2015. URL http://jmlr.org/proceedings/papers/v40/ Steinhardt15.html.  Arvind K. Sujeeth, Hyoukjoong Lee, Kevin J. Brown, Hassan Chafi, Michael Wu, Anand R. Atreya, Kunle Olukotun, Tiark Rompf, and Martin Odersky. OptiML: an implicitly parallel domainspecific language for machine learning. In ICML, 2011.  B. Szorenyi. Characterizing statistical query learning:simplified notions and proofs. In ALT, pages  186-200, 2009.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.  Ke Yang. On learning correlated boolean functions using statistical queries. In ALT, pages 59-76,  Ke Yang. New lower bounds for statistical query learning. Journal of Computer and System Sciences,  2001.  70(4):485-509, 2005. Yuchen Zhang, John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In NIPS, pages 2328-2336, 2013.  "}, "Stochastic Composite Least-Squares Regression with Convergence Rate O(1n)": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Stochastic Composite Least-Squares Regression with Convergence Rate O(1n)", "abstract": "We consider the minimization of composite objective functions composed of the expectation of quadratic functions and an arbitrary convex function. We study the stochastic dual averaging algorithm with a constant step-size, showing that it leads to a convergence rate of O(1/n) without strong convexity assumptions. This thus extends earlier results on least-squares regression with the Euclidean geometry to (a) all convex regularizers and constraints, and (b) all geometries represented by a Bregman divergence. This is achieved by a new proof technique that relates stochastic and deterministic recursions", "pdf_url": "http://proceedings.mlr.press/v65/flammarion17a/flammarion17a.pdf", "keywords": ["Convex optimization", "stochastic approximation", "dual averaging", "mirror descent"], "reference": "J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the Dark: An Efficient Algorithm for Ban- In Proceedings of the International Conference on Learning Theory  dit Linear Optimization. (COLT), pages 263-274, 2008.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Duality between subgradient and conditional gradient methods. SIAM J. Optim., 25(1):  115-129, 2015.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  H. H. Bauschke and J. M. Borwein. Legendre functions and the method of random Bregman pro-  jections. J. Convex Anal., 4(1):27-67, 1997.  H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011.  H. H. Bauschke, J. Bolte, and M. Teboulle. A descent Lemma beyond Lipschitz gradient continuity:  first-order methods revisited and applications. Mathematics of Operations Research, 2016.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, 2003.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  J. Bolte and M. Teboulle. Smooth optimization with approximate gradient. SIAM J. Optim., 43(3):  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge,  L. M. Bregman. A relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming. \u02c7Z. Vy\u02c7cisl. Mat. i Mat. Fiz., 7:620-631, 1967.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  1266-1292, 2003.  2004.  Cambridge, 2006.  G. Chen and M. Teboule. Convergence analysis of a proximal-like minimization algorithm using  Bregman functions. SIAM J. Optim., 3(3):538-543, 1993.  I. Colin, A. Bellet, J. Salmon, and S. Cl\u00b4emenc\u00b8on. Gossip Dual Averaging for Decentralized Opti- mization of Pairwise Functions. In Proceedings of the Conference on Machine Learning (ICML), 2016.  14   FLAMMARION BACH  References  J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the Dark: An Efficient Algorithm for Ban- In Proceedings of the International Conference on Learning Theory  dit Linear Optimization. (COLT), pages 263-274, 2008.  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Duality between subgradient and conditional gradient methods. SIAM J. Optim., 25(1):  115-129, 2015.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  H. H. Bauschke and J. M. Borwein. Legendre functions and the method of random Bregman pro-  jections. J. Convex Anal., 4(1):27-67, 1997.  H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011.  H. H. Bauschke, J. Bolte, and M. Teboulle. A descent Lemma beyond Lipschitz gradient continuity:  first-order methods revisited and applications. Mathematics of Operations Research, 2016.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, 2003.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM J. Imaging Sci., 2(1):183-202, 2009.  J. Bolte and M. Teboulle. Smooth optimization with approximate gradient. SIAM J. Optim., 43(3):  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge,  L. M. Bregman. A relaxation method of finding a common point of convex sets and its application to the solution of problems in convex programming. \u02c7Z. Vy\u02c7cisl. Mat. i Mat. Fiz., 7:620-631, 1967.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  1266-1292, 2003.  2004.  Cambridge, 2006.  G. Chen and M. Teboule. Convergence analysis of a proximal-like minimization algorithm using  Bregman functions. SIAM J. Optim., 3(3):538-543, 1993.  I. Colin, A. Bellet, J. Salmon, and S. Cl\u00b4emenc\u00b8on. Gossip Dual Averaging for Decentralized Opti- mization of Pairwise Functions. In Proceedings of the Conference on Machine Learning (ICML), 2016. STOCHASTIC COMPOSITE LEAST-SQUARES REGRESSION  P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, volume 49 of Springer Optim. Appl., pages 185-212. Springer, New York, 2011.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. In Advances in Neural Information Processing Systems (NIPS), pages 1646-1654, 2014.  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using  mini-batches. J. Mach. Learn. Res., 13:165-202, 2012.  O. Devolder, F. Glineur, and Y. Nesterov. First-order methods with inexact oracle: the strongly  convex case. CORE Discussion Papers, 2013016, 2013.  L. Devroye, L. Gy\u00a8orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, 1996.  A. Dieuleveut, N. Flammarion, and F. Bach. Harder, Better, Faster, Stronger Convergence Rates for  Least-Squares Regression. arXiv preprint arXiv:1602.05419v2, 2016.  J. Duchi and F. Ruan. Local Asymptotics for some Stochastic Optimization Problems: Optimality,  Constraint Identification, and Dual Averaging. arXiv preprint arXiv:1612.05612, 2016.  J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite Objective Mirror Descent. In Proceedings of the International Conference on Learning Theory (COLT), pages 14-26, 2010.  J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: convergence  analysis and network scaling. IEEE Trans. Automat. Control, 57(3):592-606, 2012.  N. Flammarion and F. Bach. From averaging to acceleration, there is only a step-size. In Proceed-  ings of the International Conference on Learning Theory (COLT), 2015.  C. Gentile and N. Littlestone. The robustness of the p-norm algorithms. International Conference on Learning Theory (COLT), pages 1-11, 1999.  In Proceedings of the  O. Hanner. On the uniform convexity of Lp and lp. Ark. Mat., 3:239-244, 1956.  J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Fundamentals of Convex Analysis. Grundlehren Text  Editions. Springer-Verlag, Berlin, 2001.  P. Jain, S. M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Parallelizing Stochastic Approxi- mation Through Mini-Batching and Tail-Averaging. arXiv preprint arXiv:1610.03774, 2016.  A. Juditsky and A. S. Nemirovski. Functional aggregation for nonparametric regression. Ann.  Statist., 28(3):681-712, 2000.  71(3):291-307, 2005.  A. Kalai and S. Vempala. Efficient algorithms for online decision problems. J. Comput. System Sci.,  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Inform. and Comput., 132(1):1-63, 1997. FLAMMARION BACH  K. C. Kiwiel. Proximal minimization methods with generalized Bregman functions. SIAM J. Con-  trol Optim., 35(4):1142-1168, 1997.  W. Krichene, A. Bayen, and P. L. Bartlett. Accelerated mirror descent in continuous and discrete time. In Advances in Neural Information Processing Systems (NIPS), pages 2845-2853, 2015.  H. Kushner and G G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.  Springer, 2003.  G. Lecu\u00b4e. Optimal oracle inequality for aggregation of classifiers under low noise condition. In Learning theory, volume 4005 of Lecture Notes in Comput. Sci., pages 364-378. Springer, Berlin, 2006.  G. Lecu\u00b4e. Optimal rates of aggregation in classification under low noise assumption. Bernoulli, 13  (4):1000-1022, 2007.  S. Lee and S. J. Wright. Manifold identification in dual averaging for regularized stochastic online  learning. J. Mach. Learn. Res., 13:1705-1744, 2012.  H. Lu, R. Freund, and Y. Nesterov. Relatively-Smooth Convex Optimization by First-Order Meth-  ods, and Applications. arXiv preprint arXiv:1610.05708, 2016.  O. Macchi. Adaptive Processing: the Least-Mean-Squares Approach with Applications in Trans-  mission. Wiley West Sussex, 1995.  B. Martinet. Breve communication. R\u00b4egularisation d\u2019in\u00b4equations variationnelles par approxima- tions successives. ESAIM: Mathematical Modelling and Numerical Analysis - Mod\u00b4elisation Math\u00b4ematique et Analyse Num\u00b4erique, 4:154-158, 1970.  H. B. McMahan. Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and  L1 Regularization. In AISTATS, pages 525-533, 2011.  J.-J. Moreau. Fonctions convexes duales et points proximaux dans un espace Hilbertien. C. R. Acad.  Sci. Paris, 255:2897-2899, 1962.  A. S. Nemirovski and D. B. Yudin. Effective methods for the solution of convex programming  problems of large dimensions. `Ekonom. i Mat. Metody, 15(1):135-152, 1979.  A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience Publication. John Wiley & Sons, Inc., New York, 1983. Translated from the Russian and with a preface by E. R. Dawson, Wiley-Interscience Series in Discrete Mathematics.  Y. Nesterov. Introductory Lectures on Convex Optimization, volume 87 of Applied Optimization.  Kluwer Academic Publishers, Boston, MA, 2004. A basic course.  Y. Nesterov. Primal-dual subgradient methods for convex problems. Math. Program., 120(1, Ser.  Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser.  B):221-259, 2009.  B):125-161, 2013. STOCHASTIC COMPOSITE LEAST-SQUARES REGRESSION  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J.  Control Optim., 30(4):838-855, 1992.  M. Raginsky and A. Rakhlin.  Information-based complexity, feedback and dynamics in convex  programming. IEEE Trans. Inform. Theory, 57(10):7036-7056, 2011.  I. Rish and G. Grabarnik. Sparse Modeling: Theory, Algorithms, and Applications. CRC press,  2014.  Press, Princeton, N.J., 1970.  R. T. Rockafellar. Convex Analysis. Princeton Mathematical Series, No. 28. Princeton University  S. Shalev-Shwartz and S. m. Kakade. Mind the duality gap: Logarithmic regret algorithms for online optimization. In Advances in Neural Information Processing Systems (NIPS), pages 1457-1464, 2009.  S. Shalev-Shwartz and Y. Singer. Online learning meets optimization in the dual.  In Learning  theory, volume 4005 of Lecture Notes in Comput. Sci., pages 423-437. Springer, Berlin, 2006.  T. Suzuki. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multi- plier Method. In Proceedings of the Conference on Machine Learning (ICML), pages 392-400, 2013.  A. B. Tsybakov. Optimal rates of aggregation. In Proceedings of the Annual Conference on Com-  putational Learning Theory, 2003.  J.-P. Vial. Strong and weak convexity of sets and functions. Math. Oper. Res., 8(2):231-259, 1983.  A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in  optimization. Proceedings of the National Academy of Sciences, 113(47), 2016.  A. I. Wilson, B. Recht, and M. I. Jordan. A Lyapunov Analysis of Momentum Methods in Opti-  mization. arXiv preprint arXiv:1611.02635v3, 2016.  S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo. Sparse reconstruction by separable approxi-  mation. IEEE Trans. Signal Process., 57(7):2479-2493, 2009.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J.  Mach. Learn. Res., 11:2543-2596, 2010.  M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Pro-  ceedings of the Conference on Machine Learning (ICML), 2003. FLAMMARION BACH  "}, "ZigZag: A New Approach to Adaptive Online Learning": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "ZigZag: A New Approach to Adaptive Online Learning", "abstract": "We develop a new family of algorithms for the online learning setting with regret against any data sequence bounded by the empirical Rademacher complexity of that sequence. To develop a general theory of when this type of adaptive regret bound is achievable we establish a connection to the theory of decoupling inequalities for martingales in Banach spaces. When the hypothesis class is a set of linear functions bounded in some norm, such a regret bound is achievable if and only if the norm satisfies certain decoupling inequalities for martingales. Donald Burkholder\u2019s celebrated geometric characterization of decoupling inequalities (1984) states that such an inequality holds if and only if there exists a special function called a Burkholder function satisfying certain restricted concavity properties. Our online learning algorithms are efficient in terms of queries to this function. We realize our general theory by giving new efficient and adaptive algorithms for classes including $\\ell_p$ norms, group norms, and reproducing kernel Hilbert spaces. The empirical Rademacher complexity regret bound implies \u2014 when used in the i.i.d. setting \u2014 a data-dependent complexity bound for excess risk after online-to-batch conversion. To showcase the power of the empirical Rademacher complexity regret bound, we derive improved rates for a supervised learning generalization of the online learning with low rank experts task and for the online matrix prediction task. In addition to obtaining tight data-dependent regret bounds, our algorithms enjoy improved efficiency over previous techniques based on Rademacher complexity, automatically work in the infinite horizon setting, and adapt to scale. To obtain such adaptive methods, we introduce novel machinery, and the resulting algorithms are not based on the standard tools of online convex optimization. We conclude with a number of open problems and new directions, both algorithmic and information-theoretic.", "pdf_url": "http://proceedings.mlr.press/v65/foster17a/foster17a.pdf", "keywords": [], "reference": "Rados\u0142aw Adamczak and Pawe\u0142 Wolff. Concentration inequalities for non-lipschitz functions with bounded derivatives of higher order. Probability Theory and Related Fields, 162(3-4):531-586, 2015.  Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2003. ISSN 1532-4435.  Peter L. Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Donald L. Burkholder. Boundary value problems and sharp inequalities for martingale transforms.  The Annals of Probability, 12(3):647-702, 1984.  Donald L. Burkholder. Martingales and fourier analysis in banach spaces. In Probability and analysis,  pages 61-108. Springer, 1986.  Alon Cohen and Shie Mannor. Online learning with many experts. CoRR, abs/1702.07870, 2017.  URL http://arxiv.org/abs/1702.07870.  Sonja Cox and Mark Veraar. Some remarks on tangent martingale difference sequences in l1-spaces.  Electron. Comm. Probab, 12(421-433):380, 2007.  Sonja Cox and Mark Veraar. Vector-valued decoupling and the burkholder-davis-gundy inequality.  Illinois Journal of Mathematics, 55(1):343-375, 2011.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Richard M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian  processes. Journal of Functional Analysis, 1(3):290-330, 1967.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances  in Neural Information Processing Systems, pages 3375-3383, 2015.  Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruction.  In 24th Annual Conference on Learning Theory (COLT), 2011.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization,  2(3-4):157-325, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136, 2012. URL http://arxiv.org/abs/1204.0136.  Elad Hazan, Tomer Koren, Roi Livni, and Yishay Mansour. Online learning with low rank experts.  In 29th Annual Conference on Learning Theory, pages 1096-1114, 2016.  19   ZIGZAG  1521529 and 1521544. Part of this work was performed while D.F. and K.S. were visiting the Simons Institute for the Theory of Computing and A.R. was visiting MIT.  References  Rados\u0142aw Adamczak and Pawe\u0142 Wolff. Concentration inequalities for non-lipschitz functions with bounded derivatives of higher order. Probability Theory and Related Fields, 162(3-4):531-586, 2015.  Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2003. ISSN 1532-4435.  Peter L. Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Donald L. Burkholder. Boundary value problems and sharp inequalities for martingale transforms.  The Annals of Probability, 12(3):647-702, 1984.  Donald L. Burkholder. Martingales and fourier analysis in banach spaces. In Probability and analysis,  pages 61-108. Springer, 1986.  Alon Cohen and Shie Mannor. Online learning with many experts. CoRR, abs/1702.07870, 2017.  URL http://arxiv.org/abs/1702.07870.  Sonja Cox and Mark Veraar. Some remarks on tangent martingale difference sequences in l1-spaces.  Electron. Comm. Probab, 12(421-433):380, 2007.  Sonja Cox and Mark Veraar. Vector-valued decoupling and the burkholder-davis-gundy inequality.  Illinois Journal of Mathematics, 55(1):343-375, 2011.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.  Richard M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian  processes. Journal of Functional Analysis, 1(3):290-330, 1967.  Dylan J. Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In Advances  in Neural Information Processing Systems, pages 3375-3383, 2015.  Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruction.  In 24th Annual Conference on Learning Theory (COLT), 2011.  Elad Hazan. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization,  2(3-4):157-325, 2016.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136, 2012. URL http://arxiv.org/abs/1204.0136.  Elad Hazan, Tomer Koren, Roi Livni, and Yishay Mansour. Online learning with low rank experts.  In 29th Annual Conference on Learning Theory, pages 1096-1114, 2016. FOSTER RAKHLIN SRIDHARAN  Pawe\u0142 Hitczenko. Domination inequality for martingale transforms of a rademacher sequence. Israel  Journal of Mathematics, 84(1-2):161-178, 1993.  Pawel Hitczenko. On a domination of sums of random variables by sums of conditionally independent  ones. The Annals of Probability, pages 453-468, 1994.  Tuomas Hyt\u00f6nen, Jan van Neerven, Mark Veraar, and Lutz Weis. Analysis in Banach Spaces,  volume 1. 2016.  Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems 21, pages 793-800. MIT Press, 2009.  Fedor Nazarov and Sergei Treil. The hunt for a bellman function: applications to estimates for  singular integral operators and to other classical problems of harmonic analysis. 1996.  Fedor Nazarov, Sergei Treil, and Alexander Volberg. Bellman function in stochastic control and harmonic analysis. In Systems, approximation, singular integral operators, and related topics, pages 393-423. Springer, 2001.  Adam Osekowski. Sharp martingale and semimartingale inequalities. Monografie Matematyczne, 72,  2012.  2016.  2-8, 2011. 2011.  Adam Osekowski. On the umd constant of the space (cid:96)N  1 . Colloquium Mathematicum, 142:135-147,  Gilles Pisier. Martingales in banach spaces (in connection with type and cotype). course ihp, feb.  David Pollard. Empirical Processes: Theory and Applications, volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. Institute of Mathematical Statistics, Hayward, CA, 1990.  Alexander Rakhlin and Karthik Sridharan. Statistical learning and sequential prediction, 2012.  Available at http://stat.wharton.upenn.edu/~rakhlin/book_draft.pdf.  Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. In Conference on  Learning Theory, 2014.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize: From value to algorithms. In Advances in Neural Information Processing Systems 25, pages 2150-2158, 2012.  Ohad Shamir and Shai Shalev-Shwartz. Matrix completion with the trace norm: learning, bounding,  and transducing. Journal of Machine Learning Research, 15(1):3401-3423, 2014.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012. ZIGZAG  Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S Song. Operator norm inequalities  between tensor unfoldings on the partition lattice. arXiv preprint arXiv:1603.05621, 2016.  "}, "Memoryless Sequences for Differentiable Losses": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Memoryless Sequences for Differentiable Losses", "abstract": "One way to define the \u201crandomness\u201d of a fixed individual sequence is to ask how hard it is to predict.  When prediction error is measured via squared loss, it has been established that memoryless sequences (which are, in a precise sense, hard to predict) have some of the stochastic attributes of truly random sequences.  In this paper, we ask how changing the loss function used changes the set of memoryless sequences, and in particular, the stochastic attributes they possess.  We answer this question for differentiable convex loss functions using tools from property elicitation, showing that the property elicited by the loss determines the stochastic attributes of the corresponding memoryless sequences.  We apply our results to price calibration in prediction markets.", "pdf_url": "http://proceedings.mlr.press/v65/frongillo17a/frongillo17a.pdf", "keywords": ["Algorithmic randomness", "property elicitation", "loss functions", "prediction markets"], "reference": "J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceed-  ings of the 25th Conference on Learning Theory, pages 1-27, 2012.  Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elici-  tation. In JMLR Workshop and Conference Proceedings, volume 40, pages 1-19, 2015.  G.W. Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78  (1):1-3, 1950. ISSN 1520-0493.  Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicolo Cesa-Bianchi, G\u00e1bor Lugosi, and others. On prediction of individual sequences. The Annals  of Statistics, 27(6):1865-1895, 1999.  Dean P Foster and Rakesh Vohra. Regret in the on-line decision problem. Games and Economic  Behavior, 29(1):7-35, 1999.  Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation.  In Proceedings of the 28th  Conference on Learning Theory, pages 1-18, 2015.  Rafael Frongillo and Bo Waggoner. An Axiomatic Study of Scoring Rule Markets. Preprint, 2017.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Associa-  tion, 106(494):746-762, 2011.  13   MEMORYLESS SEQUENCES FOR DIFFERENTIABLE LOSSES  such online settings, which could allow for a formal link to similar statements made in the literature on game-theoretic probability (Shafer and Vovk, 2005; Vovk, 2014).  6. Discussion and Future Work  We have generalized the notion of memoryless sequences of Nobel (2004) to higher dimensions and differentiable losses. We conclude that memoryless sequences are characterized by the stochas- tic behavior of their finite dimensional weak limits, and that the the distribution of these limits is governed by the property elicited by the loss function. In particular, the broad class of Bregman divergences share the same set of memoryless sequences with squared loss, and their weak limits form martingale difference sequences. Finally, we showed how these results can show that prices in prediction markets are calibrated (or traders can make infinite profits).  A promising future direction would be to extend these results to non-differentiable losses, if possible. This would allow for losses eliciting the median, as all losses eliciting the median, such as absolute loss (cid:96)(x, y) = |x\u2212y|, are nondifferentiable (Gneiting, 2011). As mentioned in Section 5, it would also be interesting to extend our results to a more online setting, where the outcome sequence can adapt to the predictions adversarially, a setting closer to game-theoretic probability.  References  J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceed-  ings of the 25th Conference on Learning Theory, pages 1-27, 2012.  Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via con- vex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.  Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elici-  tation. In JMLR Workshop and Conference Proceedings, volume 40, pages 1-19, 2015.  G.W. Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78  (1):1-3, 1950. ISSN 1520-0493.  Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicolo Cesa-Bianchi, G\u00e1bor Lugosi, and others. On prediction of individual sequences. The Annals  of Statistics, 27(6):1865-1895, 1999.  Dean P Foster and Rakesh Vohra. Regret in the on-line decision problem. Games and Economic  Behavior, 29(1):7-35, 1999.  Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation.  In Proceedings of the 28th  Conference on Learning Theory, pages 1-18, 2015.  Rafael Frongillo and Bo Waggoner. An Axiomatic Study of Scoring Rule Markets. Preprint, 2017.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Associa-  tion, 106(494):746-762, 2011. FRONGILLO NOBEL  Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.  Journal of the American Statistical Association, 102(477):359-378, 2007.  Irving John Good. Rational decisions. Journal of the Royal Statistical Society. Series B (Method-  ological), pages 107-114, 1952.  R. Hanson. Combinatorial Information Market Design. Information Systems Frontiers, 5(1):107-  119, 2003.  David Haussler, Jyrki Kivinen, and Manfred K Warmuth. Sequential prediction of individual se- quences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906- 1925, 1998.  Andrei N Kolmogorov. Three approaches to the quantitative definition ofinformation\u2019. Problems of  information transmission, 1(1):1-7, 1965.  Nicolas S. Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions. In  Proceedings of the 10th ACM conference on Electronic commerce, pages 109-118, 2009.  Nicolas S. Lambert, David M. Pennock, and Yoav Shoham. Eliciting properties of probability distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129- 138, 2008.  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  Per Martin-L\u00f6f. The definition of random sequences.  Information and Control, 9(6):602-619,  December 1966. ISSN 0019-9958.  J. McCarthy. Measures of the value of information. Proceedings of the National Academy of  Sciences of the United States of America, 42(9):654, 1956.  R v Mises. Grundlagen der wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 5(1-2):52-99,  1919.  Berkeley, 1985.  Andrew B. Nobel. Some stochastic properties of memoryless individual sequences. IEEE Transac-  tions on Information Theory, 50(7):1497-1505, 2004.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California,  M.D. Reid and R.C. Williamson. Composite binary losses. The Journal of Machine Learning  Research, 9999:2387-2422, 2010.  L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American Statis-  tical Association, pages 783-801, 1971.  Glenn Shafer and Vladimir Vovk. Probability and finance: it\u2019s only a game!, volume 491. John  Wiley & Sons, 2005.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R(cid:13)  in Machine Learning, 4(2):107-194, 2012. MEMORYLESS SEQUENCES FOR DIFFERENTIABLE LOSSES  Ingo Steinwart, Chlo\u00e9 Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties. In Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014.  Vladimir Andreevich Uspenskii, Alexei L Semenov, and A Kh Shen. Can an individual sequence  of zeros and ones be random? Russian Mathematical Surveys, 45(1):121, 1990.  A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press, June 2000. ISBN 978-0-  521-78450-4. Google-Books-ID: UEuQEM5RjWgC.  E. Vernet, R.C. Williamson, and M.D. Reid. Composite Multiclass Losses. NIPS, 2011.  V. Vovk. Probability theory for the Brier game. Theoretical Computer Science, 261(1):57-79, June  2001. ISSN 0304-3975.  Vladimir Vovk. Laws of probabilities in efficient markets, November 2014.  Vladimir V V\u2019yugin. Effective convergence in probability and an ergodic theorem forindividual  random sequences. Theory of Probability & Its Applications, 42(1):39-50, 1998.  J. Wolfers and E. Zitzewitz. Prediction Markets. Journal of Economic Perspective, 18(2):107-126,  2004. "}, "Matrix Completion from O(n) Samples in Linear Time": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Matrix Completion from O(n) Samples in Linear Time", "abstract": "We consider the problem of reconstructing a rank-$k$ $n \\times n$ matrix $M$ from a sampling of its entries. Under a certain incoherence assumption on $M$ and for the case when both the  rank and the condition number of $M$ are bounded, it was shown in (Cand\u00e8s and Recht, 2009; Cand\u00e8s and Tao, 2010; Keshavan et al., 2010; Recht, 2011; Jain et al., 2012; Hardt, 2014) that $M$ can be recovered exactly or approximately (depending on some trade-off between accuracy and computational complexity) using $O(n \u2009\\text{poly}(\\log n))$ samples in super-linear time $O(n^a \u2009\\text{poly}(\\log n))$ for some constant $a \u22651$. In this paper, we propose a new matrix completion algorithm using a novel sampling scheme based on a union of independent sparse random regular bipartite graphs. We show that under the same conditions w.h.p. our algorithm recovers an $\u03b5$-approximation of $M$ in terms of the Frobenius norm using $O(n \\log^2(1/\u03b5))$ samples and in linear time $O(n \\log^2(1/\u03b5))$. This provides the best known bounds both on the sample complexity and computational cost for reconstructing (approximately) an unknown low-rank matrix. The novelty of  our algorithm  is two new steps of thresholding singular values and rescaling singular vectors in the application of the \u201cvanilla\u201d alternating minimization algorithm. The structure of sparse random regular graphs is used heavily for controlling the impact of these regularization steps.", "pdf_url": "http://proceedings.mlr.press/v65/gamarnik17a/gamarnik17a.pdf", "keywords": ["matrix completion", "alternating minimization", "singular value thresholding", "sparse random graphs"], "reference": "Mohsen Bayati, Jeong Han Kim, and Amin Saberi. A sequential algorithm for generating random  graphs. Algorithmica, 4(58):860-910, 2010.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proceedings of The 31st  International Conference on Machine Learning, pages 1881-1889, 2014.  Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for  matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98  (6):925-936, 2010.  7   MATRIX COMPLETION FROM O(n) SAMPLES IN LINEAR TIME  the operations T2 to lift the small singular values and truncate the large singular values of U t St+1,L \u2200j \u2208 [n], before each row vector of \u02dcV t+1 = (\u02dcvt+1,T , 1 \u2264 j \u2264 n) is computed. This singular value thresholding step enforces that all the singular values of the Gramian matrix inverted in (8) deviate from their expected values by at most 1 \u2212 \u03b2 after proper normalization, and as a result, guarantees the nonsingularity of this (adjusted) Gramian matrix. The convergence of the algorithm relies on the fact that w.h.p. the number of times the algorithm applies the operation T2 in each iteration is a small fraction of n. Also, the operators T1 are applied at the end of each iteration to guarantee the incoherence of the input V t+1 (or U t+1) for the next iteration while maintaining that V t+1 (or U t+1) is still close enough to V \u2217 (or U \u2217).  ,  j  j  Our main result concerns the performance of the algorithm T AM under Assumption 1 and  under both Assumptions 1 and 2, respectively. We recall that T AM is parameterized by \u03b4 and \u03b2.  Theorem 1 Suppose M \u2208 Rn\u00d7n is a rank-k matrix satisfying Assumption 1. Suppose the observed index set \u2126 is sampled according to the model RRG(d, n, N ) in (6). Given any \u03b4 \u2208 (0, 1), \u03b2 \u2208 (0, 1 \u2212 \u03b4) and (cid:15) \u2208 (0, 2/3), there exists a C(\u03b4, \u03b2) > 0 such that for  d \u2265 C(\u03b4, \u03b2)k4\u00b52 0  (cid:19)2  (cid:18) \u03c3\u2217 1 \u03c3\u2217 k  +  5\u00b50k(1 + \u03b4/3) \u03b42  log  (cid:19)  (cid:18) 1 (cid:15)  and N \u2265 1 + (cid:100)log( 2 MN (cid:107)F \u2264 (cid:15)(cid:107)M (cid:107)F w.h.p.  (cid:15) )/ log 4(cid:101), the T AM algorithm produces a matrix MN satisfying (cid:107)M \u2212  Furthermore, suppose M satisfies both Assumptions 1 and 2. Then for \u03b4 \u2208 (0, 1) as defined in  Assumption 2 and \u03b2 \u2208 (0, 1 \u2212 \u03b4), the same result holds when  (11)  (12)  d \u2265 C(\u03b4, \u03b2)k4\u00b52 0  (cid:19)2  ,  (cid:18) \u03c3\u2217 1 \u03c3\u2217 k  for the same constant C(\u03b4, \u03b2) in (11).  Theorem 1 states that under Assumption 1 the T AM algorithm produces a rank-k (cid:15)-approximation  of matrix M using O(dn log(1/(cid:15))) samples for d satisfying (11). Furthermore, under both Assump- tion 1 and Assumption 2 the T AM algorithm produces a rank-k (cid:15)-approximation of matrix M using O(dn log(1/(cid:15))) samples for d satisfying (12).  References  Mohsen Bayati, Jeong Han Kim, and Amin Saberi. A sequential algorithm for generating random  graphs. Algorithmica, 4(58):860-910, 2010.  Srinadh Bhojanapalli and Prateek Jain. Universal matrix completion. In Proceedings of The 31st  International Conference on Machine Learning, pages 1881-1889, 2014.  Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for  matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98  (6):925-936, 2010. GAMARNIK LI ZHANG  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of computational mathematics, 9(6):717-772, 2009.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions  on Information Theory, 57(3):1548-1566, 2011.  Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 651-660. IEEE, 2014.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat-  ing minimization. arXiv preprint arXiv:1212.0467, 2012.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.  Jeong Han Kim. Poisson cloning model for random graphs. In Proceedings of the International  Congress of Mathematicians, pages 873-897, 2006.  Franz J Kir\u00b4aly, Louis Theran, and Ryota Tomioka. The algebraic combinatorial approach for low-  rank matrix completion. Journal of Machine Learning Research, 16:1391-1436, 2015.  Raghu Meka, Prateek Jain, and Inderjit S Dhillon. Matrix completion from power-law distributed  samples. In Advances in neural information processing systems, pages 1258-1266, 2009.  Daniel L Pimentel-Alarc\u00b4on, Nigel Boston, and Robert D Nowak. A characterization of deterministic sampling patterns for low-rank matrix completion. IEEE Journal of Selected Topics in Signal Processing, 10(4):623-636, 2016.  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12(Dec):3413-3430, 2011. "}, "High Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transtition": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "High Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transtition", "abstract": "We consider a sparse linear regression model $Y=X\u03b2^*+W$ where $X$ is $n\\times p$ matrix Gaussian i.i.d.  entries, $W$ is $n\\times 1$ noise vector with i.i.d. mean zero Gaussian entries and standard deviation $\u03c3$, and $\u03b2^*$ is $p\\times 1$ binary vector with support size (sparsity)  $k$. Using a  novel conditional second moment method  we obtain a tight up to a multiplicative constant approximation of the optimal squared error $\\min_\u03b2\\|Y-X\u03b2\\|_2$, where the minimization is over all $k$-sparse binary vectors $\u03b2$. The approximation reveals interesting structural properties of the underlying regression problem. In particular, \\beginenumerate \\item [(a)] We establish that $n^*=2k\\log p/\\log (2k/\u03c3^2+1)$ is a phase transition point with the following \u201call-or-nothing\u201d property. When $n$ exceeds $n^*$,  $(2k)^-1\\|\\beta_2-\u03b2^*\\|_0\u22480$, and when $n$ is  below $n^*$, $(2k)^-1\\|\\beta_2-\u03b2^*\\|_0\u22481$, where $\\beta_2$ is the optimal solution achieving the smallest squared error. As a corollary  $n^*$ is the asymptotic threshold for recovering $\u03b2^*$  information theoretically. Note that $n^*$ is asymptotically below the threshold  $n_\\text{LASSO}/CS=(2k+\u03c3^2)\\log p$, above which the LASSO and Compressive Sensing methods are able to recover $\u03b2^*$. \\item [(b)] We compute the squared error for an intermediate problem $\\min_\u03b2\\|Y-X\u03b2\\|_2$ where the minimization is restricted to vectors $\u03b2$ with $\\|\u03b2-\u03b2^*\\|_0=2k \u03b6$, for some fixed ratio $\u03b6\u2208[0,1]$. We show that a lower bound part $\u0393(\u03b6)$ of the estimate, which essentially corresponds to the estimate based on the first moment method, undergoes a phase transition  at three different thresholds, namely $n_\\text{inf},1=\u03c3^2\\log p$, which is information theoretic bound for recovering $\u03b2^*$ when $k=1$ and $\u03c3$ is large, then at $n^*$ and finally at $n_\\text{LASSO}/CS$. \\item [(c)] We establish a certain Overlap Gap Property (OGP) on the space of all $k$-sparse binary vectors $\u03b2$ when $n\\le ck\\log p$ for sufficiently small constant $c$. By drawing a connection with a similar OGP exhibited by many randomly generated constraint satisfaction problems and statistical physics models, we conjecture that OGP is the source of algorithmic hardness of solving the minimization problem $\\min_\u03b2\\|Y-X\u03b2\\|_2$ in the regime $n", "pdf_url": "http://proceedings.mlr.press/v65/david17a/david17a.pdf", "keywords": ["Linear regression", "High-dimensional inference", "Second moment method", "Phase transitions"], "reference": "The authors would like to thank Philippe Rigollet for helpful discussions during the preparation of this paper.  D. Achlioptas, A. Coja-Oghlan, and F. Ricci-Tersenghi. On the solution space geometry of random  formulas. Random Structures and Algorithms, 38:251-268, 2011.  Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase transitions. In Foun- dations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 793-802. IEEE, 2008.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X.  5   HIGH-DIMENSIONAL REGRESSION WITH BINARY COEFFICIENTS  Theorem 4 (The Overlap Gap Property) Suppose the assumptions of Theorem 1 hold. Suppose in addition \u03c32 \u2192 +\u221e. For every sufficiently large constant D0 there exist sequences 0 < \u03b61,k,n < \u03b62,k,n < 1 satisfying  lim k\u2192\u221e  k (\u03b62,k,n \u2212 \u03b61,k,n) = +\u221e,  as k \u2192 \u221e, and such that if rk = D0 max (\u0393(0), \u0393(1)) and max{ 1 n \u2264 k log p/(3 log D0) then w.h.p. as k increases the following holds  C k log k, (cid:0)e7D2  0 + 1(cid:1) \u03c32 log p} \u2264  (a) For every \u03b2 \u2208 Srk  (b) \u03b2\u2217 \u2208 Srk . In particular the set  is non-empty.  (c) The cardinality of the set  (2k)\u22121 (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)0 < \u03b61,k,n or (2k)\u22121 (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)0 > \u03b62,k,n.  Srk \u2229 {\u03b2 : (2k)\u22121 (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)0 < \u03b61,k,n}  |Srk \u2229 {\u03b2 : (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)0} = 2k}|,  0 . In particular the set Srk \u2229 {\u03b2 : (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)0} = 2k} has exponentially many in  n 3  is at least D n elements.  Acknowledgments  References  The authors would like to thank Philippe Rigollet for helpful discussions during the preparation of this paper.  D. Achlioptas, A. Coja-Oghlan, and F. Ricci-Tersenghi. On the solution space geometry of random  formulas. Random Structures and Algorithms, 38:251-268, 2011.  Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase transitions. In Foun- dations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 793-802. IEEE, 2008.  Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on  information theory, 51(12):4203-4215, 2005.  Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by ISSN 0036-1445. doi: 10.1137/  basis pursuit. SIAM Rev., 43(1):129-159, January 2001. S003614450037906X. URL http://dx.doi.org/10.1137/S003614450037906X. GAMARNIK ZADIK  A. Coja-Oghlan and C. Efthymiou. On independent sets in random graphs. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, pages 136-144. SIAM, 2011.  Amin Coja-Oghlan, Amir Haqshenas, and Samuel Hetterich. Walksat stalls well below the satisfia-  bility threshold. arXiv preprint arXiv:1608.00346, 2016.  David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-  1306, 2006.  2013.  Alyson K. Fletcher and Sundeep Rangan. Orthogonal matching pursuit from noisy measurements: A new analysis. In Proceedings of the 22Nd International Conference on Neural Information Processing Systems, NIPS\u201909, pages 540-548, USA, 2009. Curran Associates Inc. ISBN 978-1- 61567-911-9. URL http://dl.acm.org/citation.cfm?id=2984093.2984154.  Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing. Springer,  David Gamarnik and Quan Li. Finding a large submatrix of a gaussian random matrix. arXiv  preprint arXiv:1602.08529, 2016.  of Probability. To appear, a.  David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs. Annals  David Gamarnik and Madhu Sudan. Performance of sequential local algorithms for the random  nae-k-sat problem. SIAM Journal on Computing. To appear, b.  Nicolai Meinshausen and Peter Bhlmann. High-dimensional graphs and variable selection with the lasso. Ann. Statist., 34(3):1436-1462, 06 2006. doi: 10.1214/009053606000000281. URL http://dx.doi.org/10.1214/009053606000000281.  Andrea Montanari, Ricardo Restrepo, and Prasad Tetali. Reconstruction and clustering in random constraint satisfaction problems. SIAM Journal on Discrete Mathematics, 25(2):771-808, 2011.  Mustazee Rahman and Balint Virag. Local algorithms for independent sets are half-optimal. arXiv  preprint arXiv:1402.0485, 2014.  Martin J Wainwright. Information-theoretic limits on sparsity recovery in the high-dimensional and  noisy setting. Information Theory, IEEE Transactions on, 55(12):5728-5741, 2009a.  Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using IEEE transactions on information theory, 55(5):  constrained quadratic programming (lasso). 2183-2202, 2009b.  Wei Wang, Martin J Wainwright, and Kannan Ramchandran. Information-theoretic limits on sparse signal recovery: Dense versus sparse measurement matrices. Information Theory, IEEE Trans- actions on, 56(6):2967-2979, 2010.  Peng Zhao and Bin Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:2541- 2563, December 2006. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm? id=1248547.1248637. "}, "Two-Sample Tests for Large Random Graphs Using Network Statistics": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Two-Sample Tests for Large Random Graphs Using Network Statistics", "abstract": "We consider a two-sample hypothesis testing problem, where the distributions are defined on the space of undirected graphs, and one has access to only one observation from each model. A motivating example for this problem is comparing the friendship networks on Facebook and LinkedIn. The practical approach to such problems is to compare the networks based on certain network statistics. In this paper, we present a general principle for two-sample hypothesis testing in such scenarios without making any assumption about the network generation process. The main contribution of the paper is a general formulation of the problem based on concentration of network statistics, and consequently, a consistent two-sample test that arises as the natural solution for this problem. We also show that the proposed test is minimax optimal for certain network statistics.", "pdf_url": "http://proceedings.mlr.press/v65/ghoshdastidar17a/ghoshdastidar17a.pdf", "keywords": ["Two-sample test", "Random graphs", "Minimax testing", "Concentration inequalities"], "reference": "N. Alon, M. Krivelevich, and V. H. Vu. On the concentration of eigenvalues of random symmetric  matrices. Israel Journal of Mathematics, 131:259-267, 2002.  E. Arias-Castro and N. Verzelen. Community detection in dense random networks. Annals of  Statistics, 42(3):940-969, 2014.  J. Banks, C. Moore, N. Verzelen, R. Vershynin, and J. Xu.  Information-theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization. ArXiv preprint, arXiv:1607.05222v2:1-36, 2016.  Y. Baraud. Non-asymptotic minimax rates of testing in signal detection. Bernoulli, 8(5):577-606,  2002.  B. Bollobas, S. Janson, and O. Riordan. The phase transition in inhomogeneous random graphs.  Random Structures and Algorithms, 31(3):122, 2007.  G. Bounova and O. de Weck. Overview of metrics and their correlation patterns for multiple-metric topology analysis on heterogeneous graph ensembles. Physical Review E, 85:016117, 2012.  S. Bubeck, J. Ding, R. Eldan, and M. Z. R\u00b4acz. Testing for high-dimensional geometry in random  graphs. Random Structures & Algorithms, 49(3):503-532, 2016.  23   TWO-SAMPLE TESTS FOR RANDOM GRAPHS  concentrate for generic model classes. For instance, concentration of functions like clustering coefficient and modularity in combination with our results will help to theoretically validate various claims about properties of brain networks.  \u2022 Bootstrapped variant of proposed two-sample test  The proposed test primarily relies on concentration of the test statistic (5) under the null and alternative hypotheses. It is often observed that the practical performance of concentration based tests can be improved by using bootstrapped variants (Gretton et al., 2012; Tang et al., 2017). In the present context, we feel that bootstrapping can help to achieve low error rate even for smaller and sparser graphs.  However, bootstrapping is a challenging problem in the present setting. Gretton et al. (2012) consider a large population problem, where random mixing of the two population helps to estimate the null distribution for the test statistic. Tang et al. (2017) deal with the two graph setting, but the assumption that the graphs are generated from RDPG model allows parameter estimation, which in turn, aids in generating bootstrapped samples from the estimated models. It would be interesting to come up with bootstrapping procedures without such assumptions.  The work of D. Ghoshdastidar and U. von Luxburg is supported by the German Research Foundation (Research Unit 1735) and the Institutional Strategy of the University of T\u00a8ubingen (DFG, ZUK 63). The work of M. Gutzeit and A. Carpentier is supported by the Deutsche Forschungsgemeinschaft (DFG) Emmy Noether grant MuSyAD (CA 1488/1-1).  Acknowledgments  References  N. Alon, M. Krivelevich, and V. H. Vu. On the concentration of eigenvalues of random symmetric  matrices. Israel Journal of Mathematics, 131:259-267, 2002.  E. Arias-Castro and N. Verzelen. Community detection in dense random networks. Annals of  Statistics, 42(3):940-969, 2014.  J. Banks, C. Moore, N. Verzelen, R. Vershynin, and J. Xu.  Information-theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization. ArXiv preprint, arXiv:1607.05222v2:1-36, 2016.  Y. Baraud. Non-asymptotic minimax rates of testing in signal detection. Bernoulli, 8(5):577-606,  2002.  B. Bollobas, S. Janson, and O. Riordan. The phase transition in inhomogeneous random graphs.  Random Structures and Algorithms, 31(3):122, 2007.  G. Bounova and O. de Weck. Overview of metrics and their correlation patterns for multiple-metric topology analysis on heterogeneous graph ensembles. Physical Review E, 85:016117, 2012.  S. Bubeck, J. Ding, R. Eldan, and M. Z. R\u00b4acz. Testing for high-dimensional geometry in random  graphs. Random Structures & Algorithms, 49(3):503-532, 2016. GHOSHDASTIDAR GUTZEIT CARPENTIER LUXBURG  A. Carpentier and R. Nickl. On signal detection and confidence sets for low rank inference problems.  Electronic Journal of Statistics, 9(2):2675-2688, 2015.  S. Chatterjee. Matrix estiamtion via universal singular value thresholding. Annals of Statistics, 43  (1):177-214, 2012.  Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix localiza- tion with a growing number of clusters and submatrices. Journal of Machine Learning Research, 17(27):1-57, 2016.  F. R. K. Chung. Spectral graph theory, volume 92. American Mathematical Society, 1997.  O. Collier. Minimax hypothesis testing for curve registration. In AISTATS, 2012.  A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00a8olkopf, and A. Smola. A kernel two-sample test.  Journal of Machine Learning Research, 13:723-733, 2012.  Y. I. Ingster and I. A. Suslina. Minimax nonparametric hypothesis testing for ellipsoids and Besov  bodies. ESAIM: Probability and Statistics, 4:53-135, 2000.  F. Klimm, D. S. Bassett, J. M. Carlson, and P. J. Mucha. Resolving structural variability in network  models and the brain. PloS Computational Biology, 10(3):e1003491, 2014.  R. Kondor and H. Pan. The multiscale Laplacian graph kernel. In NIPS, 2016.  M. Krivelevich and B. Sudakov. The largest eigenvalue of sparse random graphs. Combinatorics,  Probability and Computing, 12(1):61-72, 2003.  J. Lei. A goodness-of-fit test for stochastic block models. Annals of Statistics, 44(1):401-424, 2016.  J. Lei and A. Rinaldo. Consistency of spectral clustering in stochastic block models. Annals of  Statistics, 43(1):215-237, 2015.  natorics, 20(4):P27, 2013.  L. Lu and X. Peng. Spectra of edge-independent random graphs. The Electronic Journal of Combi-  E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model.  Probability Theory and Related Fields, 162(3-4):431-461, 2015.  R. Mukherjee, N. S. Pillai, and X. Lin. Hypothesis testing for high-dimensional sparse binary  regression. Annals of Statistics, 43(1):352-381, 2015.  M. Penrose. Random geometric graphs, volume 5. Oxford University Press, 2003.  M. Rubinov and O. Sporns. Complex network measures of brain connectivity: Uses and interpreta-  tions. Neuroimage, 52:1059-1069, 2010.  C. J. Stam, B. F. Jones, G. Nolte, M. Breakspear, and P. Scheltens. Small-world networks and  functional connectivity in Alzheimer\u2019s disease. Cerebral Cortex, 17(1):92-99, 2007.  M. Tang, A. Athreya, D. L. Sussman, V. Lyzinski, and C. E. Priebe. A semiparametric two-sample hypothesis testing problem for random graphs. Journal of Computational and Graphical Statis- tics, 26:344-354, 2017. "}, "Effective Semisupervised Learning on Manifolds": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Effective Semisupervised Learning on Manifolds", "abstract": "The abundance of unlabeled data makes semi-supervised learning (SSL) an attractive approach for improving the accuracy of learning systems. However, we are still far from a complete theoretical understanding of the benefits of this learning scenario in terms of sample complexity. In particular, for many natural learning settings it can in fact be shown that SSL does not improve sample complexity. Thus far, the only case where SSL provably helps, without compatibility assumptions, is a recent combinatorial construction of Darnstadt et al. Deriving similar theoretical guarantees for more commonly used approaches to SSL remains a challenge. Here, we provide the first analysis of manifold based SSL, where there is a provable gap between supervised learning and SSL, and this gap can be arbitrarily large. Proving the required lower bound is a technical challenge, involving tools from geometric measure theory. The algorithm we analyse is similar to subspace clustering, and thus our results demonstrate that this method can be used to improve sample complexity.", "pdf_url": "http://proceedings.mlr.press/v65/globerson17a/globerson17a.pdf", "keywords": [], "reference": "Yasemin Altun, David McAllester, and Mikhail Belkin. Maximum margin semi-supervised learning  for structured variables. Advances in neural information processing systems, 18:33, 2006.  Martin Azizyan, Aarti Singh, and Larry Wasserman. Density-sensitive semisupervised inference.  The Annals of Statistics, 41(2):751-771, 2013.  Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Jour-  nal of the ACM (JACM), 57(3):19, 2010.  Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian manifolds. Machine  Learning, 56(1):209-239, 2004.  Shai Ben-David and Ruth Urner. The sample complexity of agnostic learning under deterministic  labels. In Proceedings of The 27th Conference on Learning Theory, pages 527-542, 2014.  Shai Ben-David, Tyler Lu, and D\u00b4avid P\u00b4al. Does unlabeled data provably help? worst-case anal- ysis of the sample complexity of semi-supervised learning. In Proceedings of the 21st Annual Conference on Learning Theory, pages 33-44, 2008.  Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts.  Proc. 18th International Conf. on Machine Learning, 2001.  Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern  Recognition Letters, 16(1):105-111, 1995.  Olivier Chapelle, Bernhard Sch\u00a8olkopf, Alexander Zien, et al. Semi-supervised learning. MIT press  Cambridge, 2006.  Fabio Gagliardi Cozman, Ira Cohen, Marcelo Cesar Cirelo, et al. Semi-supervised learning of mixture models. In Proceedings of the Twentieth International Conference on Machine Learning, pages 99-106, 2003.  15  204060801001201406065707580859095100105labeled sample sizeAccuracy  \u03c32=0\u03c32= 0.01\u03c32=0.1 EFFECTIVE SEMI-SUPERVISED LEARNING ON MANIFOLDS  Figure 4: Supervised Learner vs. Semi Supervised Learner. Solid lines are SSL, dashed lines are SL. We measure accuracy as a function of labeled sample size. The SSL algorithm was based on subspace clustering, as described in Section 8.  References  Yasemin Altun, David McAllester, and Mikhail Belkin. Maximum margin semi-supervised learning  for structured variables. Advances in neural information processing systems, 18:33, 2006.  Martin Azizyan, Aarti Singh, and Larry Wasserman. Density-sensitive semisupervised inference.  The Annals of Statistics, 41(2):751-771, 2013.  Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Jour-  nal of the ACM (JACM), 57(3):19, 2010.  Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian manifolds. Machine  Learning, 56(1):209-239, 2004.  Shai Ben-David and Ruth Urner. The sample complexity of agnostic learning under deterministic  labels. In Proceedings of The 27th Conference on Learning Theory, pages 527-542, 2014.  Shai Ben-David, Tyler Lu, and D\u00b4avid P\u00b4al. Does unlabeled data provably help? worst-case anal- ysis of the sample complexity of semi-supervised learning. In Proceedings of the 21st Annual Conference on Learning Theory, pages 33-44, 2008.  Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts.  Proc. 18th International Conf. on Machine Learning, 2001.  Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern  Recognition Letters, 16(1):105-111, 1995.  Olivier Chapelle, Bernhard Sch\u00a8olkopf, Alexander Zien, et al. Semi-supervised learning. MIT press  Cambridge, 2006.  Fabio Gagliardi Cozman, Ira Cohen, Marcelo Cesar Cirelo, et al. Semi-supervised learning of mixture models. In Proceedings of the Twentieth International Conference on Machine Learning, pages 99-106, 2003.204060801001201406065707580859095100105labeled sample sizeAccuracy  \u03c32=0\u03c32= 0.01\u03c32=0.1 GLOBERSON LIVNI SHALEV-SHWARTZ  Malte Darnst\u00a8adt. An investigation on the power of unlabeled data. PhD thesis, Ruhr University  Bochum, 2015.  STACS, pages 185-196, 2013.  1996.  Malte Darnst\u00a8adt, Hans Ulrich Simon, and Bal\u00b4azs Sz\u00a8or\u00b4enyi. Unlabeled data does provably help. In  L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Thoery of Pattern Recognition. Springer,  Mathias Drton, Bernd Sturmfels, and Seth Sullivant. Lectures on algebraic statistics, volume 39.  Springer Science & Business Media, 2008.  Ehsan Elhamifar and Ren\u00b4e Vidal. Sparse subspace clustering.  In Computer Vision and Pattern  Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2790-2797. IEEE, 2009.  Paolo Gibilisco. Algebraic and geometric methods in statistics. Cambridge University Press, 2010.  Robin Hartshorne. Algebraic geometry, volume 52. Springer Science & Business Media, 1977.  Daniel Heldt, Martin Kreuzer, Sebastian Pokutta, and Hennie Poulisse. Approximate computation  of zero-dimensional polynomial ideals. J. Symb. Comput., 44(11):1566-1591, 2009.  Diederik P. Kingma and Max Welling. Auto-encoding variational bayes.  In Proceedings of the  Second International Conference on Learning Representations (ICLR 2014), April 2014.  Franz J Kir\u00b4aly, Paul Von B\u00a8unau, Jan Saputra M\u00a8uller, Duncan AJ Blythe, Frank C Meinecke, and Klaus-Robert M\u00a8uller. Regression for sets of polynomial equations. In AISTATS, pages 628-637, 2012.  Steven G Krantz and Harold R Parks. Geometric integration theory. Springer Science & Business  Media, 2008.  Jason Lafferty, Xiaojin Zhu, and Yan Liu. Kernel conditional random fields: representation and clique selection. In Proceedings of the twenty-first international conference on Machine learning, page 64. ACM, 2004.  John D. Lafferty and Larry A. Wasserman. Statistical analysis of semi-supervised regression. In  Advances in Neural Information Processing Systems 20, pages 801-808, 2007.  Roi Livni, David Lehavi, Sagi Schein, Hila Nachliely, Shai Shalev-Shwartz, and Amir Globerson. Vanishing component analysis. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 597-605, 2013.  Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses.  Computer Science Dept., University of Chicago, Tech. Rep. TR-2008-01, 2008.  Dohyung Park, Constantine Caramanis, and Sujay Sanghavi. Greedy subspace clustering. In Ad-  vances in Neural Information Processing Systems, pages 2753-2761, 2014.  Lance Parsons, Ehtesham Haque, and Huan Liu. Subspace clustering for high dimensional data: a  review. ACM SIGKDD Explorations Newsletter, 6(1):90-105, 2004. EFFECTIVE SEMI-SUPERVISED LEARNING ON MANIFOLDS  Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold  tangent classifier. In NIPS, volume 271, page 523, 2011.  Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster  assumption. arXiv preprint math/0604233, 2006.  Igor Rostislavovi\u02c7c \u02c7Safarevi\u02c7c. Basic algebraic geometry, volume 1. Springer, 1994.  Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn\u2019t. In  Advances in neural information processing systems, pages 1513-1520, 2009.  Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering with  outliers. The Annals of Statistics, 40(4):2195-2238, 2012.  Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace clustering.  The Annals of Statistics, 42(2):669-699, 2014.  Vladimir Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Ren\u00b4e Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52-68,  2010.  Ren\u00b4e Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca). Pattern  Analysis and Machine Intelligence, IEEE Transactions on, 27(12):1945-1959, 2005.  Dongming Wang.  Irreducible decomposition of algebraic varieties via characteristics sets and  gr\u00a8obner bases. Computer Aided Geometric Design, 9(6):471-484, 1992.  Sumio Watanabe. Algebraic geometry and statistical learning theory, volume 25. Cambridge Uni-  Hassler Whitney. Elementary structure of real algebraic varieties. The Annals of Mathematics, 66  versity Press, 2009.  (3):545-556, 1957.  Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In Proceedings of the 33nd International Conference on Machine Learn- ing (ICML), pages 40-48, 2016.  Xiaojin Zhu.  Semi-supervised learning literature survey. Computer Science, University of  Wisconsin-Madison, 2:3, 2006.  "}, "Reliably Learning the ReLU in Polynomial Time": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Reliably Learning the ReLU in Polynomial Time", "abstract": "We give the first dimension-efficient algorithms for learning Rectified Linear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto \\mathsf{max}(0, \u00a0\\mathbf{w} \u22c5\\mathbf{x})$ with $\\mathbf{w} \u2208\\mathbb{S}^n-1$. Our algorithm works in the challenging Reliable Agnostic learning model of Kalai, Kanade and Mansour (2012) where the learner is given access to a distribution $\\mathcal{D}$ on labeled examples but the labeling may be arbitrary.  We construct a hypothesis that simultaneously minimizes the false-positive rate and the loss on inputs given positive labels by $\\mathcal{D}$, for any convex, bounded, and Lipschitz loss function. The algorithm runs in polynomial-time (in $n$) with respect to \\em any distribution on $\\mathbb{S}^n-1$ (the unit sphere in $n$ dimensions) and for any error parameter $\u03b5= \u03a9(1 / \\log n)$ (this yields a PTAS for a question raised by F. Bach on the complexity of maximizing ReLUs).  These results are in contrast to known efficient algorithms for reliably learning linear threshold functions, where $\u03b5$ must be $\u03a9(1)$ and strong assumptions are required on the marginal distribution. We can compose our results to obtain the first set of efficient algorithms for learning constant-depth networks of ReLU with fixed polynomial-dependence in the dimension. For depth-2 networks of sigmoids, we obtain the first algorithms that have a polynomial dependency in \\em all parameters. Our techniques combine kernel methods and polynomial approximations with a \u201cdual-loss\u201d approach to convex programming. As a byproduct we obtain a number of applications including the first set of efficient algorithms for \u201cconvex piecewise-linear fitting\u201d and the first efficient algorithms for noisy polynomial reconstruction of low-weight polynomials on the unit sphere.", "pdf_url": "http://proceedings.mlr.press/v65/goel17a/goel17a.pdf", "keywords": ["ReLU", "agnostic learning", "reliable", "kernel methods"], "reference": "Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning sparse polynomial functions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algo- rithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 500-510, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anribit Mukherjee. Understanding deep neural networks with rectified linear units, 2016. URL: https://arxiv.org/abs/1611.01491.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. 2014.  Peter Bartlett, Daniel Kane, and Adam Klivans. Personal communication. 2017.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. JACM: Journal of the ACM, 50, 2003.  Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other  kernel-based learning methods. Cambridge University Press, 2000.  36   GOEL KANADE KLIVANS THALER  Corollary 49 Let A be an algorithm that learns ReLUs on all domains X \u2286 Rn where (w \u00b7 x) may take on values that are \u03c9(1) with respect to the dimension n. Then any algorithm for reli- ably learning C in time g((cid:15)) \u00b7 poly(n) will break the Sparse Learning Parity with Noise hardness assumption.  Finally, we point out Kalai et al. (2012) proved that reliably learning conjunctions is also as hard as PAC Learning DNF formulas. Thus, by our above reduction, any efficient algorithm for reliably learning ReLUs would give an efficient algorithm for PAC learning DNF formulas (again this would be considered a breakthrough result in computational learning theory).  6. Conclusions and Open Problems  We have given the first set of efficient algorithms for ReLUs in a natural learning model. ReLUs are both effective in practice and, unlike linear threshold functions (halfspaces), admit non-trivial learning algorithms for all distributions with respect to adversarial noise. We \u201csidestepped\u201d the hardness results in Boolean function learning by focusing on problems that are not entirely scale- invariant with respect to the choice of domain (e.g., reliably learning ReLUs). The obvious open question is to improve the dependence of our main result on 1/(cid:15). We can handle (cid:15) = 1/ log n, and as mentioned in the introduction, (cid:15) = 1/poly(n) seems difficult. Is it possible to obtain a run-time of poly(n, k) \u00b7 2O(1/(cid:15)) for depth-2 networks of ReLUs with k hidden units?  Acknowledgements. The authors are grateful to Sanjeev Arora and Roi Livni for helpful feedback and useful discussions on this work.  References  Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning sparse polynomial functions. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algo- rithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 500-510, 2014.  Raman Arora, Amitabh Basu, Poorya Mianjy, and Anribit Mukherjee. Understanding deep neural networks with rectified linear units, 2016. URL: https://arxiv.org/abs/1611.01491.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. 2014.  Peter Bartlett, Daniel Kane, and Adam Klivans. Personal communication. 2017.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463-482, 2002. URL http: //www.jmlr.org/papers/v3/bartlett02a.html.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. JACM: Journal of the ACM, 50, 2003.  Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other  kernel-based learning methods. Cambridge University Press, 2000. RELIABLY LEARNING THE RELU IN POLYNOMIAL TIME  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC, pages 105-117. ACM, 2016. ISBN 978-1-4503-4132-5. URL http://dl.acm.org/citation.cfm?id= 2897518.  Ilias Diakonikolas, Daniel M. Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In FOCS, pages 11-20. IEEE Computer Society, 2010. ISBN 978-0-7695- 4244-7.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks.  In Pro- ceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23- 26, 2016, pages 907-940, 2016. URL http://jmlr.org/proceedings/papers/v49/ eldan16.html.  Bassey Etim. Approve or Reject: Can You Moderate Five New York Times Comments? The New York Times, 2016. URL http://www.nytimes.com/interactive/2016/09/ 20/insider/approve-or-reject-moderation-quiz.html. Originally published September 20, 2016. Retrieved October 4, 2016.  V. Feldman, P. Gopalan, S. Khot, and A. K. Ponnuswami. On agnostic learning of parities, mono- mials, and halfspaces. SIAM J. Comput, 39(2):606-645, 2009. URL http://dx.doi.org/ 10.1137/070684914.  Vitaly Feldman and Pravesh Kothari. Agnostic learning of disjunctions on symmetric distributions.  Journal of Machine Learning Research, 16:3455-3467, 2015.  Jerome H. Friedman. Multivariate adaptive regression splines. Ann. Statist, 1991.  David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Inf. Comput., 100(1):78-150, 1992.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026-1034, 2015.  Thomas Hofmann, Bernhard Sch\u00a8olkopf, and Alexander J Smola. Kernel methods in machine learn-  ing. The annals of statistics, pages 1171-1220, 2008.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.  Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:  Risk bounds, margin bounds, and regularization. 2008.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008. doi: 10.1137/ 060649057.  Adam Tauman Kalai, Varun Kanade, and Yishay Mansour. Reliable agnostic learning. Journal of  Computer and System Sciences, 78(5):1481-1495, 2012. GOEL KANADE KLIVANS THALER  Kenji Kawaguchi.  Deep learning without poor  Masashi Sugiyama, Ulrike V. Luxburg, itors, NIPS, pages 586-594, advances-in-neural-information-processing-systems-29-2016.  In Daniel D. Lee, Isabelle Guyon, ed- URL http://papers.nips.cc/book/  and Roman Garnett,  local minima.  2016.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning.  Mach. Learn., 17(2-3):115-141, 1994.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In  RANDOM, 2014.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci, 75(1):2-12, 2009. URL http://dx.doi.org/10.1016/ j.jcss.2008.07.008.  Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7533):436-444, May 2015.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of train- ing neural networks. pages 855-863, 2014. URL http://papers.nips.cc/paper/ 5267-on-the-computational-efficiency-of-training-neural-networks.  Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural  network acoustic models. In Proc. ICML, volume 30, 2013.  Alessandro Magnani and Stephen P. Boyd. Convex piecewise-linear fitting. Optimization and ISSN 1573-2924. doi: 10.1007/s11081-008-9045-3. URL  Engineering, 10(1):1-17, 2009. http://dx.doi.org/10.1007/s11081-008-9045-3.  James Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character, 209:415-446, 1909.  Donald J. Newman. Rational approximation to |x|. Michigan Math. J., 11(1):11-14, 03 1964.  Phillippe Rigollet. High-Dimensional Statistics. MIT, 1st edition, 2015.  Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse  connectivity. arXiv preprint arXiv:1412.2693, 2014.  Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with  the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, 2011.  Alexander A. Sherstov. Making polynomials robust to noise. In Proceedings of the Forty-fourth Annual ACM Symposium on Theory of Computing, STOC \u201912, pages 747-758, New York, NY, USA, 2012. ACM. RELIABLY LEARNING THE RELU IN POLYNOMIAL TIME  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. J. ACM, 62(2):13:1-13:45, May 2015. doi: 10.1145/2728167. URL http://doi.acm.org/10.1145/2728167.  Wikipedia. Multinomial theorem \u2014 Wikipedia, the free encyclopedia, 2016a. URL: https:  //en.wikipedia.org/wiki/Multinomial_theorem.  Wikipedia. Polynomial kernel \u2014 Wikipedia, the free encyclopedia, 2016b. URL: https://en.  wikipedia.org/wiki/Polynomial_kernel.  Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and  neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.  Yuchen Zhang, Jason Lee, and Michael Jordan. (cid:96)1 networks are improperly learnable in polynomial-  time. In ICML, 2016a.  Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks.  arXiv preprint arXiv:1609.01000, 2016b. "}, "Fast Rates for Empirical Risk Minimization of Strict Saddle Problems": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Fast Rates for Empirical Risk Minimization of Strict Saddle Problems", "abstract": "We derive bounds on the sample complexity of empirical risk minimization (ERM) in the context of minimizing non-convex risks that admit the strict saddle property. Recent progress in non-convex optimization has yielded efficient algorithms for minimizing such functions. Our results imply that these efficient algorithms are statistically stable and also generalize well. In particular, we derive fast rates which resemble the bounds that are often attained in the strongly convex setting. We specify our bounds to Principal Component Analysis and Independent Component Analysis. Our results and techniques may pave the way for statistical analyses of additional strict saddle problems.", "pdf_url": "http://proceedings.mlr.press/v65/gonen17a/gonen17a.pdf", "keywords": [], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding local  minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Anima Anandkumar, Yuan Deng, Rong Ge, and Hossein Mobah. Homotopy method for tensor  principal component analysis. arXiv preprint arXiv:1610.09322, 2016.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Global optimality of local search  for low rank matrix recovery. arXiv preprint arXiv:1605.07221, 2016.  Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal  component analysis. Machine Learning, 66(2-3):259-294, 2007.  Jonathan M Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and  examples. Springer Science & Business Media, 2010.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. The Journal of Machine Learn-  ing Research, 2:499-526, 2002.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points-online stochastic gradient descent for tensor decomposition. In Proceedings of The 29th Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. arXiv preprint arXiv:1601.04011, 2016.  Alon Gonen, Dan Rosenbaum, Yonina C Eldar, and Shai Shalev-Shwartz. Subspace learning with  partial information. Journal of Machine Learning Research, 17(52):1-21, 2016.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems, pages 1477-1485, 2015.  Kfir Y Levy. The power of normalization: Faster evasion of saddle points.  arXiv preprint  arXiv:1611.04831, 2016.  Ji\u02c7r\u00b4\u0131 Matou\u02c7sek. Lectures on discrete geometry, volume 108. Springer New York, 2002.  Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk mini- mization. Advances in Computational Mathematics, 25(1-3):161-193, 2006.  13   EMPIRICAL RISK MINIMIZATION OF STRICT SADDLE PROBLEMS  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding local  minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Anima Anandkumar, Yuan Deng, Rong Ge, and Hossein Mobah. Homotopy method for tensor  principal component analysis. arXiv preprint arXiv:1610.09322, 2016.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Global optimality of local search  for low rank matrix recovery. arXiv preprint arXiv:1605.07221, 2016.  Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal  component analysis. Machine Learning, 66(2-3):259-294, 2007.  Jonathan M Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and  examples. Springer Science & Business Media, 2010.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. The Journal of Machine Learn-  ing Research, 2:499-526, 2002.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points-online stochastic gradient descent for tensor decomposition. In Proceedings of The 29th Conference on Learning Theory, pages 797-842, 2015.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In  Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. arXiv preprint arXiv:1601.04011, 2016.  Alon Gonen, Dan Rosenbaum, Yonina C Eldar, and Shai Shalev-Shwartz. Subspace learning with  partial information. Journal of Machine Learning Research, 17(52):1-21, 2016.  Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems, pages 1477-1485, 2015.  Kfir Y Levy. The power of normalization: Faster evasion of saddle points.  arXiv preprint  arXiv:1611.04831, 2016.  Ji\u02c7r\u00b4\u0131 Matou\u02c7sek. Lectures on discrete geometry, volume 108. Springer New York, 2002.  Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk mini- mization. Advances in Computational Mathematics, 25(1-3):161-193, 2006. GONEN SHALEV-SHWARTZ  Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global perfor-  mance. Mathematical Programming, 108(1):177-205, 2006.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint  arXiv:1510.06096, 2015.  arXiv:1501.01571, 2015.  Joel A Tropp.  An introduction to matrix concentration inequalities.  arXiv preprint EMPIRICAL RISK MINIMIZATION OF STRICT SADDLE PROBLEMS  "}, "Nearly-tight VC-dimension bounds for piecewise linear neural networks": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Nearly-tight VC-dimension bounds for piecewise linear neural networks", "abstract": "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$, and provide examples with VC-dimension $\u03a9( W L \\log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\u0398(W U)$ on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.", "pdf_url": "http://proceedings.mlr.press/v65/harvey17a/harvey17a.pdf", "keywords": ["VC-dimension", "neural networks", "ReLU activation function Extended abstract The full version with all the proofs appears as [arXiv:170302930", "v2]"], "reference": "University Press, 1999.  published manuscript.  CL is supported by an NSERC graduate scholarship. AM is supported by an NSERC Postdoctoral Fellowship and a Simons-Berkeley Research Fellowship. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing at UC Berkeley.  Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge  Peter Bartlett. The impact of the nonlinearity on the VC-dimension of a deep network, 2017. un-  Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise  polynomial networks. Neural Computation, 10(8):2159-2173, Nov 1998.  Eric B. Baum and David Haussler. What size net gives valid generalization? Neural Computation,  1(1):151-160, 1989.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. JACM, 36(4), 1989. (Conference version in STOC\u201986).  4   HARVEY LIAW MEHRABIAN  functions can easily be simulated using ReLU functions. We refer the reader to the excellent mono- graph by Anthony and Bartlett (1999) that covers these and many other theoretical results on neural networks.  Recently there have been several papers that study neural networks from an approximation the- ory point of view and aim to understand which functions can be expressed using a neural network of given a depth and size. There are technical similarities between our work and these. Last year, two striking papers considered the problem of approximating a deep neural network with a shallower network. Telgarsky (2016) shows that there is a ReLU network with L layers and U = \u0398(L) units such that any network approximating it with only O(L1/3) layers must have \u2126(2L1/3) units; this phenomenon holds even for real-valued functions. Eldan and Shamir (2016) show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.  Very recently, several authors have shown that deep neural networks are capable of approxi- mating broad classes of functions. Safran and Shamir (2017) show that a sufficiently non-linear C2 function on [0, 1]d can be approximated with (cid:15) error in L2 by a ReLU network with O(polylog(1/(cid:15))) layers and weights, but any such approximation with O(1) layers requires \u2126(1/(cid:15)) weights. Yarot- sky (2017) shows that any Cn-function on [0, 1]d can be approximated with (cid:15) error in L\u221e by a ReLU network with O(log(1/(cid:15))) layers and O(( 1 (cid:15) )d/n log(1/(cid:15))) weights. Liang and Srikant (2017) show that a sufficiently smooth univariate function can be approximated with (cid:15) error in L\u221e by a network with ReLU and threshold gates with \u0398(log(1/(cid:15))) layers and O(polylog(1/(cid:15))) weights, but that \u2126(poly(1/(cid:15))) weights would be required if there were only o(log(1/(cid:15))) layers; they also prove analogous results for multivariate functions. Lastly, Cohen et al. (2016) draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks.  Acknowledgments  References  University Press, 1999.  published manuscript.  CL is supported by an NSERC graduate scholarship. AM is supported by an NSERC Postdoctoral Fellowship and a Simons-Berkeley Research Fellowship. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing at UC Berkeley.  Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge  Peter Bartlett. The impact of the nonlinearity on the VC-dimension of a deep network, 2017. un-  Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise  polynomial networks. Neural Computation, 10(8):2159-2173, Nov 1998.  Eric B. Baum and David Haussler. What size net gives valid generalization? Neural Computation,  1(1):151-160, 1989.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. JACM, 36(4), 1989. (Conference version in STOC\u201986). NEARLY-TIGHT VC-DIMENSION BOUNDS FOR NEURAL NETWORKS  N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis.  In COLT, 2016.  2016.  Thomas M. Cover. Capacity problems for linear machines. In L. Kanal, editor, Pattern Recognition,  pages 283-289. Thompson Book Co., 1968.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In COLT,  Paul W. Goldberg and Mark R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning, 18(2):131-148, 1995. (Conference version in COLT\u201993).  Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:  //www.deeplearningbook.org.  Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks, 2017. URL https://arxiv.org/abs/1703.02930.  Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,  2015.  arXiv:1610.04161.  Sept 1994.  Shyu Liang and R. Srikant. Why deep neural networks for function approximation?, 2017.  Wolfgang Maass. Neural nets with superlinear VC-dimension. Neural Computation, 6(5):877-884,  I. Safran and O. Shamir. Depth-width tradeoffs in approximating natural functions with neural  networks, 2017. arXiv:1610.09887.  Matus Telgarsky. Benefits of depth in neural networks. In COLT, 2016.  V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971. doi: 10.1137/1116025.  Hugh E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the  American Mathematical Society, 133(1):167-178, 1968.  Dmitry Yarotsky.  Error bounds for approximations with deep ReLU networks, 2017.  arXiv:1610.01145. "}, "Submodular Optimization under Noise": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Submodular Optimization under Noise", "abstract": "We consider the problem of maximizing a monotone submodular function under noise.  Since the 1970s there has been a great deal of work on optimization of submodular functions under various constraints, resulting in algorithms that provide desirable approximation guarantees.  In many applications, however, we do not have access to the submodular function we aim to optimize, but rather to some erroneous or noisy version of it.  This raises the question of whether provable guarantees are obtainable in the presence of error and noise. We provide initial answers by focusing on the problem of maximizing a monotone submodular function under a cardinality constraint when given access to a noisy oracle of the function.  We show that there is an algorithm whose approximation ratio is arbitrarily close to the optimal $1-1/e$ when the cardinality is sufficiently large.  The algorithm can be applied in a variety of related problems including maximizing approximately submodular functions, and optimization with correlated noise.  When the noise is adversarial we show that no non-trivial approximation guarantee can be obtained.", "pdf_url": "http://proceedings.mlr.press/v65/hassidim17a/hassidim17a.pdf", "keywords": ["Submodular", "optimization", "noise"], "reference": "Alexander A. Ageev and Maxim Sviridenko. Pipage rounding: A new method of constructing  algorithms with proven performance guarantee. J. Comb. Optim., 8(3), 2004.  Mikl\u00b4os Ajtai, Vitaly Feldman, Avinatan Hassidim, and Jelani Nelson. Sorting and selection with  imprecise comparisons. In ICALP. 2009.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Ashwinkumar Badanidiyuru, Shahar Dobzinski, Hu Fu, Robert Kleinberg, Noam Nisan, and Tim  Roughgarden. Sketching valuation functions. In SODA, 2012.  Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Stream-  ing submodular maximization: massive data summarization on the \ufb02y. In KDD, 2014.  Maria-Florina Balcan. Learning submodular functions with applications to multi-agent systems. In  Maria-Florina Balcan and Nicholas J. A. Harvey. Learning submodular functions. In STOC, 2011.  Maria-Florina Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions.  AAMAS, 2015.  In COLT, 2012.  Michael Ben Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search  (and pretty good for quantum as well). In FOCS, 2008.  J. Bilmes. Deep mathematical properties of submodularity with applications to machine learning.  Tutorial at NIPS, 2013.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In SODA, 2008.  Nader H. Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership  queries. Journal of Machine Learning Research, 2:359-395, 2002.  Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. A tight linear time (1/2)-  approximation for unconstrained submodular maximization. In FOCS.  Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. Submodular maximization with  cardinality constraints. In SODA, 2014.  D. Buchfuhrer, S. Dughmi, H. Fu, R. Kleinberg, E. Mossel, C. H. Papadimitriou, M. Schapira, Y. Singer, and C. Umans. Inapproximability for VCG-based combinatorial auctions. In SIAM- ACM Symposium on Discrete Algorithms (SODA), 2010a.  D. Buchfuhrer, M. Schapira, and Y. Singer. Computation and incentives in combinatorial public  projects. In EC, 2010b.  Gruia Calinescu, Chandra Chekuri, Martin P\u00b4al, and Jan Vondr\u00b4ak. Maximizing a submodular set  function subject to a matroid constraint. In IPCO. 2007.  23   SUBMODULAR OPTIMIZATION UNDER NOISE  References  Alexander A. Ageev and Maxim Sviridenko. Pipage rounding: A new method of constructing  algorithms with proven performance guarantee. J. Comb. Optim., 8(3), 2004.  Mikl\u00b4os Ajtai, Vitaly Feldman, Avinatan Hassidim, and Jelani Nelson. Sorting and selection with  imprecise comparisons. In ICALP. 2009.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1988.  Ashwinkumar Badanidiyuru, Shahar Dobzinski, Hu Fu, Robert Kleinberg, Noam Nisan, and Tim  Roughgarden. Sketching valuation functions. In SODA, 2012.  Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Stream-  ing submodular maximization: massive data summarization on the \ufb02y. In KDD, 2014.  Maria-Florina Balcan. Learning submodular functions with applications to multi-agent systems. In  Maria-Florina Balcan and Nicholas J. A. Harvey. Learning submodular functions. In STOC, 2011.  Maria-Florina Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions.  AAMAS, 2015.  In COLT, 2012.  Michael Ben Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search  (and pretty good for quantum as well). In FOCS, 2008.  J. Bilmes. Deep mathematical properties of submodularity with applications to machine learning.  Tutorial at NIPS, 2013.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In SODA, 2008.  Nader H. Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership  queries. Journal of Machine Learning Research, 2:359-395, 2002.  Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. A tight linear time (1/2)-  approximation for unconstrained submodular maximization. In FOCS.  Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. Submodular maximization with  cardinality constraints. In SODA, 2014.  D. Buchfuhrer, S. Dughmi, H. Fu, R. Kleinberg, E. Mossel, C. H. Papadimitriou, M. Schapira, Y. Singer, and C. Umans. Inapproximability for VCG-based combinatorial auctions. In SIAM- ACM Symposium on Discrete Algorithms (SODA), 2010a.  D. Buchfuhrer, M. Schapira, and Y. Singer. Computation and incentives in combinatorial public  projects. In EC, 2010b.  Gruia Calinescu, Chandra Chekuri, Martin P\u00b4al, and Jan Vondr\u00b4ak. Maximizing a submodular set  function subject to a matroid constraint. In IPCO. 2007. HASSIDIM SINGER  Chandra Chekuri and Alina Ene. Approximation algorithms for submodular multiway partition. In  FOCS, 2011.  In FOCS, 2015.  Chandra Chekuri, T. S. Jayram, and Jan Vondr\u00b4ak. On multiplicative weight updates for concave and  submodular function maximization. In ITCS, 2015.  Flavio Chierichetti, Abhimanyu Das, Anirban Dasgupta, and Ravi Kumar. Approximate modularity.  Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset  selection, sparse approximation and dictionary selection. In ICML, 2011.  Abhimanyu Das, Anirban Dasgupta, and Ravi Kumar. Selecting diverse features via spectral regu-  larization. In NIPS, 2012.  models. In NIPS, 2014.  J. Djolonga and A. Krause. From MAP to marginals: Variational inference in bayesian submodular  Shahar Dobzinski and Michael Schapira. An improved approximation algorithm for combinatorial  auctions with submodular bidders. In SODA, 2006.  Shahar Dobzinski and Jan Vondr\u00b4ak. The computational complexity of truthfulness in combinatorial  auctions. In EC, pages 405-422, 2012.  Shahar Dobzinski, Noam Nisan, and Michael Schapira. Approximation algorithms for combinato-  rial auctions with complement-free bidders. In STOC, pages 610-618, 2005.  Shahar Dobzinski, Ron Lavi, and Noam Nisan. Multi-unit auctions with budget limits. In FOCS,  easy. In STOC, 2011.  networks. In ICML, 2014a.  2008.  2014b.  Shahar Dobzinski, Hu Fu, and Robert D. Kleinberg. Optimal auctions with correlated bidders are  N. Du, Y. Liang, M. Balcan, and L. Song.  In\ufb02uence function learning in information diffusion  N. Du, Y. Liang, M. Balcan, and L. Song. Learning time-varying coverage functions. In NIPS,  Shaddin Dughmi, Tim Roughgarden, and Qiqi Yan. From convex optimization to randomized mech-  anisms: toward optimal combinatorial auctions. In STOC, pages 149-158, 2011.  Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted strong convexity implies weak submodularity. In NIPS Workshop on Learning in High Dimensions with Structure, 2016.  Clemens Elster and Arnold Neumaier. A grid algorithm for bound constrained optimization of noisy  functions. IMA Journal of Numerical Analysis, 15(4):585-608, 1995.  Uriel Feige. A threshold of ln n for approximating set cover. Journal of the ACM (JACM), 45(4):  634-652, 1998. SUBMODULAR OPTIMIZATION UNDER NOISE  Uriel Feige and Jan Vondrak. Approximation algorithms for allocation problems: Improving the  factor of 1-1/e. In FOCS, 2006.  Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information.  SIAM Journal on Computing, 23(5):1001-1018, 1994.  Uriel Feige, Vahab S. Mirrokni, and Jan Vondr\u00b4ak. Maximizing non-monotone submodular func-  tions. SIAM J. Comput., 40(4):1133-1153, 2011.  Vitaly Feldman. On the power of membership queries in agnostic learning. Journal of Machine  Learning Research, 10:163-182, 2009.  Vitaly Feldman and Pravesh Kothari. Colt. 2014.  tions by juntas. In FOCS, 2013.  and xos functions. In FOCS, 2015.  Vitaly Feldman and Jan Vondr\u00b4ak. Optimal bounds on approximation of submodular and XOS func-  Vitaly Feldman and Jan Vondr\u00b4ak. Tight bounds on low-degree spectral concentration of submodular  Vitaly Feldman, Pravesh Kothari, and Jan Vondr\u00b4ak. Representation, approximation and learning of  submodular functions using low-rank decision trees. In COLT, 2013.  Marshall L Fisher, George L Nemhauser, and Laurence A Wolsey. An analysis of approximations  for maximizing submodular set functionsII. Springer, 1978.  Torkel Glad and Allen Goldstein. Optimization of functions whose values are subject to small  errors. BIT Numerical Mathematics, 17(2):160-169, 1977.  Michel X Goemans, Nicholas JA Harvey, Satoru Iwata, and Vahab Mirrokni. Approximating sub-  modular functions everywhere. In SODA, 2009.  Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. Exact identification of circuits using  fixed points of amplification functions. In COLT, 1990.  D. Golovin and A. Krause. Adaptive submodularity: Theory and applications in active learning and  stochastic optimization. JAIR, 42:427-486, 2011.  D. Golovin, M. Faulkner, and A. Krause. Online distributed sensor selection. In IPSN, 2010.  R. Gomes and A. Krause. Budgeted nonparametric learning from data streams. In ICML, 2010.  Trevor J. Hastie, Robert John Tibshirani, and Jerome H. Friedman. The elements of statistical learning : data mining, inference, and prediction. Springer series in statistics. Springer, New York, 2009.  Thibaut Horel and Yaron Singer. Maximizing approximately submodular functions. In NIPS 2016.  Thibaut Horel, Stratis Ioannidis, and S. Muthukrishnan. Budget feasible mechanisms for experi-  mental design. In LATIN, 2014. HASSIDIM SINGER  2011a.  cuts. In CVPR, 2011b.  work. In KDD, 2003.  Jeffrey C. Jackson. An efficient membership-query algorithm for learning DNF with respect to the  uniform distribution. In FOCS, 1994.  Jeffrey C. Jackson, Eli Shamir, and Clara Shwartzman. Learning with queries corrupted by classi-  fication noise. Discrete Applied Mathematics, 1999.  S. Jegelka and J. Bilmes. Approximation bounds for inference using cooperative cuts. In ICML,  S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: Coupling edges in graph  D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of in\ufb02uence through a social net-  Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In STOC, 2007.  Subhash Khot, Richard J Lipton, Evangelos Markakis, and Aranyak Mehta.  Inapproximability  results for combinatorial auctions with submodular utility functions. In WINE. 2005.  Andr\u00b4e I Khuri and John A Cornell. Response surfaces: designs and analyses, volume 152. CRC  P. Kohli, A. Osokin, and S. Jegelka. A principled deep random field for image segmentation. In  A. Krause and C. Guestrin. Nonmyopic active learning of gaussian processes. an exploration-  exploitation approach. In ICML, 2007.  A. Krause and C. Guestrin. Submodularity and its applications in optimized information gathering.  ACM Trans. on Int. Systems and Technology, 2(4), 2011.  A. Krause and S. Jegelka. Submodularity in Machine Learning: New directions. Tutorial ICML,  press, 1996.  CVPR, 2013.  2013.  ICML, 2010.  Andreas Krause and Volkan Cevher. Submodular dictionary selection for sparse representation. In  Andreas Krause and Carlos Guestrin. A note on the budgeted maximization of submodular func-  tions. In Technical Report, 2005.  Andreas Krause, H. Brendan McMahan, Carlos Guestrin, and Anupam Gupta. Selecting observa-  tions against adversarial objectives. In NIPS, 2007.  Andreas Krause, Ajit Paul Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9:235-284, 2008.  Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani. Fast greedy algorithms  in mapreduce and streaming. In SPAA, 2013. SUBMODULAR OPTIMIZATION UNDER NOISE  Harold J Kushner and Dean S Clark. Stochastic Approximation Methods for Constrained and Un-  constrained Systems (Applied Mathematical Sciences, Vol. 26), volume 8. Springer, 1978.  Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. Non-monotone sub-  modular maximization under matroid and knapsack constraints. In STOC, 2009.  Benny Lehmann, Daniel Lehmann, and Noam Nisan. Combinatorial auctions with decreasing  marginal utilities. In ACM conference on electronic commerce, 2001.  J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance. Cost-effective  outbreak detection in networks. In KDD, 2007.  H. Lin and J. Bilmes. A class of submodular functions for document summarization. In ACL/HLT,  2011a.  2011b.  H. Lin and J. Bilmes. Optimal selection of limited vocabulary speech corpora. In Proc. Interspeech,  Brendan Lucier, Yaron Singer, Vasilis Syrgkanis, and \u00b4Eva Tardos. Equilibrium in combinatorial  public projects. In WINE, 2013.  Vahab S. Mirrokni, Michael Schapira, and Jan Vondr\u00b4ak. Tight information-theoretic lower bounds  for welfare maximization in combinatorial auctions. In EC, 2008.  George L Nemhauser and Leonard A Wolsey. Best algorithms for approximating the maximum of  a submodular set function. Mathematics of operations research, 3(3):177-188, 1978.  George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265-294, 1978a.  G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. An analysis of approximations for maximizing  submodular set functions\u2014I. Mathematical Programming, 14(1):265-294, 1978b.  C. H. Papadimitriou, M. Schapira, and Y. Singer. On the hardness of being truthful. In FOCS, 2008.  Christos H. Papadimitriou and George Pierrakos. On optimal single-item auctions. In STOC, 2011.  Boris T Polyak. Introduction to optimization. Optimization Software New York, 1987.  M. Gomez Rodriguez, J. Leskovec, and A. Krause. Inferring networks of diffusion and in\ufb02uence.  ACM TKDD, 5(4), 2011.  CoRR, abs/1608.02402, 2016.  2008.  Tim Roughgarden, Inbal Talgam-Cohen, and Jan Vondr\u00b4ak. When are welfare guarantees robust?  Michael Schapira and Yaron Singer. Inapproximability of combinatorial public projects. In WINE,  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to ISBN 1107057132,  Algorithms. Cambridge University Press, New York, NY, USA, 2014. 9781107057135. HASSIDIM SINGER  Eli Shamir and Clara Schwartzman. Learning by extended statistical queries and its relation to PAC In Computational Learning Theory: Eurocolt 95, pages 357-366. Springer-Verlag,  learning. 1995.  L. S. Shapley. Cores of convex games. International Journal of Game Theory, 1(1):11-26, 1971.  Yaron Singer and Jan Vondrak. Information-theoretic lower bounds for convex optimization with  erroneous oracles. In NIPS, pages 3204-3212, 2015.  Adish Singla, Sebastian Tschiatschek, and Andreas Krause. Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization. In AAAI, 2016.  M. Streeter, D. Golovin, and A. Krause. Online learning of assignments. In Advances in Neural  Information Processing Systems (NIPS), 2009.  Leslie G. Valiant. A Theory of the Learnable. Commun. ACM, 1984.  Jan Vondr\u00b4ak. Optimal approximation for the submodular welfare problem in the value oracle model.  In STOC, pages 67-74, 2008.  Comput., 42(1):265-304, 2013.  Jan Vondr\u00b4ak. Symmetry and approximability of submodular maximization problems. SIAM J.  Jan Vondr\u00b4ak, Chandra Chekuri, and Rico Zenklusen. Submodular function maximization via the  multilinear relaxation and contention resolution schemes. In STOC \u201911, 2011.  Larry Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer Publishing  Company, Incorporated, 2010. ISBN 1441923225, 9781441923226. SUBMODULAR OPTIMIZATION UNDER NOISE  "}, "Surprising properties of dropout in deep networks": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Surprising properties of dropout in deep networks", "abstract": "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also  show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.", "pdf_url": "http://proceedings.mlr.press/v65/helmbold17a/helmbold17a.pdf", "keywords": ["Dropout", "deep neural networks", "regularization", "learning theory"], "reference": "P. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. NIPS, 2014.  P. Baldi and P. Sadowski. The dropout learning algorithm. Artificial intelligence, 210:78-122, 2014.  Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information  Processing Systems, pages 2814-2822, 2013.  P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal  of the American Statistical Association, 101(473):138-156, 2006.  L. Breiman. Some infinity theory for predictor ensembles. Annals of Statistics, 32(1):1-11, 2004.  18   HELMBOLD LONG  2013; Gal and Ghahramani, 2015; Wager et al., 2014)), including the possibility that dropout reduces the amount of coadaptation in a network\u2019s weights (Hinton et al., 2012).  The dropout criterion is an expected loss over dropout patterns, and the variance in the output values over dropout patterns contributes to this expected loss. Therefore dropout may co-adapt weights in order to reduce this (artificial) variance. We prove that this happens even in very simple situations where nothing in the training data justifies negative weights (Theorem 8). This indicates that the relationship between dropout and co-adaption is not a simple one.  The effects of dropout in deep neural networks are rather complicated, and approximations can be misleading since the dropout penalty is very non-convex even in 1-layer networks (Helmbold and Long, 2015). In Section 3 we show that dropout does enjoy several scale-invariance properties that are not shared by weight-decay. A perhaps surprising consequence of these invariances is that there are never isolated local minima when learning a deep network with dropout. Further exploration of these scale invariance properties is warranted to see if they are a contributor to dropout\u2019s empirical success or can be exploited to facilitate training. While contrasting dropout to weight-decay in simple situations, we found that a degenerate all-zero network results (Theorem 7) when the L2 regularization parameter is above a threshold. This is in dramatic contrast to our previous intuition from the 1-layer case.  In (Wager et al., 2013), dropout was viewed as a regularization method, adding a data dependent penalty to the empirical loss of (presumably) undesirable solutions. Section 5 shows that, unlike the generalized linear models case analyzed there, the dropout penalty in deeper networks can be negative and depends on the labels in the training data, and thus behaves unlike most regularizers. On the other hand, the dropout penalty can grow exponentially in the depth of the network, and thus may better re\ufb02ect the complexity of the underlying model space than L2 regularization.  This paper uncovers a number of dropout\u2019s interesting fundamental properties using formal analysis of simple cases. However, the effects of using dropout training in deep networks are subtle and complex, and we hope that this paper lays a foundation to promote further formal analysis of dropout\u2019s properties and behavior.  We are very grateful to Peter Bartlett, Seshadhri Comandur, and anonymous reviewers for valuable communications.  Acknowledgments  References  P. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. NIPS, 2014.  P. Baldi and P. Sadowski. The dropout learning algorithm. Artificial intelligence, 210:78-122, 2014.  Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information  Processing Systems, pages 2814-2822, 2013.  P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal  of the American Statistical Association, 101(473):138-156, 2006.  L. Breiman. Some infinity theory for predictor ensembles. Annals of Statistics, 32(1):1-11, 2004. DROPOUT IN DEEP NETWORKS  Caffe. Caffe, 2016. http://caffe.berkeleyvision.edu.  Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural  networks. In EMNLP, pages 740-750, 2014.  G. E. Dahl. Deep learning how I did it: Merck 1st place interview, 2012. http://blog.kaggle.com.  G. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neural networks for LVCSR using  rectified linear units and dropout. ICASSP, 2013.  L. Deng, J. Li, J. Huang, K. Yao, D. Yu, F. Seide, M. L. Seltzer, G. Zweig, X. He, J. Williams, Y. Gong, and A. Acero. Recent advances in deep learning for speech research at microsoft. ICASSP, 2013.  Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model  uncertainty in deep learning. arXiv:1506.02142, 2015.  Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1019-1027, 2016.  Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron C. Courville, and Yoshua Bengio.  Maxout networks. In ICML, pages 1319-1327, 2013.  Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron Courville, Yoshua Bengio, Chris Pal, Pierre-Marc Jodoin, and Hugo Larochelle. Brain tumor segmentation with deep neural networks. Medical image analysis, 35:18-31, 2017.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing  human-level performance on imagenet classification. In ICCV, pages 1026-1034, 2015.  D. P. Helmbold and P. M. Long. On the inductive bias of dropout. JMLR, 16:3403-3454, 2015.  G. E. Hinton. Dropout: a simple and effective way to improve neural networks, 2012. videolec-  tures.net.  G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors, 2012. Arxiv, arXiv:1207.0580v1.  Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for  modelling sentences. In ACL, pages 655-665, 2014.  P. M. Long and R. A. Servedio. Random classification noise defeats all convex potential boosters.  Machine Learning, 78(3):287-304, 2010.  Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In  ICML, pages 807-814, 2010.  Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural  networks. In COLT, pages 1376-1401, 2015.  J\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117,  2015. HELMBOLD LONG  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way  to prevent neural networks from overfitting. JMLR, 15:1929-1958, 2014.  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CVPR, 2015.  TensorFlow. Tensor\ufb02ow, 2016. https://www.tensor\ufb02ow.org.  Torch. Torch, 2016. http://torch.ch.  COLT, pages 949-974, 2014.  T. Van Erven, W. Kot\u0142owski, and M. K. Warmuth. Follow the leader with dropout perturbations.  S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. NIPS, 2013.  S. Wager, W. Fithian, S. Wang, and P. S. Liang. Altitude training: Strong bounds for single-layer  dropout. NIPS, 2014.  L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus. Regularization of neural networks using  dropconnect. In ICML, pages 1058-1066, 2013.  Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21-29, 2016.  T. Zhang. Statistical behavior and consistency of classification methods based on convex risk  minimization. Annals of Statistics, 32(1):56-85, 2004. DROPOUT IN DEEP NETWORKS  "}, "Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC Classes": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC Classes", "abstract": "In this work we study the quantitative relation between the recursive teaching dimension (RTD) and the VC dimension (VCD) of concept classes of finite sizes. The RTD of a concept class $\\mathcal C\u2286{0,1}^n$ , introduced by Zilles et al. (2011), is a combinatorial complexity measure characterized by the worst-case number of examples necessary to identify a concept in $\\mathcal C$ according to the recursive teaching model. For any finite concept class $\\mathcal C\u2286{0,1}^n$ with $\\mathrm{VCD}(\\mathcal C) = d$, Simon and Zilles (2015) posed an open problem $\\mathrm{RTD}(\\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD? Previously, the best known result is an exponential upper bound $\\mathrm{RTD}(\\mathcal C) = O(d\\cdot2^d)$, due to Chen et al. (2016). In this paper, we show a quadratic upper bound: $\\mathrm{RTD}(\\mathcal C) = O(d^2)$, much closer to an answer to the open problem. We also discuss the challenges in fully solving the problem.", "pdf_url": "http://proceedings.mlr.press/v65/hu17a/hu17a.pdf", "keywords": ["Recursive teaching dimension", "VC dimension", "Recursive teaching model"], "reference": "pages 2164-2171, 2016.  235-242, 2005.  In NIPS, 2016.  Dana Angluin. Queries revisited. Theoretical Computer Science, 313(2):175-194, 2004.  Martin Anthony, Graham Brightwell, and John Shawe-Taylor. On specifying Boolean functions by  labelled examples. Discrete Applied Mathematics, 61(1):1-25, 1995.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Xi Chen, Yu Cheng, and Bo Tang. On the recursive teaching dimension of VC classes. In NIPS,  Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, volume 18, pages  Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning via the lens of compression.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimen- sion, VC-dimension and sample compression. Journal of Machine Learning Research, 15(1): 3107-3131, 2014.  Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of Computer and  System Sciences, 50(1):20-31, 1995.  Sally A Goldman and H David Mathias. Teaching a smart learner.  In Proceedings of the sixth  annual conference on Computational learning theory, pages 67-76. ACM, 1993.  Steve Hanneke. Teaching dimension and the complexity of active learning. In International Con-  ference on Computational Learning Theory, pages 66-81. Springer, 2007.  Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Proceed- ings of the eighth annual conference on Computational learning theory, pages 108-117. ACM, 1995.  9   QUADRATIC UPPER BOUND FOR RECURSIVE TEACHING DIMENSION OF FINITE VC CLASSES  Using a similar argument as in the proof of Lemma 7 and optimizing the parameters with respect to the specific case of (3, 5)-classes (choosing x = 2, y = 3, z = 5, k = 1), we can show  max C\u2208(3,5)  RTD(C) = 2;  And hence for (3, 4)-classes maxC\u2208(3,4) RTD(C) = 2.  The relationship between RTD and VCD is intriguing. Analyzing special cases and sub-classes  with specific structure may provide insights for finally solving the open problem.  Liwei Wang was partially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502) and NSFC (61573026).  Acknowledgments  References  pages 2164-2171, 2016.  235-242, 2005.  In NIPS, 2016.  Dana Angluin. Queries revisited. Theoretical Computer Science, 313(2):175-194, 2004.  Martin Anthony, Graham Brightwell, and John Shawe-Taylor. On specifying Boolean functions by  labelled examples. Discrete Applied Mathematics, 61(1):1-25, 1995.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and  the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989.  Xi Chen, Yu Cheng, and Bo Tang. On the recursive teaching dimension of VC classes. In NIPS,  Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, volume 18, pages  Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning via the lens of compression.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimen- sion, VC-dimension and sample compression. Journal of Machine Learning Research, 15(1): 3107-3131, 2014.  Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of Computer and  System Sciences, 50(1):20-31, 1995.  Sally A Goldman and H David Mathias. Teaching a smart learner.  In Proceedings of the sixth  annual conference on Computational learning theory, pages 67-76. ACM, 1993.  Steve Hanneke. Teaching dimension and the complexity of active learning. In International Con-  ference on Computational Learning Theory, pages 66-81. Springer, 2007.  Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Proceed- ings of the eighth annual conference on Computational learning theory, pages 108-117. ACM, 1995. HU WU LI WANG  Christian Kuhlmann. On teaching and learning intersection-closed concept classes. In European  Conference on Computational Learning Theory, pages 168-182. Springer, 1999.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical  report, Technical report, University of California, Santa Cruz, 1986.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff. Compressing and teaching for low VC-dimension. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 40-51. IEEE, 2015.  Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A, 13  (1):145-147, 1972.  Saharon Shelah. A combinatorial problem; stability and order for models and theories in infinitary  languages. Pacific Journal of Mathematics, 41(1):247-261, 1972.  Ayumi Shinohara and Satoru Miyano. Teachability in computational learning. New Generation  Computing, 8(4):337-347, 1991.  Hans Ulrich Simon and Sandra Zilles. Open problem: Recursive teaching dimension versus VC  dimension. In COLT, pages 1770-1772, 2015.  VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events  to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971.  Manfred K Warmuth. Compressing to VC dimension many points.  In COLT, volume 3, pages  743-744. Springer, 2003.  Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching  and learning. Journal of Machine Learning Research, 12(Feb):349-384, 2011. "}, "A Unified Analysis of Stochastic Optimization Methods Using Jump System Theory and Quadratic Constraints": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "A Unified Analysis of Stochastic Optimization Methods Using Jump System Theory and Quadratic Constraints", "abstract": "We develop a simple routine unifying the analysis of several important recently-developed stochastic optimization methods including SAGA, Finito, and stochastic dual coordinate ascent (SDCA). First, we show an intrinsic connection between stochastic optimization methods and dynamic jump systems, and propose a general jump system model for stochastic optimization methods. Our proposed model recovers SAGA, SDCA, Finito, and SAG as special cases. Then we combine jump system theory with several simple quadratic inequalities to derive sufficient conditions for convergence rate certifications of the proposed jump system model under various assumptions (with or without individual convexity, etc). The derived conditions are linear matrix inequalities (LMIs) whose size roughly scale with the size of the training set. We make use of the symmetry in the stochastic optimization methods and reduce these LMIs to some equivalent small LMIs whose sizes are at most 3 by 3. We solve these small LMIs to provide analytical proofs of new convergence rates for SAGA, Finito and SDCA (with or without individual convexity). We also explain why our proposed LMI fails in analyzing SAG. We reveal a key difference between SAG and other methods, and briefly discuss how to extend our LMI analysis for SAG. An advantage of our approach is that the proposed analysis can be automated for a large class of stochastic methods under various assumptions (with or without individual convexity, etc).", "pdf_url": "http://proceedings.mlr.press/v65/hu17b/hu17b.pdf", "keywords": ["Empirical risk minimization", "SAGA", "Finito", "SDCA", "SAG", "semidefinite programming", "jump systems", "quadratic constraints", "control theory"], "reference": "Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In Proceedings of The 33rd International Conference on Machine Learning, pages 699-707, 2016.  S. Bittanti and P. Colaneri. Periodic systems: filtering and control. Springer Science & Business  Media, 2008.  systems, pages 217-224, 2003.  L. Bottou and Y. LeCun. Large scale online learning. In Advances in neural information processing  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Ma- chine Learning, 3(1):1-122, 2011.  C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent. Mathematical Programming, 155(1-2): 57-79, 2016.  O. Costa, M. Fragoso, and R. Marques. Discrete-time Markov jump linear systems. Springer Science  & Business Media, 2006.  Inc. CVX Research. CVX: Matlab software for disciplined convex programming, version 2.0.  http://cvxr.com/cvx, August 2012.  A. Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Informa-  tion Processing Systems, pages 676-684, 2016.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014a.  A. Defazio, J. Domke, and T. Caetano. Finito: A faster, permutable incremental gradient method for big data problems. In Proceedings of the 31st International Conference on Machine Learning, pages 1125-1133, 2014b.  V. Dragan, T. Morozan, and A. Stoica. Mathematical methods in robust control of discrete-time  linear stochastic systems. Springer, 2010.  Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: a  novel approach. Mathematical Programming, 145(1-2):451-482, 2014.  M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In Recent Ad- vances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95- 110. Springer-Verlag Limited, 2008.  J. Hespanha. Linear systems theory. Princeton university press, 2009.  M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers.  arXiv preprint arXiv:1208.3922, 2012.  25   ANALYSIS OF STOCHASTIC OPTIMIZATION METHODS USING JUMP SYSTEM THEORY  References  Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization.  In Proceedings of The 33rd International Conference on Machine Learning, pages 699-707, 2016.  S. Bittanti and P. Colaneri. Periodic systems: filtering and control. Springer Science & Business  Media, 2008.  systems, pages 217-224, 2003.  L. Bottou and Y. LeCun. Large scale online learning. In Advances in neural information processing  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Ma- chine Learning, 3(1):1-122, 2011.  C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent. Mathematical Programming, 155(1-2): 57-79, 2016.  O. Costa, M. Fragoso, and R. Marques. Discrete-time Markov jump linear systems. Springer Science  & Business Media, 2006.  Inc. CVX Research. CVX: Matlab software for disciplined convex programming, version 2.0.  http://cvxr.com/cvx, August 2012.  A. Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Informa-  tion Processing Systems, pages 676-684, 2016.  A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014a.  A. Defazio, J. Domke, and T. Caetano. Finito: A faster, permutable incremental gradient method for big data problems. In Proceedings of the 31st International Conference on Machine Learning, pages 1125-1133, 2014b.  V. Dragan, T. Morozan, and A. Stoica. Mathematical methods in robust control of discrete-time  linear stochastic systems. Springer, 2010.  Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization: a  novel approach. Mathematical Programming, 145(1-2):451-482, 2014.  M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In Recent Ad- vances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95- 110. Springer-Verlag Limited, 2008.  J. Hespanha. Linear systems theory. Princeton university press, 2009.  M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers.  arXiv preprint arXiv:1208.3922, 2012. HU SEILER RANTZER  B. Hu. A Robust Control Perspective on Optimization of Strongly-Convex Functions. PhD thesis,  University of Minnesota, 2016.  B. Hu and L. Lessard. Dissipativity theory for Nesterov\u2019s accelerated method. In Proceedings of  the 34th International Conference on Machine Learning, 2017.  B. Hu and P. Seiler. Exponential decay rate conditions for uncertain linear systems using integral  quadratic constraints. IEEE Transactions on Automatic Control, 61(11):3561-3567, 2016.  R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-  tion. In Advances in Neural Information Processing Systems, pages 315-323, 2013.  C. Kao. On stability of discrete-time LTI systems with varying time delays. IEEE Transactions on  Automatic Control, 57:1243-1248, 2012.  C. Kao and A. Rantzer. Stability analysis of systems with uncertain time-varying delays. Automat-  ica, 43(6):959-970, 2007.  40:1429-1434, 2004.  C.Y. Kao and B. Lincoln. Simple stability criteria for systems with time-varying delays. Automatica,  H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient meth- ods under the Polyak-Lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795-811, 2016.  D. Kim and J. Fessler. Optimized first-order methods for smooth convex minimization. Mathemat-  ical programming, 159(1-2):81-107, 2016.  L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral  quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.  H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances  in Neural Information Processing Systems, pages 3384-3392, 2015.  A. Megretski and A. Rantzer. System analysis via integral quadratic constraints. IEEE Transactions  on Automatic Control, 42:819-830, 1997.  R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan. A general analysis of the conver- gence of ADMM. In Proceedings of the 32nd International Conference on Machine Learning, pages 343-352, 2015.  A. Nitanda. Stochastic proximal gradient descent with acceleration techniques.  In Advances in  Neural Information Processing Systems, pages 1574-1582, 2014.  H. Pfifer and P. Seiler. Integral quadratic constraints for delayed nonlinear and parameter-varying  systems. Automatica, 56:36 - 43, 2015.  B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693-701, 2011. ANALYSIS OF STOCHASTIC OPTIMIZATION METHODS USING JUMP SYSTEM THEORY  S. Reddi, A. Hefny, S. Sra, B. P\u00b4ocz\u00b4os, and A. Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of The 33rd International Conference on Machine Learning, pages 314-323, 2016a.  S. Reddi, S. Sra, B. P\u00b4ocz\u00b4os, and A. Smola. Fast incremental method for nonconvex optimization.  In IEEE Conf. on Decision and Control, pages 1971-1977, 2016b.  H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-  tics, 22(3):400-407, 1951.  N. Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. In Advances in Neural Information Processing Systems, 2012.  M. Schmidt, N. Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.  ArXiv preprint, 2013.  S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss.  The Journal of Machine Learning Research, 14(1):567-599, 2013.  S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-  larized loss minimization. Mathematical Programming, 155:105-145, 2016.  Shai Shalev-Shwartz. SDCA without duality. arXiv preprint arXiv:1502.06177, 2015.  Shai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In Proceed-  ings of the 33rd International Conference on Machine Learning, pages 747-754, 2016.  R. Sun, Z. Luo, and Y. Ye. On the expected convergence of randomly permuted ADMM. arXiv  preprint arXiv:1503.06387, 2015.  A. Taylor, J. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case  performance of first-order methods. Mathematical Programming, 161(1-2):307-345, 2017.  C. Teo, A. Smola, S. Vishwanathan, and Q. Le. A scalable modular convex solver for regular- ized risk minimization. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 727-736, 2007.  K.C. Toh, M.J. Todd, and R.H. Tutuncu. SDPT3 - a matlab software package for semidefinite  programming. Optimization Methods and Software, 11:545-581, 1999.  R.H Tutuncu, K.C. Toh, and M.J. Todd. Solving semidefinite-quadratic-linear programs using  SDPT3. Mathematical Programming Ser. B, 95:189-217, 2003.  R. Zhang and J. Kwok. Asynchronous distributed ADMM for consensus optimization. In Proceed-  ings of the 31st International Conference on Machine Learning, pages 1701-1709, 2014. HU SEILER RANTZER  "}, "The Hidden Hubs Problem": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "The Hidden Hubs Problem", "abstract": "We introduce the following \\em hidden hubs model $H(n,k,\\sigma_0, \\sigma_1)$: the input is an $n \\times n$ random matrix $A$ with a subset $S$ of $k$ special rows (hubs); entries in rows outside $S$ are generated from the Gaussian distribution $p_0 = N(0,\\sigma_0^2)$, while for each row in $S$, an unknown subset of $k$ of its entries are generated from $p_1 = N(0,\\sigma_1^2)$, $\\sigma_1>\\sigma_0$, and the rest of the entries from $p_0$. The special rows with higher variance entries can be viewed as hidden higher-degree hubs. The problem we address is to identify the hubs efficiently. The planted Gaussian Submatrix Model is the special case where the higher variance entries must all lie in a $k \\times k$ submatrix. If $k\u2265c\\sqrt{n}\\ln n$, just the row sums are sufficient to find $S$ in the general model. For the Gaussian submatrix problem (and the related planted clique problem), this can be improved by a $\\sqrt\\ln n$ factor to $k \\ge c\\sqrt{n}$ by spectral or combinatorial methods. We give a polynomial-time algorithm to identify all the hidden hubs with high probability for $k \\ge n^0.5-\u03b4$ for some $\u03b4>0$, when $\\sigma_1^2>2\\sigma_0^2$.  The algorithm extends to the setting where planted entries might have different variances, each at least $\\sigma_1^2$. We also show a nearly matching lower bound: for $\\sigma_1^2 \\le 2\\sigma_0^2$, there is no polynomial-time Statistical Query algorithm for distinguishing between a matrix whose entries are all from $N(0,\\sigma_0^2)$ and a matrix with $k=n^0.5-\u03b4$ hidden hubs for any $\u03b4>0$. The lower bound as well as the algorithm are related to whether the chi-squared distance of the two distributions diverges. At the critical value $\\sigma_1^2=2\\sigma_0^2$, we show that the hidden hubs problem can be solved for $k\u2265c\\sqrt n(\\ln n)^1/4$, improving on the naive row sum-based method.", "pdf_url": "http://proceedings.mlr.press/v65/kannan17a/kannan17a.pdf", "keywords": ["Planted Clique", "Hidden Gaussian", "Spectral Methods", "Concentration", "Statistical Queries"], "reference": "N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13:457-466, 1998.  Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. CoRR, abs/1604.03084, 2016. URL http://arxiv.org/abs/1604.03084.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an O(n1/4) approximation for densest k-subgraph. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 201-210, 2010. doi: 10.1145/1806689.1806718. URL http:// doi.acm.org/10.1145/1806689.1806718.  Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253-262, 1994.  R. Boppana. Eigenvalues and graph bisection: An average-case analysis. Proceedings of the 28th  IEEE Symposium on Foundations of Computer Science, pages 280-285, 1987.  S. Charles Brubaker and Santosh Vempala. Random tensors and planted cliques. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 12th International Workshop, APPROX 2009, and 13th International Workshop, RANDOM 2009, Berkeley, CA, USA, August 21-23, 2009. Proceedings, pages 406-419, 2009. doi: 10.1007/978-3-642-03685-9 31. URL http://dx.doi.org/10.1007/978-3-642-03685-9_31.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high probability. In Proceedings of the Meeting on Analytic Algorithmics and Combinatorics, ANALCO \u201911, pages 67-75, Philadelphia, PA, USA, 2011. Society for Industrial and Applied Mathematics. URL http://dl.acm.org/citation.cfm?id=2790409.2790417.  22   KANNAN VEMPALA  SDA(P, \u00af\u03b3) \u2265 n\u03b4(cid:96).  and for any \u03b4 > 0, with k = n0.5\u2212\u03b4, (cid:96) = 8\u03b4/C\u03b1, we have \u00af\u03b3 = 2n\u2212\u03b4\u22121 and  Hence any statistical algorithm that solves P with probability at least 3/4 needs n\u03c9(1) calls to VSTAT(n1+\u03b4).  Corollary 26 For \u03c31 = \u03c30,  k2 n2 exp \u221a and for any \u03b4 > 0, with k = n0.5\u2212\u03b4, \u00b52 = c\u03c32 ln(  \u00af\u03b3 = 2  (cid:18) \u00b52(cid:96) \u03c32  (cid:19)  .  n/k), we have \u00af\u03b3 = 2nc\u03b4(cid:96)\u22122\u03b4\u22121 and  SDA(P, \u00af\u03b3) = \u2126(n2\u03b4(cid:96)).  n/k)), any statistical algorithm that solves P with probability at least 3/4 needs  \u221a  If \u00b52 = o(\u03c32 ln( n\u03c9(1) calls to VSTAT(n1+\u03b4).  References  N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13:457-466, 1998.  Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. CoRR, abs/1604.03084, 2016. URL http://arxiv.org/abs/1604.03084.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an O(n1/4) approximation for densest k-subgraph. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 201-210, 2010. doi: 10.1145/1806689.1806718. URL http:// doi.acm.org/10.1145/1806689.1806718.  Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253-262, 1994.  R. Boppana. Eigenvalues and graph bisection: An average-case analysis. Proceedings of the 28th  IEEE Symposium on Foundations of Computer Science, pages 280-285, 1987.  S. Charles Brubaker and Santosh Vempala. Random tensors and planted cliques. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 12th International Workshop, APPROX 2009, and 13th International Workshop, RANDOM 2009, Berkeley, CA, USA, August 21-23, 2009. Proceedings, pages 406-419, 2009. doi: 10.1007/978-3-642-03685-9 31. URL http://dx.doi.org/10.1007/978-3-642-03685-9_31.  Yael Dekel, Ori Gurel-Gurevich, and Yuval Peres. Finding hidden cliques in linear time with high probability. In Proceedings of the Meeting on Analytic Algorithmics and Combinatorics, ANALCO \u201911, pages 67-75, Philadelphia, PA, USA, 2011. Society for Industrial and Applied Mathematics. URL http://dl.acm.org/citation.cfm?id=2790409.2790417. HIDDEN HUBS  Yash Deshpande and Andrea Montanari. Improved sum-of-squares lower bounds for hidden clique and hidden submatrix problems. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 523-562, 2015a. URL http://jmlr.org/ proceedings/papers/v40/Deshpande15.html.  Yash Deshpande and Andrea Montanari. Finding hidden cliques of size (cid:112)N/e in nearly linear time. Found. Comput. Math., 15(4):1069-1128, August 2015b. ISSN 1615-3375. doi: 10.1007/ s10208-014-9215-y. URL http://dx.doi.org/10.1007/s10208-014-9215-y.  U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph.  Random Structures and Algorithms, 16(2):195-208, 2000.  Uriel Feige and Dorit Ron. Finding hidden cliques in linear time. In 21st International Meeting on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of Algorithms (AofA\u201910), pages 189-204. Discrete Mathematics and Theoretical Computer Science, 2010.  Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, and Ying Xiao. Statistical algo- rithms and a lower bound for planted clique. In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 655-664. ACM, 2013a.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability problems with planted solutions. CoRR, abs/1311.4821, 2013b. Extended abstract in STOC 2015.  Vitaly Feldman, Cristobal Guzman, and Santosh Vempala. Statistical query algorithms for mean In SIAM Symposium on Discrete Algorithms,  estimation and stochastic convex optimization. 2017. URL http://arxiv.org/abs/1512.09170.  Delphine F\u00b4eral and Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of rank one deformation of large wigner matrices. Communications in Mathematical Physics, 272(1):185-228, 2007. ISSN 1432-0916. doi: 10.1007/s00220-007-0209-3. URL http://dx.doi.org/10.1007/ s00220-007-0209-3.  Alan M. Frieze and Ravi Kannan. A new approach to the planted clique problem. In IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science, FSTTCS 2008, December 9-11, 2008, Bangalore, India, pages 187-198, 2008. doi: 10.4230/LIPIcs. FSTTCS.2008.1752. URL http://dx.doi.org/10.4230/LIPIcs.FSTTCS.2008. 1752.  Samuel B. Hopkins, Pravesh Kothari, Aaron Henry Potechin, Prasad Raghavendra, and Tselil Schramm. On the integrality gap of degree-4 sum of squares for planted clique. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, Arling- ton, VA, USA, January 10-12, 2016, pages 1079-1095, 2016. doi: 10.1137/1.9781611974331. ch76. URL http://dx.doi.org/10.1137/1.9781611974331.ch76.  M. Jerrum. Large cliques elude the metropolis process. Random Structures and Algorithms, 3(4):  347-360, 1992. KANNAN VEMPALA  Ravindran Kannan. A new probability inequality using typical moments and concentration results. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA, pages 211-220, 2009. doi: 10.1109/FOCS.2009.20. URL http://dx.doi.org/10.1109/FOCS.2009.20.  Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45  (6):983-1006, 1998.  Michael J. Kearns. Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, May 16-18, 1993, San Diego, CA, USA, pages 392-401, 1993. doi: 10.1145/167088.167200. URL http://doi.acm. org/10.1145/167088.167200.  L. Kucera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics,  57:193-212, 1995.  Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. Ann. Statist., 43(3):1089-1116, 06 2015. doi: 10.1214/14-AOS1300. URL http://dx.doi.org/ 10.1214/14-AOS1300.  Raghu Meka, Aaron Potechin, and Avi Wigderson. Sum-of-squares lower bounds for planted clique. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 87-96, 2015. doi: 10.1145/2746539.2746600. URL http://doi.acm.org/10.1145/2746539.2746600.  Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spectral methods: In From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 217-225. Curran Associates, Inc., 2015. "}, "Predicting with Distributions": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Predicting with Distributions", "abstract": "We consider a new learning model in which a joint distribution over vector pairs $(x,y)$ is determined by an unknown function $c(x)$ that maps input vectors $x$ not to individual outputs, but to entire \\em distributions\\/ over output vectors $y$.  Our main results take the form of rather general reductions from our model to algorithms for PAC learning the function class and the distribution class separately, and show that virtually every such combination yields an efficient algorithm in our model.  Our methods include a randomized reduction to classification noise and an application of Le Cam\u2019s method to obtain robust learning algorithms.", "pdf_url": "http://proceedings.mlr.press/v65/kearns17a/kearns17a.pdf", "keywords": ["PAC learning", "distribution learning", "learning with classification noise"], "reference": "Dana Angluin and Philip D. Laird. Learning from noisy examples. Machine Learning, 2 (4):343\u2013370, 1987. doi: 10.1007/BF00116829. URL http://dx.doi.org/10.1007/ BF00116829.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians.  In Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing, STOC \u201901, pages 247\u2013257, New York, NY, USA, 2001. ACM. ISBN 1-58113-349-9. doi: 10.1145/380752.380808. URL http://doi.acm.org/10.1145/380752.380808.  Avrim Blum and Tom M. Mitchell. Combining labeled and unlabeled data with co-training.  In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, Madison, Wisconsin, USA, July 24-26, 1998., pages 92\u2013100, 1998. doi: 10.1145/279943.279962. URL http://doi.acm.org/10.1145/279943.279962.  Sanjoy Dasgupta. Learning mixtures of gaussians.  In 40th Annual Symposium on Foundations of Computer Science, FOCS \u201999, 17-18 October, 1999, New York, NY, USA, pages 634\u2013644, 1999. doi: 10.1109/SFFCS.1999.814639. URL http://dx.doi.org/10.1109/SFFCS. 1999.814639.  14   KEARNS WU  Lemma 22 Suppose the class P satisfies the parametric mixture learning assumption (Assump- tion 18), the class C is CN learnable, and mixture distribution over Y is \u03b3-healthy for some \u03b3 > 0. Then there exists an algorithm L that given \u03b5, \u03b4 and \u03b3 as inputs and sample access from Gen, halts in time bounded by poly(1/\u03b5, 1/\u03b4, 1/\u03b3, n, k), and with probability at least 1 \u2212 \u03b4, outputs a list of probability models T that contains some \u02c6T with err( \u02c6T ) \u2264 \u03b5.  Finally, to wrap up and prove Theorem 19, we also need to handle the case where healthy mixture condition in Theorem 16 does not hold. We will again appeal to the robust distribution learner in Lemma 17 to learn the distributions directly, and construct hypothesis models based on the output distributions. To guarantee that the output hypothesis model is accurate, we will again use the maximum likelihood method to select the model with the minimum empirical log-loss (formal proof deferred to the "}, "Bandits with Movement Costs and Adaptive Pricing": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Bandits with Movement Costs and Adaptive Pricing", "abstract": "We extend the model of Multi-Armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\\widetilde{O}(\\sqrt{k}T + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\\widetilde{\u0398}(T^2/3)$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\\widetilde{O}(T^2/3)$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\\widetilde{O}(T^3/4)$ for the problem.", "pdf_url": "http://proceedings.mlr.press/v65/koren17a/koren17a.pdf", "keywords": [], "reference": "Yasin Abbasi, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari. On- line learning in markov decision processes with adversarially chosen transition probability distributions. In Advances in Neural Information Processing Systems, pages 2508-2516, 2013.  Noga Alon, Nicol`o Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Proceedings of The 28th Conference on Learning Theory, pages 23-35, 2015.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic  bandits. In COLT, pages 217-226, 2009.  P. Auer, R. Ortner, and C. Szepesv\u00b4ari. Improved rates for the stochastic continuum-armed bandit problem. Proceedings of the 20th Annual Conference on Learning Theory, pages 454-468, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for item pricing. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages 29-35. ACM, 2006.  Maria-Florina Balcan and Florin Constantin. Sequential item pricing for unlimited supply. In International Workshop on Internet and Network Economics, pages 50-62. Springer Berlin Heidelberg, 2010.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Item pricing for revenue max- In Proceedings of the 9th ACM conference on Electronic commerce, pages  imization. 50-59. ACM, 2008.  25   Bandits with Movement Costs and Adaptive Pricing  By Lemma 13 we now obtain  btp\u03c1\u02da, . . . , \u03c1\u02daq \u00b4 E  btp\u03c1t, . . . , \u03c1t`\u03c4 q  \u201c O  \u03c4 kT `  \u201c Op\u03c4 1{3T 2{3q ,  \u00ab  T\u00ff  t\u201c1  \ufb00  \u02c6a  \u02d9  \u03c4 T k  `  T k  T\u00ff  t\u201c1  and using our choice of k gives the result.  Acknowledgments  RL is supported by funding from the Eric and Wendy Schmidt Fund for Strategic Innovation. YM is supported in part by a grant from the Israel Science Foundation, a grant from the United States-Israel Binational Science Foundation (BSF), and the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).  References  Yasin Abbasi, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari. On- line learning in markov decision processes with adversarially chosen transition probability distributions. In Advances in Neural Information Processing Systems, pages 2508-2516, 2013.  Noga Alon, Nicol`o Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Proceedings of The 28th Conference on Learning Theory, pages 23-35, 2015.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic  bandits. In COLT, pages 217-226, 2009.  P. Auer, R. Ortner, and C. Szepesv\u00b4ari. Improved rates for the stochastic continuum-armed bandit problem. Proceedings of the 20th Annual Conference on Learning Theory, pages 454-468, 2007.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for item pricing. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages 29-35. ACM, 2006.  Maria-Florina Balcan and Florin Constantin. Sequential item pricing for unlimited supply. In International Workshop on Internet and Network Economics, pages 50-62. Springer Berlin Heidelberg, 2010.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Item pricing for revenue max- In Proceedings of the 9th ACM conference on Electronic commerce, pages  imization. 50-59. ACM, 2008. Koren Livni Mansour  Nikhil Bansal, Ning Chen, Neva Cherniavsky, Atri Rurda, Baruch Schieber, and Maxim Sviridenko. Dynamic pricing for impatient bidders. ACM Transactions on Algorithms (TALG), 6(2):35, 2010.  Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In 37th Annual Symposium on Foundations of Computer Science, FOCS \u201996, Burlington, Vermont, USA, 14-16 October, 1996, pages 184-193, 1996.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms. Operations Research, 57(6):1407-1420, 2009.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X -armed bandits. Journal of Machine  Learning Research, 12:1587-1627, 2011.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex  optimization. arXiv preprint arXiv:1607.03084, 2016.  Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for  prediction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  E.W. Cope. Regret and convergence bounds for a class of continuum-armed bandit prob-  lems. IEEE Transactions on Automatic Control, 54(6):1243-1253, 2009.  Varsha Dani, Sham M Kakade, and Thomas P Hayes. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems, pages 345-352, 2007.  Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: T 2/3 regret. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 459-467, 2014a.  Ofer Dekel, Elad Hazan, and Tomer Koren. The blinded bandit: Learning with adap- tive feedback. In Advances in Neural Information Processing Systems, pages 1610-1618, 2014b.  Michal Feldman, Tomer Koren, Roi Livni, Yishay Mansour, and Aviv Zohar. Online pric- In Annual Conference on Neural Information  ing with strategic and patient buyers. Processing Systems, 2016.  Sudipto Guha and Kamesh Munagala. Multi-armed bandits with metric switching costs. In International Colloquium on Automata, Languages, and Programming, pages 496-507. Springer, 2009.  Robert Kleinberg and Aleksandrs Slivkins. Sharp dichotomies for regret minimization in metric spaces. In Proceedings of the twenty-first annual ACM-SIAM symposium on Dis- crete Algorithms, pages 827-846. Society for Industrial and Applied Mathematics, 2010. Bandits with Movement Costs and Adaptive Pricing  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681-690. ACM, 2008.  Robert D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In  Advances in Neural Information Processing Systems, pages 697-704, 2004.  Robert D. Kleinberg and Frank Thomson Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In 44th Symposium on Foundations of Computer Science FOCS, pages 594-605, 2003.  Aleksandrs Slivkins. Multi-armed bandits on implicit metric spaces. In Advances in Neural  Information Processing Systems, pages 1602-1610, 2011.  Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. Ranked bandits in metric learning diverse rankings over large document collections. Journal of Machine  spaces: Learning Research, 14(Feb):399-436, 2013.  J.Y. Yu and S. Mannor. Unimodal bandits. In Proceedings of the 28th International Con-  ference on Machine Learning, 2011. "}, "Sparse Stochastic Bandits": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Sparse Stochastic Bandits", "abstract": "In the classical multi-armed bandit problem, $d$ arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with $d$ (or with $\\sqrt{d}$ in the minimax sense). We here consider the \\emphsparse case of this classical problem in the sense that only a small number of arms, namely $s", "pdf_url": "http://proceedings.mlr.press/v65/kwon17a/kwon17a.pdf", "keywords": ["stochastic multi-armed bandit problem", "regret", "sparsity", "UCB"]}, "On the Ability of Neural Nets to Express Distributions": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "On the Ability of Neural Nets to Express Distributions", "abstract": "Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution\u2014also theoretically not understood\u2014concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron\u2019s Theorem (Barron, 1993), which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions (\u201cBarron functions\u201d) can be approximated by a $n+1$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance\u2014a natural metric on probability distributions\u2014by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.", "pdf_url": "http://proceedings.mlr.press/v65/lee17a/lee17a.pdf", "keywords": ["neural network", "generative model", "function approximation", "Fourier transform"], "reference": "Martin Arjovsky and L\u00b4eon Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein gan.  arXiv preprint  arXiv:1701.07875, 2017.  Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930-945, 1993. ISSN 00189448. doi: 10.1109/ 18.256500.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994. ISSN 08856125. doi: 10.1007/BF00993164.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. arXiv preprint arXiv:1509.05009, 554, 2015.  George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.  Amit Daniely. Depth separation for neural networks. arXiv preprint arXiv:1702.08489, 2017.  Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. CoRR abs/1506.08473, 2015.  Daniel M Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 633-643. ACM, 2016.  14   LEE GE MA RISTESKI ARORA  References  Martin Arjovsky and L\u00b4eon Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein gan.  arXiv preprint  arXiv:1701.07875, 2017.  Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930-945, 1993. ISSN 00189448. doi: 10.1109/ 18.256500.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994. ISSN 08856125. doi: 10.1007/BF00993164.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor  analysis. arXiv preprint arXiv:1509.05009, 554, 2015.  George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.  Amit Daniely. Depth separation for neural networks. arXiv preprint arXiv:1702.08489, 2017.  Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.  Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-  versal approximators. Neural networks, 2(5):359-366, 1989.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. CoRR abs/1506.08473, 2015.  Daniel M Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 633-643. ACM, 2016. ON THE ABILITY OF NEURAL NETS TO EXPRESS DISTRIBUTIONS  Leonid Vasilevich Kantorovich and G Sh Rubinstein. On a space of completely additive functions.  Vestnik Leningrad. Univ, 13(7):52-59, 1958.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. In Proceedings of the twenty-sixth annual ACM  On the learnability of discrete distributions. symposium on Theory of computing, pages 273-282. ACM, 1994.  Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Pro- ceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 180-191. VLDB Endowment, 2004.  Diederik P Kingma and Max Welling. Auto-encoding variational bayes.  arXiv preprint  arXiv:1312.6114, 2013.  Ilia Krasikov. Approximations for the bessel and airy functions with an explicit error term. LMS  Journal of Computation and Mathematics, 17(01):209-225, 2014.  Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and  approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.  J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015. doi: 10.1016/j.neunet.2014.09.003. Published online 2014; based on TR arXiv:1404.7828 [cs.NE].  Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016. LEE GE MA RISTESKI ARORA  "}, "Fundamental limits of symmetric low-rank matrix estimation": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Fundamental limits of symmetric low-rank matrix estimation", "abstract": "We consider the high-dimensional inference problem where the signal is a low-rank symmetric matrix which is corrupted by an additive Gaussian noise. Given a probabilistic model for the low-rank matrix, we compute the limit in the large dimension setting for the mutual information between the signal and the observations, as well as the matrix minimum mean square error, while the rank of the signal remains constant. We unify and generalize a number of recent works on PCA, sparse PCA, submatrix localization or community detection by computing the information-theoretic limits for these problems in the high noise regime. This allows to locate precisely the information-theoretic thresholds for the above mentioned problems.", "pdf_url": "http://proceedings.mlr.press/v65/lelarge17a/lelarge17a.pdf", "keywords": ["Matrix factorization", "PCA", "community detection", "spin glasses"], "reference": "IEEE International Symposium on Information Theory, ISIT 2015, Hong Kong, China, June 14- 19, 2015, 2015. IEEE. ISBN 978-1-4673-7704-1. URL http://ieeexplore.ieee.org/ xpl/mostRecentIssue.jsp?punumber=7270906.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Michael Aizenman, Robert Sims, and Shannon L Starr. Extended variational principle for the  sherrington-kirkpatrick spin-glass model. Physical Review B, 68(21):214403, 2003.  Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. Annals of Probability, pages 1643-1697, 2005.  Afonso S Bandeira, Nicolas Boumal, and Amit Singer. Tightness of the maximum likelihood semidefinite relaxation for angular synchronization. Mathematical Programming, pages 1-23, 2016.  Jess Banks, Cristopher Moore, Nicolas Verzelen, Roman Vershynin, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization. arXiv preprint arXiv:1607.05222v2, 2016.  Jean Barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, and Lenka Zde- borov\u00b4a. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. In Advances in Neural Information Processing Systems, pages 424-432, 2016.  Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford university press, 2013.  Francesco Caltagirone, Marc Lelarge, and L\u00b4eo Miolane. Recovering asymmetric communities in  the stochastic block model. arXiv preprint arXiv:1610.03680, 2016.  Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic  thresholds from the cavity method. arXiv preprint arXiv:1611.00814, 2016.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical Review E, 84(6):066106, 2011.  Yash Deshpande and Emmanuel Abbe. Asymptotic mutual information for the balanced binary  stochastic block model. Information and Inference, page iaw017, 2016.  Yash Deshpande and Andrea Montanari.  Information-theoretically optimal sparse pca. IEEE International Symposium on Information Theory, pages 2197-2201. IEEE, 2014.  In 2014  3   FUNDAMENTAL LIMITS OF SYMMETRIC LOW-RANK MATRIX ESTIMATION  References  IEEE International Symposium on Information Theory, ISIT 2015, Hong Kong, China, June 14- 19, 2015, 2015. IEEE. ISBN 978-1-4673-7704-1. URL http://ieeexplore.ieee.org/ xpl/mostRecentIssue.jsp?punumber=7270906.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Michael Aizenman, Robert Sims, and Shannon L Starr. Extended variational principle for the  sherrington-kirkpatrick spin-glass model. Physical Review B, 68(21):214403, 2003.  Jinho Baik, G\u00b4erard Ben Arous, and Sandrine P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. Annals of Probability, pages 1643-1697, 2005.  Afonso S Bandeira, Nicolas Boumal, and Amit Singer. Tightness of the maximum likelihood semidefinite relaxation for angular synchronization. Mathematical Programming, pages 1-23, 2016.  Jess Banks, Cristopher Moore, Nicolas Verzelen, Roman Vershynin, and Jiaming Xu. Information- theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization. arXiv preprint arXiv:1607.05222v2, 2016.  Jean Barbier, Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, and Lenka Zde- borov\u00b4a. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. In Advances in Neural Information Processing Systems, pages 424-432, 2016.  Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-  totic theory of independence. Oxford university press, 2013.  Francesco Caltagirone, Marc Lelarge, and L\u00b4eo Miolane. Recovering asymmetric communities in  the stochastic block model. arXiv preprint arXiv:1610.03680, 2016.  Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic  thresholds from the cavity method. arXiv preprint arXiv:1611.00814, 2016.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical Review E, 84(6):066106, 2011.  Yash Deshpande and Emmanuel Abbe. Asymptotic mutual information for the balanced binary  stochastic block model. Information and Inference, page iaw017, 2016.  Yash Deshpande and Andrea Montanari.  Information-theoretically optimal sparse pca. IEEE International Symposium on Information Theory, pages 2197-2201. IEEE, 2014.  In 2014 LELARGE MIOLANE  Delphine F\u00b4eral and Sandrine P\u00b4ech\u00b4e. The largest eigenvalue of rank one deformation of large wigner  matrices. Communications in mathematical physics, 272(1):185-228, 2007.  Francesco Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communi-  cations in mathematical physics, 233(1):1-12, 2003.  Dongning Guo, Shlomo Shamai, and Sergio Verd\u00b4u. Mutual information and minimum mean-square error in gaussian channels. IEEE Transactions on Information Theory, 51(4):1261-1282, 2005.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Information limits for recovering a hidden community. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 1894-1898. IEEE, 2016.  Yukito Iba. The nishimori line and bayesian statistics. Journal of Physics A: Mathematical and  General, 32(21):3875, 1999.  Satish Babu Korada and Nicolas Macris. Exact solution of the gauge symmetric p-spin glass model  on a complete graph. Journal of Statistical Physics, 136(2):205-230, 2009.  Satish Babu Korada and Nicolas Macris. Tight bounds on the capacity of binary input random cdma  systems. IEEE Transactions on Information Theory, 56(11):5590-5613, 2010.  Satish Babu Korada and Andrea Montanari. Applications of the lindeberg principle in communi- IEEE Transactions on Information Theory, 57(4):2440-2450,  cations and statistical learning. 2011.  Florent Krzakala, Jiaming Xu, and Lenka Zdeborov\u00b4a. Mutual information in rank-one matrix esti-  mation. In Information Theory Workshop (ITW), 2016 IEEE, pages 71-75. IEEE, 2016.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. MMSE of probabilistic low-rank matrix estimation: Universality with respect to the output channel. In 53rd Annual Allerton Conference on Communication, Control, and Computing, Allerton 2015, Allerton Park & Retreat Center, Monticello, IL, USA, September 29 - October 2, 2015, pages 680-687. IEEE, 2015a. ISBN 978- 1-5090-1824-6. doi: 10.1109/ALLERTON.2015.7447070. URL http://dx.doi.org/10. 1109/ALLERTON.2015.7447070.  Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00b4a. Phase transitions in sparse PCA.  In IEEE International Symposium on Information Theory, ISIT 2015, Hong Kong, China, June 14- 19, 2015 DBL (2015), pages 1635-1639. ISBN 978-1-4673-7704-1. doi: 10.1109/ISIT.2015. 7282733. URL http://dx.doi.org/10.1109/ISIT.2015.7282733.  Cyril M\u00b4easson, Andrea Montanari, Thomas J Richardson, and R\u00a8udiger Urbanke. The generalized area theorem and some of its consequences. IEEE Transactions on Information Theory, 55(11): 4793-4821, 2009.  Marc M\u00b4ezard, Giorgio Parisi, and Miguel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Co Inc, 1987.  Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):  583-601, 2002. FUNDAMENTAL LIMITS OF SYMMETRIC LOW-RANK MATRIX ESTIMATION  Andrea Montanari. Estimating random variables from random sparse observations. European Trans-  actions on Telecommunications, 19(4):385-403, 2008.  Andrea Montanari. Finding one community in a sparse graph. Journal of Statistical Physics, 161  (2):273-299, 2015.  preprint arXiv:1404.6304, 2014.  Joe Neeman and Praneeth Netrapalli. Non-reconstructability in the stochastic block model. arXiv  Hidetoshi Nishimori. Statistical physics of spin glasses and information processing: an introduc-  tion, volume 111. Clarendon Press, 2001.  Dmitry Panchenko. The Sherrington-Kirkpatrick model. Springer Science & Business Media, 2013.  Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and arXiv preprint  sub-optimality of pca for spiked random matrices and synchronization. arXiv:1609.05573, 2016.  Sundeep Rangan and Alyson K Fletcher. Iterative estimation of constrained rank-one matrices in noise. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, pages 1246-1250. IEEE, 2012.  Alaa Saade, Marc Lelarge, Florent Krzakala, and Lenka Zdeborov\u00b4a. Spectral detection in the cen- sored block model. In 2015 IEEE International Symposium on Information Theory (ISIT), pages 1184-1188. IEEE, 2015.  Alaa Saade, Marc Lelarge, Florent Krzakala, and Lenka Zdeborov\u00b4a. Clustering from sparse pairwise In Information Theory (ISIT), 2016 IEEE International Symposium on, pages  measurements. 780-784. IEEE, 2016.  Michel Talagrand. Mean field models for spin glasses: Volume I: Basic examples, volume 54.  Springer Science & Business Media, 2010.  Michel Talagrand. Mean field models for spin glasses: Volume II: Advanced Replica-Symmetry and  Low Temperature, volume 55. Springer Science & Business Media, 2011.  Lenka Zdeborov\u00b4a and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms.  Advances in Physics, 65(5):453-552, 2016. "}, "Robust and Proper Learning for Mixtures of Gaussians via Systems of Polynomial Inequalities": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Robust and Proper Learning for Mixtures of Gaussians via Systems of Polynomial Inequalities", "abstract": "Learning a Gaussian mixture model (GMM) is a fundamental statistical problem. One common notion of learning a GMM is proper learning: here, the goal is to find a mixture of $k$ Gaussians $\\mathcal{M}$ that is close to the unknown density $f$ from which we draw samples. The distance between $\\mathcal{M}$ and $f$ is often measured in the total variation / $L_1$-distance. Our main result is an algorithm for learning a mixture of $k$ univariate Gaussians that is \\emphnearly-optimal for any fixed $k$.  It is well known that the sample complexity of properly learning a univariate $k$-GMM is $O(k / \u03b5^2)$.  However, the best prior \\emphrunning time for this problem is $\\widetilde{O}(1 / \u03b5^3k-1)$; in particular, the dependence between $1/\u03b5$ and $k$ is exponential.  In this paper, we significantly improve this dependence by replacing the $1/\u03b5$ term with $\\log 1/\u03b5$, while only increasing the exponent moderately.  Specifically, the running time of our algorithm is $(k \u22c5\\log 1/ \u03b5)^O(k^4) + \\widetilde{O}(k / \u03b5^2)$.  For any fixed $k$, the $\\widetilde{O}(k / \u03b5^2)$ term dominates our running time, and thus our algorithm runs in time which is \\emphnearly-linear in the number of samples drawn.  Achieving a running time of $\\text{poly}(k, 1 / \u03b5)$ for proper learning of $k$-GMMs has recently been stated as an open problem by multiple researchers, and we make progress on this question. Our main algorithmic ingredient is a new connection between proper learning of parametric distributions and systems of polynomial inequalities.  We leverage results for piecewise polynomial approximation of GMMs and reduce the learning problem to a much smaller sub-problem.  While tihs sub-problem is still non-convex, its size depends only logarithmically on the final accuracy $\u03b5$.  Hence we can invoke computationally expensive methods for solving the sub-problem. We show that our connection is also useful in the multivariate setting, where we get new results for learning a mixture of two spherical Gaussians.  A variant of our approach is also within reach of modern computer algebra systems.  Experiments for learning a 2-GMM show promising results: our algorithm improves over the popular Expectation-Maximization (EM) algorithm in the noisy setting.", "pdf_url": "http://proceedings.mlr.press/v65/li17a/li17a.pdf", "keywords": ["Mixtures of Gaussians", "GMM", "proper learning", "expectation maximization"], "reference": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Near-optimal-  sample estimators for spherical Gaussian mixtures. In NIPS, 2014.  Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density esti-  mation in nearly-linear time. 2017.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 458-469. 2005.  In  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R. Voss. The more, the merrier: the blessing of dimensionality for learning large Gaussian mixtures. In COLT, 2014.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM  algorithm: From population to sample-based analysis. arXiv:1408.2156, 2014.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, 2010.  Jarle Berntsen, Terje O. Espelid, and Alan Genz. An adaptive algorithm for the approximate cal- culation of multiple integrals. ACM Transactions on Mathematical Software, 17(4):437-451, 1991.  Aditja Bhaskara, Ananda Theertha Suresh, and Morteza Zadimoghaddam. Sparse solutions to non-  negative linear systems and applications. In AISTATS, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In STOC, 2014.  Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Inc.,  S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In SODA, 2009.  S. Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. In FOCS,  1995.  2008.  Siu-On Chan, Ilias Diakonikolas, Rocco Servedio, and Xiaorui Sun. Learning mixtures of structured  distributions over discrete domains. In SODA, 2013.  78   LI SCHMIDT  to approximate the original 2-GMM better than this perturbation, i.e., the algorithm succceeds in \u201cdenoising\u201d the perturbed distribution.  We thank Jayadev Acharya, Ilias Diakonikolas, Piotr Indyk, Gautam Kamath, Ankur Moitra, Cameron Musco, Christopher Musco, Ilya Razenshteyn, Rocco Servedio, Ananda Theertha Suresh, and San- tosh Vempala for helpful discussions.  Acknowledgments  References  Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Near-optimal-  sample estimators for spherical Gaussian mixtures. In NIPS, 2014.  Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density esti-  mation in nearly-linear time. 2017.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 458-469. 2005.  In  Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James R. Voss. The more, the merrier: the blessing of dimensionality for learning large Gaussian mixtures. In COLT, 2014.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.  Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM  algorithm: From population to sample-based analysis. arXiv:1408.2156, 2014.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, 2010.  Jarle Berntsen, Terje O. Espelid, and Alan Genz. An adaptive algorithm for the approximate cal- culation of multiple integrals. ACM Transactions on Mathematical Software, 17(4):437-451, 1991.  Aditja Bhaskara, Ananda Theertha Suresh, and Morteza Zadimoghaddam. Sparse solutions to non-  negative linear systems and applications. In AISTATS, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In STOC, 2014.  Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Inc.,  S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In SODA, 2009.  S. Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. In FOCS,  1995.  2008.  Siu-On Chan, Ilias Diakonikolas, Rocco Servedio, and Xiaorui Sun. Learning mixtures of structured  distributions over discrete domains. In SODA, 2013. ROBUST AND PROPER LEARNING FOR MIXTURES OF GAUSSIANSVIA SYSTEMS OF POLYNOMIAL INEQUALITIES  Siu-On Chan, Ilias Diakonikolas, Rocco Servedio, and Xiaorui Sun. Efficient density estimation via  piecewise polynomial approximation. In STOC, 2014.  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-  munications and Signal Processing). Wiley-Interscience, 2006. ISBN 0471241954.  Daniel Dadush. A deterministic polynomial space construction for eps-nets under any norm. CoRR,  abs/1311.6671, 2013.  Sanjoy Dasgupta. Learning mixtures of Gaussians. In FOCS, 1999.  Sanjoy Dasgupta and Leonard J. Schulman. A probabilistic analysis of em for mixtures of separated,  spherical gaussians. Journal of Machine Learning Research, 8:203-226, 2007.  Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for  proper learning mixtures of gaussians. In COLT, 2014.  Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for mixtures of two gaussians. In NIPS 2016 Workshop on Non-Convex Optimization for Machine Learning, 2016. URL https://arxiv.org/abs/1609.00368.  Luc Devroye and Gabor Lugosi. Combinatorial methods in density estimation. Springer Series in  Statistics, Springer, 2001.  Ilias Diakonikolas. Learning structured distributions. In Peter B\u00fchlmann, Petros Drineas, Michael J. Kane, and Mark Van Der Laan, editors, Handbook of Big Data. Chapman and Hall/CRC, 2016.  Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In FOCS, 2016a.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Properly learning poisson binomial dis-  tributions in almost polynomial time. In COLT, 2016b.  Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart.  Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. arXiv preprint arXiv:1611.03473, 2016c.  Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Learning multivariate log-concave distri- butions. CoRR, abs/1605.08188, 2016d. URL http://arxiv.org/abs/1605.08188.  Jon Feldman, Rocco A. Servedio, and Ryan O\u2019Donnell. PAC learning axis-aligned mixtures of  Gaussians with no separation assumption. In COLT, 2006.  Jon Feldman, Ryan O\u2019Donnell, and Rocco A. Servedio. Learning mixtures of product distributions  over discrete domains. SIAM Journal on Computing, 37(5):1536-1564, 2008.  Yoav Freund and Yishay Mansour. Estimating a mixture of two product distributions. In COLT,  Rong Ge, Qingqing Huang, and Sham M. Kakade. Learning mixtures of gaussians in high dimen-  1999.  sions. In STOC, 2015. LI SCHMIDT  W. Morven Gentleman. Implementing clenshaw-curtis quadrature, i methodology and experience.  Communications of the ACM, 15(5):337-342, 1972.  Alan C. Genz and Arham A. Malik. Remarks on algorithm 006: An adaptive algorithm for numeri- cal integration over an n-dimensional rectangular region. Journal of Computational and Applied Mathematics, 6(4):295 - 302, 1980.  Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two Gaussians. In STOC, 2015.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: Moment methods and  spectral decompositions. In ITCS, 2013.  Arian Maleki Ji Xu, Daniel Hsu. Global analysis of expectation maximization for mixtures of two  gaussians. In NIPS, 2016.  Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright, and Michael I. Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In NIPS, 2016.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two  Gaussians. In STOC, 2010.  Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture  models. SIAM Journal on Computing, 38(3):1141-1156, 2008.  Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning.  Machine Learning, 17(2-3):115-141, 1994.  Arlene K. H. Kim and Richard J. Samworth. Global rates of convergence in log-concave density  estimation. Annals of Statistics, 44(6):2756-2779, 12 2016.  Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-  motor policies. Journal of Machine Learning Research, 17(1):1334-1373, 2016.  Ankur Moitra. Algorithmic aspects of machine learning. http://people.csail.mit.edu/  moitra/docs/bookex.pdf, 2014.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians.  In FOCS, 2010.  Cameron Musco and Christopher Musco. Stronger and faster approximate singular value decompo-  sition via the block lanczos method. In NIPS, 2015.  Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 185:71-110, 1894.  James Renegar. On the computational complexity and geometry of the first-order theory of the reals. Part i: Introduction. Preliminaries. The geometry of semi-algebraic sets. the decision problem for the existential theory of the reals. Journal of Symbolic Computation, 13(3):255 - 299, 1992a. ROBUST AND PROPER LEARNING FOR MIXTURES OF GAUSSIANSVIA SYSTEMS OF POLYNOMIAL INEQUALITIES  James Renegar. On the computational complexity of approximating solutions for real algebraic  formulae. SIAM Journal on Computing, 21(6):1008-1025, 1992b.  Aleksandr F. Timan. Theory of Approximation of Functions of a Real Variable. Pergamon, New  York, 1963.  Timo Tossavainen. On the zeros of finite sums of exponential functions. Australian Mathematical  Society Gazette, 33(1):47 - 50, 2006.  Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company,  Incorporated, 1st edition, 2008. ISBN 0387790519, 9780387790510.  Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of  Computer and System Sciences, 68(4):841-860, 2004.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010. "}, "Adaptivity to Noise Parameters in Nonparametric Active Learning": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Adaptivity to Noise Parameters in Nonparametric Active Learning", "abstract": "This work addresses various open questions in the theory of active learning for nonparametric classification.  Our contributions are both statistical and algorithmic: \\beginitemize \\item We establish new minimax-rates for active learning under common noise conditions. These rates display interesting transitions \u2013 due to the interaction between noise smoothness and margin \u2013 not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed.  \\item We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for adaptive confidence sets, resulting in strictly milder distributional requirements. \\enditemize", "pdf_url": "http://proceedings.mlr.press/v65/locatelli andrea17a/locatelli andrea17a.pdf"}, "Noisy Population Recovery from Unknown Noise": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Noisy Population Recovery from Unknown Noise", "abstract": "The noisy population recovery problem is a statistical inference problem, which is a special case of the problem of learning mixtures of product distributions. Given an unknown distribution on $n$-bit strings with support of size $k$, and given access only to noisy samples from it, where each bit is flipped independently with some unknown noise probability, estimate from a few samples the underlying parameters of the model. Previous work [De et al., FOCS 2016] designed polynomial time algorithms which work under the assumption that the noise parameters are known exactly. In this work, we remove this assumption, and show how to recover the underlying parameters, even when the noise is unknown, in quasi-polynomial time.", "pdf_url": "http://proceedings.mlr.press/v65/lovett17a/lovett17a.pdf", "keywords": ["Noisy recovery", "Partial information", "learning mixtures of product distributions"]}, "Inapproximability of VC Dimension and Littlestone\u2019s Dimension": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Inapproximability of VC Dimension and Littlestone\u2019s Dimension", "abstract": "We study the complexity of computing the VC Dimension and Littlestone\u2019s Dimension. Given an explicit description of a finite universe and a concept class (a binary matrix whose $(x,C)$-th entry is $1$ iff element $x$ belongs to concept $C$), both can be computed exactly in quasi-polynomial time ($n^O(\\log n)$). Assuming the randomized Exponential Time Hypothesis (ETH), we prove nearly matching lower bounds on the running time, that hold even for \\em approximation algorithms.", "pdf_url": "http://proceedings.mlr.press/v65/manurangsi17a/manurangsi17a.pdf", "keywords": ["VC Dimension", "Littlestone\u2019s Dimension", "Hardness of Approximation"], "reference": "Scott Aaronson, Russell Impagliazzo, and Dana Moshkovitz. AM with multiple Merlins. In IEEE 29th Conference on Computational Complexity, CCC 2014, Vancouver, BC, Canada, June 11-13, 2014, pages 44-55, 2014. doi: 10.1109/CCC.2014.13. URL http: //dx.doi.org/10.1109/CCC.2014.13.  Sanjeev Arora, Rong Ge, Sushant Sachdeva, and Grant Schoenebeck. Finding overlapping communities in social networks: toward a rigorous approach. In ACM Conference on Electronic Commerce, EC \u201912, Valencia, Spain, June 4-8, 2012, pages 37-54, 2012. doi: 10.1145/2229012.2229020. URL http://doi.acm.org/10.1145/2229012.2229020.  Yakov Babichenko, Christos H. Papadimitriou, and Aviad Rubinstein. Can almost everybody In Proceedings of the 2016 ACM Conference on Innovations in be almost happy? Theoretical Computer Science, Cambridge, MA, USA, January 14-16, 2016, pages 1- 9, 2016. doi: 10.1145/2840728.2840731. URL http://doi.acm.org/10.1145/2840728. 2840731.  4. Roughly, self-directed learning is similar to the online learning model corresponding to Littlestone\u2019s dimension, but where the learner chooses the order elements; see Ben-David and Eiron (1998) for details.  25   Inapproximability of VC Dimension and Littlestone\u2019s Dimension  almost matching running time lower bound of nlog1\u2212o(1) n for both problems while ruling out approximation ratios of 1/2 + o(1) and 1 \u2212 \u03b5 for some \u03b5 > 0 for VC Dimension and Littlestone\u2019s Dimension respectively. Even though our results help us gain more insights on approximability of both problems, it is not yet completely resolved. More specifically, we are not aware of any constant factor no(log n)-time approximation algorithm for either problem; it is an intriguing open question whether such algorithm exists and, if not, whether our reduction can be extended to rule out such algorithm. Another potentially interesting research direction is to derandomize our construction; note that the only place in the proof in which the randomness is used is in Lemma 19.  A related question which remains open, originally posed by Ben-David and Eiron Ben-David and Eiron (1998), is that of computing the self-directed learning4 mistake bound. Similarly, it may be interesting to understand the complexity of computing (approximating) the recursive teaching dimension Doliwa et al. (2014); Moran et al. (2015).  Acknowledgement  We thank Shai Ben-David for suggesting the question of approximability of Littlestone\u2019s dimension, and several other fascinating discussions. We also thank Yishay Mansour and COLT anonymous reviewers for their useful comments.  Pasin Manurangsi is supported by NSF Grants No. CCF 1540685 and CCF 1655215.  Aviad Rubinstein was supported by a Microsoft Research PhD Fellowship, as well as NSF grant CCF1408635 and Templeton Foundation grant 3966. This work was done in part at the Simons Institute for the Theory of Computing.  References  Scott Aaronson, Russell Impagliazzo, and Dana Moshkovitz. AM with multiple Merlins. In IEEE 29th Conference on Computational Complexity, CCC 2014, Vancouver, BC, Canada, June 11-13, 2014, pages 44-55, 2014. doi: 10.1109/CCC.2014.13. URL http: //dx.doi.org/10.1109/CCC.2014.13.  Sanjeev Arora, Rong Ge, Sushant Sachdeva, and Grant Schoenebeck. Finding overlapping communities in social networks: toward a rigorous approach. In ACM Conference on Electronic Commerce, EC \u201912, Valencia, Spain, June 4-8, 2012, pages 37-54, 2012. doi: 10.1145/2229012.2229020. URL http://doi.acm.org/10.1145/2229012.2229020.  Yakov Babichenko, Christos H. Papadimitriou, and Aviad Rubinstein. Can almost everybody In Proceedings of the 2016 ACM Conference on Innovations in be almost happy? Theoretical Computer Science, Cambridge, MA, USA, January 14-16, 2016, pages 1- 9, 2016. doi: 10.1145/2840728.2840731. URL http://doi.acm.org/10.1145/2840728. 2840731.  4. Roughly, self-directed learning is similar to the online learning model corresponding to Littlestone\u2019s dimension, but where the learner chooses the order elements; see Ben-David and Eiron (1998) for details. Manurangsi Rubinstein  Siddharth Barman. Approximating Nash equilibria and dense bipartite subgraphs via an approximate version of caratheodory\u2019s theorem. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 361-369, 2015. doi: 10.1145/2746539.2746566. URL http: //doi.acm.org/10.1145/2746539.2746566.  Cristina Bazgan, Florent Foucaud, and Florian Sikora. On the approximability of partial VC dimension. In Combinatorial Optimization and Applications - 10th International Conference, COCOA 2016, Hong Kong, China, December 16-18, 2016, Proceedings, pages 92-106, 2016. doi: 10.1007/978-3-319-48749-6 7. URL http://dx.doi.org/10.1007/ 978-3-319-48749-6_7.  Shai Ben-David and Nadav Eiron. Self-directed learning and its relation to the vc-dimension and to teacher-directed learning. Machine Learning, 33(1):87-104, 1998. doi: 10.1023/A: 1007510732151. URL http://dx.doi.org/10.1023/A:1007510732151.  Shai Ben-David, D\u00b4avid P\u00b4al, and Shai Shalev-Shwartz. Agnostic online learning. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009. URL http://www.cs.mcgill.ca/\u02dccolt2009/papers/032.pdf#page=1.  Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for signaling in Bayesian zero-sum and network routing games. In Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916, Maastricht, The Netherlands, July 24-28, 2016, pages 479-496, 2016. doi: 10.1145/2940716.2940753. URL http: //doi.acm.org/10.1145/2940716.2940753.  Avrim Blum.  Separating distribution-free and mistake-bound learning models over 10.1137/  the boolean domain. S009753979223455X. URL http://dx.doi.org/10.1137/S009753979223455X.  SIAM J. Comput., 23(5):990-1000, 1994.  doi:  Mark Braverman, Young Kun-Ko, and Omri Weinstein. Approximating the best Nash equilibrium in no(log n)-time breaks the exponential time hypothesis. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 970-982, 2015. doi: 10.1137/1. 9781611973730.66. URL http://dx.doi.org/10.1137/1.9781611973730.66.  Mark Braverman, Young Kun-Ko, Aviad Rubinstein, and Omri Weinstein. ETH hardness for densest-k-subgraph with perfect completeness. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pages 1326-1341, 2017. doi: 10.1137/1.9781611974782.86. URL http://dx.doi.org/10.1137/1.9781611974782.86.  Yu Cheng, Ho Yee Cheung, Shaddin Dughmi, Ehsan Emamjomeh-Zadeh, Li Han, and Shang-Hua Teng. Mixture selection, mechanism design, and signaling. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 1426-1445, 2015. doi: 10.1109/FOCS.2015.91. URL http://dx.doi.org/10.1109/FOCS.2015.91. Inapproximability of VC Dimension and Littlestone\u2019s Dimension  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 105-117, 2016. doi: 10.1145/2897518.2897520. URL http://doi.acm.org/10.1145/2897518.2897520.  Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning DNF\u2019s. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pages 815-830, 2016. URL http://jmlr.org/proceedings/papers/ v49/daniely16.html.  Argyrios Deligkas, John Fearnley, and Rahul Savani. Inapproximability results for approxi- mate Nash equilibria. CoRR, abs/1608.03574, 2016. URL http://arxiv.org/abs/1608. 03574.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimension, vc-dimension and sample compression. Journal of Machine Learning Research, 15(1):3107-3131, 2014. URL http://dl.acm.org/citation.cfm?id=2697064.  Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. New results for learning noisy parities and halfspaces. In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006), 21-24 October 2006, Berkeley, California, USA, Proceedings, pages 563-574, 2006. doi: 10.1109/FOCS.2006.51. URL http://dx. doi.org/10.1109/FOCS.2006.51.  Moti Frances and Ami Litman. Optimal mistake bound learning is hard. Inf. Comput., 144 (1):66-82, 1998. doi: 10.1006/inco.1998.2709. URL http://dx.doi.org/10.1006/inco. 1998.2709.  Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-SAT. J. Comput. Syst. Sci., 62(2):367-375, 2001. doi: 10.1006/jcss.2000.1727. URL http://dx.doi.org/ 10.1006/jcss.2000.1727.  Russell Impagliazzo, Ramamohan Paturi, and Francis Zane. Which problems have strongly exponential complexity? J. Comput. Syst. Sci., 63(4):512-530, 2001. doi: 10.1006/jcss. 2001.1774. URL http://dx.doi.org/10.1006/jcss.2001.1774.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. doi:  SIAM J. Comput., 37(6):1777-1805, 2008.  Agnostically learning halfspaces. 10.1137/060649057. URL http://dx.doi.org/10.1137/060649057.  Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and finite automata. J. ACM, 41(1):67-95, 1994. doi: 10.1145/174644.174647. URL http://doi.acm.org/10.1145/174644.174647.  Michael Kharitonov. Cryptographic hardness of distribution-specific learning. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, May 16-18, 1993, San Diego, CA, USA, pages 372-381, 1993. doi: 10.1145/167088.167197. URL http://doi.acm.org/10.1145/167088.167197. Manurangsi Rubinstein  Michael Kharitonov. Cryptographic lower bounds for learnability of boolean functions on the uniform distribution. J. Comput. Syst. Sci., 50(3):600-610, 1995. doi: 10.1006/jcss. 1995.1046. URL http://dx.doi.org/10.1006/jcss.1995.1046.  Adam R. Klivans. Cryptographic hardness of learning. In Encyclopedia of Algorithms, pages 475-477. 2016. doi: 10.1007/978-1-4939-2864-4 96. URL http://dx.doi.org/10.1007/ 978-1-4939-2864-4_96.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersec- tions of halfspaces. J. Comput. Syst. Sci., 75(1):2-12, 2009. doi: 10.1016/j.jcss.2008.07.008. URL http://dx.doi.org/10.1016/j.jcss.2008.07.008.  Nathan Linial, Yishay Mansour, and Ronald L. Rivest. Results on learnability and the Vapnik- Chervonenkis dimension. Inf. Comput., 90(1):33-49, 1991. doi: 10.1016/0890-5401(91) 90058-A. URL http://dx.doi.org/10.1016/0890-5401(91)90058-A.  Richard J. Lipton, Evangelos Markakis, and Aranyak Mehta. Playing large games using simple strategies. In Proceedings 4th ACM Conference on Electronic Commerce (EC-2003), San Diego, California, USA, June 9-12, 2003, pages 36-41, 2003. doi: 10.1145/779928.779933. URL http://doi.acm.org/10.1145/779928.779933.  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Mach. Learn., 2(4):285-318, April 1988. ISSN 0885-6125. doi: 10.1023/A: 1022869011914. URL http://dx.doi.org/10.1023/A:1022869011914.  Pasin Manurangsi. Almost-polynomial ratio ETH-hardness of approximating densest k- subgraph. In Proceedings of the Fortieth-ninth Annual ACM Symposium on Theory of Computing, STOC \u201917, 2017. To appear.  Pasin Manurangsi and Prasad Raghavendra. A birthday repetition theorem and complexity of approximating dense CSPs. CoRR, abs/1607.02986, 2016. URL http://arxiv.org/ abs/1607.02986.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayo\ufb00. Compressing and teaching for low vc-dimension. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 40-51, 2015. doi: 10.1109/FOCS.2015.12. URL https://doi.org/10.1109/FOCS.2015.12.  Dana Moshkovitz and Ran Raz. Two-query PCP with subconstant error. J. ACM, 57(5): 29:1-29:29, 2010. doi: 10.1145/1754399.1754402. URL http://doi.acm.org/10.1145/ 1754399.1754402.  Elchanan Mossel and Christopher Umans. On the complexity of approximating the VC dimension. J. Comput. Syst. Sci., 65(4):660-671, 2002. doi: 10.1016/S0022-0000(02) 00022-3. URL http://dx.doi.org/10.1016/S0022-0000(02)00022-3.  Christos H. Papadimitriou and Mihalis Yannakakis. On limited nondeterminism and the complexity of the V-C dimension. J. Comput. Syst. Sci., 53(2):161-170, 1996. doi: 10.1006/jcss.1996.0058. URL http://dx.doi.org/10.1006/jcss.1996.0058. Inapproximability of VC Dimension and Littlestone\u2019s Dimension  Aviad Rubinstein. ETH-hardness for signaling in symmetric zero-sum games. CoRR,  abs/1510.04991, 2015. URL http://arxiv.org/abs/1510.04991.  Aviad Rubinstein. Settling the complexity of computing approximate two-player Nash equilibria. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 258-265, 2016a. doi: 10.1109/FOCS.2016.35. URL http://dx.doi.org/10.1109/FOCS.2016.35.  Aviad Rubinstein. Detecting communities is hard, and counting them is even harder. CoRR,  abs/1611.08326, 2016b. URL http://arxiv.org/abs/1611.08326.  Marcus Schaefer. Deciding the Vapnik-Cervonenkis dimension in \u03a3P  3 -complete. J. Comput. Syst. Sci., 58(1):177-182, 1999. doi: 10.1006/jcss.1998.1602. URL http://dx.doi.org/ 10.1006/jcss.1998.1602.  Marcus Schaefer. Deciding the k-dimension is PSPACE-complete. In Proceedings of the 15th Annual IEEE Conference on Computational Complexity, Florence, Italy, July 4-7, 2000, pages 198-203, 2000. doi: 10.1109/CCC.2000.856750. URL http://dx.doi.org/ 10.1109/CCC.2000.856750.  Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16 (2):264-280, 1971. doi: 10.1137/1116025. URL http://dx.doi.org/10.1137/1116025.  "}, "A Second-order Look at Stability and Generalization": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "A Second-order Look at Stability and Generalization", "abstract": "Using differentiability assumptions on the loss function and a concentration inequality for bounded second order differences it is shown that the generalization error for classification with L2 regularisation obeys a Bernstein-type inequality.", "pdf_url": "http://proceedings.mlr.press/v65/maurer17a/maurer17a.pdf", "keywords": ["generalisation", "stability", "concentration", "Bernstein inequality"], "reference": "University Press, 1999.  M. Anthony and Peter Bartlett. Learning in Neural Networks: Theoretical Foundations. Cambridge  8   MAURER  so  2 (B (\u2206) /3 + J (\u2206)) \u2264  68 max {1, c, c(cid:48)(cid:48)}3 \u03bb3n  +  24c(cid:48)(cid:48)(cid:48) max {1, c(cid:48), c(cid:48)(cid:48)}3 \u03bb4n  =  \u03b12 ((cid:96), \u03bb) n  ,  which proves (4).  Also, for any given x \u2208 B we have  (cid:96) ((cid:104)x, g (x)(cid:105)) \u2212 (cid:96) ((cid:104)x, g (x1, ..., xk\u22121, x, xk+1, ..., xn)(cid:105)) = Dk  xk,x(cid:96) ((cid:104)x, g (x)(cid:105)) .  But the expectation of the second term on the left, as x \u223c \u00b5 and x \u223c \u00b5n, is equal to Ex\u223c\u00b5n [(cid:96) ((cid:104)xi, g (x)(cid:105))], so  E [\u2206]) =  E [(cid:96) ((cid:104)x, g (x)(cid:105)) \u2212 (cid:96) ((cid:104)xk, g (x)(cid:105))] =  Ex\u223c\u00b5,x\u223c\u00b5n  1 n  (cid:88)  k  (cid:104)  Dk  (cid:105) xk,x(cid:96) ((cid:104)x, g (x)(cid:105))  \u2264  sup x\u2208Bn  sup y,y(cid:48)\u2208B  Ex\u223c\u00b5  (cid:104)  (cid:105) y,y(cid:48)(cid:96) ((cid:104)x, g (x)(cid:105))  Dk  \u2264 max  k  sup x\u2208Bn  sup y,y(cid:48)\u2208B  (cid:12) (cid:12)L(cid:48) (x) (cid:12)  (cid:104)\u02c6k (cid:0)y \u2212 y(cid:48)(cid:1)(cid:105)(cid:12) (cid:12) (cid:12) \u2264  k  = max  sup x\u2208Bn 2 max {1, c(cid:48), c(cid:48)(cid:48)}2 \u03bb3/2n  sup y,y(cid:48)\u2208B  Dk  y,y(cid:48)L (x)  =  \u03b11 ((cid:96), \u03bb) n  ,  1 n  1 n  (cid:88)  k (cid:88)  k  which proves (3). This completes the proof of Theorem 2.  Proof [of Theorem 3] Substitution of the above bound on B (\u2206) into the first concentration in- equality of Theorem 4, solving for the deviation and using the bound (3) on E [\u2206] gives the first inequality. For any x \u2208 Bn we have  \u03a32 (\u2206) (x) =  E(y,y(cid:48))\u223c\u00b52  Dk  y,y(cid:48)\u2206 (x)  (cid:17)2(cid:21)  (cid:20)(cid:16)  (cid:20)  sup x\u2208Bn  \uf8ee  (cid:32)  (cid:88)  k (cid:88)  k  (cid:88)  1 2  1 2  1 2  \u2264  \u2264  =  E(y,y(cid:48))\u223c\u00b52  \uf8f0  k 1 ((cid:96), \u03bb) \u03c32 9\u03b12  x\u223c\u00b5 (x)  .  n  E(y,y(cid:48))\u223c\u00b52  (cid:12) (cid:12)  (cid:12)\u2206(cid:48) (x) \u02c6k (cid:0)y \u2212 y(cid:48)(cid:1)(cid:12)  2(cid:21) (cid:12) (cid:12)  6 max {1, c(cid:48), c(cid:48)(cid:48)}2 \u03bb3/2n  (cid:33)2  (cid:13)y \u2212 y(cid:48)(cid:13) (cid:13) 2 (cid:13)  \uf8f9  \uf8fb  Substitution in the first inequality then gives the second inequality.  References  University Press, 1999.  M. Anthony and Peter Bartlett. Learning in Neural Networks: Theoretical Foundations. Cambridge SECOND-ORDER STABILITY  P. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural  results. Journal of Machine Learning Research, 3:463-482, 2002.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities. Oxford University Press,  O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Founda-  tions of Computational Mathematics, 7(3):331-368, 2007.  F. Cucker and S. Smale. Best choices for regularization parameters in learning theory: on the  bias-variance problem. Foundations of Computational Mathematics, 2(4):413-428, 2002.  B. Efron and C. Stein. The jackknife estimate of variance. The Annals of Statistics, pages 586-596,  M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient  descent. arXiv preprint arXiv:1509.01240, 2015.  C. Houdr\u00b4e. The iterated jackknife estimate of variance. Statistics and probability letters, 35(2):  A. Maurer. A Bernstein-type inequality for functions of bounded interaction. ArXiv e-prints, Jan-  C. McDiarmid. Concentration.  In Probabilistic Methods of Algorithmic Discrete Mathematics,  pages 195-248, Berlin, 1998. Springer.  T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of the IEEE, 78(9):  W. Rudin. Principles of mathematical analysis. McGraw-Hill New York, 1964.  J. M. Steele. An efron-stein inequality for nonsymmetric statistics. The Annals of Statistics, pages  2013.  2:499-526, 2002.  1981.  197-201, 1997.  uary 2017.  1481-1497, 1990.  753-758, 1986.  "}, "Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality", "abstract": "A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically. In this paper we study the rank-constrained version of SDPs arising in MaxCut and in $\\mathbb Z_2$ and $\\rm SO(d)$ synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and  dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be  of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a $(1 - 1/(k-1)) \\times 0.878$ approximation of the MaxCut, when the rank is fixed to $k$. We then apply our results to data matrices generated according to the Gaussian $\\mathbb Z_2$ synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods.", "pdf_url": "http://proceedings.mlr.press/v65/mei17a/mei17a.pdf", "keywords": ["Semidefinite programming", "non-convex optimization", "MaxCut", "group synchronization", "Grothendieck inequality"], "reference": "A.M. was partially supported by the NSF grant CCF-1319979 and DMS-1613091. S.M. was sup- ported by Office of Technology Licensing Stanford Graduate Fellowship.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods on Riemannian mani-  folds. Foundations of Computational Mathematics, 7(3):303-330, 2007.  Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices, volume  118. Cambridge university press, 2010.  Mica Arie-Nachimson, Shahar Z Kovalsky, Ira Kemelmacher-Shlizerman, Amit Singer, and Ronen In 3D Imaging, Modeling, Processing, Basri. Global motion estimation from point matches. Visualization and Transmission (3DIMPVT), 2012 Second International Conference on, pages 81-88. IEEE, 2012.  33   SOLVING SDPS VIA THE GROTHENDIECK INEQUALITY  Case 2. Then, we consider the case where we set \u00b5G = F  gradf (\u03c3) (cid:107) of gradient steps. According to Lemma 10, we have  F > \u00b5G, and use the eigen-step as (cid:107)  gradf (\u03c3) (cid:107)  (cid:107)  A  (cid:107) \u2264  2, and we use the gradient step as (cid:107) \u00b5G. First let us bound the number  \u00b52 G A (cid:107) Hence, we deduce the upper bound TG  40 (cid:107)  TG  g(\u03c30)  g(\u03c3T )  Rg(A).  1 \u2264  \u2212  \u2264  2 2. Then let us bound the number of eigen-steps. Let us denote  and the subsets of indices corresponding to eigensteps with respectively \u03bbH \u03bbH < 3  1). According to Lemma 12, we have for all t  1Rg(A)/  A (cid:107)\u00b7 (cid:107)  \u2264  A  I  (cid:107)  (cid:107)  A (cid:107)  2 A 2/(2 (cid:107)  (cid:107)  (cid:107)  1 g(\u03c3t+1) \u22121 A  g(\u03c3t) g(\u03c3t+1) \u2265  g(\u03c3t) \u2265  864 (cid:107)  1n2 (cid:107)  \u2208 J 1 A  864 (cid:107)  1n2 , (cid:107)  whereas for t  \u2208 I 1 g(\u03c3t+1)2 \u22122 2n3 (cid:107) (cid:107) Summing the contributions of the above two equations gives the convergence rate  g(\u03c3t)2 \u2265576  \u2265  (cid:19)  (cid:18)  (cid:107)  1 A  g(\u03c3t) g(\u03c3t+1)  +  g(\u03c3t)2 g(\u03c3t+1)2  1 A (cid:107)2n3 .  0, 1, . . . , T  J \u2282 { A 3 (cid:107) \u2265  2 A 2/(2 (cid:107)  (cid:107)  1 \u2212 } 1) and (cid:107)  g(\u03c3T )  c  max  \u2264  \u00b7  A  1 (cid:107)  (cid:107) (cid:18)  n2 T  ,  A  2n (cid:107)  (cid:107)  (cid:114)  (cid:19)  n T  \u2264  This guarantees that g(\u03c3T )  n\u03b5/2 as soon as T  \u2265  for some universal constant \u02dcc.  for a universal constant c. 2/\u03b52, 2 1/\u03b5 \u02dcc  n max  A (cid:107)  (cid:107)  \u00b7  A (cid:107)  (cid:107)  (cid:0)  (cid:1)  Acknowledgements  References  A.M. was partially supported by the NSF grant CCF-1319979 and DMS-1613091. S.M. was sup- ported by Office of Technology Licensing Stanford Graduate Fellowship.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. IEEE Transactions on Information Theory, 62(1):471-487, 2016.  P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods on Riemannian mani-  folds. Foundations of Computational Mathematics, 7(3):303-330, 2007.  Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices, volume  118. Cambridge university press, 2010.  Mica Arie-Nachimson, Shahar Z Kovalsky, Ira Kemelmacher-Shlizerman, Amit Singer, and Ronen In 3D Imaging, Modeling, Processing, Basri. Global motion estimation from point matches. Visualization and Transmission (3DIMPVT), 2012 Second International Conference on, pages 81-88. IEEE, 2012. MEI MISIAKIEWICZ MONTANARI OLIVEIRA  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227- 236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Jinho Baik, G\u00b4erard Ben Arous, Sandrine P\u00b4ech\u00b4e, et al. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.  Afonso S Bandeira, Moses Charikar, Amit Singer, and Andy Zhu. Multireference alignment using semidefinite programming. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 459-470. ACM, 2014.  Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for arXiv preprint  semidefinite programs arising in synchronization and community detection. arXiv:1602.04426, 2016.  Alexander I. Barvinok. Problems of distance geometry and convex properties of quadratic maps.  Discrete & Computational Geometry, 13(2):189-202, 1995.  Nicolas Boumal. A Riemannian low-rank method for optimization over semidefinite matrices with  block-diagonal constraints. arXiv:1506.00575, 2015.  Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex burer-monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757-2765, 2016.  Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefi- nite programs via low-rank factorization. Mathematical Programming, 95(2):329-357, 2003.  Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for the  two-groups stochastic block model. arXiv preprint arXiv:1507.08685, 2015.  Joel Friedman. A proof of alon\u2019s second eigenvalue conjecture. In Proceedings of the thirty-fifth  annual ACM symposium on Theory of computing, pages 720-724. ACM, 2003.  Dan Garber and Elad Hazan. Approximating semidefinite programs in sublinear time. In Advances  in Neural Information Processing Systems, pages 1080-1088, 2011.  Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42 (6):1115-1145, 1995. SOLVING SDPS VIA THE GROTHENDIECK INEQUALITY  Alexander Grothendieck. R\u00b4esum\u00b4e de la th\u00b4eorie m\u00b4etrique des produits tensoriels topologiques. Re- senhas do Instituto de Matem\u00b4atica e Estat\u00b4\u0131stica da Universidade de S\u02dcao Paulo, 2(4):401-481, 1996.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via grothendiecks  inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef-  inite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, 2016.  Adel Javanmard, Andrea Montanari, and Federico Ricci-Tersenghi. Phase transitions in semidefinite relaxations. Proceedings of the National Academy of Sciences, 113(16):E2218-E2223, 2016.  Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis.  Annals of statistics, pages 295-327, 2001.  Subhash Khot and Assaf Naor. Grothendieck-type inequalities in combinatorial optimization. Com-  munications on Pure and Applied Mathematics, 65(7):992-1035, 2012.  Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O\u2019Donnell. Optimal inapproximability results for max-cut and other 2-variable csps? SIAM Journal on Computing, 37(1):319-357, 2007.  Satish Babu Korada and Nicolas Macris. Exact solution of the gauge symmetric p-spin glass model  on a complete graph. Journal of Statistical Physics, 136(2):205-230, 2009.  J Kuczy\u00b4nski and H Wo\u00b4zniakowski. Estimating the largest eigenvalue by the power and lanczos algorithms with a random start. SIAM journal on matrix analysis and applications, 13(4):1094- 1122, 1992.  Laurent Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. In Pro- ceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694-703. ACM, 2014.  Ankur Moitra, William Perry, and Alexander S Wein. How robust are reconstruction thresholds for community detection? In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 828-841. ACM, 2016.  Andrea Montanari. A Grothendieck-type inequality for local maxima. arXiv:1603.04064, 2016.  Andrea Montanari and Subhabrata Sen. Semidefinite programs on sparse random graphs and their application to community detection. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages 814-827. ACM, 2016.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  arXiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition  model. Probability Theory and Related Fields, 162(3-4):431-461, 2015. MEI MISIAKIEWICZ MONTANARI OLIVEIRA  Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer  Science &amp; Business Media, 2013.  G\u00b4abor Pataki. On the rank of extreme matrices in semidefinite programs and the multiplicity of  optimal eigenvalues. Mathematics of operations research, 23(2):339-358, 1998.  Amit Singer. Angular synchronization by eigenvectors and semidefinite programming. Applied and  computational harmonic analysis, 30(1):20-36, 2011.  Amit Singer and Yoel Shkolnisky. Three-dimensional structure determination from common lines in cryo-em by eigenvectors and semidefinite programming. SIAM journal on imaging sciences, 4 (2):543-572, 2011.  David Steurer. Fast sdp algorithms for constraint satisfaction problems.  In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 684-697. SIAM, 2010.  "}, "Mixing Implies Lower Bounds for Space Bounded Learning": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Mixing Implies Lower Bounds for Space Bounded Learning", "abstract": "One can learn any hypothesis class H with O(log |H|) labeled examples. Alas, learning with so few examples requires saving the examples in memory, and this requires |X|^(O(log|H|)) memory states, where X is the set of all labeled examples. This motivates the question of how many labeled examples are needed in case the memory is bounded. Previous work showed, using techniques such as linear algebra and Fourier analysis, that parities cannot be learned with bounded memory and less than |H|^(Omega(1)) examples. One might wonder whether a general combinatorial condition exists for unlearnability with bounded memory, as we have with the condition  VCdim(H) = Infinity for PAC unlearnability. In this paper we give such a condition. We show that if an hypothesis class H, when viewed as a bipartite graph between hypotheses H and labeled examples X, is mixing, then learning it requires |H|^(Omega(1)) examples under a certain bound on the memory. Note that the class of parities is mixing. Moreover, as an immediate corollary, we get that most hypothesis classes are unlearnable with bounded memory. Our proof technique is combinatorial in nature and very different from previous analyses.", "pdf_url": "http://proceedings.mlr.press/v65/moshkovitz17a/moshkovitz17a.pdf", "keywords": ["Bounded space", "Lower bound", "Mixing", "PAC learning", "Time-space tradeoff", "VC-dimension"], "reference": "495-519, 2006.  M. Aigner. Turan\u2019s graph theorem. American Mathematical Monthly, 102(9):808-816, 1995.  Y. Bilu and N. Linial. Lifts, discrepancy and nearly optimal spectral gap. Combinatorica, 26(5):  S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. Bulletin of the  American Mathematical Society, 43(4):439-561, 2006.  G. Kol, R. Raz, and A. Tal. Time-space hardness of learning sparse parities. In Proc. 49th ACM  Symp. on Theory of Computing, 2017.  M. Krivelevich and B. Sudakov. Pseudo-random graphs. In More sets, graphs and numbers, pages  199-262. Springer, 2006.  F.J. MacWilliams and N.J.A Sloane. The theory of error correcting codes. Elsevier, 1977.  50   MOSHKOVITZ MOSHKOVITZ  A code C \u2286 {0, 1}n can be viewed as an hypothesis class HC: the hypotheses correspond to the codewords, the examples correspond to the n coordinates, and an hypothesis hc \u2208 HC is defined by hc(i) which is equal to the i-th coordinate of c. So the number of labeled examples X , is |X | = 2n. 2 \u2212 (cid:15), then the number of common neighbors of any two  If a code C has distance at least \u03b4 \u2265 1  hypotheses is at most  (1 \u2212 \u03b4)n = (1 \u2212 \u03b4)  \u2264  + (cid:15)  |X | 2  (cid:18) 1 2  (cid:19) |X | 2  =  (cid:18) 1 2  (cid:19)2  |X | + (cid:15)  |X | 2  Denote \u00b5 = (cid:15) |X | codewords from C as hypotheses) and then we can bound the mixing parameter by  2 . To use Theorem 25 we need to make sure that |C| \u2265 2n (and take only 2n  (cid:115)(cid:18) 1 2  + \u00b5  (cid:19) |X | 2  = (cid:112)|X | \u00b7  (cid:114)  1 4  +  (cid:15)|X | 4  Summing up the discussion so far, using Theorem 23 with the mixing parameter d2 = |X | and |H| = |X | = 2n, we have the following theorem.  4 (1+(cid:15)|X |)  Theorem 28 For any code C \u2286 {0, 1}n with |C| = 2n and relative distance at least 1 constant s \u2208 (0, 1), if the number of memory states is bounded by  2 \u2212 (cid:15) and any  (cid:18) 4|C|  (cid:19)1.25  1 + 2(cid:15)n  \u00b7  1 (5 + 8(cid:15)n)1.25 |C|s  ,  then any learning algorithm for HC that outputs the underlying hypothesis (or an approximation of it) with probability at least 1/3 must use at least |C|s/130 examples.  Note that the theorem is useful for codes that have very small rate but very high distance (for  reference, see, e.g., MacWilliams and Sloane (1977)).  References  495-519, 2006.  M. Aigner. Turan\u2019s graph theorem. American Mathematical Monthly, 102(9):808-816, 1995.  Y. Bilu and N. Linial. Lifts, discrepancy and nearly optimal spectral gap. Combinatorica, 26(5):  S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. Bulletin of the  American Mathematical Society, 43(4):439-561, 2006.  G. Kol, R. Raz, and A. Tal. Time-space hardness of learning sparse parities. In Proc. 49th ACM  Symp. on Theory of Computing, 2017.  M. Krivelevich and B. Sudakov. Pseudo-random graphs. In More sets, graphs and numbers, pages  199-262. Springer, 2006.  F.J. MacWilliams and N.J.A Sloane. The theory of error correcting codes. Elsevier, 1977. MIXING IMPLIES LOWER BOUNDS FOR SPACE BOUNDED LEARNING  D. Moshkovitz and M. Moshkovitz. Mixing implies strong lower bounds for space bounded learn-  ing. Manuscript, 2017.  R. Raz. Fast learning requires good memory: A time-space lower bound for parity learning. In  Proc. 57th IEEE Symp. on Foundations of Computer Science, 2016.  R. Raz. A time-space lower bound for a large class of learning problems. Technical report, ECCC  Report TR17-020, 2017.  O. Shamir. Fundamental limits of online and distributed algorithms for statistical learning and esti- mation. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS\u201914, pages 163-171, 2014.  J. Steinhardt, G. Valiant, and S. Wager. Memory, communication, and statistical queries. In Con-  ference on Learning Theory (COLT), 2016.  A. Thomason. Dense expanders and pseudo-random bipartite graphs. Discrete Mathematics, 75  (1-3):381-386, 1989.  LG. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984. "}, "Fast rates for online learning in Linearly Solvable Markov Decision Processes": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Fast rates for online learning in Linearly Solvable Markov Decision Processes", "abstract": "We study the problem of online learning in a class of Markov decision processes known as \\emphlinearly solvable MDPs. In the stationary version of this problem, a learner interacts with its environment by directly controlling the state transitions, attempting to balance a fixed state-dependent cost and a certain smooth cost penalizing extreme control inputs. In the current paper, we consider an online setting where the state costs may change arbitrarily between consecutive rounds, and the learner only observes the costs at the end of each respective round. We are interested in constructing algorithms for the learner that guarantee small regret against the best stationary control policy chosen in full knowledge of the cost sequence. Our main result is showing that the smoothness of the control cost enables the simple algorithm of \\emphfollowing the leader to achieve a regret of order $\\log^2 T$ after $T$ rounds, vastly improving on the best known regret bound of order $T^3/4$ for this setting.", "pdf_url": "http://proceedings.mlr.press/v65/neu17a/neu17a.pdf", "keywords": ["Online learning", "fast rates", "Markov decision processes", "optimal control"], "reference": "Y. Abbasi-Yadkori, P. L. Bartlett, X. Chen, and A. Malek. Large-scale Markov decision problems with KL control cost and its application to crowdsourcing. In 32nd International Conference on Machine Learning (ICML) 2015, pages 1053-1062, 2015.  Y. Abbasi-Yadkori and Cs. Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic  systems. In COLT, 2011.  369-377, 2014.  Y. Abbasi-Yadkori, P. Bartlett, and V. Kanade. Tracking adversarial targets. In ICML 2014, pages  Y. Ariki, T. Matsubara, and S. H. Hyon. Latent Kullback-Leibler control for dynamic imitation In 2016 IEEE-RAS 16th International  learning of whole-body behaviors in humanoid robots. Conference on Humanoid Robots (Humanoids), pages 946-951, 2016.  P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In UAI 2009, 2009.  D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, Bel-  mont, MA, 3 edition, 2007.  MA, 1996.  New York, NY, USA, 2006.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. Accepted to the Journal of Machine Learning Research, 2014.  T. Dick, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. Online learning in markov decision processes with chang-  ing cost sequences. In ICML 2014, 2014.  K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable mdps. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 335-342, 2010.  E. Even-Dar, S. M. Kakade, and Y. Mansour. Online Markov decision processes. Mathematics of  Operations Research, 34(3):726-736, 2009.  V. G\u00b4omez, H. J. Kappen, J. Peters, and G. Neumann. Policy search for path integral control. Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases, 8724 LNAI (PART 1):482-497, 2014.  V. G\u00b4omez, S. Thijssen, A. C. Symington, S. Hailes, and H. J. Kappen. Real-time stochastic opti- mal control for multi-agent quadrotor systems. In 26th International Conference on Automated Planning and Scheduling, 2016.  P. Guan, M. Raginsky, and R. M. Willett. Online markov decision processes with kullback-leibler  control cost. Automatic Control, IEEE Transactions on, 59(6):1423-1438, 2014.  13   FAST RATES FOR ONLINE LEARNING IN LMDPS  References  Y. Abbasi-Yadkori, P. L. Bartlett, X. Chen, and A. Malek. Large-scale Markov decision problems with KL control cost and its application to crowdsourcing. In 32nd International Conference on Machine Learning (ICML) 2015, pages 1053-1062, 2015.  Y. Abbasi-Yadkori and Cs. Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic  systems. In COLT, 2011.  369-377, 2014.  Y. Abbasi-Yadkori, P. Bartlett, and V. Kanade. Tracking adversarial targets. In ICML 2014, pages  Y. Ariki, T. Matsubara, and S. H. Hyon. Latent Kullback-Leibler control for dynamic imitation In 2016 IEEE-RAS 16th International  learning of whole-body behaviors in humanoid robots. Conference on Humanoid Robots (Humanoids), pages 946-951, 2016.  P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning  in weakly communicating MDPs. In UAI 2009, 2009.  D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, Bel-  mont, MA, 3 edition, 2007.  MA, 1996.  New York, NY, USA, 2006.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont,  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. Accepted to the Journal of Machine Learning Research, 2014.  T. Dick, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. Online learning in markov decision processes with chang-  ing cost sequences. In ICML 2014, 2014.  K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable mdps. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 335-342, 2010.  E. Even-Dar, S. M. Kakade, and Y. Mansour. Online Markov decision processes. Mathematics of  Operations Research, 34(3):726-736, 2009.  V. G\u00b4omez, H. J. Kappen, J. Peters, and G. Neumann. Policy search for path integral control. Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases, 8724 LNAI (PART 1):482-497, 2014.  V. G\u00b4omez, S. Thijssen, A. C. Symington, S. Hailes, and H. J. Kappen. Real-time stochastic opti- mal control for multi-agent quadrotor systems. In 26th International Conference on Automated Planning and Scheduling, 2016.  P. Guan, M. Raginsky, and R. M. Willett. Online markov decision processes with kullback-leibler  control cost. Automatic Control, IEEE Transactions on, 59(6):1423-1438, 2014. NEU AND G \u00b4OMEZ  E. Hazan. The convex optimization approach to regret minimization. In S. Sra, S. Nowozin, and  S. Wright, editors, Optimization for Machine Learning, pages 287-303. MIT press, 2011.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69:169-192, 2007.  E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimization,  2(3-4):157-325, 2016.  T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal  of Machine Learning Research, 99:1563-1600, August 2010. ISSN 1532-4435.  H. J. Kappen. Linear theory for control of nonlinear stochastic systems. Physical review letters, 95  (20):200201, 2005.  H. J. Kappen, V. G\u00b4omez, and M. Opper. Optimal control as a graphical model inference problem.  Machine learning, 87(2):159-182, 2012.  K. Kinjo, E. Uchibe, and K. Doya. Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task. Frontiers in Neurorobotics, 7:1-13, 2013.  J. Kivinen and M. Warmuth. Averaging expert predictions.  In Proceedings of the Fourth Euro- pean Conference on Computational Learning Theory, pages 153-167. Lecture Notes in Artificial Intelligence, Vol. 1572. Springer, 1999.  W. Kot\u0142owski. On minimaxity of follow the leader strategy in the stochastic setting. In International  Conference on Algorithmic Learning Theory, pages 261-275, 2016.  T. Matsubara, V. G\u00b4omez, and H. J. Kappen. Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models. 30th Conference on Uncertainty in Artificial Intel- ligence (UAI), 2014.  N. Merhav and M. Feder. Universal sequential learning and decision from individual data sequences. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory. ACM Press, 1992.  C. D. Meyer. Matrix analysis and applied linear algebra, volume 2. Siam, 2000.  G. Neu, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. The online loop-free stochastic shortest-path problem. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 231-243, 2010.  G. Neu, A. Gy\u00a8orgy, and Cs. Szepesv\u00b4ari. The adversarial stochastic shortest path problem with  unknown transition probabilities. In AISTATS 2012, pages 805-813, 2012.  G. Neu, A. Gy\u00a8orgy, Cs. Szepesv\u00b4ari, and A. Antos. Online Markov decision processes under bandit  feedback. IEEE Transactions on Automatic Control, 59:676-691, 2014.  M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-  Interscience, April 1994. FAST RATES FOR ONLINE LEARNING IN LMDPS  E. Rombokas, M. Malhotra, E. A. Theodorou, E. Todorov, and Y. Matsuoka. Reinforcement learning and synergistic control of the act hand. IEEE/ASME Transactions on Mechatronics, 18(2):569- 577, 2013.  A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in online optimization. In NIPS-27, pages  810-818, 2014.  E. Seneta. Non-negative matrices and Markov chains. Springer Science & Business Media, 2006.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012.  R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  Cs. Szepesv\u00b4ari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelli-  gence and Machine Learning. Morgan & Claypool Publishers, 2010.  D. Thalmeier, V. G\u00b4omez, and H. J. Kappen. Action selection in growing state spaces: control of network structure growth. Journal of Physics A: Mathematical and Theoretical, 50(3):034006, 2017.  E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforce-  ment learning. Journal of Machine Learning Research, 11:3137-3181, 2010.  E. Todorov. Linearly-solvable Markov decision problems. In NIPS-18, pages 1369-1376, 2006.  ISBN 0-262-23253-7.  E. Todorov. General duality between optimal control and estimation.  In Decision and Control,  2008. CDC 2008. 47th IEEE Conference on, pages 4286-4292. IEEE, 2008.  E. Todorov. Compositionality of optimal control laws. In NIPS-22, pages 1856-1864, 2009.  E. Todorov. Policy gradients in linearly-solvable mdps. In NIPS-23, pages 2298-2306. CURRAN,  2010.  T. van Erven, P. D. Gr\u00a8unwald, N. A. Mehta, M. D. Reid, and R. C. Williamson. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16:1793-1861, 2015.  G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou. Aggressive driving with model predictive path integral control. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 1433-1440, May 2016. doi: 10.1109/ICRA.2016.7487277.  J. Y. Yu, S. Mannor, and N. Shimkin. Markov decision processes with arbitrary reward processes.  Mathematics of Operations Research, 34(3):737-757, 2009.  A. Zimin and G. Neu. Online learning in episodic Markovian decision processes by relative entropy  policy search. In NIPS-26, pages 1583-1591, 2013. NEU AND G \u00b4OMEZ  "}, "Sample complexity of population recovery": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Sample complexity of population recovery", "abstract": "The problem of population recovery refers to estimating a distribution based on incomplete or corrupted samples.  Consider a random poll of sample size $n$ conducted on a population of individuals, where each pollee is asked to answer $d$ binary questions.  We consider one of the two polling impediments: \\beginitemize \\item in lossy population recovery, a pollee may skip each question with probability $\u03b5$; \\item in noisy population recovery, a pollee may lie on each question with probability $\u03b5$. \\enditemize Given $n$ lossy or noisy samples, the goal is to estimate the probabilities of all $2^d$ binary vectors simultaneously within accuracy $\u03b4$ with high probability. This paper settles the sample complexity of population recovery.  For lossy model, the optimal sample complexity is $\\tilde\u0398(\u03b4^ -2\\max{\\frac\u03b51-\u03b5,1})$, improving the state of the art by Moitra and Saks in several ways: a lower bound is established, the upper bound is improved and the result is dimension-free. Surprisingly, the sample complexity undergoes a phase transition from parametric to nonparametric rate when $\u03b5$ exceeds $1/2$. For noisy population recovery, the sharp sample complexity turns out to be dimension-dependent and scales as $\\exp(\u0398(d^1/3 \\log^2/3(1/\u03b4)))$ except for the trivial cases of $\u03b5=0,1/2$ or $1$. For both models, our estimators simply compute the empirical mean of a certain function, which is found by pre-solving a linear program (LP). Curiously, the dual LP can be understood as Le Cam\u2019s method for lower-bounding the minimax risk, thus establishing the statistical optimality of the proposed estimators. The value of the LP is determined by complex-analytic methods.", "pdf_url": "http://proceedings.mlr.press/v65/polyanskiy17a/polyanskiy17a.pdf", "keywords": [], "reference": "M. Abramowitz and I. A. Stegun. Handbook of mathematical functions with formulas,  graphs, and mathematical tables. Wiley-Interscience, New York, NY, 1964.  Lucia Batman, Russell Impagliazzo, Cody Murray, and Ramamohan Paturi. Finding heavy hitters from lossy or noisy data. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 347-362. Springer, 2013.  Peter Borwein and Tam\u00b4as Erd\u00b4elyi. Littlewood-type problems on subarcs of the unit circle.  Indiana University Mathematics Journal, 46(4):1323, 1997.  T Tony Cai and Mark G Low. Nonquadratic estimators of a quadratic functional. The  Annals of Statistics, pages 2930-2956, 2005.  Carl C Cowen. Linear fractional composition operators on H 2.  Integral equations and  operator theory, 11(2):151-160, 1988.  Anindya De, Ryan O\u2019Donnell, and Rocco Servedio. Optimal mean-based algorithms for  trace reconstruction. arXiv preprint arXiv:1612.03148, 2016a.  Anindya De, Michael Saks, and Sijian Tang. Noisy population recovery in polynomial time.  arXiv preprint arXiv:1602.07616, 2016b.  Anindya De, Ryan O\u2019Donnell, and Rocco Servedio. Sharp bounds for population recovery.  arXiv preprint arXiv:1703.01474, 2017.  Zeev Dvir, Anup Rao, Avi Wigderson, and Amir Yehudayo\ufb00. Restriction access. In Pro- ceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 19-33. ACM, 2012.  Jianqing Fan, Philippe Rigollet, and Weichen Wang. Estimation of functionals of sparse  covariance matrices. The Annals of Statistics, 43(6):2706, 2015.  Jon Feldman, Ryan O\u2019Donnell, and Rocco A Servedio. Learning mixtures of product dis- tributions over discrete domains. SIAM Journal on Computing, 37(5):1536-1564, 2008.  I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals Series and Products. Academic Press,  New York, NY, seventh edition, 2007.  I.A. Ibragimov, A.S. Nemirovskii, and R.Z. Khas\u2019minskii. Some problems on nonparametric estimation in gaussian white noise. Theory of Probability & Its Applications, 31(3):391- 406, 1987.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. On the learnability of discrete distributions. In Proceedings of the twenty- sixth annual ACM symposium on Theory of computing, pages 273-282. ACM, 1994.  Lucien Le Cam. Asymptotic methods in statistical decision theory. Springer-Verlag, New  York, NY, 1986.  29   Sample complexity of population recovery  References  M. Abramowitz and I. A. Stegun. Handbook of mathematical functions with formulas,  graphs, and mathematical tables. Wiley-Interscience, New York, NY, 1964.  Lucia Batman, Russell Impagliazzo, Cody Murray, and Ramamohan Paturi. Finding heavy hitters from lossy or noisy data. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 347-362. Springer, 2013.  Peter Borwein and Tam\u00b4as Erd\u00b4elyi. Littlewood-type problems on subarcs of the unit circle.  Indiana University Mathematics Journal, 46(4):1323, 1997.  T Tony Cai and Mark G Low. Nonquadratic estimators of a quadratic functional. The  Annals of Statistics, pages 2930-2956, 2005.  Carl C Cowen. Linear fractional composition operators on H 2.  Integral equations and  operator theory, 11(2):151-160, 1988.  Anindya De, Ryan O\u2019Donnell, and Rocco Servedio. Optimal mean-based algorithms for  trace reconstruction. arXiv preprint arXiv:1612.03148, 2016a.  Anindya De, Michael Saks, and Sijian Tang. Noisy population recovery in polynomial time.  arXiv preprint arXiv:1602.07616, 2016b.  Anindya De, Ryan O\u2019Donnell, and Rocco Servedio. Sharp bounds for population recovery.  arXiv preprint arXiv:1703.01474, 2017.  Zeev Dvir, Anup Rao, Avi Wigderson, and Amir Yehudayo\ufb00. Restriction access. In Pro- ceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 19-33. ACM, 2012.  Jianqing Fan, Philippe Rigollet, and Weichen Wang. Estimation of functionals of sparse  covariance matrices. The Annals of Statistics, 43(6):2706, 2015.  Jon Feldman, Ryan O\u2019Donnell, and Rocco A Servedio. Learning mixtures of product dis- tributions over discrete domains. SIAM Journal on Computing, 37(5):1536-1564, 2008.  I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals Series and Products. Academic Press,  New York, NY, seventh edition, 2007.  I.A. Ibragimov, A.S. Nemirovskii, and R.Z. Khas\u2019minskii. Some problems on nonparametric estimation in gaussian white noise. Theory of Probability & Its Applications, 31(3):391- 406, 1987.  Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda Sellie. On the learnability of discrete distributions. In Proceedings of the twenty- sixth annual ACM symposium on Theory of computing, pages 273-282. ACM, 1994.  Lucien Le Cam. Asymptotic methods in statistical decision theory. Springer-Verlag, New  York, NY, 1986. Polyanskiy Suresh Wu  Jian Li, Yuval Rabani, Leonard J Schulman, and Chaitanya Swamy. Learning arbitrary statistical mixtures of discrete distributions. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 743-752. ACM, 2015.  Karim Lounici. High-dimensional covariance matrix estimation with missing observations.  Bernoulli, 20(3):1029-1058, 2014.  Shachar Lovett and Jiapeng Zhang.  Bonami-Beckner inequality for sparse functions. Annual ACM on Symposium on Theory of Computing, pages 137-142. ACM, 2015.  Improved noisy population recovery, and reverse In Proceedings of the Forty-Seventh  Ankur Moitra and Michael Saks. A polynomial time algorithm for lossy population recovery. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 110-116. IEEE, 2013.  F. Nazarov and Y. Peres. Trace reconstruction with exp(O(n1/3)) samples. arXiv preprint  arXiv:1612.03599, 2016.  DJ Newman. A simple proof of Wieners 1/f theorem. Proceedings of the American Math-  ematical Society, 48(1):264-265, 1975.  Alon Orlitsky, Ananda Theertha Suresh, and Yihong Wu. Optimal prediction of the number of unseen species. Proceedings of the National Academy of Sciences (PNAS), 113(47): 13283-13288, 2016.  Barry Simon. Convexity: An analytic viewpoint. Cambridge University Press, 2011.  G. Szeg\u00a8o. Orthogonal polynomials. American Mathematical Society, Providence, RI, 4th  edition, 1975.  NY, 2009.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Verlag, New York,  Avi Wigderson and Amir Yehudayo\ufb00. Population recovery and partial identification. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 390-399. IEEE, 2012. "}, "Exact tensor completion with sum-of-squares": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Exact tensor completion with sum-of-squares", "abstract": "We obtain the first polynomial-time algorithm for exact tensor completion that improves over the bound implied by reduction to matrix completion. The algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal components in $\\mathbb R^n$ from $r\u22c5\\tilde O(n^1.5)$ randomly observed entries of the tensor. This bound improves over the previous best one of $r\u22c5\\tilde O(n^2)$ by reduction to exact matrix completion. Our bound also matches the best known results for the easier problem of approximate tensor completion (Barak & Moitra, 2015). Our algorithm and analysis extends seminal results for exact matrix completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares method. The main technical challenge is to show that a small number of randomly chosen monomials are enough to construct a degree-3 polynomial with precisely planted orthogonal global optima over the sphere and that this fact can be certified within the sum-of-squares proof system.", "pdf_url": "http://proceedings.mlr.press/v65/potechin17a/potechin17a.pdf", "keywords": ["tensor completion", "sum-of-squares method", "semidefinite programming", "exact recovery", "matrix polynomials", "matrix norm bounds"], "reference": "Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In COLT, volume 49 of JMLR Workshop and Conference Proceedings, pages 417-445. JMLR.org, 2016.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  Electronic Colloquium on Computational Complexity (ECCC), 21:59, 2014.  Boaz Barak, Fernando G. S. L. Brand\u00e3o, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In STOC, pages 307-326. ACM, 2012.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations. In STOC,  pages 31-40. ACM, 2014.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition  via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. CoRR, abs/1604.03084, 2016.  Vijay V. S. P. Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Certifying random polynomials  over the unit sphere via sum of squares hierarchy. CoRR, abs/1605.00903, 2016.  Srinadh Bhojanapalli and Sujay Sanghavi. A new sampling technique for tensors. CoRR,  abs/1502.05023, 2015.  Emmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational Mathematics, 9(6):717-772, 2009.  37   Exact tensor completion with sum-of-squares  The analysis is the same for the PUV , PUW , and PVW cases except for the following di\ufb00er- (cid:3) (cid:2) , but still makes it much less than we had for tr((YY T )q)  ences which increase the bound on E E  (cid:2) tr((Y4Y T  (cid:3) .  4 )q)  1. In the PVW case there are now two global indices, one for the top of the outer hourglasses and one for the bottom of the outer hourglasses. This gives us a global factor of r 2 rather than r.  2. Since one of the outer indices is now merged with the corresponding inner index, instead of the (cid:17) 2q for the inner indices, we will only by a factor of at most  UVW terms giving us factors of (cid:16) \u00b5 have two of these factors. This increases our bound on E (cid:16) nma x \u00b5  (cid:2) tr((YY T )q)  (cid:17) 2q, and (cid:16) \u00b5  (cid:17) 2q, (cid:16) \u00b5  (cid:17) 2q  n1  n2  n3  (cid:3)  Putting these di\ufb00erences together, our bound will now be (cid:16) \u02dcO( n1n2n3nma x B (cid:2) tr((Y4Y T less than the bound we had for E  m\u00b5  (cid:3) 4 )q)  .  )(cid:17) 2q  r 2 which is still much  References  Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In COLT, volume 49 of JMLR Workshop and Conference Proceedings, pages 417-445. JMLR.org, 2016.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  Electronic Colloquium on Computational Complexity (ECCC), 21:59, 2014.  Boaz Barak, Fernando G. S. L. Brand\u00e3o, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In STOC, pages 307-326. ACM, 2012.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations. In STOC,  pages 31-40. ACM, 2014.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition  via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  Boaz Barak, Samuel B. Hopkins, Jonathan A. Kelner, Pravesh Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. CoRR, abs/1604.03084, 2016.  Vijay V. S. P. Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Certifying random polynomials  over the unit sphere via sum of squares hierarchy. CoRR, abs/1605.00903, 2016.  Srinadh Bhojanapalli and Sujay Sanghavi. A new sampling technique for tensors. CoRR,  abs/1502.05023, 2015.  Emmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization.  Foundations of Computational Mathematics, 9(6):717-772, 2009. Potechin Steurer  Emmanuel J. Cand\u00e8s and Terence Tao. The power of convex relaxation: near-optimal matrix  completion. IEEE Trans. Information Theory, 56(5):2053-2080, 2010.  Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. Theory of  Computing, 3(1):25-43, 2007.  Michael A. Forbes and Amir Shpilka. On identity testing of tensors, low-rank recovery and compressed  sensing. In STOC, pages 163-172. ACM, 2012.  Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. In APPROX-RANDOM, volume 40 of LIPIcs, pages 829-849. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2015.  Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. CoRR,  Dima Grigoriev. Complexity of positivstellensatz proofs for the knapsack. Computational Complexity,  abs/1605.07272, 2016.  10(2):139-154, 2001a.  Dima Grigoriev. Linear lower bound on degrees of positivstellensatz calculus proofs for the parity.  Theor. Comput. Sci., 259(1-2):613-622, 2001b.  Dima Grigoriev and Nicolai Vorobjov. Complexity of null-and positivstellensatz proofs. Ann. Pure  Appl. Logic, 113(1-3):153-160, 2001.  David Gross. Recovering low-rank matrices from few coe\ufb03cients in any basis.  IEEE Trans.  Information Theory, 57(3):1548-1566, 2011.  Moritz Hardt. Understanding alternating minimization for matrix completion. In FOCS, pages  651-660. IEEE Computer Society, 2014.  Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In COLT, volume 35 of JMLR Workshop and Conference Proceedings, pages 638-678. JMLR.org, 2014.  Elad Hazan and Tengyu Ma. A non-generative framework and convex relaxations for unsupervised  learning. CoRR, abs/1610.01132, 2016.  Samuel B. Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via sum-of-square proofs. In COLT, volume 40 of JMLR Workshop and Conference Proceedings, pages 956-1006. JMLR.org, 2015.  Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In STOC, pages 178-191. ACM, 2016.  Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. In COLT, volume 40 of JMLR Workshop and Conference Proceedings, pages 1007-1034. JMLR.org, 2015.  Prateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In NIPS, pages  1431-1439, 2014. Exact tensor completion with sum-of-squares  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating  minimization. In STOC, pages 665-674. ACM, 2013.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. In NIPS, pages 952-960. Curran Associates, Inc., 2009.  Subhash Khot. On the power of unique 2-prover 1-round games. In STOC, pages 767-775. ACM,  2002.  Jean B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM J. Optim., 11(3):796-817, 2000/01. ISSN 1052-6234. doi: 10.1137/S1052623400366802. URL http://dx.doi.org/10.1137/S1052623400366802.  Tengyu Ma and Avi Wigderson. Sum-of-squares lower bounds for sparse PCA. In NIPS, pages  1612-1620, 2015.  Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-  squares. CoRR, abs/1610.01980, 2016.  Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness  and optimization. PhD thesis, California Institute of Technology, 2000.  Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random csps below the  spectral threshold. CoRR, abs/1605.00058, 2016.  Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,  12:3413-3430, 2011.  N. Z. Shor. An approach to obtaining global extrema in polynomial problems of mathematical  programming. Kibernetika (Kiev), (5):102-106, 136, 1987. ISSN 0023-1274.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In COLT, volume 3559 of  Lecture Notes in Computer Science, pages 545-560. Springer, 2005.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  Acknowledgments  A. P. is supported by the Simons Collaboration for Algorithms and Geometry and by the NSF under agreement No. CCF-1412958. Part of this work was done while A. P. was at Cornell University.  D. S. is supported by a Microsoft Research Fellowship, a Alfred P. Sloan Fellowship, NSF awards (CCF-1408673,CCF-1412958,CCF-1350196), and the Simons Collaboration for Algorithms and Geometry. Potechin Steurer  "}, "Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis", "abstract": "Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic Gradient Descent, where properly scaled isotropic Gaussian noise is added to an unbiased estimate of the gradient at each iteration. This modest change allows SGLD to escape local minima and suffices to guarantee asymptotic convergence to global minimizers for sufficiently regular non-convex objectives. The present work provides a nonasymptotic analysis in the context of non-convex learning problems, giving finite-time guarantees for SGLD to find approximate minimizers of both empirical and population risks. As in the asymptotic setting, our analysis relates the discrete-time SGLD Markov chain to a continuous-time diffusion process. A new tool that drives the results is the use of weighted transportation cost inequalities to quantify the rate of convergence of SGLD to a stationary distribution in the Euclidean $2$-Wasserstein distance.", "pdf_url": "http://proceedings.mlr.press/v65/raginsky17a/raginsky17a.pdf", "keywords": [], "reference": "A. Alfonsi, B. Jourdain, and A. Kohatsu-Higa. Optimal transport bounds between the time- marginals of a multidimensional diffusion and its Euler scheme. Electron. J. Probab., 20, 2015. paper no. 70.  D. Bakry, F. Barthe, P. Cattiaux, and A. Guillin. A simple proof of the Poincar\u00b4e inequality for a large class of probability measures including the log-concave case. Electron. Comm. Probab., 13: 60-66, 2008.  D. Bakry, I. Gentil, and M. Ledoux. Analysis and Geometry of Markov Diffusion Operators.  Springer, 2014.  J.-B. Bardet, N. Gozlan, F. Malrieu, and P.-A. Zitt. Functional inequalities for Gaussian convolu- tions of compactly supported measures: explicit bounds and dimension dependence, 2015. URL http://arxiv.org/abs/1507.02389. To appear in Bernoulli.  A. Belloni, T. Liang, H. Narayanan, and A. Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In COLT, pages 240-265, 2015.  16   RAGINSKY RAKHLIN TELGARSKY  spectral gap due to Bovier et al. (2005).) For example, consider replacing the empirical risk (1.2) with a smoothed objective  \u02dcFz(w) = \u2212  log  (cid:90)  e\u2212\u03b2\u03b3(cid:107)v\u2212w(cid:107)2/2e\u2212\u03b2Fz(v)dv  1 \u03b2  \u03b3 2  {(cid:107)v(cid:107)\u2264R} (cid:90)  1 \u03b2  {(cid:107)v(cid:107)\u2264R}  =  (cid:107)w(cid:107)2 \u2212  log  e\u03b2\u03b3(cid:104)v,w(cid:105)\u2212\u03b2\u03b3(cid:107)v(cid:107)2/2e\u2212\u03b2Fz(v)dv,  and running SGLD with \u2207 \u02dcFz instead of \u2207Fz. Here, \u03b3 > 0 and R > 0 are tunable parameters. This modification is closely related to the Entropy-SGD method, recently proposed by Chaudhari et al. (2016). Observe that the modified Gibbs measures \u02dc\u03c0z(dw) \u221d e\u2212\u03b2 \u02dcFz(w) are convolutions of a Gaussian measure and a compactly supported probability measure. In this case, it follows from the results of Bardet et al. (2015) that  1 \u03bb\u2217  \u2264  1 \u03b2\u03b3  e4\u03b2\u03b3R2  .  Note that here, in contrast with (4.3), this bound is completely dimension-free. A tantalizing line of future work is, therefore, to find other settings where 1/\u03bb\u2217 is indeed small.  Acknowledgments  The authors would like to thank Arnak Dalalyan and Ramon van Handel for enlightening discus- sions. The work of M.R. was supported in part by the NSF under CAREER award CCF-1254041, and in part by the Center for Science of Information (CSoI), an NSF Science and Technology Cen- ter, under grant agreement CCF-0939370. The work of A.R. was supported in part by the NSF under grant no. CDS&E-MSS 1521529.  References  A. Alfonsi, B. Jourdain, and A. Kohatsu-Higa. Optimal transport bounds between the time- marginals of a multidimensional diffusion and its Euler scheme. Electron. J. Probab., 20, 2015. paper no. 70.  D. Bakry, F. Barthe, P. Cattiaux, and A. Guillin. A simple proof of the Poincar\u00b4e inequality for a large class of probability measures including the log-concave case. Electron. Comm. Probab., 13: 60-66, 2008.  D. Bakry, I. Gentil, and M. Ledoux. Analysis and Geometry of Markov Diffusion Operators.  Springer, 2014.  J.-B. Bardet, N. Gozlan, F. Malrieu, and P.-A. Zitt. Functional inequalities for Gaussian convolu- tions of compactly supported measures: explicit bounds and dimension dependence, 2015. URL http://arxiv.org/abs/1507.02389. To appear in Bernoulli.  A. Belloni, T. Liang, H. Narayanan, and A. Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In COLT, pages 240-265, 2015. STOCHASTIC GRADIENT LANGEVIN DYNAMICS: A NONASYMPTOTIC ANALYSIS  F. Bolley and C. Villani. Weighted Csisz\u00b4ar-Kullback-Pinsker inequalities and applications to trans- portation inequalities. Annales de la Facult\u00b4e des Science de Toulouse, XIV(3):331-352, 2005.  V. S. Borkar and S. K. Mitter. A strong approximation theorem for stochastic recursive algorithms.  Journal of Optimization Theory and Applications, 100(3):499-513, 1999.  O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  2(Mar):499-526, 2002.  A. Bovier, V. Gayrard, and M. Klein. Metastability in reversible diffusion processes II. Precise  asymptotics for small eigenvalues. J. Eur. Math. Soc., 7:69-99, 2005.  S. Bubeck, R. Eldan, and J. Lehec. Sampling from a log-concave distribution with Projected Langevin Monte Carlo. arXiv preprint 1507.02564, 2015. URL http://arxiv.org/abs/ 1507/02564.  P. Cattiaux, A. Guillin, and L. Wu. A note on Talagrand\u2019s transportation inequality and logarithmic  Sobolev inequality. Prob. Theory Rel. Fields, 148:285-334, 2010.  P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and R. Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. arXiv preprint 1611.01838, 2016. URL http://arxiv.org/abs/1611.01838.  T.-S. Chiang, C.-R. Hwang, and S.-J. Sheu. Diffusion for global optimization in Rn. SIAM Journal  on Control and Optimization, 25(3):737-753, 1987.  T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, 2nd edition,  2006.  A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave  densities. J. Roy. Stat. Soc. Ser. B, 2016. To appear.  A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin Monte  Carlo. J. Comp. Sys. Sci., 78(1423-1443), 2012.  H. Djellout, A. Guillin, and L. Wu. Transportation cost-information inequalities and applications to random dynamical systems and diffusions. Annals of Probability, 32(3B):2702-2732, 2004.  A. Durmus and E. Moulines. Non-asymptotic convergence analysis for the unadjusted langevin al- gorithm. arXiv preprint 1507.05021, 2015. URL http://arxiv.org/abs/1507.05021.  R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points-online stochastic gradient for  tensor decomposition. In COLT, pages 797-842, 2015.  S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in Rd. SIAM  Journal on Control and Optimization, 29(5):999-1018, 1991.  I. Gy\u00a8ongy. Mimicking the one-dimensional marginal distributions of processes having an Ito differ-  ential. Prob. Theory Rel. Fields, 71:501-516, 1986.  J. K. Hale. Asymptotic Behavior of Dissipative Systems. American Mathematical Society, 1988. RAGINSKY RAKHLIN TELGARSKY  M Hardt, B Recht, and Y Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint 1509.01240, 2015. URL http://arxiv.org/abs/1509.01240.  E. Hazan, K. Levi, and S. Shalev-Shwartz. On graduated optimization for stochastic non-convex  problems. In ICML, 2016.  Probability, 8(1177-1182), 1980.  C.-R. Hwang. Laplace\u2019s method revisited: weak convergence of probability measures. Annals of  A. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances in Neural Information Processing Systems 4, pages 950-957. 1992.  R. S. Liptser and A. N. Shiryaev. Statistics of Random Processes I: General Theory. Springer, 2nd  edition, 2001.  ity, pages 1118-1139, 1997.  D. M\u00b4arquez. Convergence rates for annealing diffusion processes. The Annals of Applied Probabil-  S. Mukherjee, P. Niyogi, T. Poggio, and R. Rifkin. Learning theory: stability is sufficient for gener- alization and necessary and sufficient for consistency of empirical risk minimization. Advances in Computational Mathematics, 25(1):161-193, 2006.  Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer, 2004.  M. Pelletier. Weak convergence rates for stochastic approximation with application to multiple  targets and simulated annealing. Annals of Applied Probability, pages 10-44, 1998.  Y. Polyanskiy and Y. Wu. Wasserstein continuity of entropy and outer bounds for interference  channels. IEEE Trans. Inf. Theory, 62(7):3992-4002, July 2016.  A. Rakhlin, S. Mukherjee, and T. Poggio. Stability results in learning theory. Analysis and Appli-  cations, 3(04):397-417, 2005.  versity Press, 1996.  A. M. Stuart and A. R. Humphries. Dynamical Systems and Numerical Analysis. Cambridge Uni-  C. Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics. Amer.  Math. Soc., Providence, RI, 2003.  M. Welling and Y. W Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML,  pages 681-688, 2011. STOCHASTIC GRADIENT LANGEVIN DYNAMICS: A NONASYMPTOTIC ANALYSIS  "}, "On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities", "abstract": "We study an equivalence of (i) deterministic pathwise statements appearing in the online learning literature (termed \\emphregret bounds), (ii) high-probability tail bounds for the supremum of a collection of martingales (of a specific form arising from uniform laws of large numbers), and (iii) in-expectation bounds for the supremum. By virtue of the equivalence, we prove exponential tail bounds for norms of Banach space valued martingales via deterministic regret bounds for the online mirror descent algorithm with an adaptive step size. We show that the phenomenon extends beyond the setting of online linear optimization and present the equivalence for the supervised online learning setting.", "pdf_url": "http://proceedings.mlr.press/v65/rakhlin17a/rakhlin17a.pdf", "keywords": ["martingale inequalities", "online learning"], "reference": "J. Abernethy, A. Agarwal, P. Bartlett, and A. Rakhlin. A stochastic view of optimal regret In Proceedings of the 22th Annual Conference on Learning  through minimax duality. Theory, 2009.  B. Acciaio, M. Beiglbck, F. Penkner, W. Schachermayer, and J. Temme. A trajectorial interpretation of Doob\u2019s martingale inequalities. Ann. Appl. Probab., 23(4):1494-1505, 08 2013. URL http://dx.doi.org/10.1214/12-AAP878.  M. Beiglb\u00a8ock and M. Nutz. Martingale inequalities and deterministic counterparts. Electron.  M. Beiglb\u00a8ock and P. Siorpaes. Pathwise versions of the burkholder-davis-gundy inequality.  J. Probab, 19(95):1-15, 2014.  Bernoulli, 21(1):360-373, 2015.  2015.  B. Bercu, B. Delyon, and E. Rio. Concentration inequalities for sums and martingales,  J. Borwein, A. Guirao, P. H\u00b4ajek, and J. Vanderwer\ufb00. Uniformly convex functions on banach  spaces. Proceedings of the American Mathematical Society, 137(3):1081-1091, 2009.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic  theory of independence. Oxford University Press, 2013.  D. Burkholder. The best constant in the davis inequality for the expectation of the martin- gale square function. Transactions of the American Mathematical Society, 354(1):91-105, 2002.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  T. Cover. Behaviour of sequential predictors of binary sequences. In Proc. 4th Prague Conf.  Inform. Theory, Statistical Decision Functions, Random Processes, 1965.  V. H de la Pe\u02dcna, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and  Statistical Applications. Springer, 2008.  D. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning, 2015. In Submission.  D. A Freedman. On tail probabilities for martingales.  the Annals of Probability, pages  E. Gin\u00b4e and J. Zinn. Some limit theorems for empirical processes. Annals of Probability,  100-118, 1975.  12(4):929-989, 1984.  14   Rakhlin Sridharan  Research is supported in part by the NSF under grants no. CDS&E-MSS 1521529 and 1521544.  Acknowledgements  References  J. Abernethy, A. Agarwal, P. Bartlett, and A. Rakhlin. A stochastic view of optimal regret In Proceedings of the 22th Annual Conference on Learning  through minimax duality. Theory, 2009.  B. Acciaio, M. Beiglbck, F. Penkner, W. Schachermayer, and J. Temme. A trajectorial interpretation of Doob\u2019s martingale inequalities. Ann. Appl. Probab., 23(4):1494-1505, 08 2013. URL http://dx.doi.org/10.1214/12-AAP878.  M. Beiglb\u00a8ock and M. Nutz. Martingale inequalities and deterministic counterparts. Electron.  M. Beiglb\u00a8ock and P. Siorpaes. Pathwise versions of the burkholder-davis-gundy inequality.  J. Probab, 19(95):1-15, 2014.  Bernoulli, 21(1):360-373, 2015.  2015.  B. Bercu, B. Delyon, and E. Rio. Concentration inequalities for sums and martingales,  J. Borwein, A. Guirao, P. H\u00b4ajek, and J. Vanderwer\ufb00. Uniformly convex functions on banach  spaces. Proceedings of the American Mathematical Society, 137(3):1081-1091, 2009.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic  theory of independence. Oxford University Press, 2013.  D. Burkholder. The best constant in the davis inequality for the expectation of the martin- gale square function. Transactions of the American Mathematical Society, 354(1):91-105, 2002.  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University  Press, 2006.  T. Cover. Behaviour of sequential predictors of binary sequences. In Proc. 4th Prague Conf.  Inform. Theory, Statistical Decision Functions, Random Processes, 1965.  V. H de la Pe\u02dcna, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and  Statistical Applications. Springer, 2008.  D. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning, 2015. In Submission.  D. A Freedman. On tail probabilities for martingales.  the Annals of Probability, pages  E. Gin\u00b4e and J. Zinn. Some limit theorems for empirical processes. Annals of Probability,  100-118, 1975.  12(4):929-989, 1984. On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities  A. Gushchin. On pathwise counterparts of Doob\u2019s maximal inequalities. Proceedings of the  Steklov Institute of Mathematics, 1(287):118-121, 2014.  D. Panchenko. Symmetrization approach to concentration inequalities for empirical pro-  cesses. Annals of Probability, 31(4):2068-2081, 2003.  I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The  Annals of Probability, 22(4):1679-1706, 1994.  A. Rakhlin and K. Sridharan. On equivalence of martingale tail bounds and deterministic  regret inequalities. arXiv preprint arXiv:1510.03925, 2015.  A. Rakhlin and K. Sridharan. A tutorial on online supervised learning with applications to node classification in social networks. CoRR, abs/1608.09014, 2016. URL http: //arxiv.org/abs/1608.09014.  A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial parameters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984-1992, 2010.  A. Rakhlin, K. Sridharan, and A. Tewari. Online learning via sequential complexities.  Journal of Machine Learning Research, 2014.  N. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In  NIPS, pages 2645-2653, 2011.  A. W. Van Der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With  Applications to Statistics. Springer Series, March 1996.  "}, "Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization", "abstract": "In this paper, we consider the problem of sequentially optimizing a black-box function $f$ based on noisy samples and bandit feedback.  We assume that $f$ is smooth in the sense of having a bounded norm in some reproducing kernel Hilbert space (RKHS), yielding a commonly-considered non-Bayesian form of Gaussian process bandit optimization.  We provide algorithm-independent lower bounds on the simple regret, measuring the suboptimality of a single point reported after $T$ rounds, and on the cumulative regret, measuring the sum of regrets over the $T$ chosen points. For the isotropic squared-exponential kernel in $d$ dimensions, we find that an average simple regret of $\u03b5$ requires $T = \u03a9\\big(\\frac1\u03b5^2 (\\log\\frac1\u03b5)^d/2\\big)$, and the average cumulative regret is at least $\u03a9\\big( \\sqrt{T}(\\log T)^d \\big)$, thus matching existing upper bounds up to the replacement of $d/2$ by $d+O(1)$ in both cases.  For the Mat\u00e9rn-$\u03bd$ kernel, we give analogous bounds of the form $\u03a9\\big( (\\frac1\u03b5)^2+d/\u03bd\\big)$ and $\u03a9\\big( T^\\frac\u03bd+ d2\u03bd+ d \\big)$, and discuss the resulting gaps to the existing upper bounds.", "pdf_url": "http://proceedings.mlr.press/v65/scarlett17a/scarlett17a.pdf", "keywords": ["Gaussian processes", "bandits", "online optimization", "reproducing kernel Hilbert space", "lower bounds", "cumulative regret", "simple regret", "Bayesian optimization"], "reference": "N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc, 68(3):337\u2013404, 1950.  J.-Y. Audibert and S. Bubeck. Best arm identi\ufb01cation in multi-armed bandits. In Conf. Learning  Theory (COLT), 2010.  P. Auer, N. Cesa-Bianchi, Y. Freund, The  and R. E. Schapire. adversarial multi-armed bandit problem.  rigged casino: http://www.dklevine.com/archive/refs4462.pdf, 1998.  Gambling in a report, Technical  I. Bogunovic, J. Scarlett, A. Krause, and V. Cevher. Truncated variance reduction: A uni\ufb01ed ap- proach to Bayesian optimization and level-set estimation. In Conf. Neur. Inf. Proc. Sys. (NIPS), 2016.  S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-Armed  Bandit Problems. Found. Trend. Mach. Learn. Now Publishers, 2012.  S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Conf.  Alg. Learn. Theory (ALT), 2009.  (Oct.):2879\u20132904, 2011.  A. D. Bull. Convergence rates of ef\ufb01cient global optimization algorithms. J. Mach. Learn. Res., 12  19   LOWER BOUNDS ON REGRET FOR NOISY GAUSSIAN PROCESS BANDIT OPTIMIZATION  "}, "An Improved Parametrization and Analysis of the EXP3++ Algorithm for Stochastic and Adversarial Bandits": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "An Improved Parametrization and Analysis of the EXP3++ Algorithm for Stochastic and Adversarial Bandits", "abstract": "We present a new strategy for gap estimation in randomized algorithms for multiarmed bandits and combine it with the EXP3++ algorithm of Seldin and Slivkins (2014). In the stochastic regime the strategy reduces dependence of regret on a time horizon from $(\\ln t)^3$ to $(\\ln t)^2$ and eliminates an additive factor of order $\u2206e^1/\u2206^2$, where $\u2206$ is the minimal gap of a problem instance. In the adversarial regime regret guarantee remains unchanged.", "pdf_url": "http://proceedings.mlr.press/v65/seldin17a/seldin17a.pdf", "keywords": [], "reference": "Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Annual IEEE Symposium on Foundations of Computer Science, 1995.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal of Computing, 32(1), 2002b.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial In Proceedings of the International Conference on Computational Learning Theory  bandits. (COLT), 2012.  13   AN IMPROVED PARAMETRIZATION AND ANALYSIS OF THE EXP3++ ALGORITHM  reduced down to ln t. We note that Auer and Chiang (2016) have a lower bound on achievable regret guarantees in the stochastic regime when simultaneously certain expected regret guarantees against an adaptive adversary or high-probability regret guarantees against an oblivious adversary are re- quired. However, it is still unknown whether ln t regret in the stochastic regime can be achieved t expected regret against an oblivious adversary. While it does not seem pos- simultaneously with sible to achieve it with the EXP3++ algorithm, some modifications of the playing rule, such as the one used in BOA (Wintenberger, 2017), could potentially do better.  \u221a  A second question is whether improved regret guarantees can be achieved in the moderately contaminated stochastic regime and adversarial regime with a gap. We believe that it might not be possible with gap estimation strategies based on unweighted rewards and that in order to achieve that we should improve gap estimation based on importance-weighted rewards. The analysis technique suggested in our paper could potentially be useful for that.  There are also a number of more technical questions. For example, can we achieve high- probability regret guarantees by turning to modifications of the EXP3 algorithm, such as EXP3-IX 1 (Neu, 2015)? Or could we replace \u2206(a) factors with more refined measures of complexity, such as those in kl-UCB-type algorithms (Capp\u00b4e et al., 2013)?  We would like to thank Tor Lattimore and anonymous COLT reviewers for valuable suggestions for improvement of the manuscript. G\u00b4abor Lugosi was supported by the Spanish Ministry of Economy and Competitiveness, Grant MTM2015-67304-P and FEDER.  Acknowledgments  References  Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochas- tic and adversarial bandits. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2016.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Annual IEEE Symposium on Foundations of Computer Science, 1995.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal of Computing, 32(1), 2002b.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: stochastic and adversarial In Proceedings of the International Conference on Computational Learning Theory  bandits. (COLT), 2012. SELDIN LUGOSI  Olivier Capp\u00b4e, Aur\u00b4elien Garivier, Odalric-Ambrym Maillard, R\u00b4emi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. The Annals of Statis- tics, 41(3), 2013.  David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1), 1975.  Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58(301), 1963.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in Applied Mathematics, 6, 1985.  Colin McDiarmid. Concentration. In Probabilistic Methods for Algorithmic Discrete Mathematics.  Springer, 1998.  Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.  In Advances in Neural Information Processing Systems (NIPS), 2015.  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American  Mathematical Society, 1952.  Yevgeny Seldin. The space of online learning problems. ECML-PKDD Tutorial. https:  //sites.google.com/site/spaceofonlinelearningproblems/, 2015.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial  bandits. In JMLR Workshop and Conference Proceedings, volume 32 (ICML), 2014.  William R. Thompson. On the likelihood that one unknown probability exceeds another in view of  the evidence of two samples. Biometrika, 25, 1933.  Olivier Wintenberger. Optimal learning with Bernstein online aggregation. Machine Learning, 106,  2017.  "}, "Fast and robust tensor decomposition with applications to dictionary learning": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Fast and robust tensor decomposition with applications to dictionary learning", "abstract": "We develop fast spectral algorithms for tensor decomposition that match the robustness guarantees of the best known polynomial-time algorithms for this problem based on the sum-of-squares (SOS) semidefinite programming hierarchy. Our algorithms can decompose a 4-tensor with $n$-dimensional orthonormal components in the presence of error with constant spectral norm (when viewed as an $n^2$-by-$n^2$ matrix).  The running time is $n^5$ which is close to linear in the input size $n^4$. We also obtain algorithms with similar running time to learn sparsely-used orthogonal dictionaries even when feature representations have constant relative sparsity and non-independent coordinates. The only previous polynomial-time algorithms to solve these problem are based on solving large semidefinite programs.  In contrast, our algorithms are easy to implement directly and are based on spectral projections and tensor-mode rearrangements. Or work is inspired by recent of Hopkins, Schramm, Shi, and Steurer (STOC\u201916) that shows how fast spectral algorithms can achieve the guarantees of SOS for average-case problems.  In this work, we introduce general techniques to capture the guarantees of SOS for worst-case problems.", "pdf_url": "http://proceedings.mlr.press/v65/schramm17a/schramm17a.pdf", "keywords": ["tensor decomposition", "dictionary learning", "sum of squares", "iterative projection method", "orthogonal tensor", "spectral algorithm"], "reference": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learning sparsely used overcomplete dictionaries. In COLT, volume 35 of JMLR Workshop and Conference Proceedings, pages 123-137. JMLR.org, 2014.  Anima Anandkumar, Dean P. Foster, Daniel J. Hsu, Sham Kakade, and Yi-Kai Liu. A spectral  algorithm for latent dirichlet allocation. In NIPS, pages 926-934, 2012.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT, volume 30 of JMLR Workshop and Conference Proceedings, pages 867-881. JMLR.org, 2013.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. More algorithms for provable dictio-  nary learning. CoRR, abs/1401.0579, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In COLT, volume 40 of JMLR Workshop and Conference Proceedings, pages 113-149. JMLR.org, 2015.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks.  CoRR, abs/1612.08795, 2016.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition  via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In STOC, pages 594-603. ACM, 2014.  Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over  learned dictionaries. Image Processing, IEEE Transactions on, 15(12):3736-3745, 2006.  Andreas Argyriou Theodoros Evgeniou and Massimiliano Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, volume 19, pages 41-48. MIT Press, 2007.  Tom Goldstein and Stanley Osher. The split bregman method for l1-regularized problems. SIAM  journal on imaging sciences, 2(2):323-343, 2009.  31   FAST AND ROBUST TENSOR DECOMPOSITION WITH APPLICATIONS TO DICTIONARY LEARNING  T.S. is supported by an NSF Graduate Research Fellowship (NSF award no 1106400). D.S. is supported by a Microsoft Research Fellowship, a Alfred P. Sloan Fellowship, an NSF award, and the Simons Collaboration for Algorithms and Geometry.  Acknowledgments  References  Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learning sparsely used overcomplete dictionaries. In COLT, volume 35 of JMLR Workshop and Conference Proceedings, pages 123-137. JMLR.org, 2014.  Anima Anandkumar, Dean P. Foster, Daniel J. Hsu, Sham Kakade, and Yi-Kai Liu. A spectral  algorithm for latent dirichlet allocation. In NIPS, pages 926-934, 2012.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT, volume 30 of JMLR Workshop and Conference Proceedings, pages 867-881. JMLR.org, 2013.  Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15 (1):2773-2832, 2014.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. More algorithms for provable dictio-  nary learning. CoRR, abs/1401.0579, 2014.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In COLT, volume 40 of JMLR Workshop and Conference Proceedings, pages 113-149. JMLR.org, 2015.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski. Provable learning of noisy-or networks.  CoRR, abs/1612.08795, 2016.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition  via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015.  Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed anal-  ysis of tensor decompositions. In STOC, pages 594-603. ACM, 2014.  Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over  learned dictionaries. Image Processing, IEEE Transactions on, 15(12):3736-3745, 2006.  Andreas Argyriou Theodoros Evgeniou and Massimiliano Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, volume 19, pages 41-48. MIT Press, 2007.  Tom Goldstein and Stanley Osher. The split bregman method for l1-regularized problems. SIAM  journal on imaging sciences, 2(2):323-343, 2009. SCHRAMM STEURER  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In  NIPS, pages 2861-2869, 2014.  Richard A Harshman. Foundations of the parafac procedure: Models and conditions for an\" ex-  planatory\" multi-modal factor analysis. 1970.  Elad Hazan and Tengyu Ma. A non-generative framework and convex relaxations for unsupervised  learning. In NIPS, pages 3306-3314, 2016.  Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Speeding up sum-of-squares  for tensor decomposition and planted sparse vectors. CoRR, abs/1512.02337, 2015.  Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In STOC, pages 178-191. ACM, 2016.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In ITCS\u201913\u2014Proceedings of the 2013 ACM Conference on Innovations in Theoretical Computer Science, pages 11-19. ACM, New York, 2013.  Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. Blind source separation by simultaneous  third-order tensor diagonalization. In EUSIPCO, pages 1-4. IEEE, 1996.  Lieven De Lathauwer, Jos\u00e9phine Castaing, and Jean-Fran\u00e7ois Cardoso. Fourth-order cumulant- based blind identification of underdetermined mixtures. IEEE Trans. Signal Processing, 55(6-2): 2965-2973, 2007.  Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-  of-squares. In FOCS, pages 438-446. IEEE Computer Society, 2016.  Julien Mairal, Marius Leordeanu, Francis Bach, Martial Hebert, and Jean Ponce. Discriminative In Computer  sparse image models for class-specific edge detection and image interpretation. Vision-ECCV 2008, pages 43-56. Springer, 2008.  Y Marc\u2019Aurelio Ranzato, Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief  networks. Advances in neural information processing systems, 20:1185-1192, 2007.  Elchanan Mossel and S\u00e9bastien Roch. Learning nonsingular phylogenies and hidden markov mod-  els. In STOC, pages 366-375. ACM, 2005.  Roberto I. Oliveira. Sums of random Hermitian matrices and an inequality by Rudelson. Electron.  Commun. Probab., 15:203-212, 2010. ISSN 1083-589X/e.  Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy  employed by v1? Vision research, 37(23):3311-3325, 1997.  R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14(5):877-898, 1976. doi: 10.1137/0314056. URL http://dx. doi.org/10.1137/0314056. FAST AND ROBUST TENSOR DECOMPOSITION WITH APPLICATIONS TO DICTIONARY LEARNING  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-  tional Mathematics, 12(4):389-434, 2012.  Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution as sparse repre- sentation of raw image patches. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008.  "}, "The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime", "abstract": "We propose a novel technique for analyzing adaptive sampling called the Simulator. Our approach differs from the existing methods by considering not how much information could be gathered by any fixed sampling strategy, but how difficult it is to distinguish a good sampling strategy from a bad one given the limited amount of data collected up to any given time. This change of perspective allows us to match the strength of both Fano and change-of-measure techniques, without succumbing to the limitations of either method. For concreteness, we apply our techniques to a structured multi-arm bandit problem in the fixed-confidence pure exploration setting, where we show that the constraints on the means imply a substantial gap between the moderate-confidence sample complexity, and the asymptotic sample complexity as the confidence delta tends to zero, as found in the literature. We also prove the first instance-based lower bounds for the top-k problem which incorporate the appropriate log-factors. Moreover, our lower bounds zero-in on the number of times each individual arm needs to be pulled, uncovering new phenomena which are drowned out in the aggregate sample complexity. Our new analysis inspires a simple and near-optimal algorithm for the best-arm and top-k identification, the first practical algorithm of its kind for the latter problem which removes extraneous log factors, and outperforms the state-of-the-art in experiments.", "pdf_url": "http://proceedings.mlr.press/v65/simchowitz17a/simchowitz17a.pdf", "keywords": [], "reference": "Ery Arias-Castro, Emmanuel J Candes, and Mark A Davenport. On the fundamental limits of adaptive  sensing. IEEE Transactions on Information Theory, 59(1):472-481, 2013.  Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs. Biometrics, 14(3):408-429, 1958.  Malgorzata Bogdan, Ewout van den Berg, Weijie Su, and Emmanuel Candes. Statistical estimation and  testing via the sorted l1 norm. arXiv preprint arXiv:1310.1969, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed  bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification  bandit problem. In Proceedings of the 29th Conference on Learning Theory, 2016.  Rui M Castro. Adaptive sensing performance lower bounds for sparse signal detection and support estimation.  Bernoulli, 20(4):2217-2246, 2014.  Theory, 54(5):2339-2353, 2008.  arXiv:1511.03774, 2015.  Rui M Castro and Robert D Nowak. Minimax bounds for active learning. IEEE Transactions on Information  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid constraints.  In 29th Annual Conference on Learning Theory, pages 647-669, 2016.  Lijie Chen, Jian Li, and Mingda Qiao. Nearly instance optimal sample complexity bounds for top-k arm  selection, 2017.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of  multi-armed bandits. In Advances in Neural Information Processing Systems, pages 379-387, 2014.  Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre Proutiere, et al. Combinatorial  bandits revisited. In Advances in Neural Information Processing Systems, pages 2116-2124, 2015.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun): 1079-1105, 2006.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence.  In 29th  Annual Conference on Learning Theory, pages 998-1027, 2016.  Aur\u00b4elien Garivier, Pierre M\u00b4enard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in  bandit problems. arXiv preprint arXiv:1602.07182, 2016.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online problems.  2014.  Steve Hanneke. Theoretical foundations of active learning. 2009.  16   SIMCHOWITZ JAMIESON RECHT  References  Ery Arias-Castro, Emmanuel J Candes, and Mark A Davenport. On the fundamental limits of adaptive  sensing. IEEE Transactions on Information Theory, 59(1):472-481, 2013.  Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs. Biometrics, 14(3):408-429, 1958.  Malgorzata Bogdan, Ewout van den Berg, Weijie Su, and Emmanuel Candes. Statistical estimation and  testing via the sorted l1 norm. arXiv preprint arXiv:1310.1969, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed  bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1-122, 2012.  Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification  bandit problem. In Proceedings of the 29th Conference on Learning Theory, 2016.  Rui M Castro. Adaptive sensing performance lower bounds for sparse signal detection and support estimation.  Bernoulli, 20(4):2217-2246, 2014.  Theory, 54(5):2339-2353, 2008.  arXiv:1511.03774, 2015.  Rui M Castro and Robert D Nowak. Minimax bounds for active learning. IEEE Transactions on Information  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid constraints.  In 29th Annual Conference on Learning Theory, pages 647-669, 2016.  Lijie Chen, Jian Li, and Mingda Qiao. Nearly instance optimal sample complexity bounds for top-k arm  selection, 2017.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of  multi-armed bandits. In Advances in Neural Information Processing Systems, pages 379-387, 2014.  Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre Proutiere, et al. Combinatorial  bandits revisited. In Advances in Neural Information Processing Systems, pages 2116-2124, 2015.  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun): 1079-1105, 2006.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence.  In 29th  Annual Conference on Learning Theory, pages 998-1027, 2016.  Aur\u00b4elien Garivier, Pierre M\u00b4enard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in  bandit problems. arXiv preprint arXiv:1602.07182, 2016.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online problems.  2014.  Steve Hanneke. Theoretical foundations of active learning. 2009. THE SIMULATOR: UNDERSTANDING ADAPTIVE SAMPLING IN THE MODERATE-CONFIDENCE REGIME  Kevin G Jamieson, Matthew Malloy, Robert D Nowak, and S\u00b4ebastien Bubeck. lil\u2019ucb: An optimal exploration  algorithm for multi-armed bandits. In COLT, volume 35, pages 423-439, 2014.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML- 12), pages 655-662, 2012.  Zohar Shay Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.  ICML (3), 28:1238-1246, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identification in  multi-armed bandit models. The Journal of Machine Learning Research, 2015.  Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: Fast  combinatorial optimization with learning. arXiv preprint arXiv:1403.5045, 2014.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied  mathematics, 6(1):4-22, 1985.  T. Lattimore and C. Szepesvari. The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear  Bandits. ArXiv e-prints, October 2016.  Alexander Luedtke, Emilie Kaufmann, and Antoine Chambaz. Asymptotically optimal algorithms for multi-  ple play bandits with partial feedback. arXiv preprint arXiv:1606.09388, 2016.  Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and  optimal algorithms. In COLT, pages 975-999, 2014.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem.  Journal of Machine Learning Research, 5(Jun):623-648, 2004.  Frank Nielsen and Vincent Garcia. Statistical exponential families: A digest with \ufb02ash cards. arXiv preprint  arXiv:0911.4863, 2009.  Maxim Raginsky and Alexander Rakhlin. Lower bounds for passive and active learning. In Advances in  Neural Information Processing Systems, pages 1026-1034, 2011.  Maxim Raginsky and Igal Sason. Concentration of Measure Inequalities in Information Theory, Communi-  cations, and Coding. Now Publishers Inc., 2014.  Dan Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.  In Advances in Neural Information Processing Systems, pages 2256-2264, 2013.  Daniel Russo. Simple bayesian algorithms for best arm identification. In 29th Annual Conference on Learning  Theory, pages 1417-1418, 2016.  Max Simchowitz, Kevin Jamieson, and Benjamin Recht. Best-of-k-bandits. In 29th Annual Conference on  Learning Theory, pages 1440-1489, 2016.  Marta Soare, Alessandro Lazaric, and R\u00b4emi Munos. Best-arm identification in linear bandits. In Advances in  Neural Information Processing Systems, pages 828-836, 2014.  Mohammad Sadegh Talebi and Alexandre Proutiere. An optimal algorithm for stochastic matroid bandit optimization. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 548-556. International Foundation for Autonomous Agents and Multiagent Systems, 2016. SIMCHOWITZ JAMIESON RECHT  Alexandre B Tsybakov. Introduction to nonparametric estimation. revised and extended from the 2004 french  original. translated by vladimir zaiats, 2009.  Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified retrieval. In  Advances in Neural Information Processing Systems, pages 2483-2491, 2011. THE SIMULATOR: UNDERSTANDING ADAPTIVE SAMPLING IN THE MODERATE-CONFIDENCE REGIME  "}, "On Learning vs. Refutation": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "On Learning vs. Refutation", "abstract": "Building on the work of Daniely et al. (STOC 2014, COLT 2016), we study the connection between computationally efficient PAC learning and refutation of constraint satisfaction problems.  Specifically, we prove that for every concept class $\\mathcal{P}$, PAC-learning $\\mathcal{P}$ is \\em polynomially equivalent to \u201crandom-right-hand-side-refuting\u201d (\u201cRRHS-refuting\u201d) a dual class $\\mathcal{P}^*$, where RRHS-refutation of a class $\\mathcal{Q}$ refers to refuting systems of equations where the constraints are (worst-case) functions from the class $\\mathcal{Q}$ but the right-hand-sides of the equations are uniform and independent random bits. The reduction from refutation to PAC learning can be viewed as an abstraction of (part of) the work of Daniely, Linial, and Shalev-Schwartz (STOC 2014). The converse, however, is new, and is based on a combination of techniques from pseudorandomness (Yao \u201882) with boosting (Schapire \u201890).  In addition, we show that PAC-learning the class of $\\mathit{DNF}$ formulas is polynomially equivalent to PAC-learning its dual class $\\mathit{DNF}^*$, and thus PAC-learning $\\mathit{DNF}$ is equivalent to RRHS-refutation of $\\mathit{DNF}$, suggesting an avenue to obtain stronger lower bounds for PAC-learning $\\mathit{DNF}$ than the quasipolynomial lower bound that was obtained by Daniely and Shalev-Schwartz (COLT 2016) assuming the hardness of refuting $k$-SAT.", "pdf_url": "http://proceedings.mlr.press/v65/vadhan17a/vadhan17a.pdf", "keywords": [], "reference": "Benny Applebaum, Boaz Barak, and David Xiao. On basing lower-bounds for learning on worst-case assumptions. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA, pages 211-220. IEEE Computer Society, 2008. ISBN 978-0-7695-3436-7. doi: 10.1109/FOCS.2008.35. URL http://dx.doi.org/10.1109/FOCS.2008.35.  12   Vadhan  Ostrovsky (1991) introduced a similar notion of auxiliary-input one-way functions. In the works of Ostrovsky (1991); Ostrovsky and Wigderson (1993); Ong and Vadhan (2008), it was shown that the existence of zero-knowledge proofs (or even zero-knowledge arguments) for a language outside BPP implies the existence of auxiliary-input one-way functions. Applebaum, Barak, and Xiao (2008) observed that the existence of auxiliary-input one-way functions implies the existence of a auxiliary-input (strong) pseudorandom function family P, and that the latter implies hardness of PAC-learning P. Here, we can directly see that if P is an auxiliary-input weak PRF family, then P \u2217 is RRHS-unrefutable (which is equivalent to P not being PAC-learnable, by Theorem 5). (Here the pair (x, w) should be treated as a single index for function p(x,w). Unlike an auxiliary-input weak PRF adversary, a refuter is not given w, which only makes the task of refutation harder.) Still, the notion of auxiliary- input weak PRFs appears to be substantially stronger than RRHS-unrefutability, due to the worst-case choices of the key x and inputs y1, . . . , yn in the latter. This may explain why Daniely et al. (2014); Daniely and Shalev-Schwartz (2016); Daniely (2016) were able to obtain hardness of PAC-learning results that eluded past work.  In the work of Vadhan (2006), a notion of \u201cinstance-dependent one-way functions\u201d was introduced, which is stronger than the auxiliary-input notion above in that the infinite set of hard indices w does not depend on the adversary B. This notion captures the di\ufb00erence between computational zero knowledge and statistical zero knowledge (Vadhan, 2006; Ong and Vadhan, 2008).  It is informative to compare which values are random or worst case in the di\ufb00erent  notions examined:  RRHS-unrefutability of P \u2217 Ordinary refutability of P \u2217 (Weak) PRF family P  Auxiliary-input weak PRF family P worst-case public part  function index (in completeness) worst case and secret worst case and secret random and secret  random secret part  function inputs  worst case random random random random  I thank Boaz Barak, Amit Daniely, Ryan O\u2019Donnell, Rocco Servedio, and Jon Ullman for illuminating conversations, and the anonymous reviewers for helpful corrections and suggestions.  Acknowledgments  References  Benny Applebaum, Boaz Barak, and David Xiao. On basing lower-bounds for learning on worst-case assumptions. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA, pages 211-220. IEEE Computer Society, 2008. ISBN 978-0-7695-3436-7. doi: 10.1109/FOCS.2008.35. URL http://dx.doi.org/10.1109/FOCS.2008.35. Learning vs. Refutation  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 105-117. ACM, 2016. ISBN 978-1-4503-4132-5. doi: 10.1145/2897518.2897520. URL http://doi.acm.org/10.1145/2897518.2897520.  Amit Daniely and Shai Shalev-Schwartz. Complexity theoretic limitations on learning dnf\u2019s. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop and Conference Proceedings, pages 815-830. JMLR.org, 2016. URL http://jmlr.org/proceedings/papers/v49/daniely16.html.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper learning complexity. In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 441- 448. ACM, 2014. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591820. URL http: //doi.acm.org/10.1145/2591796.2591820.  Uriel Feige. Relations between average case complexity and approximation complexity. In Proceedings of the Thirty-Fourth Annual ACM Symposium on Theory of Computing, pages 534-543. ACM, New York, 2002. doi: 10.1145/509907.509985. URL http://dx. doi.org/10.1145/509907.509985.  Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to construct random functions. Journal of the Association for Computing Machinery, 33(4):792-807, 1986. ISSN 0004- 5411. doi: 10.1145/6490.6503. URL http://dx.doi.org/10.1145/6490.6503.  Michael Kearns and Leslie Valiant. Cryptographic limitations on learning Boolean formulae and finite automata. Journal of the Association for Computing Machinery, 41(1):67-95, 1994. ISSN 0004-5411. doi: 10.1145/174644.174647. URL http://dx.doi.org/10.1145/ 174644.174647.  Michael J. Kearns, Ming Li, Leonard Pitt, and Leslie G. Valiant. On the learnability of boolean formulae. In Alfred V. Aho, editor, Proceedings of the 19th Annual ACM Symposium on Theory of Computing, 1987, New York, New York, USA, pages 285-295. ACM, 1987. ISBN 0-89791-221-7. doi: 10.1145/28395.28426. URL http://doi.acm. org/10.1145/28395.28426.  Adam R. Klivans and Rocco A. Servedio. Learning DNF in time 2 \u02dcO(n1/3). Journal of Computer and System Sciences, 68(2):303-318, 2004. ISSN 0022-0000. doi: 10.1016/j. jcss.2003.07.007. URL http://dx.doi.org/10.1016/j.jcss.2003.07.007.  Moni Naor and Omer Reingold. From unpredictability to indistinguishability: A sim- ple construction of pseudo-random functions from MACs (extended abstract). In Hugo Krawczyk, editor, Advances in Cryptology - CRYPTO \u201998, 18th Annual International Cryptology Conference, Santa Barbara, California, USA, August 23-27, 1998, Proceed- ings, volume 1462 of Lecture Notes in Computer Science, pages 267-282. Springer, 1998. Vadhan  ISBN 3-540-64892-5. doi: 10.1007/BFb0055734. URL http://dx.doi.org/10.1007/ BFb0055734.  Shien Jin Ong and Salil Vadhan. An equivalence between zero knowledge and commitments. In Theory of cryptography, volume 4948 of Lecture Notes in Computer Sciences, pages 482-500. Springer, Berlin, 2008. doi: 10.1007/978-3-540-78524-8 27. URL http://dx. doi.org/10.1007/978-3-540-78524-8_27.  Rafail Ostrovsky. One-way functions, hard on average problems, and statistical zero- knowledge proofs. In Proceedings of the Sixth Annual Structure in Complexity The- ory Conference, Chicago, Illinois, USA, June 30 - July 3, 1991, pages 133-138. IEEE Computer Society, 1991. ISBN 0-8186-2255-5. doi: 10.1109/SCT.1991.160253. URL https://doi.org/10.1109/SCT.1991.160253.  Rafail Ostrovsky and Avi Wigderson. One-way fuctions are essential for non-trivial zero- knowledge. In Second Israel Symposium on Theory of Computing Systems, ISTCS 1993, Natanya, Israel, June 7-9, 1993, Proceedings, pages 3-17. IEEE Computer So- ciety, 1993. doi: 10.1109/ISTCS.1993.253489. URL https: //doi.org/10.1109/ISTCS.1993.253489.  ISBN 0-8186-3630-0.  Leonard Pitt and Manfred K. Warmuth. Prediction-preserving reducibility. Journal of ISSN 0022-0000. doi: 10.1016/  Computer and System Sciences, 41(3):430-467, 1990. 0022-0000(90)90028-J. URL http://dx.doi.org/10.1016/0022-0000(90)90028-J.  Robert E. Schapire. The strength of weak learnability. Machine Learning, 5:197-227, 1990.  doi: 10.1007/BF00116037. URL http://dx.doi.org/10.1007/BF00116037.  Salil P. Vadhan. An unconditional study of computational zero knowledge. SIAM Journal on Computing, 36(4):1160-1214, 2006. ISSN 0097-5397. doi: 10.1137/S0097539705447207. URL http://dx.doi.org/10.1137/S0097539705447207.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134- 1142, 1984. doi: 10.1145/1968.1972. URL http://doi.acm.org/10.1145/1968.1972.  Andrew Chi-Chih Yao. Theory and applications of trapdoor functions (extended abstract). In 23rd Annual Symposium on Foundations of Computer Science, Chicago, Illinois, USA, 3-5 November 1982, pages 80-91. IEEE Computer Society, 1982. doi: 10.1109/SFCS.1982. 45. URL https://doi.org/10.1109/SFCS.1982.45. "}, "Ignoring Is a Bliss:  Learning with Large Noise Through Reweighting-Minimization": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Ignoring Is a Bliss:  Learning with Large Noise Through Reweighting-Minimization", "abstract": "We consider learning in the presence of  arbitrary noise that can overwhelm the signal in terms of magnitude on a fraction of data points observed (aka outliers). Standard approaches based on minimizing empirical loss can fail miserably and lead to arbitrary bad solutions in this setting. We propose an approach that iterates between finding a solution with minimal empirical loss and re-weighting the data, reinforcing data points where the previous solution works well.  We show that our approach can handle arbitrarily large noise, is robust as having a non-trivial breakdown point, and converges linearly under certain conditions. The intuitive idea of our approach is to automatically exclude \u201cdifficult\u201d data points from model fitting. More importantly (and perhaps surprisingly), we validate this intuition by establishing guarantees for generalization and iteration complexity that \\em essentially ignore the presence of outliers", "pdf_url": "http://proceedings.mlr.press/v65/vainsencher17a/vainsencher17a.pdf", "keywords": [], "reference": "Andreas Alfons, Christophe Croux, and Sarah Gelper. Sparse least trimmed squares regression for analyzing high-dimensional large data sets. The Annals of Applied Statistics, 7(1):226-248, 2013.  30  0.000.101.0010.00100.001000.00ka\u2212bk20.000.050.100.150.200.250.300.350.400.45Proportionofnoisypoints.RWloss.0.000.101.0010.00100.001000.00ka\u2212bk2l1reg.dataadj.0.000.101.0010.00100.001000.00ka\u2212bk2std.averaging.\u22121.5\u22121.0\u22120.50.00.51.01.52.02.5 VAINSENCHER MANNOR XU  subspace of R10, and the remainder are from a 4 dimensional subspace sharing 3 basis elements with the first.  We compare three algorithms. The first one is the proposed RW algorithm (i.e., Algorithm 1). Here, we choose \u03b2 by first solving the uniformly weighted problem, and then take \u03b2 equal to the average of empirical losses. The second one - the alternative robust estimation method - is the approach proposed in Giannakis et al. (2011); Mateos and Giannakis (2012a,b) which also uses alternating minimization. In particular, each data point is modeled as the sum of an estimated value and an adjustment; the sum of the norms of the adjustments is taken as a penalty. At each iteration, the model is fit to the estimated data and the adjustment for each data point is updated so as to minimize the sum of the model loss and the l1 penalty on the adjustments. Due to the sparsity encouraging property of l1 regularization, data points that fit the model well are not affected by the adjustment at all, while adjustments for outliers can be large. In simple cases, this scheme corresponds to replacing the squared distance loss with the Huber loss. This approach is generic conceptually, although applying it to each individual problem type requires some customization. The third algorithm, as a baseline, minimizes the average of quadratic losses.  Figure 4: Location estimation. The majority source follows  (a, d\u22121I), while the noisy points fol- N (b, 100d\u22121I). The goal is an estimate \u02c6a of a. Color corresponds to error measured \u02c6a (cid:107)2; the average additional errors due to outliers are (0.19, 0.55, 1.47) for (cid:107)  low N as log RM, the robust competitor and the mean, respectively.  \u2212  a  In each of the problem types, the proposed method achieves smallest additional error due to outliers. Large perturbations cause severe error for standard linear regression, smaller error for the l1 regularized correction method, and essentially no additional error for regularized weighting.  Finally, we comment on the implementation issue about tuning the \u03b2 parameter. Our theorems identify a tradeoff between robustness and generalization by tuning \u03b2, which should be set according to the losses for non-outliers. In practice as a rule of thumb we suggest choosing \u03b2 by first solving the uniformly weighted problem, and then take \u03b2 equal to the average of empirical losses, which is also how we implement the algorithm in the simulation section.  References  Andreas Alfons, Christophe Croux, and Sarah Gelper. Sparse least trimmed squares regression for analyzing high-dimensional large data sets. The Annals of Applied Statistics, 7(1):226-248, 2013.0.000.101.0010.00100.001000.00ka\u2212bk20.000.050.100.150.200.250.300.350.400.45Proportionofnoisypoints.RWloss.0.000.101.0010.00100.001000.00ka\u2212bk2l1reg.dataadj.0.000.101.0010.00100.001000.00ka\u2212bk2std.averaging.\u22121.5\u22121.0\u22120.50.00.51.01.52.02.5 LEARNING THROUGH REWEIGHTING-MINIMIZATION  Figure 5: Linear regression. The data are generated as x  +  N  + (0, (10)\u22121/2) where s = 0 except with probability varying with the ver- m(cid:48), x s (cid:105) (cid:104) tical axis. The value of s when non-zero varies with the horizontal axis. Color corre- sponds to error measured as log (cid:107)2; the average additional errors due to outliers are (0.0089, 0.15, 0.52) decibels respectively for RM, the robust competitor and least squares regression.  m, x (cid:105)  (0, d\u22121/2I) and y  \u223c N  \u223c (cid:104)  \u02c6m  m  \u2212  (cid:107)  \u2208  (0, I), A  R4 follow z  Figure 6: Principal component analysis. Each data point is generated according to x = Az + sBz1, R10\u00d71 is where z \u223c N perpendicular to the columns of A. The probability that s = 0 and the value of s are the vertical axis and the horizontal axis respectively. Color corresponds to the logarithm of the squared Euclidean norm of the sines of the 4 principal angles between the column space of A and the estimated 4 dimensional subspace. The average additional errors due to outliers are (0.41, 2.19, 2.93, 2.93) respectively for RM, the robust competitor at two parameter values, and standard PCA.  R10\u00d74 is orthonormal and B  \u2208  \u2208  Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-  bridge University Press, 1999.  Ozlem Aslan, Dale Schuurmans, and Yao-liang Yu. A polynomial-time form of robust regression.  In Advances in Neural Information Processing Systems, pages 2483-2491, 2012.  Jean-Yves Audibert and Olivier Catoni. Robust linear least squares regression. The Annals of  Statistics, pages 2766-2794, 2011.  George EP Box. Non-normality and tests on variances. Biometrika, 40(3/4):318-335, 1953.0.100.180.320.561.001.783.165.6210.00Str.ofalt.source.0.000.050.100.150.200.250.300.350.400.45Proportionofnoisypoints.RWloss.0.100.180.320.561.001.783.165.6210.00Str.ofalt.source.l1reg.dataadj.0.100.180.320.561.001.783.165.6210.00Str.ofalt.source.std.regression.\u22121.50\u22121.25\u22121.00\u22120.75\u22120.50\u22120.250.000.250.500.100.321.003.1610.00Str.ofalt.source.0.000.050.100.150.200.250.300.350.400.45Proportionofnoisypoints.RW0.100.321.003.1610.00Str.ofalt.source.l1reg.\u03bb=0.01.0.100.321.003.1610.00Str.ofalt.source.l1reg.\u03bb=1.0.100.321.003.1610.00Str.ofalt.source.PCA\u22124.5\u22124.0\u22123.5\u22123.0\u22122.5\u22122.0\u22121.5\u22121.0\u22120.5(cid:54)  VAINSENCHER MANNOR XU  Matthew Coudron and Gilad Lerman. On the sample complexity of robust PCA. In Advances in  Neural Information Processing Systems, pages 3221-3229, 2012.  John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272-279. ACM, 2008.  Georgios B. Giannakis, Gonzalo Mateos, Shahrokh Farahmand, Vassilis Kekatos, and Hao Zhu. Uspacor: Universal sparsity-controlling outlier rejection. In Acoustics, Speech and Signal Pro- cessing (ICASSP), 2011 IEEE International Conference on, pages 1952-1955. IEEE, 2011.  Frank Rudolf Hampel. Contributions to the theory of robust estimation. PhD thesis, University of  California Berkeley, 1968.  Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. Journal  of Machine Learning Research, 17(18):1-40, 2016.  Peter J Huber et al. Robust estimation of a location parameter. The Annals of Mathematical Statis-  tics, 35(1):73-101, 1964.  Samuel Kutin. Extensions to McDiarmid\u2019s inequality when differences are bounded with high probability. Technical Report TR-2002-04, Department of Computer Science, The University of Chicago, 2002.  Po-Ling Loh. Statistical consistency and asymptotic normality for high-dimensional robust m-  estimators. arXiv preprint arXiv:1501.00312, 2015.  Po-Ling Loh and Martin J Wainwright. Support recovery without incoherence: A case for noncon-  vex regularization. arXiv preprint arXiv:1412.5632, 2014.  Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: statistical and algorithmic theory for local optima. The Journal of Machine Learning Research, 16(1):559-616, 2015.  Gonzalo Mateos and Georgios B. Giannakis. Robust nonparametric regression via sparsity control with application to load curve data cleansing. Signal Processing, IEEE Transactions on, 60(4): 1571-1584, 2012a.  Gonzalo Mateos and Georgios B. Giannakis. Robust PCA as bilinear decomposition with outlier- sparsity regularization. Signal Processing, IEEE Transactions on, 60(10):5176-5190, 2012b.  Shahar Mendelson. Learning without concentration. In Proceedings of The 27th Conference on  Learning Theory, pages 25-39, 2014.  TD Nguyen and R Welsch. Outlier detection and least trimmed squares approximation using semi- definite programming. Computational Statistics & Data Analysis, 54(12):3212-3226, 2010.  Peter J Rousseeuw. Least median of squares regression. Journal of the American statistical associ-  ation, 79(388):871-880, 1984. LEARNING THROUGH REWEIGHTING-MINIMIZATION  Peter J Rousseeuw and Katrien Van Driessen. An algorithm for positive-breakdown regression  based on concentration steps. In Data Analysis, pages 335-346. Springer, 2000.  J. Tukey. The future of data analysis. Annals of Mathematical Statistics, 33:1-67, 1962.  John W Tukey. A survey of sampling from contaminated distributions. Contributions to probability  and statistics, 2:448-485, 1960.  Daniel Vainsencher, Shie Mannor, and Huan Xu. Learning multiple models via regularized weight-  ing. In Advances in Neural Information Processing Systems 26, pages 1977-1985, 2013.  Aad W. Van der Vaart and Jon A. Wellner. Weak convergence and empirical processes. Springer  Verlag, 1996.  Huan Xu, Constantine Caramanis, and Shie Mannor. Outlier-robust PCA: The high-dimensional  case. IEEE transactions on information theory, 59(1):546-572, 2013. "}, "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox", "abstract": "We present and analyze statistically optimal, communication and memory efficient distributed stochastic optimization algorithms with near-linear speedups (up to $\\log$-factors).  This improves over prior work which includes methods with near-linear speedups but polynomial communication requirements (accelerated minibatch SGD) and communication efficient methods which do not exhibit any runtime speedups over a naive single-machine approach.  We first analyze a distributed SVRG variant as a distributed stochastic optimization method and show that it can achieve near-linear speedups with logarithmic rounds of communication, at the cost of high memory requirements. We then present a novel method, MB-DSVRG, which trades off memory for communication and still allows for optimization with communication which scales only logarithmically with the desired accuracy while also being memory efficient.  MB-DSVRG is based on a minibatch prox procedure, solving a non-linearized subproblem on a minibatch at each iteration.  We provide a novel analysis for this procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, and thus significantly improving on prior work.", "pdf_url": "http://proceedings.mlr.press/v65/wang17a/wang17a.pdf", "keywords": [], "reference": "Dimitri P. Bertsekas. Incremental proximal methods for large scale convex optimization. Mathe-  matical programming, 129(2):163, 2011.  Dimitri P. Bertsekas.  Incremental aggregated proximal and augmented Lagrangian algorithms.  arXiv:1509.09257 [cs.SY], November 4 2015.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Li Cheng, S. V. N. Vishwanathan, Dale Schuurmans, Shaojun Wang, and Terry Caelli.  Implicit In Proceedings of the 19th International Conference on Neural  online learning with kernels. Information Processing Systems, pages 249-256. MIT Press, 2006.  Andrew Cotter, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages 1647-1655, 2011.  Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar):551-585, 2006.  Aaron Defazio. A simple practical accelerated method for finite sums.  In Advances In Neural  Information Processing Systems, pages 676-684, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  13   EFFICIENT DISTRIBUTED STOCHASTIC OPTIMIZATION  Instead of using DSVRG to solve each proximal subproblem in a minibatch-prox iteration, we can also use any other distributed optimization approach. For example, we can consider using DiSCO or DANE. This is depicted as \u201cMP-DANE\u201d in Figure 2. Again, an external minibatch-prox loop allows trading off memory for communication. For small minibatch sizes, up to a critical value of bmp-dane = \u0398(n(\u03b5)/(m2B2)), MP-DANE enjoys the same guarantees as MP-DSVRG. But for larger minibatch sizes, such an approach starts suffering from DANE/DiSCO\u2019s inferior runtime and communication requirements compared to DSVRG.  We emphasize that the above discussion is based on guarantees established only for least square problems and ignores log-factors. We are unfortunately not aware of distributed stochastic opti- mization guarantees that improve over minibatch SGD (i.e., achieve even near-linear speedup with lower communication requirements) for general smooth objectives, or achieve true linear speedup (and improved communication guarantees) even for least-square problems.  Research was partially supported by an Intel ICRI-CI award and NSF awards IIS 1302662 and BIGDATA 1546500. We would like to thank Ohad Shamir for discussions about Distributed SVRG and Tong Zhang for discussions about minibatch-prox.  Acknowledgement  References  Dimitri P. Bertsekas. Incremental proximal methods for large scale convex optimization. Mathe-  matical programming, 129(2):163, 2011.  Dimitri P. Bertsekas.  Incremental aggregated proximal and augmented Lagrangian algorithms.  arXiv:1509.09257 [cs.SY], November 4 2015.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Li Cheng, S. V. N. Vishwanathan, Dale Schuurmans, Shaojun Wang, and Terry Caelli.  Implicit In Proceedings of the 19th International Conference on Neural  online learning with kernels. Information Processing Systems, pages 249-256. MIT Press, 2006.  Andrew Cotter, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages 1647-1655, 2011.  Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar):551-585, 2006.  Aaron Defazio. A simple practical accelerated method for finite sums.  In Advances In Neural  Information Processing Systems, pages 676-684, 2016.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014. WANG WANG SREBRO  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction  using mini-batches. Journal of Machine Learning Research, 13(Jan):165-202, 2012.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance  reduction. In Advances in Neural Information Processing Systems, pages 315-323, 2013.  Brian Kulis and Peter L. Bartlett. Implicit online learning. In Proceedings of the 27th International  Conference on Machine Learning (ICML-10), pages 575-582, 2010.  Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method. arXiv:1212.2002 [cs.LG], 2012.  Jason D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance re- duced gradient methods and a lower bound for communication complexity. arXiv preprint arXiv:1507.07595, 2015.  Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for stochastic optimization. In Proc. of the 20th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining (SIGKDD 2014), pages 661-670, 2014.  Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization.  In Advances in Neural Information Processing Systems, pages 3384-3392, 2015.  Deanna Needell and Joel A. Tropp. Paved with good intentions: Analysis of a randomized block  kaczmarz method. Linear Algebra and its Applications, 441:199-221, 2014.  A. Nemirovskii and D. B. Yudin. Problem complexity and method efficiency in optimization, 1983.  Sashank J Reddi, Jakub Kone\u02c7cn`y, Peter Richt\u00b4arik, Barnab\u00b4as P\u00b4ocz\u00b4os, and Alex Smola. Aide: Fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.  Mark Schmidt, Nicolas Le Roux, and Francis Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages 1458-1466, 2011.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In Proc. of the 22th Annual Conference on Learning Theory (COLT\u201909), 2009.  Ohad Shamir. Without-replacement sampling for stochastic gradient methods: Convergence results  and application to distributed optimization. arXiv preprint arXiv:1603.00570, 2016.  Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850-857. IEEE, 2014.  Ohad Shamir, Nathan Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate Newton-type method. In Proc. of the 31st Int. Conf. Machine Learning (ICML 2014), pages 1000-1008, 2014. EFFICIENT DISTRIBUTED STOCHASTIC OPTIMIZATION  Panos Toulis and Edoardo M. Airoldi.  Implicit stochastic gradient descent.  arXiv preprint  arXiv:1408.2923, 2014.  1995.  Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,  Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-  tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.  Yuchen Zhang and Xiao Lin. DiSCO: Distributed optimization for self-concordant empirical loss. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 362-370, 2015.  Yuchen Zhang, Martin J. Wainwright, and John C. Duchi. Communication-efficient algorithms for In Advances in Neural Information Processing Systems, pages 1502-  statistical optimization. 1510, 2012.  "}, "Learning Non-Discriminatory Predictors": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Learning Non-Discriminatory Predictors", "abstract": "We consider learning a predictor which is non-discriminatory with respect to a \u201cprotected attribute\u201d according to the notion of \u201cequalized odds\u201d proposed by Hardt et al. (2016).  We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.  We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.", "pdf_url": "http://proceedings.mlr.press/v65/woodworth17a/woodworth17a.pdf", "keywords": [], "reference": "Olivier Bousquet, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Introduction to statistical learning the-  ory. In Advanced lectures on machine learning, pages 169-207. Springer, 2004.  Amit Daniely.  Complexity theoretic limitations on learning halfspaces.  arXiv preprint  arXiv:1505.05800, 2015.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper learning complexity. In Proceedings of the 46th Annual ACM Symposium on Theory of Comput- ing, pages 441-448. ACM, 2014.  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Con- ference, pages 214-226. ACM, 2012.  Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra- manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD In- ternational Conference on Knowledge Discovery and Data Mining, pages 259-268. ACM, 2015.  Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In  Advances in Neural Information Processing Systems, 2016.  Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Rawlsian fair-  ness for machine learning. arXiv preprint arXiv:1610.09559, 2016a.  Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325-333, 2016b.  Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Pro- ceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560-568. ACM, 2008.  White House.  Big data: A report on algorithmic systems, opportunity,  and civil rights. 2016. URL https://obamawhitehouse.archives.gov/sites/default/ files/microsites/ostp/2016_0504_data_discrimination.pdf.  Muhammad Bilal Zafar,  Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gum- Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification madi. without Disparate Mistreatment. CoRR, abs/1610.08452, 2016. URL http://dblp. uni-trier.de/db/journals/corr/corr1610.html#ZafarVGG16;http: //arxiv.org/abs/1610.08452.  14   WOODWORTH GUNASEKAR OHANNESSIAN SREBRO  References  Olivier Bousquet, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Introduction to statistical learning the-  ory. In Advanced lectures on machine learning, pages 169-207. Springer, 2004.  Amit Daniely.  Complexity theoretic limitations on learning halfspaces.  arXiv preprint  arXiv:1505.05800, 2015.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper learning complexity. In Proceedings of the 46th Annual ACM Symposium on Theory of Comput- ing, pages 441-448. ACM, 2014.  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Con- ference, pages 214-226. ACM, 2012.  Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra- manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD In- ternational Conference on Knowledge Discovery and Data Mining, pages 259-268. ACM, 2015.  Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In  Advances in Neural Information Processing Systems, 2016.  Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Rawlsian fair-  ness for machine learning. arXiv preprint arXiv:1610.09559, 2016a.  Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325-333, 2016b.  Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Pro- ceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560-568. ACM, 2008.  White House.  Big data: A report on algorithmic systems, opportunity,  and civil rights. 2016. URL https://obamawhitehouse.archives.gov/sites/default/ files/microsites/ostp/2016_0504_data_discrimination.pdf.  Muhammad Bilal Zafar,  Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gum- Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification madi. without Disparate Mistreatment. CoRR, abs/1610.08452, 2016. URL http://dblp. uni-trier.de/db/journals/corr/corr1610.html#ZafarVGG16;http: //arxiv.org/abs/1610.08452. LEARNING NON-DISCRIMINATORY PREDICTORS  "}, "Empirical Risk Minimization for Stochastic Convex Optimization: O(1n)- and O(1n^2)-type of Risk Bounds": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Empirical Risk Minimization for Stochastic Convex Optimization: O(1n)- and O(1n^2)-type of Risk Bounds", "abstract": "Although there exist plentiful theories of empirical risk minimization (ERM) for supervised learning, current theoretical understandings of ERM for a related problem\u2014stochastic convex optimization (SCO), are limited. In this work, we strengthen the realm of ERM for SCO by exploiting smoothness and strong convexity conditions to improve the risk bounds. First, we establish an $\\widetilde{O}(d/n + \\sqrt{F}_*/n)$ risk bound when the random function is nonnegative, convex and smooth, and the expected function is Lipschitz continuous, where $d$ is the dimensionality of the problem, $n$ is the number of samples, and $F_*$ is the minimal risk. Thus, when $F_*$ is small we obtain an $\\widetilde{O}(d/n)$ risk bound, which is analogous to the $\\widetilde{O}(1/n)$ optimistic rate of ERM for supervised learning. Second, if the objective function is also $\u03bb$-strongly convex, we prove an $\\widetilde{O}(d/n  + \u03baF_*/n )$ risk bound where $\u03ba$ is the condition number, and improve it to $O(1/[\u03bbn^2] + \u03baF_*/n)$ when $n=\\widetilde{\u03a9}(\u03bad)$. As a result, we obtain an $O(\u03ba/n^2)$ risk bound under the condition that $n$ is large and $F_*$ is small, which to the best of our knowledge, is the first $O(1/n^2)$-type of risk bound of ERM. Third, we stress that the above results are established in a unified framework, which allows us to derive new risk bounds under weaker conditions, e.g., without convexity of the random function.  Finally, we demonstrate that to achieve an $O(1/[\u03bbn^2] + \u03baF_*/n)$ risk bound for supervised learning,  the $\\widetilde{\u03a9}(\u03bad)$ requirement on $n$ can be replaced with $\u03a9(\u03ba^2)$, which is dimensionality-independent.", "pdf_url": "http://proceedings.mlr.press/v65/zhang17a/zhang17a.pdf", "keywords": ["Empirical Risk Minimization", "Stochastic Convex Optimization", "Excess Risk"], "reference": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Giulia Desalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In Proceedings of  the 26th International Conference on Algorithmic Learning Theory, pages 254-269, 2015.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. ArXiv e-prints, arXiv:1608.04414, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. ArXiv e-prints, arXiv:1601.04011, 2016.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery  Problems. Springer, 2011.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning In Proceedings of the 9th Annual Conference on Computational Learning  with squared loss. Theory, pages 140-146, 1996.  13   O(1/n)- AND O(1/n2)-TYPE OF RISK BOUNDS OF ERM  References  Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright.  Information- theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Tran- sactions on Information Theory, 58(5):3235-3249, 2012.  Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pages 773- 781, 2013.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2002.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The  Annals of Statistics, 33(4):1497-1537, 2005.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Giulia Desalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In Proceedings of  the 26th International Conference on Algorithmic Learning Theory, pages 254-269, 2015.  Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes  back. ArXiv e-prints, arXiv:1608.04414, 2016.  Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning. impli- cations to exp-concave empirical risk minimization. ArXiv e-prints, arXiv:1601.04011, 2016.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for sto- chastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421-436, 2011.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315-323, 2013.  Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery  Problems. Springer, 2011.  Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances  in Neural Information Processing Systems 28, pages 1477-1485, 2015.  Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer, second edition, 2003.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.  Springer, 1991.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning In Proceedings of the 9th Annual Conference on Computational Learning  with squared loss. Theory, pages 140-146, 1996. ZHANG YANG JIN  Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Lower and upper bounds on the generalization In Proceedings of the 28th Conference on  of stochastic exponentially concave optimization. Learning Theory, 2015.  Colin McDiarmid. On the method of bounded differences.  In Surveys in Combinatorics, pages  148-188, 1989.  prints, arXiv:1605.01288, 2016.  Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. ArXiv e-  Ron Meir and Tong Zhang. Generalization error bounds for bayesian mixture algorithms. Journal  of Machine Learning Research, 4:839-860, 2003.  Eric Moulines and Francis R. Bach. Non-asymptotic analysis of stochastic approximation algo- rithms for machine learning. In Advances in Neural Information Processing Systems 24, pages 451-459, 2011.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  Yurii Nesterov. Introductory lectures on convex optimization: a basic course, volume 87 of Applied  optimization. Kluwer Academic Publishers, 2004.  Dmitriy Panchenko. Some extensions of an inequality of vapnik and chervonenkis. Electronic  Communications in Probability, 7:55-65, 2002.  Gilles Pisier. The volume of convex bodies and Banach space geometry. Cambridge Tracts in  Mathematics (No. 94). Cambridge University Press, 1989.  Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming. Communi-  cations on Pure and Applied Mathematics, 66(8):1275-1297, 2013.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449-456, 2012.  Bernhard Sch\u00a8olkopf and Alexander J. Smola. Learning with kernels: support vector machines,  regularization, optimization, and beyond. MIT Press, 2002.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-  mization. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy\u00b4nski. Lectures on Stochastic Pro-  gramming: Modeling and Theory. SIAM, second edition, 2014.  Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their ap-  proximations. Constructive Approximation, 26(2):153-172, 2007. O(1/n)- AND O(1/n2)-TYPE OF RISK BOUNDS OF ERM  Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth  loss. ArXiv e-prints, arXiv:1009.3896, 2010.  Karthik Sridharan, Shai Shalev-shwartz, and Nathan Srebro. Fast rates for regularized objectives.  In Advances in Neural Information Processing Systems 21, pages 1545-1552, 2009.  Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of  Statistics, 32:135-166, 2004.  Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 2000.  Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde- pendent access of full gradients. In Advance in Neural Information Processing Systems 26, pages 980-988, 2013a.  Lijun Zhang, Tianbao Yang, Rong Jin, and Xiaofei He. O(log T ) projections for stochastic op- timization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning, 2013b.  "}, "A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics", "abstract": "We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for non-convex optimization. The algorithm performs stochastic gradient descent, where in each step it injects appropriately scaled Gaussian noise to the update. We analyze the algorithm\u2019s hitting time to an arbitrary subset of the parameter space. Two results follow from our general theory: First, we prove that for empirical risk minimization, if the empirical risk is point-wise close to the (smooth) population risk, then the algorithm achieves an approximate local minimum of the population risk in polynomial time, escaping suboptimal local minima that only exist in the empirical risk. Second, we show that SGLD improves on one of the best known learnability results for learning linear classifiers under the zero-one loss.", "pdf_url": "http://proceedings.mlr.press/v65/zhang17b/zhang17b.pdf", "keywords": [], "reference": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding local  minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in  non-convex optimization. arXiv preprint arXiv:1602.05908, 2016.  Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on, pages 724-733. IEEE, 1993.  P. Awasthi, M.-F. Balcan, N. Haghtalab, and R. Urner. Efficient learning of linear separators under  bounded noise. In Proceedings of the 28th Conference on Learning Theory, 2015.  Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, 3:463-482, 2003.  Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing In 2013 IEEE International Conference on Acoustics, Speech and Signal  recurrent networks. Processing, pages 8624-8628. IEEE, 2013.  Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. Neural  Networks, 5(1):117-127, 1992.  preprint arXiv:1602.02616, 2016.  Thomas Bonis. Guarantees in wasserstein distance for the langevin monte carlo algorithm. arXiv  Anton Bovier, Michael Eckhoff, V\u00b4eronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society, 6(4):399-424, 2004.  14   ZHANG LIANG CHARIKAR  5. Conclusion  In this paper, we analyzed the hitting time of the SGLD algorithm on non-convex functions. Our approach is different from existing analyses on Langevin dynamics (Bubeck et al., 2015; Dalalyan, 2016; Bonis, 2016; Teh et al., 2016; Raginsky et al., 2017), which connect LMC to a continuous- time Langevin diffusion process, then study the mixing time of the latter process. In contrast, we are able to establish polynomial-time guarantees for achieving certain optimality sets, regardless of the exponential mixing time.  For future work, we hope to establish stronger results on non-convex optimization using the techniques developed in this paper. Our current analysis doesn\u2019t apply to training over-specified models. For these models, the empirical risk can be minimized far below the population risk (Safran and Shamir, 2015), thus the assumption of Corollary 8 is violated. In practice, over-specification often makes the optimization easier, thus it could be interesting to show that this heuristic actually improves the restricted Cheeger constant. Another open problem is avoiding poor population local minima. Jin et al. (2016) proved that there are many poor population local minima in training Gaussian mixture models. It would be interesting to investigate whether a careful initialization could prevent SGLD from hitting such bad solutions.  References  Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding local  minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.  Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in  non-convex optimization. arXiv preprint arXiv:1602.05908, 2016.  Sanjeev Arora, L\u00b4aszl\u00b4o Babai, Jacques Stern, and Z Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on, pages 724-733. IEEE, 1993.  P. Awasthi, M.-F. Balcan, N. Haghtalab, and R. Urner. Efficient learning of linear separators under  bounded noise. In Proceedings of the 28th Conference on Learning Theory, 2015.  Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and  structural results. The Journal of Machine Learning Research, 3:463-482, 2003.  Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing In 2013 IEEE International Conference on Acoustics, Speech and Signal  recurrent networks. Processing, pages 8624-8628. IEEE, 2013.  Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. Neural  Networks, 5(1):117-127, 1992.  preprint arXiv:1602.02616, 2016.  Thomas Bonis. Guarantees in wasserstein distance for the langevin monte carlo algorithm. arXiv  Anton Bovier, Michael Eckhoff, V\u00b4eronique Gayrard, and Markus Klein. Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical Society, 6(4):399-424, 2004. A HITTING TIME ANALYSIS OF STOCHASTIC GRADIENT LANGEVIN DYNAMICS  Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte carlo. In Advances in Neural Information Processing Systems, pages 1243-1251, 2015.  Jeff Cheeger. A lower bound for the smallest eigenvalue of the laplacian. 1969.  Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: the power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788-2806, 2015.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning  applications. Information and computation, 100(1):78-150, 1992.  Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J Wainwright, and Michael I Jordan. On local maxima in the population likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances In Neural Information Processing Systems, pages 4116- 4124, 2016.  \u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228,  2015.  Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. arXiv  preprint arXiv:1511.06392, 2015.  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-  tion. Annals of Statistics, pages 1302-1338, 2000.  Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges  to minimizers. University of California, Berkeley, 1050:16, 2016.  Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: statistical and algorithmic theory for local optima. The Journal of Machine Learning Research, 16(1):559-616, 2015.  L\u00b4aszl\u00b4o Lov\u00b4asz and Mikl\u00b4os Simonovits. Random walks in a convex body and an improved volume  algorithm. Random structures & algorithms, 4(4):359-412, 1993.  Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs  with gradient descent. arXiv preprint arXiv:1511.04834, 2015a.  Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015b. ZHANG LIANG CHARIKAR  Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global perfor-  mance. Mathematical Programming, 108(1):177-205, 2006.  Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural  networks. ICML (3), 28:1310-1318, 2013.  Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic  gradient langevin dynamics: a nonasymptotic analysis. 2017.  Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and  their discrete approximations. Bernoulli, pages 341-363, 1996.  Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.  arXiv preprint arXiv:1511.04210, 2015.  Yee Whye Teh, Alexandre H. Thiery, and Sebastian J. Vollmer. Consistency and \ufb02uctuations for stochastic gradient langevin dynamics. Journal of Machine Learning Research, 17(7):1-33, 2016.  Yung Liang Tong. The multivariate normal distribution. Springer Science & Business Media, 2012.  Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural net-  works, 10(5):988-999, 1999.  Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of conver-  gence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014.  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.  In  Proceedings of the 28th International Conference on Machine Learning, pages 681-688, 2011.  Albert Zeyer, Patrick Doetsch, Paul Voigtlaender, Ralf Schl\u00a8uter, and Hermann Ney. A comprehen- sive study of deep bidirectional lstm rnns for acoustic modeling in speech recognition. arXiv preprint arXiv:1606.06871, 2016.  "}, "Optimal learning via local entropies and sample compression": {"volumn": "v65", "url": "http://proceedings.mlr.press/v65/", "header": "Optimal learning via local entropies and sample compression", "abstract": "Under margin assumptions, we prove several risk bounds, represented via the distribution dependent local entropies of the classes or the sizes of specific sample compression schemes. In some cases, our guarantees are optimal up to constant factors for families of classes. We discuss limitations of our approach and give several applications. In particular, we provide a new tight PAC bound for the hard-margin SVM, an extended analysis of certain empirical risk minimizers under log-concave distributions, a new variant of an online to batch conversion, and distribution dependent localized bounds in the aggregation framework. As a part of our results, we give a new upper bound for the uniform deviations under Bernstein assumptions, which may be of independent interest. The proofs for the sample compression schemes are based on the moment method combined with the analysis of voting algorithms.", "pdf_url": "http://proceedings.mlr.press/v65/nikita17a/nikita17a.pdf", "keywords": ["Empirical risk minimization", "sample compression", "local entropy", "stability", "bracketing conditions", "VC classes", "hard margin SVM", "online to batch conversion"], "reference": "[1] R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applica-  tions to Markov chains. Electron. J. Probab., 1000-1034, 2008.  [2] T. M. Adams, A. B. Nobel. Uniform approximation and bracketing properties of VC classes.  [3] M. Anthony, P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  Bernoulli, 18:1310-1319, 2012.  University Press, 1999.  ing, 66(2-3): 151-163, 2007.  [4] P. Auer, R. Ortner. A new PAC bound for intersection-closed concept classes. Machine Learn-  [5] M.F. Balcan, P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Conference on Learning Theory, 2013.  [6] P. L. Bartlett, O. Bousquet, S. Mendelson. Local Rademacher Complexities. The Annals of  Statistics, 33(4):1497-1537, 08, 2005.  [7] P. L. Bartlett, S. Mendelson. Empirical minimization. Probability Theory and Related Fields,  135(3):311-334, 2006.  2002.  [8] O. Bousquet, A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  [9] S. Boucheron, G. Lugosi, P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Cambridge, 2013.  [10] N. H. Bshouty, Y. Li, P. M. Long. Using the doubling dimension to analyze the generalization  of learning algorithms. Journal of Computer and System Sciences, 2009.  [11] L. Devroye, L. Gy\u00a8orfi, G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, New York, 1996.  [12] A. Ehrenfeucht, D. Haussler, M. Kearns, L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  [13] S. Floyd and M. Warmuth. Sample Compression, learnability, and the Vapnik Chervonenkis  Dimension, Machine Learning, 21, 269-304 (1995).  [14] E. Gassiat, R. van Handel. The local geometry of finite mixtures, Trans. Amer. Math. Soc.  366, 1047-1072, 2014.  16   ZHIVOTOVSKIY  We would like to thank Steve Hanneke for several helpful discussions and anonymous reviewers for their useful suggestions. The author was supported solely by the Russian Science Foundation grant (project 14-50-00150).  Acknowledgments  References  [1] R. Adamczak. A tail inequality for suprema of unbounded empirical processes with applica-  tions to Markov chains. Electron. J. Probab., 1000-1034, 2008.  [2] T. M. Adams, A. B. Nobel. Uniform approximation and bracketing properties of VC classes.  [3] M. Anthony, P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge  Bernoulli, 18:1310-1319, 2012.  University Press, 1999.  ing, 66(2-3): 151-163, 2007.  [4] P. Auer, R. Ortner. A new PAC bound for intersection-closed concept classes. Machine Learn-  [5] M.F. Balcan, P. M. Long. Active and passive learning of linear separators under log-concave  distributions. In Proceedings of the 26th Conference on Learning Theory, 2013.  [6] P. L. Bartlett, O. Bousquet, S. Mendelson. Local Rademacher Complexities. The Annals of  Statistics, 33(4):1497-1537, 08, 2005.  [7] P. L. Bartlett, S. Mendelson. Empirical minimization. Probability Theory and Related Fields,  135(3):311-334, 2006.  2002.  [8] O. Bousquet, A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,  [9] S. Boucheron, G. Lugosi, P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Cambridge, 2013.  [10] N. H. Bshouty, Y. Li, P. M. Long. Using the doubling dimension to analyze the generalization  of learning algorithms. Journal of Computer and System Sciences, 2009.  [11] L. Devroye, L. Gy\u00a8orfi, G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of  Applications of Mathematics. Springer-Verlag, New York, 1996.  [12] A. Ehrenfeucht, D. Haussler, M. Kearns, L. Valiant. A general lower bound on the number of  examples needed for learning. Information and Computation, 82(3):247-261, 1989.  [13] S. Floyd and M. Warmuth. Sample Compression, learnability, and the Vapnik Chervonenkis  Dimension, Machine Learning, 21, 269-304 (1995).  [14] E. Gassiat, R. van Handel. The local geometry of finite mixtures, Trans. Amer. Math. Soc.  366, 1047-1072, 2014. OPTIMAL LEARNING VIA LOCAL ENTROPIES AND SAMPLE COMPRESSION  [15] E. Gin\u00b4e, V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type em-  pirical processes. The Annals of Probability, 34(3):1143-1216, 2006.  [16] S. Hanneke, L. Yang. Minimax analysis of active learning. Journal of Machine Learning Re-  search, 16 (12): 3487-3602, 2015.  [17] S. Hanneke. Refined error bounds for several learning algorithms. Journal of Machine Learning  [18] S. Hanneke. The Optimal Sample Complexity of PAC Learning. Journal of Machine Learning  Research 17, 1-55, 2016  Research, 17 (38): 1-15, 2016.  38-53, 1973.  Bernoulli 19 2153-2166, 2013.  [19] D. Haussler, N. Littlestone, M. Warmuth. Predicting {0, 1}-functions on randomly drawn  points. Information and Computation, 115:248-292, 1994.  [20] L. M. Le Cam. Convergence of estimates under dimensionality restrictions. Ann. Statist. 1,  [21] G. Lecu\u00b4e. Empirical risk minimization is optimal for the convex aggregation problem.  [22] G. Lecu\u00b4e. Interplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis. Habilitation thesis, Universit\u00b4e Paris-Est, 2011.  [23] G. Lecu\u00b4e, S. Mendelson On the optimality of the aggregate with exponential weights for low  temperature. Bernoulli, 2013.  [24] G. Lecu\u00b4e, S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. http:  //arxiv.org/abs/1305.4825, 2013.  [25] G. Lecu\u00b4e, C. Mitchell. Oracle inequalities for cross-validation type procedures. Electronic  Journal of Statistics, 6, 1803-1837, 2012.  [26] T. Liang, A. Rakhlin, K. Sridharan. Learning with square loss: Localization through offset Rademacher complexity. Proceedings of The 28th Conference on Learning Theory, 2015.  [27] N. Littlestone. From On-line to batch learning. In COLT, 1989.  [28] P. M. Long. On the sample complexity of PAC learning halfspaces against the uniform distri-  bution. IEEE Transactions on Neural Networks, 6(6):1556-1559, 1995.  [29] P. Massart, E. N\u00b4ed\u00b4elec. Risk bounds for statistical learning. Annals of Statistics, 2006.  [30] S. Mendelson. Obtaining fast error rates in nonconvex situations. Journal of Complexity. Vol-  ume 24, Issue 3, 380-397, 2008.  [31] S. Mendelson. \u2018Local\u2019 vs. \u2018global\u2019 parameters - breaking the Gaussian complexity barrier. https://arxiv.org/abs/1504.02191, to appear in Annals of Statisitcs, 2017  [32] S.Mendelson. Learning without concentration. Journal of the ACM, Volume 62, Issue 3, 2015. ZHIVOTOVSKIY  [33] A. Rakhlin, K. Sridharan, A. B. Tsybakov. Empirical entropy, minimax regret and minimax  risk. Bernoulli, 2017.  [34] H. Simon. An almost optimal PAC-algorithm. Proceedings of The 28th Conference on Learn-  ing Theory, pp. 1552-1563, 2015.  [35] A. B. Tsybakov. Optimal rates of aggregation. In Computational Learning Theory and Kernel  Machines 303-313. Lecture Notes in Artificial Intelligence, 2003.  [36] A. B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statis-  tics. Vol. 32, No. 1, 135-166, 2004  [37] A. W. van der Vaart, J. A. Wellner. Weak Convergence and Empirical Processes. Springer,  1996.  [38] V. Vapnik, A. Chervonenkis. On the uniform convergence of relative frequencies of events to  their probabilities. Proc. USSR Acad. Sci. 181(4), 781-783, 1968.  [39] V. Vapnik, A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974.  [40] M. K. Warmuth. The optimal PAC algorithm. In Proceedings of the 17th Conference on Learn-  ing Theory, 2004.  1, 252-273, 2003.  [41] M. Wegkamp. Model selection in nonparametric regression. Annals of Statistics, Vol. 31, No.  [42] Y. Yang, A. Barron. Information-theoretic determination of minimax rates of convergence. An-  nals of Statistics, 27, 1564-1599, 1999.  [43] N. Zhivotovskiy, S. Hanneke. Localization of VC classes: Beyond Local Rademacher com-  plexities. https://arxiv.org/abs/1606.00922, 2016.  "}}