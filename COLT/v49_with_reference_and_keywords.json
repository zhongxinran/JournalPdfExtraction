{"Conference on Learning Theory 2016: Preface": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Conference on Learning Theory 2016: Preface", "abstract": "Preface to COLT 2016", "pdf_url": "http://proceedings.mlr.press/v49/preface.pdf", "keywords": []}, "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies", "abstract": "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model. More complex models like Partially Observable Markov Decision Process (POMDP) can compensate for this drawback. Fitting this model to the environment, where the partial observation is given to the agent, generally gives dramatic performance improvement, sometimes unbounded improvement, compared to MDP. In general, finding the optimal policy for the POMDP model is computationally intractable and fully non convex, even for the class of memoryless policies. The open problem is to come up with a method to find an exact or an approximate optimal stochastic memoryless policy for POMDP models.", "pdf_url": "http://proceedings.mlr.press/v49/azizzadenesheli16b.pdf", "keywords": [], "reference": "Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In  Advances in neural information processing systems, pages 89-96, 2009.  Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of  pomdp\u2019s using spectral methods. arXiv preprint arXiv:1602.07764, 2016.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially  observable stochastic domains. Artificial intelligence, 101(1):99-134, 1998.  Steven M LaValle. Planning algorithms. Cambridge university press, 2006.  Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of pomdps under the expected average reward criterion. European Journal of Operational Research, 211(3):556-567, 2011.  Christos Papadimitriou and John N. Tsitsiklis. The complexity of markov decision processes. Math. Oper.  Res., 12(3):441-450, August 1987. ISSN 0364-765X.  Joelle Pineau, Geoffrey Gordon, and Sebastian Thrun. Anytime point-based approximations for large  pomdps. Journal of Artificial Intelligence Research, pages 335-380, 2006.  P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In  International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2008.  Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps.  In Advances in neural  information processing systems, pages 1225-1232, 2007.  Trey Smith and Reid Simmons. Heuristic search value iteration for pomdps. In Proceedings of the 20th  conference on Uncertainty in artificial intelligence, pages 520-527. AUAI Press, 2004.  E. J. Sondik. The optimal control of partially observable Markov processes. PhD thesis, Stanford University,  1971.  Edward J Sondik. The optimal control of partially observable markov processes over the infinite horizon:  Discounted costs. Operations Research, 26(2):282-304, 1978.  Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press, 1998.  4   AZIZZADENESHELI LAZARIC ANANDKUMAR  of the state distribution and then the policy is just a mapping from these partitions to the action. In general, planning in the space of memoryless policy has a lower level of complexity. Although, it is easier than belief based planning, it is still an NP \u2212 hard problem. To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies. Meanwhile, deterministic policies act poorly in the general case of POMDPs. Therefore, proposing a novel method to find the exact or approximated optimal memoryless policy (policy with performance \u01eb \u2212 close to the performance of optimal policy) or limited history dependent policy is the next step in the world of POMDP planning.  References  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In  Advances in neural information processing systems, pages 89-96, 2009.  Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of  pomdp\u2019s using spectral methods. arXiv preprint arXiv:1602.07764, 2016.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially  observable stochastic domains. Artificial intelligence, 101(1):99-134, 1998.  Steven M LaValle. Planning algorithms. Cambridge university press, 2006.  Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of pomdps under the expected average reward criterion. European Journal of Operational Research, 211(3):556-567, 2011.  Christos Papadimitriou and John N. Tsitsiklis. The complexity of markov decision processes. Math. Oper.  Res., 12(3):441-450, August 1987. ISSN 0364-765X.  Joelle Pineau, Geoffrey Gordon, and Sebastian Thrun. Anytime point-based approximations for large  pomdps. Journal of Artificial Intelligence Research, pages 335-380, 2006.  P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In  International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2008.  Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps.  In Advances in neural  information processing systems, pages 1225-1232, 2007.  Trey Smith and Reid Simmons. Heuristic search value iteration for pomdps. In Proceedings of the 20th  conference on Uncertainty in artificial intelligence, pages 520-527. AUAI Press, 2004.  E. J. Sondik. The optimal control of partially observable Markov processes. PhD thesis, Stanford University,  1971.  Edward J Sondik. The optimal control of partially observable markov processes over the infinite horizon:  Discounted costs. Operations Research, 26(2):282-304, 1978.  Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press, 1998. "}, "Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture", "abstract": "The best arm identification problem (BEST-1-ARM) is the most basic pure exploration problem in stochastic multi-armed bandits. The problem has a long history and attracted significant attention for the last decade. However, we do not yet have a complete understanding of the optimal sample complexity of the problem: The state-of-the-art algorithms achieve a sample complexity of O(\\sum_i=2^n \\Delta_i^-2(\\ln\u03b4^-1 + \\ln\\ln\\Delta_i^-1)) (\\Delta_i is the difference between the largest mean and the i^th mean), while the best known lower bound is \u03a9(\\sum_i=2^n \\Delta_i^-2\\ln\u03b4^-1) for general instances and \u03a9(\u2206^-2 \\ln\\ln \u2206^-1) for the two-arm instances. We propose to study the instance-wise optimality for the BEST-1-ARM problem. Previous work has proved that it is impossible to have an instance optimal algorithm for the 2-arm problem. However, we conjecture that modulo the additive term \u03a9(\\Delta_2^-2 \\ln\\ln \\Delta_2^-1) (which is an upper bound and worst case lower bound for the 2-arm problem), there is an instance optimal algorithm for BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropy for a best-arm problem instance, and conjecture that it is the instance-wise lower bound. Hence, resolving this conjecture would provide a final answer to the old and basic problem.", "pdf_url": "http://proceedings.mlr.press/v49/chen16b.pdf", "keywords": [], "reference": "Peyman Afshani, J\u00b4er\u00b4emy Barbay, and Timothy M Chan. Instance-optimal geometric algorithms. In Foundations of Computer Science, 2009. FOCS\u201909. 50th Annual IEEE Symposium on, pages 129-138. IEEE, 2009.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  arXiv:1511.03774, 2015.  Ronald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation algorithms for middleware.  Journal of Computer and System Sciences, 66(4):614-656, 2003.  RH Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals of  Mathematical Statistics, pages 36-72, 1964.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. COLT, 2014.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004.  4   CHEN LI  need to assign the failure probability \u03b4r carefully to each round (by union bound, we need (cid:80) r \u03b4r \u2264 \u03b4). The algorithm in Karnin et al. (2013) used \u03b4r = O(\u03b4 \u00b7 r\u22122), and we used a better way to assign \u03b4r. Indeed, if one can assign \u03b4r\u2019s optimally (i.e., minimize (cid:80) r \u03b4r \u2264 \u03b4), one could achieve the entropy bound (cid:80) i Hi). Of course, this does not lead to an algorithm directly, as we do not know His in advance.  subject to (cid:80) r Hr \u00b7 (ln \u03b4\u22121 + Ent(I)) (by letting \u03b4r = \u03b4Hr/ (cid:80)  r Hr ln \u03b4\u22121 r  Using our techniques, we can estimate the values Hr\u2019s when we enter the rth elimination stage. The only obstacle for implementing the above idea of assigning \u03b4r\u2019s optimally is that we do not know (cid:80) r Hr initially. We believe the difficulty can be overcome by additional new algorithmic ideas.  Lower Bounds:  In Chen and Li (2015), we prove the following lower bound, improving the MT lower bound. Theorem 4.1 (Theorem 1.6 in Chen and Li (2015)) There exist constants c, c1 > 0 and N \u2208 N such that, for any \u03b4 < 0.005 and any \u03b4-correct algorithm A, and any n \u2265 N , there exists an n arms instance I such that TA[I] \u2265 c \u00b7 (cid:80)n ln n \u00b7 (cid:80)n  i=2 \u2206\u22121 In fact, in the lower bound instances, there are log n nonempty groups Gi and they have almost the same weight Hi (hence, Ent(I) = \u0398(ln ln n)). Combining with the MT lower bound, we have covered the two extreme ends of Conjecture 3.5.  [i] ln ln n. Furthermore, \u2206\u22122  [2] ln ln \u2206\u22121  [2] < c1  [i] ln ln n.  i=2 \u2206\u22121  Moreover, it is possible to extend our current technique to construct many instances IS such that any algorithm A requires at least \u2126(H(IS) \u00b7 Ent(IS)) samples. This strongly suggests \u2126(H(I) \u00b7 Ent(I)) is the right lower bound. However, a complete resolution of Conjecture 3.5 seems to require new techniques.  References  Peyman Afshani, J\u00b4er\u00b4emy Barbay, and Timothy M Chan. Instance-optimal geometric algorithms. In Foundations of Computer Science, 2009. FOCS\u201909. 50th Annual IEEE Symposium on, pages 129-138. IEEE, 2009.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  arXiv:1511.03774, 2015.  Ronald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation algorithms for middleware.  Journal of Computer and System Sciences, 66(4):614-656, 2003.  RH Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals of  Mathematical Statistics, pages 36-72, 1964.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. COLT, 2014.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004. "}, "Open Problem: Kernel methods on manifolds and metric spaces. What is the probability of a positive definite geodesic exponential kernel?": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Kernel methods on manifolds and metric spaces. What is the probability of a positive definite geodesic exponential kernel?", "abstract": "Radial kernels are well-suited for machine learning over general geodesic metric spaces, where pairwise distances are often the only computable quantity available. We have recently shown that geodesic exponential kernels are only positive definite for all bandwidths when the input space has strong linear properties. This negative result hints that radial kernel are perhaps not suitable over geodesic metric spaces after all. Here, however, we present evidence that large intervals of bandwidths exist where geodesic exponential kernels have high probability of being positive definite over finite datasets, while still having significant predictive power. From this we formulate conjectures on the probability of a positive definite kernel matrix for a finite random sample, depending on the geometry of the data space and the spread of the sample.", "pdf_url": "http://proceedings.mlr.press/v49/feragen16.pdf", "keywords": ["Kernel methods", "geodesic metric spaces", "geodesic exponential kernel", "positive definiteness", "curvature", "bandwidth selection"], "reference": "S. Amari and H. Nagaoka. Methods of information geometry. Translations of mathematical mono-  graphs; v. 191. American Mathematical Society, 2000.  M.R. Bridson and A. Hae\ufb02iger. Metric spaces of non-positive curvature. Springer, 1999.  O. Chapelle, P. Haffner, and V.N. Vapnik. Support vector machines for histogram-based image  classification. Neural Networks, IEEE Transactions on, 10(5):1055-1064, 1999.  N. Courty, T. Burger, and P.-F. Marteau. Geodesic analysis on the Gaussian RKHS hypersphere. In  ECML PKDD, volume 7523 of LNCS, pages 299-313. 2012.  A. Feragen, F. Lauze, and S. Hauberg. Geodesic exponential kernels: When curvature and linearity  con\ufb02ict. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.  P.T. Fletcher and S.C. Joshi. Principal geodesic analysis on symmetric spaces: Statistics of diffu- sion tensors. In Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis, Revised Selected Papers, pages 87-98, 2004.  U. Grenander and M.I. Miller. Computational anatomy: An emerging discipline. Q. Appl. Math.,  LVI(4):617-694, December 1998.  M. Harandi and M. Salzmann. Riemannian coding and dictionary learning: Kernels to the rescue.  In CVPR, pages 3926-3935, Boston, USA, jun 2015.  M. Harandi, M. Salzmann, S. Jayasumana, R. Hartley, and H. Li. Expanding the family of grassman- nian kernels: An embedding perspective. In European Conference on Computer Vision (ECCV), pages 408-423, 2014.  T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Advances  in Neural Information Processing Systems (NIPS), pages 487-493, 1998.  S. Jayasumana, M. Salzmann, H. Li, and M. Harandi. A Framework for Shape Analysis via Hilbert  Space Embedding. In International Conference on Computer Vision (ICCV), 2013.  S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi. Optimizing over radial kernels on compact manifolds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3802-3809, 2014.  S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi. Kernel Methods on Riemannian IEEE Transactions on Pattern Analysis and Machine  Manifolds with Gaussian RBF Kernels. Intelligence (TPAMI), 2015.  J. Matousek and A. Sidiropoulos. Inapproximability for metric embeddings into Rd. IEEE Sympo-  sium on Foundations of Computer Science (FOCS 2008); Transactions of the AMS, 2010.  B. Sch\u00a8olkopf and A.J. Smola. Learning with kernels : support vector machines, regularization,  optimization, and beyond. Adaptive computation and machine learning. MIT Press, 2002.  A. Sidiropoulos and Y. Wang. Metric embeddings with outliers. ArXiV preprint, http://arxiv.  org/abs/1508.03600, 2015.  4   FERAGEN HAUBERG  References  S. Amari and H. Nagaoka. Methods of information geometry. Translations of mathematical mono-  graphs; v. 191. American Mathematical Society, 2000.  M.R. Bridson and A. Hae\ufb02iger. Metric spaces of non-positive curvature. Springer, 1999.  O. Chapelle, P. Haffner, and V.N. Vapnik. Support vector machines for histogram-based image  classification. Neural Networks, IEEE Transactions on, 10(5):1055-1064, 1999.  N. Courty, T. Burger, and P.-F. Marteau. Geodesic analysis on the Gaussian RKHS hypersphere. In  ECML PKDD, volume 7523 of LNCS, pages 299-313. 2012.  A. Feragen, F. Lauze, and S. Hauberg. Geodesic exponential kernels: When curvature and linearity  con\ufb02ict. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.  P.T. Fletcher and S.C. Joshi. Principal geodesic analysis on symmetric spaces: Statistics of diffu- sion tensors. In Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis, Revised Selected Papers, pages 87-98, 2004.  U. Grenander and M.I. Miller. Computational anatomy: An emerging discipline. Q. Appl. Math.,  LVI(4):617-694, December 1998.  M. Harandi and M. Salzmann. Riemannian coding and dictionary learning: Kernels to the rescue.  In CVPR, pages 3926-3935, Boston, USA, jun 2015.  M. Harandi, M. Salzmann, S. Jayasumana, R. Hartley, and H. Li. Expanding the family of grassman- nian kernels: An embedding perspective. In European Conference on Computer Vision (ECCV), pages 408-423, 2014.  T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Advances  in Neural Information Processing Systems (NIPS), pages 487-493, 1998.  S. Jayasumana, M. Salzmann, H. Li, and M. Harandi. A Framework for Shape Analysis via Hilbert  Space Embedding. In International Conference on Computer Vision (ICCV), 2013.  S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi. Optimizing over radial kernels on compact manifolds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3802-3809, 2014.  S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi. Kernel Methods on Riemannian IEEE Transactions on Pattern Analysis and Machine  Manifolds with Gaussian RBF Kernels. Intelligence (TPAMI), 2015.  J. Matousek and A. Sidiropoulos. Inapproximability for metric embeddings into Rd. IEEE Sympo-  sium on Foundations of Computer Science (FOCS 2008); Transactions of the AMS, 2010.  B. Sch\u00a8olkopf and A.J. Smola. Learning with kernels : support vector machines, regularization,  optimization, and beyond. Adaptive computation and machine learning. MIT Press, 2002.  A. Sidiropoulos and Y. Wang. Metric embeddings with outliers. ArXiV preprint, http://arxiv.  org/abs/1508.03600, 2015. "}, "Open Problem: Second order regret bounds based on scaling time": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Second order regret bounds based on scaling time", "abstract": "We argue that the second order bounds given in Cesa-Bianchi2006, which accumulate the square of the loss of each action separately, are loose. We propose a different form of a second order bound and conjecture the it is satisfied by NormalHedge ChaudhuriFrHs2009.", "pdf_url": "http://proceedings.mlr.press/v49/freund16.pdf", "keywords": [], "reference": "Nicol`o Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and In Proceedings of the Twenty-Fifth Annual  Manfred K. Warmuth. How to use expert advice. ACM Symposium on the Theory of Computing, pages 382-391, 1993.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.  Improved second-order bounds ISSN for prediction with expert advice. 1573-0565. doi: 10.1007/s10994-006-5001-7. URL http://dx.doi.org/10.1007/ s10994-006-5001-7.  Machine Learning, 66(2):321-352, 2006.  Kamalika Chaudhuri, Yoav Freund, and Daniel J. Hsu. A parameter-free hedging algorithm. CoRR,  abs/0903.2851, 2009. URL http://arxiv.org/abs/0903.2851.  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts.  CoRR, abs/1006.0475, 2010. URL http://arxiv.org/abs/1006.0475.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinato- rial games. CoRR, abs/1502.08009, 2015. URL http://arxiv.org/abs/1502.08009.  Nick Littlestone and Manfred Warmuth. The weighted majority algorithm. In 30th Annual Sympo-  sium on Foundations of Computer Science, pages 256-261, October 1989.  Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive normalhedge.  CoRR, abs/1502.05934, 2015. URL http://arxiv.org/abs/1502.05934.  4   FREUND  I would like to thank Gergely Neu for pointing out that both types of variance are analyzed in Cesa- Bianchi et al. (2006).  I wish to also apologize in advance for any omittions, errors or misrepresentation I made in this  5. Thanks and apologies  note. I welcome any comments.  References  Nicol`o Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and In Proceedings of the Twenty-Fifth Annual  Manfred K. Warmuth. How to use expert advice. ACM Symposium on the Theory of Computing, pages 382-391, 1993.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.  Improved second-order bounds ISSN for prediction with expert advice. 1573-0565. doi: 10.1007/s10994-006-5001-7. URL http://dx.doi.org/10.1007/ s10994-006-5001-7.  Machine Learning, 66(2):321-352, 2006.  Kamalika Chaudhuri, Yoav Freund, and Daniel J. Hsu. A parameter-free hedging algorithm. CoRR,  abs/0903.2851, 2009. URL http://arxiv.org/abs/0903.2851.  Alexey V. Chernov and Vladimir Vovk. Prediction with advice of unknown number of experts.  CoRR, abs/1006.0475, 2010. URL http://arxiv.org/abs/1006.0475.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games  and Economic Behavior, 29:79-103, 1999.  Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinato- rial games. CoRR, abs/1502.08009, 2015. URL http://arxiv.org/abs/1502.08009.  Nick Littlestone and Manfred Warmuth. The weighted majority algorithm. In 30th Annual Sympo-  sium on Foundations of Computer Science, pages 256-261, October 1989.  Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive normalhedge.  CoRR, abs/1502.05934, 2015. URL http://arxiv.org/abs/1502.05934. "}, "Open Problem: Property Elicitation and Elicitation Complexity": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Property Elicitation and Elicitation Complexity", "abstract": "The study of property elicitation is gaining ground in statistics and machine learning as a way to view and reason about the expressive power of emiprical risk minimization (ERM).  Yet beyond a widening frontier of special cases, the two most fundamental questions in this area remain open: which statistics are elicitable (computable via ERM), and which loss functions elicit them?  Moreover, recent work suggests a complementary line of questioning: given a statistic, how many ERM parameters are needed to compute it?  We give concrete instantiations of these important questions, which have numerous applications to machine learning and related fields.", "pdf_url": "http://proceedings.mlr.press/v49/frongillo16.pdf", "keywords": ["Property elicitation", "empirical risk minimization"], "reference": "J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the 25th Conference on Learning Theory, pages 1-27, 2012. URL http://jmlr.csail.mit.edu/ proceedings/papers/v23/abernethy12/abernethy12.pdf.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138-156, 2006. URL http://amstat. tandfonline.com/doi/abs/10.1198/016214505000000907.  Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osband\u2019s principle. arXiv:1503.08123 [math, q-fin, stat], March 2015. URL http://arxiv.org/abs/1503.08123. arXiv: 1503.08123.  Tobias Fissler, Johanna F. Ziegel, and Tilmann Gneiting. Expected Shortfall is jointly elicitable with Value at Risk - Implications for backtesting. arXiv:1507.00244 [q-fin], July 2015. URL http://arxiv.org/ abs/1507.00244. arXiv: 1507.00244.  Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis.  In Web and  Internet Economics, pages 354-370. Springer, 2014.  Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th Conference  on Learning Theory, pages 1-18, 2015a.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity and Conditional Elicitation. arXiv preprint  arXiv:1506.07212, 2015b. URL http://arxiv.org/abs/1506.07212.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity. In Advances in Neural Information Processing  Systems 29, 2015c.  (494):746-762, 2011.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings  of the 9th ACM Conference on Electronic Commerce, pages 129-138, 2008.  J. A. Nelder and R. J. Baker. Generalized Linear Models. In Encyclopedia of Statistical Sciences. John Wiley & Sons, Inc., 2004. ISBN 978-0-471-66719-3. URL http://onlinelibrary.wiley.com/doi/ 10.1002/0471667196.ess0866.pub2/abstract.  J. A. Nelder and R. W. M. Wedderburn. Generalized Linear Models. Journal of the Royal Statistical Society. ISSN 0035-9238. doi: 10.2307/2344614. URL http:  Series A (General), 135(3):370-384, 1972. //www.jstor.org/stable/2344614.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley,  1985.  Ingo Steinwart, Chlo Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties.  In Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014.  4   FRONGILLO KASH BECKER  References  J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the 25th Conference on Learning Theory, pages 1-27, 2012. URL http://jmlr.csail.mit.edu/ proceedings/papers/v23/abernethy12/abernethy12.pdf.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138-156, 2006. URL http://amstat. tandfonline.com/doi/abs/10.1198/016214505000000907.  Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osband\u2019s principle. arXiv:1503.08123 [math, q-fin, stat], March 2015. URL http://arxiv.org/abs/1503.08123. arXiv: 1503.08123.  Tobias Fissler, Johanna F. Ziegel, and Tilmann Gneiting. Expected Shortfall is jointly elicitable with Value at Risk - Implications for backtesting. arXiv:1507.00244 [q-fin], July 2015. URL http://arxiv.org/ abs/1507.00244. arXiv: 1507.00244.  Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis.  In Web and  Internet Economics, pages 354-370. Springer, 2014.  Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th Conference  on Learning Theory, pages 1-18, 2015a.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity and Conditional Elicitation. arXiv preprint  arXiv:1506.07212, 2015b. URL http://arxiv.org/abs/1506.07212.  Rafael Frongillo and Ian A. Kash. On Elicitation Complexity. In Advances in Neural Information Processing  Systems 29, 2015c.  (494):746-762, 2011.  T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106  N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.  N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings  of the 9th ACM Conference on Electronic Commerce, pages 129-138, 2008.  J. A. Nelder and R. J. Baker. Generalized Linear Models. In Encyclopedia of Statistical Sciences. John Wiley & Sons, Inc., 2004. ISBN 978-0-471-66719-3. URL http://onlinelibrary.wiley.com/doi/ 10.1002/0471667196.ess0866.pub2/abstract.  J. A. Nelder and R. W. M. Wedderburn. Generalized Linear Models. Journal of the Royal Statistical Society. ISSN 0035-9238. doi: 10.2307/2344614. URL http:  Series A (General), 135(3):370-384, 1972. //www.jstor.org/stable/2344614.  Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley,  1985.  Ingo Steinwart, Chlo Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties.  In Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014. "}, "Open Problem: Parameter-Free and Scale-Free Online Algorithms": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Open Problem: Parameter-Free and Scale-Free Online Algorithms", "abstract": "Existing vanilla algorithms for online linear optimization have O((\u03b7R(u) + 1/\u03b7) \\sqrtT) regret with respect to any competitor u, where R(u) is a 1-strongly convex regularizer and \u03b7> 0 is a tuning parameter of the algorithm. For certain decision sets and regularizers, the so-called \\emphparameter-free algorithms have \\widetilde O(\\sqrtR(u) T) regret with respect to any competitor u.  Vanilla algorithm can achieve the same bound only for a fixed competitor u known ahead of time by setting \u03b7= 1/\\sqrtR(u). A drawback of both vanilla and parameter-free algorithms is that they assume that the norm of the loss vectors is bounded by a constant known to the algorithm. There exist \\emphscale-free algorithms that have O((\u03b7R(u) + 1/\u03b7) \\sqrtT \\max_1 \\le t \\le T \\norm\\ell_t) regret with respect to any competitor u and for any sequence of loss vector \\ell_1, \u2026, \\ell_T. Parameter-free analogue of scale-free algorithms have never been designed. Is is possible to design algorithms that are simultaneously \\emphparameter-free and \\emphscale-free?", "pdf_url": "http://proceedings.mlr.press/v49/orabona16.pdf", "keywords": [], "reference": "2006.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In Y. Bengio, D. Schu- urmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22 (NIPS 2009), December 7-12, Vancouver, BC, Canada, pages 297-305, 2009.  A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Peter Gr\u00a8unwald and Peter Spirtes, editors, Proc. of the 26th Conf. on Uncertainty in Artificial Intelligence (UAI 2010), July 8-11, Catalina Island, California, USA. AUAI Press, 2010.  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. The Journal of Machine Learning Research, 15(1):1281-1316, 2014.  D. J. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 3375-3383. Curran Associates, Inc., 2015.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an appli-  cation to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proc. of the 28th Conf. on  games. Learning Theory (COLT 2015), July 3-6, Paris, France, pages 1155-1175, 2015.  H. Luo and R. E. Schapire. A drifting-games analysis for online learning and applications to boost- ing. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1368-1376. Curran Associates, Inc., 2014.  H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge.  In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proc. of the 28th Conf. on Learning Theory (COLT 2015), July 3-6, Paris, France, pages 1286-1304, 2015.  H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimiza- tion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 2724-2732, 2013.  H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations. In Vitaly Feldman, Csaba Szepesvri, and Maria Flo- rina Balcan, editors, Proc. of The 27th Conf. on Learning Theory (COLT 2014), June 13-15, Barcelona, Spain, pages 1020-1039, 2014.  F. Orabona. Dimension-free exponentiated gradient. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys- tems 26, pages 1806-1814. Curran Associates, Inc., 2013.  5   OPEN PROBLEM: PARAMETER-FREE AND SCALE-FREE ONLINE ALGORITHMS  References  2006.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In Y. Bengio, D. Schu- urmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22 (NIPS 2009), December 7-12, Vancouver, BC, Canada, pages 297-305, 2009.  A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Peter Gr\u00a8unwald and Peter Spirtes, editors, Proc. of the 26th Conf. on Uncertainty in Artificial Intelligence (UAI 2010), July 8-11, Catalina Island, California, USA. AUAI Press, 2010.  S. de Rooij, T. van Erven, P. D. Gr\u00a8unwald, and W. M. Koolen. Follow the leader if you can, hedge  if you must. The Journal of Machine Learning Research, 15(1):1281-1316, 2014.  D. J. Foster, A. Rakhlin, and K. Sridharan. Adaptive online learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 3375-3383. Curran Associates, Inc., 2015.  Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an appli-  cation to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proc. of the 28th Conf. on  games. Learning Theory (COLT 2015), July 3-6, Paris, France, pages 1155-1175, 2015.  H. Luo and R. E. Schapire. A drifting-games analysis for online learning and applications to boost- ing. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1368-1376. Curran Associates, Inc., 2014.  H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge.  In Peter Gr\u00a8unwald, Elad Hazan, and Satyen Kale, editors, Proc. of the 28th Conf. on Learning Theory (COLT 2015), July 3-6, Paris, France, pages 1286-1304, 2015.  H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimiza- tion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 2724-2732, 2013.  H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations. In Vitaly Feldman, Csaba Szepesvri, and Maria Flo- rina Balcan, editors, Proc. of The 27th Conf. on Learning Theory (COLT 2014), June 13-15, Barcelona, Spain, pages 1020-1039, 2014.  F. Orabona. Dimension-free exponentiated gradient. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys- tems 26, pages 1806-1814. Curran Associates, Inc., 2013. ORABONA P \u00b4AL  F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic  learning. In Advances in Neural Information Processing Systems 27, 2014.  F. Orabona and D. P\u00b4al. Scale-free algorithms for online linear optimization. In Kamalika Chaudhuri and Claudio Gentile, editors, Proc. of Algorithmic Learning Theory (ALT) 2015, October 4-6, Banff, AB, Canada, pages 287-301. Springer, 2015.  F. Orabona and D. P\u00b4al. From coin betting to parameter-free online learning, 2016a. Available from:  http://arxiv.org/pdf/1602.04128.pdf.  F. Orabona and D. P\u00b4al. Scale-free online learning, 2016b. Submitted to Theoretical Computer Science, ALT 2015 Special Issue, arXiv:1601.01974, Available from: http://arxiv.org/ pdf/1601.01974.pdf.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimization. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25 (NIPS 2012), Decmber 3-8, Lake Tahoe, Nevada, USA, pages 2402-2410, 2012. "}, "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives", "abstract": "We consider a contextual version of multi-armed bandit problem with global knapsack constraints. In each round, the outcome of pulling an arm is a scalar reward and a resource consumption vector, both dependent on the context, and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget. The learning agent competes with an arbitrary set of context-dependent policies. This problem was introduced by Badanidiyuru et al., who gave a computationally inefficient algorithm with near-optimal regret bounds for it.  We give a \\emphcomputationally efficient algorithm for this problem with slightly better regret bounds, by generalizing the approach of Dudik et al. for the non-constrained version of the problem. The computational time of our algorithm scales \\emphlogarithmically in the size of the policy space. This answers the main open question of Badanidiyuru et al. We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors.", "pdf_url": "http://proceedings.mlr.press/v49/agrawal16.pdf", "keywords": [], "reference": "bandits. In NIPS, 2012.  Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In ICML, June 2014. URL http://arxiv.org/abs/1402.0555. Full version on arXiv.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In  EC, 2014.  Shipra Agrawal, Nikhil R. Devanur, and Lihong Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. CoRR, abs/1506.03374, 2016. URL http://arxiv.org/abs/1506.03374.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Moshe Babaioff, Shaddin Dughmi, Robert D. Kleinberg, and Aleksandrs Slivkins. Dynamic pricing with limited supply. ACM Trans. Economics and Comput., 3(1):4, 2015. doi: 10.1145/2559152. URL http://doi.acm.org/10.1145/2559152.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted  price mechanisms for online procurement. In EC, pages 128-145. ACM, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  In FOCS, pages 207-216, 2013.  Ashwinkumar Badanidiyuru, John Langford, and Aleksandrs Slivkins. Resourceful contextual ban-  dits. In COLT, pages 1109-1134, 2014.  Andrew G. Barto and P. Anandan. Pattern-recognizing stochastic learning automata. IEEE Trans-  actions on Systems, Man, and Cybernetics, 15(3):360-375, 1985.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk  bounds and near-optimal algorithms. Operations Research, 57(6):1407-1420, 2009.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual  bandit algorithms with supervised learning guarantees. In AISTATS, pages 19-26, 2011.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  Deepayan Chakrabarti and Erik Vee. Traffic shaping to optimize ad delivery. In EC, 2012.  Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual Bandits with Linear Payoff  Functions. In AISTATS, 2011.  Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget con-  straint and variable costs. In AAAI, pages 232-238, 2013.  14   AGRAWAL DEVANUR LI  References  bandits. In NIPS, 2012.  Yasin Abbasi-yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic  Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E. Schapire. Tam- ing the monster: A fast and simple algorithm for contextual bandits. In ICML, June 2014. URL http://arxiv.org/abs/1402.0555. Full version on arXiv.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In  EC, 2014.  Shipra Agrawal, Nikhil R. Devanur, and Lihong Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. CoRR, abs/1506.03374, 2016. URL http://arxiv.org/abs/1506.03374.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Moshe Babaioff, Shaddin Dughmi, Robert D. Kleinberg, and Aleksandrs Slivkins. Dynamic pricing with limited supply. ACM Trans. Economics and Comput., 3(1):4, 2015. doi: 10.1145/2559152. URL http://doi.acm.org/10.1145/2559152.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Yaron Singer. Learning on a budget: posted  price mechanisms for online procurement. In EC, pages 128-145. ACM, 2012.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.  In FOCS, pages 207-216, 2013.  Ashwinkumar Badanidiyuru, John Langford, and Aleksandrs Slivkins. Resourceful contextual ban-  dits. In COLT, pages 1109-1134, 2014.  Andrew G. Barto and P. Anandan. Pattern-recognizing stochastic learning automata. IEEE Trans-  actions on Systems, Man, and Cybernetics, 15(3):360-375, 1985.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk  bounds and near-optimal algorithms. Operations Research, 57(6):1407-1420, 2009.  Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual  bandit algorithms with supervised learning guarantees. In AISTATS, pages 19-26, 2011.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  Deepayan Chakrabarti and Erik Vee. Traffic shaping to optimize ad delivery. In EC, 2012.  Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual Bandits with Linear Payoff  Functions. In AISTATS, 2011.  Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget con-  straint and variable costs. In AAAI, pages 232-238, 2013. CONTEXTUAL BANDITS WITH KNAPSACKS  Miroslav Dud\u00b4\u0131k, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In UAI, pages 169-178, 2011.  Sudipto Guha and Kamesh Munagala. Approximation algorithms for budgeted learning problems.  In STOC, pages 104-113, 2007.  Andr\u00b4as Gy\u00a8orgy, Levente Kocsis, Ivett Szab\u00b4o, and Csaba Szepesv\u00b4ari. Continuous time associative  bandit problems. In IJCAI, pages 830-835, 2007.  John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits.  In NIPS, pages 1096-1103, 2008.  Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its im- plications for combinatorial and convex optimization. In FOCS, pages 1049-1065. IEEE, 2015. URL http://arxiv.org/abs/1508.04874v1. Full version on arXiv .  Omid Madani, Daniel J Lizotte, and Russell Greiner. The budgeted multi-armed bandit problem. In  Learning Theory, pages 643-645. Springer, 2004.  Sandeep Pandey and Christopher Olston. Handling advertisements of unknown quality in search  advertising. In NIPS, pages 1065-1072, 2006.  Adish Singla and Andreas Krause. Truthful incentives in crowdsourcing tasks using regret mini-  mization mechanisms. In WWW, pages 1167-1178, 2013.  Aleksandrs Slivkins and Jennifer Wortman Vaughan. Online decision making in crowdsourcing  markets: Theoretical challenges (position paper). CoRR, abs/1308.1746, 2013.  Long Tran-Thanh, Archie C. Chapman, Enrique Munoz de Cote, Alex Rogers, and Nicholas R.  Jennings. Epsilon-first policies for budget-limited multi-armed bandits. In AAAI, 2010.  Long Tran-Thanh, Archie C. Chapman, Alex Rogers, and Nicholas R. Jennings. Knapsack based  optimal policies for budget-limited multi-armed bandits. In AAAI, 2012.  Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In FOCS,  pages 338-343. IEEE, 1989a.  Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In Foundations  of Computer Science, 1989., 30th Annual Symposium on, pages 332-337. IEEE, 1989b.  Huasen Wu, R. Srikant, Xin Liu, and Chong Jiang. Algorithms with logarithmic or sublinear regret  for constrained contextual bandits. In NIPS, pages 433-441, 2015. "}, "Learning and Testing Junta Distributions": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Learning and Testing Junta Distributions", "abstract": "We consider the problem of learning distributions in the presence of irrelevant features. This problem is formalized by introducing a new notion of \\emphk-junta distributions. Informally, a distribution \\mathcalD over the domain \\mathcalX^n is a \\emphk-junta distribution with respect to another distribution \\mathcalU over the same domain if there is a set J \u2286[n] of size |J| \\le k that captures the difference between \\mathcal D and \\mathcalU. We show that it is possible to learn k-junta distributions with respect to the uniform distribution over the Boolean hypercube {0,1}^n in time \\poly(n^k, 1/\u03b5). This result is obtained via a new Fourier-based learning algorithm inspired by the Low-Degree Algorithm of Linial, Mansour, and Nisan (1993). We also consider the problem of testing whether an unknown distribution is a k-junta distribution with respect to the uniform distribution. We give a nearly-optimal algorithm for this task. Both the analysis of the algorithm and the lower bound showing its optimality are obtained by establishing connections between the problem of testing junta distributions and testing uniformity of weighted collections of distributions.", "pdf_url": "http://proceedings.mlr.press/v49/aliakbarpour16.pdf", "keywords": ["Learning distributions", "Property testing", "Juntas", "Fourier analysis"], "reference": "Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Test- ing that distributions are close. In 41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA, pages 259-269, 2000. doi: 10.1109/SFCS.2000.892113. URL http://dx.doi.org/10.1109/ SFCS.2000.892113.  Avrim Blum. Relevant examples and relevant features: Thoughts from computational  learning theory. In AAAI Fall Symposium on \u2018Relevance\u2019, volume 5, 1994.  17   Learning and Testing Junta Distributions  (cid:96) is not in J, x(J) = y(J). Thus, y is also in Xi. On the other hand, the parity of the first k bits of x and y is not the same, because (cid:96) \u2208 [k]. Thus, one of them has probability zero. Note that we can pair up all the elements in Xi as we did for to x and y. Therefore, at least half of the elements in Xi have probability zero. By definition, we know ai = (cid:80) D(cid:48)(x) for i = 0, 1, . . . , 2k \u2212 1 and D(cid:48)(x) = ai/2k2 for x \u2208 Xi. We can conclude that D(cid:48) is 1/4-far from D, since  x\u2208Xi  dTV(D, D(cid:48)) =  |D(x) \u2212 D(cid:48)(x)| =  |D(x) \u2212 D(cid:48)(x)|  (cid:88)  x  1 2  1 2  =  2k\u22121 (cid:88)  (cid:88)  i=0  x\u2208Xi  1 2  2k\u22121 (cid:88)  (cid:88)  i=0  x\u2208Xi  2k\u22121 (cid:88)  1 2  |D(x) \u2212 ai/2k2| \u2265  (cid:88)  ai/2k2 \u2265  i=0  x\u2208Xi:D(x)=0  2k\u22121 (cid:88)  i=0  ai 4  \u2265  1 4  where the first inequality follows from the fact that at least 2k2\u22121 elements in Xi have probability zero. Since D(cid:48) is an arbitrary k-junta distribution on an arbitrary set J (cid:54)= [k], it follows that D is 1/4-far from being a junta distribution on any J (cid:54)= [k].  As we mentioned earlier, this implies that D is a k-junta distribution i\ufb00 C is a collection of uniform distributions. It is not hard to see that we can convert a sample drawn from C to get a sample from D. Thus, any (cid:15)-tester for a k-junta distribution can be used as an (cid:15)-tester for a collection of uniform distributions. Diakonikolas and Kane (2016) (in section M N /(cid:15)2) = \u2126(2(k1+k2)/2) samples are required to distinguish a 3.1.1) have shown that \u2126( collection of M uniform distributions from a collection which is (cid:15)-far from being uniform with probability 2/3. This implies that we need \u2126(2n/2/(cid:15)2) samples to test k-junta distri- butions.  \u221a  5. Acknowledgements  This material is based upon work supported by the National Science Foundation Gradu- ate Research Fellowship under Grant No. CCF-1420692 and CCF-1065125. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily re\ufb02ect the views of the National Science Foundation. We thank anonymous reviewers for their insightful comments on the preliminary version of this paper.  References  Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Test- ing that distributions are close. In 41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA, pages 259-269, 2000. doi: 10.1109/SFCS.2000.892113. URL http://dx.doi.org/10.1109/ SFCS.2000.892113.  Avrim Blum. Relevant examples and relevant features: Thoughts from computational  learning theory. In AAAI Fall Symposium on \u2018Relevance\u2019, volume 5, 1994. Aliakbarpour Blais Rubinfeld  Avrim Blum and Pat Langley. Selection of relevant features and examples in machine  learning. Artificial Intelligence, 97(1-2):245-271, December 1997.  Cl\u00b4ement L Canonne, Dana Ron, and Rocco A Servedio. Testing probability distributions  using conditional samples. SIAM Journal on Computing, 44(3):540-616, 2015.  Sourav Chakraborty, Eldar Fischer, Yonatan Goldhirsh, and Arie Matsliah. On the power of conditional samples in distribution testing. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS \u201913, pages 561-580, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1859-4. doi: 10.1145/2422436.2422497. URL http: //doi.acm.org/10.1145/2422436.2422497.  Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers  & Electrical Engineering, 40(1):16 - 28, 2014.  Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning k-modal distributions via testing. Theory of Computing, 10(20):535-570, 2014. doi: 10.4086/toc. 2014.v010a020. URL http://www.theoryofcomputing.org/articles/v010a020.  Luc Devroye and G\u00b4abor Lugosi. Combinatorial methods in density estimation. Springer,  2001.  Ilias Diakonikolas. Learning structured distributions. In CRC Handbook of Big Data. 2016.  Ilias Diakonikolas and Daniel M. Kane. A new approach for testing properties of discrete distributions. CoRR, abs/1601.05557, 2016. URL http://arxiv.org/abs/1601.05557.  Isabelle Guyon and Andr\u00b4e Elissee\ufb00. An introduction to variable and feature selection.  Journal of Machine Learning Research, 3:1157-1182, 2003.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnos-  tically learning halfspaces. SIAM J. Comput., 37(6):1777-1805, 2008.  Michael J. Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. On the learnability of discrete distributions. In Proceedings of the Twenty- Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr\u00b4eal, Qu\u00b4ebec, Canada, pages 273-282, 1994. doi: 10.1145/195058.195155. URL http://doi. acm.org/10.1145/195058.195155.  Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distribu-  tions. Theory of Computing, 9(8):295-347, 2013.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier trans-  form, and learnability. Journal of the ACM (JACM, 40(3):607-620, July 1993.  Huan Liu and Hiroshi Motoda. Feature selection for knowledge discovery and data mining,  volume 454. Springer Science & Business Media, 2012.  Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York, NY, USA, 2005. ISBN 0521835402. Learning and Testing Junta Distributions  Elchanan Mossel, Ryan O\u2019Donnell, and Rocco A Servedio. Learning functions of k relevant variables. Journal of Computer and System Sciences, 69(3):421-434, November 2004.  Ryan O\u2019Donnell. Analysis of Boolean functions. Cambridge University Press, October 2014.  Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE Trans. Inf. Theor., 54(10):4750-4755, October 2008. ISSN 0018-9448. doi: 10.1109/TIT.2008.928987. URL http://dx.doi.org/10.1109/TIT.2008.928987.  Gregory Valiant. Finding correlations in subquadratic time, with applications to learning  parities and juntas. FOCS, pages 11-20, 2012. Aliakbarpour Blais Rubinfeld  "}, "Sign rank versus VC dimension": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Sign rank versus VC dimension", "abstract": "This work studies the maximum possible sign rank of N \\times N sign matrices with a given VC dimension d. For d=1, this maximum is three. For d=2, this maximum is \\tilde\u0398(N^1/2). For d >2, similar but slightly less accurate statements hold. Our lower bounds improve over previous ones by Ben-David et al.\u00a0and can be interpreted as exhibiting a weakness of kernel-based classifiers. Our upper bounds, on the other hand, can be interpreted as exhibiting the universality of kernel-based classifiers. The lower bounds are obtained by probabilistic constructions, using a theorem of Warren in real algebraic topology. The upper bounds are obtained using a result of Welzl about spanning trees with low stabbing number, and using the moment curve. The upper bound technique is also used to: (i) provide estimates on the number of classes of a given VC dimension, and the number of maximum classes of a given VC dimension \u2013 answering a question of Frankl from \u201989, and (ii) design an efficient algorithm that provides an O(N/\\log(N)) multiplicative approximation for the sign rank (computing the sign rank is equivalent to the existential theory of the reals). We also observe a general connection between sign rank and spectral gaps which is based on Forster\u2019s argument. Consider the N \\times N adjacency matrix of a \u2206regular graph with a second eigenvalue of absolute value \u03bband \u2206\u2264N/2. We show that the sign rank of the signed version of this matrix is at least \u2206/\u03bb. We use this connection to prove the existence of a maximum class C\u2286{\\pm 1}^N with VC dimension 2 and sign rank \\tilde\u0398(N^1/2). This answers a question of Ben-David et al.\u00a0regarding the sign rank of large VC classes. We also describe limitations of this approach, in the spirit of the Alon-Boppana theorem. We further describe connections to communication complexity, geometry, learning theory, and combinatorics.", "pdf_url": "http://proceedings.mlr.press/v49/alon16.pdf", "keywords": ["VC dimension", "dimension complexity", "sign rank", "kernel machines", "maximum classes", "spectral gap", "communication complexity"], "reference": "Noga Alon. Eigenvalues and expanders. Combinatorica, 6(2):83-96, 1986a. doi: 10.1007/  BF02579166. URL http://dx.doi.org/10.1007/BF02579166.  Noga Alon. Eigenvalues, geometric expanders, sorting in rounds, and ramsey theory. Combina- torica, 6(3):207-219, 1986b. doi: 10.1007/BF02579382. URL http://dx.doi.org/10. 1007/BF02579382.  11   SIGN RANK VERSUS VC DIMENSION  Theorem 12 For every fixed d \u2265 1, the number of U (d + 1)-free graphs on N vertices is at most 2O(N 2\u22121/d log N ).  The proof of the theorem is given in Section C.1.4.  3. Concluding remarks and open problems  We have given explicit examples of N \u00d7 N sign matrices with small VC dimension and large sign rank. However, we have not been able to prove that any of them has sign rank exceeding N 1/2. Indeed this seems to be the limit of Forster\u2019s approach, even if we do not bound the VC dimension. Forster\u2019s theorem shows that the sign rank of any N \u00d7 N Hadamard matrix is at least N 1/2. It is easy to see that there are Hadamard matrices of sign rank significantly smaller than linear in N . Indeed, the sign rank of the 4 \u00d7 4 signed identity matrix is 3, and hence the sign rank of its k\u2019th tensor power, which is an N \u00d7 N Hadamard matrix with N = 4k, is at most 3k = N log 3/ log 4 (a similar argument was given by Forster and Simon (2006) for the Sylvester-Hadamard matrix). It may well be, however, that some Hadamard matrices have sign rank linear in N , as do random sign matrices, and it will be very interesting to show that this is the case for some such matrices. It will also be interesting to decide what is the correct behavior of the sign rank of the incidence graph of the points and lines of a projective plane with N points. We have seen that it is at least \u2126(N 1/4) and at most O(N 1/2).  Using our spectral technique we can give many additional explicit examples of matrices with high sign rank, including ones for which the matrices not only have VC dimension 2, but are more restricted than that (for example, no 3 columns have more than 6 distinct projections).  We have shown that the maximum sign rank f (N, d) of an N \u00d7 N matrix with VC dimension d > 1 is at most O(N 1\u22121/d), and that this is tight up to a logarithmic factor for d = 2, and close to being tight for large d. It seems plausible to conjecture that f (N, d) = \u02dc\u0398(N 1\u22121/d) for all d > 1.  We have also showed how to use this upper bound to get a nontrivial approximation algorithm for the sign rank. It will be interesting to fully understand the computational complexity of comput- ing the sign rank.  Finally we note that most of the analysis in this paper can be extended to deal with M \u00d7 N matrices, where M and N are not necessarily equal, and we restricted the attention here for square matrices mainly in order to simplify the presentation.  We wish to thank Rom Pinchasi, Amir Shpilka, and Avi Wigderson for helpful discussions and comments.  Acknowledgments  References  Noga Alon. Eigenvalues and expanders. Combinatorica, 6(2):83-96, 1986a. doi: 10.1007/  BF02579166. URL http://dx.doi.org/10.1007/BF02579166.  Noga Alon. Eigenvalues, geometric expanders, sorting in rounds, and ramsey theory. Combina- torica, 6(3):207-219, 1986b. doi: 10.1007/BF02579382. URL http://dx.doi.org/10. 1007/BF02579382. ALON MORAN YEHUDAYOFF  Noga Alon. A parallel algorithmic version of the local lemma. Random Struct. Algorithms, 2 (4):367-378, 1991. doi: 10.1002/rsa.3240020403. URL http://dx.doi.org/10.1002/ rsa.3240020403.  Noga Alon and V. D. Milman. Eigenvalues, expanders and superconcentrators (extended abstract). In 25th Annual Symposium on Foundations of Computer Science, West Palm Beach, Florida, USA, 24-26 October 1984, pages 320-322, 1984. doi: 10.1109/SFCS.1984.715931. URL http: //dx.doi.org/10.1109/SFCS.1984.715931.  Noga Alon and V. D. Milman. lambda1, isoperimetric inequalities for graphs, and superconcentra- tors. J. Comb. Theory, Ser. B, 38(1):73-88, 1985. doi: 10.1016/0095-8956(85)90092-9. URL http://dx.doi.org/10.1016/0095-8956(85)90092-9.  Noga Alon, Peter Frankl, and Vojtech R\u00a8odl. Geometrical realization of set systems and probabilistic communication complexity. In 26th Annual Symposium on Foundations of Computer Science, Portland, Oregon, USA, 21-23 October 1985, pages 277-280, 1985. doi: 10.1109/SFCS.1985.30. URL http://dx.doi.org/10.1109/SFCS.1985.30.  Noga Alon, David Haussler, and Emo Welzl. Partitioning and geometric embedding of range spaces In Proceedings of the Third Annual Symposium on of finite vapnik-chervonenkis dimension. Computational Geometry, Waterloo, Ontario, Canada, June 8-10, 1987, pages 331-340, 1987. doi: 10.1145/41958.41994. URL http://doi.acm.org/10.1145/41958.41994.  Noga Alon, Lajos R\u00b4onyai, and Tibor Szab\u00b4o. Norm-graphs: Variations and applications. J. Comb. Theory, Ser. B, 76(2):280-290, 1999. doi: 10.1006/jctb.1999.1906. URL http://dx.doi. org/10.1006/jctb.1999.1906.  Noga Alon, J\u00b4ozsef Balogh, B\u00b4ela Bollob\u00b4as, and Robert Morris. The structure of almost all graphs in a hereditary property. J. Comb. Theory, Ser. B, 101(2):85-110, 2011. doi: 10.1016/j.jctb.2010. 10.001. URL http://dx.doi.org/10.1016/j.jctb.2010.10.001.  Richard P. Anstee, Lajos R\u00b4onyai, and Attila Sali. Shattering news. Graphs and Combinatorics, 18(1):59-73, 2002. doi: 10.1007/s003730200003. URL http://dx.doi.org/10.1007/ s003730200003.  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161-182, 2006. doi: 10.1007/s10994-006-6265-7. URL http://dx.doi.org/10.1007/s10994-006-6265-7.  Hans-J\u00a8urgen Bandelt, Victor Chepoi, Andreas W. M. Dress, and Jack H. Koolen. Combinatorics of lopsided sets. Eur. J. Comb., 27(5):669-689, 2006. doi: 10.1016/j.ejc.2005.03.001. URL http://dx.doi.org/10.1016/j.ejc.2005.03.001.  Ronen Basri, Pedro F. Felzenszwalb, Ross B. Girshick, David W. Jacobs, and Caroline J. Klivans. Visibility constraints on features of 3d objects. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 1231-1238, 2009. doi: 10.1109/CVPRW.2009.5206726. URL http://dx.doi.org/ 10.1109/CVPRW.2009.5206726. SIGN RANK VERSUS VC DIMENSION  Shai Ben-David and Michael Lindenbaum. Localization vs. identification of semi-algebraic sets. Machine Learning, 32(3):207-224, 1998. doi: 10.1023/A:1007447530834. URL http://dx. doi.org/10.1023/A:1007447530834.  Shai Ben-David, Nadav Eiron, and Hans-Ulrich Simon. Limitations of learning via embeddings in euclidean half spaces. Journal of Machine Learning Research, 3:441-461, 2002. URL http: //www.jmlr.org/papers/v3/bendavid02a.html.  Albrecht Beutelspacher and Ute Rosenbaum. Projective geometry - from foundations to applica-  tions. Cambridge University Press, 1998. ISBN 978-0-521-48277-6.  Amey Bhangale and Swastik Kopparty. The complexity of computing the minimum rank of a sign pattern matrix. CoRR, abs/1503.04486, 2015. URL http://arxiv.org/abs/1503. 04486.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Classifying learnable geometric concepts with the vapnik-chervonenkis dimension (extended abstract). In Ju- ris Hartmanis, editor, Proceedings of the 18th Annual ACM Symposium on Theory of Computing, May 28-30, 1986, Berkeley, California, USA, pages 273-282. ACM, 1986. ISBN 0-89791-193-8. doi: 10.1145/12130.12158. URL http://doi.acm.org/10.1145/12130.12158.  B\u00b4ela Bollob\u00b4as and A. J. Radcliffe. Defect sauer results. J. Comb. Theory, Ser. A, 72(2):189-208,  1995.  Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal In David Haussler, editor, Proceedings of the Fifth Annual ACM Confer- margin classifiers. ence on Computational Learning Theory, COLT 1992, Pittsburgh, PA, USA, July 27-29, 1992., pages 144-152. ACM, 1992. ISBN 0-89791-497-X. doi: 10.1145/130385.130401. URL http://doi.acm.org/10.1145/130385.130401.  William G. Brown. On graphs that do not contain a Thomsen graph. Canadian Mathematical  Bulletin, 9:281-85, 1966.  Christopher J. C. Burges. A tutorial on support vector machines for pattern recognition. Data Min. Knowl. Discov., 2(2):121-167, 1998. doi: 10.1023/A:1009715923555. URL http:// dx.doi.org/10.1023/A:1009715923555.  Bernard Chazelle and Emo Welzl. Quasi-optimal range searching in space of finite vc-dimension. Discrete &amp; Computational Geometry, 4:467-489, 1989. doi: 10.1007/BF02187743. URL http://dx.doi.org/10.1007/BF02187743.  Fan R. K. Chung, Ronald L. Graham, Peter Frankl, and James B. Shearer. Some intersection theorems for ordered sets and graphs. J. Comb. Theory, Ser. A, 43(1):23-37, 1986. doi: 10. 1016/0097-3165(86)90019-1. URL http://dx.doi.org/10.1016/0097-3165(86) 90019-1.  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273-297, 1995. doi: 10.1007/BF00994018. URL http://dx.doi.org/10.1007/BF00994018. ALON MORAN YEHUDAYOFF  J. Dodziuk. Difference equations, isoperimetric inequality and transience of certain random walks.  Trans. Am. Math. Soc., 284:787-794, 1984.  Thorsten Doliwa, Hans-Ulrich Simon, and Sandra Zilles. Recursive teaching dimension, learn- ing complexity, and maximum classes. In Marcus Hutter, Frank Stephan, Vladimir Vovk, and Thomas Zeugmann, editors, Algorithmic Learning Theory, 21st International Confer- ence, ALT 2010, Canberra, Australia, October 6-8, 2010. Proceedings, volume 6331 of Lecture Notes in Computer Science, pages 209-223. Springer, 2010. ISBN 978-3-642- 16107-0. doi: 10.1007/978-3-642-16108-7 19. URL http://dx.doi.org/10.1007/ 978-3-642-16108-7_19.  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching dimen- sion, vc-dimension and sample compression. Journal of Machine Learning Research, 15(1): 3107-3131, 2014. URL http://dl.acm.org/citation.cfm?id=2697064.  Sally Floyd and Manfred K. Warmuth.  learnability, and the vapnik- chervonenkis dimension. Machine Learning, 21(3):269-304, 1995. doi: 10.1007/BF00993593. URL http://dx.doi.org/10.1007/BF00993593.  Sample compression,  J\u00a8urgen Forster. A linear lower bound on the unbounded error probabilistic communication com- plexity. J. Comput. Syst. Sci., 65(4):612-625, 2002. doi: 10.1016/S0022-0000(02)00019-3. URL http://dx.doi.org/10.1016/S0022-0000(02)00019-3.  J\u00a8urgen Forster and Hans-Ulrich Simon. On the smallest possible dimension and the largest possible margin of linear arrangements representing given concept classes. Theor. Comput. Sci., 350(1): 40-48, 2006. doi: 10.1016/j.tcs.2005.10.015. URL http://dx.doi.org/10.1016/j. tcs.2005.10.015.  J\u00a8urgen Forster, Matthias Krause, Satyanarayana V. Lokam, Rustam Mubarakzjanov, Niels Schmitt, and Hans-Ulrich Simon. Relations between communication complexity, linear arrangements, and computational complexity. In Ramesh Hariharan, Madhavan Mukund, and V. Vinay, editors, FST TCS 2001: Foundations of Software Technology and Theoretical Computer Science, 21st Conference, Bangalore, India, December 13-15, 2001, Proceedings, volume 2245 of Lecture Notes in Computer Science, pages 171-182. Springer, 2001. ISBN 3-540-43002-4. doi: 10.1007/ 3-540-45294-X 15. URL http://dx.doi.org/10.1007/3-540-45294-X_15.  J\u00a8urgen Forster, Niels Schmitt, Hans-Ulrich Simon, and Thorsten Suttorp. Estimating the optimal margins of embeddings in euclidean half spaces. Machine Learning, 51(3):263-281, 2003. doi: 10.1023/A:1022905618164. URL http://dx.doi.org/10.1023/A:1022905618164.  Peter Frankl. Traces of antichains. Graphs and Combinatorics, 5(1):295-299, 1989. doi: 10.1007/  BF01788682. URL http://dx.doi.org/10.1007/BF01788682.  Bernd G\u00a8artner and Emo Welzl. Vapnik-chervonenkis dimension and (pseudo-)hyperplane arrange- ments. Discrete &amp; Computational Geometry, 12:399-432, 1994. doi: 10.1007/BF02574389. URL http://dx.doi.org/10.1007/BF02574389.  David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-  chervonenkis dimension. J. Comb. Theory, Ser. A, 69(2):217-232, 1995. SIGN RANK VERSUS VC DIMENSION  David Haussler and Emo Welzl. epsilon-nets and simplex range queries. Discrete &amp; Computa- tional Geometry, 2:127-151, 1987. doi: 10.1007/BF02187876. URL http://dx.doi.org/ 10.1007/BF02187876.  Shlomo Hoory, Nathan Linial, and Avi Wigderson. Expander graphs and their applications. doi: 10.1090/  ISSN 0273-0979.  Bull. Amer. Math. Soc., 43(04):439-562, August 2006. s0273-0979-06-01126-8.  W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mapping into Hilbert space. In Conf. in modern analysis and probability, volume 26 of Contemporary Mathematics, pages 189-206. American Mathematical Society, 1984.  P. Keevash. The existence of designs. CoRR, abs/1401.366, 2014.  J\u00b4anos Koml\u00b4os, J\u00b4anos Pach, and Gerhard J. Woeginger. Almost tight bounds for epsilon-nets. Dis- crete &amp; Computational Geometry, 7:163-173, 1992. doi: 10.1007/BF02187833. URL http://dx.doi.org/10.1007/BF02187833.  Ilan Kremer, Noam Nisan, and Dana Ron. On randomized one-round communication complexity. Computational Complexity, 8(1):21-49, 1999. doi: 10.1007/s000370050018. URL http:// dx.doi.org/10.1007/s000370050018.  Eyal Kushilevitz and Noam Nisan. Communication complexity. Cambridge University Press, 1997.  ISBN 978-0-521-56067-2.  Dima Kuzmin and Manfred K. Warmuth. Unlabeled compression schemes for maximum classes. Journal of Machine Learning Research, 8:2047-2081, 2007. URL http://dl.acm.org/ citation.cfm?id=1314566.  Troy Lee and Adi Shraibman. An approximation algorithm for approximation rank. In Proceed- ings of the 24th Annual IEEE Conference on Computational Complexity, CCC 2009, Paris, France, 15-18 July 2009, pages 351-357. IEEE Computer Society, 2009. ISBN 978-0-7695- 3717-7. doi: 10.1109/CCC.2009.25. URL http://doi.ieeecomputersociety.org/ 10.1109/CCC.2009.25.  Nathan Linial and Adi Shraibman.  Combinatorics, Probability &amp; Computing, 18(1-2):227-245, 2009. S0963548308009656. URL http://dx.doi.org/10.1017/S0963548308009656.  Learning complexity vs communication complexity. 10.1017/  doi:  Satyanarayana V. Lokam. Complexity lower bounds using linear algebra. Foundations and Trends in Theoretical Computer Science, 4(1-2):1-155, 2009. doi: 10.1561/0400000011. URL http: //dx.doi.org/10.1561/0400000011.  Jir\u00b4\u0131 Matousek. Intersection graphs of segments and $\\exists\\mathbb{R}$. CoRR, abs/1406.2636,  2014. URL http://arxiv.org/abs/1406.2636.  Jir\u00b4\u0131 Matousek, Emo Welzl, and Lorenz Wernisch. Discrepancy and approximations for bounded vc-dimension. Combinatorica, 13(4):455-466, 1993. doi: 10.1007/BF01303517. URL http: //dx.doi.org/10.1007/BF01303517. ALON MORAN YEHUDAYOFF  N. E. Mnev. The universality theorems on the classification problem of configuration varieties and  convex polytopes varieties. Topology and Geometry, 1346:527-544, 1989.  Shay Moran. Shattering-extremal systems. CoRR, abs/1211.2980, 2012. URL http://arxiv.  org/abs/1211.2980.  Shay Moran and Manfred K. Warmuth. Labeled compression schemes for extremal classes. CoRR,  abs/1506.00165, 2015. URL http://arxiv.org/abs/1506.00165.  A. Nilli. On the second eigenvalue of a graph. Discrete Mathematics, 91(2):207 - 210, 1991. ISSN 0012-365X. doi: http://dx.doi.org/10.1016/0012-365X(91)90112-F. URL http://www. sciencedirect.com/science/article/pii/0012365X9190112F.  Ramamohan Paturi and Janos Simon. Probabilistic communication complexity. J. Comput. Syst. Sci., 33(1):106-123, 1986. doi: 10.1016/0022-0000(86)90046-2. URL http://dx.doi. org/10.1016/0022-0000(86)90046-2.  Alexander A. Razborov and Alexander A. Sherstov. The sign-rank of ac0. SIAM J. Comput., 39 (5):1833-1855, 2010. doi: 10.1137/080744037. URL http://dx.doi.org/10.1137/ 080744037.  J. Richter-Gebert. Mn\u00a8ev\u2019s universality theorem revisited. S\u00b4emin. Lothar. Comb. (electronic), B34h,  1995.  Frank Rosenblatt. The perceptron-a perceiving and recognizing automaton. Technical Report 85-  460-1, Cornell Aeronautical Laboratory, 1957.  Benjamin I. P. Rubinstein and J. Hyam Rubinstein. A geometric approach to sample compression. Journal of Machine Learning Research, 13:1221-1261, 2012. URL http://dl.acm.org/ citation.cfm?id=2343686.  J. Hyam Rubinstein, Benjamin I. P. Rubinstein, and Peter L. Bartlett. Bounding embeddings of VC classes into maximum classes. CoRR, abs/1401.7388, 2014. URL http://arxiv.org/ abs/1401.7388.  Norbert Sauer. On the density of families of sets.  J. Comb. Theory, Ser. A, 13(1):145- 147, 1972. doi: 10.1016/0097-3165(72)90019-2. URL http://dx.doi.org/10.1016/ 0097-3165(72)90019-2.  Bernhard Sch\u00a8olkopf, Alexander J. Smola, and Klaus-Robert M\u00a8uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299-1319, 1998. doi: 10.1162/ 089976698300017467. URL http://dx.doi.org/10.1162/089976698300017467.  Alexander A. Sherstov.  2008. doi: s00037-008-0242-4.  Halfspace matrices. 10.1007/s00037-008-0242-4.  Computational Complexity, 17(2):149-178, URL http://dx.doi.org/10.1007/  Alexander A. Sherstov. Communication complexity under product and nonproduct distributions. Computational Complexity, 19(1):135-150, 2010. doi: 10.1007/s00037-009-0285-1. URL http://dx.doi.org/10.1007/s00037-009-0285-1. SIGN RANK VERSUS VC DIMENSION  Peter W. Shor. Stretchabilltv of pseudolines is np-hard.  In Peter Gritzmann and Bernd Sturm- fels, editors, Applied Geometry And Discrete Mathematics, Proceedings of a DIMACS Workshop, Providence, Rhode Island, USA, September 18, 1990, volume 4 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 531-554. DIMACS/AMS, 1990. URL http://dimacs.rutgers.edu/Volumes/Vol04.html.  V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264-280, 1971. doi: 10.1137/1116025. URL http://link.aip.org/link/?TPR/16/264/1.  Vladimir Vapnik. Statistical learning theory. Wiley, 1998. ISBN 978-0-471-03003-4.  H. E. Warren. Lower bounds for approximation by nonlinear manifolds. Trans. Amer. Math. Soc.,  133:167-178, 1968.  Emo Welzl. Partition trees for triangle counting and other range searching problems. In Herbert Edelsbrunner, editor, Proceedings of the Fourth Annual Symposium on Computational Geometry, Urbana-Champaign, IL, USA, June 6-8, 1988, pages 23-33. ACM, 1988. ISBN 0-89791-270-5. doi: 10.1145/73393.73397. URL http://doi.acm.org/10.1145/73393.73397.  "}, "Efficient approaches for escaping higher order saddle points  in non-convex optimization": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Efficient approaches for escaping higher order saddle points  in non-convex optimization", "abstract": "Local search heuristics for non-convex optimizations are popular   in applied machine learning. However, in general it is  hard to  guarantee that such algorithms even  converge to a \\em local minimum, due to the existence of complicated saddle point structures in high dimensions. Many functions have \\em degenerate saddle points such that the first and second order derivatives cannot distinguish them with local optima.  In this paper we use higher order derivatives to escape these saddle points: we design the first efficient algorithm  guaranteed to converge to a third order local optimum (while existing techniques are at most second order). We also show that it is NP-hard to extend this further to finding fourth order local optima.", "pdf_url": "http://proceedings.mlr.press/v49/anandkumar16.pdf", "keywords": ["Non-convex optimization", "degenerate saddle points", "higher order conditions for local optimality", "trust region methods"], "reference": "Shun-Ichi Amari, Hyeyoung Park, and Tomoko Ozeki. Singularities a\ufb00ect dynamics of  learning in neuromanifolds. Neural computation, 18(5):1007-1065, 2006.  Peter Auer, Mark Herbster, Manfred K Warmuth, et al. Exponentially many local minima for single neurons. Advances in neural information processing systems, pages 316-322, 1996.  Antonio Au\ufb03nger, Gerard Ben Arous, et al. Complexity of random smooth functions on  the high-dimensional sphere. The Annals of Probability, 41(6):4214-4247, 2013.  Michel Baes. Estimate sequence methods: extensions and approximations. Institute for  Operations Research, ETH, Z\u00a8urich, Switzerland, 2009.  Dennis S Bernstein. A systematic approach to higher-order necessary conditions in opti-  mization theory. SIAM journal on control and optimization, 22(2):211-238, 1984.  Anthony Carbery and James Wright. Distributional and l\u02c6 q norm inequalities for polyno- mials over convex bodies in r\u02c6 n. Mathematical Research Letters, 8(3):233-248, 2001.  Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear  algebra and its applications, 438(2):942-952, 2013.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pages 2933-2941, 2014.  Peter JC Dickinson and Luuk Gijben. On the computational complexity of membership problems for the completely positive cone and its dual. Computational optimization and applications, 57(2):403-415, 2014.  Alan Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In focs,  page 359. IEEE, 1996.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  Rich Caruana Steve Lawrence Lee Giles. Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference, volume 13, page 402, 2001.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the  ACM (JACM), 60(6):45, 2013.  Masato Inoue, Hyeyoung Park, and Masato Okada. On-line learning theory of soft commit- tee machines with correlated hidden units-steepest gradient descent and natural gradient descent-. Journal of the Physical Society of Japan, 72(4):805-810, 2003.  16   Anandkumar Ge  References  Shun-Ichi Amari, Hyeyoung Park, and Tomoko Ozeki. Singularities a\ufb00ect dynamics of  learning in neuromanifolds. Neural computation, 18(5):1007-1065, 2006.  Peter Auer, Mark Herbster, Manfred K Warmuth, et al. Exponentially many local minima for single neurons. Advances in neural information processing systems, pages 316-322, 1996.  Antonio Au\ufb03nger, Gerard Ben Arous, et al. Complexity of random smooth functions on  the high-dimensional sphere. The Annals of Probability, 41(6):4214-4247, 2013.  Michel Baes. Estimate sequence methods: extensions and approximations. Institute for  Operations Research, ETH, Z\u00a8urich, Switzerland, 2009.  Dennis S Bernstein. A systematic approach to higher-order necessary conditions in opti-  mization theory. SIAM journal on control and optimization, 22(2):211-238, 1984.  Anthony Carbery and James Wright. Distributional and l\u02c6 q norm inequalities for polyno- mials over convex bodies in r\u02c6 n. Mathematical Research Letters, 8(3):233-248, 2001.  Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear  algebra and its applications, 438(2):942-952, 2013.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pages 2933-2941, 2014.  Peter JC Dickinson and Luuk Gijben. On the computational complexity of membership problems for the completely positive cone and its dual. Computational optimization and applications, 57(2):403-415, 2014.  Alan Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In focs,  page 359. IEEE, 1996.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797-842, 2015.  Rich Caruana Steve Lawrence Lee Giles. Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference, volume 13, page 402, 2001.  Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the  ACM (JACM), 60(6):45, 2013.  Masato Inoue, Hyeyoung Park, and Masato Okada. On-line learning theory of soft commit- tee machines with correlated hidden units-steepest gradient descent and natural gradient descent-. Journal of the Physical Society of Japan, 72(4):805-810, 2003. Efficient approaches for escaping higher order saddle points in non-convex optimization  Katta G Murty and Santosh N Kabadi. Some np-complete problems in quadratic and  nonlinear programming. Mathematical programming, 39(2):117-129, 1987.  Yurii Nesterov. Squared functional systems and optimization problems. In High performance  optimization, pages 405-440. Springer, 2000.  Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global  performance. Mathematical Programming, 108(1):177-205, 2006.  Jiawang Nie. The hierarchy of local minimums in polynomial optimization. Mathematical  Programming, 151(2):555-583, 2015.  David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review  E, 52(4):4225, 1995.  Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural  networks. arXiv preprint arXiv:1511.04210, 2015.  Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary?  arXiv  preprint arXiv:1510.06096, 2015.  Santosh S Vempala and Ying Xiao. Structure from local optima: Learning subspace juntas  via higher order pca. arXiv preprint arXiv:1108.3329, 2011.  Jack Warga. Higher order conditions with and without lagrange multipliers. SIAM journal  on control and optimization, 24(4):715-730, 1986.  Haikun Wei, Jun Zhang, Florent Cousseau, Tomoko Ozeki, and Shun-ichi Amari. Dynamics of learning near singularities in layered networks. Neural computation, 20(3):813-843, 2008. Anandkumar Ge  "}, "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes", "abstract": "Strongly Rayleigh distributions are natural generalizations of product and determinantal probability distributions and satisfy the strongest form of negative dependence properties. We show that the \"natural\" Monte Carlo Markov Chain (MCMC) algorithm mixes rapidly in the support of a homogeneous strongly Rayleigh distribution. As a byproduct, our proof implies Markov chains can be used to efficiently generate approximate samples of a k-determinantal point process. This answers an open question raised by Deshpande and Rademacher which was studied recently by Kang, Li-Jegelka-Sra, and Rebeschini-Karbasi.", "pdf_url": "http://proceedings.mlr.press/v49/anari16.pdf", "keywords": ["determinantal point processes sampling", "strongly Rayleigh distributions", "markov chains", "MCMC algorithms"], "reference": "FOCS, pages 20-39, 2015.  Nima Anari and Shayan Oveis Gharan. Effective-Resistance-Reducing Flows and Asymmetric TSP.  In  Julius Borcea, Petter Branden, and Thomas M. Liggett. Negative dependence and the geometry of polyno-  mials. Journal of American Mathematical Society, 22:521-567, 2009.  Christos Boutsidis, Michael W Mahoney, and Petros Drineas. An improved approximation algorithm for  the column subset selection problem. In SODA, pages 968-977, 2009.  Petter Br\u00a8and\u00b4en. Polynomials with the half-plane property and matroid theory. Advances in Mathematics,  216(1):302-320, 2007.  Ali C\u00b8 ivril and Malik Magdon-Ismail. On selecting a maximum volume sub-matrix of a matrix and related  problems. Theoretical Computer Science, 410(47):4801-4811, 2009.  Ali C\u00b8 ivril and Malik Magdon-Ismail. Exponential inapproximability of selecting a maximum volume sub-  matrix. Algorithmica, 65(1):159-176, 2013.  Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection.  In  FOCS, pages 329-338. IEEE, 2010.  Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projec-  tive clustering via volume sampling. In SODA, pages 1117-1126, 2006.  Persi Diaconis and Laurent Saloff-Coste. Comparison theorems for reversible markov chains. The Annals  of Applied Probability, pages 696-730, 1993.  Persi Diaconis and Daniel Stroock. Geometric bounds for eigenvalues of markov chains. The Annals of  Applied Probability, pages 36-61, 1991.  Tom\u00b4as Feder and Milena Mihail. Balanced matroids.  In Proceedings of the twenty-fourth annual ACM  symposium on Theory of Computing, pages 26-38, New York, NY, USA, 1992. ACM.  J.B. Hough, M. Krishnapur, Y. Peres, and B. Vir\u00b4ag. Determinantal processes and independence. Probability  Surveys, (3):206-229, 2006.  12   c(X, Y ) = (cid:80)  x\u2208X,y\u2208Y cx,y. We have  c(B, B) \u2265 c(s, \u21261 \\ B1) + c(B0, t) \u00b5(\u21261 \\ B1) \u00b5(\u21261)  =  +  \u00b5(B0) \u00b5(\u21260) \u00b5(B0) \u00b5(\u21260)  = 1 \u2212  \u00b5(B1) \u00b5(\u21261)  +  \u2265 1 \u2212  \u00b5(N (B1)) \u00b5(\u21260)  +  \u00b5(B0) \u00b5(\u21260)  ,  (3.6)  where the inequality follows by Lemma 13. If there are any edge from B1 to \u21260 \\ B0, then c(B, B) = \u221e and we are done. Otherwise, N (B1) \u2286 B0. Therefore, \u00b5(N (B1)) \u2264 \u00b5(B0), and the RHS of the above inequality is at least 1. So, c(B, B) \u2265 1 as desired.  References  FOCS, pages 20-39, 2015.  Nima Anari and Shayan Oveis Gharan. Effective-Resistance-Reducing Flows and Asymmetric TSP.  In  Julius Borcea, Petter Branden, and Thomas M. Liggett. Negative dependence and the geometry of polyno-  mials. Journal of American Mathematical Society, 22:521-567, 2009.  Christos Boutsidis, Michael W Mahoney, and Petros Drineas. An improved approximation algorithm for  the column subset selection problem. In SODA, pages 968-977, 2009.  Petter Br\u00a8and\u00b4en. Polynomials with the half-plane property and matroid theory. Advances in Mathematics,  216(1):302-320, 2007.  Ali C\u00b8 ivril and Malik Magdon-Ismail. On selecting a maximum volume sub-matrix of a matrix and related  problems. Theoretical Computer Science, 410(47):4801-4811, 2009.  Ali C\u00b8 ivril and Malik Magdon-Ismail. Exponential inapproximability of selecting a maximum volume sub-  matrix. Algorithmica, 65(1):159-176, 2013.  Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection.  In  FOCS, pages 329-338. IEEE, 2010.  Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projec-  tive clustering via volume sampling. In SODA, pages 1117-1126, 2006.  Persi Diaconis and Laurent Saloff-Coste. Comparison theorems for reversible markov chains. The Annals  of Applied Probability, pages 696-730, 1993.  Persi Diaconis and Daniel Stroock. Geometric bounds for eigenvalues of markov chains. The Annals of  Applied Probability, pages 36-61, 1991.  Tom\u00b4as Feder and Milena Mihail. Balanced matroids.  In Proceedings of the twenty-fourth annual ACM  symposium on Theory of Computing, pages 26-38, New York, NY, USA, 1992. ACM.  J.B. Hough, M. Krishnapur, Y. Peres, and B. Vir\u00b4ag. Determinantal processes and independence. Probability  Surveys, (3):206-229, 2006. Mark Jerrum and Jung Bae Son. Spectral gap and log-sobolev constant for balanced matroids. In FOCS,  pages 721-729, 2002.  Mark Jerrum, Jung-Bae Son, Prasad Tetali, and Eric Vigoda. Elementary bounds on poincar\u00b4e and log- sobolev constants for decomposable markov chains. Annals of Applied Probability, pages 1741-1765, 2004.  Byungkon Kang. Fast determinantal point process sampling with application to clustering. In NIPS, pages  2319-2327, 2013.  4:157-288, 2009.  R. Kannan and S. Vempala. Spectral algorithms. Foundations and Trends in Theoretical Computer Science,  Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. 2013. URL http:  //arxiv.org/abs/1207.6083.  David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. American  Mathematical Society, 2006.  Chengtao Li, Stefanie Jegelka, and Suvrit Sra. Efficient sampling for k-determinantal point processes. 2015.  URL http://arxiv.org/abs/1509.01618.  Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in Markov chains. Found. Trends  Theor. Comput. Sci., 1(3):237-354, May 2006. ISSN 1551-305X.  Aleksandar Nikolov. Randomized rounding for the largest simplex problem.  In STOC, pages 861-870,  Shayan Oveis Gharan, Amin Saberi, and Mohit Singh. A Randomized Rounding Approach to the Traveling  Salesman Problem. In FOCS, pages 550-559, 2011.  Robin Pemantle and Yuval Peres. Concentration of Lipschitz Functionals of Determinantal and Other Strong  Rayleigh Measures. Combinatorics, Probability and Computing, 23:140-160, 1 2014.  Patrick Rebeschini and Amin Karbasi. Fast mixing for discrete point processes. In COLT, pages 1480-1500,  2015.  2015. "}, "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "abstract": "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is O(K\\sqrtn \\log n) and against stochastic bandits the pseudo-regret is O(\\sum_i (\\log n)/\\Delta_i). We also show that no algorithm with O(\\log n) pseudo-regret against stochastic bandits can achieve \\tildeO(\\sqrtn) expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show \\tildeO(\\sqrtn) expected adversarial regret with O((\\log n)^2) stochastic pseudo-regret.", "pdf_url": "http://proceedings.mlr.press/v49/auer16.pdf", "keywords": [], "reference": "Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002b.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In COLT - The 25th Annual Conference on Learning Theory, pages 42.1-42.23, 2012.  Herbert Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58  (5):527-535, 1952.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1287-1295, 2014.  5   NEARLY OPTIMAL PSEUDO-REGRET FOR STOCHASTIC AND ADVERSARIAL BANDITS  additional tests to detect non-stochastic arms. A different approach is taken in (Seldin and Slivkins, 2014): here the starting point is an algorithm for adversarial bandit problems that is modified by adding an additional exploration parameter to achieve also low pseudo-regret in stochastic bandit problems. While this approach has not yet allowed for the tight O (log n) regret bound in stochastic bandit problems (they achieve a O (cid:0)log3 n(cid:1) bound), the approach is quite \ufb02exible and more generally applicable than the SAO and SAPO algorithms.  We thank the anonymous reviewers for their very valuable comments. The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 231495 (CompLACS) and from the Austrian Science Fund (FWF) under contract P 26219-N15.  Acknowledgments  References  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multi-  armed bandit problem. SIAM J. Comput., 32(1):48-77, 2002b.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In COLT - The 25th Annual Conference on Learning Theory, pages 42.1-42.23, 2012.  Herbert Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58  (5):527-535, 1952.  Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1287-1295, 2014. "}, "Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models", "abstract": "In this paper we study a model-based approach to calculating approximately optimal policies in Markovian Decision Processes. In particular, we derive novel bounds on the loss of using a policy derived from a factored linear model, a class of models which generalize numerous previous models out of those that come with strong computational guarantees. For the first time in the literature, we derive performance bounds for model-based techniques where the model inaccuracy is measured in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount factor and, unlike similar bounds derived for other approaches, they are insensitive to measure mismatch. Similarly to previous works, our proofs are also based on contraction arguments, but with the main differences that we use carefully constructed norms building on Banach lattices, and the contraction property is only assumed for operators acting on \u201ccompressed\u201d spaces, thus weakening previous assumptions, while strengthening previous results.", "pdf_url": "http://proceedings.mlr.press/v49/avilapires16.pdf", "keywords": [], "reference": "Andr\u00e9 Barreto, Doina Precup, and Joelle Pineau. Practical kernel-based reinforcement learning.  arXiv preprint arXiv:1407.5358, 2014a.  Andr\u00e9 M. S. Barreto and Marcelo D Fragoso. Computing the stationary distribution of a finite Markov chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applica- tions, 32(4):1513-1523, 2011.  Andr\u00e9 M. S. Barreto, Joelle Pineau, and Doina Precup. Policy iteration based on stochastic factor-  ization. Journal of Artificial Intelligence Research, 50:763-803, 2014b.  Andr\u00e9 S. M. Barreto, Doina Precup, and Joelle Pineau. Reinforcement learning using kernel-based stochastic factorization. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Wein- berger, editors, Advances in Neural Information Processing Systems 24, pages 720-728. Curran Associates, Inc., 2011.  Dimitri P. Bertsekas. Dynamic programming and optimal control, volume 2. Athena Scientific,  1995.  Dimitri P. Bertsekas. Approximate policy iteration: A survey and some new methods. Journal of  Control Theory and Applications, 9(3):310-335, 2011.  19   POLICY ERROR BOUNDS FOR MBRL WITH FACTORED LINEAR MODELS  of M TQR, where R = R\u02dc\u03c0 is chosen to depend on the policy (for some policy \u03c0, R\u03c0 is a weighted Euclidean projection to the compressed space induced by the aggregation, where the weights depend \u02dcU \u2217 where on the stationary distribution of \u03c0). Formally, the policy is defined by \u02dc\u03c0 = GTQR\u02dc\u03c0 \u02dcU \u2217 = M TQR\u02dc\u03c0 \u02dcU \u2217. Thus, the policy whose error he bounds is different from ours in two respects: As pointed out above, U \u2217 = M TQRu\u2217 (that our \u02c6\u03c0 is greedy with respect to) is not necessarily the fixed point of M TQR. Further, our result is proven for general R. At this time it is not clear whether with a specific choice of R (like R\u02c6\u03c0) the terms involved in the definition of \u03b51 would cancel the additional 1/(1 \u2212 \u03b3) factor. For what it is worth, we note that for the \u201ccounterexample\u201d that Van Roy (2006) presents, when R = R\u02c6\u03c0, \u03b51 scales with 1/(1 \u2212 \u03b3) only (as opposed to scaling with 1/(1 \u2212 \u03b3)2), showing that our bound has the ability to exploit the benefits of a \u201cgood\u201d choice of R. However, it remains to be seen whether this or some other systematic way of choosing R will always cancel the extra 1/(1 \u2212 \u03b3) factor.  To summarize, this paper advances our understanding of model errors on policy error in rein- forcement learning. We do this by improving previous bounds by using a versatile set of norms and introducing a completely new bound which has the potential of better scaling with the discount factor, while at the same time we extend the range of the models by relaxing previous assumptions. We also showed that (some) of our bounds are unimprovable. By effectively using the language of Banach lattices, our proofs are shorter, while at the same time hold the promise of being generaliz- able beyond MDPs. We believe that our approach may lead to advances in the analysis and design of alternate approaches to reinforcement learning, namely both in approximate linear programming and approximate dynamic programming.  This work was supported by Alberta Innovates Technology Futures and NSERC.  Acknowledgements  References  Andr\u00e9 Barreto, Doina Precup, and Joelle Pineau. Practical kernel-based reinforcement learning.  arXiv preprint arXiv:1407.5358, 2014a.  Andr\u00e9 M. S. Barreto and Marcelo D Fragoso. Computing the stationary distribution of a finite Markov chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applica- tions, 32(4):1513-1523, 2011.  Andr\u00e9 M. S. Barreto, Joelle Pineau, and Doina Precup. Policy iteration based on stochastic factor-  ization. Journal of Artificial Intelligence Research, 50:763-803, 2014b.  Andr\u00e9 S. M. Barreto, Doina Precup, and Joelle Pineau. Reinforcement learning using kernel-based stochastic factorization. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Wein- berger, editors, Advances in Neural Information Processing Systems 24, pages 720-728. Curran Associates, Inc., 2011.  Dimitri P. Bertsekas. Dynamic programming and optimal control, volume 2. Athena Scientific,  1995.  Dimitri P. Bertsekas. Approximate policy iteration: A survey and some new methods. Journal of  Control Theory and Applications, 9(3):310-335, 2011. \u00c1VILA PIRES SZEPESV\u00c1RI  Dimitri P. Bertsekas. Weighted sup-norm contractions in dynamic programming: A review and some new applications. Dept. Elect. Eng. Comput. Sci., Massachusetts Inst. Technol., Cambridge, MA, USA, Tech. Rep. LIDS-P-2884, 2012.  Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement Learning and Dynamic Programming Using Function Approximators. CRC Press, Inc., Boca Raton, FL, USA, 1st edition, 2010. ISBN 1439821089, 9781439821084.  Daniela Pucci de Farias and Benjamin Van Roy. The linear programming approach to approximate  dynamic programming. Operations Research, 51(6):850-865, 2003.  Amir Massoud Farahmand, R\u00e9 mi Munos, and Csaba Szepesv\u00e1 ri. Error propagation for approxi- mate policy and value iteration. In Advances in Neural Information Processing Systems 23, pages 568-576, 2010.  Steffen Gr\u00fcnew\u00e4lder, Luca Baldassarre, Massimiliano Pontil, Arthur Gretton, and Guy Lever. Mod- eling transition dynamics in MDPs with RKHS embeddings of conditional distributions. CoRR, abs/1112.4722, 2011.  Steffen Gr\u00fcnew\u00e4lder, Guy Lever, Luca Baldassarre, Massimilano Pontil, and Arthur Gretton. Mod- elling transition dynamics in MDPs with RKHS embeddings. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning, ICML\u201912, pages 535-542. Omnipress, 2012. ISBN 978-1-4503-1285-1.  Branislav Kveton and Georgios Theocharous. Kernel-based reinforcement learning on represen- In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,  tative states. AAAI\u201912, pages 977-983. AAAI Press, 2012.  Guy Lever, John Shawe-Taylor, Ronnie Stafford, and Csaba Szepesv\u00e1ri. Compressed conditional mean embeddings for model-based reinforcement learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916. AAAI Press, 2016.  Peter Meyer-Nieber. Banach Lattices. Springer, 1991.  R\u00e9mi Munos. Error bounds for approximate policy iteration. In Proceedings of the 20th Interna-  tional Conference on Machine Learning (ICML-03), pages 560-567, 2003.  Dirk Ormoneit and \u00b4Saunak Sen. Kernel-based reinforcement learning. Machine Learning, 49(2-3):  161-178, 2002.  Doina Precup, Joelle Pineau, and Andr\u00e9 S Barreto. On-line reinforcement learning using incremen- tal kernel-based stochastic factorization. In Advances in Neural Information Processing Systems 25, pages 1484-1492, 2012.  Martin L. Puterman. Markov Decision Processes \u2014 Discrete Stochastic Dynamic Programming.  John Wiley & Sons, Inc., 1994.  Balaraman Ravindran. An algebraic approach to abstraction in reinforcement learning. PhD thesis,  University of Massachusetts Amherst, 2004. POLICY ERROR BOUNDS FOR MBRL WITH FACTORED LINEAR MODELS  S Singh, T Jaakkola, and M Jordan. Reinforcement learning with soft state aggregation. In NIPS-7,  pages 361-368, 1995.  Satinder P Singh and Richard C Yee. An upper bound on the loss from approximate optimal-value  functions. Machine Learning, 16(3):227-233, 1994.  Jonathan Sorg and Satinder Singh. Transfer via soft homomorphisms. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, pages 741- 748. International Foundation for Autonomous Agents and Multiagent Systems, 2009.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive Com-  putation and Machine Learning). The MIT Press, 1998. ISBN 0262193981.  Csaba Szepesv\u00e1ri. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.  Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation.  Mathematics of Operations Research, 31(2):234-244, 2006.  Ward Whitt. Approximations of dynamic programs, I. Mathematics of Operations Research, 3(3):  Marco Wiering and Martijn van Otterlo, editors. Reinforcement Learning: State-of-the-Art.  231-243, 1978.  Springer, 2012.  Hengshuai Yao, Csaba Szepesv\u00e1ri, Bernardo Avila Pires, and Xinhua Zhang. Pseudo-MDPs and In IEEE Symposium on Adaptive Dynamic Programming and  factored linear action models. Reinforcement Learning (ADPRL), 2014, pages 1-9. IEEE, 2014. \u00c1VILA PIRES SZEPESV\u00c1RI  "}, "Learning and 1-bit Compressed Sensing under Asymmetric Noise": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Learning and 1-bit Compressed Sensing under Asymmetric Noise", "abstract": "We study the \\emphapproximate recovery problem: Given corrupted 1-bit measurements of the form sign(w^* \u22c5x_i), recover a vector w that is a good approximation to w^* \u2208\\Re^d. This problem has been studied by both the learning theory and signal processing communities. In learning theory, this is known as the problem of \\emphlearning halfspaces with noise, and in signal processing, as \\emph1-bit compressed sensing, in which there is an additional assumption that w^* is t-sparse. The challenge in both cases is to design computationally efficient algorithms that are tolerant to large amounts of noise under realistic noise models. Furthermore, in the case of 1-bit compressed sensing, we require  the number of measurements x_i to scale polynomially in t and only polylogarithmically in d, the ambient dimension. In this work, we introduce algorithms with nearly optimal guarantees for both problems under two realistic noise models, \\emphbounded (Massart) noise and \\emphadversarial (agnostic) noise, when the measurements x_i\u2019s are drawn from any isotropic log-concave distribution. In bounded (Massart) noise, an adversary can flip the measurement of each point x with probability \u03b7(x)\u2264\u03b7< 1/2. For this problem, we present an efficient algorithm that returns w such that \\|w- w^*\\|_2 \u2264\u03b5in time poly(d, \\frac 1 \u03b5) for \\emphany constant \u03b7< 1/2.  This improves significantly over the best known result of Awasthi et al. 2015, in this space that required the noise to be as small as \u03b7\u224810^-6. We then introduce an attribute-efficient variant of this algorithm for 1-bit compressed sensing that achieves the same guarantee with poly(t, \\log(d), \\frac 1 \u03b5) measurements when \\|w^*\\|_0\u2264t. For adversarial (agnostic) noise, where any \u03bdfraction of measurements can be corrupted, we provide an algorithm that returns w such that \\|w-w^*\\|_2 \u2264O(\u03bd) + \u03b5, with  \\tilde\u03a9( \\frac t \u03b5^3  \\polylog(d))  measurements. Our results improve on the best known approximation results in this space and under some regimes improve on the sample complexity of the existing results. Furthermore, this  is the first result of its kind in 1-bit compressed sensing that goes beyond the Gaussian marginal distribution and works for any isotrpic log-concave distribution.", "pdf_url": "http://proceedings.mlr.press/v49/awasthi16.pdf", "keywords": [], "reference": "Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC), pages 449-458. ACM, 2014.  Pranjal Awasthi, Maria Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear In Proceedings of the 28th Annual Conference on Learning  separators under bounded noise. Theory (COLT), 2015a.  Pranjal Awasthi, Maria Florina Balcan, and Philip M. Long. The power of localization for efficiently  learning linear separators with noise. Submitted to Journal of the ACM, 2015b.  Maria Florina Balcan and Phillip M. Long. Active and passive learning of linear separators under log-concave distributions. In Proceedings of the 26th Annual Conference on Learning Theory (COLT), 2013.  Maria Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In Proceed-  ings of the 20th Annual Conference on Learning Theory (COLT), 2007.  Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  18   AWASTHI BALCAN HAGHTALAB ZHANG  7. Conclusion  Our work improves the state of the art results on classification and 1-bit compressed in presence of asymmetric noise in multiple directions. For the general non-sparse case, our work provides the first algorithm for finding a halfspace that is arbitrarily close to w\u2217 in presence of bounded noise for any constant maximum \ufb02ipping probability. Our analysis and algorithm combine the strengths of two algorithms that are individually insufficient: polynomial regression, with runtime that is inverse exponential in the required accuracy, and margin-based localization technique that only achieves a multiplicative approximation to the optimum. We show how using ideas from the localization technique helps us boost the performance of polynomial regression method. That is, by applying polynomial regression, which only guarantees a constant excess error in polynomial time, iteratively on the conditional distributions within the margin of the previous classifiers, we can achieve an arbitrarily small excess error while maintaining computational efficiency. It would be interesting to see if similar ideas can be applied to more general decision boundaries.  Furthermore, we extend the margin-based platform used for approximate recovery in presence of bounded or adversarial noise to an attribute-efficient algorithm. Our work improves on the best known result of Plan and Vershynin (2013a) on 1-bit compressed sensing in presence of adversarial noise, and achieves an improved approximation factor while allowing broader class of distribution. We also improve on the sample complexity of existing results when \u03bd is small. Our hope is that this first application of the margin-based technique to compressed sensing will lead to improved results for wider class of problems in compressed sensing.  Acknowledgments This work was supported in part by grants NSF-CCF 1535967, NSF CCF- 1422910, NSF CCF-145117, a Sloan Fellowship, and an IBM Ph.D. fellowship.  References  Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC), pages 449-458. ACM, 2014.  Pranjal Awasthi, Maria Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear In Proceedings of the 28th Annual Conference on Learning  separators under bounded noise. Theory (COLT), 2015a.  Pranjal Awasthi, Maria Florina Balcan, and Philip M. Long. The power of localization for efficiently  learning linear separators with noise. Submitted to Journal of the ACM, 2015b.  Maria Florina Balcan and Phillip M. Long. Active and passive learning of linear separators under log-concave distributions. In Proceedings of the 26th Annual Conference on Learning Theory (COLT), 2013.  Maria Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In Proceed-  ings of the 20th Annual Conference on Learning Theory (COLT), 2007.  Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006. LEARNING AND 1-BIT COMPRESSED SENSING UNDER ASYMMETRIC NOISE  Avrim Blum. Learning boolean functions in an infinite attribute space. In Proceedings of the 22nd  Annual ACM Symposium on Theory of Computing (STOC), pages 64-72. ACM, 1990.  Avrim Blum. Machine learning: a tour through some favorite results, directions, and open problems. FOCS 2003 tutorial slides, available at http://www.cs.cmu.edu/\u02dcavrim/Talks/ FOCS03/tutorial.ppt, 2003.  Avrim Blum and Pat Langley. Selection of relevant features and examples in machine learning.  Artificial Intelligence, 97(1):245-271, 1997.  Avrim Blum, Alan M. Frieze, Ravi Kannan, and Santosh Vempala. A polynomial time algorithm  for learning noisy linear threshold functions. Algorithmica, 22(1/2):35-52, 1997.  Petros T Boufounos and Richard G Baraniuk. 1-bit compressive sensing. In Proceedings of the 42nd Annual Conference on Information Sciences and Systems (CISS), pages 16-21. IEEE, 2008.  Olivier Bousquet, St\u00b4ephane Boucheron, and Gabor Lugosi. Theory of classification: a survey of  recent advances. ESAIM: Probability and Statistics, 9:9:323-375, 2005.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406-5425, 2006.  Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other  kernel-based learning methods. Cambridge University Press, 2000.  Amit Daniely. Complexity theoretic limitations on learning halfspaces. CoRR, abs/1505.05800,  2015a.  Amit Daniely. A PTAS for agnostically learning halfspaces. In Proceedings of the 28th Annual  Conference on Learning Theory, 2015b.  Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single and multiple teachers. The Journal of Machine Learning Research, 13(1):2655-2697, 2012.  David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-  1306, 2006.  John Dunagan and Santosh Vempala. A simple polynomial-time rescaling algorithm for solving  linear programs. Mathematical Programming, 114(1):101-114, 2008.  Uriel Feige, Yishay Mansour, and Robert Schapire. Learning and inference in the presence of In Proceedings of The 28th Conference on Learning Theory (COLT), pages  corrupted inputs. 637-657, 2015.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997. AWASTHI BALCAN HAGHTALAB ZHANG  Sivakant Gopi, Praneeth Netrapalli, Prateek Jain, and Aditya Nori. One-bit compressed sensing: Provable support and vector recovery. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 154-162, 2013.  Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2006.  Laurent Jacques, Jason N Laska, Petros T Boufounos, and Richard G Baraniuk. Robust 1-bit com- pressive sensing via binary stable embeddings of sparse vectors. IEEE Transactions on Informa- tion Theory, 59(4):2082-2102, 2013.  Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Agnostically In Proceedings of the 46th Annual IEEE Symposium on Foundations of  learning halfspaces. Computer Science (FOCS), 2005.  Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learning. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing (STOC), pages 629-638. ACM, 2008.  Michael Kearns and Ming Li. Learning in the presence of malicious errors. In Proceedings of the  20th Annual ACM Symposium on Theory of Computing (STOC), 1988.  Michael Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efficient agnostic learning. Ma-  chine Learning, 17(2-3), November 1994.  Michael J Kearns and Umesh Virkumar Vazirani. An introduction to computational learning theory.  MIT Press, Cambridge, MA, 1994.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. Ap- proximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (AP- PROX/RANDOM 2014), 28:793-809, 2014.  Adam R Klivans and Rocco A Servedio. Toward attribute efficient learning of decision lists and  parities. The Journal of Machine Learning Research, 7:587-602, 2006.  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-  rithm. Machine learning, 2(4):285-318, 1988.  Philip M Long and Rocco Servedio. Attribute-efficient learning of decision lists and linear threshold In Advances in Neural Information Processing  functions under unconcentrated distributions. Systems (NIPS), pages 921-928, 2006.  L\u00b4aszl\u00b4o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sampling algo-  rithms. Random Structures and Algorithms, 30(3):307-358, 2007.  Marvin L Minsky and Seymour A Papert. Perceptrons - Expanded Edition: An Introduction to  Computational Geometry. MIT press Boston, MA:, 1987.  Elchanan Mossel, Ryan O\u2019Donnell, and Rocco P Servedio. Learning juntas. In Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC), pages 206-212. ACM, 2003. LEARNING AND 1-BIT COMPRESSED SENSING UNDER ASYMMETRIC NOISE  Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, 2013a.  Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming. Communi-  cations on Pure and Applied Mathematics, 66(8):1275-1297, 2013b.  Ronald L Rivest and Robert Sloan. A formal model of hierarchical concept-learning. Information  and Computation, 114(1):88-114, 1994.  Rocco A Servedio, Li-Yang Tan, and Justin Thaler. Attribute-efficient learning and weight-degree  tradeoffs for polynomial threshold functions. In COLT, volume 23, pages 14-1, 2012.  Rocco Anthony Servedio. Efficient algorithms in computational learning theory. Harvard Univer-  sity, 2001.  Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Al-  gorithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with  the zero-one loss. arXiv preprint arXiv:1005.3681, 2010.  Robert H Sloan. PAC learning, noise, and geometry. In Learning and Geometry: Computational  Approaches, pages 21-41. Springer, 1996.  Alexandre B Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statistics,  pages 135-166, 2004.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.  Vladimir Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Lijun Zhang, Jinfeng Yi, and Rong Jin. Efficient algorithms for robust one-bit compressive sensing. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 820- 828, 2014.  Tong Zhang. Covering number bounds of certain regularized linear function classes. The Journal  of Machine Learning Research, 2:527-550, 2002. AWASTHI BALCAN HAGHTALAB ZHANG  "}, "Reinforcement Learning of POMDPs using Spectral Methods": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Reinforcement Learning of POMDPs using Spectral Methods", "abstract": "We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging  since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound w.r.t. the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.", "pdf_url": "http://proceedings.mlr.press/v49/azizzadenesheli16a.pdf", "keywords": ["Spectral Methods", "Method of Moments", "Partially Observable Markov Decision Process", "Latent Variable Model", "Upper Confidence Reinforcement Learning"], "reference": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear  quadratic systems. In COLT, pages 1-26, 2011.  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pages 2312-2320, 2011.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  A. Atrash and J. Pineau. Efficient planning and tracking in pomdps with large observation spaces. In AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems, 2006.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement  learning. In Advances in neural information processing systems, pages 89-96, 2009.  J. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic programming. In S. Thrun, L.K. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems 16, pages 831-838. MIT Press, 2004.  Sevi Baltaoglu, Lang Tong, and Qing Zhao. Online learning and optimization of markov jump affine  models. arXiv preprint arXiv:1605.02213, 2016.  Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement In Proceedings of the 25th Annual Conference on  learning in weakly communicating MDPs. Uncertainty in Artificial Intelligence, 2009.  A.G. Barto, R.S. Sutton, and C.W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, SMC-13(5): 834-846, Sept 1983. ISSN 0018-9472. doi: 10.1109/TSMC.1983.6313077.  Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. J. Artif. Int. Res.,  15(1):319-350, November 2001. ISSN 1076-9757.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954-966, 2011.  Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near- optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003.  23   REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear  quadratic systems. In COLT, pages 1-26, 2011.  Yasin Abbasi-Yadkori, D\u00b4avid P\u00b4al, and Csaba Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pages 2312-2320, 2011.  Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture  models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773-2832, 2014.  A. Atrash and J. Pineau. Efficient planning and tracking in pomdps with large observation spaces. In AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems, 2006.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47(2-3):235-256, 2002.  Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement  learning. In Advances in neural information processing systems, pages 89-96, 2009.  J. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic programming. In S. Thrun, L.K. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems 16, pages 831-838. MIT Press, 2004.  Sevi Baltaoglu, Lang Tong, and Qing Zhao. Online learning and optimization of markov jump affine  models. arXiv preprint arXiv:1605.02213, 2016.  Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement In Proceedings of the 25th Annual Conference on  learning in weakly communicating MDPs. Uncertainty in Artificial Intelligence, 2009.  A.G. Barto, R.S. Sutton, and C.W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, SMC-13(5): 834-846, Sept 1983. ISSN 0018-9472. doi: 10.1109/TSMC.1983.6313077.  Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. J. Artif. Int. Res.,  15(1):319-350, November 2001. ISSN 1076-9757.  D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.  Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954-966, 2011.  Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near- optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003. AZIZZADENESHELI LAZARIC ANANDKUMAR  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework In Sanjoy Dasgupta and David Mcallester, editors, Proceedings of the 30th and applications. International Conference on Machine Learning (ICML-13), volume 28, pages 151-159. JMLR Workshop and Conference Proceedings, 2013.  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1-38, 1977.  M. Gheshlaghi-Azar, A. Lazaric, and E. Brunskill. Regret bounds for reinforcement learning with policy advice. In Proceedings of the European Conference on Machine Learning (ECML\u201913), 2013.  M. Gheshlaghi-Azar, A. Lazaric, and E. Brunskill. Resource-efficient stochastic optimization of a locally smooth function under correlated bandit feedback. In Proceedings of the Thirty-First International Conference on Machine Learning (ICML\u201914), 2014.  Mohammad Gheshlaghi azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra- mani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2220-2228. Curran Associates, Inc., 2013.  Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A pac rl algorithm for episodic pomdps. In Proceedings of the 19th International Conference on Artificial Intelligence and Statis- tics, pages 510-518, 2016.  William Hamilton, Mahdi Milani Fard, and Joelle Pineau. Efficient learning and planning with compressed predictive states. The Journal of Machine Learning Research, 15(1):3395-3439, 2014.  Milos Hauskrecht and Hamish Fraser. Planning treatment of ischemic heart disease with partially observable markov decision processes. Artificial Intelligence in Medicine, 18(3):221 - 244, 2000. ISSN 0933-3657.  Daniel J Hsu, Aryeh Kontorovich, and Csaba Szepesv\u00b4ari. Mixing time estimation in reversible markov chains from a single sample path. In Advances in Neural Information Processing Systems, pages 1459-1467, 2015.  Tommi Jaakkola, Satinder P. Singh, and Michael I. Jordan. Reinforcement learning algorithm for partially observable markov decision problems. In Advances in Neural Information Processing Systems 7, pages 345-352. MIT Press, 1995.  Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement  learning. J. Mach. Learn. Res., 11:1563-1600, August 2010. ISSN 1532-4435.  Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-  chine Learning, 49(2-3):209-232, 2002.  Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In Machine Learning:  ECML 2006, pages 282-293. Springer, 2006. REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS  Aryeh Kontorovich, Boaz Nadler, and Roi Weiss. On learning parametric-output hmms. arXiv  preprint arXiv:1302.6009, 2013.  Aryeh Kontorovich, Roi Weiss, et al. Uniform chernoff and dvoretzky-kiefer-wolfowitz-type in- equalities for markov chains and related processes. Journal of Applied Probability, 51(4):1100- 1113, 2014.  Leonid Aryeh Kontorovich, Kavita Ramanan, et al. Concentration inequalities for dependent ran- dom variables via the martingale method. The Annals of Probability, 36(6):2126-2158, 2008.  Akshay Krishnamurthy, Alekh Agarwal, and John Langford.  Contextual-mdps for pac-  reinforcement learning with rich observations. arXiv preprint arXiv:1602.02722v1, 2016.  Steven M LaValle. Planning algorithms. Cambridge university press, 2006.  Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of pomdps under the expected average reward criterion. European Journal of Operational Research, 211(3):556- 567, 2011.  Michael L. Littman. Memoryless policies: Theoretical limitations and practical results. In Proceed- ings of the Third International Conference on Simulation of Adaptive Behavior : From Animals to Animats 3: From Animals to Animats 3, SAB94, pages 238-245, Cambridge, MA, USA, 1994. MIT Press. ISBN 0-262-53122-4.  Michael L. Littman, Richard S. Sutton, and Satinder Singh. Predictive representations of state. In In Advances In Neural Information Processing Systems 14, pages 1555-1561. MIT Press, 2001.  John Loch and Satinder P Singh. Using eligibility traces to find the best memoryless policy in  partially observable markov decision processes. In ICML, pages 323-331, 1998.  Omid Madani. On the computability of infinite-horizon partially observable markov decision pro-  cesses. In AAAI98 Fall Symposium on Planning with POMDPs, Orlando, FL, 1998.  Lingsheng Meng and Bing Zheng. The optimal perturbation bounds of the moore-penrose inverse  under the frobenius norm. Linear Algebra and its Applications, 432(4):956-963, 2010.  Andrew Y. Ng and Michael Jordan. Pegasus: A policy search method for large mdps and pomdps. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201900, pages 406-415, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860- 709-9.  P Ortner and R Auer. Logarithmic online regret bounds for undiscounted reinforcement learning.  Advances in Neural Information Processing Systems, 19:49, 2007.  Ronald Ortner, Odalric-Ambrym Maillard, and Daniil Ryabko. Selecting near-optimal approximate state representations in reinforcement learning. In Peter Auer, Alexander Clark, Thomas Zeug- mann, and Sandra Zilles, editors, Algorithmic Learning Theory, volume 8776 of Lecture Notes in Computer Science, pages 140-154. Springer International Publishing, 2014. ISBN 978-3-319- 11661-7. AZIZZADENESHELI LAZARIC ANANDKUMAR  Christos Papadimitriou and John N. Tsitsiklis. The complexity of markov decision processes. Math.  Oper. Res., 12(3):441-450, August 1987. ISSN 0364-765X.  Theodore J. Perkins. Reinforcement learning for POMDPs based on action values and stochastic optimization. In Proceedings of the Eighteenth National Conference on Artificial Intelligence and Fourteenth Conference on Innovative Applications of Artificial Intelligence (AAAI/IAAI 2002), pages 199-204. AAAI Press, 2002.  Shaowei Png, J. Pineau, and B. Chaib-draa. Building adaptive dialogue systems via bayes-adaptive pomdps. Selected Topics in Signal Processing, IEEE Journal of, 6(8):917-927, Dec 2012. ISSN 1932-4553. doi: 10.1109/JSTSP.2012.2229962.  P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2008.  Pascal Poupart and Craig Boutilier. Bounded finite state controllers.  In Sebastian Thrun,  Lawrence K. Saul, and Bernhard Sch\u00a8olkopf, editors, NIPS, pages 823-830. MIT Press, 2003.  Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. In Advances in  neural information processing systems, pages 1225-1232, 2007.  Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in partially observable markovian decision processes. In ICML, pages 284-292. Citeseer, 1994.  E. J. Sondik. The optimal control of partially observable Markov processes. PhD thesis, Stanford  University, 1971.  Le Song, Animashree Anandkumar, Bo Dai, and Bo Xie. Nonparametric estimation of multi-view  latent variable models. arXiv preprint arXiv:1311.3287, 2013.  Matthijs T.J. Spaan. Partially observable markov decision processes. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning, volume 12 of Adaptation, Learning, and Optimiza- tion, pages 387-414. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-27644-6.  Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press, 1998.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational  mathematics, 12(4):389-434, 2012.  Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.  John K. Williams and Satinder P. Singh. Experimental results on learning stochastic memoryless policies for partially observable markov decision processes. In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, NIPS, pages 1073-1080. The MIT Press, 1998. REINFORCEMENT LEARNING OF POMDPS USING SPECTRAL METHODS  "}, "Highly-Smooth Zero-th Order Online Optimization": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Highly-Smooth Zero-th Order Online Optimization", "abstract": "The minimization of convex functions which are only available through partial and noisy information is a key methodological problem in many disciplines. In this paper we consider  convex optimization with noisy zero-th order information, that is noisy function evaluations at any desired point. We focus on problems with high degrees of smoothness, such as  logistic regression. We show that as opposed to gradient-based algorithms, high-order smoothness may be used to improve estimation rates, with a precise dependence  of our upper-bounds on the degree of smoothness. In particular, we show that for infinitely differentiable functions, we recover the same dependence on sample size as gradient-based algorithms, with an extra dimension-dependent factor. This is done for both convex and strongly-convex functions, with finite horizon and anytime algorithms. Finally, we also recover similar results in the online optimization setting.", "pdf_url": "http://proceedings.mlr.press/v49/bach16.pdf", "keywords": ["Online learning", "Optimization", "Smoothness"], "reference": "A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-  point bandit feedback. In Proc. COLT, 2010.  A. Agarwal, D. Foster, D. Hsu, S. Kakade, and A. Rakhlin. Stochastic convex optimization with  bandit feedback. SIAM J. Optim., 23:188-212, 2013.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-  414, 2010.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for  machine learning. In Adv. NIPS, 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate o(1/n). In Adv. NIPS, 2013.  S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R (cid:13)  chine Learning, 8(3-4):231-357, 2015.  in Ma-  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  H. Chen. Lower rate of convergence for locating a maximum of a function. The Annals of Statistics,  16(3):1330-1334, 1988.  13   HIGHLY-SMOOTH OPTIMIZATION  7. Conclusion  In this paper, we have considered zero-th order online optimization with a special focus on highly- smooth functions such as for online logistic regression. We considered one-point estimates and two-point estimates of the gradient (with then two independent noises). For infinitely differentiable functions, our main result leads to the same dependence on sample size as gradient-based algo- rithms, with an extra dimension-dependent factor.  The present analysis could be extended in a number of ways: (a) we do not cover the bandit setting. A simple extension of our results allows us to recover existing bounds for \u03b2 = 1 (Shamir, 2013) but we are currently unable to obtain high-smoothness improvements for \u03b2 > 1; (b) while the two-point analysis considers unconstrained problems, the one-point analysis still requires a compact set of constraints and queries slightly outside (in a \u03b4 band around it), which might be avoided by using barrier tools like done by Hazan and Levy (2014). Finally, (c) in the strongly-convex case, the dependence on sample size is optimal in the optimization setting (Polyak and Tsybakov, 1990), however, the optimality of the scaling in dimension, of the plain convex case, and beyond the optimization setting remains open.  Part of this work was performed when Francis Bach was holding the Schlumberger chair at IHES and when Vianney Perchet was a researcher at INRIA. Vianney Perchet also acknowledges fundings from the ANR under grant number ANR-13-JS01-0004 and the CNRS under grant project Parasol.  Acknowledgments  References  A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-  point bandit feedback. In Proc. COLT, 2010.  A. Agarwal, D. Foster, D. Hsu, S. Kakade, and A. Rakhlin. Stochastic convex optimization with  bandit feedback. SIAM J. Optim., 23:188-212, 2013.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384-  414, 2010.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for  machine learning. In Adv. NIPS, 2011.  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence  rate o(1/n). In Adv. NIPS, 2013.  S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R (cid:13)  chine Learning, 8(3-4):231-357, 2015.  in Ma-  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  H. Chen. Lower rate of convergence for locating a maximum of a function. The Annals of Statistics,  16(3):1330-1334, 1988. BACH PERCHET  J. Dippon. Accelerated randomized stochastic optimization. Ann. Statist., 31(4):1260-1281, 08  2003.  J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono. Optimal rates for zero-order convex optimization: the power of two function evaluations. Technical Report 1312.2139, arXiv, 2013.  V. Fabian. Stochastic approximation of minima with improved asymptotic speed. Ann. Math.  Statist., 38(1):191-200, 02 1967.  A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proc. Symposium on Discrete algorithms (SODA). Society for Industrial and Applied Mathematics, 2005.  E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Adv. NIPS, 2014.  E. Hazan, T. Koren, and K. Levy. Logistic regression: Tight bounds for stochastic and online  optimization. In Proc. Conference On Learning Theory (COLT), 2014.  C. Hu, W. Pan, and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online  learning. In Advances in Neural Information Processing Systems, 2009.  S. M. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-  dimensions: Strong convexity and sparsity. Technical Report 0911.0054-v2, ArXiv, 2009.  H. Kushner and G G. Yin. Stochastic approximation and Recursive Algorithms and Applications,  volume 35. Springer, 2003.  133(1-2):365-397, 2012.  G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming,  G. Lan, A. Nemirovski, and A. Shapiro. Validation analysis of mirror descent stochastic approxi-  mation method. Mathematical programming, 134(2):425-458, 2012.  T. Liang, H. Narayanan, and S. Sakhalin. On zeroth-order stochastic convex optimization via ran-  dom walks. Technical Report 1402.2667, arXiv, 2014.  A. S. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization.  Wiley & Sons, 1983.  2004.  Arkadi Nemirovski. Interior point polynomial time methods in convex programming. Lecture Notes,  Y. Nesterov. Introductory Lectures on Convex Optimization, volume 87 of Applied Optimization.  Kluwer Academic Publishers, Boston, MA, 2004.  Y. Nesterov. Random gradient-free minimization of convex functions. Technical report, Universit\u00b4e  catholique de Louvain (CORE), 2011.  B. T. Polyak and A. B. Tsybakov. Optimal order of accuracy of search algorithms in stochastic  optimization. Problemy Peredachi Informatsii, 26(2):45-53, 1990. HIGHLY-SMOOTH OPTIMIZATION  B. Recht, G. G. Jamieson, and R. Nowak. Query complexity of derivative-free optimization. In Adv.  H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,  NIPS, 2012.  22(3):400-407, 1951.  A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proc. International Conference on Artificial Intelligence and Statistics (AIS- TATS), 2011.  M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in neural information processing systems, pages 1458-1466, 2011.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Proc.  Conference on Learning Theory, 2013.  J. C. Spall. Introduction to stochastic search and optimization: estimation, simulation, and control,  volume 65. John Wiley & Sons, 2005.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Jour-  nal of Machine Learning Research, 9:2543-2596, 2010. BACH PERCHET  "}, "An Improved Gap-Dependency Analysis of the Noisy Power Method": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "An Improved Gap-Dependency Analysis of the Noisy Power Method", "abstract": "We consider the \\emphnoisy power method algorithm, which has wide applications in machine learning and statistics, especially those related to principal component analysis (PCA) under resource (communication, memory or privacy) constraints. Existing analysis of the noisy power method shows an unsatisfactory dependency over the \u201cconsecutive\" spectral gap (\\sigma_k-\\sigma_k+1) of an input data matrix, which could be very small and hence limits the algorithm\u2019s applicability. In this paper, we present a new analysis of the noisy power method that achieves improved gap dependency for both sample complexity and noise tolerance bounds. More specifically, we improve the dependency over (\\sigma_k-\\sigma_k+1) to dependency over (\\sigma_k-\\sigma_q+1), where q is an intermediate algorithm parameter and could be much larger than the target rank k. Our proofs are built upon a novel characterization of proximity between two subspaces that differ from canonical angle characterizations analyzed in previous works. Finally, we apply our improved bounds to distributed private PCA and memory-efficient streaming PCA and obtain bounds that are superior to existing results in the literature.", "pdf_url": "http://proceedings.mlr.press/v49/balcan16a.pdf", "keywords": ["principal component analysis", "noisy power method", "spectral gap"], "reference": "Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, and David Woodruff.  Improved  distributed principal component analysis. In NIPS, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental  PCA. In NIPS, pages 3174-3182, 2013.  Christos Boutsidis, David Woodruff, and Peilin Zhong. Optimal principal component analysis in  distributed and streaming models. arXiv: 1504.06729, 2015.  Kamalika Chaudhuri, Anand Sarwate, and Kaushik Sinha. Near-optimal algorithms for differen-  tially private principal components. In NIPS, 2012.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,  ourselves: Privacy via distributed noise generation. In EUROCRYPT, 2006.  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal bounds  for privacy-preserving principal component analysis. In STOC, 2014.  Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychome-  trika, 1(3):211-218, 1936.  Ming Gu. Subspace iteration randomization and singular value problems. SIAM Journal on Scien-  tific Computing, 37(3):A1139-A1173, 2015.  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53 (2):217-288, 2011.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In  NIPS, 2014.  In STOC, 2013.  Moritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector computation.  Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching ma- trix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm. arxiv:1602.06929, 2016.  Chun-Liang Li, Hsuan-Tien Lin, and Chi-Jen Lu. Rivalry of two families of algorithms for memory-  restricted streaming pca. In AISTATS, 2016.  Ziqi Liu, Yu-Xiang Wang, and Alex Smola. Fast differentially private matrix factorization.  In  RecSys, 2015.  14   BALCAN DU WANG YU  We thank Cameron Musco for pointing out an error in the original proof. We also thank an anony- mous reviewer for providing an alternative and elegant proof for Lemma 2.3.  Acknowledgments  References  Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, and David Woodruff.  Improved  distributed principal component analysis. In NIPS, 2014.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental  PCA. In NIPS, pages 3174-3182, 2013.  Christos Boutsidis, David Woodruff, and Peilin Zhong. Optimal principal component analysis in  distributed and streaming models. arXiv: 1504.06729, 2015.  Kamalika Chaudhuri, Anand Sarwate, and Kaushik Sinha. Near-optimal algorithms for differen-  tially private principal components. In NIPS, 2012.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,  ourselves: Privacy via distributed noise generation. In EUROCRYPT, 2006.  Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: optimal bounds  for privacy-preserving principal component analysis. In STOC, 2014.  Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychome-  trika, 1(3):211-218, 1936.  Ming Gu. Subspace iteration randomization and singular value problems. SIAM Journal on Scien-  tific Computing, 37(3):A1139-A1173, 2015.  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53 (2):217-288, 2011.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In  NIPS, 2014.  In STOC, 2013.  Moritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector computation.  Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching ma- trix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm. arxiv:1602.06929, 2016.  Chun-Liang Li, Hsuan-Tien Lin, and Chi-Jen Lu. Rivalry of two families of algorithms for memory-  restricted streaming pca. In AISTATS, 2016.  Ziqi Liu, Yu-Xiang Wang, and Alex Smola. Fast differentially private matrix factorization.  In  RecSys, 2015. IMPROVED ANALYSIS OF THE NOISY POWER METHOD  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming pca. In  Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  Cameron Musco and Christopher Musco. Stronger approximate singular value decomposition via  the block lanczos and power methods. In NIPS, 2015.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gradient  descent for some non-convex matrix problems. In ICML, pages 2332-2341, 2015.  Ohad Shamir. Convergence of stochastic gradient descent for PCA. arxiv:1509.09002, 2015.  Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. Siam, 1997.  Shusen Wang, Zhihua Zhang, and Tong Zhang.  Improved Analyses of the Randomized Power  Method and Block Lanczos Method. ArXiv e-prints: 1508.06429, August 2015.  "}, "Learning Combinatorial Functions from Pairwise Comparisons": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Learning Combinatorial Functions from Pairwise Comparisons", "abstract": "A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support.", "pdf_url": "http://proceedings.mlr.press/v49/balcan16b.pdf", "keywords": ["learning combinatorial functions", "pairwise comparisons", "submodularity", "coverage functions"], "reference": "Ashwinkumar Badanidiyuru, Shahar Dobzinski, Hu Fu, Robert Kleinberg, Noam Nisan, and Tim Roughgarden. Sketching valuation functions. In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201912, pages 1025-1035. SIAM, 2012.  Maria-Florina Balcan and Nicholas JA Harvey. Learning submodular functions. In Pro- ceedings of the Forty-Third Annual ACM Symposium on Theory of Computing (STOC), pages 793-802. ACM, 2011.  Maria-Florina Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions. In The 25th Annual Conference on Learning Theory, pages 4.1-4.24, 2012.  William Barnett. The modern theory of consumer behavior: Ordinal or cardinal? The  Quarterly Journal of Austrian Economics, 6(1):41-65, 2003.  Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the  method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.  Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking In Proceedings of the sixth ACM international  aggregation in a crowdsourced setting. conference on Web search and data mining, pages 193-202. ACM, 2013.  David Easley and Jon Kleinberg. Networks, Crowds, and Markets. Cambridge University  Press, 2010.  Vitaly Feldman. Open problem: The statistical query complexity of learning sparse halfs-  paces. In Conference on Learning Theory, pages 1283-1289, 2014.  Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals. In Proceedings of The 27th Conference on Learning Theory, pages 679-702, 2014.  22   Balcan Vitercik White  \u02dc\u2126(n1/3) lower bound. Another open question is determining whether the sample complex- ity for the additive error results in Section 4 can be improved. We note, both of these questions are unresolved even in the setting where the sample consists of function values and the goal is to learn an approximate function. Another interesting question is to find nontrivial generalizations of the pairwise comparison model and show corresponding results. For instance, the distribution is over k-tuples and the top k(cid:48) sets in the tuple are ranked.  Acknowledgments  We thank Peter Bartlett for insightful initial discussions which led to the development of this research project.  This work was supported in part by NSF grants CCF-1451177, CCF-1422910, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship.  References  Ashwinkumar Badanidiyuru, Shahar Dobzinski, Hu Fu, Robert Kleinberg, Noam Nisan, and Tim Roughgarden. Sketching valuation functions. In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201912, pages 1025-1035. SIAM, 2012.  Maria-Florina Balcan and Nicholas JA Harvey. Learning submodular functions. In Pro- ceedings of the Forty-Third Annual ACM Symposium on Theory of Computing (STOC), pages 793-802. ACM, 2011.  Maria-Florina Balcan, Florin Constantin, Satoru Iwata, and Lei Wang. Learning valuation functions. In The 25th Annual Conference on Learning Theory, pages 4.1-4.24, 2012.  William Barnett. The modern theory of consumer behavior: Ordinal or cardinal? The  Quarterly Journal of Austrian Economics, 6(1):41-65, 2003.  Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the  method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.  Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking In Proceedings of the sixth ACM international  aggregation in a crowdsourced setting. conference on Web search and data mining, pages 193-202. ACM, 2013.  David Easley and Jon Kleinberg. Networks, Crowds, and Markets. Cambridge University  Press, 2010.  Vitaly Feldman. Open problem: The statistical query complexity of learning sparse halfs-  paces. In Conference on Learning Theory, pages 1283-1289, 2014.  Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals. In Proceedings of The 27th Conference on Learning Theory, pages 679-702, 2014. Learning Combinatorial Functions from Pairwise Comparisons  Vitaly Feldman and Jan Vondrak. Optimal bounds on approximation of submodular and xos functions by juntas. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS \u201913, pages 227-236, 2013.  Vitaly Feldman and Jan Vondr\u00b4ak. Tight bounds on low-degree spectral concentration of submodular and XOS functions. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 923- 942, 2015.  Michel X Goemans, Nicholas JA Harvey, Satoru Iwata, and Vahab Mirrokni. Approximating submodular functions everywhere. In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 535-544. Society for Industrial and Applied Mathematics, 2009.  Anupam Gupta, Moritz Hardt, Aaron Roth, and Jonathan Ullman. Privately releasing conjunctions and the statistical query barrier. SIAM Journal on Computing, 42(4):1494- 1520, 2013.  Sandra Heldsinger and Stephen Humphry. Using the method of pairwise comparison to obtain reliable teacher assessments. The Australian Educational Researcher, 37(2):1-19, 2010.  John C. Hull. Options, Futures, and Other Derivatives. Pearson, 2011.  Rishabh K Iyer, Stefanie Jegelka, and Je\ufb00 A Bilmes. Curvature and optimal algorithms for learning and minimizing submodular functions. In Advances in Neural Information Processing Systems, pages 2742-2750, 2013.  Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons.  In  Advances in Neural Information Processing Systems, pages 2240-2248, 2011.  Tyler Neylon. Sparse solutions for linear prediction problems. PhD thesis, New York  University, 2006.  Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani. Algorithmic Game  Theory. Cambridge University Press, 2007.  Sofya Raskhodnikova and Grigory Yaroslavtsev. Learning pseudo-boolean k-dnf and sub- modular functions. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1356-1368. SIAM, 2013.  Nihar B. Shah, Sivaraman Balakrishnan, Joseph K. Bradley, Abhay Parekh, Kannan Ram- chandran, and Martin J. Wainwright. Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. In Proceedings of the Eighteenth Interna- tional Conference on Artificial Intelligence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015, 2015.  Neil Stewart, Gordon DA Brown, and Nick Chater. Absolute identification by relative  judgment. Psychological review, 112(4):881, 2005. Balcan Vitercik White  Peter Stobbe and Andreas Krause. Learning fourier sparse set functions. In International  Conference on Artificial Intelligence and Statistics, pages 1125-1133, 2012.  Daniel Vainsencher, Ofer Dekel, and Shie Mannor. Bundle selling by online estimation of valuation functions. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1137-1144, 2011. Learning Combinatorial Functions from Pairwise Comparisons  "}, "Instance-dependent Regret Bounds for Dueling Bandits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Instance-dependent Regret Bounds for Dueling Bandits", "abstract": "We study the multi-armed dueling bandit problem in which feedback is provided in the form of relative comparisons between pairs of actions, with the goal of eventually learning to select actions that are close to the best. Following  Dudik et al. (2015), we aim for algorithms whose performance approaches that of the optimal randomized choice of actions, the von Neumann winner, expressly avoiding more restrictive assumptions, for instance, regarding the existence of a single best action (a Condorcet winner). In this general setting, the best known algorithms achieve regret O(\\sqrtKT) in T rounds with K actions. In this paper, we present the first instance-dependent regret bounds for the general problem, focusing particularly on when the von Neumann winner is sparse. Specifically, we propose a new algorithm whose regret, relative to a unique von Neumann winner with sparsity s, is at most O(\\sqrtsT), plus an instance-dependent constant. Thus, when the sparsity is much smaller than the total number of actions, our result indicates that learning can be substantially faster.", "pdf_url": "http://proceedings.mlr.press/v49/balsubramani16.pdf", "keywords": ["dueling bandits", "online learning", "game theory"], "reference": "Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In  Proceedings of the International Conference on Machine Learning (ICML), 2014.  R. Busa-Fekete, B. Sz\u00a8or\u00b4enyi, P. Weng, W. Cheng, and E. H\u00a8ullermeier. Top-k selection based on adaptive sampling of noisy preferences. In Proceedings of the International Conference on Ma- chine Learning (ICML), 2013.  R. Busa-Fekete, B. Sz\u00a8or\u00b4enyi, and E. H\u00a8ullermeier. PAC rank elicitation through adaptive sampling of stochastic pairwise preferences. In National Conference on Artificial Intelligence (AAAI), 2014.  Yuxin Chen and Changho Suh. Spectral mle: Top-k rank aggregation from pairwise comparisons. In Proceedings of the International Conference on Machine Learning (ICML), pages 371-380, 2015.  Massimiliano Ciaramita, Vanessa Murdock, and Vassilis Plachouras. Online learning from click  data for sponsored search. In World Wide Web. ACM, 2008.  Miroslav Dud\u00b4\u0131k, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and Masrour Zoghi.  Contextual dueling bandits. In Conference on Learning Theory (COLT), 2015.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and Markov  decision processes. In Conference on Learning Theory (COLT). Springer, 2002.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval, 16(1):63-90, 2013.  Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse dueling bandits. In  Artificial Intelligence and Statistics, pages 416-424, 2015.  Thorsten Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.  Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on Learning Theory (COLT), 2015.  Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Copeland dueling bandit problem: Regret lower bound, optimal algorithm, and computationally efficient algorithm. In Proceedings of the International Conference on Machine Learning (ICML), 2016.  Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons.  In NIPS, 2012.  24   BALSUBRAMANI KARNIN SCHAPIRE ZOGHI  We would like to thank Alekh Agarwal, Miroslav Dud\u00b4\u0131k, and Akshay Krishnamurthy for many formative discussions on the problem.  Acknowledgments  References  Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In  Proceedings of the International Conference on Machine Learning (ICML), 2014.  R. Busa-Fekete, B. Sz\u00a8or\u00b4enyi, P. Weng, W. Cheng, and E. H\u00a8ullermeier. Top-k selection based on adaptive sampling of noisy preferences. In Proceedings of the International Conference on Ma- chine Learning (ICML), 2013.  R. Busa-Fekete, B. Sz\u00a8or\u00b4enyi, and E. H\u00a8ullermeier. PAC rank elicitation through adaptive sampling of stochastic pairwise preferences. In National Conference on Artificial Intelligence (AAAI), 2014.  Yuxin Chen and Changho Suh. Spectral mle: Top-k rank aggregation from pairwise comparisons. In Proceedings of the International Conference on Machine Learning (ICML), pages 371-380, 2015.  Massimiliano Ciaramita, Vanessa Murdock, and Vassilis Plachouras. Online learning from click  data for sponsored search. In World Wide Web. ACM, 2008.  Miroslav Dud\u00b4\u0131k, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and Masrour Zoghi.  Contextual dueling bandits. In Conference on Learning Theory (COLT), 2015.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and Markov  decision processes. In Conference on Learning Theory (COLT). Springer, 2002.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval, 16(1):63-90, 2013.  Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse dueling bandits. In  Artificial Intelligence and Statistics, pages 416-424, 2015.  Thorsten Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.  Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on Learning Theory (COLT), 2015.  Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Copeland dueling bandit problem: Regret lower bound, optimal algorithm, and computationally efficient algorithm. In Proceedings of the International Conference on Machine Learning (ICML), 2016.  Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons.  In NIPS, 2012. REGRET BOUNDS FOR DUELING BANDITS  Guillermo Owen. Game Theory. Emerald Group Publishing Limited, 3rd edition, 1995.  Sandeep Pandey, Deepak Agarwal, Deepayan Chakrabarti, and Vanja Josifovski. Bandits for tax- onomies: A model-based approach. In SIAM Conference on Data Mining (SDM). SIAM, 2007.  Seung-Taek Park and Wei Chu. Pairwise preference regression for cold-start recommendation. In  Proceedings of the Third ACM Conference on Recommender Systems. ACM, 2009.  Ronald L. Rivest and Emily Shen. An optimal single-winner preferential voting system based on In Proceedings Third International Workshop on Computational Social Choice,  game theory. 2010.  Tanguy Urvoy, Fabrice Clerot, Raphael F\u00b4eraud, and Sami Naamane. Generic exploration and k- In Proceedings of the International Conference on Machine Learning  armed voting bandits. (ICML), pages 91-99, 2013.  Yisong Yue and Thorsten Joachims. Beat the mean bandit.  In Proceedings of the International  Conference on Machine Learning (ICML), 2011.  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The K-armed dueling bandits  problem. Journal of Computer and System Sciences, 78(5):1538-1556, September 2012.  Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R\u00b4emi Munos. Relative confidence sam- pling for efficient on-line ranker evaluation. In Proceedings of the International Conference on Web Search and Data Mining (WSDM), 2014a.  Masrour Zoghi, Shimon Whiteson, R\u00b4emi Munos, and Maarten de Rijke. Relative upper confidence bound for the k-armed dueling bandits problem. In Proceedings of the International Conference on Machine Learning (ICML), 2014b.  Masrour Zoghi, Zohar Karnin, Shimon Whiteson, and Maarten de Rijke. Copeland dueling bandits.  In Proceedings of the International Conference on Machine Learning (ICML), 2015a.  Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale In Proceedings of the International Conference on Web Search and  online ranker evaluation. Data Mining (WSDM), 2015b. "}, "On the low-rank approach for semidefinite programs arising in synchronization and community detection": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "On the low-rank approach for semidefinite programs arising in synchronization and community detection", "abstract": "To address difficult optimization problems, convex relaxations based on semidefinite programming are now common place in many fields. Although solvable in polynomial time, large semidefinite programs tend to be computationally challenging. Over a decade ago, exploiting the fact that in many applications of interest the desired solutions are low rank, Burer and Monteiro proposed a heuristic to solve such semidefinite programs by restricting the search space to low-rank matrices. The accompanying theory does not explain the extent of the empirical success. We focus on Synchronization and Community Detection problems and provide theoretical guarantees shedding light on the remarkable efficiency of this heuristic.", "pdf_url": "http://proceedings.mlr.press/v49/bandeira16.pdf", "keywords": ["Semidefinite Programming", "Burer-Monteiro heuristic", "SDPLR", "Synchronization", "Community Detection"], "reference": "E. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. Network Science and Engineering, IEEE Transactions on, 1(1):10-22, 2014.  E. Abbe, A.S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Information  Theory, IEEE Transactions on, 62(1):471-487, 2016.  17   THE LOW-RANK APPROACH FOR CERTAIN SEMIDEFINITE PROGRAMS  To ensure rank(Q) = 1, it remains to force rank(ddiag(AQQT ) \u2212 \u03c3\u2206) = n. Since the first matrix is diagonal, this is the case in particular if  This can be controlled by Lemmas 12 and 13:  diag(AQQT )i > \u03c3(cid:107)\u2206(cid:107).  min i  (cid:107)eT  min i  i AQ(cid:107)2 \u2212 \u03c3(cid:107)\u2206(cid:107) = min  (cid:107)eT  i zzT Q + \u03c3 \u00b7 eT (cid:107)eT  \u2265 (cid:107)QT z(cid:107)2 \u2212 \u03c3 \u00b7 max  i  i \u2206Q(cid:107)2 \u2212 \u03c3(cid:107)\u2206(cid:107)  i  i \u2206Q(cid:107)2 \u2212 \u03b3cn (cid:17)  \u221a  (cid:16)(cid:112)log n + 4  \u03b3cn  .  \u2265 n \u2212 9\u03b3cn \u2212 \u03b3cn  Forcing the latter to be positive (as with the condition in this lemma\u2019s statement) is sufficient to imply rank(Q) = 1.  Theorem 15 Let A = zzT + \u03c3\u2206 for some planted solution z \u2208 {\u00b11}n, with \u03c3 = c diag(\u2206) = 0, (cid:107)\u2206(cid:107) \u2264 \u03b3  n log n, for some \u03b3, c \u2265 0. If  n and (cid:107)\u2206z(cid:107)\u221e \u2264 \u03b3  \u221a  \u221a  \u221a  n, \u2206 = \u2206T ,  \u03b3c <  \u221a  1 log n + 4  \u221a  ,  \u03b3cn  9 +  then all second-order critical points Q of (10) with cost matrix A are global optima of rank 1 such that QQT = zzT . There exists a constant k such that, if \u03b3c \u2264 kn\u22121/3, then the theorem applies.  Proof By Lemma 14, all second-order critical points Q have rank 1. By Lemma 10, such Q\u2019s are thus globally optimal. By Lemma 11 (whose conditions are satisfied a fortiori), they all satisfy QQT = zzT .  Acknowledgments  ASB was supported by NSF grant DMS-1317308. ASB acknowledges Wotao Yin for pointing the author to the relevant reference (Wen and Yin, 2013) which helped motivate the start of the investigation in this paper. NB, formerly hosted by the SIERRA group at Inria and ENS, was supported by the \u201cFonds Sp\u00b4eciaux de Recherche\u201d (FSR) at UCLouvain and by the Chaire Havas \u201cChaire Economie et gestion des nouvelles donn\u00b4ees\u201d, the ERC Starting Grant SIPA and a Research in Paris grant in Paris. VV acknowledges support from the Office of Naval Research.  References  E. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. Network Science and Engineering, IEEE Transactions on, 1(1):10-22, 2014.  E. Abbe, A.S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Information  Theory, IEEE Transactions on, 62(1):471-487, 2016. BANDEIRA BOUMAL VORONINSKI  P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton  University Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3.  J. Baik, G. Ben-Arous, and S. P\u00b4ech\u00b4e. Phase transition of the largest eigenvalue for nonnull complex  sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.  A. S. Bandeira and R. v. Handel. Sharp nonasymptotic bounds on the norm of random matrices  with independent entries. Annals of Probability, to appear, 2015.  A. S. Bandeira, M. Charikar, A. Singer, and A. Zhu. Multireference alignment using semidefinite  programming. 5th Innovations in Theoretical Computer Science (ITCS 2014), 2014a.  A.S. Bandeira. A note on probably certifiably correct algorithms. Available at arXiv:1509.00824  [math.OC], 2015a.  arXiv:1504.03987, 2015b.  A.S. Bandeira.  Random Laplacian matrices and convex relaxations.  arXiv preprint  A.S. Bandeira, C. Kennedy, and A. Singer. Approximating the little Grothendieck problem over the  orthogonal and unitary groups. arXiv preprint arXiv:1308.5207, 2013.  A.S. Bandeira, N. Boumal, and A. Singer. Tightness of the maximum likelihood semidefinite relax-  ation for angular synchronization. arXiv preprint arXiv:1411.3272, 2014b.  A.S. Bandeira, Y. Chen, and A. Singer. Non-unique games over compact groups and orientation  estimation in cryo-EM. arXiv preprint arXiv:1505.03840, 2015.  A.I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete  & Computational Geometry, 13(1):189-202, 1995. doi: 10.1007/BF02574037.  N. Boumal. A Riemannian low-rank method for optimization over semidefinite matrices with block-  diagonal constraints. arXiv preprint arXiv:1506.00575, 2015.  N. Boumal. Nonconvex phase synchronization. arXiv preprint arXiv:1601.06114, 2016.  N. Boumal, P.-A. Absil, and C. Cartis. Global rates of convergence for nonconvex optimization on  manifolds. arXiv preprint arXiv:1605.08101, 2016.  J. Bri\u00a8et, F.M. de Oliveira Filho, and F. Vallentin. Grothendieck inequalities for semidefinite pro- grams with rank constraint. Theory of Computing, 10(4):77-105, 2014. doi: 10.4086/toc.2014. v010a004.  S. Burer and R.D.C. Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329-357, 2003. doi: 10.1007/s10107-002-0352-8.  S. Burer and R.D.C. Monteiro. Local minima and convergence in low-rank semidefinite program-  ming. Mathematical Programming, 103(3):427-444, 2005.  S. Burer, R.D.C. Monteiro, and Y. Zhang. Rank-two relaxation heuristics for Max-Cut and other  binary quadratic programs. SIAM Journal on Optimization, 12(2):503-521, 2002. THE LOW-RANK APPROACH FOR CERTAIN SEMIDEFINITE PROGRAMS  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84, December 2011.  U. Feige and J. Kilian. Heuristics for semirandom graph problems. Journal of Computer and System  Sciences, 63(4):639 - 671, 2001.  D. F\u00b4eral and S. P\u00b4ech\u00b4e. The largest eigenvalue of rank one deformation of large wigner matrices.  Communications in Mathematical Physics, 272(1):185-228, 2006.  M.X. Goemans and D.P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6): 1115-1145, 1995. doi: 10.1145/227683.227684.  O. Guedon and R. Vershynin. Community detection in sparse networks via Grothendieck\u2019s inequal-  ity. Available online at arXiv:1411.4686 [math.ST], 2014.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite program-  ming. Available online at arXiv:1412.6156, 2014.  A. Javanmard, A. Montanari, and F. Ricci-Tersenghi. Phase transitions in semidefinite relaxations.  arXiv preprint arXiv:1511.08769, 2015.  M. Journ\u00b4ee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327-2351, 2010. doi: 10.1137/ 080731359.  M. Krivelevich and B. Sudakov. The largest eigenvalue of sparse random graphs. Combinatorics,  Probability and Computing, 12:61-72, 2003.  L. Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 694-703, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591857. URL http://doi.acm.org/10.1145/2591796.2591857.  A. Moitra, W. Perry, and A. S. Wein. How robust are reconstruction thresholds for community  detection? Available online at arXiv:1511.01473 [cs.DS], 2015.  A. Montanari and S. Sen. Semidefinite programs on sparse random graphs. Available online at  arXiv:1504.05910 [cs.DM], 2015.  E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. Probability Theory  and Related Fields (to appear), 2014a.  E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture. Available  online at arXiv:1311.4115 [math.PR], January 2014b.  E. Mossel, J. Neeman, and A. Sly. Consistency thresholds for the planted bisection model. Available  online at arXiv:1407.1591v2 [math.PR], July 2014c.  Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied  optimization. Springer, 2004. ISBN 978-1-4020-7553-7. BANDEIRA BOUMAL VORONINSKI  G. Pataki. On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues. Mathematics of operations research, 23(2):339-358, 1998. doi: 10.1287/moor.23. 2.339.  A. Shapiro. Rank-reducibility of a symmetric matrix and sampling theory of minimum trace factor  analysis. Psychometrika, 47(2):187-199, 1982.  A. Singer. Angular synchronization by eigenvectors and semidefinite programming. Applied and  Computational Harmonic Analysis, 30(1):20-36, 2011. doi: 10.1016/j.acha.2010.02.001.  Z. Wen and W. Yin. A feasible method for optimization with orthogonality constraints. Mathemat-  ical Programming, 142(1-2):397-434, 2013. doi: 10.1007/s10107-012-0584-1.  "}, "Information-theoretic thresholds for community detection in sparse networks": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Information-theoretic thresholds for community detection in sparse networks", "abstract": "We give upper and lower bounds on the information-theoretic threshold for community detection in the stochastic block model.  Specifically, consider a symmetric stochastic block model with q groups, average degree d, and connection probabilities c_\\mathrmin/n and c_\\mathrmout/n for within-group and between-group edges respectively; let \u03bb= (c_\\mathrmin-c_\\mathrmout)/(qd).  We show that, when q is large, and \u03bb= O(1/q), the critical value of d at which community detection becomes possible\u2014in physical terms, the condensation threshold\u2014is $ d_\\mathrmc = \u0398\\left( \\frac\\log qq \u03bb^2 \\right) \u2009,  with tighter results in certain regimes.  Above this threshold, we show that any partition of the nodes into q groups which is as \u2018good\u2019 as the planted one, in terms of the number of within- and between-group edges, is correlated with it. This gives an exponential-time algorithm that performs better than chance; specifically, community detection becomes possible below the Kesten-Stigum bound for q \\ge 5 in the disassortative case \u03bb< 0, and for q \\ge 11 in the assortative case \u03bb> 0 (similar upper bounds were obtained independently by Abbe and Sandon). Conversely, below this threshold, we show that no algorithm can label the vertices better than chance, or even distinguish the block model from an Erd\u0151s-R\u00e9nyi random graph with high probability. Our lower bound on d_\\mathrmc uses Robinson and Wormald\u2019s small subgraph conditioning method, and we also give (less explicit) results for non-symmetric stochastic block models.  In the symmetric case, we obtain explicit results by using bounds on certain functions of doubly stochastic matrices due to Achlioptas and Naor; indeed, our lower bound on d_\\mathrmc is their second moment lower bound on the q$-colorability threshold for random graphs with a certain effective degree.", "pdf_url": "http://proceedings.mlr.press/v49/banks16.pdf", "keywords": [], "reference": "E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap. ArXiv e-prints, 2015.  E. Abbe and C. Sandon. Crossing the KS threshold in the stochastic block model with information  theory. Proc. International Symposium on Information Theory (ISIT), 2016.  E. Abbe, A.S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. IEEE Transac-  tions on Information Theory, 62(1):471-487, 2016.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pages 670-688, 2015. doi: 10.1109/FOCS.2015.47. URL http://dx.doi.org/10.1109/FOCS.2015.47.  Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase transitions. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS, pages 793-802, 2008.  Dimitris Achlioptas and Assaf Naor. The two possible values of the chromatic number of a random  graph. Annals of Mathematics, 162:1335-1351, 2005.  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. CoRR, abs/1507.02323, 2015. URL http://arxiv.org/abs/1507.02323.  Victor Bapst, Amin Coja-Oghlan, Samuel Hetterich, Felicia Ra\u00dfmann, and Dan Vilenchik. The condensation phase transition in random graph coloring. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM, pages 449-464, 2014.  Victor Bapst, Amin Coja-Oghlan, and Felicia Ra\u00dfmann. A positive temperature phase transition in random hypergraph 2-coloring. Annals of Applied Probability, abs/1410.2190, to appear. URL http://arxiv.org/abs/1410.2190.  Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and Newman-Girvan  and other modularities. Proc. Natl. Acad. Sci. USA, 106:21068-21073, 2009.  B\u00b4ela Bollob\u00b4as, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random  graphs. Random Structures and Algorithms, 31:3-122, 2007.  29   THRESHOLDS FOR SPARSE COMMUNITY DETECTION  Plugging (37), (38) and (39) in (36) shows that  Nii \u2212  E  (cid:18)  1 n  Ni(\u03c3)Ni(  (G))  A  = o(n2).(cid:19)  This finishes the proof.  References  E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap. ArXiv e-prints, 2015.  E. Abbe and C. Sandon. Crossing the KS threshold in the stochastic block model with information  theory. Proc. International Symposium on Information Theory (ISIT), 2016.  E. Abbe, A.S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. IEEE Transac-  tions on Information Theory, 62(1):471-487, 2016.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pages 670-688, 2015. doi: 10.1109/FOCS.2015.47. URL http://dx.doi.org/10.1109/FOCS.2015.47.  Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase transitions. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS, pages 793-802, 2008.  Dimitris Achlioptas and Assaf Naor. The two possible values of the chromatic number of a random  graph. Annals of Mathematics, 162:1335-1351, 2005.  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. CoRR, abs/1507.02323, 2015. URL http://arxiv.org/abs/1507.02323.  Victor Bapst, Amin Coja-Oghlan, Samuel Hetterich, Felicia Ra\u00dfmann, and Dan Vilenchik. The condensation phase transition in random graph coloring. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM, pages 449-464, 2014.  Victor Bapst, Amin Coja-Oghlan, and Felicia Ra\u00dfmann. A positive temperature phase transition in random hypergraph 2-coloring. Annals of Applied Probability, abs/1410.2190, to appear. URL http://arxiv.org/abs/1410.2190.  Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and Newman-Girvan  and other modularities. Proc. Natl. Acad. Sci. USA, 106:21068-21073, 2009.  B\u00b4ela Bollob\u00b4as, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random  graphs. Random Structures and Algorithms, 31:3-122, 2007. BANKS MOORE NEEMAN NETRAPALLI  B\u00b4ela Bollob\u00b4as, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random  graphs. Random Structures & Algorithms, 31(1):3-122, 2007.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Non-backtracking spectrum of random graphs: Community detection and non-regular ramanujan graphs. In IEEE 56th Annual Sympo- sium on Foundations of Computer Science, FOCS, pages 1347-1357, 2015.  Amin Coja-Oghlan and Charilaos Efthymiou. On independent sets in random graphs. Random  Struct. Algorithms, 47(3):436-486, 2015.  Amin Coja-Oghlan and Dan Vilenchik. Chasing the k-colorability threshold. In 54th Annual IEEE  Symposium on Foundations of Computer Science, FOCS, pages 380-389, 2013.  J. R. L. de Almeida and D. J. Thouless. Stability of the Sherrington-Kirkpatrick solution of a spin-  glass model. J. Phys. A, 11:983-990, 1978.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Inference and phase transitions in the detection of modules in sparse networks. Physical Review Letters, 107:065701, 2011a.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical Review E, 84:066106, 2011b.  W. Evans, C. Kenyon, Y. Peres, and L.J. Schulman. Broadcasting on trees and the Ising model. The  Annals of Applied Probability, 10(2):410-433, 2000.  P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: Some first steps. Social  Networks, 5:109-137, 1983.  Svante Janson. Random regular graphs: asymptotic distributions and contiguity. Combinatorics,  Probability and Computing, 4(04):369-405, 1995.  Svante Janson and Elchanan Mossel. Robust reconstruction on trees is determined by the second  eigenvalue. Annals of Probability, pages 2630-2649, 2004.  Varun Kanade, Elchanan Mossel, and Tselil Schramm. Global and local information in cluster- ing labeled block models. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM, pages 779-792, 2014.  Harry Kesten and Bernt P. Stigum. A limit theorem for multidimensional Galton-Watson processes.  The Annals of Mathematical Statistics, 37(5):1211-1223, 1966a.  Harry Kesten and Bernt P. Stigum. Additional limit theorems for indecomposable multidimensional Galton-Watson processes. The Annals of Mathematical Statistics, 37(6):1463-1481, 1966b.  F. Krzakala, A. Montanari, F. Ricci-Tersenghi, G. Semerjian, and L. Zdeborov\u00b4a. Gibbs states and the set of solutions of random constraint satisfaction problems. Proc. Natl. Acad. Sci. USA, 104: 10318, 2007. THRESHOLDS FOR SPARSE COMMUNITY DETECTION  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov\u00b4a, and Pan Zhang. Spectral redemption in clustering sparse networks. Proc. Natl. Acad. Sci. USA, 110:20935-20940, 2013.  A.M. Mathai  and Serge B. Provost. Statistics Series. Taylor & Francis, http://books.google.com/books?id=tFOqQgAACAAJ.  Quadratic Forms  1992.  ISBN 9780824786915.  in Random Variables. URL  Frank McSherry. Spectral partitioning of random graphs. In 42nd Annual Symposium on Founda- tions of Computer Science, FOCS, pages 529-537, 2001. doi: 10.1109/SFCS.2001.959929. URL http://dx.doi.org/10.1109/SFCS.2001.959929.  E. Mossel, J. Neeman, and A. Sly. Stochastic Block Models and Reconstruction. ArXiv e-prints,  2012.  E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. Probability Theory  and Related Fields, 2014a. (to appear).  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction and In Proceedings of The 27th Conference on Learning optimal recovery of block models. Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, pages 356-370, 2014b. URL http://jmlr.org/proceedings/papers/v35/mossel14.html.  R.W. Robinson and N.C. Wormald. Almost all cubic graphs are Hamiltonian. Random Structures  and Algorithms, 3(2):117-125, 1992.  Bo S\u00a8oderberg. General formalism for inhomogeneous random graphs. Physical Review E, 66:  066121, 2002.  Series, pages 239-298, 1999.  N.C. Wormald. Models of random regular graphs. London Mathematical Society Lecture Note  Pan Zhang and Cristopher Moore.  ties and hierarchies, using message passing for modularity. Academy of Sciences, 111(51):18144-18149, 2014. http://www.pnas.org/content/111/51/18144.abstract.  Scalable detection of statistically significant communi- Proceedings of the National doi: 10.1073/pnas.1409770111. URL  Pan Zhang, Cristopher Moore, and M. E. J. Newman. Community detection in networks with  unequal groups. Phys. Rev. E, 93:012303, 2016. BANKS MOORE NEEMAN NETRAPALLI  "}, "Noisy Tensor Completion via the Sum-of-Squares Hierarchy": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Noisy Tensor Completion via the Sum-of-Squares Hierarchy", "abstract": "In the noisy tensor completion problem we observe m entries (whose location is chosen uniformly at random) from an unknown n_1 \\times n_2 \\times n_3 tensor T. We assume that T is entry-wise close to being rank r. Our goal is to fill in its missing entries using as few observations as possible. Let n = \\max(n_1, n_2, n_3). We show that if m =  n^3/2 r then there is a polynomial time algorithm based on the sixth level of the sum-of-squares hierarchy for completing it. Our estimate agrees with almost all of T\u2019s entries almost exactly and works even when our observations are corrupted by noise. This is also the first algorithm for tensor completion that works in the overcomplete case when r > n, and in fact it works all the way up to r = n^3/2-\u03b5. Our proofs are short and simple and are based on establishing a new connection between noisy tensor completion (through the language of Rademacher complexity) and the task of refuting random constant satisfaction problems. This connection seems to have gone unnoticed even in the context of matrix completion. Furthermore, we use this connection to show matching lower bounds. Our main technical result is in characterizing the Rademacher complexity of the sequence of norms that arise in the sum-of-squares relaxations to the tensor nuclear norm.  These results point to an interesting new direction: Can we explore computational vs. sample complexity tradeoffs through the sum-of-squares hierarchy?", "pdf_url": "http://proceedings.mlr.press/v49/barak16.pdf", "keywords": ["tensor completion", "sum-of-squares", "computational vs statistical tradeoffs"], "reference": "Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Yi-Kai Liu. A spectral algorithm for latent dirichlet allocation. Algorithmica, 72(1):193-214, 2015. doi: 10.1007/ s00453-014-9909-1.  Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 867-881, 2013.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  CoRR, abs/1404.5236, 2014.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Proceed- ings of the 44th Symposium on Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19 - 22, 2012, pages 307-326, 2012. doi: 10.1145/2213977.2214006.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations.  In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 31-40, 2014. doi: 10.1145/2591796.2591886.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 143-151, 2015. doi: 10.1145/2746539.2746605.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. J. Mach. Learn. Res., 3:463-482, March 2003. ISSN 1532-4435.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 1046-1066, 2013.  Srinadh Bhojanapalli and Sujay Sanghavi. A new sampling technique for tensors. CoRR,  abs/1502.05023, 2015.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun- dations of Computational Mathematics, 9(6):717-772, 2009. doi: 10.1007/s10208-009-9045-5.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: near-optimal ma- doi:  IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  trix completion. 10.1109/TIT.2010.2044061.  24   BARAK MOITRA  We would like to thank Aram Harrow for many helpful discussions. Part of this work was done while BB was at Microsoft Research, New England. AM is supposed by NSF CAREER Award CCF-1453261, a grant from the MIT NEC Corporation and a Google Faculty Research Award.  Acknowledgments  References  Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Yi-Kai Liu. A spectral algorithm for latent dirichlet allocation. Algorithmica, 72(1):193-214, 2015. doi: 10.1007/ s00453-014-9909-1.  Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham Kakade. A tensor spectral approach to learning mixed membership community models. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 867-881, 2013.  Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms.  CoRR, abs/1404.5236, 2014.  Boaz Barak, Fernando G. S. L. Brand\u02dcao, Aram Wettroth Harrow, Jonathan A. Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Proceed- ings of the 44th Symposium on Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19 - 22, 2012, pages 307-326, 2012. doi: 10.1145/2213977.2214006.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations.  In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 31-40, 2014. doi: 10.1145/2591796.2591886.  Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 143-151, 2015. doi: 10.1145/2746539.2746605.  Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and  structural results. J. Mach. Learn. Res., 3:463-482, March 2003. ISSN 1532-4435.  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 1046-1066, 2013.  Srinadh Bhojanapalli and Sujay Sanghavi. A new sampling technique for tensors. CoRR,  abs/1502.05023, 2015.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun- dations of Computational Mathematics, 9(6):717-772, 2009. doi: 10.1007/s10208-009-9045-5.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: near-optimal ma- doi:  IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  trix completion. 10.1109/TIT.2010.2044061. NOISY TENSOR COMPLETION  Anthony Carbery and James Wright. Distributional and l\u02c6 q norm inequalities for polynomials over  convex bodies in r\u02c6 n. Mathematical Research Letters, 8(3):233-248, 2001.  Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, and Alan S. Willsky. The convex ge- ometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012. doi: 10.1007/s10208-012-9135-7.  Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple- tion. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 674-682, 2014.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for ran- doi: 10.1017/  dom k-sat. Combinatorics, Probability & Computing, 16(1):5-28, 2007. S096354830600784X.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning halfspaces over sparse vectors. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 145-153, 2013.  M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.  Uriel Feige. Relations between average case complexity and approximation complexity. In Pro- ceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21, 2002, Montr\u00b4eal, Qu\u00b4ebec, Canada, pages 534-543, 2002. doi: 10.1145/509907.509985.  Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. Theory of  Computing, 3(1):25-43, 2007. doi: 10.4086/toc.2007.v003a002.  Uriel Feige, Jeong Han Kim, and Eran Ofek. Witnesses for non-satisfiability of dense random 3cnf formulas. In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006), 21-24 October 2006, Berkeley, California, USA, Proceedings, pages 497-508, 2006. doi: 10.1109/FOCS.2006.78.  Joel Friedman, Jeff Kahn, and Endre Szemer\u00b4edi. On the second eigenvalue in random regular graphs. In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, May 14- 17, 1989, Seattle, Washigton, USA, pages 587-598, 1989. doi: 10.1145/73007.73063.  Joel Friedman, Andreas Goerdt, and Michael Krivelevich. Recognizing more unsatisfiable ran- doi: 10.1137/  SIAM J. Comput., 35(2):408-430, 2005.  dom k-sat instances efficiently. S009753970444096X.  Silvia Gandy, Benjamin Recht, and Isao Yamada. Tensor completion and low-n-rank tensor recovery  via convex optimization. Inverse Problems, 27(2):025010, 2011.  Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM 2015, August 24-26, 2015, Princeton, NJ, USA, pages 829-849, 2015. doi: 10.4230/LIPIcs.APPROX-RANDOM.2015.829. BARAK MOITRA  Andreas Goerdt and Michael Krivelevich. Efficient recognition of random unsatisfiable k-sat in- stances by spectral methods. In STACS 2001, 18th Annual Symposium on Theoretical Aspects of Computer Science, Dresden, Germany, February 15-17, 2001, Proceedings, pages 294-304, 2001. doi: 10.1007/3-540-44693-1 26.  Dima Grigoriev. Linear lower bound on degrees of positivstellensatz calculus proofs for the parity.  Theor. Comput. Sci., 259(1-2):613-622, 2001. doi: 10.1016/S0304-3975(00)00157-2.  Leonid Gurvits. Classical deterministic complexity of edmonds\u2019 problem and quantum entangle- ment. In Proceedings of the 35th Annual ACM Symposium on Theory of Computing, June 9-11, 2003, San Diego, CA, USA, pages 10-19, 2003. doi: 10.1145/780542.780545.  Moritz Hardt. Understanding alternating minimization for matrix completion. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 651-660, 2014. doi: 10.1109/FOCS.2014.75.  Aram Wettroth Harrow and Ashley Montanaro. Testing product states, quantum merlin-arthur  games and tensor optimization. J. ACM, 60(1):3, 2013. doi: 10.1145/2432622.2432625.  Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Speeding up sum-of-squares for tensor decomposition and planted sparse vectors. arXiv preprint arXiv:1512.02337, 2015.  Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013, pages 11-20, 2013. doi: 10.1145/2422436.2422439.  Prateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In Advances in Neu- ral Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 1431-1439, 2014.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Symposium on Theory of Computing Conference, STOC\u201913, Palo Alto, CA, USA, June 1-4, 2013, pages 665-674, 2013. doi: 10.1145/2488608.2488693.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy  entries. Journal of Machine Learning Research, 11:2057-2078, 2010.  Daniel Kressner, Michael Steinlechner, and Bart Vandereycken. Low-rank tensor completion by  riemannian optimization. BIT Numerical Mathematics, 54(2):447-468, 2014.  Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal  on Optimization, 11(3):796-817, 2001.  Troy Lee and Adi Shraibman. Matrix completion from any given set of observations. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Pro- cessing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 1781-1787, 2013.  Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating missing values in visual data. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):208-220, 2013. NOISY TENSOR COMPLETION  Ji\u02c7r\u00b4\u0131 Matou\u02c7sek. Lectures on discrete geometry, volume 212. Springer New York, 2002.  Elchanan Mossel and S\u00b4ebastien Roch. Learning nonsingular phylogenies and hidden markov mod- els. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing, Baltimore, MD, USA, May 22-24, 2005, pages 366-375, 2005. doi: 10.1145/1060590.1060645.  Cun Mu, Bo Huang, John Wright, and Donald Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 73-81, 2014.  Yurii Nesterov. Squared functional systems and optimization problems. In High performance opti-  mization, pages 405-440. Springer, 2000.  Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robust-  ness and optimization. PhD thesis, California Institute of Technology, 2000.  Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471-501, 2010. doi: 10.1137/070697835.  Grant Schoenebeck. Linear level lasserre lower bounds for certain k-csps. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadel- phia, PA, USA, pages 593-602, 2008. doi: 10.1109/FOCS.2008.74.  Naum Zuselevich Shor. An approach to obtaining global extremums in polynomial mathematical  programming problems. Cybernetics, 23(5):695-700, 1988.  Marco Signoretto, Lieven De Lathauwer, and Johan AK Suykens. Nuclear norms for tensors and  their use for convex multilinear estimation. Tech Report 10-186, K. U. Leuven, 2010.  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005, Pro- ceedings, pages 545-560, 2005. doi: 10.1007/11503415 37.  Gongguo Tang, Badri Narayan Bhaskar, Parikshit Shah, and Benjamin Recht. Compressed sensing off the grid. IEEE Transactions on Information Theory, 59(11):7465-7490, 2013. doi: 10.1109/ TIT.2013.2277451.  Ming Yuan and Cun-Hui Zhang. On tensor completion via nuclear norm minimization. Foundations  of Computational Mathematics, pages 1-38, 2014.  "}, "Basis Learning as an Algorithmic Primitive": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Basis Learning as an Algorithmic Primitive", "abstract": "A number of important problems in theoretical computer science and machine learning can be interpreted as recovering a certain basis. These include  symmetric matrix eigendecomposition, certain tensor decompositions, Independent Component Analysis (ICA), spectral clustering and Gaussian mixture learning. Each of these problems reduces to an instance of our general model, which we call a \u201cBasis Encoding Function\" (BEF). We show that learning a basis within this model can then be provably and efficiently achieved using a first order  iteration algorithm (gradient iteration). Our algorithm goes beyond tensor methods while generalizing a number of existing algorithms\u2014e.g., the power method for symmetric matrices, the tensor power iteration for orthogonal decomposable tensors, and cumulant-based FastICA\u2014all within a broader function-based dynamical systems framework. Our framework also unifies the unusual phenomenon observed in these domains that they can be solved using efficient non-convex optimization. Specifically, we describe a class of BEFs such that their local maxima on the unit sphere are in one-to-one correspondence with the basis elements. This description relies on a certain \u201chidden convexity\" property of these functions. We provide a complete theoretical analysis of the gradient iteration even when the BEF is perturbed. We show convergence and complexity bounds polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a  non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. In addition we show that   our algorithm exhibits fast (superlinear) convergence and relate the speed of convergence to the properties of the BEF. Moreover, the gradient iteration algorithm can be easily and efficiently implemented in practice.", "pdf_url": "http://proceedings.mlr.press/v49/belkin16.pdf", "keywords": [], "reference": "Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm for latent Dirichlet allocation. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems., pages 926-934, 2012a. URL http://books.nips.cc/papers/files/nips25/NIPS2012_0441. pdf.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559, 2012b. URL http: //arxiv.org/abs/1210.7559.  Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture mod- els and hidden Markov models. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scot- land, volume 23 of JMLR Proceedings, pages 33.1-33.34. JMLR.org, 2012c. URL http://www. jmlr.org/proceedings/papers/v23/anandkumar12/anandkumar12.pdf.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Learning overcomplete latent variable  models through tensor methods. In Conference on Learning Theory (COLT), 2015.  38   BELKIN RADEMACHER VOSS  \u03bamin  d2 \u03b5, and N \u2265 C3d8[M8+\u00b58( \u02c6X,X)]  Theorem 7.3 Fix \u03b4 > 0 and \u03b5 > 0. Suppose \u03c3 \u2264 C0 d2  (cid:1)9/2d\u22126, \u00b54( \u02c6X, X) \u2264 , \u03b5 \u2264 C1\u03c3(cid:0) \u03bamin \u03bamax . Let \u02c6A1, . . . , \u02c6Ad \u2190 ROBUSTGI-RECOVERY(d, \u03c3), where C2 \u03b5 ))(cid:7), and FINDBASISELEMENT is implemented with I \u2265 C6d log(d/\u03b4), N1 \u2265 C4 \u03b5 ))(cid:7). Then, with probability at least 1 \u2212 \u03b4, there N2 \u2265 C5 exists a permutation \u03c0 of [d] and sign values si \u2208 {\u00b11} such that (cid:107) \u02c6Ai \u2212 siA\u03c0(i)(cid:107) \u2264 \u03b5 for all i \u2208 [d]. ROBUSTGI-RECOVERY recovers such \u02c6Ais in C7N [d4 + d2N1 + d2IN2] time.  )(cid:7) + (cid:6) log2(log2( 1  (cid:1)3 log(d \u00b7 \u03bamax \u03bamin  (cid:6) log2(log2( 1  (cid:0) \u03bamax \u03bamin  (cid:6) d2.5 \u03c3  \u03ba2 min\u03b52\u03b4  \u03bamin \u03bamax  Proof By Lemma 7.2 with the choice of \u03b7 = O( \u03bamin  (cid:107)\u2207F (u) \u2212 \u2207 \u02c6F (u)(cid:107) \u2264 4(\u03b7 + \u00b54(X, \u02c6X))d2 \u2264 O  d2 \u03b5), we obtain that with probablity at least 1 \u2212 \u03b4 2 , (cid:17)  (cid:16) \u03bamin  d2 = O(\u03bamin\u03b5),  d2 \u03b5 +  \u03bamin d2 \u03b5  for all u \u2208 Sd\u22121. In particular, \u02c6F is an O(\u03bamin\u03b5)-approximation to F .  We recall from Lemma 7.1 that F is an (2\u03bamax, 2\u03bamin, 1, 1)-robust BEF. As such, we may apply Corollary 6.6 to obtain that ROBUSTGI-RECOVERY returns vectors \u02c6A1, . . . , \u02c6Ad of the de- sired form. Finally, we note that within our computational model for this theorem, computations of \u2207 \u02c6F (u) take O(N d) time. Thus, applying Theorem 6.5 with \u02c6m = d yields the claimed time bound.  This material is based upon work supported by the National Science Foundation under Grants No. 1350870, 1422830, 15507576, and 1117707.  Acknowledgments  References  Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm for latent Dirichlet allocation. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems., pages 926-934, 2012a. URL http://books.nips.cc/papers/files/nips25/NIPS2012_0441. pdf.  Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559, 2012b. URL http: //arxiv.org/abs/1210.7559.  Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture mod- els and hidden Markov models. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scot- land, volume 23 of JMLR Proceedings, pages 33.1-33.34. JMLR.org, 2012c. URL http://www. jmlr.org/proceedings/papers/v23/anandkumar12/anandkumar12.pdf.  Animashree Anandkumar, Rong Ge, and Majid Janzamin. Learning overcomplete latent variable  models through tensor methods. In Conference on Learning Theory (COLT), 2015. BASIS LEARNING AS AN ALGORITHMIC PRIMITIVE  Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown In NIPS, pages  Gaussian noise, with implications for Gaussian mixtures and autoencoders. 2384-2392, 2012.  Francis R. Bach and Michael I. Jordan. Learning spectral clustering, with application to speech  separation. Journal of Machine Learning Research, 7:1963-2001, 2006.  Marian Stewart Bartlett, Javier R Movellan, and Terrence J Sejnowski. Face recognition by indepen- dent component analysis. Neural Networks, IEEE Transactions on, 13(6):1450-1464, 2002.  Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of  Gaussian noise. In JMLR W&CP, volume 30: COLT, pages 270-287, 2013.  Mikhail Belkin, Luis Rademacher, and James R. Voss. The hidden convexity of spectral clustering.  CoRR, abs/1403.0667, 2014. URL http://arxiv.org/abs/1403.0667.  Mikhail Belkin, Luis Rademacher, and James Voss. Basis Learning as an Algorithmic Primitive.  CoRR, abs/1411.1420v4, 2016. URL http://arxiv.org/abs/1411.1420v4.  A.J. Bell and T.J. Sejnowski. The \u201cindependent components\u201d of natural scenes are edge filters. Vision  research, 37(23):3327-3338, 1997.  Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287-314,  Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press,  1994.  2010.  Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM  Journal on Numerical Analysis, 7(1):1-46, 1970.  Lieven De Lathauwer, Pierre Comon, Bart De Moor, and Joos Vandewalle. Higher-order power  method. NOLTA Conference, 1995.  Nathalie Delfosse and Philippe Loubaton. Adaptive blind separation of independent sources: A  de\ufb02ation approach. Signal processing, 45(1):59-83, 1995.  Manfredo Perdigao do Carmo Valero. Riemannian geometry. 1992.  Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In FOCS, pages  359-368. IEEE Computer Society, 1996.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic  gradient for tensor decomposition. arXiv preprint arXiv:1503.02101, 2015.  Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA and robust tensor decomposition. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 584-593, 2014. doi: 10.1145/2591796.2591875. URL http://doi.acm.org/10. 1145/2591796.2591875. BELKIN RADEMACHER VOSS  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Convex Analysis and Minimization Algorithms:  Part 1: Fundamentals, volume 1. Springer, 1996.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science (ITCS), pages 11-20. ACM, 2013.  A. Hyv\u00a8arinen, J. Karhunen, and E. Oja. Independent component analysis. John Wiley & Sons, 2001.  Aapo Hyv\u00a8arinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE  Transactions on Neural Networks, 10(3):626-634, 1999.  Aapo Hyv\u00a8arinen and Erkki Oja. Independent component analysis by general nonlinear Hebbian-like  learning rules. Signal Processing, 64(3):301-313, 1998.  John Francis Kenney and Ernest Sydney Keeping. Mathematics of Statistics, part 2. van Nostrand,  1962.  2008.  math/0607648, 2006.  Lek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach. arXiv preprint  David G Luenberger and Yinyu Ye. Linear and nonlinear programming, volume 116. Springer,  Albert CJ Luo. Regularity and complexity in dynamical systems. Springer, 2012.  Shoji Makino, Te-Won Lee, and Hiroshi Sawada. Blind speech separation. Springer, 2007.  Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.  Advances in neural information processing systems, 2:849-856, 2002.  Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU  signatures. J. Cryptology, 22(2):139-160, 2009.  Liqun Qi. Eigenvalues of a real supersymmetric tensor. Journal of Symbolic Computation, 40(6):  1302-1324, 2005.  Walter Rudin. Real and complex analysis (3rd). New York: McGraw-Hill Inc, 1986.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  Santosh S. Vempala and Ying Xiao. Structure from local optima: Learning subspace juntas via higher order PCA. CoRR, abs/1108.3329, 2011. URL http://arxiv.org/abs/1108.3329.  Ricardo Vig\u00b4ario, Jaakko Sarela, V Jousmiki, Matti Hamalainen, and Erkki Oja.  Independent component approach to the analysis of EEG and MEG recordings. Biomedical Engineering, IEEE Transactions on, 47(5):589-593, 2000.  James R Voss, Luis Rademacher, and Mikhail Belkin. Fast algorithms for gaussian noise invariant independent component analysis. In Advances in Neural Information Processing Systems, pages 2544-2552, 2013. BASIS LEARNING AS AN ALGORITHMIC PRIMITIVE  Marcus Weber, Wasinee Rungsarityotin, and Alexander Schliep. Perron cluster analysis and its connection to graph partitioning for noisy data. Konrad-Zuse-Zentrum f\u00a8ur Informationstechnik Berlin, 2004.  Tianwen Wei. A study of the fixed points and spurious solutions of the de\ufb02ation-based fastica algorithm. Neural Computing and Applications, pages 1-12, 2015. ISSN 0941-0643. doi: 10.1007/ s00521-015-2033-6. URL http://dx.doi.org/10.1007/s00521-015-2033-6.  Stella X. Yu and Jianbo Shi. Multiclass spectral clustering. In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on (ICCV), pages 313-319. IEEE Computer Society, 2003. ISBN 0-7695-1950-4.  Vicente Zarzoso and Pierre Comon. Robust independent component analysis by iterative maximiza- tion of the kurtosis contrast with algebraic optimal step size. Neural Networks, IEEE Transactions on, 21(2):248-261, 2010.  "}, "Aggregation of supports along the Lasso path": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Aggregation of supports along the Lasso path", "abstract": "In linear regression with fixed design, we propose two procedures that aggregate a data-driven collection of supports. The collection is a subset of the 2^p possible supports and both its cardinality and its elements can depend on the data. The procedures satisfy oracle inequalities with no assumption on the design matrix. Then we use these procedures to aggregate the supports that appear on the regularization path of the Lasso in order to construct an estimator that mimics the best Lasso estimator. If the restricted eigenvalue condition on the design matrix is satisfied, then this estimator achieves optimal prediction bounds. Finally, we discuss the computational cost of these procedures.", "pdf_url": "http://proceedings.mlr.press/v49/bellec16.pdf", "keywords": ["Linear regression", "prediction loss", "aggregation", "Lasso path", "oracle inequalities"], "reference": "Pierre C. Bellec. Aggregation of supports along the lasso path. arXiv:1602.03427, 2016.  URL http://arxiv.org/abs/1602.03427.  2   Bellec  where C > 0 is a numerical constant and |\u03b2|0 is the number of nonzero coe\ufb03cients of any \u03b2 \u2208 Rp. Keywords: Linear regression, prediction loss, aggregation, Lasso path, oracle inequalities.  References  Pierre C. Bellec. Aggregation of supports along the lasso path. arXiv:1602.03427, 2016.  URL http://arxiv.org/abs/1602.03427. "}, "Dropping Convexity for Faster Semi-definite Optimization": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Dropping Convexity for Faster Semi-definite Optimization", "abstract": "We study the minimization of a convex function f(X) over the set of n \\times n positive semi-definite matrices, but when the problem is recast as \\min_U g(U) :=  f(UU^\u22a4), with U \u2208\\mathbbR^n \\times r and r \u2264n. We study the performance of gradient descent on g\u2014which we refer to as Factored Gradient Descent (\\textscFgd)\u2014under standard assumptions on the \\em original function f. We provide a rule for selecting the step size and, with this choice, show that the \\emphlocal convergence rate of \\textscFgd mirrors that of standard gradient descent on the original f: \\emphi.e., after k steps, the error is O(1/k) for smooth f, and exponentially small in k when f is (restricted) strongly convex. In addition, we provide a procedure to initialize \\textscFgd for (restricted) strongly convex objectives and when one only has access to f via a first-order oracle; for several problem instances, such proper initialization leads to \\emphglobal convergence guarantees. \\textscFgd and similar procedures are widely used in practice for problems that can be posed as matrix factorization. To the best of our knowledge, this is the first paper to provide precise convergence rate guarantees for general convex functions under standard convex assumptions.", "pdf_url": "http://proceedings.mlr.press/v49/bhojanapalli16.pdf", "keywords": ["Non-convex analysis and optimization", "semi-definite matrix", "rank minimization"], "reference": "Scott Aaronson. The learnability of quantum states. In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 463, pages 3089-3114. The Royal Society, 2007.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227- 236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis.  Sparse PCA via bipartite matchings. arXiv preprint arXiv:1508.00625, 2015.  Stephen Becker, Volkan Cevher, and Anastasios Kyrillidis. Randomized low-memory singular value projection. In 10th International Conference on Sampling Theory and Applications (Sampta), 2013.  Rajendra Bhatia. Perturbation bounds for matrix eigenvalues, volume 53. SIAM, 1987.  Nicolas Boumal. Optimization and estimation on manifolds. PhD thesis, UC Louvain, Belgium,  2014.  20   BHOJANAPALLI KYRILLIDIS SANGHAVI  a non-trivial step size selection results in linear convergence when f is smooth and (restricted) strongly convex, even though the problem is now non-convex. In the case where f is only smooth, only sublinear rate is guaranteed. In addition, we present initialization schemes that use only first order information and guarantee to find a starting point with small relative distance from optimum. There are many possible directions for future work, extending the idea of using non-convex formulation for semi-definite optimization. Showing convergence under weaker initialization con- dition or without any initialization requirement is definitely of great interest. Another interesting direction is to improve the convergence rates presented in this work, by using acceleration tech- niques and thus, extend ideas used in the case of convex gradient descent Nesterov (2004). Finally, it would be valuable to see how the techniques presented in this paper can be generalized to other standard algorithms like stochastic gradient descent and coordinate descent.  Furthermore, we identify applications, such as sparse PCA Vu et al. (2013); Asteris et al. (2015), that require non-smooth constraints on the factors U . That being said, an extension of this work to proximal techniques for the non-convex case is a very interesting future research direction.  References  Scott Aaronson. The learnability of quantum states. In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 463, pages 3089-3114. The Royal Society, 2007.  Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pages 37-45, 2010.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5(1):13-51, 1995.  Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 227- 236. ACM, 2007.  Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semidefinite pro- gramming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339-348. IEEE, 2005.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis.  Sparse PCA via bipartite matchings. arXiv preprint arXiv:1508.00625, 2015.  Stephen Becker, Volkan Cevher, and Anastasios Kyrillidis. Randomized low-memory singular value projection. In 10th International Conference on Sampling Theory and Applications (Sampta), 2013.  Rajendra Bhatia. Perturbation bounds for matrix eigenvalues, volume 53. SIAM, 1987.  Nicolas Boumal. Optimization and estimation on manifolds. PhD thesis, UC Louvain, Belgium,  2014. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Nicolas Boumal. A riemannian low-rank method for optimization over semidefinite matrices with  block-diagonal constraints. arXiv preprint arXiv:1506.00575, 2015.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.  S\u00b4ebastien Bubeck.  Theory of convex optimization for machine learning.  arXiv preprint  arXiv:1405.4980, 2014.  Samuel Burer. Semidefinite programming in the space of partial positive semidefinite matrices.  SIAM Journal on Optimization, 14(1):139-172, 2003.  Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefi- nite programs via low-rank factorization. Mathematical Programming, 95(2):329-357, 2003.  Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite  programming. Mathematical Programming, 103(3):427-444, 2005.  Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. Information Theory, IEEE Transactions on, 57 (4):2342-2359, 2011.  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval  via matrix completion. SIAM Review, 57(2):225-251, 2015a.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. Information Theory, IEEE Transactions on, 61(4):1985-2007, 2015b.  Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:  General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.  Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, and Rachel Ward. Coherent matrix comple- tion. In Proceedings of The 31st International Conference on Machine Learning, pages 674-682, 2014.  Yuxin Chen and Sujay Sanghavi. A general framework for high-dimensional estimation in the In Communication, Control, and Computing (Allerton), 2010 48th  presence of incoherence. Annual Allerton Conference on, pages 1570-1576. IEEE, 2010.  Kenneth L Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM  Transactions on Algorithms (TALG), 6(4):63, 2010.  Alexandre d\u2019Aspremont, Laurent El Ghaoui, Michael I Jordan, and Gert RG Lanckriet. A direct for- mulation for sparse PCA using semidefinite programming. SIAM review, 49(3):434-448, 2007.  Quoc Tran Dinh, Anastasios Kyrillidis, and Volkan Cevher. Composite self-concordant minimiza-  tion. Journal of Machine Learning Research, 16:371-416, 2015. BHOJANAPALLI KYRILLIDIS SANGHAVI  Alan Edelman, Tom\u00b4as A Arias, and Steven T Smith. The geometry of algorithms with orthogonality  constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303-353, 1998.  Mituhiro Fukuda, Masakazu Kojima, Kazuo Murota, and Kazuhide Nakata. Exploiting sparsity in semidefinite programming via matrix completion I: General framework. SIAM Journal on Optimization, 11(3):647-674, 2001.  Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42 (6):1115-1145, 1995.  Elad Hazan. Sparse approximate solutions to semidefinite programs. In LATIN 2008: Theoretical  Informatics, pages 306-316. Springer, 2008.  Christoph Helmberg and Franz Rendl. A spectral bundle method for semidefinite programming.  SIAM Journal on Optimization, 10(3):673-696, 2000.  Christoph Helmberg, Michael L Overton, and Franz Rendl. The spectral bundle method with  second-order information. Optimization Methods and Software, 29(4):855-876, 2014.  Roger A Horn and Charles R Johnson. Topics in matrix analysis. Cambridge University Presss,  Cambridge, 37:39, 1991.  Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and M\u00b4aty\u00b4as A Sustik. Sparse inverse covariance matrix estimation using quadratic approximation. In Advances in Neural Information Processing Systems, pages 2330-2338, 2011.  Martin Jaggi. Convex optimization without projection steps. arXiv preprint arXiv:1108.1170, 2011.  Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value  projection. In Advances in Neural Information Processing Systems, pages 937-945, 2010.  Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternat- ing minimization. In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 665-674. ACM, 2013.  Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Computing matrix squareroot via  non convex local search. arXiv preprint arXiv:1507.05854, 2015.  Adel Javanmard and Andrea Montanari. Localization from incomplete noisy distance measure-  ments. Foundations of Computational Mathematics, 13(3):297-345, 2013.  Kaifeng Jiang, Defeng Sun, and Kim-Chuan Toh. An inexact accelerated proximal gradient method for large scale linearly constrained convex SDP. SIAM Journal on Optimization, 22(3):1042- 1064, 2012.  Michel Journ\u00b4ee, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327-2351, 2010.  Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the sixteenth annual ACM symposium on Theory of computing, pages 302-311. ACM, 1984. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few  entries. Information Theory, IEEE Transactions on, 56(6):2980-2998, 2010.  Philip Klein and Hsueh-I Lu. Efficient approximation algorithms for semidefinite programs arising from MAX CUT and COLORING. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pages 338-347. ACM, 1996.  Anastasios Kyrillidis and Volkan Cevher. Matrix recipes for hard thresholding methods. Journal of  mathematical imaging and vision, 48(2):235-265, 2014.  Anastasios Kyrillidis, Rabeeh Karimi, Quoc Tran Dinh, and Volkan Cevher. Scalable sparse co- variance estimation via self-concordance. In Twenty-Eighth AAAI Conference on Artificial Intel- ligence, 2014.  Soeren Laue. A hybrid algorithm for convex semidefinite optimization. In Proceedings of the 29th  International Conference on Machine Learning (ICML-12), pages 177-184, 2012.  Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Ad-  vances in neural information processing systems, pages 556-562, 2001.  Jason Lee, Yuekai Sun, and Michael Saunders. Proximal newton-type methods for convex opti-  mization. In Advances in Neural Information Processing Systems, pages 836-844, 2012.  Yi-Kai Liu. Universal low-rank matrix recovery from Pauli measurements. In Advances in Neural  Information Processing Systems, pages 1638-1646, 2011.  Leon Mirsky. A trace inequality of John von Neumann. Monatshefte f\u00a8ur Mathematik, 79(4):303-  306, 1975.  Bamdev Mishra, Gilles Meyer, and Rodolphe Sepulchre. Low-rank optimization for distance matrix completion. In Decision and control and European control conference (CDC-ECC), 2011 50th IEEE conference on, pages 4455-4460. IEEE, 2011.  Renato DC Monteiro. First-and second-order methods for semidefinite programming. Mathematical  Programming, 97(1-2):209-244, 2003.  Kazuhide Nakata, Katsuki Fujisawa, Mituhiro Fukuda, Masakazu Kojima, and Kazuo Murota. Ex- ploiting sparsity in semidefinite programming via matrix completion II: Implementation and nu- merical results. Mathematical Programming, 95(2):303-327, 2003.  Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix com- pletion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1):1665- 1697, 2012.  Yurii Nesterov.  Introductory lectures on convex optimization, volume 87. Springer Science &  Business Media, 2004.  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Mathemati-  cal Programming, 110(2):245-259, 2007. BHOJANAPALLI KYRILLIDIS SANGHAVI  Yurii Nesterov and Arkadi Nemirovski. A general approach to polynomial-time algorithms de- sign for convex programming. Report, Central Economical and Mathematical Institute, USSR Academy of Sciences, Moscow, 1988.  Yurii Nesterov and Arkadi Nemirovski. Self-concordant functions and polynomial-time methods in convex programming. USSR Academy of Sciences, Central Economic & Mathematic Institute, 1989.  Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-  tion. In Advances in Neural Information Processing Systems, pages 2796-2804, 2013.  Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear  matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  Christopher D Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2332-2341, 2015.  Shai Shalev-shwartz, Alon Gonen, and Ohad Shamir. Large-scale convex minimization with a low-rank constraint. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 329-336, 2011.  Suvrit Sra. On the matrix square root via geometric optimization. arXiv preprint arXiv:1507.08366,  2015.  arXiv:1602.06664, 2016.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. arXiv preprint  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv  preprint arXiv:1411.8003, 2014.  Kim-Chuan Toh. Solving large scale semidefinite programs via an iterative solver on the augmented  systems. SIAM Journal on Optimization, 14(3):670-698, 2004.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear  matrix equations via Procrustes \ufb02ow. arXiv preprint arXiv:1507.03566, 2015.  Andre Uschmajew and Bart Vandereycken. Greedy rank updates combined with riemannian descent methods for low-rank optimization. In 12th International Conference on Sampling Theory and Applications (Sampta), 2015.  Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In Founda- tions of Computer Science, 1989., 30th Annual Symposium on, pages 338-343. IEEE, 1989.  Vincent Q Vu, Jing Lei, et al. Minimax sparse principal subspace estimation in high dimensions.  The Annals of Statistics, 41(6):2905-2947, 2013.  Andrew E Waters, Aswin C Sankaranarayanan, and Richard Baraniuk. Sparcs: Recovering low- rank and sparse matrices from compressive measurements. In Advances in neural information processing systems, pages 1089-1097, 2011. DROPPING CONVEXITY FOR FASTER SEMI-DEFINITE OPTIMIZATION  Zaiwen Wen, Donald Goldfarb, and Wotao Yin. Alternating direction augmented Lagrangian meth- ods for semidefinite programming. Mathematical Programming Computation, 2(3-4):203-230, 2010.  Chris D White, Sujay Sanghavi, and Rachel Ward. The local convexity of solving systems of  quadratic equations. arXiv preprint arXiv:1506.07868, 2015.  Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of The 31st International Conference on Machine Learning, pages 593-601, 2014.  Dejiao Zhang and Laura Balzano. Global convergence of a grassmannian gradient descent algorithm  for subspace estimation. arXiv preprint arXiv:1506.07405, 2015.  Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix  estimation. In Advances in Neural Information Processing Systems, pages 559-567, 2015.  Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank mini- arXiv preprint  mization and semidefinite programming from random linear measurements. arXiv:1506.06081, 2015.  "}, "Multi-scale exploration of convex functions and bandit convex optimization": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Multi-scale exploration of convex functions and bandit convex optimization", "abstract": "We construct a new map from a convex function to a distribution on its domain, with the property that this distribution is a multi-scale exploration of the function. We use this map to solve a decade-old open problem in adversarial bandit convex optimization by showing that the minimax regret for this problem is \\tildeO(\\mathrmpoly(n) \\sqrtT), where n is the dimension and T the number of rounds. This bound is obtained by studying the dual Bayesian maximin regret via the information ratio analysis of Russo and Van Roy, and then using the multi-scale exploration to construct a new algorithm for the Bayesian convex bandit problem.", "pdf_url": "http://proceedings.mlr.press/v49/bubeck16.pdf", "keywords": [], "reference": "A. Agarwal, D.P. Foster, D. Hsu, S.M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex optimiza-  tion. Arxiv preprint arXiv:1507.06580, 2015.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:  T regret in one  dimension. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), 2015.  \u221a  A. Flaxman, A. Kalai, and B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural  Information Processing Systems (NIPS), 2004.  D. Russo and B. Van Roy. An information-theoretic analysis of thompson sampling. arXiv preprint  arXiv:1403.5341, 2014a.  arXiv:1403.5556, 2014b.  D. Russo and B. Van Roy. Learning to optimize via information directed sampling. arXiv preprint  7   MULTI-SCALE EXPLORATION FOR CONVEX BANDITS  By definition of the measure \u00b5, we have that whenever k \u2264 N , one has  \u00b5 (A) \u2265  1 N + 2  \u2265  1 8 log(1 + 1/\u03b5)  .  Finally, if k > N , it means that |\u03b1 \u2212 x0| < 2\u2212N < \u03b5 implies that g(x0) \u2264 \u2212\u03b5/2 which in turn gives |f (x0) \u2212 g(x0)| \u2265 1 x0 \u2208 A and thus \u00b5(A) \u2265 \u00b5({x0}) = 14 . Since the function g is 1-Lipschitz, this 8 max(\u03b5, f (x0)). Consequently,  N +2 \u2265  8 log(1+1/\u03b5) . The proof is complete.  References  A. Agarwal, D.P. Foster, D. Hsu, S.M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex optimiza-  tion. Arxiv preprint arXiv:1507.06580, 2015.  S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:  T regret in one  dimension. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), 2015.  \u221a  A. Flaxman, A. Kalai, and B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2005.  R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural  Information Processing Systems (NIPS), 2004.  D. Russo and B. Van Roy. An information-theoretic analysis of thompson sampling. arXiv preprint  arXiv:1403.5341, 2014a.  arXiv:1403.5556, 2014b.  D. Russo and B. Van Roy. Learning to optimize via information directed sampling. arXiv preprint "}, "Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem", "abstract": "We consider the problem of \\textitbest arm identification with a \\textitfixed budget T, in the K-armed stochastic bandit setting, with arms distribution defined on [0,1]. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity H, will misidentify the best arm with probability lower bounded by $\\exp\\Big(-\\frac{T}\\log(K)H\\Big)$, where $H$ is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by $\\exp(-T/H)$. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem.", "pdf_url": "http://proceedings.mlr.press/v49/carpentier16.pdf", "keywords": ["Bandit Theory", "Best Arm Identification", "Simple Regret", "Fixed Confidence Setting", "Lower Bounds1"], "reference": "bandits.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits. In COLT  - 23rd Conference on Learning Theory - 2010, 2010.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob-  lems. In Algorithmic Learning Theory, pages 23-37. Springer, 2009.  Florentina Bunea, Alexandre Tsybakov, Marten Wegkamp, et al. Sparsity oracle inequalities for the  lasso. Electronic Journal of Statistics, 1:169-194, 2007.  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  arXiv:1511.03774, 2015.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- In Advances in Neural Information Processing Systems, pages  ration of multi-armed bandits. 379-387, 2014.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Pro- cessing Systems, pages 3212-3220, 2012.  Kevin Jamieson and Robert Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In Information Sciences and Systems (CISS), 2014 48th Annual Conference on, pages 1-6. IEEE, 2014.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On finding the largest  mean among many. arXiv preprint arXiv:1306.3917, 2013a.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. arXiv preprint arXiv:1312.7308, 2013b.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013.  14   CARPENTIER LOCATELLI  Acknowledgement This work is supported by the DFG\u2019s Emmy Noether grant MuSyAD (CA 1488/1-1).  References  bandits.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits. In COLT  - 23rd Conference on Learning Theory - 2010, 2010.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  S\u00b4ebastien Bubeck, R\u00b4emi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits prob-  lems. In Algorithmic Learning Theory, pages 23-37. Springer, 2009.  Florentina Bunea, Alexandre Tsybakov, Marten Wegkamp, et al. Sparsity oracle inequalities for the  lasso. Electronic Journal of Statistics, 1:169-194, 2007.  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  arXiv:1511.03774, 2015.  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- In Advances in Neural Information Processing Systems, pages  ration of multi-armed bandits. 379-387, 2014.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Pro- cessing Systems, pages 3212-3220, 2012.  Kevin Jamieson and Robert Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In Information Sciences and Systems (CISS), 2014 48th Annual Conference on, pages 1-6. IEEE, 2014.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. On finding the largest  mean among many. arXiv preprint arXiv:1306.3917, 2013a.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. arXiv preprint arXiv:1312.7308, 2013b.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013. FIXED BUDGET BEST ARM IDENTIFICATION  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. arXiv preprint arXiv:1407.4443, 2014.  Oleg V Lepski and VG Spokoiny. Optimal pointwise adaptive methods in nonparametric estimation.  The Annals of Statistics, pages 2512-2546, 1997.  S Mannor and J N Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit  Problem. Journal of Machine Learning Research, 5:623-648, 2004.  Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identification with applications In Proceedings of the 31st International Conference on Machine Learning  to crowdsourcing. (ICML-14), pages 217-225, 2014. "}, "Delay and Cooperation in Nonstochastic Bandits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Delay and Cooperation in Nonstochastic Bandits", "abstract": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order \\sqrt\\left(d+1 + \\fracKN\\alpha_\u2264d\\right)(T\\ln K), where \\alpha_\u2264d is the independence number of the d-th power of the communication graph G. We then show that for any connected graph, for d=\\sqrtK the regret bound is K^1/4\\sqrtT, strictly better than the minimax regret \\sqrtKT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret \\sqrtT\\ln K when G is dense. When G has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.", "pdf_url": "http://proceedings.mlr.press/v49/cesa-bianchi16.pdf", "keywords": [], "reference": "Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Informa- tion Processing Systems 24, pages 873-881. Curran Associates, Inc., 2011.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Baruch Awerbuch and Robert Kleinberg. Competitive collaborative learning. Journal of Computer  and System Sciences, 74(8):1271-1288, 2008.  Samuel Barrett and Peter Stone. Ad hoc teamwork modeled with multi-armed bandits: An exten- sion to discounted infinite rewards. In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents, pages 9-14, 2011.  Nicolo\u2019 Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and coopera-  tion in nonstochastic bandits. arXiv preprint, arXiv:1602.04741v2, 2016.  15   DELAY AND COOPERATION IN NONSTOCHASTIC BANDITS  at distance s from any given agent v are indeed used at time t by agent v i.e., as soon as these losses become available to v. The resulting regret bounds mix delays and independence numbers of graphs at different levels of delay. (Details will be given in the full version of this paper.) More ambitiously, it is natural to think of ways to adaptively tune our algorithms so as to automatically determine the best delay parameter d. For instance, disregarding message complexity, is there a way for each agent to adaptively tune d locally so to minimize the bound in Theorem 4?  4. Our messages mt(v) contain both action/loss information and distribution information. Is it possible to drop the distribution information and still achieve average welfare regret bounds similar to those in Theorems 3 and 4?  5. Even for the single-agent setting, we do not know whether regret bounds of the form (cid:112)(D + T ) ln K, where D is the total delay experienced over the T rounds, could be proven \u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the full- information setting. In general, the study of learning on a communication network with time-varying delays, and its impact on the regret rates, is a topic which is certainly worth of attention.  Acknowledgments  We thank the anonymous reviewers for their careful reading, and for their thoughtful suggestions that greatly improved the presentation of this paper. Yishay Mansour is supported in part by the Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF) and by a grant from the Len Blavatnik and the Blavatnik Family Foundation.  References  Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Informa- tion Processing Systems 24, pages 873-881. Curran Associates, Inc., 2011.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-  armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.  Baruch Awerbuch and Robert Kleinberg. Competitive collaborative learning. Journal of Computer  and System Sciences, 74(8):1271-1288, 2008.  Samuel Barrett and Peter Stone. Ad hoc teamwork modeled with multi-armed bandits: An exten- sion to discounted infinite rewards. In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents, pages 9-14, 2011.  Nicolo\u2019 Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and coopera-  tion in nonstochastic bandits. arXiv preprint, arXiv:1602.04741v2, 2016. CESA-BIANCHI GENTILE MANSOUR MINORA  John Duchi, Michael I Jordan, and Brendan McMahan. Estimation, optimization, and parallelism when data is sparse. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2832-2840. Curran Associates, Inc., 2013.  John C Duchi, Sorathan Chaturapruek, and Christopher R\u00b4e. Asynchronous stochastic convex opti-  mization. arXiv preprint arXiv:1508.00882, 2015.  Miroslav Dud\u00b4\u0131k, Daniel J. Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, pages 169-178, 2011.  P. Firby and J. Haviland. Independence and average distance in graphs. Discrete Applied Mathe-  matics, 75:27-37, 1997.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online learning under delayed feedback. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1453-1461, 2013.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms. In Proceedings of the Thirtieth AAAI Confer- ence on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 1744-1750, 2016.  Soummya Kar, H Vincent Poor, and Shuguang Cui. Bandit problems in networks: Asymptotically In 50th IEEE Conference on Decision and Control and  efficient distributed allocation rules. European Control Conference (CDC-ECC), pages 1771-1778. IEEE, 2011.  Robert Kleinberg, Georgios Piliouras, and \u00b4Eva Tardos. Multiplicative updates outperform generic no-regret learning in congestion games. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 533-542. ACM, 2009.  Tom\u00b4a\u02c7s Koc\u00b4ak, Gergely Neu, Michal Valko, and Remi Munos. Efficient learning by implicit explo- ration in bandit problems with side observations. In Advances in Neural Information Processing Systems 27, pages 613-621. 2014.  Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. On distributed cooperative  decision-making in multiarmed bandits. arXiv preprint arXiv:1512.06888, 2015.  Mu Li, David G. Andersen, and Alexander Smola. Distributed delayed proximal gradient methods.  In NIPS Workshop on Optimization for Machine Learning, 2013.  Nathan Linial. Locality in distributed graph algorithms. SIAM J. Comput., 21(1):193-201, 1992.  Ji Liu, Stephen J Wright, Christopher R\u00b4e, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. The Journal of Machine Learning Research, 16 (1):285-322, 2015. DELAY AND COOPERATION IN NONSTOCHASTIC BANDITS  Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint arXiv:1507.06970, 2015.  Brendan McMahan and Matthew Streeter. Delay-tolerant algorithms for asynchronous distributed online learning. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2915-2923. Curran Asso- ciates, Inc., 2014.  Chris Mesterharm. On-line learning with delayed label feedback. In Algorithmic Learning Theory,  pages 399-413. Springer, 2005.  Rutgers University, 2007.  Chris Mesterharm.  Improving Online Learning. PhD thesis, Department of Computer Science,  Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.  In Advances in Neural Information Processing Systems 28 (NIPS), 2015.  Gergely Neu, Andras Antos, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online Markov decision pro- cesses under bandit feedback. In Advances in Neural Information Processing Systems 23, pages 1804-1812. Curran Associates, Inc., 2010.  Gergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos. Online markov decision pro- cesses under bandit feedback. Automatic Control, IEEE Transactions on, 59(3):676-691, 2014.  Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Parallel correlation clustering on big graphs. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 82-90. Curran Associates, Inc., 2015.  Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. In C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1270-1278. Curran Associates, Inc., 2015.  Jonathan Rosenski, Ohad Shamir, and Liran Szlak. Multi-player bandits - a musical chairs ap-  proach. CoRR, abs/1512.02866, 2015.  Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited advice and multiarmed bandits with paid observations. In Proceedings of The 31st International Conference on Machine Learning, pages 280-287, 2014.  Ruben Stranders, Long Tran-Thanh, Francesco M Delle Fave, Alex Rogers, and Nicholas R Jen- nings. Dcops and bandits: Exploration and exploitation in decentralised coordination. In Proceed- ings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1, pages 289-296. International Foundation for Autonomous Agents and Multiagent Systems, 2012.  Jukka Suomela. Survey of local algorithms. ACM Computing Surveys, 45(2):24, 2013. CESA-BIANCHI GENTILE MANSOUR MINORA  Balazs Szorenyi, R\u00b4obert Busa-Fekete, Istv\u00b4an Heged\u00a8us, R\u00b4obert Orm\u00b4andi, M\u00b4ark Jelasity, and Bal\u00b4azs K\u00b4egl. Gossip-based distributed stochastic bandit algorithms. In 30th International Conference on Machine Learning (ICML 2013), volume 28, pages 19-27. ACM Press, 2013.  Cem Tekin and Mihaela van der Schaar. Distributed online learning via cooperative contextual  bandits. IEEE Transactions on Signal Processing, 63(14):3700-3714, 2015.  Cem Tekin, Simpson Z. Zhang, and Mihaela van der Schaar. Distributed online learning in social  recommender systems. J. Sel. Topics Signal Processing, 8(4):638-652, 2014.  Klaus Wehmuth and Artur Ziviani. Daccer: Distributed assessment of the closeness centrality  ranking in complex networks. Computer Networks, 57(13):2536-2548, 2013.  Marcelo J Weinberger and Erik Ordentlich. On delayed prediction of individual sequences. IEEE  Transactions on Information Theory, 48(7):1959-1976, 2002.  Martin Zinkevich, John Langford, and Alex J. Smola. Slow learners are fast. In Y. Bengio, D. Schu- urmans, J.D. Lafferty, C.K.I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2331-2339. Curran Associates, Inc., 2009. "}, "On the Approximability of Sparse PCA": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "On the Approximability of Sparse PCA", "abstract": "It is well known that Sparse PCA (Sparse Principal Component Analysis) is NP-hard to solve exactly on worst-case instances. What is the complexity of solving Sparse PCA approximately? Our contributions include: \\beginenumerate \\item a simple and efficient algorithm that achieves an n^-1/3-approximation; \\item NP-hardness of approximation to within (1-\\varepsilon), for some small constant \\varepsilon > 0; \\item SSE-hardness of approximation to within \\em any constant factor; and \\item an \\exp\\exp\\left(\u03a9\\left(\\sqrt\\log \\log n\\right)\\right) (\u201cquasi-quasi-polynomial\u201d) gap for the standard semidefinite program. \\endenumerate", "pdf_url": "http://proceedings.mlr.press/v49/chan16.pdf", "keywords": ["Sparse PCA", "hardness of approximation"], "reference": "Noga Alon, Tali Kaufman, Michael Krivelevich, Simon Litsyn, and Dana Ron. Testing Reed-Muller codes. IEEE Transactions on Information Theory, 51(11):4032-4039, 2005. doi: 10.1109/TIT. 2005.856958. URL http://dx.doi.org/10.1109/TIT.2005.856958.  Noga Alon, Sanjeev Arora, Rajsekar Manokaran, Dana Moshkovitz, and Omri Weinstein. Inapprox- imability of densest \u03ba-subgraph from average case hardness. Unpublished manuscript, 2011.  Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 675-684. ACM, 2013.  Arash Amini and Martin Wainwright. High-dimensional analysis of semidefinite relaxations for sparse principal components. In Information Theory, 2008. ISIT 2008. IEEE International Sym- posium on, pages 2454-2458. IEEE, 2008.  Ei Ando, Toshio Nakata, and Masafumi Yamashita. Approximating the longest path length of a stochastic DAG by a normal distribution in linear time. Journal of Discrete Algorithms, 7(4):420 - 438, 2009. ISSN 1570-8667. doi: http://dx.doi.org/10.1016/j.jda.2009.01.001. URL http: //www.sciencedirect.com/science/article/pii/S1570866709000033.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games and related problems. In Symposium on Foundations of Computer Science, FOCS, pages 563-572, Washington, DC, USA, 2010. IEEE. ISBN 978-0-7695-4244-7.  Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse PCA with provable guarantees. In Proceedings of the 31st International Conference on Machine Learn- ing (ICML-14), pages 1728-1736, 2014.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G. Dimakis.  Sparse pca via bipartite matchings. preprint, 2015.  Boaz Barak, Fernando Guadalupe dos Santos Lins Brand\u02dcao, Aram Wettroth Harrow, Jonathan Kel- ner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their appli- cations. In Symposium on Theory of Computing, STOC, pages 307-326, New York, NY, USA, 2012a. ACM. ISBN 978-1-4503-1245-5.  Boaz Barak, Parikshit Gopalan, Johan H\u02daastad, Raghu Meka, Prasad Raghavendra, and David In FOCS, pages 370-379, 2012b. URL http: Steurer. Making the long code shorter. //dx.doi.org/10.1109/FOCS.2012.83. Lemma numbers refer to the full version avail- able at arXiv:1111.0405 [cs.CC].  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. Journal of Machine Learning Research (JMLR), 30:1046-1066 (electronic), 2013a.  20   CHAN PAPAILLIOPOULOS RUBINSTEIN  We would like to thank Robert Krauthgamer, Tsz Chiu Kwok, Lap Chi Lau, Prasad Raghavendra, Aaron Schild, and anonymous reviewers for numerous suggestions and helpful discussions.  Acknowledgments  References  Noga Alon, Tali Kaufman, Michael Krivelevich, Simon Litsyn, and Dana Ron. Testing Reed-Muller codes. IEEE Transactions on Information Theory, 51(11):4032-4039, 2005. doi: 10.1109/TIT. 2005.856958. URL http://dx.doi.org/10.1109/TIT.2005.856958.  Noga Alon, Sanjeev Arora, Rajsekar Manokaran, Dana Moshkovitz, and Omri Weinstein. Inapprox- imability of densest \u03ba-subgraph from average case hardness. Unpublished manuscript, 2011.  Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 675-684. ACM, 2013.  Arash Amini and Martin Wainwright. High-dimensional analysis of semidefinite relaxations for sparse principal components. In Information Theory, 2008. ISIT 2008. IEEE International Sym- posium on, pages 2454-2458. IEEE, 2008.  Ei Ando, Toshio Nakata, and Masafumi Yamashita. Approximating the longest path length of a stochastic DAG by a normal distribution in linear time. Journal of Discrete Algorithms, 7(4):420 - 438, 2009. ISSN 1570-8667. doi: http://dx.doi.org/10.1016/j.jda.2009.01.001. URL http: //www.sciencedirect.com/science/article/pii/S1570866709000033.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games and related problems. In Symposium on Foundations of Computer Science, FOCS, pages 563-572, Washington, DC, USA, 2010. IEEE. ISBN 978-0-7695-4244-7.  Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse PCA with provable guarantees. In Proceedings of the 31st International Conference on Machine Learn- ing (ICML-14), pages 1728-1736, 2014.  Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G. Dimakis.  Sparse pca via bipartite matchings. preprint, 2015.  Boaz Barak, Fernando Guadalupe dos Santos Lins Brand\u02dcao, Aram Wettroth Harrow, Jonathan Kel- ner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their appli- cations. In Symposium on Theory of Computing, STOC, pages 307-326, New York, NY, USA, 2012a. ACM. ISBN 978-1-4503-1245-5.  Boaz Barak, Parikshit Gopalan, Johan H\u02daastad, Raghu Meka, Prasad Raghavendra, and David In FOCS, pages 370-379, 2012b. URL http: Steurer. Making the long code shorter. //dx.doi.org/10.1109/FOCS.2012.83. Lemma numbers refer to the full version avail- able at arXiv:1111.0405 [cs.CC].  Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. Journal of Machine Learning Research (JMLR), 30:1046-1066 (electronic), 2013a. ON THE APPROXIMABILITY OF SPARSE PCA  Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high  dimension. Ann. Statist., 41(1):1780-1815, 2013b.  Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, and Aravindan Vijayaraghavan. Detecting high log-densities: an O(n1/4) approximation for densest k-subgraph. In Proceedings of the 42nd ACM Symposium on Theory of Computing, (STOC) 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 201-210, 2010. doi: 10.1145/1806689.1806718. URL http:// doi.acm.org/10.1145/1806689.1806718.  Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan, Venkatesan Guruswami, and Yuan Zhou. Polynomial integrality gaps for strong SDP relaxations of densest k-subgraph. In Proceed- ings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, January 17-19, 2012, pages 388-405, 2012. URL http://portal.acm. org/citation.cfm?id=2095150&CFID=63838676&CFTOKEN=79617016.  Arnab Bhattacharyya, Swastik Kopparty, Grant Schoenebeck, Madhu Sudan, and David Zucker- In FOCS, pages 488-497, 2010. URL http:  man. Optimal testing of Reed-Muller codes. //dx.doi.org/10.1109/FOCS.2010.54.  Mark Braverman, Young Kun-Ko, Aviad Rubinstein, and Omri Weinstein. ETH Hardness for Densest-k-Subgraph with Perfect Completeness. volume abs/1504.08352, 2015a. URL http: //arxiv.org/abs/1504.08352.  Mark Braverman, Young Kun-Ko, and Omri Weinstein. Approximating the best nash equilib- rium in no(log n)-time breaks the exponential time hypothesis. In Proceedings of the Twenty- Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 970-982, 2015b. doi: 10.1137/1.9781611973730.66. URL http://dx.doi.org/10.1137/1.9781611973730.66.  J. Cadima and I.T. Jolliffe. Loading and correlations in the interpretation of principle compenents.  Journal of Applied Statistics, 22(2):203-214, 1995.  T. Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: Optimal rates and adaptive estimation. The Annals of Statistics, 41(6):3074-3110, 12 2013a. doi: 10.1214/13-AOS1178. URL http: //dx.doi.org/10.1214/13-AOS1178.  Tony Cai, Zongming Ma, and Yihong Wu. Optimal estimation and rank detection for sparse spiked  covariance matrices. arXiv preprint arXiv:1305.3235, 2013b.  Siu On Chan, Tsz Chiu Kwok, and Lap Chi Lau. Random walks and evolving sets: Faster conver-  gences and limitations. manuscript, 2015.  A. d\u2019Aspremont, L. El Ghaoui, M.I. Jordan, and G.R.G. Lanckriet. A direct formulation for sparse  PCA using semidefinite programming. SIAM review, 49(3):434-448, 2007a.  Alexandre d\u2019Aspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A direct formulation for sparse PCA using semidefinite programming. SIAM Rev., 49(3):434-448, July 2007b. ISSN 0036-1445. URL http://dx.doi.org/10.1137/050645506. CHAN PAPAILLIOPOULOS RUBINSTEIN  Alexandre d\u2019Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse prin- cipal component analysis. The Journal of Machine Learning Research, 9:1269-1294, 2008.  Alexandre d\u2019Aspremont, Francis R. Bach, and Laurent El Ghaoui. Approximation Bounds for Sparse Principal Component Analysis. Math. Program., 148(1-2):89-110, 2014. doi: 10.1007/ s10107-014-0751-7. URL http://dx.doi.org/10.1007/s10107-014-0751-7.  Yash Deshpande and Andrea Montanari. Sparse PCA via covariance thresholding. arXiv preprint  arXiv:1311.5179, 2013.  Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634-652, 1998.  Uriel Feige, Guy Kortsarz, and David Peleg. The dense k-subgraph problem. Algorithmica, 29 (3):410-421, 2001. doi: 10.1007/s004530010050. URL http://dx.doi.org/10.1007/ s004530010050.  Chao Gao, Zongming Ma, and Harrison H. Zhou. Sparse CCA: Adaptive Estimation and Compu-  tational Barriers. arXiv preprint arXiv:1409.8565, 2014.  Johan H\u02daastad. Clique is hard to approximate within n1\u2212(cid:15). Acta Mathematica, 182(1):105-142,  1999.  I.T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal of  Applied Statistics, 22(1):29-35, 1995.  I.T. Jolliffe, N.T. Trendafilov, and M. Uddin. A modified principal component technique based on  the lasso. Journal of Computational and Graphical Statistics, 12(3):531-547, 2003.  M. Journ\u00b4ee, Y. Nesterov, P. Richt\u00b4arik, and R. Sepulchre. Generalized power method for sparse principal component analysis. The Journal of Machine Learning Research, 11:517-553, 2010.  H.F. Kaiser. The varimax criterion for analytic rotation in factor analysis. Psychometrika, 23(3):  187-200, 1958.  Subhash Khot. Improved inaproximability results for maxclique, chromatic number and approxi- mate graph coloring. In 42nd Annual Symposium on Foundations of Computer Science, FOCS 2001, 14-17 October 2001, Las Vegas, Nevada, USA, pages 600-609, 2001. doi: 10.1109/SFCS. 2001.959936. URL http://dx.doi.org/10.1109/SFCS.2001.959936.  Subhash Khot. Ruling out PTAS for graph min-bisection, dense k-subgraph, and bipartite clique.  SIAM Journal on Computing, 36(4):1025-1071, 2006.  Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse  PCA up to the information limit? Annals of Probability, 43:1300-1322, 2015.  Volodymyr Kuleshov. Fast algorithms for sparse principal component analysis based on rayleigh In Proceedings of the 30th International Conference on Machine Learning  quotient iteration. (ICML-13), pages 1418-1425, 2013. ON THE APPROXIMABILITY OF SPARSE PCA  Tsz Chiu Kwok and Lap Chi Lau. Lower bounds on expansions of graph powers. In APPROX, pages 313-324, 2014. URL http://dx.doi.org/10.4230/LIPIcs.APPROX-RANDOM. 2014.313.  Tengyu Ma and Avi Wigderson. Sum-of-Squares Lower Bounds for Sparse PCA. In NIPS, 2015.  URL http://arxiv.org/abs/1507.06370. To appear.  Zongming Ma. Sparse principal component analysis and iterative thresholding. The Annals of  Statistics, 41(2):772-801, 2013.  Malik Magdon-Ismail. NP-hardness and inapproximability of sparse PCA. CoRR, abs/1502.05675,  2015. URL http://arxiv.org/abs/1502.05675.  B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse pca: Exact and greedy algo-  rithms. NIPS, 18:915, 2006.  B. Moghaddam, Y. Weiss, and S. Avidan. Fast pixel/part selection with sparse eigenvectors. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1-8. IEEE, 2007.  Abhiram Natarajan and Yi Wu. Computational complexity of certifying restricted isometry prop- erty. In APPROX, pages 371-380, 2014. URL http://dx.doi.org/10.4230/LIPIcs. APPROX-RANDOM.2014.371.  Prasad Raghavendra and Tselil Schramm. Gap amplification for small-set expansion via ran- In APPROX, pages 381-391, 2014. URL http://dx.doi.org/10.4230/  dom walks. LIPIcs.APPROX-RANDOM.2014.381.  Prasad Raghavendra and David Steurer. Graph expansion and the unique games conjecture.  In Proceedings of the 42nd ACM Symposium on Theory of Computing, (STOC) 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 755-764, 2010a. doi: 10.1145/1806689.1806788. URL http://dblp.uni-trier.de/rec/bib/conf/stoc/RaghavendraS10.  Prasad Raghavendra and David Steurer. Graph expansion and the unique games conjecture.  In STOC, pages 755-764, New York, NY, USA, 2010b. ACM. ISBN 978-1-4503-0050-6. URL http://doi.acm.org/10.1145/1806689.1806792.  Prasad Raghavendra, David Steurer, and Prasad Tetali. Approximations for the isoperimetric and In STOC, pages 631-640, New York, NY, ISBN 978-1-4503-0050-6. URL http://doi.acm.org/10.1145/  spectral profile of graphs and related parameters. USA, 2010. ACM. 1806689.1806776.  Prasad Raghavendra, David Steurer, and Madhur Tulsiani. Reductions between expansion problems. In CCC, pages 64-73, Washington, DC, USA, 2012. IEEE Computer Society. ISBN 978-0-7695- 4708-4. URL http://dx.doi.org/10.1109/CCC.2012.43.  Muli Safra. Lecture notes in computational complexity theory. URL http://www.tau.ac.  il/\u02dcsafra/Complexity/pdf/2SAT_Handouts.pdf. CHAN PAPAILLIOPOULOS RUBINSTEIN  H. Shen and J.Z. Huang. Sparse principal component analysis via regularized low rank matrix  approximation. Journal of multivariate analysis, 99(6):1015-1034, 2008.  B.K. Sriperumbudur, D.A. Torres, and G.R.G. Lanckriet. Sparse eigen methods by D.C. program- ming. In Proceedings of the 24th international conference on Machine learning, pages 831-838. ACM, 2007.  Tengyao Wang, Quentin Berthet, and Richard J. Samworth. Statistical and computational trade-offs  in estimation of sparse principal components. arXiv preprint arXiv:1408.5369, 2014.  Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. The  Journal of Machine Learning Research, 14(1):899-925, 2013.  Y. Zhang, A. d\u2019Aspremont, and L.E. Ghaoui. Sparse PCA: Convex relaxations, algorithms and applications. Handbook on Semidefinite, Conic and Polynomial Optimization, pages 915-940, 2012.  Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of  computational and graphical statistics, 15(2):265-286, 2006.  David Zuckerman. Linear degree extractors and the inapproximability of max clique and chromatic number. Theory of Computing, 3(1):103-128, 2007. doi: 10.4086/toc.2007.v003a006. URL http://dx.doi.org/10.4086/toc.2007.v003a006. "}, "Pure Exploration of Multi-armed Bandit Under Matroid Constraints": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Pure Exploration of Multi-armed Bandit Under Matroid Constraints", "abstract": "We study the pure exploration problem subject to a matroid constraint\t(Best-Basis) in a stochastic multi-armed bandit game. In a Best-Basis instance, we are given n stochastic arms with unknown reward distributions, as well as a matroid \\mathcalM over the arms. Let the weight of an arm be the mean of its reward distribution. Our goal is to identify a basis of \\mathcalM with the maximum total weight, using as few samples as possible. The problem is a significant generalization of the best arm identification problem and the top-k arm identification problem, which have attracted significant attentions in recent years. We study both the exact and PAC versions of Best-Basis, and provide algorithms with nearly-optimal sample complexities for these versions. Our results generalize and/or improve on several previous results for the top-k arm identification problem and the combinatorial pure exploration problem when the combinatorial constraint is a matroid.", "pdf_url": "http://proceedings.mlr.press/v49/chen16a.pdf", "keywords": ["matroid", "multi-armed bandit", "pure exploration"], "reference": "Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  S\u00b4eebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed bandits. In Proceedings of The 30th International Conference on Machine Learning, pages 258- 265, 2013.  20   r \u2265 s, we have:  (cid:32) +\u221e (cid:88)  O  r=s+1  |BADr\u22121,s| \u00b7 (ln |Scur| + ln \u03b4\u22121  r )\u03b5\u22122 r  = O  (cid:33)  (cid:32) +\u221e (cid:88)  r=s+1  1 8r\u2212s \u00b7 |BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln r)\u03b5\u22122  r  (cid:33)  = O (cid:0)|BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s(cid:1)  Putting them together, we can see the number of samples incurred by UniformSample is bounded by:  (cid:32)+\u221e (cid:88)  O  (|BADs| + |OPTs|) \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s  ,  (cid:33)  s=1 which simplifies to O (cid:0)(cid:80) e )(cid:1) . Finally, we consider the number of samples taken by PAC-SamplePrune. Noticing nopt = rank(Mcur), the number of samples is O(|Scur|(ln nopt + ln \u03b4\u22121 r ). So PAC-SamplePrune does not affect the sample complexity, and we finish our proof.  e (ln k + ln \u03b4\u22121 + ln ln \u2206\u22121  e\u2208S \u2206\u22122  r )\u03b5\u22122  5. Future Work  In this paper, we present nearly-optimal algorithms for both the exact and PAC versions of the pure- exploration problem subject to a matroid constraint in a stochastic multi-armed bandit game: given a set of arms with a matroid constraint on them, pick a basis of the matroid whose weight (the sum of expectations over arms in this basis) is as large as possible, with high probability.  An immediate direction for investiation is to extend our results to other polynomial-time-computable combinatorial constraints: s-t paths, matchings (or more generally, the intersection of two matroid- s), etc. The model also extends to NP-hard combinatorial constraints, but there we would likely compare our solution against \u03b1-approximate solutions, instead of the optimal solution. Considering non-linear functions of the means is another natural next step. Yet another, perhaps more challeng- ing, direction is to consider stochastic optimization problems, where the solution may depend on other details of the distributions than just the means.  References  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In  COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Jean-Yves Audibert, S\u00b4ebastien Bubeck, and G\u00b4abor Lugosi. Regret in online combinatorial opti-  mization. Mathematics of Operations Research, 39(1):31-45, 2013.  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.  S\u00b4ebastien Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed  bandits. arXiv preprint arXiv:1205.3181, 2012.  S\u00b4eebastian Bubeck, Tengyao Wang, and Nitin Viswanathan. Multiple identifications in multi-armed bandits. In Proceedings of The 30th International Conference on Machine Learning, pages 258- 265, 2013. PURE EXPLORATION OF MULTI-ARMED BANDIT UNDER MATROID CONSTRAINTS [EXTENDED ABSTRACT]  Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Advances in Neural Information Processing Systems, pages 1036-1044, 2015.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Sciences, 78(5):1404-1422, 2012.  arXiv:1511.03774, 2015.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Combinatorial bandits. Journal of Computer and System  Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint  Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure explo- In Advances in Neural Information Processing Systems, pages  ration of multi-armed bandits. 379-387, 2014.  Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework In Proceedings of the 30th International Conference on Machine Learning,  and applications. pages 151-159, 2013.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov  decision processes. In Computational Learning Theory, pages 255-270. Springer, 2002.  Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research, 7:1079-1105, 2006.  RH Farrell. Asymptotic behavior of expected sample size in certain one sided tests. The Annals of  Mathematical Statistics, pages 36-72, 1964.  Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric, and S\u00b4ebastien Bubeck. Multi- In Advances in Neural Information Processing Systems, pages  bandit best arm identification. 2222-2230, 2011.  Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Pro- cessing Systems, pages 3212-3220, 2012.  Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett. Improved learning complexity in combinatorial pure exploration bandits. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 1004-1012, 2016.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck.  lil\u2019ucb: An optimal  exploration algorithm for multi-armed bandits. COLT, 2014.  Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 655-662, 2012.  David R Karger. Random sampling and greedy sparsification for matroid optimization problems.  Mathematical Programming, 82(1-2):41-81, 1998. David R Karger, Philip N Klein, and Robert E Tarjan. A randomized linear-time algorithm to find  minimum spanning trees. Journal of the ACM (JACM), 42(2):321-328, 1995.  Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1238-1246, 2013.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. arXiv preprint arXiv:1407.4443, 2014.  Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit  problem. The Journal of Machine Learning Research, 5:623-648, 2004.  Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms. Chapman & Hall/CRC, 2010.  Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Select-  ed Papers, pages 169-177. Springer, 1985.  Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identification with applications In Proceedings of the 31st International Conference on Machine Learning  to crowdsourcing. (ICML-14), pages 217-225, 2014.  "}, "Provably manipulation-resistant reputation systems": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Provably manipulation-resistant reputation systems", "abstract": "Reputation and reliability play a central role in a wide range of applications, from online marketplaces to review aggregators to ridesharing services.  Many reputation systems are vulnerable to manipulation, and protected only by keeping algorithms secret, avoiding high-stakes applications, or using side information to identify malicious users.  The current situation is reminiscent of pre-modern cryptography, characterized by a patchwork of ad hoc techniques with minimal formal understanding of their security. We propose a reputation system which provably achieves a very strong correctness guarantee under extremely pessimistic assumptions\u2014it works even given a supermajority of malicious users, converges to optimal behavior after a constant number of interactions per user, does not require repeated interactions, and accommodates time-varying quality of resources. Our formal model is simple but general.  In each period, a user is given an opportunity to interact with a resource, and must accept or reject the proposed interaction.  If they accept, they receive a payoff in [-1, 1].  Ideally all users would behave honestly, pooling their data and quickly learning which resources are worth interacting with.  Our protocol essentially matches this performance when all users are honest, while guaranteeing that adding malicious users or users with varying tastes does very little damage. We also extend our results to a more challenging setting where users interact with each other rather than with static resources, and where the two parties to an interaction may receive different payoffs.", "pdf_url": "http://proceedings.mlr.press/v49/christiano16.pdf", "keywords": ["Online learning", "collaborative learning", "manipulation-resistance", "collaborative filtering", "reputation systems"], "reference": "Alon, Awerbuch, Azar, and Patt-Shamir. Tell me who I am: An interactive recommendation system.  In SPAA: Annual ACM Symposium on Parallel Algorithms and Architectures, 2006.  Awerbuch and Kleinberg. Competitive collaborative learning. In COLT: Proceedings of the Work-  shop on Computational Learning Theory, Morgan Kaufmann Publishers, 2005.  Awerbuch, Azar, Lotker, Patt-Shamir, and Tuttle. Collaborate with strangers to find own prefer-  ences. MST: Mathematical Systems Theory, 42, 2008.  Paul Christiano. Online local learning via semidefinite programming. In STOC: ACM Symposium  on Theory of Computing (STOC), 2014.  B. Cohen. Incentives build robustness in bittorrent. In Proceedings of the Workshop on Economics of Peer-to-Peer Systems, Berkeley, CA, USA, 2003. URL http://citeseer.nj.nec.com/ cohen03incentives.html.  Minaxi Gupta, Paul Judge, and Mostafa Ammar. A reputation system for peer-to-peer networks. In Proceedings of the 13th International Workshop on Network and Operating System Support for Digital Audio and Video Archive (NOSSDAV-03), pages 144-152, New York, June 1-3 2003. ACM Press.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136, 2012. URL http://arxiv.org/abs/1204.0136.  Sepandar D. Kamvar, Mario T. Schlosser, and Hector Garcia-Molina. The eigentrust algorithm for reputation management in P2P networks. In Guszt\u00b4av Hencsey, Bebo White, Yih-Farn Robin Chen, L\u00b4aszl\u00b4o Kov\u00b4acs, and Steve Lawrence, editors, WWW, pages 640-651. ACM, 2003. ISBN 1-58113-680-3. URL http://doi.acm.org/10.1145/775152.775242.  M. Meulpolder, J. A. Pouwelse, D. H. J. Epema, and H. J. Sips. Bartercast: A practical approach to prevent lazy freeriding in P2P networks. In State University of New York Yuanyuan Yang, editor, Proceedings of the 23rd IEEE International Parallel & Distributed Processing Sym- posium, pages 1-8, Los Alamitos, USA, May 2009. IEEE Computer Society. ISBN 978-1- 4244-3750-4. URL http://www.pds.ewi.tudelft.nl/%7Eepema/Papers/2009/ HotP2P2009.pdf.  Paul Resnick and Rahul Sami. The in\ufb02uence limiter: provably manipulation-resistant recom- mender systems. In Joseph A. Konstan, John Riedl, and Barry Smyth, editors, RecSys, pages 25-32. ACM, 2007. ISBN 978-1-59593-730-8. URL http://doi.acm.org/10.1145/ 1297231.1297236.  Sven Seuken and David C. Parkes. Sybil-proof accounting mechanisms with transitive trust. In Ana L. C. Bazzan, Michael N. Huhns, Alessio Lomuscio, and Paul Scerri, editors, AAMAS, pages 205-212. IFAAMAS/ACM, 2014. ISBN 978-1-4503-2738-1. URL http://dl.acm.org/ citation.cfm?id=2615731.  14   CHRISTIANO  References  Alon, Awerbuch, Azar, and Patt-Shamir. Tell me who I am: An interactive recommendation system.  In SPAA: Annual ACM Symposium on Parallel Algorithms and Architectures, 2006.  Awerbuch and Kleinberg. Competitive collaborative learning. In COLT: Proceedings of the Work-  shop on Computational Learning Theory, Morgan Kaufmann Publishers, 2005.  Awerbuch, Azar, Lotker, Patt-Shamir, and Tuttle. Collaborate with strangers to find own prefer-  ences. MST: Mathematical Systems Theory, 42, 2008.  Paul Christiano. Online local learning via semidefinite programming. In STOC: ACM Symposium  on Theory of Computing (STOC), 2014.  B. Cohen. Incentives build robustness in bittorrent. In Proceedings of the Workshop on Economics of Peer-to-Peer Systems, Berkeley, CA, USA, 2003. URL http://citeseer.nj.nec.com/ cohen03incentives.html.  Minaxi Gupta, Paul Judge, and Mostafa Ammar. A reputation system for peer-to-peer networks. In Proceedings of the 13th International Workshop on Network and Operating System Support for Digital Audio and Video Archive (NOSSDAV-03), pages 144-152, New York, June 1-3 2003. ACM Press.  Elad Hazan, Satyen Kale, and Shai Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. CoRR, abs/1204.0136, 2012. URL http://arxiv.org/abs/1204.0136.  Sepandar D. Kamvar, Mario T. Schlosser, and Hector Garcia-Molina. The eigentrust algorithm for reputation management in P2P networks. In Guszt\u00b4av Hencsey, Bebo White, Yih-Farn Robin Chen, L\u00b4aszl\u00b4o Kov\u00b4acs, and Steve Lawrence, editors, WWW, pages 640-651. ACM, 2003. ISBN 1-58113-680-3. URL http://doi.acm.org/10.1145/775152.775242.  M. Meulpolder, J. A. Pouwelse, D. H. J. Epema, and H. J. Sips. Bartercast: A practical approach to prevent lazy freeriding in P2P networks. In State University of New York Yuanyuan Yang, editor, Proceedings of the 23rd IEEE International Parallel & Distributed Processing Sym- posium, pages 1-8, Los Alamitos, USA, May 2009. IEEE Computer Society. ISBN 978-1- 4244-3750-4. URL http://www.pds.ewi.tudelft.nl/%7Eepema/Papers/2009/ HotP2P2009.pdf.  Paul Resnick and Rahul Sami. The in\ufb02uence limiter: provably manipulation-resistant recom- mender systems. In Joseph A. Konstan, John Riedl, and Barry Smyth, editors, RecSys, pages 25-32. ACM, 2007. ISBN 978-1-59593-730-8. URL http://doi.acm.org/10.1145/ 1297231.1297236.  Sven Seuken and David C. Parkes. Sybil-proof accounting mechanisms with transitive trust. In Ana L. C. Bazzan, Michael N. Huhns, Alessio Lomuscio, and Paul Scerri, editors, AAMAS, pages 205-212. IFAAMAS/ACM, 2014. ISBN 978-1-4503-2738-1. URL http://dl.acm.org/ citation.cfm?id=2615731. PROVABLY MANIPULATION-RESISTANT REPUTATION SYSTEMS  Sven Seuken, Jie Tang, and David C. Parkes. Accounting mechanisms for distributed work systems. In Maria Fox and David Poole, editors, AAAI. AAAI Press, 2010. URL http://www.aaai. org/ocs/index.php/AAAI/AAAI10/paper/view/1802.  Shai Shalev-Shwartz. Online learning and online convex optimization.  Foundations and Trends in Machine Learning, 4(2):107-194, 2012. URL http://dx.doi.org/10.1561/ 2200000018.  Xiaoyuan Su and Taghi M. Khoshgoftaar. A survey of collaborative filtering techniques. Adv. Arti- ficial Intellegence, 2009, 2009. URL http://dx.doi.org/10.1155/2009/421425.  Vivek Vishnumurthy, Sangeeth Chandrakumar, and Emin Gun Sirer.  KARMA : A se- cure economic framework for peer-to-peer resource sharing, November 13 2003. URL http://citeseer.ist.psu.edu/682464.html;http://www.cs.cornell. edu/People/egs/papers/karma.pdf.  Haifeng Yu, Chenwei Shi, Michael Kaminsky, Phillip B. Gibbons, and Feng Xiao. DSybil: Optimal In IEEE Symposium on Security and Privacy, sybil-resistance for recommendation systems. pages 283-298. IEEE Computer Society, 2009. ISBN 978-0-7695-3633-0. URL http://dx. doi.org/10.1109/SP.2009.26.  "}, "On the Expressive Power of Deep Learning: A Tensor Analysis": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "On the Expressive Power of Deep Learning: A Tensor Analysis", "abstract": "It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.", "pdf_url": "http://proceedings.mlr.press/v49/cohen16.pdf", "keywords": ["Deep Learning", "Expressive Power", "Arithmetic Circuits", "Tensor Decompositions"], "reference": "Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom- positions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Richard Bellman, Richard Ernest Bellman, Richard Ernest Bellman, and Richard Ernest Bellman. Introduc-  tion to matrix analysis, volume 960. SIAM, 1970.  Yoshua Bengio. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1):  1-127, 2009.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. Neural Networks and Learning Systems, IEEE Transactions on, 25(8):1553-1565, 2014.  Joan Bruna and St\u00b4ephane Mallat. Invariant Scattering Convolution Networks. IEEE TPAMI, 2012.  Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report 05-02, 2005.  13   ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS  we show in app. A, pooling over large windows and trimming down a network\u2019s depth may bring to an exponential decrease in expressive efficiency.  The second point our theory sheds light on is sharing. As discussed in sec. 3.3, introducing weight sharing to a shallow network (CP model) considerably limits its expressive power. The net- work can only represent symmetric tensors, which in turn means that it is location invariant w.r.t. input vectors (patches). In the case of a deep network (HT model) the limitation posed by sharing is not as strict. Generated tensors need not be symmetric, implying that the network is capable of mod- eling location - a crucial ability in almost any real-world task. The above findings suggest that the sharing constraint is increasingly limiting as a network gets shallower, to the point where it causes complete ignorance to location. This could serve as an argument supporting the empirical success of deep convolutional networks - they bind together the statistical and computational advantages of sharing with many layers that mitigate its expressive limitations.  Lastly, our construction advocates locality, or more specifically, 1 \u00d7 1 receptive fields. Recent convolutional networks providing state of the art recognition performance (e.g. Lin et al. (2014); Szegedy et al. (2015)) make extensive use of 1 \u00d7 1 linear transformations, proving them to be very successful in practice. In view of our model, such 1 \u00d7 1 operators factorize tensors while providing universality with a minimal number of parameters. It seems reasonable to conjecture that for this task of factorizing coefficient tensors, larger receptive fields are not significantly helpful, as they lead to redundancy which may deteriorate performance in presence of limited training data. Investigation of this conjecture is left for future work.  Acknowledgments  Amnon Shashua would like to thank Tomaso Poggio and Shai S. Shwartz for illuminating discus- sions during the preparation of this manuscript. We would also like to thank Tomer Galanti, Tamir Hazan and Lior Wolf for commenting on draft versions of the paper. The work is partly funded by Intel grant ICRI-CI no. 9-2012-6133 and by ISF Center grant 1790/12. Nadav Cohen is supported by a Google Fellowship in Machine Learning.  References  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom- positions for learning latent variable models. Journal of Machine Learning Research, 15(1):2773-2832, 2014.  Richard Bellman, Richard Ernest Bellman, Richard Ernest Bellman, and Richard Ernest Bellman. Introduc-  tion to matrix analysis, volume 960. SIAM, 1970.  Yoshua Bengio. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1):  1-127, 2009.  Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. Neural Networks and Learning Systems, IEEE Transactions on, 25(8):1553-1565, 2014.  Joan Bruna and St\u00b4ephane Mallat. Invariant Scattering Convolution Networks. IEEE TPAMI, 2012.  Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report 05-02, 2005. COHEN SHARIR SHASHUA  Nadav Cohen and Amnon Shashua. Simnets: A generalization of convolutional networks. Advances in  Neural Information Processing Systems (NIPS), Deep Learning Workshop, 2014.  Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions.  International Conference on Machine Learning (ICML), 2016.  Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), 2016.  Systems, 2(4):303-314, 1989.  G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and  Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks.  In Advances in Neural  Information Processing Systems, pages 666-674, 2011.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint  arXiv:1512.03965, 2015.  176, 1990.  F Girosi and T Poggio. Networks and the best approximation property. Biological cybernetics, 63(3):169-  W Hackbusch and S K\u00a8uhn. A New Scheme for the Tensor Representation. Journal of Fourier Analysis and  Applications, 15(5):706-722, 2009.  Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics. Springer Science & Business Media, Berlin, Heidelberg, February 2012.  Benjamin D Haeffele and Ren\u00b4e Vidal. Global Optimality in Tensor Factorization, Deep Learning, and Be-  yond. CoRR abs/1202.2745, cs.NA, 2015.  Andr\u00b4as Hajnal, Wolfgang Maass, Pavel Pudl\u00b4ak, M\u00b4arl\u00b4o Szegedy, and Gy\u00a8orgy Tur\u00b4an. Threshold circuits of bounded depth. In Foundations of Computer Science, 1987., 28th Annual Symposium on, pages 99-110. IEEE, 1987.  Johan Hastad. Almost optimal lower bounds for small depth circuits. In Proceedings of the eighteenth annual  ACM symposium on Theory of computing, pages 6-20. ACM, 1986.  Johan H\u02daastad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational Com-  plexity, 1(2):113-129, 1991.  Kurt Hornik, Maxwell B Stinchcombe, and Halbert White. Multilayer feedforward networks are universal  approximators. Neural networks, 2(5):359-366, 1989.  Brian Hutchinson, Li Deng, and Dong Yu. Tensor Deep Stacking Networks. IEEE Trans. Pattern Anal. Mach.  Intell. (), 35(8):1944-1957, 2013.  Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the Perils of Non-Convexity: Guaranteed  Training of Neural Networks using Tensor Methods. CoRR abs/1506.08473, 2015.  Frank Jones. Lebesgue integration on Euclidean space. Jones & Bartlett Learning, 2001.  Mauricio Karchmer. Communication complexity a new approach to circuit depth. 1989.  Tamara G Kolda and Brett W Bader. Tensor Decompositions and Applications. SIAM Review (), 51(3):  455-500, 2009. ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS  Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V Oseledets, and Victor S Lempitsky. Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition. CoRR abs/1202.2745, cs.CV, 2014.  Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook  of brain theory and neural networks, 3361(10), 1995.  Min Lin, Qiang Chen, and Shuicheng Yan. Network In Network.  International Conference on Learning  Representations, 2014.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural net-  works. Advances in Neural Information Processing Systems, 2014.  Wolfgang Maass, Georg Schnitger, and Eduardo D Sontag. A comparison of the computational power of  sigmoid and Boolean threshold circuits. Springer, 1994.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks. arXiv  preprint arXiv:1411.7717, 2014.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In Advances in Neural Information Processing Systems, pages 2877- 2885, 2013.  Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems, pages 2924-2932, 2014.  Alexander Novikov, Anton Rodomanov, Anton Osokin, and Dmitry Vetrov. Putting MRFs on a Tensor Train.  ICML, pages 811-819, 2014.  Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of inference regions of deep feed  forward networks with piece-wise linear activations. arXiv preprint arXiv, 1312, 2013.  Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 8:143-195,  January 1999.  Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 689-690. IEEE, 2011.  Ran Raz and Amir Yehudayoff. Lower bounds and separations for constant depth multilinear circuits. Com-  putational Complexity, 18(2):171-207, 2009.  Benjamin Rossman, Rocco A Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem for  boolean circuits. arXiv preprint arXiv:1504.03398, 2015.  Walter Rudin. Functional analysis. international series in pure and applied mathematics, 1991.  Thomas Serre, Lior Wolf, and Tomaso Poggio. Object Recognition with Features Inspired by Visual Cortex.  CVPR, 2:994-1000, 2005.  Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib, Richard M Schwartz, and John Makhoul. Statistical Machine Translation Features with Multitask Tensor Networks. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, cs.CL, 2015.  Amir Shpilka and Amir Yehudayoff. Arithmetic circuits: A survey of recent results and open questions.  Foundations and Trends in Theoretical Computer Science, 5(3-4):207-388, 2010. COHEN SHARIR SHASHUA  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-  tion. arXiv preprint arXiv:1409.1556, 2014.  Michael Sipser. Borel sets and circuit complexity. ACM, New York, New York, USA, December 1983.  Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Y Ng. Reasoning With Neural Tensor Networks for Knowledge Base Completion. Advances in Neural Information Processing Systems, pages 926-934, 2013.  Le Song, Mariya Ishteva, Ankur P Parikh, Eric P Xing, and Haesun Park. Hierarchical Tensor Decomposition  of Latent Tree Graphical Models. ICML, pages 334-342, 2013.  Maxwell Stinchcombe and Halbert White. Universal approximation using feedforward networks with non- International Joint Conference on Neural Networks, pages  sigmoid hidden layer activation functions. 613-617 vol.1, 1989.  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,  Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. CVPR, 2015.  Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. DeepFace: Closing the Gap to Human- In CVPR \u201914: Proceedings of the 2014 IEEE Conference on  Level Performance in Face Verification. Computer Vision and Pattern Recognition. IEEE Computer Society, June 2014.  Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101,  2015.  Y Yang and D B Dunson. Bayesian conditional tensor factorizations for high-dimensional classification.  Journal of the American Statistical, 2015.  Dong Yu, Li Deng, and Frank Seide. Large Vocabulary Speech Recognition Using Deep Tensor Neural  Networks. INTERSPEECH, pages 6-9, 2012.  Daniel Zoran and Yair Weiss. \u201dNatural Images, Gaussian Mixtures and Dead Leaves\u201d. Advances in Neural  Information Processing Systems, pages 1745-1753, 2012. ON THE EXPRESSIVE POWER OF DEEP LEARNING: A TENSOR ANALYSIS  "}, "A Light Touch for Heavily Constrained SGD": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "A Light Touch for Heavily Constrained SGD", "abstract": "Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions, such as monotonic functions, submodular functions, classifiers that guarantee a certain class label for some subset of examples, etc. However, these restrictions may result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints.", "pdf_url": "http://proceedings.mlr.press/v49/cotter16.pdf", "keywords": [], "reference": "N. P. Archer and S. Wang. Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems. Decision Sciences, 24:60-75, 1993.  H. H. Bauschke. Projection Algorithms and Monotone Operators. Ph.D. Thesis, Simon Fraser  University, Canada, 1996.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, May 2003.  K. L. Clarkson, E. Hazan, and D. P. Woodruff. Sublinear optimization for machine learning. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS\u201910, pages 449-457, Washington, DC, USA, 2010. IEEE Computer Society.  H. Daniels and M. Velikova. Monotone and partially monotone neural networks.  IEEE Trans.  Neural Networks, 21(6):906-917, 2010.  K. Dzhaparidze and J. H. van Zanten. On Bernstein-type inequalities for martingales. Stochastic  Processes and their Applications, 93(1):109-117, May 2001.  D. Garber and E. Hazan. Playing non-linear games with linear oracles. In FOCS, pages 420-428.  IEEE Computer Society, 2013.  E. K. Garcia, R. Arora, and M. Gupta. Optimized regression for efficient function evaluation. IEEE  Trans. Image Processing, 21(9):4128-4140, September 2012.  D. Goldfarb and S. Liu. An O(Ln3) primal interior point algorithm for convex quadratic program-  ming. Math. Program., 49(3):325-340, January 1991.  M. R. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov, W. Moczydlowski, and A. van Esbroeck. Monotonic calibrated interpolated look-up tables. JMLR (to appear), 2016.  E. Hazan and S. Kale. Projection-free online learning. In ICML\u201912, 2012.  M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML\u201913, vol-  ume 28, pages 427-435, 2013.  York, NY, USA, 2002.  T. Joachims. Optimizing search engines using clickthrough data. In KDD\u201902, pages 133-142, New  R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-  tion. In NIPS\u201913, pages 315-323. 2013.  M. Mahdavi, T. Yang, R. Jin, S. Zhu, and J. Yi. Stochastic gradient descent with only one projection.  In NIPS\u201912, pages 494-502. 2012.  A. Nemirovski. Lecture notes: Interior point polynomial time methods in convex programming.  2004. URL http://www2.isye.gatech.edu/\u02dcnemirovs/Lect_IPM.pdf.  22   COTTER GUPTA PFEIFER  References  N. P. Archer and S. Wang. Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems. Decision Sciences, 24:60-75, 1993.  H. H. Bauschke. Projection Algorithms and Monotone Operators. Ph.D. Thesis, Simon Fraser  University, Canada, 1996.  A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex  optimization. Oper. Res. Lett., 31(3):167-175, May 2003.  K. L. Clarkson, E. Hazan, and D. P. Woodruff. Sublinear optimization for machine learning. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS\u201910, pages 449-457, Washington, DC, USA, 2010. IEEE Computer Society.  H. Daniels and M. Velikova. Monotone and partially monotone neural networks.  IEEE Trans.  Neural Networks, 21(6):906-917, 2010.  K. Dzhaparidze and J. H. van Zanten. On Bernstein-type inequalities for martingales. Stochastic  Processes and their Applications, 93(1):109-117, May 2001.  D. Garber and E. Hazan. Playing non-linear games with linear oracles. In FOCS, pages 420-428.  IEEE Computer Society, 2013.  E. K. Garcia, R. Arora, and M. Gupta. Optimized regression for efficient function evaluation. IEEE  Trans. Image Processing, 21(9):4128-4140, September 2012.  D. Goldfarb and S. Liu. An O(Ln3) primal interior point algorithm for convex quadratic program-  ming. Math. Program., 49(3):325-340, January 1991.  M. R. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov, W. Moczydlowski, and A. van Esbroeck. Monotonic calibrated interpolated look-up tables. JMLR (to appear), 2016.  E. Hazan and S. Kale. Projection-free online learning. In ICML\u201912, 2012.  M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML\u201913, vol-  ume 28, pages 427-435, 2013.  York, NY, USA, 2002.  T. Joachims. Optimizing search engines using clickthrough data. In KDD\u201902, pages 133-142, New  R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc-  tion. In NIPS\u201913, pages 315-323. 2013.  M. Mahdavi, T. Yang, R. Jin, S. Zhu, and J. Yi. Stochastic gradient descent with only one projection.  In NIPS\u201912, pages 494-502. 2012.  A. Nemirovski. Lecture notes: Interior point polynomial time methods in convex programming.  2004. URL http://www2.isye.gatech.edu/\u02dcnemirovs/Lect_IPM.pdf. LIGHTTOUCH  A. Nemirovski and D. Yudin. Problem complexity and method efficiency in optimization. John  Wiley & Sons Ltd, 1983.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, January 2009.  Y.-J. Qu and B.-G. Hu. Generalized constraint neural network regression model subject to linear  priors. IEEE Trans. on Neural Networks, 22(11):2447-2459, 2011.  A. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In  NIPS\u201913, pages 3066-3074. 2013.  S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal Estimated sub-GrAdient  SOlver for SVM. Mathematical Programming, 127(1):3-30, March 2011.  J. Sill. Monotonic networks. Advances in Neural Information Processing Systems (NIPS), 1998.  J. Spouge, H. Wan, and W. J. Wilbur. Least squares isotonic regression in two dimensions. Journal  of Optimization Theory and Applications, 117(3):585-605, 2003.  N. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In NIPS\u201911,  2011.  A. W\u00a8achter and L. T. Biegler. On the implementation of a primal-dual interior point filter line search algorithm for large-scale nonlinear programming. Mathematical Programming, 106(1): 25-57, 2006.  M. Wang, Y. Chen, J. Liu, and Y. Gu. Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints. ArXiv e-prints, November 2015.  Wikipedia. Coupon collector\u2019s problem \u2014 Wikipedia,  http://en.wikipedia.org/wiki/Coupon_collector%27s_problem. accessed 20-November-2014].  the free encyclopedia, 2014. URL [Online;  Wikipedia. Properties of polynomial roots \u2014 Wikipedia, the free encyclopedia, 2015. URL [On-  http://en.wikipedia.org/wiki/Properties_of_polynomial_roots. line; accessed 27-March-2015].  M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent.  In  ICML\u201903, 2003. COTTER GUPTA PFEIFER  Table 4: New notation in "}, "Adaptive Learning with Robust Generalization Guarantees": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Adaptive Learning with Robust Generalization Guarantees", "abstract": "The traditional notion of \\emphgeneralization\u2014i.e., learning a hypothesis whose empirical error is close to its true error\u2014is surprisingly brittle. As has recently been noted [Dwork et al. 2015], even if several algorithms have this guarantee in isolation, the guarantee need not hold if the algorithms are composed adaptively. In this paper, we study three notions of generalization\u2014increasing in strength\u2014that are \\emphrobust to postprocessing and amenable to adaptive composition, and examine the relationships between them.  We call the weakest such notion \\emphRobust Generalization. A second, intermediate, notion is the stability guarantee known as \\emphdifferential privacy. The strongest guarantee we consider we call \\emphPerfect Generalization. We prove that every hypothesis class that is PAC learnable is also PAC learnable in a robustly generalizing fashion, with almost the same sample complexity. It was previously known that differentially private algorithms satisfy robust generalization. In this paper, we show that robust generalization is a strictly weaker concept, and that there is a learning task that can be carried out subject to robust generalization guarantees, yet cannot be carried out subject to differential privacy. We also show that perfect generalization is a strictly stronger guarantee than differential privacy, but that, nevertheless, many learning tasks can be carried out subject to the guarantees of perfect generalization.", "pdf_url": "http://proceedings.mlr.press/v49/cummings16.pdf", "keywords": ["Adaptive learning", "generalizations", "compression schemes", "and composition"], "reference": "2016.  Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. In Proceedings of the 48th Annual ACM on  Algorithmic stability for adaptive data analysis. Symposium on Theory of Computing, STOC, 2016.  25   LEARNING UNDER ROBUST GENERALIZATION  Theorems 39 and Theorem 40 only show a relationship between (\u03b5, 0)-differential privacy and strong generalization. To show such a relationship when \u03b4 > 0, we appeal to group privacy, first studied by Dwork et al. (2006a), which says that if M is (\u03b5, \u03b4)-differentially private and two samples S, S(cid:48) differ on k entries, then M(S) \u2248k\u03b5,ke(k\u22121)\u03b5\u03b4 M(S(cid:48)). Using simulator SIMD = M(S\u2217) for any fixed sample S\u2217 \u223ci.i.d. Dn and by the fact that any sample S can differ from S\u2217 in an most n samples, we see that M is (0, n\u03b5, ne(n\u22121)\u03b5\u03b4)-perfectly generalizing.  Unfortunately, this blowup in parameters is generally unacceptable for most tasks. We suspect as with (\u03b5, 0)-differential  (cid:16)(cid:112)n ln(1/\u03b2)  that the necessary blowup in the \u03b5 parameter is closer to \u0398 privacy, but leave a formal proof as an open question for future work.  (cid:17)  On the positive side, most known (\u03b5, \u03b4)-differentially private algorithms are designed by com- posing several (\u03b5(cid:48), 0)-differentially private algorithms, where the \u03b4 > 0 is an artifact of the compo- sition (see, e.g., Theorem 3.20 of Dwork and Roth (2014) for more details). Since perfect general- ization enjoys adaptive composition (as shown in Bassily and Freund (2016)), we could also obtain (\u03b2, \u03b5, \u03b4)-perfectly generalizing algorithms by composing a collection of (\u03b2, \u03b5, 0)-perfectly general- izing algorithms together. This will give better generalization parameters than a direct reduction via group privacy.  Acknowledgments  We thank Adam Smith and Raef Bassily for helpful comments about adaptive composition of per- fectly generalizing mechanisms and pointing out an error in an earlier version of this paper. We thank Shay Moran for telling us about variable length compression schemes and sharing with us his manuscript David et al. (2016). We thank our anonymous reviewers for numerous helpful com- ments.  The first author is supported in part by NSF grant 1254169, US-Israel Binational Science Foun- dation grant 2012348, and a Simons Graduate Fellowship. The second author is supported in part by NSF grants 1254169 and 1518941, US-Israel Binational Science Foundation Grant 2012348, the Charles Lee Powell Foundation, a Google Faculty Research Award, an Okawa Foundation Research Grant, a subcontract through the DARPA Brandeis project, a grant from the HUJI Cyber Security Research Center, and a startup grant from Hebrew University\u2019s School of Computer Science. Part of this work was completed when the second author was visiting the Simons Institute for the The- ory of Computing at Berkeley. The third author is supported by grants from the Sloan Foundation, a Simons Investigator grant to Salil Vadhan, and NSF grant CNS-1237235. The fourth author is supported in part by an NSF CAREER award, NSF grant CNS-1513694, a subcontract through the DARPA Brandeis project, and a grant from the Sloan Foundation.  References  2016.  Raef Bassily and Yoav Freund. Typicality-based stability and privacy. CoRR, abs/1604.03336,  Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. In Proceedings of the 48th Annual ACM on  Algorithmic stability for adaptive data analysis. Symposium on Theory of Computing, STOC, 2016. CUMMINGS LIGETT NISSIM ROTH WU  Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competi- tions. In Proceedings of the 32nd International Conference on Machine Learning, ICML, pages 1006-1014, 2015.  Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to noninteractive database privacy. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, STOC, pages 609-618, 2008.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occams razor.  Readings in machine learning, pages 201-204, 1990.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2:499-526, 2002.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and learning of threshold functions. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pages 634-649, 2015.  Ofir David, Shay Moran, and Amir Yehudayof. Supervised learning through the lens of compres-  sion. Preprint, 2016.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations  and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Advances in Cryptology - EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques. Proceedings, chapter Our Data, Ourselves: Privacy Via Distributed Noise Generation, pages 486-503. Springer Berlin Heidelberg, 2006a.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC, pages 265-284, 2006b.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toni Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. In Advances in Neural Information Processing Systems, NIPS, pages 2341-2349, 2015a.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636-638, 2015b.  Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the 47th Annual ACM on Symposium on Theory of Computing, STOC, pages 117-126, 2015c.  Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an  application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.  Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam  Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011. LEARNING UNDER ROBUST GENERALIZATION  Michael J Kearns and Umesh Virkumar Vazirani. An introduction to computational learning theory.  MIT press, 1994.  report, 1986.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical  Frank McSherry and Kunal Talwar. Mechanism design via differential privacy.  In 48th Annual  IEEE Symposium on Foundations of Computer Science, FOCS, pages 94-103, 2007.  Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predic-  tivity in learning theory. Nature, 428(6981):419-422, 2004.  Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information the- ory. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS, 2016.  S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-  rithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.  Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of Complexity, pages 11-30. Springer International Publishing, 1971.  Manfred K. Warmuth. Compressing to VC Dimension Many Points, volume 2777, pages 743-744.  Springer Berlin Heidelberg, 2003.  "}, "Complexity Theoretic Limitations on Learning DNF\u2019s": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Complexity Theoretic Limitations on Learning DNF\u2019s", "abstract": "Using the recently developed framework of Daniely, Linial and Shalev-Shwartz, we show that under a natural assumption on the complexity of random K-SAT, learning DNF formulas is hard. Furthermore, the same assumption implies the hardness of various learning problems, including intersections of logarithmically many halfspaces, agnostically learning conjunctions, as well as virtually all (distribution free) learning problems that were previously shown hard (under various complexity assumptions).", "pdf_url": "http://proceedings.mlr.press/v49/daniely16.pdf", "keywords": ["DNFs", "Hardness of learning"], "reference": "Mikhail Alekhnovich, Sanjeev Arora, and Iannis Tourlakis. Towards strong nonapproximability results in the lov\u00b4asz-schrijver hierarchy. In Proceedings of the thirty-seventh annual ACM sym- posium on Theory of computing, pages 294-303. ACM, 2005.  12   DANIELY SHALEV-SHWARTZ  consider the formula h : {\u00b11}2Kn \u2192 {0, 1} defined by h(x) = \u2228T x \u2208 Xn,K we have,  t=1 \u2227Rt  r=1 \u2227n  i=1xjt,r,\u03c8ibt,r,i. For  h(g(x)) = 1 \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], i \u2208 [n], gjt,r,\u03c8ibt,r,i(x) = 1  \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], i \u2208 [n], x(jt,r) (cid:54)= (\u2212\u03c8ibt,r, i) \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], x1(jt,r) (cid:54)= \u2212\u03c8x2(jt,r)bt,r \u21d0\u21d2 \u2203t \u2208 [T ] \u2200r \u2208 [Rt], x1(jt,r)\u03c8x2(jt,r) = bt,r \u21d0\u21d2 h\u03c8(x) = x(\u03c8) = P (x1(1)\u03c8x2(1), . . . , x1(K)\u03c8x2(K)) = 1 .  (cid:3)  3.5. Wrapping up - concluding theorem 3 We are now ready to conclude the proof. Let q : N \u2192 N be any function such that q(n) = \u03c9(log(n)). W.l.o.g., we assume that q(n) = O (cid:0)log2(n)(cid:1). By theorem 12 it is enough to show that for every d, it is hard to distinguish samples that are realizable by DNFq(n) and (cid:0)nd, 1/4(cid:1)-scattered samples.  By assumption 1, there is K such that CSPrand  nd+2(SATK) is hard. Denote q(cid:48)(n) = q(2Kn). By  nd+1(TK,q(cid:48)(n)) is hard. By lemma 15, the problem CSPrand  lemma 13, the problem CSPrand is hard. Now, since \u00acTK,q(cid:48)(n) can be realized by a DNF formula with q(cid:48)(n) clauses, by lemma 16, the problem CSPrand nd+1(TK,q(cid:48)(n), \u00acTK,q(cid:48)(n)) can be reduced to a problem of distinguishing samples 8 nd+1, 1/4(cid:1)- that are realizable by a DNF formula with 2Kn variables and q(cid:48)(n) clauses, from (cid:0) 1 scattered samples. Changing variables (i.e., replacing 2Kn with n(cid:48)), we conclude that it is hard to distinguish samples that are realizable by DNFq(n) from 1 -scattered samples, which are in particular (cid:0)nd, 1/4(cid:1)-scattered. The theorem follows.  8(2K)d\u22121 nd+1, 1/4  (cid:16)  (cid:17)  nd+1(TK,q(cid:48)(n), \u00acTK,q(cid:48)(n))  Basic learning problems that we are unable to resolve even under the random K-SAT (or K-XOR) assumption include decision trees and intersections of a constantly many halfspaces. (It is worth noting that no known algorithm can learn even intersections of 2 halfspaces).  Amit Daniely was a recipient of the Google Europe Fellowship in Learning Theory, and this research was supported in part by this Google Fellowship. Shai Shalev-Shwartz is supported by the Israeli Science Foundation grant number 590-10. We thank Uri Feige, Guy Kindler and Nati Linial for valuable discussions.  4. Open questions  Acknowledgments  References  Mikhail Alekhnovich, Sanjeev Arora, and Iannis Tourlakis. Towards strong nonapproximability results in the lov\u00b4asz-schrijver hierarchy. In Proceedings of the thirty-seventh annual ACM sym- posium on Theory of computing, pages 294-303. ACM, 2005. COMPLEXITY THEORETIC LIMITATIONS ON LEARNING DNF\u2019S  Sarah Allen, Ryan O\u2019Donnell, and David Witmer. How to refute a random csp? In FOCS, 2015.  Dana Angluin and Michael Kharitonov. When won\u2019t membership queries help? In STOC, pages  444-454, May 1991.  B. Applebaum, B. Barak, and D. Xiao. On basing lower-bounds for learning on worst-case assump- tions. In Foundations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 211-220. IEEE, 2008.  Paul Beame and Toniann Pitassi. Simplified and improved resolution lower bounds. In Foundations of Computer Science, 1996. Proceedings., 37th Annual Symposium on, pages 274-282. IEEE, 1996.  Paul Beame, Richard Karp, Toniann Pitassi, and Michael Saks. On the complexity of unsatisfiability proofs for random k-cnf formulas. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 561-571. ACM, 1998.  Eli Ben-Sasson. Expansion in proof complexity. In Hebrew University. Citeseer, 2001.  Eli Ben-Sasson and Avi Wigderson. Short proofs are narrowresolution made simple. In Proceedings of the thirty-first annual ACM symposium on Theory of computing, pages 517-526. ACM, 1999.  Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and  the statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.  Joshua Buresh-Oppenheim, Nicola Galesi, Shlomo Hoory, Avner Magen, and Toniann Pitassi. Rank bounds and integrality gaps for cutting planes procedures. In Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 318-327. IEEE, 2003.  Amin Coja-Oghlan, Andreas Goerdt, and Andr\u00b4e Lanka. Strong refutation heuristics for random k-sat. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech- niques, pages 310-321. Springer, 2004.  Amin Coja-Oghlan, Colin Cooper, and Alan Frieze. An efficient sparse regularity concept. SIAM  Journal on Discrete Mathematics, 23(4):2000-2034, 2010.  Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC, 2016.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning  halfspaces over sparse vectors. In NIPS, 2013.  Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper  learning complexity. In STOC, 2014.  Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem-proving.  Communications of the ACM, 5(7):394-397, 1962.  Uriel Feige. Relations between average case complexity and approximation complexity. In Pro- ceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 534-543. ACM, 2002. DANIELY SHALEV-SHWARTZ  Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. In Au-  tomata, languages and programming, pages 519-530. Springer, 2004.  V. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy parities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, 2006.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. In STOC, 2015.  Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to construct random functions. Journal  of the Association for Computing Machinery, 33(4):792-807, October 1986.  Dima Grigoriev. Linear lower bound on degrees of positivstellensatz calculus proofs for the parity.  Theoretical Computer Science, 259(1):613-622, 2001.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proceedings of  the 47th Foundations of Computer Science (FOCS), 2006.  Armin Haken. The intractability of resolution. Theoretical Computer Science, 39:297-308, 1985.  Johan H\u02daastad. Some optimal inapproximability results. Journal of the ACM (JACM), 48(4):798-  859, 2001.  A. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In Pro-  ceedings of the 46th Foundations of Computer Science (FOCS), 2005.  Michael Kearns and Leslie G. Valiant. Cryptographic limitations on learning Boolean formulae and  finite automata. In STOC, pages 433-444, May 1989.  Michael Kharitonov. Cryptographic hardness of distribution-specific learning. In Proceedings of the twenty-fifth annual ACM symposium on Theory of computing, pages 372-381. ACM, 1993.  Subhash Khot and Rishi Saket. Hardness of minimizing and learning dnf expressions. In Foun- dations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 231-240. IEEE, 2008.  Subhash Khot and Rishi Saket. On the hardness of learning intersections of two halfspaces. Journal  of Computer and System Sciences, 77(1):129-141, 2011.  Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space. In  RANDOM, 2014.  Adam R Klivans and Rocco Servedio. Learning dnf in time 2O(n1/3). In Proceedings of the thirty-  third annual ACM symposium on Theory of computing, pages 258-265. ACM, 2001.  Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of  halfspaces. In FOCS, 2006.  Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. Efficient agnostic learning of neural IEEE Transactions on Information Theory, 42(6):2118-2132,  networks with bounded fan-in. 1996. COMPLEXITY THEORETIC LIMITATIONS ON LEARNING DNF\u2019S  N. Linial and Z. Luria. Chernoff\u2019s Inequality - A very elementary proof. Arxiv preprint  arXiv:1403.7739 v2, 2014.  Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier transform, and  learnability. In FOCS, pages 574-579, October 1989.  Yishay Mansour. An o(n log log n) learning algorithm for dnf under the uniform distribution. Jour-  nal of Computer and System Sciences, 50(3):543-550, 1995.  L. Pitt and L.G. Valiant. Computational limitations on learning from examples. Journal of the  Association for Computing Machinery, 35(4):965-984, October 1988.  Leonard Pitt and Manfred K. Warmuth. Prediction preserving reducibility. Technical Report UCSC- CRL-88-26, University of California Santa Cruz, Computer Research Laboratory, November 1988.  Prasad Raghavendra. Optimal algorithms and inapproximability results for every csp?  In Pro- ceedings of the 40th annual ACM symposium on Theory of computing, pages 245-254. ACM, 2008.  R.E. Schapire. The strength of weak learnability. In FOCS, pages 28-33, October 1989.  Grant Schoenebeck. Linear level lasserre lower bounds for certain k-csps. In Foundations of Com- puter Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 593-602. IEEE, 2008.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, Novem-  ber 1984.  "}, "Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables", "abstract": "We study the structure and learnability of sums of independent integer random variables (SIIRVs). For k \u2208\\mathbbZ_+, a \\emk-SIIRV of order n \u2208\\mathbbZ_+ is the probability distribution of the sum of n mutually independent random variables each supported on {0, 1, \u2026, k-1}. We denote by \\cal S_n,k the set of all k-SIIRVs of order n. How many samples are required to learn an arbitrary distribution in \\cal S_n,k? In this paper, we tightly characterize the sample and computational complexity of this problem. More precisely, we design a computationally efficient algorithm that uses \\widetildeO(k/\u03b5^2) samples, and learns an arbitrary k-SIIRV within error \u03b5, in total variation distance. Moreover, we show that the \\em optimal sample complexity of this learning problem is \u0398((k/\u03b5^2)\\sqrt\\log(1/\u03b5)), i.e., we prove an upper bound and a matching information-theoretic lower bound. Our algorithm proceeds by learning the Fourier transform of the target k-SIIRV in its effective support. Its correctness relies on the \\em approximate sparsity of the Fourier transform of k-SIIRVs \u2013 a structural property that we establish, roughly stating that the Fourier transform of k-SIIRVs has small magnitude outside a small set. Along the way we prove several new structural results about k-SIIRVs. As one of our main structural contributions, we give an efficient algorithm to construct a sparse \\em proper \u03b5-cover for \\cal S_n,k, in total variation distance. We also obtain a novel geometric characterization of the space of k-SIIRVs. Our characterization allows us to prove a tight lower bound on the size of \u03b5-covers for \\cal S_n,k \u2013 establishing that our cover upper bound is optimal \u2013 and is the key ingredient in our tight sample complexity lower bound. Our approach of exploiting the sparsity of the Fourier transform in distribution learning is general, and has recently found additional applications. In a subsequent work, we use a generalization of this idea to obtain the first computationally efficient learning algorithm for Poisson multinomial distributions. In\u00a0a separate work, we build on our Fourier-based approach to obtain the fastest known proper learning algorithm for Poisson binomial distributions (2-SIIRVs).", "pdf_url": "http://proceedings.mlr.press/v49/diakonikolas16a.pdf", "keywords": ["density estimation", "distribution learning", "sums of independent random variables", "Fourier transform", "metric entropy 1"], "reference": "NY, 1992.  2010.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  P. Assouad. Deux remarques sur l\u2019estimation. C. R. Acad. Sci. Paris S\u00b4er. I, 296:1021-1024, 1983.  A.D. Barbour, L. Holst, and S. Janson. Poisson Approximation. Oxford University Press, New York,  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  15   LEARNING SIIRVS VIA THE FOURIER TRANSFORM  some neighborhood of Pn(p). Specifically, there is some \u03b4 > 0 such that P \u22121 x \u2208 Tn with (cid:107)x \u2212 Pn(p)(cid:107)\u221e \u2264 \u03b4.  n is defined at every  Let Q be a distribution over [n] satisfying dK(P, Q) \u2264 \u03b4. Equivalently, if y = (Q(< i))n  i=1 \u2208 Tn is the CDF of Q, then (cid:107)Pn(p) \u2212 y(cid:107)\u221e \u2264 \u03b4. Thus, P \u22121 n (y) \u2208 Tn are the parameters of a 2-SIIRV with distribution Q. It follows that Q is a 2-SIIRV with parameters q, which completes the proof of (i). Note that the proof also implies that Q in this neighborhood can be taken to be Pn(q(cid:48)) for q(cid:48) in some small neighborhood of p.  n is defined at y and q = P \u22121  To prove part (ii) of Lemma 9, we use a geometric argument. Recall that the parameters of 1 4(n+1) . 4(n+1) , and therefore  P0 are p0 = ( 1 We show in the full version that any Q in Pn(\u2202S) satisfies dTV (P0, Q) \u2265 e\u22123n dK(P0, Q) \u2265 e\u22123n  n+1 ). Let S \u2286 Tn be the set of vectors p with (cid:107)p \u2212 p0(cid:107)\u221e \u2264  n+1 , . . . , n  8(n+1)2 \u2265 2\u22129n.  Let B be the set of distributions Q on [n] so that dK(P0, Q) \u2264 2\u22129n. We claim that Pn(S) \u2229 B = B. To begin, note that S is compact, and therefore this intersection is closed. On the other hand, since Pn(\u2202S) is disjoint from B, this intersection is Pn(int(S)) \u2229 B. On the other hand, since Pn has non-singular Jacobian on int(S), the open mapping theorem implies that Pn(int(S)) \u2229 B is an open subset of B. Therefore, Pn(S) \u2229 B is both a closed and open subset of B, and therefore, since B is connected, it must be all of B. This completes the proof of part (ii).  Given Lemma 9, our sample lower bound proceeds as follows: Starting from the 2-SIIRV P0, we perturb its pdf by a small amount to construct the \u201chypercube\u201d distributions Pb satisfying the conditions of Assouad\u2019s lemma. Lemma 9 guarantees that, if the perturbation is small enough, all these distributions are indeed 2-SIIRVs.  Acknowledgements. Part of this work was performed while I.D. and A.S. were at the University of Edinburgh, supported in part by EPSRC grant EP/L021749/1 and a Marie Curie Career Inte- gration Grant (CIG). The research of D.K. was supported in part by NSF Award CCF-1553288 (CAREER).  References  NY, 1992.  2010.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  P. Assouad. Deux remarques sur l\u2019estimation. C. R. Acad. Sci. Paris S\u00b4er. I, 296:1021-1024, 1983.  A.D. Barbour, L. Holst, and S. Janson. Poisson Approximation. Oxford University Press, New York,  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112, DIAKONIKOLAS KANE STEWART  L. Birg\u00b4e. On estimating a density using Hellinger distance and some other strange facts. Probab.  Theory Relat. Fields, 71(2):271-291, 1986.  R. Blei, F. Gao, and W. V. Li. Metric entropy of high dimensional distributions. Proceedings of the  American Mathematical Society (AMS), 135(12):4009 - 4018, 2007.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.  Learnability and the Vapnik-  Chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  B. Carl and I. Stephani. Entropy, compactness and the approximation of operators, volume 98 of  Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, 1990.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  L. Chen, L. Goldstein, and Q.-M. Shao. Normal Approximation by Stein\u2019s Method. Springer, 2011.  L. H. Y. Chen and Y. K. Leong. From zero-bias to discretized normal approximation. 2010.  S.X. Chen and J.S. Liu. Statistical applications of the Poisson-Binomial and Conditional Bernoulli  Distributions. Statistica Sinica, 7:875-892, 1997.  X. Chen, D. Durfee, and A. Orfanou. On the complexity of nash equilibria in anonymous games.  In STOC, 2015.  H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. Ann. Math. Statist., 23:493-507, 1952.  M. Cryan, L. Goldberg, and P. Goldberg. Evolutionary trees can be learned in polynomial time in  the two state general Markov model. SIAM Journal on Computing, 31(2):375-397, 2002.  C. Daskalakis and C. Papadimitriou. On Oblivious PTAS\u2019s for Nash Equilibrium. In STOC, pages  75-84, 2009.  C. Daskalakis and C. Papadimitriou. Sparse covers for sums of indicators. Probability Theory and  Related Fields, pages 1-27, 2014.  C. Daskalakis and C. H. Papadimitriou. Computing equilibria in anonymous games.  In FOCS,  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  pages 83-93, 2007.  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In LEARNING SIIRVS VIA THE FOURIER TRANSFORM  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of Inde-  pendent Integer Random Variables. In FOCS, pages 217-226, 2013.  C. Daskalakis, A. De, G. Kamath, and C. Tzamos. A size-free CLT for poisson multinomials and  its applications. CoRR, abs/1511.03641, 2015a.  C. Daskalakis, G. Kamath, and C. Tzamos. On the structure, covering, and learning of poisson  multinomial distributions. In FOCS, 2015b. Available as arxiv report abs/1504.08363.  L. Devroye and L. Gy\u00a8orfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in Statis-  1985.  tics, Springer, 2001.  I. Diakonikolas, D. M. Kane, and A. Stewart. Nearly optimal learning and sparse covers for sums  of independent integer random variables. CoRR, abs/1505.00662, 2015a.  I. Diakonikolas, D. M. Kane, and A. Stewart. Properly learning poisson binomial distributions in  almost polynomial time. CoRR, abs/1511.04066, 2015b.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distri-  butions and its algorithmic applications. CoRR, abs/1511.03592, 2015c.  D. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algo-  rithms. Cambridge University Press, Cambridge, 2009.  R.M Dudley. Metric entropy of some classes of sets with differentiable boundaries. Journal of Approximation Theory, 10(3):227 - 236, 1974. ISSN 0021-9045. doi: 10.1016/0021-9045(74) 90120-8.  D. E. Edmunds and H. Triebel. Function spaces, entropy numbers, differential operators, volume  120 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, 1996.  J. Feldman, R. O\u2019Donnell, and R. Servedio. Learning mixtures of product distributions over discrete  domains. In Proc. 46th IEEE FOCS, pages 501-510, 2005.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In Proceedings of the  12th Annual COLT, pages 183-192, 1999.  P. Groeneboom and G. Jongbloed. Nonparametric Estimation under Shape Constraints: Estimators,  Algorithms and Asymptotics. Cambridge University Press, 2014.  A. Guntuboyina and B. Sen. Covering numbers for convex functions. Information Theory, IEEE  Transactions on, 59(4):1957-1965, April 2013.  S. Har-peled. Geometric Approximation Algorithms. American Mathematical Society, Boston, MA,  USA, 2011.  R. Hasminskii and I. Ibragimov. On density estimation in the view of kolmogorov\u2019s ideas in ap-  proximation theory. Ann. Statist., 18(3):999-1010, 1990. DIAKONIKOLAS KANE STEWART  D. Haussler and M. Opper. Mutual information, metric entropy and cumulative relative entropy  risk. Ann. Statist., 25(6):2451-2492, 1997.  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58:13-30, 1963.  A. J. Izenman. Recent developments in nonparametric density estimation. Journal of the American  Statistical Association, 86(413):205-224, 1991.  A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In STOC,  M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cam-  pages 553-562, 2010.  bridge, MA, 1994.  M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. On the learnability of  discrete distributions. In Proc. 26th STOC, pages 273-282, 1994.  A. N. Kolmogorov and V. M. Tihomirov. \u03b5-entropy and \u03b5-capacity of sets in function spaces. Uspehi  Mat. Nauk, 14:3-86, 1959.  J. Kruopis. Precision of approximation of the generalized binomial distribution by convolutions of  poisson measures. Lithuanian Mathematical Journal, 26(1):37-49, 1986.  G. G. Lorentz. Metric entropy and approximation. Bull. Amer. Math. Soc., 72:903-937, 1966.  Y. Makovoz. On the kolmogorov complexity of functions of finite smoothness. Journal of Com-  plexity, 2(2):121 - 130, 1986.  Bachelier, Paris, 1837.  Appl., 28:393-403, 1983.  S.D. Poisson. Recherches sur la Probabilit`e des jugements en mati\u00b4e criminelle et en mati\u00b4ere civile.  E. L. Presman. Approximation of binomial distributions by infinitely divisible ones. Theory Probab.  B. Roos. Binomial approximation to the Poisson binomial distribution: The Krawtchouk expansion.  Theory Probab. Appl., 45:328-344, 2000.  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  1992.  porated, 2008.  B. W. Silverman. Density Estimation. Chapman and Hall, London, 1986.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incor-  A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer Series  in Statistics. Springer-Verlag, New York, 1996. With applications to statistics.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 113-122, 2002. LEARNING SIIRVS VIA THE FOURIER TRANSFORM  Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Ann.  Statist., 27(5):1564-1599, 1999.  Y. G. Yatracos. Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy.  Annals of Statistics, 13:768-774, 1985. "}, "Properly Learning Poisson Binomial Distributions in Almost Polynomial Time": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Properly Learning Poisson Binomial Distributions in Almost Polynomial Time", "abstract": "We give an algorithm for properly learning Poisson binomial distributions. A Poisson binomial distribution (PBD) of order n \u2208\\mathbbZ_+ is the discrete probability distribution of the sum of n mutually independent Bernoulli random variables. Given \\widetildeO(1/\u03b5^2) samples from an unknown PBD P, our algorithm runs in time (1/\\eps)^O(\\log \\log (1/\u03b5)), and outputs a hypothesis PBD that is \u03b5-close to P in total variation distance. The sample complexity of our algorithm is known to be nearly-optimal, up to logarithmic factors, as established in previous work. However, the previously best known running time for properly learning PBDs was (1/\u03b5)^O(\\log(1/\u03b5)), and was essentially obtained by enumeration over an appropriate  \u03b5-cover. We remark that the running time of this cover-based approach cannot be improved, as any \u03b5-cover for the space of PBDs has size  (1/\u03b5)^\u03a9(\\log(1/\u03b5)). As one of our main contributions, we provide a novel structural characterization of PBDs, showing that any PBD P is \u03b5-close to another PBD Q with O(\\log(1/\u03b5)) distinct parameters. More precisely, we prove that, for all \u03b5>0, there exists an explicit collection \\calM of (1/\u03b5)^O(\\log \\log (1/\u03b5)) vectors of multiplicities, such that for any PBD P there exists a PBD Q with O(\\log(1/\u03b5)) distinct parameters whose multiplicities are given by some element of \\cal M, such that Q is \u03b5-close to P.  Our proof combines tools from Fourier analysis and algebraic geometry. Our approach to the proper learning problem is as follows: Starting with an accurate non-proper hypothesis, we fit a PBD to this hypothesis. This fitting problem can be formulated as a natural polynomial optimization problem. Our aforementioned structural characterization allows us to reduce the corresponding fitting problem to a collection of (1/\u03b5)^O(\\log \\log(1/\u03b5)) systems of low-degree polynomial inequalities. We show that each such system can be solved in time (1/\u03b5)^O(\\log \\log(1/\u03b5)), which yields the overall running time of our algorithm.", "pdf_url": "http://proceedings.mlr.press/v49/diakonikolas16b.pdf", "keywords": ["distribution learning", "proper learning", "Poisson binomial distribution", "optimization over polynomials with real roots"], "reference": "J. Acharya and C. Daskalakis. Testing poisson binomial distributions. In Proceedings of the Twenty- Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1829-1840, 2015.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015b.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  F. Balabdaoui and C. R. Doss.  Inference for a Mixture of Symmetric Distributions under Log-  Concavity. Available at http://arxiv.org/abs/1411.4708, 2014.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  2010.  A. Bhaskara, D. Desai, and S. Srinivasan. Optimal hitting sets for combinatorial shapes. In 15th In- ternational Workshop, APPROX 2012, and 16th International Workshop, RANDOM 2012, pages 423-434, 2012.  C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete  distributions. CoRR, abs/1507.03558, 2015.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S.X. Chen and J.S. Liu. Statistical applications of the Poisson-Binomial and Conditional Bernoulli  Distributions. Statistica Sinica, 7:875-892, 1997.  Y. Chen and R. J. Samworth. Smoothed log-concave maximum likelihood estimation with applica-  tions. Statist. Sinica, 23:1373-1398, 2013.  14   DIAKONIKOLAS KANE STEWART  Acknowledgements. Part of this work was performed while I.D. and A.S. were at the University of Edinburgh, supported in part by EPSRC grant EP/L021749/1 and a Marie Curie Career Inte- gration Grant (CIG). The research of D.K. was supported in part by NSF Award CCF-1553288 (CAREER).  References  J. Acharya and C. Daskalakis. Testing poisson binomial distributions. In Proceedings of the Twenty- Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1829-1840, 2015.  J. Acharya, C. Daskalakis, and G. Kamath. Optimal testing for properties of distributions. CoRR,  abs/1507.05952, 2015a.  J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-  linear time. CoRR, abs/1506.00671, 2015b.  S. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings of the 33rd  Symposium on Theory of Computing, pages 247-257, 2001.  F. Balabdaoui and C. R. Doss.  Inference for a Mixture of Symmetric Distributions under Log-  Concavity. Available at http://arxiv.org/abs/1411.4708, 2014.  R.E. Barlow, D.J. Bartholomew, J.M. Bremner, and H.D. Brunk. Statistical Inference under Order  Restrictions. Wiley, New York, 1972.  M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, pages 103-112,  2010.  A. Bhaskara, D. Desai, and S. Srinivasan. Optimal hitting sets for combinatorial shapes. In 15th In- ternational Workshop, APPROX 2012, and 16th International Workshop, RANDOM 2012, pages 423-434, 2012.  C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld. Testing shape restrictions of discrete  distributions. CoRR, abs/1507.03558, 2015.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions  over discrete domains. In SODA, pages 1380-1394, 2013.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise  polynomial approximation. In STOC, pages 604-613, 2014a.  S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear  time using variable-width histograms. In NIPS, pages 1844-1852, 2014b.  S.X. Chen and J.S. Liu. Statistical applications of the Poisson-Binomial and Conditional Bernoulli  Distributions. Statistica Sinica, 7:875-892, 1997.  Y. Chen and R. J. Samworth. Smoothed log-concave maximum likelihood estimation with applica-  tions. Statist. Sinica, 23:1373-1398, 2013. PROPER LEARNING OF PBDS  H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of  observations. Ann. Math. Statist., 23:493-507, 1952.  M. Cryan, L. Goldberg, and P. Goldberg. Evolutionary trees can be learned in polynomial time in  the two state general Markov model. SIAM Journal on Computing, 31(2):375-397, 2002.  C. Daskalakis and G. Kamath. Faster and sample near-optimal algorithms for proper learning mix- In Proceedings of The 27th Conference on Learning Theory, COLT 2014,  tures of gaussians. pages 1183-1213, 2014.  75-84, 2009.  C. Daskalakis and C. Papadimitriou. On Oblivious PTAS\u2019s for Nash Equilibrium. In STOC, pages  C. Daskalakis and C. Papadimitriou. Sparse covers for sums of indicators. Probability Theory and  Related Fields, pages 1-27, 2014a.  C. Daskalakis and C. H. Papadimitriou. Computing equilibria in anonymous games.  In FOCS,  C. Daskalakis and C. H. Papadimitriou. Approximate nash equilibria in anonymous games. Journal  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In  pages 83-93, 2007.  of Economic Theory, 2014b.  SODA, pages 1371-1385, 2012a.  STOC, pages 709-728, 2012b.  C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning Poisson Binomial Distributions. In  C. Daskalakis, I. Diakonikolas, R. O\u2019Donnell, R.A. Servedio, and L. Tan. Learning Sums of Inde-  pendent Integer Random Variables. In FOCS, pages 217-226, 2013.  C. Daskalakis, I. Diakonikolas, and R. A. Servedio. Learning poisson binomial distributions. Algo-  rithmica, 72(1):316-357, 2015a.  C. Daskalakis, G. Kamath, and C. Tzamos. On the structure, covering, and learning of poisson  multinomial distributions. In FOCS, 2015b.  A. De. Beyond the central limit theorem: asymptotic expansions and pseudorandomness for com-  binatorial sums. In FOCS, 2015.  L. Devroye and L. Gy\u00a8orfi. Nonparametric Density Estimation: The L1 View. John Wiley & Sons,  L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in Statis-  1985.  tics, Springer, 2001.  I. Diakonikolas, D. M. Kane, and A. Stewart. Optimal learning via the fourier transform for sums  of independent integer random variables. CoRR, abs/1505.00662, 2015a.  I. Diakonikolas, D. M. Kane, and A. Stewart. The fourier transform of poisson multinomial distri-  butions and its algorithmic applications. CoRR, abs/1511.03592, 2015b. DIAKONIKOLAS KANE STEWART  C. R. Doss and J. A. Wellner. Global Rates of Convergence of the MLEs of Log-concave and  s-concave Densities. Available at http://arxiv.org/abs/1306.1438, 2013.  D. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algo-  rithms. Cambridge University Press, Cambridge, 2009.  L. D umbgen and K. Rufibach. Maximum likelihood estimation of a log-concave density and its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40-68, 2009.  J. Feldman, R. O\u2019Donnell, and R. Servedio. Learning mixtures of product distributions over discrete  domains. In Proc. 46th IEEE FOCS, pages 501-510, 2005.  V. Feldman. Hardness of proper learning (1988; pitt, valiant). In Encyclopedia of Algorithms. 2015.  Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In Proceedings of the  12th Annual COLT, pages 183-192, 1999.  F. Gao and J. A. Wellner. On the rate of convergence of the maximum likelihood estimator of a  k-monotone density. Science in China Series A: Mathematics, 52:1525-1538, 2009.  P. W. Goldberg and S. Turchetta. Query complexity of approximate equilibria in anonymous games.  CoRR, abs/1412.6455, 2014.  P. Gopalan, R. Meka, O. Reingold, and D. Zuckerman. Pseudorandom generators for combinatorial  shapes. In STOC, pages 253-262, 2011.  P. Gopalan, D. M. Kane, and R. Meka. Pseudorandomness via the discrete fourier transform. In  FOCS, 2015.  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the  American Statistical Association, 58:13-30, 1963.  A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In STOC,  M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cam-  pages 553-562, 2010.  bridge, MA, 1994.  M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. On the learnability of  discrete distributions. In Proc. 26th STOC, pages 273-282, 1994.  A. K. H. Kim and R. J. Samworth. Global rates of convergence in log-concave density estimation.  Available at http://arxiv.org/abs/1404.2298, 2014.  J. Li and L. Schmidt. A nearly optimal and agnostic algorithm for properly learning a mixture of k  gaussians, for any constant k. CoRR, abs/1506.01367, 2015.  S.D. Poisson. Recherches sur la Probabilit`e des jugements en mati\u00b4e criminelle et en mati\u00b4ere civile.  Bachelier, Paris, 1837.  J. Renegar. On the computational complexity and geometry of the first-order theory of the reals. J.  Symb. Comput., 13(3):255-352, 1992a. PROPER LEARNING OF PBDS  J. Renegar. On the computational complexity of approximating solutions for real algebraic formulae.  SIAM J. Comput., 21(6):1008-1025, 1992b.  C. Riener. Symmetries in Semidefinite and Polynomial Optimization. PhD thesis, Johann Wolfgang  Goethe-Universitat, 2011.  1992.  D.W. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. Wiley, New York,  B. W. Silverman. Density Estimation. Chapman and Hall, London, 1986.  A. T. Suresh, A. Orlitsky, J. Acharya, and A. Jafarpour. Near-optimal-sample estimators for spher- ical gaussian mixtures. In Advances in Neural Information Processing Systems (NIPS), pages 1395-1403, 2014.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In Proceedings of the 43rd Annual Symposium on Foundations of Computer Science, pages 113-122, 2002.  G. Walther.  Inference and modeling with log-concave distributions. Statistical Science, 24(3):  319-327, 2009.  "}, "Asymptotic behavior of \\ell_p-based Laplacian regularization in semi-supervised learning": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Asymptotic behavior of \\ell_p-based Laplacian regularization in semi-supervised learning", "abstract": "Given a weighted graph with N vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes n labeled vertices, and the task is to label the remaining ones. We present a theoretical study of \\ell_p-based Laplacian regularization under a d-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as N grows to infinity while n stays constant; the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate \\widehatf.  From this formulation we derive several predictions on the limiting behavior the function \\fhat, including (a) a phase transition in its smoothness at the threshold p = d + 1; and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution P.  Thus, over the range p \u2264d, the function estimate \\widehatf is degenerate and \u201cspiky,\u201d whereas for p\u2265d+1, the function estimate \\fhat is smooth.  We show that the effect of the underlying density vanishes monotonically with p, such that in the limit p = \u221e, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate \\widehatf is independent of the distribution P. Under the assumption of semi-supervised smoothness, ignoring P can lead to poor statistical performance; in particular, we construct a specific example for d=1 to demonstrate that p=2 has lower risk than p=\u221edue to the former penalty adapting to P and the latter ignoring it.  We also provide simulations that verify the accuracy of our predictions for finite sample sizes.  Together, these properties show that p = d+1 is an optimal choice, yielding a function estimate \\fhat that is both smooth and non-degenerate, while remaining maximally sensitive to P.", "pdf_url": "http://proceedings.mlr.press/v49/elalaoui16.pdf", "keywords": ["(cid:96)p-based Laplacian regularization", "semi-supervised learning", "asymptotic behavior", "geometric random graph model", "absolutely minimal Lipschitz extension", "phase transition"], "reference": "2003.  Robert A Adams and John JF Fournier. Sobolev spaces, volume 140. Academic press, 2003.  Morteza Alamgir and Ulrike V Luxburg. Phase transition in the family of p-resistances. In Advances  in Neural Information Processing Systems, pages 379\u2013387, 2011.  S Amghibech. Eigenvalues of the discrete p-Laplacian for graphs. Ars Combinatoria, 67:283\u2013302,  Rie Kubota Ando and Tong Zhang. Learning on graph with Laplacian regularization. Advances in  neural information processing systems, 19:25, 2007.  15   (cid:96)2, (cid:96)p AND (cid:96)\u221e  labeled data. One can see that under our asymptotics, we can make the same conclusions about the unconstrained solution. Hence, the conclusions of this paper do not hinge on the fact that we modeled the problem with equality constraints, and do apply more generally. For completeness, we outline this argument in "}, "The Power of Depth for Feedforward Neural Networks": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "The Power of Depth for Feedforward Neural Networks", "abstract": "We show that there is a simple (approximately radial) function on \\mathbbR^d, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth \u2013 even if increased by 1 \u2013 can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.", "pdf_url": "http://proceedings.mlr.press/v49/eldan16.pdf", "keywords": ["Neural networks", "Depth vs Width", "Function approximation", "Fourier transform"], "reference": "ESANN, 2014.  OS is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Sci- ence Foundation grant 425/13. We thank James Martens and the anonymous COLT 2016 reviewers for several helpful comments.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  M. Bianchini and F. Scarselli. On the complexity of shallow and deep neural network classifiers. In  N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis.  arXiv preprint arXiv:1509.05009, 2015.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 2(4):303-314, 1989.  C. Debao. Degree of approximation by superpositions of a sigmoidal function. Approximation  Theory and its Applications, 9(3):17-28, 1993.  O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In NIPS, pages 666-674,  2011.  DLMF. NIST Digital Library of Mathematical Functions. http://dlmf.nist.gov/, Release 1.0.10 of  2015-08-07, 2015. URL http://dlmf.nist.gov/.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Loukas Grafakos and Gerald Teschl. On fourier transforms of radial functions and distributions.  Journal of Fourier Analysis and Applications, 19(1):167-179, 2013.  Andr\u00b4as Hajnal, Wolfgang Maass, Pavel Pudl\u00b4ak, M\u00b4ari\u00b4o Szegedy, and Gy\u00a8orgy Tur\u00b4an. Threshold circuits of bounded depth. Journal of Computer and System Sciences, 46(2):129-154, 1993.  J. H\u02daastad. Almost optimal lower bounds for small depth circuits. In Proceedings of the eighteenth  annual ACM symposium on Theory of computing, pages 6-20. ACM, 1986.  K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approx-  imators. Neural networks, 2(5):359-366, 1989.  John K. Hunter and Bruno Nachtergaele. Applied analysis. World Scientific Publishing Co., Inc.,  River Edge, NJ, 2001.  Ilia Krasikov. Approximations for the bessel and airy functions with an explicit error term. LMS  Journal of Computation and Mathematics, 17(01):209-225, 2014.  22   ELDAN SHAMIR  Acknowledgments  References  ESANN, 2014.  OS is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Sci- ence Foundation grant 425/13. We thank James Martens and the anonymous COLT 2016 reviewers for several helpful comments.  Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine  Learning, 14(1):115-133, 1994.  M. Bianchini and F. Scarselli. On the complexity of shallow and deep neural network classifiers. In  N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor analysis.  arXiv preprint arXiv:1509.05009, 2015.  G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,  signals and systems, 2(4):303-314, 1989.  C. Debao. Degree of approximation by superpositions of a sigmoidal function. Approximation  Theory and its Applications, 9(3):17-28, 1993.  O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In NIPS, pages 666-674,  2011.  DLMF. NIST Digital Library of Mathematical Functions. http://dlmf.nist.gov/, Release 1.0.10 of  2015-08-07, 2015. URL http://dlmf.nist.gov/.  Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.  Neural networks, 2(3):183-192, 1989.  Loukas Grafakos and Gerald Teschl. On fourier transforms of radial functions and distributions.  Journal of Fourier Analysis and Applications, 19(1):167-179, 2013.  Andr\u00b4as Hajnal, Wolfgang Maass, Pavel Pudl\u00b4ak, M\u00b4ari\u00b4o Szegedy, and Gy\u00a8orgy Tur\u00b4an. Threshold circuits of bounded depth. Journal of Computer and System Sciences, 46(2):129-154, 1993.  J. H\u02daastad. Almost optimal lower bounds for small depth circuits. In Proceedings of the eighteenth  annual ACM symposium on Theory of computing, pages 6-20. ACM, 1986.  K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approx-  imators. Neural networks, 2(5):359-366, 1989.  John K. Hunter and Bruno Nachtergaele. Applied analysis. World Scientific Publishing Co., Inc.,  River Edge, NJ, 2001.  Ilia Krasikov. Approximations for the bessel and airy functions with an explicit error term. LMS  Journal of Computation and Mathematics, 17(01):209-225, 2014. THE POWER OF DEPTH FOR FEEDFORWARD NEURAL NETWORKS  W. Maass, G. Schnitger, and E. Sontag. A comparison of the computational power of sigmoid and boolean threshold circuits. In V. P. Roychowdhury, K. Y. Siu, and A. Orlitsky, editors, Theoretical Advances in Neural Computation and Learning, pages 127-151. Springer, 1994.  J. Martens and V. Medabalimi. On the expressive efficiency of sum product networks. arXiv preprint  arXiv:1411.7717, 2014.  James Martens. Private Communication, 2015.  James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational  efficiency of restricted boltzmann machines. In NIPS, pages 2877-2885, 2013.  G. F Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems, pages 2924-2932, 2014.  I. Parberry. Circuit complexity and neural networks. MIT press, 1994.  R. Pascanu, G. Montufar, and Y. Bengio. On the number of inference regions of deep feed forward  networks with piece-wise linear activations. arXiv preprint arXiv, 1312, 2013.  B. Rossman, R. Servedio, and L.-Y. Tan. An average-case depth hierarchy theorem for boolean  circuits. In FOCS, 2015.  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In D. E. Rumelhart and J.L. McClelland, editors, Parallel distributed Processing, volume 1. MIT Press, 1986.  A. Shpilka and A. Yehudayoff. Arithmetic circuits: A survey of recent results and open questions.  Foundations and Trends R(cid:13) in Theoretical Computer Science, 5(3-4):207-388, 2010.  Sasha Sodin. Tail-Sensitive Gaussian Asymptotics for Marginals of Concentrated Measures in High Dimension. In Vitali D. Milman and Gideon Schechtman, editors, Geometric Aspects of Func- tional Analysis, volume 1910 of Lecture Notes in Mathematics, pages 271-295. Springer Berlin Heidelberg, 2007.  M. Telgarsky.  Representation benefits of deep feedforward networks.  arXiv preprint  arXiv:1509.08101, 2015. ELDAN SHAMIR  "}, "Online Learning and Blackwell Approachability in Quitting Games": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Online Learning and Blackwell Approachability in Quitting Games", "abstract": "We consider  the sequential decision problem known as regret minimization, or more precisely its generalization to the  vectorial or multi-criteria  setup called Blackwell approachability. We  assume that   Nature, the decision maker, or both, might have some quitting (or terminating) actions so that the stream of payoffs is constant whenever they are chosen. We call those environments  \u201cquitting games\u201d. We characterize convex target sets \\cC that are Blackwell approachable, in the sense that the decision maker has a policy ensuring that the expected average vector payoff converges to \\cC at some given horizon known in advance. Moreover, we also compare these results to the cases where  the horizon is not known and show that, unlike in standard online learning literature, the necessary or sufficient conditions for the anytime version of this problem are drastically different than those for the fixed horizon.", "pdf_url": "http://proceedings.mlr.press/v49/flesch16.pdf", "keywords": ["Online Learning", "Blackwell Approachability", "Markov Decision Process", "Absorbing Games"]}, "Spectral thresholds in the bipartite stochastic block model": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Spectral thresholds in the bipartite stochastic block model", "abstract": "We consider a bipartite stochastic block model on vertex sets V_1 and V_2, with planted partitions in each, and ask at what densities efficient algorithms can recover the partition of the smaller vertex set. When |V_2| \u226b|V_1|, multiple thresholds emerge.  We first locate a sharp threshold for detection of the partition, in the sense of the results of Mossel, Neeman and Sly and  Massoulie for the stochastic block model.  We then show that at a higher edge density, the singular vectors of the rectangular biadjacency matrix exhibit a localization / delocalization phase transition, giving recovery above the threshold and no recovery below.  Nevertheless, we propose a simple spectral algorithm,  Diagonal Deletion SVD, which recovers the partition at a nearly optimal edge density. The bipartite stochastic block model studied here was used by Feldman, Perkins, Vempala to give a unified algorithm for recovering planted partitions and assignments in random hypergraphs and random k-SAT formulae respectively.  Our results give the best known bounds for the clause density at which solutions can be found efficiently in these models as well as showing a barrier to further improvement via this reduction to the bipartite block model.", "pdf_url": "http://proceedings.mlr.press/v49/florescu16.pdf", "keywords": ["Stochastic block model", "spectral algorithms", "random matrices", "planted satisfiability"], "reference": "Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. Information Theory, IEEE Transactions on, 62(1):471-487, 2016.  Benny Applebaum. Cryptographic hardness of random local functions-survey. In Theory of Cryp-  tography, pages 599-599. Springer, 2013.  Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120- 135, 2012.  Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 1997.  Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and newmangirvan and other modularities. Proceedings of the National Academy of Sciences, 106(50):21068-21073, 2009.  Andrej Bogdanov and Youming Qiao. On the security of goldreich\u2019s one-way function. In Approx- imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 392-405. Springer, 2009.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1347-1357. IEEE, 2015.  15   SPECTRAL THRESHOLDS IN THE BIPARTITE STOCHASTIC BLOCK MODEL  Now Lemma 4 says that if vi is the ith eigenvector of DV \u2212 EDV + B, then there is a vector u in the span of the first r eigenvectors of DV \u2212 EDV so that  (cid:107)vi \u2212 u(cid:107) \u2264 C  n1n2p2 + n1/2 n2p log n1  \u221a  1 n1/2 2 p  (cid:18)  = O  \u221a  (cid:19)  .  1 log n1  The span of the first r eigenvectors of DV \u2212 EDV is supported on only r coordinates, so u is  far from \u03c3 = \u03c3/  n1:  \u221a  (cid:107)u \u2212 \u03c3(cid:107) \u2265  (cid:113)  2 \u2212 2(cid:112)r/n1 =  \u221a  2 \u2212 O(1/(cid:112)log n1). \u221a  |vi \u00b7 \u03c3| = O(1/  log n1). This proves  By the triangle inequality, vi must also be far from \u03c3: Theorem 4.  We thank the Institute for Mathematics and its Applications (IMA) in Minneapolis, where part of this work was done, for its support and hospitality.  Acknowledgments  References  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. Information Theory, IEEE Transactions on, 62(1):471-487, 2016.  Benny Applebaum. Cryptographic hardness of random local functions-survey. In Theory of Cryp-  tography, pages 599-599. Springer, 2013.  Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis, 111:120- 135, 2012.  Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 1997.  Peter J. Bickel and Aiyou Chen. A nonparametric view of network models and newmangirvan and other modularities. Proceedings of the National Academy of Sciences, 106(50):21068-21073, 2009.  Andrej Bogdanov and Youming Qiao. On the security of goldreich\u2019s one-way function. In Approx- imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 392-405. Springer, 2009.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1347-1357. IEEE, 2015. FLORESCU PERKINS  T.N. Bui, S. Chaudhuri, F.T. Leighton, and M. Sipser. Graph bisection algorithms with good average  case behavior. Combinatorica, 7(2):171-191, 1987.  Cristina Butucea, Yuri I Ingster, and Irina A Suslina. Sharp variable selection of a sparse submatrix  in a high-dimensional noisy matrix. ESAIM: Probability and Statistics, 19:115-134, 2015.  Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability  & Computing, 19(2):227, 2010.  Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted partition  model. Random Struct. Algorithms, 18(2):116-140, March 2001.  Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii.  SIAM Journal on Numerical Analysis, 7(1):1-46, 1970.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical Review E, 84(6):066106, 2011.  M.E Dyer and A.M Frieze. The solution of some random np-hard problems in polynomial expected  time. Journal of Algorithms, 10(4):451 - 489, 1989.  William Evans, Claire Kenyon, Yuval Peres, and Leonard J Schulman. Broadcasting on trees and  the ising model. Annals of Applied Probability, pages 410-433, 2000.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability In STOC 2015: 47th Annual Symposium on the Theory of  problems with planted solutions. Computing, 2015a.  Vitaly Feldman, Will Perkins, and Santosh Vempala. Subsampled power iteration: a new algorithm  for block models and planted csp\u2019s. In NIPS, 2015b.  Oded Goldreich. Candidate one-way functions based on expander graphs. IACR Cryptology ePrint  Archive, 2000:63, 2000.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidef- inite programming. In Information Theory (ISIT), 2015 IEEE International Symposium on, pages 1442-1446. IEEE, 2015a.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Submatrix localization via message passing. arXiv  preprint arXiv:1510.09219, 2015b.  Paul W. Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps.  Social Networks, 5(2):109-137, June 1983.  Mark Jerrum and Gregory B. Sorkin. The metropolis algorithm for graph bisection. Discrete Applied  Mathematics, 82(13):155 - 175, 1998.  Florent Krzakala, Marc M\u00b4ezard, and Lenka Zdeborov\u00b4a. Reweighted belief propagation and quiet planting for random k-sat. Journal on Satisfiability, Boolean Modeling and Computation, 8: 149-171, 2014. SPECTRAL THRESHOLDS IN THE BIPARTITE STOCHASTIC BLOCK MODEL  Daniel B Larremore, Aaron Clauset, and Abigail Z Jacobs. Efficiently inferring community struc-  ture in bipartite networks. Physical Review E, 90(1):012805, 2014.  Marc Lelarge, Laurent Massouli\u00b4e, and Jiaming Xu. Reconstruction in the labeled stochastic block  model. In Information Theory Workshop (ITW), 2013 IEEE, pages 1-5. IEEE, 2013.  Russell Lyons. Random walks and percolation on trees. The annals of Probability, pages 931-958,  1990.  Laurent Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. In STOC  2014: 46th Annual Symposium on the Theory of Computing, pages 1-10, 2014.  Frank McSherry. Spectral partitioning of random graphs.  In Foundations of Computer Science,  2001. Proceedings. 42nd IEEE Symposium on, pages 529-537. IEEE, 2001.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  arXiv preprint arXiv:1311.4115, 2013.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition  model. Probability Theory and Related Fields, pages 1-31, 2014.  Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for the planted bisection model. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 69-75. ACM, 2015.  Mark EJ Newman. Scientific collaboration networks. i. network construction and fundamental re-  sults. Physical review E, 64(1):016131, 2001.  Ryan O\u2019Donnell and David Witmer. Goldreich\u2019s prg: Evidence for near-optimal polynomial stretch.  In CCC, 2014.  Robin Pemantle and Yuval Peres. The critical ising model on trees, concave recursions and nonlinear  capacity. The Annals of Probability, 38(1):184-206, 2010.  Tom A. B. Snijders and Krzysztof Nowicki. Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure. Journal of Classification, 14(1):75-100, January 1997.  Tao Zhou, Jie Ren, Mat\u00b4u\u02c7s Medo, and Yi-Cheng Zhang. Bipartite network projection and personal  recommendation. Physical Review E, 76(4):046115, 2007. "}, "Online Sparse Linear Regression": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Online Sparse Linear Regression", "abstract": "We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an \\em inefficient algorithm that obtains regret bounded by \\tildeO(\\sqrtT) after T prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T^1-\u03b4) for any constant \u03b4> 0 unless \\textsfNP \u2286\\textsfBPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "pdf_url": "http://proceedings.mlr.press/v49/foster16.pdf", "keywords": [], "reference": "Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992. ISBN 0-471-53588-5.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Shai Ben-David and Eli Dichterman. Learning with restricted focus of attention. In COLT, pages  287-296, 1993.  Andreas Birkendorf, Eli Dichterman, Jeffrey C. Jackson, Norbert Klasner, and Hans-Ulrich Simon. On restricted-focus-of-attention learnability of boolean functions. Machine Learning, 30(1):89- 123, 1998.  Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Efficient learning with partially  observed attributes. Journal of Machine Learning Research, 12:2857-2878, 2011.  10   FOSTER KALE KARLOFF  We can now extend the hardness results to the parameter settings k = O(d(cid:15)) for any (cid:15) \u2208 (0, 1) and k(cid:48) = (cid:98)D ln(d)k(cid:99) either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = (cid:15) if (cid:15) is close enough to 1, or if (cid:15) is small, by adding O(d1/(cid:15)) all-zero columns to the matrix M\u03c6. The two combinatorial properties of M\u03c6 in Reduction 1 are clearly still satisfied, and the proof of Theorem 2 goes through.  5.2. Allowing \ufb02exibility in choosing features to observe  While the hardness result described above applies in the setting where the online learner is restricted to choose k(cid:48) features to observe in each round, our hardness proof easily extends to more \ufb02exible situations such as that in which the online learner is only required to keep the average number of features observed over the rounds bounded by k(cid:48).  To see this, suppose that k(cid:48) is such that it is NP-hard to distinguish between cases where either the instance has a set cover of size k or no collection of 2k(cid:48) sets covers all the elements. Then consider an algorithm for the online sparse regression problem which chooses at most k(cid:48) features on average over the rounds. By Markov\u2019s inequality, on at least half the rounds, the algorithm chooses at most 2k(cid:48) features, and in these rounds, the arguments in the proof of Theorem 2 imply that the algorithm incurs at least constant loss in expectation, leading to linear (in T ) total loss in expectation. This suffices to show hardness, since any sublinear regret algorithm for the problem can then be used to distinguish between the two cases, exactly as in the proof of Theorem 2.  6. Conclusions  In this paper, we prove that minimizing regret in the online sparse regression problem is compu- tationally hard even if the learner is allowed access to many more features than the comparator, a sparse linear regressor. We complement this result by giving an inefficient no-regret algorithm.  The main open question remaining from this work is what extra assumptions can one make on the examples arriving online to make the problem tractable. Note that the sequence of examples constructed in the lower bound proof is i.i.d., so clearly stronger assumptions than that are necessary to obtain any efficient algorithms.  References  Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992. ISBN 0-471-53588-5.  Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-  algorithm and applications. Theory of Computing, 8(1):121-164, 2012.  Shai Ben-David and Eli Dichterman. Learning with restricted focus of attention. In COLT, pages  287-296, 1993.  Andreas Birkendorf, Eli Dichterman, Jeffrey C. Jackson, Norbert Klasner, and Hans-Ulrich Simon. On restricted-focus-of-attention learnability of boolean functions. Machine Learning, 30(1):89- 123, 1998.  Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Efficient learning with partially  observed attributes. Journal of Machine Learning Research, 12:2857-2878, 2011. ONLINE SPARSE LINEAR REGRESSION  Irit Dinur and David Steurer. Analytical approach to parallel repetition. In STOC, pages 624-633,  2014.  709, 2015.  Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization  in the bandit setting: gradient descent without a gradient. In SODA, pages 385-394, 2005.  Dean Foster, Howard Karloff, and Justin Thaler. Variable selection is hard. In COLT, pages 696-  Elad Hazan and Tomer Koren. Linear regression with limited observation. In ICML, 2012.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex  optimization. Machine Learning, 69(2-3):169-192, 2007.  Satyen Kale. Open problem: Efficient online sparse regression. In COLT, pages 1299-1301, 2014.  Doron Kukliansky and Ohad Shamir. Attribute efficient linear regression with distribution-  dependent sampling. In ICML, pages 153-161, 2015.  B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing, 25(2):227-  234, 1995.  Navid Zolghadr, G\u00b4abor Bart\u00b4ok, Russell Greiner, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online  learning with costly features and labels. In NIPS, pages 1241-1249, 2013. "}, "Preference-based Teaching": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Preference-based Teaching", "abstract": "We introduce a new model of teaching named \u201cpreference-based teaching\u201d and a corresponding complexity parameter\u2014the preference-based teaching dimension (PBTD)\u2014representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well-known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t.\u00a0a given closure operator, including various classes related to linear sets over \\mathbbN_0 (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces (and some other geometric classes). On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD.", "pdf_url": "http://proceedings.mlr.press/v49/gao16.pdf", "keywords": ["teaching dimension", "preference relation", "recursive teaching dimension"], "reference": "Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching di- mension, VC-dimension and sample compression. Journal of Machine Learning Research, 15: 3107-3131, 2014.  Ziyuan Gao, Hans U. Simon, and Sandra Zilles. On the teaching complexity of linear sets.  In Proceedings of the 26th International Conference on Algorithmic Learning Theory, pages 102- 116, 2015.  13   PREFERENCE-BASED TEACHING  Proof The proof is very simple. Choose the following hierarchical preference relation. With first priority choose a union of as few as possible boxes. With second priority, prefer a concept R (= union of boxes) over all concepts R(cid:48) that are proper subsets of R. Fig. 3 shows how the teaching set for each single box of the target concept should be chosen. If the student gets (up to) k teaching sets of this kind, she will put the largest possible box around each positive example. The negative examples at the border of the box ensure that these boxes do not become too large.  10. Conclusions  Preference-based teaching uses the natural notion of preference relation to extend the classical teaching model. The resulting model is (i) more powerful than the classical one, (ii) resolves dif- ficulties with the recursive teaching model in the case of infinite concept classes, and (iii) is at the same time free of coding tricks even according to the definition by Goldman and Mathias (1996). Our examples of algebraic and geometric concept classes demonstrate that preference-based teach- ing can be achieved very efficiently with naturally defined teaching sets and based on intuitive preference relations such as inclusion. We believe that further studies of the PBTD will provide in- sights into structural properties of concept classes that render them easy or hard to learn in a variety of formal learning models.  We have shown that spanning sets lead to a general-purpose construction for preference-based teaching sets of only positive examples. While this result is fairly obvious, it provides further justification of the model of preference-based teaching, since the teaching sets it yields are often intuitively exactly those a teacher would choose in the classroom (for instance, one would repre- sent convex polygons by their vertices). It should be noted, too, that it can sometimes be difficult to establish whether the upper bound on PBTD obtained this way is tight, or whether the use of negative examples or preference relations other than inclusion yield smaller teaching sets. A further challenge is posed by the study of unions of geometric objects such as axis-aligned boxes. There seems to be no obvious way of combining preference-based teaching sets for a number of objects to a preference-based teaching set for their union, and it is unclear how to choose preference relations in the best possible way. Generally, the choice of preference relation provides a degree of freedom that increases the power of the teacher but also increases the difficulty of establishing lower bounds on the number of examples required for teaching.  Acknowledgements. Sandra Zilles was supported by the Natural Sciences and Engineering Re- search Council of Canada (NSERC), in the Discovery Grant and Canada Research Chairs programs.  References  Thorsten Doliwa, Gaojian Fan, Hans Ulrich Simon, and Sandra Zilles. Recursive teaching di- mension, VC-dimension and sample compression. Journal of Machine Learning Research, 15: 3107-3131, 2014.  Ziyuan Gao, Hans U. Simon, and Sandra Zilles. On the teaching complexity of linear sets.  In Proceedings of the 26th International Conference on Algorithmic Learning Theory, pages 102- 116, 2015. GAO RIES SIMON ZILLES  Sally A. Goldman and Michael J. Kearns. On the complexity of teaching. Journal of Computer and  System Sciences, 50(1):20-31, 1995.  Sally A. Goldman and H. David Mathias. Teaching a smarter learner. Journal of Computer and  System Sciences, 52(2):255-267, 1996.  David Helmbold, Robert Sloan, and Manfred K. Warmuth. Learning nested differences of  intersection-closed concept classes. Machine Learning, 5:165-196, 1990.  Thomas J. Jech. The Axiom of Choice. North-Holland Pub. Co., Amsterdam, 1973.  Zeinab Mazadi, Ziyuan Gao, and Sandra Zilles. Distinguishing pattern languages with membership examples. In Proceedings of the 8th International Conference on Language and Automata Theory and Applications, pages 528-540, 2014.  Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff. Compressing and teaching for low VC-dimension. In Proceedings of the 56\u2019th Annual Symposium on the Foundations of Computer Science, pages 40-51, 2015.  Jos\u00b4e C. Rosales and Pedro A. Garc\u00b4\u0131a-S\u00b4anchez. Numerical Semigroups. Springer, 2009.  Ayumi Shinohara and Satoru Miyano. Teachability in computational learning. New Generation  Computing, 8(4):337-348, 1991.  Hans U. Simon and Sandra Zilles. Open problem: Recursive teaching dimension versus VC di- mension. In Proceedings of the 28th Annual Conference on Learning Theory, pages 1770-1772, 2015.  Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching  and learning. Journal of Machine Learning Research, 12:349-384, 2011.  "}, "Optimal Best Arm Identification with Fixed Confidence": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Optimal Best Arm Identification with Fixed Confidence", "abstract": "We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the \u2018Track-and-Stop\u2019 strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.", "pdf_url": "http://proceedings.mlr.press/v49/garivier16a.pdf", "keywords": ["multi-armed bandits", "best arm identification", "MDL"], "reference": "Y. Abbasi-Yadkori, D.P\u00b4al, and C.Szepesv\u00b4ari. Improved Algorithms for Linear Stochastic Bandits.  In Advances in Neural Information Processing Systems, 2011.  S. Agrawal and N. Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs. In  International Conference on Machine Learning (ICML), 2013.  A. Antos, V. Grover, and C. Szepesv\u00b4ari. Active learning in multi-armed bandits. In Algorithmic  Learning Theory, 2008.  A. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. Information Theory, IEEE Transactions on, 44(6):2743-2760, Oct 1998. ISSN 0018- 9448. doi: 10.1109/18.720554.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. Journal of Machine Learning  Research, 12:1587-1627, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  Antoine Chambaz, Aur\u00b4elien Garivier, and Elisabeth Gassiat. A MDL approach to HMM with pois- son and gaussian emissions. application to order identification. Journal of Statistical Planning and Inference, 139(3):962-977, 2009.  H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  R. Combes and A. Prouti`ere. Unimodal Bandits without Smoothness. Technical report, 2014.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Re- search, 7:1079-1105, 2006.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to In Advances in Neural Information Processing Systems,  Fixed Budget and Fixed Confidence. 2012.  Aurlien Garivier. Consistency of the unlimited BIC context tree estimator. IEEE Transactions on  Information Theory, 52(10):4630-4635, 2006.  T.L. Graves and T.L. Lai. Asymptotically Efficient adaptive choice of control laws in controlled  markov chains. SIAM Journal on Control and Optimization, 35(3):715-743, 1997.  Peter D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive Computation and Ma-  chine Learning). The MIT Press, 2007. ISBN 0262072815.  15   OPTIMAL BEST ARM IDENTIFICATION WITH FIXED CONFIDENCE  References  Y. Abbasi-Yadkori, D.P\u00b4al, and C.Szepesv\u00b4ari. Improved Algorithms for Linear Stochastic Bandits.  In Advances in Neural Information Processing Systems, 2011.  S. Agrawal and N. Goyal. Thompson Sampling for Contextual Bandits with Linear Payoffs. In  International Conference on Machine Learning (ICML), 2013.  A. Antos, V. Grover, and C. Szepesv\u00b4ari. Active learning in multi-armed bandits. In Algorithmic  Learning Theory, 2008.  A. Barron, J. Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. Information Theory, IEEE Transactions on, 44(6):2743-2760, Oct 1998. ISSN 0018- 9448. doi: 10.1109/18.720554.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv\u00b4ari. X-armed bandits. Journal of Machine Learning  Research, 12:1587-1627, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  Antoine Chambaz, Aur\u00b4elien Garivier, and Elisabeth Gassiat. A MDL approach to HMM with pois- son and gaussian emissions. application to order identification. Journal of Statistical Planning and Inference, 139(3):962-977, 2009.  H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics, 30(3):  755-770, 1959.  R. Combes and A. Prouti`ere. Unimodal Bandits without Smoothness. Technical report, 2014.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Re- search, 7:1079-1105, 2006.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to In Advances in Neural Information Processing Systems,  Fixed Budget and Fixed Confidence. 2012.  Aurlien Garivier. Consistency of the unlimited BIC context tree estimator. IEEE Transactions on  Information Theory, 52(10):4630-4635, 2006.  T.L. Graves and T.L. Lai. Asymptotically Efficient adaptive choice of control laws in controlled  markov chains. SIAM Journal on Control and Optimization, 35(3):715-743, 1997.  Peter D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive Computation and Ma-  chine Learning). The MIT Press, 2007. ISBN 0262072815. GARIVIER KAUFMANN  K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil\u2019UCB: an Optimal Exploration Algorithm for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014.  S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-  armed bandits. In International Conference on Machine Learning (ICML), 2012.  E. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Pro-  ceeding of the 26th Conference On Learning Theory., 2013.  E. Kaufmann, O. Capp\u00b4e, and A. Garivier. On the Complexity of A/B Testing. In Proceedings of the  27th Conference On Learning Theory, 2014.  E. Kaufmann, O. Capp\u00b4e, and A. Garivier. On the Complexity of Best Arm Identification in Multi-  Armed Bandit Models. Journal of Machine Learning Research (to appear), 2015.  Raphail E. Krichevsky and Victor K. Trofimov. The performance of universal encoding.  IEEE Transactions on Information Theory, 27(2):199-206, 1981. doi: 10.1109/TIT.1981.1056331. URL http://dx.doi.org/10.1109/TIT.1981.1056331.  T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6(1):4-22, 1985.  S. Magureanu, R. Combes, and A. Prouti`ere. Lipschitz Bandits: Regret lower bounds and optimal  algorithms. In Proceedings on the 27th Conference On Learning Theory, 2014.  S. Mannor and J. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit  Problem. Journal of Machine Learning Research, pages 623-648, 2004.  R. Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimiza-  tion and planning., volume 7. Foundations and Trends in Machine Learning, 2014.  J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465 - 471, 1978.  ISSN doi: http://dx.doi.org/10.1016/0005-1098(78)90005-5. URL http://www.  0005-1098. sciencedirect.com/science/article/pii/0005109878900055.  N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian Process Optimization in the Bandit Setting : No Regret and Experimental Design. In Proceedings of the International Conference on Machine Learning, 2010.  N.K. Vaidhyan and R. Sundaresan. Learning to detect an oddball target. arXiv:1508.05572, 2015.  Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. The context tree weighting method:  Basic properties. IEEE Transactions on Information Theory, 41:653-664, 1995. OPTIMAL BEST ARM IDENTIFICATION WITH FIXED CONFIDENCE  "}, "Maximin Action Identification: A New Bandit Framework for Games": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Maximin Action Identification: A New Bandit Framework for Games", "abstract": "We study an original problem of pure exploration in a strategic bandit model motivated by Monte Carlo Tree Search. It consists in identifying the best action in a game, when the player may sample random outcomes of sequentially chosen pairs of actions. We propose two strategies for the fixed-confidence setting: Maximin-LUCB, based on lower- and upper- confidence bounds; and Maximin-Racing, which operates by successively eliminating the sub-optimal actions. We discuss the sample complexity of both methods and compare their performance empirically. We sketch a lower bound analysis, and possible connections to an optimal algorithm.", "pdf_url": "http://proceedings.mlr.press/v49/garivier16b.pdf", "keywords": ["multi-armed bandit problems", "games", "best-arm identification", "racing", "LUCB"], "reference": "J-Y. Audibert, S. Bubeck, and R. Munos. Best Arm Identification in Multi-armed Bandits. In Proceedings  of the 23rd Conference on Learning Theory, 2010.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine  Learning, 47(2):235-256, 2002.  347(6218):145-149, January 2015.  M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold\u2019em poker is solved. Science,  C. Browne, E. Powley, D. Whitehouse, S. Lucas, P. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games,, 4(1):1-49, 2012.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, and G. Stoltz. Pure Exploration in Finitely Armed and Continuous Armed Bandits.  Theoretical Computer Science 412, 1832-1852, 412:1832-1852, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi- Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Research, 7: 1079-1105, 2006.  J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer, 1996.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to Fixed  Budget and Fixed Confidence. In Advances in Neural Information Processing Systems, 2012.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence.  In  Proceedings of the 29th Conference On Learning Theory (to appear), 2016.  S. Gelly, L. Kocsis, M. Schoenauer, M. Sebag, D. Silver, C. Szepesv\u00b4ari, and O. Teytaud. The grand challenge of computer go: Monte carlo tree search and extensions. Commun. ACM, 55(3):106-113, 2012.  K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck.  lil\u2019UCB: an Optimal Exploration Algorithm for  Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014.  14   GARIVIER, KAUFMANN AND KOOLEN  Acknowledgments  This work was partially supported by the CIMI (Centre International de Math\u00b4ematiques et d\u2019Informa- tique) Excellence program while Emilie Kaufmann visited Toulouse in November 2015. Garivier and Kaufmann acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA). Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO) under Veni grant 639.021.439.  References  J-Y. Audibert, S. Bubeck, and R. Munos. Best Arm Identification in Multi-armed Bandits. In Proceedings  of the 23rd Conference on Learning Theory, 2010.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine  Learning, 47(2):235-256, 2002.  347(6218):145-149, January 2015.  M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold\u2019em poker is solved. Science,  C. Browne, E. Powley, D. Whitehouse, S. Lucas, P. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games,, 4(1):1-49, 2012.  S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit  problems. Fondations and Trends in Machine Learning, 5(1):1-122, 2012.  S. Bubeck, R. Munos, and G. Stoltz. Pure Exploration in Finitely Armed and Continuous Armed Bandits.  Theoretical Computer Science 412, 1832-1852, 412:1832-1852, 2011.  O. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence  bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516-1541, 2013.  E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi- Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Research, 7: 1079-1105, 2006.  J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer, 1996.  V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identification: A Unified Approach to Fixed  Budget and Fixed Confidence. In Advances in Neural Information Processing Systems, 2012.  Aur\u00b4elien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence.  In  Proceedings of the 29th Conference On Learning Theory (to appear), 2016.  S. Gelly, L. Kocsis, M. Schoenauer, M. Sebag, D. Silver, C. Szepesv\u00b4ari, and O. Teytaud. The grand challenge of computer go: Monte carlo tree search and extensions. Commun. ACM, 55(3):106-113, 2012.  K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck.  lil\u2019UCB: an Optimal Exploration Algorithm for  Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014. MAXIMIN ACTION IDENTIFICATION  S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-armed  bandits. In International Conference on Machine Learning (ICML), 2012.  E. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Proceeding  of the 26th Conference On Learning Theory., 2013.  E. Kaufmann, O. Capp\u00b4e, and A. Garivier. On the Complexity of Best Arm Identification in Multi-Armed  Bandit Models. Journal of Machine Learning Research (to appear), 2015.  L. Kocsis and C. Szepesv\u00b4ari. Bandit based monte-carlo planning. In Proceedings of the 17th European Conference on Machine Learning, ECML\u201906, pages 282-293, Berlin, Heidelberg, 2006. Springer- Verlag. ISBN 3-540-45375-X, 978-3-540-45375-8.  O. Maron and A. Moore. The Racing algorithm: Model selection for Lazy learners. Artificial Intelligence  Review, 11(1-5):113-131, 1997.  R. Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization  and planning., volume 7(1). Foundations and Trends in Machine Learning, 2014.  D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbren- ner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484-489, 2016.  B. Szorenyi, G. Kedenburg, and R. Munos. Optimistic planning in markov decision processes using a  generative model. In Advances in Neural Information Processing Systems, 2014.  "}, "Semidefinite Programs for Exact Recovery of a Hidden Community": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Semidefinite Programs for Exact Recovery of a Hidden Community", "abstract": "We study a semidefinite programming (SDP) relaxation of the maximum likelihood estimation for exactly recovering a hidden community of cardinality K from an n \\times n symmetric data matrix A, where for distinct indices i,j, A_ij \u223cP if i, j are both in the community and A_ij \u223cQ otherwise, for two known probability distributions P and Q. We identify a  sufficient condition  and a  necessary condition for the success of SDP  for the general model. For both the Bernoulli case (P=\\rm Bern(p) and Q=\\rm Bern(q) with p>q) and the Gaussian case (P=\\mathcalN(\u03bc,1) and Q=\\mathcalN(0,1) with \u03bc>0), which correspond to the problem of planted dense subgraph recovery and submatrix localization respectively, the general results lead to the following findings: (1) If K=\u03c9( n /\\log n), SDP attains the information-theoretic recovery limits with sharp constants; (2) If K=\u0398(n/\\log n), SDP is order-wise optimal, but strictly suboptimal by a constant factor; (3) If K=o(n/\\log n) and K \\to \u221e, SDP is order-wise suboptimal. The same critical scaling for K is found to hold, up to constant factors, for the performance of SDP on the stochastic block model of n vertices partitioned into multiple communities of equal size K. A key ingredient in the proof of the necessary condition is a construction of a primal feasible solution based on random perturbation of the true cluster matrix.", "pdf_url": "http://proceedings.mlr.press/v49/hajek16.pdf", "keywords": ["Semidefinite programming relaxations", "Planted dense subgraph recovery", "Submatrix localization", "Stochastic block model"], "reference": "N. Agarwal, A. S. Bandeira, K. Koiliaris, and A. Kolla. Multisection in the stochastic block model  using semidefinite programming. arXiv 1507.02323, July 2015.  N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13(3-4):457-466, 1998.  Z.D. Bai and Y.Q. Yin. Necessary and sufficient conditions for almost sure convergence of the largest  eigenvalue of a Wigner matrix. The Annals of Probability, 16(4):1729-1741, 1988.  A. S. Bandeira and R. van Handel. Sharp nonasymptotic bounds on the norm of random matrices  with independent entries. arXiv 1408.6185, 2014.  A.S. Bandeira. Random Laplacian matrices and convex relaxations. arXiv 1504.03987, April, 2015.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Oxford University Press, 2013.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY,  USA, 2004.  C. Butucea, Y.I. Ingster, and I. Suslina. Sharp variable selection of a sparse submatrix in a high-  dimensional noisy matrix. ESAIM: Probability and Statistics, 19:115-134, June 2015.  T. T. Cai, T. Liang, and A. Rakhlin. Computational and statistical boundaries for submatrix localiza-  tion in a large noisy matrix. arXiv:1502.01988, Feb. 2015.  Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix localiza- tion with a growing number of clusters and submatrices. In Proceedings of ICML 2014 (Also arXiv:1402.1267), Feb 2014.  K.-L. Chung and P. Erd\u00a8os. On the application of the Borel-Cantelli lemma. Transactions of the  American Mathematical Society, pages 179-186, 1952.  H.A. David and H.N. Nagaraja. Order Statistics. Wiley-Interscience, Hoboken, New Jersey, USA, 3  edition, 2003.  K.R. Davidson and S. Szarek. Local operator theory, random matrices and Banach spaces. In W.B. Johnson and J. Lindenstrauss, editors, Handbook on the Geometry of Banach Spaces, volume 1, pages 317-366. Elsevier Science, 2001.  Y. Deshpande and A. Montanari. Finding hidden cliques of size (cid:112)N/e in nearly linear time.  Foundations of Computational Mathematics, 15(4):1069-1128, August 2015a.  Y. Deshpande and A. Montanari. Improved sum-of-squares lower bounds for hidden clique and  hidden submatrix problems. In Proceedings of COLT 2015, pages 523-562, June 2015b.  U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph.  Random Structures & Algorithms, 16(2):195-208, 2000.  29   SDP FOR HIDDEN COMMUNITY  References  N. Agarwal, A. S. Bandeira, K. Koiliaris, and A. Kolla. Multisection in the stochastic block model  using semidefinite programming. arXiv 1507.02323, July 2015.  N. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. Random  Structures and Algorithms, 13(3-4):457-466, 1998.  Z.D. Bai and Y.Q. Yin. Necessary and sufficient conditions for almost sure convergence of the largest  eigenvalue of a Wigner matrix. The Annals of Probability, 16(4):1729-1741, 1988.  A. S. Bandeira and R. van Handel. Sharp nonasymptotic bounds on the norm of random matrices  with independent entries. arXiv 1408.6185, 2014.  A.S. Bandeira. Random Laplacian matrices and convex relaxations. arXiv 1504.03987, April, 2015.  S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of  independence. Oxford University Press, 2013.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY,  USA, 2004.  C. Butucea, Y.I. Ingster, and I. Suslina. Sharp variable selection of a sparse submatrix in a high-  dimensional noisy matrix. ESAIM: Probability and Statistics, 19:115-134, June 2015.  T. T. Cai, T. Liang, and A. Rakhlin. Computational and statistical boundaries for submatrix localiza-  tion in a large noisy matrix. arXiv:1502.01988, Feb. 2015.  Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix localiza- tion with a growing number of clusters and submatrices. In Proceedings of ICML 2014 (Also arXiv:1402.1267), Feb 2014.  K.-L. Chung and P. Erd\u00a8os. On the application of the Borel-Cantelli lemma. Transactions of the  American Mathematical Society, pages 179-186, 1952.  H.A. David and H.N. Nagaraja. Order Statistics. Wiley-Interscience, Hoboken, New Jersey, USA, 3  edition, 2003.  K.R. Davidson and S. Szarek. Local operator theory, random matrices and Banach spaces. In W.B. Johnson and J. Lindenstrauss, editors, Handbook on the Geometry of Banach Spaces, volume 1, pages 317-366. Elsevier Science, 2001.  Y. Deshpande and A. Montanari. Finding hidden cliques of size (cid:112)N/e in nearly linear time.  Foundations of Computational Mathematics, 15(4):1069-1128, August 2015a.  Y. Deshpande and A. Montanari. Improved sum-of-squares lower bounds for hidden clique and  hidden submatrix problems. In Proceedings of COLT 2015, pages 523-562, June 2015b.  U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph.  Random Structures & Algorithms, 16(2):195-208, 2000. HAJEK WU XU  U. Feige and R. Krauthgamer. The probable value of the Lov\u00b4asz-Schrijver relaxations for maximum  independent set. SIAM Journal on Computing, 32(2):345-370, 2003.  B. Hajek, Y. Wu, and J. Xu. Computational lower bounds for community detection on random graphs.  In Proceedings of COLT 2015, June 2015a.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite programming:  Extensions. arXiv 1502.07738, Feb. 2015b.  B. Hajek, Y. Wu, and J. Xu. Submatrix localization via message passing. arXiv 1510.09219, October  2015c.  B. Hajek, Y. Wu, and J. Xu. Recovering a hidden community beyond the spectral limit in  O(|E| log\u2217 |V |) time. arXiv 1510.02786, October 2015d.  B. Hajek, Y. Wu, and J. Xu. Information limits for recovering a hidden community. arXiv 1509.07859,  September 2015e.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, May 2016. (arXiv 1412.6156 Nov. 2014).  P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks,  5(2):109-137, 1983.  S. B. Hopkins, P. K. Kothari, and A. Potechin. SoS and planted clique: Tight analysis of MPW moments at all degrees and an optimal lower bound at degree four. arXiv 1507.05230, July 2015.  R.M. Karp. Reducibility among combinatorial problems. In R.E. Miller and J.W. Thacher, editors, Proceedings of a Symposium on the Complexity of Computer Computations, pages 85-103. Plenum Press, March 1972.  M. Kolar, S. Balakrishnan, A. Rinaldo, and A. Singh. Minimax localization of structural information  in large noisy matrices. In Advances in Neural Information Processing Systems, 2011.  R. Krauthgamer, B. Nadler, and D. Vilenchik. Do semidefinite relaxations solve sparse PCA up to  the information limit? The Annals of Statistics, 43(3):1300-1322, June 2015.  R. Lata\u0142a. Some estimates of norms of random matrices. Proceedings of the American Mathematical  Society, 133(5):1273-1282, 2005.  C. M. Le and R. Vershynin. Concentration and regularization of random graphs. arXiv:1506.00669,  June 2015.  43(3):1089-1116, 2015.  Z. Ma and Y. Wu. Computational barriers in minimax submatrix detection. The Annals of Statistics,  F. McSherry. Spectral partitioning of random graphs. In 42nd IEEE Symposium on Foundations of  Computer Science, pages 529 - 537, Oct. 2001. SDP FOR HIDDEN COMMUNITY  R. Meka, A. Potechin, and A. Wigderson. Sum-of-squares lower bounds for planted clique. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC \u201915, pages 87-96, New York, NY, USA, 2015. ACM.  A. Montanari. Finding one community in a sparse random graph. Journal of Statistical Physics, 161  (2):273-299, 2015. arXiv 1502.05680.  A. Montanari and S. Sen. Semidefinite programs on sparse random graphs. arXiv:1504.05910, April,  W. Perry and A.S. Wein. A semidefinite program for unbalanced multisection in the stochastic block  model. arXiv 1507.05605, July 2015.  P. Raghavendra and T. Schramm. Tight lower bounds for planted clique in the degree-4 SOS program.  arXiv:1507.05136, July 2015.  A. A. Shabalin, V. J. Weigman, C. M. Perou, and A. B Nobel. Finding large average submatrices in  high dimensional data. The Annals of Applied Statistics, 3(3):985-1012, 2009.  T. Tao. Topics in random matrix theory. American Mathematical Society, Providence, RI, USA,  2015.  2012.  R. K. Vinayak, S. Oymak, and B. Hassibi. Sharp performance bounds for graph clustering via convex optimization. In 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.  V. H. Vu. Spectral norm of random matrices. Combinatorica, 27(6):721-736, 2007.  ISSN 0209-9683. doi: 10.1007/s00493-007-2190-z. URL http://dx.doi.org/10.1007/ s00493-007-2190-z.  A. M. Zubkov and A. A. Serov. A complete proof of universal inequalities for the distribution function of the binomial law. Theory of Probability & Its Applications, 57(3):539-544, 2013.  "}, "Online Learning with Low Rank Experts": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Online Learning with Low Rank Experts", "abstract": "We consider the problem of prediction with expert advice when the losses of the experts have low-dimensional structure: they are restricted to an unknown d-dimensional subspace. We devise algorithms with regret bounds that are independent of the number of experts and depend only on the rank d. For the stochastic model we show a tight bound of \u0398(\\sqrtdT), and extend it to a setting of an approximate d subspace. For the adversarial model we show an upper bound of O(d\\sqrtT) and a lower bound of \u03a9(\\sqrtdT).", "pdf_url": "http://proceedings.mlr.press/v49/hazan16.pdf", "keywords": [], "reference": "1997.  2009.  Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 675-684. ACM, 2013.  Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1-58,  Shai Ben-David, D\u00b4avid P\u00b4al, and Shai Shalev-Shwartz. Agnostic online learning. In COLT. Citeseer,  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. 2006.  Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and  Shenghuo Zhu. Online optimization with gradual variations. In COLT, pages 6-1, 2012.  Steven De Rooij, Tim Van Erven, Peter D Gr\u00a8unwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. The Journal of Machine Learning Research, 15(1):1281-1316, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.  Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruc-  tion. arXiv preprint arXiv:1102.3923, 2011.  Eyal Gofer, Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for  branching experts. In Conference on Learning Theory, pages 618-638, 2013.  Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, and Xiaojin Zhu. Transduction with matrix completion: Three birds with one stone. In Advances in neural information processing systems, pages 757-765, 2010.  13   ONLINE LEARNING WITH LOW RANK EXPERTS  Acknowledgments  Elad Hazan acknowledges support from the National Science Foundation grant IIS-1523815. Roi Livni is a recipient of the Google Europe Fellowship in Learning Theory, and this research is sup- ported in part by this Google Fellowship. Yishay Mansour is supported in part by the Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF) and by a grant from the Len Blavatnik and the Blavatnik Family Foundation. The research leading to these results has received funding from the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u02dd 336078 ERC-SUBLRN.  References  1997.  2009.  Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 675-684. ACM, 2013.  Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1-58,  Shai Ben-David, D\u00b4avid P\u00b4al, and Shai Shalev-Shwartz. Agnostic online learning. In COLT. Citeseer,  Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational mathematics, 9(6):717-772, 2009.  Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. 2006.  Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre-  diction with expert advice. Machine Learning, 66(2-3):321-352, 2007.  Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and  Shenghuo Zhu. Online optimization with gradual variations. In COLT, pages 6-1, 2012.  Steven De Rooij, Tim Van Erven, Peter D Gr\u00a8unwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. The Journal of Machine Learning Research, 15(1):1281-1316, 2014.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.  Rina Foygel and Nathan Srebro. Concentration-based guarantees for low-rank matrix reconstruc-  tion. arXiv preprint arXiv:1102.3923, 2011.  Eyal Gofer, Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for  branching experts. In Conference on Learning Theory, pages 618-638, 2013.  Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, and Xiaojin Zhu. Transduction with matrix completion: Three birds with one stone. In Advances in neural information processing systems, pages 757-765, 2010. HAZAN KOREN LIVNI MANSOUR  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. Geometric algorithms and combinato-  rial optimization, volume 2. Springer Science & Business Media, 2012.  Elad Hazan. Introduction to Onlne Convex Optimization, Draft. now Publishers Inc., 2015.  Elad Hazan and Satyen Kale. On stochastic and worst-case models for investing. In Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I. Williams, and A. Culotta, editors, Advances in Neural In- formation Processing Systems 22, pages 709-717. Curran Associates, Inc., 2009.  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation  in costs. Machine learning, 80(2-3):165-188, 2010.  Elad Hazan and Satyen Kale. Better algorithms for benign bandits. The Journal of Machine Learn-  ing Research, 12:1287-1311, 2011.  Elad Hazan, Roi Livni, and Yishay Mansour. Classification with low rank and missing data. In Proceedings of The 32nd International Conference on Machine Learning, pages 257-266, 2015.  Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793-800, 2009.  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-  rithm. Machine learning, 2(4):285-318, 1988.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-  ence on Learning Theory, pages 993-1019, 2013.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Sys- tems, pages 1984-1992, 2010.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Localization and adaptation in online learning. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 516-526, 2013.  Sasha Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize: From value to algo-  rithms. In Advances in Neural Information Processing Systems, pages 2141-2149, 2012.  Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In  Advances in Neural Information Processing Systems, pages 810-818, 2014.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge University Press, 2014.  Shai Shalev-Shwartz, Alon Gonen, and Ohad Shamir. Large-scale convex minimization with a  low-rank constraint. arXiv preprint arXiv:1106.1622, 2011. ONLINE LEARNING WITH LOW RANK EXPERTS  Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Learning Theory, pages  545-560. Springer, 2005.  Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In  Advances in neural information processing systems, pages 1329-1336, 2004.  "}, "Optimal rates for total variation denoising": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Optimal rates for total variation denoising", "abstract": "Motivated by its practical success, we show that the 2D total variation denoiser satisfies a sharp oracle inequality that leads to near optimal rates of estimation for a large class of image models such as bi-isotonic, H\u00f6lder smooth and cartoons. Our analysis hinges on properties of the unnormalized Laplacian of the  two-dimensional grid such as eigenvector delocalization and spectral decay. We also present extensions to more than two dimensions as well as several other graphs.", "pdf_url": "http://proceedings.mlr.press/v49/huetter16.pdf", "keywords": ["Total variation regularization", "TV denoising", "sharp oracle inequalities", "image denoising", "edge Lasso", "trend filtering", "nonparametric regression", "shape constrained regression", "minimax"], "reference": "Ery Arias-Castro, Joseph Salmon, and Rebecca Willett. Oracle inequalities and minimax rates for nonlocal means and related adaptive kernel-based methods. SIAM Journal on Imaging Sciences, 5(3):944-992, 2012.  Taylor B. Arnold and Ryan J. Tibshirani. Efficient implementations of the generalized lasso dual path algorithm. Journal of Computational and Graphical Statistics, 25(1):1-27, 2016. doi: 10.1080/10618600.2015.1008638.  Pierre C. Bellec. Sharp oracle inequalities for Least Squares estimators in shape restricted regres-  sion. arXiv preprint arXiv:1510.08029, 2015.  B\u00b4ela Bollob\u00b4as. A probabilistic proof of an asymptotic formula for the number of labelled regular  graphs. European Journal of Combinatorics, 1(4):311-316, 1980.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. OUP Oxford, February 2013. ISBN 978-0-19-953525-5.  Sabyasachi Chatterjee, Adityanand Guntuboyina, and Bodhisattva Sen. On matrix estimation under  monotonicity constraints. arXiv preprint arXiv:1506.03430, 2015.  Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.  Arnak S. Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction performance of the lasso. to appear in Bernoulli, arXiv 1402.1700, February 2014. URL http://arxiv.org/ pdf/1402.1700v1.pdf.  David L. Donoho and Iain M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage.  J. Amer. Statist. Assoc., 90(432):1200-1224, 1995. ISSN 0162-1459.  David L. Donoho and Jain M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika,  81(3):425-455, 1994.  Math. Soc, 195(910), May 2004.  J Friedman. A proof of Alon\u2019s second eigenvalue conjecture and related problems. Mem. Amer.  Christophe Giraud. Introduction to high-dimensional statistics. CRC Press, 2014.  16   H \u00a8UTTER RIGOLLET  Acknowledgments  We would like to thank Vivian Viallon for bringing the paper by Sharpnack et al. (2012) to our attention. We thank the participants in the workshop \u201cComputationally and Statistically Efficient Inference for Complex Large-scale Data\u201d, that took place in Oberwolfach on March 6-12, 2016; in particular Axel Munk for pointers to the literature on the spectral decomposition of the Toeplitz ma- trix in (B.10) and Alessandro Rinaldo for interesting discussion. Finally, we thank Ryan Tibshirani for pointing us to the paper Wang et al. (2015) and discussing his results with us.  Philippe Rigollet is supported in part by NSF grants DMS-1317308 and CAREER-DMS-1053987.  References  Ery Arias-Castro, Joseph Salmon, and Rebecca Willett. Oracle inequalities and minimax rates for nonlocal means and related adaptive kernel-based methods. SIAM Journal on Imaging Sciences, 5(3):944-992, 2012.  Taylor B. Arnold and Ryan J. Tibshirani. Efficient implementations of the generalized lasso dual path algorithm. Journal of Computational and Graphical Statistics, 25(1):1-27, 2016. doi: 10.1080/10618600.2015.1008638.  Pierre C. Bellec. Sharp oracle inequalities for Least Squares estimators in shape restricted regres-  sion. arXiv preprint arXiv:1510.08029, 2015.  B\u00b4ela Bollob\u00b4as. A probabilistic proof of an asymptotic formula for the number of labelled regular  graphs. European Journal of Combinatorics, 1(4):311-316, 1980.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-  totic Theory of Independence. OUP Oxford, February 2013. ISBN 978-0-19-953525-5.  Sabyasachi Chatterjee, Adityanand Guntuboyina, and Bodhisattva Sen. On matrix estimation under  monotonicity constraints. arXiv preprint arXiv:1506.03430, 2015.  Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997.  Arnak S. Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction performance of the lasso. to appear in Bernoulli, arXiv 1402.1700, February 2014. URL http://arxiv.org/ pdf/1402.1700v1.pdf.  David L. Donoho and Iain M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage.  J. Amer. Statist. Assoc., 90(432):1200-1224, 1995. ISSN 0162-1459.  David L. Donoho and Jain M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika,  81(3):425-455, 1994.  Math. Soc, 195(910), May 2004.  J Friedman. A proof of Alon\u2019s second eigenvalue conjecture and related problems. Mem. Amer.  Christophe Giraud. Introduction to high-dimensional statistics. CRC Press, 2014. OPTIMAL RATES FOR TOTAL VARIATION DENOISING  Za\u0131d Harchaoui and C\u00b4eline L\u00b4evy-Leduc. Multiple change-point estimation with a total variation  penalty. Journal of the American Statistical Association, 2012.  Theodore Kolokolnikov, Braxton Osting, and James Von Brecht. Algebraic connectivity of Erd\u00a8os- R\u00b4enyi graphs near the connectivity threshold. Manuscript in preparation, 2014. URL http: //www.mathstat.dal.ca/\u02dctkolokol/papers/critscaling4.pdf.  A. P. Korostelev and A. B. Tsybakov. Minimax Theory of Image Reconstruction, volume 82 of Lecture Notes in Statistics. Springer New York, New York, NY, 1993. ISBN 978-0-387-94028-1 978-1-4612-2712-0.  Enno Mammen and Sara van de Geer. Locally adaptive regression splines. The Annals of Statistics,  25(1):387-413, 1997.  Deanna Needell and Rachel Ward. Stable image reconstruction using total variation minimization.  SIAM Journal on Imaging Sciences, 6(2):1035-1058, 2013a.  Deanna Needell and Rachel Ward. Near-optimal compressed sensing guarantees for total variation  minimization. IEEE Transactions on Image Processing, 22(10):3941-3949, 2013b.  Edouard Ollier and Vivian Viallon. Regression modeling on stratified data: automatic and covariate- specific selection of the reference stratum with simple L1-norm penalties. arXiv:1508.05476 [math, stat], August 2015.  Golan Pundak. Random Regular Generator. MATLAB Central File Exchange, December 2010.  Junyang Qian and Jinzhu Jia. On pattern recovery of the fused Lasso. arXiv:1211.5194, November  2012.  2952, 2009.  A. Rinaldo. Properties and refinements of the fused lasso. The Annals of Statistics, 37(5B):2922-  Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 1970.  Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal  algorithms. Physica D: Nonlinear Phenomena, 60(1):259-268, 1992.  James Sharpnack, Aarti Singh, and Alessandro Rinaldo. Sparsistency of the edge lasso over graphs. In Neil D. Lawrence and Mark A. Girolami, editors, Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12), volume 22, pages 1028-1036, 2012.  Yiyuan She. Sparse regression with exact clustering. Electronic Journal of Statistics, 4:1055-1096,  Gilbert Strang. Computational science and engineering, volume 1. Wellesley-Cambridge Press  2010.  Wellesley, 2007.  Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91-108, 2005. H \u00a8UTTER RIGOLLET  Vivian Viallon, Sophie Lambert-Lacroix, H\u00a8olger Hoe\ufb02ing, and Franck Picard. On the robustness of the generalized fused lasso to prior specifications. Statistics and Computing, 26(1-2):285-301, 2016.  Yu-Xiang Wang, James Sharpnack, Alex Smola, and Ryan Tibshirani. Trend Filtering on Graphs. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 1042-1050, 2015.  Rebecca Willett, Robert Nowak, and Rui M. Castro. Faster rates in regression via active learning.  In Advances in Neural Information Processing Systems, pages 179-186, 2005.  Bo Xin, Yoshinobu Kawahara, Yizhou Wang, and Wen Gao. Efficient Generalized Fused Lasso and Its Application to the Diagnosis of Alzheimer\u2019s Disease. In Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 2163-2169, 2014.  "}, "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja\u2019s Algorithm": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja\u2019s Algorithm", "abstract": "In this paper we provide improved guarantees for streaming principal component analysis (PCA). Given A_1, \\ldots, A_n\u2208\\mathbbR^d\\times d sampled independently from distributions satisfying \\mathbbE[A_i] = \u03a3for \u03a3\\succeq 0, we present an O(d)-space linear-time single-pass streaming algorithm for estimating the top eigenvector of \u03a3. The algorithm nearly matches (and in certain cases improves upon) the accuracy obtained by the standard batch method that computes top eigenvector of the empirical covariance \\frac1n \\sum_i \u2208[n] A_i as analyzed by the matrix Bernstein inequality. Moreover, to achieve constant accuracy, our algorithm improves upon the best previous known sample complexities of streaming algorithms by either a multiplicative factor of O(d) or 1/\\mathrmgap where \\mathrmgap is the relative distance between the top two eigenvalues of \u03a3. We achieve these results through a novel analysis of the classic Oja\u2019s algorithm, one of the oldest and perhaps, most popular algorithms for streaming PCA. We show that simply picking a random initial point w_0 and applying the natural update rule w_i + 1 = w_i + \\eta_i A_i w_i suffices for suitable choice of \\eta_i. We believe our result sheds light on how to efficiently perform streaming PCA both in theory and in practice and we hope that our analysis may serve as the basis for analyzing many variants and extensions of streaming PCA.", "pdf_url": "http://proceedings.mlr.press/v49/jain16.pdf", "keywords": [], "reference": "Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental  pca. In Advances in Neural Information Processing Systems, pages 3174-3182, 2013.  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal components anal- ysis. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887-901. SIAM, 2015.  Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.  Michael Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median in nearly linear time. To Appear in 48th Annual Symposium on the Theory of Computing (STOC) 2016, 2010.  Michael B Cohen, Jelani Nelson, and David P Woodruff. Optimal approximate matrix product in  terms of stable rank. arXiv preprint arXiv:1507.02268, 2015.  Dan Garber and Elad Hazan. Fast and simple pca via convex optimization.  arXiv preprint  arXiv:1509.05647, 2015.  Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 560-568, 2015.  Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. Frequent directions: Simple  and deterministic matrix sketching. arXiv preprint arXiv:1501.01711, 2015.  Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Peter M Hall, A David Marshall, and Ralph R Martin. Incremental eigenanalysis for classification.  In BMVC, volume 98, pages 286-295. Citeseer, 1998.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In Ad- vances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2861-2869, 2014.  Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust shift- and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector compu- tation. arXiv preprint arXiv:1510.08896, 2015.  Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis.  Annals of statistics, pages 295-327, 2001.  Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002.  17   STREAMING PCA: MATCHING MATRIX BERNSTEIN WITH OJA\u2019S ALGORITHM  References  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental  pca. In Advances in Neural Information Processing Systems, pages 3174-3182, 2013.  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal components anal- ysis. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887-901. SIAM, 2015.  Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.  Michael Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median in nearly linear time. To Appear in 48th Annual Symposium on the Theory of Computing (STOC) 2016, 2010.  Michael B Cohen, Jelani Nelson, and David P Woodruff. Optimal approximate matrix product in  terms of stable rank. arXiv preprint arXiv:1507.02268, 2015.  Dan Garber and Elad Hazan. Fast and simple pca via convex optimization.  arXiv preprint  arXiv:1509.05647, 2015.  Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 560-568, 2015.  Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. Frequent directions: Simple  and deterministic matrix sketching. arXiv preprint arXiv:1501.01711, 2015.  Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.  Peter M Hall, A David Marshall, and Ralph R Martin. Incremental eigenanalysis for classification.  In BMVC, volume 98, pages 286-295. Citeseer, 1998.  Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. In Ad- vances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2861-2869, 2014.  Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust shift- and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector compu- tation. arXiv preprint arXiv:1510.08896, 2015.  Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis.  Annals of statistics, pages 295-327, 2001.  Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002. JAIN JIN KAKADE NETRAPALLI SIDFORD  TP Krasulina. Method of stochastic approximation in the determination of the largest eigenvalue of the mathematical expectation of random matrices. Automatation and Remote Control, pages 50-56, 1970.  Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581-588. ACM, 2013.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming pca. In  Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  John Nelson and Huy L Nguy\u02c6en. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 117-126. IEEE, 2013.  Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical  biology, 15(3):267-273, 1982.  David A Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-Hsuan Yang.  Incremental learning for  robust visual tracking. International Journal of Computer Vision, 77(1-3):125-141, 2008.  Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gra- dient descent for some non-convex matrix problems. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2332-2341, 2015.  Ohad Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In Pro- ceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 144-152, 2015a.  Ohad Shamir. Convergence of stochastic gradient descent for pca. arXiv preprint arXiv:1509.09002,  2015b.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint  arXiv:1011.3027, 2010.  Manfred K. Warmuth and Dima Kuzmin. Randomized PCA algorithms with regret bounds that In Advances in Neural Information Processing Systems 19, are logarithmic in the dimension. Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 1481-1488, 2006.  Per- \u02daAke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numer-  ical Mathematics, 12(1):99-111, 1972.  Juyang Weng, Yilu Zhang, and Wey-Shiuan Hwang. Candid covariance-free incremental principal component analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(8): 1034-1040, 2003. "}, "Online Isotonic Regression": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Online Isotonic Regression", "abstract": "We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by O\\big(T^1/3 \\log^2/3(T)\\big) and present a matching \u03a9(T^1/3) lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to O(\\log T) or even to O(1) (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give  bounds for entropic loss and absolute loss.", "pdf_url": "http://proceedings.mlr.press/v49/kotlowski16.pdf", "keywords": ["online learning", "isotonic regression", "isotonic function", "monotonic", "nonparametric regression", "exp-concave loss"], "reference": "M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4): 641-647, 1955.  K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponen-  tial family of distributions. Journal of Machine Learning, 43(3):211-246, 2001.  14   KOT\u0141OWSKI, KOOLEN AND MALEK  8. Conclusions and open problem  We introduced the online version of the isotonic regression problem, in which the learner must sequentially predict the labels as well as the best isotonic function. We gave a computationally efficient version of the Exponential Weights algorithm which plays on a covering net for the set of isotonic functions and proved that its regret is bounded by O(T 1/3 log2/3(T )). We also showed an \u2126(T 1/3) lower bound on the regret of any algorithm, essentially closing the gap.  There are some interesting directions for future research. First, we believe that the discretization (covering net) is not needed in the algorithm, and a carefully devised continuous prior would work as well. We were, however, unable to find a prior that would produce the optimal regret bound and remain computationally efficient. Second, we are interested to see whether some regularized version of FTL (e.g., by means of relative entropy), or the forward algorithm (Vovk-Azoury-Warmuth) (Azoury and Warmuth, 2001) could work for this problem. However, the most interesting research direction is the extension to the partial order case. In this setting, the learner is given a set of points X = {x1, . . . , xT }, together with a partial order relation (cid:22) on X. The goal of the learner is to sequentially predict the labels not much worse than the best function which respects the isotonic constraints: xi (cid:22) xj \u2192 f (xi) \u2264 f (xj). A typical application would be nonparametric data modeling with multiple features, where domain knowledge may tell us that increasing the value of any of the features is likely to increase the value of the label. The off-line counterpart has been extensively studied in the statistics literature (Robertson et al., 1998), and the optimal isotonic function shares many properties (e.g., averaging within level sets) with the linear order case. The discretized Exponential Weights algorithm, which was presented in this paper, can be extended to deal with partial orders. The analysis closely follows the proof of Theorem 4 except that the size of the covering net FK is no longer O(T K) but now depends on the structure of (cid:22). We believe that |FK| is the right quantity to measure the complexity of the problem and the algorithm will remain competitive in this more general setting. Unfortunately, the algorithm is no longer efficiently implementable and suffers from the same problems that plague inference in graphical models on general graphs. It thus remains an open problem to find an efficient algorithm for the partial order case.  Acknowledgments  We thank the anonymous reviewers for suggestions which improved the quality of our work. Wouter Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO, Veni grant 639.021.439), Wojciech Kot\u0142owski acknowledges support from the Polish National Sci- ence Centre (grant no. 2013/11/D/ST6/03050), and Alan Malek acknowledges support from Adobe through a Digital Marketing Research Award.  References  M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4): 641-647, 1955.  K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponen-  tial family of distributions. Journal of Machine Learning, 43(3):211-246, 2001. ONLINE ISOTONIC REGRESSION  L. Birg\u00b4e and P. Massart. Rates of convergence for minimum contrast estimators. Probability Theory  and Related Fields, 97:113-150, 1993.  H. D. Brunk. Maximum likelihood estimates of monotone parameters. Annals of Mathematical  N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors. Machine  Statistics, 26(4):607-616, 1955.  Learning, 43(3):247-264, 2001.  2006.  Wiley & Sons, 2014.  rica, 86(2):429-438, 1999.  97-106, 2007.  N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,  D. DeTemple and W. Webb. Combinatorial reasoning: An introduction to the art of counting. John  R. Dykstra, J. Hewett, and T. Robertson. Nonparametric, isotonic discriminant procedures. Biomet-  T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1):  P. Gaillard and S. Gerchinovitz. A chaining algorithm for online nonparametric regression.  In  Conference on Learning Theory (COLT), pages 764-796, 2015.  S. van de Geer. Estimating a regression function. Annals of Statistics, 18:907-924, 1990.  E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.  Machine Learning, 69(2-3):169-192, 2007.  S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and  single index models with isotonic regression. In NIPS, pages 927-935. 2011.  A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT,  2009.  537-544, 2009.  J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Information and Computation, 132(1):1-63, 1997.  W. Kot\u0142owski and R. S\u0142owi\u00b4nski. Rule learning with monotonicity constraints.  In ICML, pages  W. Kot\u0142owski and R. S\u0142owinski. On nonparametric ordinal classification with monotonicity con- straints. IEEE Transactions on Knowledge and Data Engineering, 25(11):2576-2589, 2013.  J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.  Psychometrika, 29(1):1-27, 1964.  R. Kyng, A. Rao, and S. Sachdeva. Fast, provable algorithms for isotonic regression in all (cid:96)p-norms.  In NIPS, 2015.  J. de Leeuw, K. Hornik, and P. Mair. Isotone optimization in R: Pool-adjacent-violators algorithm  (PAVA) and active set methods. Journal of Statistical Software, 32:1-24, 2009. KOT\u0141OWSKI, KOOLEN AND MALEK  R. Luss, S. Rosset, and M. Shahar. Efficient regularized isotonic regression with application to  gene-gene interaction search. Annals of Applied Statistics, 6(1):253-283, 2012.  A. K. Menon, X. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabili-  ties with a ranking loss. In ICML, 2012.  T. Moon, A. Smola, Y. Chang, and Z. Zheng. Intervalrank: Isotonic regression with listwise and  pairwise constraint. In WSDM, pages 151-160. ACM, 2010.  H. Narasimhan and S. Agarwal. On the relationship between binary classification, bipartite ranking,  and binary class probability estimation. In NIPS, pages 2913-2921. 2013.  A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning.  In  ICML, pages 625-632, 2005.  G. Obozinski, C. E. Grant, G. R. G. Lanckriet, M. I. Jordan, and W. W. Noble. Consistent proba-  bilistic outputs for protein function prediction. Genome Biology, 2008 2008.  A. Rakhlin and K. Sridharan. Online nonparametric regression. In COLT, pages 1232-1264, 2014.  T. Robertson, F. T. Wright, and R. L. Dykstra. Order Restricted Statistical Inference. John Wiley &  Sons, 1998.  8:45-114, 2014.  S. de Rooij and T. van Erven. Learning the switching rate by discretising Bernoulli sources online.  In AISTATS, pages 432-439, 2009.  A. Saumard and J. A. Wellner. Log-concavity and strong log-concavity: A review. Statistics Surveys,  S. Shalev-Shwartz. Online learning and online convex optimization. In Foundations and Trends in  Machine Learning, volume 4, pages 107-194. 2012.  M. Stylianou and N. Flournoy. Dose finding using the biased coin up-and-down design and isotonic  regression. Biometrics, 58(1):171-177, 2002.  A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2009.  V. Vovk. Aggregating strategies. In COLT, pages 371-386, 1990.  V. Vovk, I. Petej, and V. Fedorova. Large-scale probabilistic predictors with and without guarantees  of validity. In NIPS, pages 892-900. 2015.  Bin Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer-  Verlag, 1997.  B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability esti-  mates. In KDD, pages 694-699, 2002.  C.-H. Zhang. Risk bounds in isotonic regression. The Annals of Statistics, 30(2):528-555, 2002.  M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML,  pages 928-936, 2003. ONLINE ISOTONIC REGRESSION  "}, "Time series prediction and online learning": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Time series prediction and online learning", "abstract": "We present a series of theoretical and algorithmic results combining the benefits of the statistical learning approach to time series prediction with that of on-line learning. We prove new generalization guarantees for hypotheses derived from regret minimization algorithms in the general scenario where the data is generated by a non-stationary non-mixing stochastic process. Our theory enables us to derive model selection techniques with favorable theoretical guarantees in the scenario of time series, thereby solving a problem that is notoriously difficult in that scenario. It also helps us devise new ensemble methods with favorable theoretical guarantees for the task of forecasting non-stationary time series.", "pdf_url": "http://proceedings.mlr.press/v49/kuznetsov16.pdf", "keywords": ["time series prediction", "on-line learning", "generalization bounds", "regret minimization", "validation", "model selection", "ensembles", "stability", "non-stationary", "non-mixing"], "reference": "A. Agarwal and J.C. Duchi. The generalization ability of online algorithms for dependent data.  IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5:13-51, 1995.  Pierre Alquier and Olivier Wintenberger. Model selection for weakly dependent time series fore-  casting. Technical Report 2010-39, Centre de Recherche en Economie et Statistique, 2010.  Pierre Alquier, Xiaoyin Li, and Olivier Wintenberger. Prediction of time series by statistical learn-  ing: general losses and fast rates. Dependence Modelling, 1:65-93, 2014.  Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction.  In proceedings of COLT, 2013.  proceedings of ICML, 2015.  243-252, 1992.  Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In  Peter L. Bartlett. Learning with a slowly changing distribution. In proceedings of COLT, pages  Rakesh D. Barve and Phil M. Long. On the complexity of learning from drifting distributions.  Information and Computation, 138(2):101-123, 1997.  15   TIME SERIES PREDICTION AND ONLINE LEARNING  As in Section 4.1, the convex optimization problem in (10) can be solved using a standard projected subgradient algorithm where at each iteration a DC-algorithm of Tao and An (1998) is used to compute the discrepancy, if H and L are convex. As before, this DC-algorithm is guaranteed to be optimal if L is the squared loss and H is a set of linear hypothesis. Furthermore, for linear hypotheses with the squared loss and (cid:100)discHA(q) in the objective, the same analysis as in Section 4.1 can be used.  5. Conclusion  Time series prediction is a fundamental learning problem. We presented a series of results exploiting its recent analysis in statistical learning theory in the general scenario of non-stationary non-mixing Kuznetsov and Mohri (2015) and other existing regret-based analysis and guarantees from the broad on-line learning literature. This combination of the benefits of different approaches can lead to a variety of rich problems and solutions in learning theory that we hope this work will promote and stimulate.  This work was partly funded by NSF IIS-1117591 and CCF-1535987.  Acknowledgments  References  A. Agarwal and J.C. Duchi. The generalization ability of online algorithms for dependent data.  IEEE Transactions on Information Theory, 59(1):573-587, 2013.  Farid Alizadeh. Interior point methods in semidefinite programming with applications to combina-  torial optimization. SIAM Journal on Optimization, 5:13-51, 1995.  Pierre Alquier and Olivier Wintenberger. Model selection for weakly dependent time series fore-  casting. Technical Report 2010-39, Centre de Recherche en Economie et Statistique, 2010.  Pierre Alquier, Xiaoyin Li, and Olivier Wintenberger. Prediction of time series by statistical learn-  ing: general losses and fast rates. Dependence Modelling, 1:65-93, 2014.  Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction.  In proceedings of COLT, 2013.  proceedings of ICML, 2015.  243-252, 1992.  Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In  Peter L. Bartlett. Learning with a slowly changing distribution. In proceedings of COLT, pages  Rakesh D. Barve and Phil M. Long. On the complexity of learning from drifting distributions.  Information and Computation, 138(2):101-123, 1997. KUZNETSOV MOHRI  Shai Ben-David, Gyora M. Benedek, and Yishay Mansour. A parametrization scheme for classifying  models of learnability. In Proceedings of COLT, pages 285-302, 1989.  Patrizia Berti and Pietro Rigo. A Glivenko-Cantelli theorem for exchangeable random variables.  Statistics & Probability Letters, 32(4):385 - 391, 1997.  Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning  Research, 2:499-526, 2002.  Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past posteri-  ors. In proceedings of COLT, 2001.  George Edward Pelham Box and Gwilym Jenkins. Time Series Analysis, Forecasting and Control.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, Learning, and Games. Cambridge University  Holden-Day, 1990.  Press, 2006.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line  learning algorithms. IEEE Trans. Inf. Theory, 50(9), 2004.  Nicol`o Cesa-Bianchi, Pierre Gaillard, G\u00b4abor Lugosi, and Gilles Stoltz. Mirror descent meets fixed  share (and feels no regret). In proceedings of NIPS, pages 989-997, 2012.  Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. An online learning-based framework for track-  ing. In UAI, 2010.  Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and  algorithm for regression. Theor. Comput. Sci., 2014.  Koby Crammer, Yishay Mansour, Eyal Even-Dar, and Jennifer Wortman Vaughan. Regret mini-  mization with concept drift. In proceedings of COLT, 2010.  Eyal Even-Dar, Yishay Mansour, and Jennifer Wortman. Regret minimization with concept drift.  In Proceedings of COLT, 2010.  Opt., 23(4):493-513, 1985.  R. Fletcher. On minimizing the maximum eigenvalue of a symmetric matrix. SIAM J. Control and  C. Helmberg and F. Oustry. Bundle methods to minimize the maximum eigenvalue function. In Handbook of Semidefinite Programming: Theory, Algorithms, and Applications. Kluwer Aca- demic Publishers, 2000.  Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32(2), 1998.  Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. JMLR, 2001.  Florian Jarre. An interior-point method for minimizing the maximum eigenvalue of a linear combi-  nation of matrices. SIAM J. Control Optim., 31(5):1360-1377, 1993.  Wouter M Koolen, Alan Malek, Peter L Bartlett, and Yasin Abbasi. Minimax time series prediction.  In proceedings of NIPS, 2015. TIME SERIES PREDICTION AND ONLINE LEARNING  Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for time series prediction with non-  stationary processes. In proceedings of ALT, 2014.  Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non-  stationary time series. In proceedings of NIPS, 2015.  Nick Littlestone. From on-line to batch learning. In Proceedings of COLT, pages 269-284, 1989.  Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learn-  ing, pages 5-34, 2000.  D.S. Modha and E. Masry. Memory-universal prediction of stationary random processes. Informa-  tion Theory, IEEE Transactions on, 44(1):117-133, Jan 1998.  Mehryar Mohri and Andres Mu\u02dcnoz Medina. New analysis and algorithm for learning with drifting  distributions. In proceedings of ALT, 2012.  Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.  In proceedings of NIPS, 2009.  Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary \u03d5-mixing and \u03b2-mixing  processes. Journal of Machine Learning Research, 11:789-814, 2010.  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.  The MIT Press, 2012.  Edward Moroshko and Koby Crammer. Weighted last-step min-max algorithm with improved sub-  logarithmic regret. In proceedings of ALT, 2012.  Edward Moroshko and Koby Crammer. A last-step regression algorithm for non-stationary online  Edward Moroshko, Nina Vaits, and Koby Crammer. Second-order non-stationary online learning  Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Math. Pro-  Michael L. Overton. On minimizing the maximum eigenvalue of a symmetric matrix. SIAM J.  learning. In AISTATS, 2013.  for regression. JMLR, 2015.  gram., 110:245-259, 2007.  Matrix Anal. Appl., 9(2), 1988.  data. In GRC, 2010.  Vladimir Pestov. Predictive PAC learnability: A paradigm for learning from exchangeable input  Alexander Rakhlin and Karthik Sridharan. Hierarchies of relaxations for online prediction problems  with evolving constraints. In proceedings of COLT, 2015.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages,  combinatorial parameters, and learnability. In proceedings of NIPS, 2010.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic, constrained,  and smoothed adversaries. In proceedings of NIPS, 2011. KUZNETSOV MOHRI  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform  martingale laws of large numbers. Probability Theory and Related Fields, 2015a.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complex-  ities. JMLR, 16(1), January 2015b.  Cosma Shalizi and Aryeh Kontorovitch. Predictive PAC learning and process decompositions. In  proceedings of NIPS, 2013.  of NIPS, 2009.  Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. In proceedings  Pham Dinh Tao and Le Thi Hoai An. A D.C. optimization algorithm for solving the trust-region  subproblem. SIAM Journal on Optimization, 8(2):476-505, 1998.  M. Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Networks  and Control Systems. Springer-Verlag New York, Inc., 1997.  Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals  of Probability, 22(1):94-116, 1994.  "}, "Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits", "abstract": "I prove near-optimal frequentist regret guarantees for the finite-horizon Gittins index strategy for multi-armed bandits with Gaussian noise and prior. Along the way I derive finite-time bounds on the Gittins index that are asymptotically exact and may be of independent interest. I also discuss computational issues and present experimental results suggesting that a particular version of the Gittins index strategy is an improvement on existing algorithms with finite-time regret guarantees such as UCB and Thompson sampling.", "pdf_url": "http://proceedings.mlr.press/v49/lattimore16.pdf", "keywords": [], "reference": "Rajeev Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, pages 1054-1078, 1995.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for Thompson sampling. In Pro- ceedings of International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.  In Proceedings of Conference on Learning Theory (COLT), pages 217-226, 2009.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322-331. IEEE, 1995.  Peter Auer, Nicol\u00b4o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47:235-256, 2002.  John Bather. Optimal stopping of Brownian motion: A comparison technique. Recent Advances in  Statistics, pages 19-50, 1983.  and Hall, 1985.  Donald Berry and Bert Fristedt. Bandit problems : sequential allocation of experiments. Chapman  Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 560-564. IEEE, 1995.  Russell N Bradt, SM Johnson, and Samuel Karlin. On sequential designs for maximizing the sum  of n observations. The Annals of Mathematical Statistics, pages 1060-1074, 1956.  Apostolos N Burnetas and Michael N Katehakis. On the finite horizon one-armed bandit problem.  Stochastic Analysis and Applications, 16(1):845-859, 1997.  Olivier Capp\u00b4e, Aur\u00b4elien Garivier, Odalric-Ambrym Maillard, R\u00b4emi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516-1541, 2013.  Jhelum Chakravorty and Aditya Mahajan. Multi-armed bandits, Gittins index, and its calculation. Methods and Applications of Statistics in Clinical Trials: Planning, Analysis, and Inferential Methods, Volume 2, pages 416-435, 2013.  Herman Chernoff and SN Ray. A bayes sequential sampling inspection plan. The Annals of Math-  ematical Statistics, pages 1387-1407, 1965.  13   REGRET ANALYSIS OF THE FINITE-HORIZON GITTINS INDEX STRATEGY  Acknowledgements  My thanks to Marcus Hutter for several useful suggestions and to Dan Russo for pointing out that the finite-horizon Gittins strategy is not generally Bayesian optimal. The experimental component of this research was enabled by the support provided by WestGrid (www.westgrid.ca) and Compute Canada (www.computecanada.ca).  References  Rajeev Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit  problem. Advances in Applied Probability, pages 1054-1078, 1995.  Shipra Agrawal and Navin Goyal. Further optimal regret bounds for Thompson sampling. In Pro- ceedings of International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.  In Proceedings of Conference on Learning Theory (COLT), pages 217-226, 2009.  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322-331. IEEE, 1995.  Peter Auer, Nicol\u00b4o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine Learning, 47:235-256, 2002.  John Bather. Optimal stopping of Brownian motion: A comparison technique. Recent Advances in  Statistics, pages 19-50, 1983.  and Hall, 1985.  Donald Berry and Bert Fristedt. Bandit problems : sequential allocation of experiments. Chapman  Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 560-564. IEEE, 1995.  Russell N Bradt, SM Johnson, and Samuel Karlin. On sequential designs for maximizing the sum  of n observations. The Annals of Mathematical Statistics, pages 1060-1074, 1956.  Apostolos N Burnetas and Michael N Katehakis. On the finite horizon one-armed bandit problem.  Stochastic Analysis and Applications, 16(1):845-859, 1997.  Olivier Capp\u00b4e, Aur\u00b4elien Garivier, Odalric-Ambrym Maillard, R\u00b4emi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516-1541, 2013.  Jhelum Chakravorty and Aditya Mahajan. Multi-armed bandits, Gittins index, and its calculation. Methods and Applications of Statistics in Clinical Trials: Planning, Analysis, and Inferential Methods, Volume 2, pages 416-435, 2013.  Herman Chernoff and SN Ray. A bayes sequential sampling inspection plan. The Annals of Math-  ematical Statistics, pages 1387-1407, 1965. LATTIMORE  Aur\u00b4elien Garivier. Informational confidence bounds for self-normalized averages and applications.  arXiv preprint arXiv:1309.3376, 2013.  John Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical  Society. Series B (Methodological), 41(2):148-177, 1979.  John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John  Wiley & Sons, 2011.  Michael N Katehakis and Herbert Robbins. Sequential choice from several populations. Proceed- ings of the National Academy of Sciences of the United States of America, 92(19):8584, 1995.  Emilie Kaufmann. On bayesian index policies for sequential resource allocation. arXiv preprint  arXiv:1601.01190, 2016.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On Bayesian upper confidence bounds for bandit problems. In International Conference on Artificial Intelligence and Statistics, pages 592-600, 2012a.  Emilie Kaufmann, Nathaniel Korda, and R\u00b4emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In NaderH. Bshouty, Gilles Stoltz, Nicolas Vayatis, and Thomas Zeugmann, editors, Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 199-213. Springer Berlin Heidelberg, 2012b. ISBN 978-3-642-34105-2. doi: 10.1007/978-3-642-34106-9 18.  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances  in applied mathematics, 6(1):4-22, 1985.  Tor Lattimore. Optimally confident UCB : Improved regret for finite-armed bandits. Technical  report, 2015a. URL http://arxiv.org/abs/1507.07880.  Tor Lattimore. The pareto regret frontier for bandits. In Proceedings of the 28th Conference on  Neural Information Processing Systems (NIPS), 2015b.  Hans R Lerche. Boundary crossing of Brownian motion: Its relation to the law of the iterated  logarithm and to sequential analysis. Springer, 1986.  Che-Yu Liu and Lihong Li. On the prior sensitivity of Thompson sampling. arXiv preprint  arXiv:1506.03378, 2015.  Computing, 23(2):254-267, 2011.  Jos\u00b4e Ni\u02dcno-Mora. Computing a classic index for finite-horizon bandits.  INFORMS Journal on  Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The  Annals of Statistics, 41(2):693-721, 2013.  Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American  Mathematical Society, 58(5):527-535, 1952.  William Thompson. On the likelihood that one unknown probability exceeds another in view of the  evidence of two samples. Biometrika, 25(3/4):285-294, 1933. REGRET ANALYSIS OF THE FINITE-HORIZON GITTINS INDEX STRATEGY  John N Tsitsiklis. A short proof of the Gittins index theorem. The Annals of Applied Probability,  Richard Weber. On the Gittins index for multiarmed bandits. The Annals of Applied Probability, 2  pages 194-199, 1994.  (4):1024-1033, 1992.  Peter Whittle. Multi-armed bandits and the Gittins index. Journal of the Royal Statistical Society.  Series B (Methodological), pages 143-149, 1980.  Yi-Ching Yao. Some results on the Gittins index for a normal reward process. In Time Series and  Related Topics, pages 284-294. Institute of Mathematical Statistics, 2006.  "}, "Gradient Descent Only Converges to Minimizers": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Gradient Descent Only Converges to Minimizers", "abstract": "We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.", "pdf_url": "http://proceedings.mlr.press/v49/lee16.pdf", "keywords": ["Gradient descent", "saddle points", "local minimum", "non-convex"], "reference": "2009.  Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent meth-  ods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.  Robert J Adler and Jonathan E Taylor. Random fields and geometry. Springer Science & Business Media,  10   LEE SIMCHOWITZ  covering of Rn, instead of a naive union bound. It is possible that for the structured problems that arise in machine learning, whether in matrix factorization or convolutional neural networks, that saddle points are isolated after taking a quotient with respect to the associated symmetry group of the problem. Techniques from dynamical systems on manifolds may be applicable to understand the behavior of optimization algorithms on problems with a high degree of symmetry.  It is also important to understand how stringent the strict saddle assumption is. Will a perturba- tion of a function always satisfy the strict saddle property? Adler and Taylor (2009) provide very general sufficient conditions for a random function to be Morse, meaning the eigenvalues at critical points are non-zero, which implies the strict saddle condition. These conditions rely on checking the density of \u22072f (x) has full support conditioned on the event that \u2207f (x) = 0. This can be explicitly verified for functions f that arise from learning problems.  However, we note that there are very difficult unconstrained optimization problems where the strict saddle condition fails. Perhaps the simplest is optimization of quartic polynomials. Indeed, checking if 0 is a local minimizer of the quartic  f (x) =  qijx2  i x2 j  n (cid:88)  i,j=1  is equivalent to checking whether the matrix Q = [qij] is co-positive, a co-NP complete problem. For this f , the Hessian at x = 0 is zero. In concurrent work, Anandkumar and Ge (2016) have proposed an algorithm to avoid third-order saddles. Interestingly, the strict saddle property failing is analogous in dynamical systems to the existence of a slow manifold where complex dynamics may emerge. Slow manifolds give rise to metastability, bifurcation, and other chaotic dynamics, and it would be intriguing to see how the analysis of chaotic systems could be applied to understand the behavior of optimization algorithms around these difficult critical points.  Acknowledgements  The authors would like to thank Chi Jin, Tengyu Ma, Robert Nishihara, Mahdi Soltanolkotabi, Yuekai Sun, Jonathan Taylor, and Yuchen Zhang for their insightful feedback. MS is generously supported by an NSF Graduate Research Fellowship. BR is generously supported by ONR awards N00014-14-1-0024, N00014-15-1-2620, and N00014-13-1-0129, and NSF awards CCF-1148243 and CCF-1217058. MIJ is generously supported by ONR award N00014-11-1-0688 and by the ARL and the ARO under grant number W911NF-11-1-0391. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple Inc., Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm, Virdata and VMware.  References  2009.  Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent meth-  ods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.  Robert J Adler and Jonathan E Taylor. Random fields and geometry. Springer Science & Business Media, GRADIENT DESCENT CONVERGES TO MINIMIZERS  Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in non-convex  optimization. arXiv preprint arXiv:1602.05908, 2016.  Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse  coding. In Proceedings of The 28th Conference on Learning Theory, pages 113-149, 2015.  H\u00b4edy Attouch, J\u00b4er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the Kurdyka-Lojasiewicz inequality. Mathematics of Operations Research, 35(2):438-457, 2010.  Hedy Attouch, J\u00b4er\u02c6ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, 137(1-2):91-129, 2013.  Antonio Auffinger, G\u00b4erard Ben Arous, and Ji\u02c7r\u00b4\u0131 \u02c7Cern`y. Random matrices and complexity of spin glasses.  Communications on Pure and Applied Mathematics, 66(2):165-201, 2013.  Mikhail Belkin, Luis Rademacher, and James Voss. Basis learning as an algorithmic primitive. arXiv preprint  arXiv:1411.1420, 2014.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, Olivier Ley, Laurent Mazet, et al. Characterizations of Lojasiewicz inequali-  ties: subgradient \ufb02ows, talweg, convexity. Trans. Amer. Math. Soc, 362(6):3319-3363, 2010.  T Tony Cai, Xiaodong Li, and Zongming Ma. Optimal rates of convergence for noisy sparse phase retrieval  via thresholded Wirtinger \ufb02ow. arXiv preprint arXiv:1506.03382, 2015.  Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow: Theory  and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.  Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The loss surface  of multilayer networks. arXiv:1412.0233, 2014.  Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. SIAM, 2000.  Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Ben- gio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems, pages 2933-2941, 2014.  Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient  for tensor decomposition. arXiv:1503.02101, 2015.  Philip E Gill and Walter Murray. Newton-type methods for unconstrained and linearly constrained optimiza-  tion. Mathematical Programming, 7(1):311-350, 1974.  M.W. Hirsch, C.C. Pugh, and M. Shub. Invariant Manifolds. Number no. 583 in Lecture Notes in Mathemat-  ics. Springer-Verlag, 1977.  Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries.  IEEE Transactions on Information Theory, 56(6):2980-2998, 2009.  K Lange. Optimization. springer texts in statistics. 2013.  Jorge J Mor\u00b4e and Danny C Sorensen. On the use of directions of negative curvature in a modified Newton  method. Mathematical Programming, 16(1):1-20, 1979. LEE SIMCHOWITZ  Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear programming.  Mathematical programming, 39(2):117-129, 1987.  Yurii Nesterov. Media, 2004.  Introductory lectures on convex optimization, volume 87. Springer Science & Business  Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance.  Mathematical Programming, 108(1):177-205, 2006.  Ioannis Panageas and Georgios Piliouras. Gradient descent converges to minimizers: The case of non-isolated  critical points. arXiv preprint arXiv:1605.00405, 2016.  Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem for  non-convex optimization. arXiv:1405.4604, 2014.  Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The  Annals of Probability, pages 698-712, 1990.  Michael Shub. Global stability of dynamical systems. Springer Science & Business Media, 1987.  Stephen Smale. Differentiable dynamical systems. Bulletin of the American mathematical Society, 73(6):  747-817, 1967.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the  geometric picture. arXiv:1511.03607, 2015a.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere II: Recovery by Riemannian  trust-region method. arXiv:1511.04777, 2015b.  Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.  Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pages 1260-1268, 2014.  Tuo Zhao, Zhaoran Wang, and Han Liu. Nonconvex low rank matrix factorization via inexact first order  oracle. "}, "Learning Communities in the Presence of Errors": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Learning Communities in the Presence of Errors", "abstract": "We study the problem of learning communities in the presence of modeling errors and give  robust recovery algorithms for the Stochastic Block Model (SBM). This model, which is also known as the Planted Partition Model,  is widely used for community detection and graph partitioning in various fields, including machine learning,  statistics, and social sciences. Many algorithms exist for learning communities in the Stochastic Block Model, but they do not work well in the presence of errors. In this paper, we initiate the study of robust algorithms for partial recovery in SBM with modeling errors or noise. We consider graphs generated according to the Stochastic Block Model and then modified by an adversary. We allow two types of adversarial errors, Feige\u2014Kilian or monotone errors, and edge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question about whether an almost exact recovery is possible when the adversary is allowed to add o(n) edges. Our work answers this question affirmatively even in the case of k>2 communities. We then show that our algorithms work not only when the instances come from SBM, but also work when the instances come from any distribution of graphs that is \\varepsilon m close to SBM in the Kullback\u2014Leibler divergence. This result also works in the presence of adversarial errors. Finally, we present almost tight lower bounds for two communities.", "pdf_url": "http://proceedings.mlr.press/v49/makarychev16.pdf", "keywords": [], "reference": "Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms. In Proceedings of the Symposium on Foun- dations of Computer Science, 2015.  Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. CoRR, abs/1405.3267, 2014. URL http://arxiv.org/abs/1405.3267.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 458-469. 2005.  In  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. CoRR, abs/1507.02323, 2015. URL http://arxiv.org/abs/1507.02323.  Brendan P.W. Ames. Guaranteed clustering and biclustering via semidefinite programming. Math-  ematical Programming, 147(1-2):429-465, 2014.  20   Proof [Proof of Lemma 21] We apply Lemma 22 to the events E and \u00afE = \u2126 \\ E:  dKL (Q, P ) \u2265 Q(E) log  Q(E) P (E)  + Q( \u00afE) log  Q( \u00afE) P ( \u00afE)  .  (7.2)  We bound the second term on the right hand side using the inequality x log x \u2265 (x \u2212 1) log e for x \u2265 0:  Q( \u00afE) log  = P ( \u00afE) \u00d7  Q( \u00afE) P ( \u00afE)  (cid:105)  log  (cid:104) Q( \u00afE) (cid:104) Q( \u00afE) P ( \u00afE) P ( \u00afE) = (Q( \u00afE) \u2212 P ( \u00afE)) log e = (P (E) \u2212 Q(E)) log e \u2265 \u2212Q(E) log e.  Q( \u00afE) P ( \u00afE)  \u2265 P ( \u00afE) \u00d7  (cid:105) \u2212 1  log e =  We have  Q(E) P (E) Thus, either Q(E) \u2264 e(cid:112)2P (E), or Q(E)/(eP (E)) \u2265 (cid:112)2/P (E), and, consequently,  dKL (Q, P ) \u2265 Q(E) log  \u2212 Q(E) log e = Q(E) log  Q(E) e \u00b7 P (E)  .  Q(E) \u2264  2dKL (Q, P ) \u2212 log(P (E)) + 1  .  We would like to thank Elchanan Mossel for some useful discussions.  Acknowledgments  References  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms. In Proceedings of the Symposium on Foun- dations of Computer Science, 2015.  Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block  model. CoRR, abs/1405.3267, 2014. URL http://arxiv.org/abs/1405.3267.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 458-469. 2005.  In  Naman Agarwal, Afonso S. Bandeira, Konstantinos Koiliaris, and Alexandra Kolla. Multisection in the stochastic block model using semidefinite programming. CoRR, abs/1507.02323, 2015. URL http://arxiv.org/abs/1507.02323.  Brendan P.W. Ames. Guaranteed clustering and biclustering via semidefinite programming. Math-  ematical Programming, 147(1-2):429-465, 2014. LEARNING COMMUNITIES IN THE PRESENCE OF ERRORS  Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, volume 7408 of Lecture Notes in Computer Science, pages 37-49, 2012. ISBN 978-3-642-32511-3.  Nikhil Bansal, Uriel Feige, Robert Krauthgamer, Konstantin Makarychev, Viswanath Nagarajan, Joseph Seffi, and Roy Schwartz. Min-max graph partitioning and small set expansion. SIAM Journal on Computing, 43(2):872-904, 2014.  Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. J. Algo-  rithms, 19:204-234, September 1995.  Ravi B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In Symposium on  Foundations of Computer Science, pages 280-285, 1987.  M. Braverman, K. Makarychev, Y. Makarychev, and A. Naor. The Grothendieck constant is strictly  smaller than Krivine\u2019s bound. In Foundations of Computer Science, pages 453-462, 2011.  S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In Proceedings of the Sympo-  sium on Discrete Algorithms, pages 1078-1087, 2009.  Thang Nguyen Bui, F. Thomson Leighton, Soma Chaudhuri, and Michael Sipser. Graph bisection  algorithms with good average case behavior. Combinatorica, 7:171-191, June 1987.  T. Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in the  presence of arbitrary outlier nodes. Ann. Statist., 43(3):1027-1059, 06 2015.  Moses Charikar, Samir Khuller, David M. Mount, and Giri Narasimhan. Algorithms for facility lo- cation problems with outliers. In Proceedings of the Symposium on Discrete Algorithms, SODA \u201901, pages 642-651, Philadelphia, PA, USA, 2001. Society for Industrial and Applied Mathemat- ISBN 0-89871-490-7. URL http://dl.acm.org/citation.cfm?id=365411. ics. 365555.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in Neural  Information Processing Systems 25, pages 2204-2212. Curran Associates, Inc., 2012.  Peter Chin, Anup Rao, and Van Vu. Stochastic block model and community detection in sparse In Proceedings of Conference on  graphs: A spectral algorithm with optimal rate of recovery. Learning Theory, pages 391-423, 2015.  Amin Coja-Oghlan. A spectral heuristic for bisecting random graphs. Random Structures & Algo-  rithms, 29(3):351-398, 2006.  19(2):227-284, March 2010.  Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Comb. Probab. Comput.,  Anne Condon and Richard Karp. Algorithms for graph partitioning on the planted partition model. In Randomization, Approximation, and Combinatorial Optimization. Algorithms and Techniques, volume 1671 of Lecture Notes in Computer Science, pages 221-232. Springer Berlin / Heidelberg, 1999. Imre Csiszar and J\u00b4anos K\u00a8orner.  Information theory: coding theorems for discrete memoryless  systems. Cambridge University Press, 2011.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106, Dec 2011.  Tassos Dimitriou and Russell Impagliazzo. Go with the winners for graph bisection. In Proceedings  of the Symposium on Discrete Algorithms, pages 510-520, 1998.  M. E. Dyer and A. M. Frieze. Fast solution of some random NP-hard problems. In 27th Annual  Symposium on Foundations of Computer Science, pages 331-336, 1986.  Uriel Feige and Joe Kilian. Heuristics for finding large independent sets, with applications to col- oring semi-random graphs. In Proceedings of Symposium on Foundations of Computer Science, pages 674-683, 1998.  Santo Fortunato. Community detection in graphs. Physics Reports, 486:75-174, 2010.  A. Grothendieck. R\u00b4esum\u00b4e de la th\u00b4eorie m\u00b4etrique des produits tensoriels topologiques. Bol. Soc.  Mat. Sao Paulo, 8:1-79, 1953.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via Grothendieck\u2019s  inequality. CoRR, abs/1411.4686, 2014.  Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First  steps. Social Networks, 5(2):109 - 137, 1983.  Mark Jerrum and Gregory Sorkin. Simulated annealing for graph bisection. In Proceedings of the  Symposium on Foundations of Computer Science, pages 94-103, 1993.  Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture models. In Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 444-457. Springer Berlin Heidelberg, 2005.  Robert Krauthgamer, Joseph Seffi Naor, and Roy Schwartz. Partitioning graphs into balanced com-  ponents. In Proceedings of the Symposium on Discrete Algorithms, pages 942-949, 2009.  Jean-Louis Krivine. Sur la constante de Grothendieck. CR Acad. Sci. Paris Ser. AB, 284(8):A445-  A446, 1977.  Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In  Proceedings of Symposium on Foundations of Computer Science, pages 299-308, 2010.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation al- In Proceedings of Symposium on Theory of  gorithms for semi-random partitioning problems. Computing, pages 367-384, 2012.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Constant factor ap- proximation for balanced cut in the random PIE model. In Proceedings of Symposium on Theory of Computing, 2014. LEARNING COMMUNITIES IN THE PRESENCE OF ERRORS  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clustering with noisy partial information. In Proceedings of the Conference on Learning Theory (COLT), 2015.  Laurent Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property. In Sympo-  sium on Theory of Computing, pages 694-703, 2014.  Colin McDiarmid. Concentration. In Probabilistic methods for algorithmic discrete mathematics,  pages 195-248. Springer, 1998.  Frank McSherry. Spectral partitioning of random graphs. In Proceedings of the 42nd IEEE Sympo-  sium on Foundations of Computer Science, pages 529-537, 2001.  Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for  community detection. CoRR, abs/1511.01473, 2015.  Andrea Montanari and Subhabrata Sen. Semidefinite programs on sparse random graphs. CoRR,  abs/1504.05910, 2015. URL http://arxiv.org/abs/1504.05910.  Elchanan Mossel, Joe Neeman, and Allan Sly. Stochastic block models and reconstruction. arXiv  preprint arXiv:1202.1499, 2012.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.  CoRR, abs/1311.4115, 2013. URL http://arxiv.org/abs/1311.4115.  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction and optimal recovery of block models. In Proceedings of the Conference on Learning Theory, pages 356-370, 2014.  Elchanan Mossel, Joe Neeman, and Allan Sly. Consistency thresholds for the planted bisection  model. In Proceedings of the Symposium on Theory of Computing, pages 69-75, 2015.  William Perry and Alexander S. Wein. A semidefinite program for unbalanced multisection in the stochastic block model. CoRR, abs/1507.05605, 2015. URL http://arxiv.org/abs/ 1507.05605.  Van Vu. A simple svd algorithm for finding hidden partitions. arXiv preprint arXiv:1404.3918,  2014.  Harrison C. White, Scott A. Boorman, and Ronald L. Breiger. Social structure from multiple net- works. i. blockmodels of roles and positions. American Journal of Sociology, 81(4):730-780, 1976.  Y. Wu, J. Xu, and B. Hajek. Achieving exact cluster recovery threshold via semidefinite program- ming under the stochastic block model. In 2015 49th Asilomar Conference on Signals, Systems and Computers, pages 1070-1074, Nov 2015. doi: 10.1109/ACSSC.2015.7421303.  Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochas- tic block models. CoRR, abs/1507.05313, 2015. URL http://arxiv.org/abs/1507. 05313. "}, "On the capacity of information processing systems": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "On the capacity of information processing systems", "abstract": "We propose and analyze a family of \\emphinformation processing systems, where a finite set of experts or servers are employed to extract information about a stream of incoming jobs. Each job is associated with a hidden label drawn from some prior distribution. An inspection by an expert produces a noisy outcome that depends both on the job\u2019s hidden label and the type of the expert, and occupies the expert for a finite time duration. A decision maker\u2019s task is to dynamically assign inspections so that the resulting outcomes can be used to accurately recover the labels of all jobs, while keeping the system stable. Among our chief motivations are applications in crowd-sourcing, diagnostics, and experiment designs, where one wishes to efficiently discover the nature of a large number of items, using a finite pool of computational resources or human agents. We focus on the \\emphcapacity of such an information processing system. Given a level of accuracy guarantee, we ask how many experts are needed in order to stabilize the system, and through what inspection architecture. Our main result provides an adaptive inspection policy that is asymptotically optimal in the following sense: the ratio between the required number of experts under our policy and the theoretical optimal converges to one, as the probability of error in label recovery tends to zero.", "pdf_url": "http://proceedings.mlr.press/v49/massoulie16.pdf", "keywords": ["sequential hypothesis testing", "stochastic resource allocation", "information processing system", "\ufb02uid model"], "reference": "755-770, 1959.  Keith Baker and J. von Beers. Shmoo plotting: The black art of IC testing. In Proceedings of IEEE  International Test Conference, pages 932-933, 1996.  Herman Chernoff. Sequential design of experiments. Annals of Mathematical Statistics, 30(3):  Jim G. Dai. On positive harris recurrence of multiclass queueing networks: a unified approach via  \ufb02uid limit models. The Annals of Applied Probability, pages 49-77, 1995.  Marie F. Gerdtz and Tracey K. Bucknall. Triage nurses\u2019 clinical decision making. An observational  study of urgency assessment. Journal of Advanced Nursing, 35(4):550-561, 2001.  David R. Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable  crowdsourcing systems. Operations Research, 62(1):1-24, 2014.  Laurent Massoulie and Kuang Xu. On the capacity of information processing systems. arXiv  preprint arXiv:1603.00544v2, 2016. URL http://arxiv.org/abs/1603.00544.  Aleksandr N. Rybko and Alexander L. Stolyar. Ergodicity of stochastic processes describing the  operation of open queueing networks. Problemy Peredachi Informatsii, 28(3):3-26, 1992.  Abraham Wald. Sequential tests of statistical hypotheses. Annals of Mathematical Statistics, 16(2):  117-186, 1945.  6   MASSOULIE XU  References  755-770, 1959.  Keith Baker and J. von Beers. Shmoo plotting: The black art of IC testing. In Proceedings of IEEE  International Test Conference, pages 932-933, 1996.  Herman Chernoff. Sequential design of experiments. Annals of Mathematical Statistics, 30(3):  Jim G. Dai. On positive harris recurrence of multiclass queueing networks: a unified approach via  \ufb02uid limit models. The Annals of Applied Probability, pages 49-77, 1995.  Marie F. Gerdtz and Tracey K. Bucknall. Triage nurses\u2019 clinical decision making. An observational  study of urgency assessment. Journal of Advanced Nursing, 35(4):550-561, 2001.  David R. Karger, Sewoong Oh, and Devavrat Shah. Budget-optimal task allocation for reliable  crowdsourcing systems. Operations Research, 62(1):1-24, 2014.  Laurent Massoulie and Kuang Xu. On the capacity of information processing systems. arXiv  preprint arXiv:1603.00544v2, 2016. URL http://arxiv.org/abs/1603.00544.  Aleksandr N. Rybko and Alexander L. Stolyar. Ergodicity of stochastic processes describing the  operation of open queueing networks. Problemy Peredachi Informatsii, 28(3):3-26, 1992.  Abraham Wald. Sequential tests of statistical hypotheses. Annals of Mathematical Statistics, 16(2):  117-186, 1945. "}, "Learning Simple Auctions": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Learning Simple Auctions", "abstract": "We present a general framework for proving polynomial sample complexity bounds for the problem of learning from samples the best auction in a class of \u201csimple\u201d auctions.  Our framework captures the most prominent examples of \u201csimple\u201d auctions, including anonymous and non-anonymous item and bundle pricings, with either a single or multiple buyers.  The first step of the framework is to show that the set of auction allocation rules have a low-dimensional representation.  The second step shows that, across the subset of auctions that share the same allocations on a given set of samples, the auction revenue varies in a low-dimensional way. Our results effectively imply that whenever it is possible to compute a near-optimal simple auction with a known prior, it is also possible to compute such an auction with an unknown prior, given a polynomial number of samples.", "pdf_url": "http://proceedings.mlr.press/v49/morgenstern16.pdf", "keywords": [], "reference": "USA, 1999.  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, NY, NY,  Moshe Babaioff, Nicole Immorlica, Brendan Lucier, and S. Matthew Weinberg. A simple and approximately optimal mechanism In Symposium on Foundations of Computer Science (FOCS 2014). IEEE - Institute of Electrical and  for an additive buyer. Electronics Engineers, October 2014.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Single price mechanisms for revenue maximization in unlimited supply  combinatorial auctions. Technical report, Carnegie Mellon University, 2007.  Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to algorithm design via  machine learning. Jour. of Comp. and System Sciences, 74(8):1245-1270, 2008a.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Item pricing for revenue maximization. In Proceedings of the 9th ACM  conference on Electronic commerce, pages 50-59. ACM, 2008b.  Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V Vazirani. Learning economic parameters from revealed  preferences. In Web and Internet Economics, pages 338-353. Springer, 2014.  Tanmoy Chakraborty, Zhiyi Huang, and Sanjeev Khanna. Dynamic and nonuniform pricing strategies for revenue maximization.  SIAM Journal on Computing, 42(6):2424-2451, 2013.  Shuchi Chawla, Jason Hartline, and Robert Kleinberg. Algorithmic pricing via virtual valuations. In Proceedings of the 8th ACM  Conf. on Electronic Commerce, pages 243-251, NY, NY, USA, 2007. ACM.  Shuchi Chawla, Jason D. Hartline, David L. Malec, and Balasubramanian Sivan. Multi-parameter mechanism design and sequential posted pricing. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, pages 311-320, NY, NY, USA, 2010. ACM.  Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of the 46th Annual ACM  Symposium on Theory of Computing, pages 243-252, NY, NY, USA, 2014. SIAM.  13   Our first two results use the framework to that grand bundle pricings and item pricings have small pseudo-dimension. The second case requires a more delicate treatment of the valuation profiles (buyers are now choosing arbitrary subsets of items, and will choose utility-maximizing bundles based on the per-item prices). It also requires us to consider a larger set of intermediate labels (the set of all possible allocations grows to [n]k from [n]).  Corollary 13 Let F be the class of anonymous grand bundle pricings. Then,  If F is the class of non-anonymous grand bundle pricings, then  Corollary 14 Let F be the class of anonymous item pricings. Then,  PD(F) = O(1).  PD(F) = O(n log n).  PD(F) = O (cid:0)k2(cid:1) .  PD(F) = O (cid:0)nk2 ln(n)(cid:1) .  If F is the class of nonanonymous item pricings, then  References  USA, 1999.  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, NY, NY,  Moshe Babaioff, Nicole Immorlica, Brendan Lucier, and S. Matthew Weinberg. A simple and approximately optimal mechanism In Symposium on Foundations of Computer Science (FOCS 2014). IEEE - Institute of Electrical and  for an additive buyer. Electronics Engineers, October 2014.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Single price mechanisms for revenue maximization in unlimited supply  combinatorial auctions. Technical report, Carnegie Mellon University, 2007.  Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to algorithm design via  machine learning. Jour. of Comp. and System Sciences, 74(8):1245-1270, 2008a.  Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Item pricing for revenue maximization. In Proceedings of the 9th ACM  conference on Electronic commerce, pages 50-59. ACM, 2008b.  Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V Vazirani. Learning economic parameters from revealed  preferences. In Web and Internet Economics, pages 338-353. Springer, 2014.  Tanmoy Chakraborty, Zhiyi Huang, and Sanjeev Khanna. Dynamic and nonuniform pricing strategies for revenue maximization.  SIAM Journal on Computing, 42(6):2424-2451, 2013.  Shuchi Chawla, Jason Hartline, and Robert Kleinberg. Algorithmic pricing via virtual valuations. In Proceedings of the 8th ACM  Conf. on Electronic Commerce, pages 243-251, NY, NY, USA, 2007. ACM.  Shuchi Chawla, Jason D. Hartline, David L. Malec, and Balasubramanian Sivan. Multi-parameter mechanism design and sequential posted pricing. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, pages 311-320, NY, NY, USA, 2010. ACM.  Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of the 46th Annual ACM  Symposium on Theory of Computing, pages 243-252, NY, NY, USA, 2014. SIAM. Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of  Machine Learning Research, 2:265-292, 2002.  Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In COLT 2014, pages 287-316, 2014. URL  http://arxiv.org/abs/1405.2420.  Nikhil R. Devanur, Zhiyi Huang, and Christos-Alexandros Psomas. The sample complexity of auctions with side information.  CoRR, abs/1511.02296, 2015. URL http://arxiv.org/abs/1511.02296.  Shaddin Dughmi, Li Han, and Noam Nisan. Sampling and representation complexity of revenue maximization. In Web and Internet Economics, volume 8877 of Lecture Notes in Computer Science, pages 277-291. Springer Intl. Publishing, Beijing, China, 2014.  Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A general lower bound on the number of exam- ples needed for learning. Information and Computation, 82(3):247-261, 1989. URL https://www.cis.upenn.edu/ \u02dcmkearns/papers/lower.pdf.  Edith Elkind. Designing and learning optimal finite support auctions. In Proceedings of the eighteenth annual ACM-SIAM sympo-  sium on Discrete algorithms, pages 736-745. SIAM, 2007.  Michal Feldman, Nick Gravin, and Brendan Lucier. Combinatorial auctions via posted prices. In Proceedings of the Twenty-Sixth  Annual ACM-SIAM Symposium on Discrete Algorithms, pages 123-135. SIAM, 2015.  Steve Hanneke. The optimal sample complexity of PAC learning. CoRR, abs/1507.00473, 2015. URL http://arxiv.org/  abs/1507.00473.  CA, USA., 2009. ACM.  Forthcoming, 1 2016.  Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. In ACM Conf. on Electronic Commerce, Stanford,  Justin Hsu, Jamie Morgenstern, Ryan Rogers, Aaron Roth, and Rakesh Vohra. Do prices coordinate markets? In STOC, page  Zhiyi Huang, Yishay Mansour, and Tim Roughgarden. Making the most of your samples. In Proceedings of the Sixteenth ACM  Conference on Economics and Computation, EC \u201915, page forthcoming, New York, NY, USA, 2015. ACM.  Alexander S Kelso Jr and Vincent P Crawford. Job matching, coalition formation, and gross substitutes. Econometrica: Journal of  the Econometric Society, pages 1483-1504, 1982.  Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical report, University of California,  Santa Cruz, 1986. URL https://users.soe.ucsc.edu/\u02dcmanfred/pubs/lrnk-olivier.pdf.  Andres Munoz Medina and Mehryar Mohri. Learning theory and algorithms for revenue optimization in second price auctions with  reserve. In Proceedings of The 31st Intl. Conf. on Machine Learning, pages 262-270, 2014.  Jamie H Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly optimal auctions. In Advances in Neural Infor-  mation Processing Systems, pages 136-144, 2015.  Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):58-73, 1981.  David Pollard. Convergence of stochastic processes. David Pollard, New Haven, Connecticut, 1984.  T. Roughgarden and O. Schrijvers. Ironing in the dark. Submitted, 2015.  Aviad Rubinstein and S. Matthew Weinberg. Simple mechanisms for a subadditive buyer and applications to revenue monotonicity. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC \u201915, pages 377-394, New York, NY, USA, 2015. ACM.  Tuomas Sandholm and Anton Likhodedov. Automated design of revenue-maximizing combinatorial auctions. Operations Re-  search, 63(5):1000-1025, 2015.  Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities.  Theory of Probability & Its Applications, 16(2):264-280, 1971. Vladimir Naumovich Vapnik and Samuel Kotz. Estimation of dependences based on empirical data. Springer, 1982.  Andrew Chi-Chih Yao. An n-to-1 bidder reduction for multi-item auctions and its applications. In Proceedings of the Twenty-Sixth  Annual ACM-SIAM Symposium on Discrete Algorithms, pages 92-109. SIAM, 2015.  "}, "Density Evolution in the Degree-correlated Stochastic Block Model": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Density Evolution in the Degree-correlated Stochastic Block Model", "abstract": "There is a recent surge of interest in identifying the sharp recovery thresholds for cluster recovery under the stochastic block model. In this paper, we address the more refined question of how many vertices that will be misclassified on average. We consider the binary form of the stochastic block model, where n vertices are partitioned into  two clusters with edge probability a/n within the first cluster, c/n within the second cluster, and b/n across clusters.  Suppose that as n \\to \u221e, a= b+ \u03bc\\sqrt b ,  c=b+  \u03bd\\sqrt b  for two fixed constants \u03bc, \u03bd, and b \\to \u221ewith b=n^o(1). When the cluster sizes are balanced and \u03bc\u2260\u03bd, we show that the minimum fraction of misclassified vertices on average is given by  Q(\\sqrtv^*), where Q(x) is the Q-function for standard normal, v^* is the unique fixed point of v= \\frac(\u03bc-\u03bd)^216 + \\frac (\u03bc+\u03bd)^2 16 \\mathbbE[ \\tanh(v+ \\sqrtv Z)], and Z is standard normal. Moreover, the minimum  misclassified  fraction on average is attained by a local algorithm, namely belief propagation, in time linear in the number of edges. Our proof techniques are based on connecting the cluster recovery problem to tree reconstruction problems, and analyzing the density evolution of belief propagation on trees with Gaussian approximations.", "pdf_url": "http://proceedings.mlr.press/v49/mossel16.pdf", "keywords": ["Belief propagation", "Density evolution", "Community detection"], "reference": "E. Abbe and C. Sandon. Community detection in general stochastic block models: fundamental  limits and efficient recovery algorithms. arXiv 1503.00609, March 2015.  E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model.  arXiv  1405.3267, October 2014.  N. Agarwal, A. S. Bandeira, K. Koiliaris, and A. Kolla. Multisection in the stochastic block model  using semidefinite programming. arXiv 1507.02323, July 2015.  18   MOSSEL XU  We are ready to prove Theorem 3 by combing Lemma 15, Lemma 16, and Lemma 17.  Proof [Proof of Theorem 3] In view of Lemma 17, for t \u2265 0,  P  (cid:110) (cid:101)\u0393t u \u2265 0|\u03c4u = \u2212  (cid:111)  lim n\u2192\u221e  = lim n\u2192\u221e  P  (cid:110) (cid:101)\u0393t u \u2264 0|\u03c4u = +  (cid:111)  \u221a  = Q(  ut),  It follows from Lemma 16 that there exists an \u03b1 \u2208 [0, 1/2) such that  E (cid:2)O(\u03c3, (cid:98)\u03c3t  BP)(cid:3) \u2264 lim  n\u2192\u221e (cid:101)qT t,\u03b1 = Q(  \u221a  ut).  lim sup n\u2192\u221e \u221a  4 (cid:101)h(v) > v for all v \u2208 (0, v\u2217). Furthermore, (cid:101)h(v) \u2264 1 and hence \u00b52  Let (cid:101)h(v) = E [tanh(v + vZ)] . In view of Lemma 14, (cid:101)h is non-decreasing and concave in [0, \u221e), and limv\u21920 (cid:101)h(cid:48)(v) = 1. Notice that h(0) = 0, and thus 0 is a fixed point of v = \u00b52 4 (cid:101)h(v). Moreover, by the mean value theorem, for v > 0, (cid:101)h(v) = h(0) + (cid:101)h(cid:48)(\u03be)v for some \u03be \u2208 (0, v). Thus \u00b52 4 (cid:101)h(v) = \u00b52 4 (cid:101)h(cid:48)(\u03be)v. By the assumption that \u00b5 > 2, and limv\u21920 (cid:101)h(cid:48)(v) = 1, it follows that there exists a v\u2217 > 0 such that \u00b52 4 (cid:101)h(v) < v for all v sufficiently large. Since (cid:101)h is continuous, v = \u00b52 4 (cid:101)h(v) must have nonzero fixed points. Let v denote the smallest nonzero fixed point. Then v > 0, \u00b52 4 (cid:101)h(cid:48)(v) < 1. Because (cid:101)h is concave, (cid:101)h(cid:48)(v) \u2264 (cid:101)h(cid:48)(v) for all v \u2265 v. Thus \u00b52 4 (cid:101)h(v) > v for all v > v. Therefore, v is the unique nonzero fixed point and also the largest fixed point. It follows that if u1 < v, then {ut} is a non-decreasing sequence upper bounded by v. If u1 > v, then {ut} is a non-increasing sequence lower bounded by v. Since u1 > 0, it follows that limt\u2192\u221e ut = v. Hence,  4 (cid:101)h(v) > v for all v \u2208 (0, v), and \u00b52  lim sup n\u2192\u221e  inf (cid:98)\u03c3  E [O(\u03c3, (cid:98)\u03c3)] \u2264 lim  t\u2192\u221e  lim sup n\u2192\u221e  E (cid:2)O(\u03c3, (cid:98)\u03c3t  BP)(cid:3) \u2264 lim  t\u2192\u221e  n\u2192\u221e (cid:101)qT t,\u03b1 = Q( lim  \u221a  v).  (19)  It follows from Theorem 2 and Lemma 15 that  lim inf n\u2192\u221e  inf (cid:98)\u03c3  E [O(\u03c3, (cid:98)\u03c3)] \u2265 lim  t\u2192\u221e  lim n\u2192\u221e  p\u2217 T t = Q(  \u221a  v).  (20)  The theorem follows by combining the last two displayed equations.  Research supported by NSF grant CCF 1320105, DOD ONR grant N00014-14-1-0823, and grant 328025 from the Simons Foundation. J. Xu would like to thank Bruce Hajek and Yihong Wu for numerous discussions on belief propagation algorithms and density evolution analysis.  5. Acknowledgement  References  E. Abbe and C. Sandon. Community detection in general stochastic block models: fundamental  limits and efficient recovery algorithms. arXiv 1503.00609, March 2015.  E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model.  arXiv  1405.3267, October 2014.  N. Agarwal, A. S. Bandeira, K. Koiliaris, and A. Kolla. Multisection in the stochastic block model  using semidefinite programming. arXiv 1507.02323, July 2015. DENSITY EVOLUTION IN STOCHASTIC BLOCK MODEL  A. Anandkumar, R. Ge, D. Hsu, and S. M. Kakade. A tensor spectral approach to learning mixed membership community models. Journal of Machine Learning Research, 15:2239-2312, June 2014.  A.S. Bandeira. Random Laplacian matrices and convex relaxations. arXiv 1504.03987, April 2015.  P. J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other modularities. Proceedings of the National Academy of Science, 106(50):21068-21073, 2009.  C. Bordenave, M. Lelarge, and L. Massouli\u00b4e. Non-backtracking spectrum of random graphs: com- munity detection and non-regular Ramanujan graphs. ArXiv 1501.06087, January 2015. URL http://arxiv.org/abs/1501.06087.  T. Cai and X. Li. Robust and computationally feasible community detection in the presence of  arbitrary outlier nodes. arXiv preprint arXiv:1404.6000, 2014.  Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix localiza- tion with a growing number of clusters and submatrices. In Proceedings of ICML 2014 (Also arXiv:1402.1267), Feb 2014.  Y. Chen, S. Sanghavi, and H. Xu. Improved graph clustering. IEEE Transactions on Information Theory. An earlier version of this work appeared under the title \u201dClustering Sparse Graphs\u201d at NIPS 2012, 60(10):6440-6455, Oct 2014. ISSN 0018-9448.  A. Coja-Oghlan. A spectral heuristic for bisecting random graphs. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, SODA \u201905, pages 850-859, Philadelphia, PA, USA, 2005. Society for Industrial and Applied Mathematics.  A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Comb. Probab. Comput., 19  (2):227-284, March 2010.  A. Condon and Richard M. Karp. Algorithms for graph partitioning on the planted partition model.  Random Structures and Algorithms, 18(2):116-140, 2001.  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106, Dec 2011a.  A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Inference and phase transitions in the detec-  tion of modules in sparse networks. Phys. Rev. Lett., 107:065701, 2011b.  Y. Deshpande, E. Abbe, and A. Montanari. Asymptotic mutual information for the two-groups  stochastic block model. arXiv:1507.08685, July 2015.  M. E. Dyer and A. M. Frieze. The solution of some random NP-hard problems in polynomial  expected time. Journal of Algorithms, 10(4):451-489, 1989.  D. Gamarnik and M. Sudan. Limits of local algorithms over sparse random graphs. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 369-376. ACM, 2014. MOSSEL XU  C. Gao, Z. Ma, A. Y. Zhang, and H. H. Zhou. Achieving optimal misclassification proportion in  stochastic block model. arXiv:1505.03772, 2015.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite program-  ming. arXiv1 412.6156, Nov. 2014.  B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semidefinite program-  ming: Extensions. arXiv:1502.07738, 2015a.  B. Hajek, Y. Wu, and J. Xu. Recovering a hidden community beyond the spectral limit in  O(|E| log\u2217 |V |) time. arXiv 1510.02786, October 2015b.  H. Hatami, L. Lov\u00b4asz, and B. Szegedy. Limits of local-global convergent graph sequences. arXiv  preprint arXiv:1205.4356, 2012.  5(2):109-137, 1983.  P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks,  M. Jerrum and G. B. Sorkin. The Metropolis algorithm for graph bisection. Discrete Applied  Mathematics, 82(1-3):155-175, 1998.  V. Kanade, E. Mossel, and T. Schramm. Global and local information in clustering labeled block In Proceedings of Approximation, Randomization, and Combinatorial Optimization.  models. Algorithms and Techniques, pages 779-810, 2014.  H. Kesten and B. P. Stigum. Additional limit theorems for indecomposable multidimensional  Galton-Watson processes. Ann. Math. Statist., 37:1463-1481, 1966.  V. Korolev and I. Shevtsova. An improvement of the Berry-Esseen inequality with applications to Poisson and mixed Poisson random sums. Scandinavian Actuarial Journal, 2012(2):81-105, 2012.  R. Lyons and F. Nazarov. Perfect matchings as iid factors on non-amenable groups. European  Journal of Combinatorics, 32(7):1115-1125, 2011.  L. Massouli\u00b4e. Community detection thresholds and the weak Ramanujan property. In Proceedings  of the Symposium on the Theory of Computation (STOC), 2014.  F. McSherry. Spectral partitioning of random graphs. In Proceedings of IEEE Conference on the  Foundations of Computer Science (FOCS), pages 529-537, 2001.  M. Mezard and A. Montanari. Information, Physics, and Computation. Oxford University Press,  Inc., New York, NY, USA, 2009. ISBN 019857083X, 9780198570837.  A. Montanari. Tight bounds for LDPC and LDGM codes under MAP decoding. IEEE Transactions  on Information Theory, 51(9):3221-3246, Sept 2005.  A. Montanari. Finding one community in a sparse random graph. arXiv 1502.05680, Feb 2015.  A. Montanari and D. Tse. Analysis of belief propagation for non-linear problems: The example of cdma (or: How to prove tanaka\u2019s formula). In Information Theory Workshop, 2006. ITW \u201906 Punta del Este. IEEE, pages 160-164, March 2006. DENSITY EVOLUTION IN STOCHASTIC BLOCK MODEL  E. Mossel and J. Xu. Local algorithms for block models with side information. In Accepted to The 7th Innovations in Theoretical Computer Science (ITCS) conference. arXiv:1508.02344, 2015.  E. Mossel, J. Neeman, and A. Sly. Belief propogation, robust reconstruction, and optimal recovery  of block models. arXiv 1309.1380, 2013a.  E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture.  arXiv  1311.4115, 2013b.  arxiv:1311.4115, 2013c.  E. Mossel, J. Neeman, and A. Sly.  A proof of the block model  threshold conjecture.  E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model. To appear in Probability Theory and Related Fields. The Arxiv version of this paper is titled Stochastic Block Models and Reconstruction, 2015a.  E. Mossel, J. Neeman, and A. Sly. Consistency thresholds for the planted bisection model.  In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC \u201915, pages 69-75, New York, NY, USA, 2015b. ACM.  W. Perry and A. S. Wein. A semidefinite program for unbalanced multisection in the stochastic  block model. arXiv:1507.05605v1, 2015.  T. Richardson and R. Urbanke. Modern Coding Theory. Cambridge University Press, 2008. ISBN  9780511791338. Cambridge Books Online.  T. A. B. Snijders and K. Nowicki. Estimation and prediction for stochastic blockmodels for graphs  with latent block structure. Journal of Classification, 14(1):75-100, 1997.  S. Yun and A. Proutiere. Accurate community detection in the stochastic block model via spectral  algorithms. arXiv 1412.7335, 2014a.  S. Yun and A. Proutiere. Community detection via random and adaptive sampling. In Proceedings  of The 27th Conference on Learning Theory, 2014b.  A. Y. Zhang and H. H. Zhou. Minimax rates of community detection in stochastic block models.  arXiv:1507.05313, 2015.  P. Zhang, F. Krzakala, J. Reichardt, and L. Zdeborov\u00b4a. Comparitive study for inference of hidden classes in stochastic block models. Journal of Statistical Mechanics : Theory and Experiment, 2012.  P. Zhang, C. Moore, and M. E. J. Newman. Community detection in networks with unequal groups.  arXiv:1509.00107, Sep. 2015. MOSSEL XU  "}, "Cortical Computation via Iterative Constructions": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Cortical Computation via Iterative Constructions", "abstract": "We study Boolean functions of an arbitrary number of input variables that can be realized by simple iterative constructions based on constant-size primitives. This restricted type of construction needs little global coordination or control and thus is a candidate for neurally feasible computation. Valiant\u2019s construction of a majority function can be realized in this manner and, as we show, can be generalized to any uniform threshold function. We study the rate of convergence, finding that while linear convergence to the correct function can be achieved for any threshold using a fixed set of primitives, for quadratic convergence, the size of the primitives must grow as the threshold approaches 0 or 1. We also study finite realizations of this process and the learnability of the functions realized. We show that the constructions realized are accurate outside a small interval near the target threshold, where the size of the construction grows as the inverse square of the interval width. This phenomenon, that errors are higher closer to thresholds (and thresholds closer to the boundary are harder to represent), is a well-known cognitive finding.", "pdf_url": "http://proceedings.mlr.press/v49/papadimitriou16.pdf", "keywords": ["Cortical Computation", "Iterative Constructions", "Monotone functions", "Threshold functions 1"], "reference": "Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161-182, 2006. doi: 10.1007/s10994-006-6265-7. URL http://dx.doi.org/10.1007/s10994-006-6265-7.  Rosa I. Arriaga, David Rutter, Maya Cakmak, and Santosh S. Vempala. Visual categorization with random projection. Neural Computation, 27(10):2132-2147, 2015. doi: 10.1162/NECO a 00769. URL http://dx.doi.org/10.1162/NECO_a_00769.  Ravi B. Boppana. Amplification of probabilistic boolean formulas. In Foundations of Computer Science, 1985., 26th Annual Symposium on, pages 20-29, Oct 1985. doi: 10.1109/SFCS.1985.5.  Alex Brodsky and Nicholas Pippenger. The boolean functions computed by random boolean formu- las or how to grow the right function. Random Structures & Algorithms, 27(4):490-519, 2005. ISSN 1098-2418. doi: 10.1002/rsa.20095. URL http://dx.doi.org/10.1002/rsa. 20095.  M. Dubiner and U. Zwick. Amplification and percolation [probabilistic boolean functions].  In Foundations of Computer Science, 1992. Proceedings., 33rd Annual Symposium on, pages 258- 267, Oct 1992. doi: 10.1109/SFCS.1992.267766.  Vitaly Feldman and Leslie G. Valiant. Experience-induced neural circuits that achieve high capacity.  Neural Computation, 21(10):2715-2754, 2009. doi: 10.1162/neco.2009.08-08-851.  Herv\u00b4e Fournier, Dani`ele Gardy, and Antoine Genitrini. Balanced and/or trees and linear threshold functions. In Proceedings of the Meeting on Analytic Algorithmics and Combinatorics, ANALCO \u201909, pages 51-57, Philadelphia, PA, USA, 2009. Society for Industrial and Applied Mathematics. URL http://dl.acm.org/citation.cfm?id=2791158.2791166.  Joel Friedman. Constructing O(nlogn) size monotone formulae for the kth threshold function of n  boolean variables. SIAM Journal on Computing, 15(3):641-654, 1986.  Sally A Goldman, Michael J Kearns, and Robert E Schapire. Exact identification of read-once formulas using fixed points of amplification functions. SIAM Journal on Computing, 22(4):705- 726, 1993.  Shlomo Hoory, Avner Magen, and Toniann Pitassi. Monotone circuits for the majority function. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 410-425. Springer, 2006.  Michael Luby, Michael Mitzenmacher, and Mohammad Amin Shokrollahi. Analysis of random  processes via and-or tree evaluation. In SODA, volume 98, pages 364-373, 1998.  Edward F Moore and Claude E Shannon. Reliable circuits using less reliable relays. Journal of the  Franklin Institute, 262(3):191-208, 1956.  18   PAPADIMITRIOU PETTI VEMPALA  Acknowledgements. We thank an anonymous referee for helpful comments. This work was sup- ported in part by NSF awards CCF-1217793, EAGER-1415498 and EAGER-1555447.  References  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161-182, 2006. doi: 10.1007/s10994-006-6265-7. URL http://dx.doi.org/10.1007/s10994-006-6265-7.  Rosa I. Arriaga, David Rutter, Maya Cakmak, and Santosh S. Vempala. Visual categorization with random projection. Neural Computation, 27(10):2132-2147, 2015. doi: 10.1162/NECO a 00769. URL http://dx.doi.org/10.1162/NECO_a_00769.  Ravi B. Boppana. Amplification of probabilistic boolean formulas. In Foundations of Computer Science, 1985., 26th Annual Symposium on, pages 20-29, Oct 1985. doi: 10.1109/SFCS.1985.5.  Alex Brodsky and Nicholas Pippenger. The boolean functions computed by random boolean formu- las or how to grow the right function. Random Structures & Algorithms, 27(4):490-519, 2005. ISSN 1098-2418. doi: 10.1002/rsa.20095. URL http://dx.doi.org/10.1002/rsa. 20095.  M. Dubiner and U. Zwick. Amplification and percolation [probabilistic boolean functions].  In Foundations of Computer Science, 1992. Proceedings., 33rd Annual Symposium on, pages 258- 267, Oct 1992. doi: 10.1109/SFCS.1992.267766.  Vitaly Feldman and Leslie G. Valiant. Experience-induced neural circuits that achieve high capacity.  Neural Computation, 21(10):2715-2754, 2009. doi: 10.1162/neco.2009.08-08-851.  Herv\u00b4e Fournier, Dani`ele Gardy, and Antoine Genitrini. Balanced and/or trees and linear threshold functions. In Proceedings of the Meeting on Analytic Algorithmics and Combinatorics, ANALCO \u201909, pages 51-57, Philadelphia, PA, USA, 2009. Society for Industrial and Applied Mathematics. URL http://dl.acm.org/citation.cfm?id=2791158.2791166.  Joel Friedman. Constructing O(nlogn) size monotone formulae for the kth threshold function of n  boolean variables. SIAM Journal on Computing, 15(3):641-654, 1986.  Sally A Goldman, Michael J Kearns, and Robert E Schapire. Exact identification of read-once formulas using fixed points of amplification functions. SIAM Journal on Computing, 22(4):705- 726, 1993.  Shlomo Hoory, Avner Magen, and Toniann Pitassi. Monotone circuits for the majority function. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 410-425. Springer, 2006.  Michael Luby, Michael Mitzenmacher, and Mohammad Amin Shokrollahi. Analysis of random  processes via and-or tree evaluation. In SODA, volume 98, pages 364-373, 1998.  Edward F Moore and Claude E Shannon. Reliable circuits using less reliable relays. Journal of the  Franklin Institute, 262(3):191-208, 1956. CORTICAL COMPUTATION VIA ITERATIVE CONSTRUCTIONS  Christos H. Papadimitriou and Santosh S. Vempala. Cortical learning via prediction. In Proc. of  Christos H. Papadimitriou and Santosh S. Vempala. Cortical computation.  In Proc. of PODC,  COLT, 2015a.  2015b.  E. Rosch. Principles of categorization. In Eleanor Rosch and Barbara Lloyd, editors, Cognition and  Categorization. Lawrence Elbaum Associates, 1978.  Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray, David M. Johnson, and Penny Boyes-braem.  Basic objects in natural categories. COGNITIVE PSYCHOLOGY, 8:382-439, 1976.  Petr Savicky. Boolean functions represented by random formulas. Commentationes Mathematicae  Universitatis Carolinae, 28(2):397-398, 1987.  Petr Savicky. Random boolean formulas representing any boolean function with asymptotically equal probability. Discrete Mathematics, 83(1):95 - 103, 1990. ISSN 0012-365X. doi: http: //dx.doi.org/10.1016/0012-365X(90)90223-5. URL http://www.sciencedirect.com/ science/article/pii/0012365X90902235.  Rocco A. Servedio. Monotone boolean formulas can approximate monotone linear threshold functions. Discrete Applied Mathematics, 142(13):181 - 187, 2004. ISSN 0166-218X. doi: http://dx.doi.org/10.1016/j.dam.2004.02.003. URL http://www.sciencedirect.com/ science/article/pii/S0166218X04000174. Boolean and Pseudo-Boolean Func- tions.  Leslie G. Valiant. Short monotone formulae for the majority function. Journal of Algorithms, 5(3),  1984.  Leslie G. Valiant. Circuits of the mind. Oxford University Press, 1994. ISBN 978-0-19-508926-4.  Leslie G. Valiant. A neuroidal architecture for cognitive computation. J. ACM, 47(5):854-882,  2000. doi: 10.1145/355483.355486.  Leslie G. Valiant. Memorization and association on a realistic neural model. Neural Computation,  17(3):527-555, 2005. doi: 10.1162/0899766053019890. "}, "When can we rank well from comparisons of O(n\\log(n)) non-actively chosen pairs?": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "When can we rank well from comparisons of O(n\\log(n)) non-actively chosen pairs?", "abstract": "Ranking from pairwise comparisons is a ubiquitous problem and has been studied in disciplines ranging from statistics to operations research and from theoretical computer science to machine learning. Here we consider a general setting where outcomes of pairwise comparisons between items i and j are drawn probabilistically by flipping a coin with unknown bias P_ij , and ask under what conditions on these unknown probabilities one can learn a good ranking from comparisons of only O(n\\log(n)) non-actively chosen pairs. Recent work has established this is possible under the Bradley-Terry-Luce (BTL) and noisy permutation (NP) models. Here we introduce a broad family of \u2018low-rank\u2019 conditions on the probabilities P_ij under which the resulting preference matrix P has low rank under some link function, and show these conditions encompass the BTL and Thurstone classes as special cases, but are considerably more general. We then give a new algorithm called low-rank pairwise ranking (LRPR) which provably learns a good ranking from comparisons of only O(n\\log(n)) randomly chosen comparisons under such low-rank models. Our algorithm and analysis make use of tools from the theory of low-rank matrix completion, and provide a new perspective on the problem of ranking from pairwise comparisons in non-active settings.", "pdf_url": "http://proceedings.mlr.press/v49/rajkumar16.pdf", "keywords": ["Ranking", "pairwise comparisons", "low-rank matrix completion"], "reference": "Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost  optimal query complexity. Journal of Machine Learning Research, 13:137-164, 2012.  Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking  and clustering. Journal of the ACM, 55(5):23, 2008.  Noga Alon. Ranking tournaments. SIAM Journal of Discrete Mathematics, 20(1):137-142, 2006.  Ralph A. Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. The method  of paired comparisons. Biometrika, pages 324-345, 1952.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the  19th Annual ACM-SIAM Symposium on Discrete Algorithms, 2008.  Mark Braverman and Elchanan Mossel.  Sorting from noisy information.  arXiv preprint  arXiv:0910.1191, 2009.  R\u00b4obert Busa-Fekete and Eyke H\u00a8ullermeier. A survey of preference-based online learning with bandit algorithms. In Proceedings of the 25th International Conference on Algorithmic Learning Theory, 2014.  Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algorithm for  matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational Mathematics, 9(6):717-772, 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  A. H. Copeland. A \u2018reasonable\u2019 social welfare function.  In Seminar on Mathematics in Social  Sciences, University of Michigan, 1951.  Don Coppersmith, Lisa Fleischer, and Atri Rudra. Ordering by weighted number of wins gives a good ranking for weighted tournaments. In Proceedings of the 17th Annual ACM-SIAM Sympo- sium on Discrete Algorithms, 2006.  H. A. David. The Method of Paired Comparisons. Oxford University Press, 1988.  David F. Gleich and Lek-Heng Lim. Rank aggregation via nuclear norm minimization. In Pro- ceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011.  David R. Hunter. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics,  32(1):384-406, 2004.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value  projection. In Advances in Neural Information Processing Systems, 2010.  25   WHEN CAN WE RANK WELL FROM COMPARISONS OF O(n log n) NON-ACTIVELY CHOSEN PAIRS?  References  Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost  optimal query complexity. Journal of Machine Learning Research, 13:137-164, 2012.  Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking  and clustering. Journal of the ACM, 55(5):23, 2008.  Noga Alon. Ranking tournaments. SIAM Journal of Discrete Mathematics, 20(1):137-142, 2006.  Ralph A. Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. The method  of paired comparisons. Biometrika, pages 324-345, 1952.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the  19th Annual ACM-SIAM Symposium on Discrete Algorithms, 2008.  Mark Braverman and Elchanan Mossel.  Sorting from noisy information.  arXiv preprint  arXiv:0910.1191, 2009.  R\u00b4obert Busa-Fekete and Eyke H\u00a8ullermeier. A survey of preference-based online learning with bandit algorithms. In Proceedings of the 25th International Conference on Algorithmic Learning Theory, 2014.  Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen. A singular value thresholding algorithm for  matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-  dations of Computational Mathematics, 9(6):717-772, 2009.  Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix com-  pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  A. H. Copeland. A \u2018reasonable\u2019 social welfare function.  In Seminar on Mathematics in Social  Sciences, University of Michigan, 1951.  Don Coppersmith, Lisa Fleischer, and Atri Rudra. Ordering by weighted number of wins gives a good ranking for weighted tournaments. In Proceedings of the 17th Annual ACM-SIAM Sympo- sium on Discrete Algorithms, 2006.  H. A. David. The Method of Paired Comparisons. Oxford University Press, 1988.  David F. Gleich and Lek-Heng Lim. Rank aggregation via nuclear norm minimization. In Pro- ceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011.  David R. Hunter. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics,  32(1):384-406, 2004.  Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular value  projection. In Advances in Neural Information Processing Systems, 2010. RAJKUMAR AGARWAL  Kevin G. Jamieson and Rob Nowak. Active ranking using pairwise comparisons. In Advances in  Neural Information Processing Systems, 2011.  Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial Hodge  theory. Mathematical Programming, 127(1):203-244, 2011.  James P. Keener. The Perron-Frobenius theorem and the ranking of football teams. SIAM Review,  35(1):80-93, 1993.  43-62, 1955.  Maurice G. Kendall. Further contributions to the theory of paired comparisons. Biometrics, 11(1):  Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In Proceedings of the  39th Annual ACM Symposium on Theory of Computing, 2007.  Raghunandan Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy en-  tries. In Advances in Neural Information Processing Systems, 2009.  Jean Lafond. Low rank matrix completion with exponential family noise. In Proceedings of the  28th Conference on Learning Theory, 2015.  Tyler Lu and Craig Boutilier. Learning Mallows models with pairwise preferences. In Proceedings  of the 28th International Conference on Machine Learning, 2011.  R. D. Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley, 1959.  Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank Centrality: Ranking from pair-wise  comparisons. arXiv preprint arXiv:1209.1688, 2012.  Arun Rajkumar and Shivani Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In Proceedings of the 31st International Conference on Machine Learning, 2014.  Arun Rajkumar, Suprovat Ghoshal, Lek-Heng Lim, and Shivani Agarwal. Ranking from stochastic pairwise preferences: Recovering Condorcet winners and tournament solution sets at the top. In Proceedings of the 32nd International Conference on Machine Learning, 2015.  Thomas L. Saaty. The Analytic Hierarchy Process. McGraw Hill, 1980.  Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, and Martin J. Wainwright. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. arXiv preprint arXiv:1510.05610.v2, 2015.  Louis L. Thurstone. A law of comparative judgment. Psychological Review, 34(4):273, 1927.  Fabian L. Wauthier, Michael I. Jordan, and Nebojsa Jojic. Efficient ranking from pairwise compar-  isons. In Proceedings of the 30th International Conference on Machine Learning, 2013.  Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The K-armed dueling bandits  problem. Journal of Computer and System Sciences, 78(5):1538-1556, 2012. "}, "How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods", "abstract": "We consider the problem of approximating partition functions for Ising models. We make use of recent tools in combinatorial optimization: the Sherali-Adams and Lasserre convex programming hierarchies, in combination with variational methods to get algorithms for calculating partition functions in these families. These techniques give new, non-trivial approximation guarantees for the partition function beyond the regime of correlation decay. They also generalize some classical results from statistical physics about the Curie-Weiss ferromagnetic Ising model, as well as provide a partition function counterpart of classical results about max-cut on dense graphs (Arora, 1995). With this, we connect techniques from two apparently disparate research areas \u2013 optimization and counting/partition function approximations. (i.e. #-P type of problems). Furthermore, we design to the best of our knowledge the first provable, convex variational methods. Though in the literature there are a host of convex versions of variational methods, they come with no guarantees (apart from some extremely special cases, like e.g. the graph has a single cycle). We consider dense and low rank graphs, and interestingly, the reason our approach works on these types of graphs is because local correlations propagate to global correlations \u2013 completely the opposite of algorithms based on correlation decay. In the process we design novel entropy approximations based on the low-order moments of a distribution. Our proof techniques are very simple and generic, and likely to be applicable to many other settings other than Ising models.", "pdf_url": "http://proceedings.mlr.press/v49/risteski16.pdf", "keywords": ["Ising models", "threshold rank", "partition function", "Lasserre hierarchy", "Sherali-Adams hierarchy", "correlation decay", "variational methods", "variational inference;"], "reference": "Sanjeev Arora and Rong Ge. New tools for graph coloring. In Approximation, Randomization, and  Combinatorial Optimization. Algorithms and Techniques, pages 1-12. Springer, 2011.  Sanjeev Arora, David Karger, and Marek Karpinski. Polynomial time approximation schemes for dense instances of np-hard problems. In Proceedings of the twenty-seventh annual ACM sympo- sium on Theory of computing, pages 284-293. ACM, 1995.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games and related problems. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Sympo- sium on, pages 563-572. IEEE, 2010.  12   RISTESKI  (cid:15) \u201cAround the transition threshold\u201d, i.e. when jJj = (cid:2)( 1  d ) for a sufficiently large constant in the (cid:2) notation, such that there is no correlation decay. Generally, unless there is some special structure, Markov Chain methods will provide no non-trivial guarantee in this regime - however, we get an order \u03f5n additive factor approximation to log Z, which translates to a (1 + \u03f5)n factor approximation of Z. We do not, to the best of our knowledge, know how to get such results using any other methods.  (cid:15) \u201cLow temperature regime\u201d, i.e. when jJj = !(1=d). In this case, in light of the variational characterization of log Z and the fact that the entropy is upper bounded by n, the dominating (i;j)2E(G) Ji;jE(cid:22)[xixj], so essentially the quality of term will typically be the energy term approximation will be dictated by the hardness of the optimization problem corresponding to the energy term. (e.g., for the anti-ferromagnetic case, where all the potentials Ji;j are negative, the optimization problem corresponding to the energy term is just max-cut, and we cannot hope for more than a constant factor approximation to log Z for general (negative) potentials.)  \u2211  6. Conclusion  We presented simple new algorithms for calculating partition functions in Ising models based on variational methods and convex programming hierarchies. To the best of our knowledge, these techniques give new, non-trivial approximation guarantees for the partition function when correla- tion decay does not hold, and are the first provable, convex variational methods. Our guarantees are for dense or low threshold rank graphs, and in the process we design novel entropy approximations based on the low-order moments of a distribution.  We barely scratched the surface, and we leave many interesting directions open. Our methods are very generic, and are probably applicable to many other classes of partition functions apart from Ising models. One natural candidate is weighted matchings due to the connections to calculating non-negative permanents.  Another intriguing question is to determine if there is a similar approach that can subsume prior results on partition function calculation in the regime of correlation decay, as our guarantees are much weaker there. This would give a convex relaxation interpretation of these types of results.  References  Sanjeev Arora and Rong Ge. New tools for graph coloring. In Approximation, Randomization, and  Combinatorial Optimization. Algorithms and Techniques, pages 1-12. Springer, 2011.  Sanjeev Arora, David Karger, and Marek Karpinski. Polynomial time approximation schemes for dense instances of np-hard problems. In Proceedings of the twenty-seventh annual ACM sympo- sium on Theory of computing, pages 284-293. ACM, 1995.  Sanjeev Arora, Boaz Barak, and David Steurer. Subexponential algorithms for unique games and related problems. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Sympo- sium on, pages 563-572. IEEE, 2010. HOW TO CALCULATE PARTITION FUNCTIONS USING CONVEX PROGRAMMING HIERARCHIES  Boaz Barak, Prasad Raghavendra, and David Steurer. Rounding semidefinite programming hier- archies via global correlation. In Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on, pages 472-481. IEEE, 2011.  Boaz Barak, Jonathan A Kelner, and David Steurer. Rounding sum-of-squares relaxations.  In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 31-40. ACM, 2014.  Alexander Barvinok. Polynomial time algorithms to approximate permanents and mixed discrimi-  nants within a simply exponential factor. Ann Arbor, 1001(48109):1109.  Mohsen Bayati, David Gamarnik, Dimitriy Katz, Chandra Nair, and Prasad Tetali. Simple deter- In Proceedings of the thirty-ninth  ministic approximation algorithms for counting matchings. annual ACM symposium on Theory of computing, pages 122-127. ACM, 2007.  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation.  the Journal of  machine Learning research, 3:993-1022, 2003.  David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-  cians. arXiv preprint arXiv:1601.00670, 2016.  Yiling Chen, Lance Fortnow, Nicolas Lambert, David M Pennock, and Jennifer Wortman. Com- plexity of combinatorial market makers. In Proceedings of the 9th ACM conference on Electronic commerce, pages 190-199. ACM, 2008.  Richard S Ellis. Entropy, large deviations, and statistical mechanics, volume 271. Springer Science  & Business Media, 2012.  Physics, 19(2):149-161, 1978.  Richard S Ellis and Charles M Newman. The statistics of curie-weiss models. Journal of Statistical  David Gamarnik and Dmitriy Katz. A deterministic approximation algorithm for computing the permanent of a 0, 1 matrix. Journal of Computer and System Sciences, 76(8):879-883, 2010.  Leonid Gurvits and Alex Samorodnitsky. Bounds on the permanent and some applications.  In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 90-99, 2014. doi: 10.1109/FOCS.2014.18. URL http: //dx.doi.org/10.1109/FOCS.2014.18.  David P Helmbold and Manfred K Warmuth. Learning permutations with exponential weights. The  Journal of Machine Learning Research, 10:1705-1736, 2009.  Tom Heskes. Convexity arguments for efficient minimization of the bethe and kikuchi free energies.  2006.  Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the ising model.  SIAM Journal on computing, 22(5):1087-1116, 1993.  Mark Jerrum and Umesh Vazirani. A mildly exponential approximation algorithm for the perma-  nent. Algorithmica, 16(4):392-401, 1996. RISTESKI  Mark Jerrum, Alistair Sinclair, and Eric Vigoda. A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries. Journal of the ACM (JACM), 51(4):671-697, 2004.  Monique Laurent. Sums of squares, moment matrices and optimization over polynomials. In Emerg-  ing applications of algebraic geometry, pages 157-270. Springer, 2009.  Liang Li, Pinyan Lu, and Yitong Yin. Approximate counting via correlation decay in spin systems. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 922-940. SIAM, 2012.  Liang Li, Pinyan Lu, and Yitong Yin. Correlation decay up to uniqueness in spin systems.  In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 67-84. SIAM, 2013.  Ofer Meshi, Ariel Jaimovich, Amir Globerson, and Nir Friedman. Convexifying the bethe free energy. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 402-410. AUAI Press, 2009.  Alistair Sinclair, Piyush Srivastava, and Marc Thurley. Approximation algorithms for two-state anti-ferromagnetic spin systems on bounded degree graphs. Journal of Statistical Physics, 155 (4):666-686, 2014.  Allan Sly. Computational transition at the uniqueness threshold.  In Foundations of Computer  Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 287-296. IEEE, 2010.  David Steurer. On the complexity of unique games and graph expansion. PhD thesis, Princeton  University, 2010.  Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational  inference. Foundations and Trends R\u20dd in Machine Learning, 1(1-2):1-305, 2008.  Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. Tree-reweighted belief propagation  algorithms and approximate ml estimation by pseudo-moment matching.  Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. A new class of upper bounds on the  log partition function. Information Theory, IEEE Transactions on, 51(7):2313-2335, 2005.  Martin J Wainwright, Michael Jordan, et al. Log-determinant relaxation for approximate inference in discrete markov random fields. Signal Processing, IEEE Transactions on, 54(6):2099-2109, 2006.  Yair Weiss. Correctness of local probability propagation in graphical models with loops. Neural  computation, 12(1):1-41, 2000.  Dror Weitz. Counting independent sets up to the tree threshold. In Proceedings of the thirty-eighth  annual ACM Symposium on Theory of Computing, pages 140-149. ACM, 2006.  Avi Wigderson. personal communication. HOW TO CALCULATE PARTITION FUNCTIONS USING CONVEX PROGRAMMING HIERARCHIES  Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding belief propagation and its  generalizations. 2003.  Yuichi Yoshida and Yuan Zhou. Approximation schemes via sherali-adams hierarchy for dense constraint satisfaction problems and assignment problems. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 423-438. ACM, 2014. "}, "Simple Bayesian Algorithms for Best Arm Identification": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Simple Bayesian Algorithms for Best Arm Identification", "abstract": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs.  An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design  after a small number of measurements. I propose three simple Bayesian algorithms for adaptively allocating measurement effort. One is Top-Two Probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant a top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. I prove that these simple algorithms satisfy a strong optimality property. In a frequestist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. I show that under the proposed algorithms this convergence occurs at an \\emphexponential rate, and the corresponding exponent is the best possible among all allocation rules.", "pdf_url": "http://proceedings.mlr.press/v49/russo16.pdf", "keywords": []}, "Interactive Algorithms: from Pool to Stream": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Interactive Algorithms: from Pool to Stream", "abstract": "We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool-based algorithms can select elements at any order, while stream-based algorithms observe elements in sequence, and can only select elements immediately after observing them. We assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further show that a maximal gap between the two settings exists also in the special case of active learning for binary classification.", "pdf_url": "http://proceedings.mlr.press/v49/sabato16.pdf", "keywords": ["Interactive algorithms", "active learning", "pool-based", "stream-based"], "reference": "A. Arlotto, E. Mossel, and J. Michael Steele. Quickest online selection of an increasing subsequence  of speci\ufb01ed size. arXiv preprint arXiv:1412.7985, 2014.  M.-F. Balcan and Phil Long. Active and passive learning of linear separators under log-concave distributions. In Proceedings of the Twenty-Sixth Annual Conference on Computational Learning Theory (COLT), pages 288\u2013316, 2013.  12   SABATO HESS  hand, for q > 22, any stream-based active learning algorithm with the same guarantee requires at least q 32  unlabeled examples in expectation.  (cid:106) m  7 log(2q)  (cid:107)  The proof is provided in "}, "Best-of-K-bandits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Best-of-K-bandits", "abstract": "This paper studies the Best-of-K Bandit game: At each time the player chooses a subset S among all N-choose-K possible options and observes reward max(X(i) : i in S) where X is a random vector drawn from a joint distribution. The objective is to identify the subset that achieves the highest expected reward with high probability using as few queries as possible. We present distribution-dependent lower bounds based on a particular construction which force a learner to consider all N-choose-K subsets, and match naive extensions of known upper bounds in the bandit setting obtained by treating each subset as a separate arm. Nevertheless, we present evidence that exhaustive search may be avoided for certain, favorable distributions because the influence of high-order order correlations may be dominated by lower order statistics. Finally, we present an algorithm and analysis for independent arms, which mitigates the surprising non-trivial information occlusion that occurs due to only observing the max in the subset. This may inform strategies for more general dependent measures, and we complement these result with independent-arm lower bounds.", "pdf_url": "http://proceedings.mlr.press/v49/simchowitz16.pdf", "keywords": [], "reference": "S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Machine Learning, 5(1):1-122, 2012.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online prob- lems. In Proceedings of The 31st International Conference on Machine Learning, pages 100-108, 2014.  Linhui Hao, Qiuling He, Zhishi Wang, Mark Craven, Michael A Newton, and Paul Ahlquist. Lim- ited agreement of independent rnai screens for virus-required host genes owes more to false- negative than false-positive factors. PLoS Comput Biol, 9(9):e1003235, 2013.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. A probabilistic method for inferring preferences from clicks. In Proceedings of the 20th ACM International Conference on Informa- tion and Knowledge Management, CIKM \u201911, pages 249-258, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0717-8. doi: 10.1145/2063576.2063618. URL http://doi.acm.org/ 10.1145/2063576.2063618.  Mark M Huycke, Daniel F Sahm, and Michael S Gilmore. Multiple-drug resistant enterococci: the nature of the problem and an agenda for the future. Emerging infectious diseases, 4(2):239, 1998.  Rishabh K Iyer and Jeff A Bilmes. Submodular optimization with submodular cover and sub- In Advances in Neural Information Processing Systems, pages  modular knapsack constraints. 2436-2444, 2013.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck. lil\u2019ucb: An optimal ex- ploration algorithm for multi-armed bandits. In Proceedings of The 27th Conference on Learning Theory, pages 423-439, 2014.  Kwang-Sung Jun, Kevin Jamieson, Rob Nowak, and Xiaojin Zhu. Top arm identification in multi- armed bandits with batch arm pulls. In The 19th International Conference on Artificial Intelli- gence and Statistics (AISTATS), 2016.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. The Journal of Machine Learning Research, 2015.  Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 767-776, 2015.  Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penal-  ization. arXiv preprint arXiv:0907.3740, 2009.  Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi- armed bandits. In Proceedings of the 25th international conference on Machine learning, pages 784-791. ACM, 2008.  15   BEST-OF-K BANDITS  References  S\u00b4ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Machine Learning, 5(1):1-122, 2012.  Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online prob- lems. In Proceedings of The 31st International Conference on Machine Learning, pages 100-108, 2014.  Linhui Hao, Qiuling He, Zhishi Wang, Mark Craven, Michael A Newton, and Paul Ahlquist. Lim- ited agreement of independent rnai screens for virus-required host genes owes more to false- negative than false-positive factors. PLoS Comput Biol, 9(9):e1003235, 2013.  Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. A probabilistic method for inferring preferences from clicks. In Proceedings of the 20th ACM International Conference on Informa- tion and Knowledge Management, CIKM \u201911, pages 249-258, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0717-8. doi: 10.1145/2063576.2063618. URL http://doi.acm.org/ 10.1145/2063576.2063618.  Mark M Huycke, Daniel F Sahm, and Michael S Gilmore. Multiple-drug resistant enterococci: the nature of the problem and an agenda for the future. Emerging infectious diseases, 4(2):239, 1998.  Rishabh K Iyer and Jeff A Bilmes. Submodular optimization with submodular cover and sub- In Advances in Neural Information Processing Systems, pages  modular knapsack constraints. 2436-2444, 2013.  Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00b4ebastien Bubeck. lil\u2019ucb: An optimal ex- ploration algorithm for multi-armed bandits. In Proceedings of The 27th Conference on Learning Theory, pages 423-439, 2014.  Kwang-Sung Jun, Kevin Jamieson, Rob Nowak, and Xiaojin Zhu. Top arm identification in multi- armed bandits with batch arm pulls. In The 19th International Conference on Artificial Intelli- gence and Statistics (AISTATS), 2016.  Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-  tion. In Conference on Learning Theory, pages 228-251, 2013.  Emilie Kaufmann, Olivier Capp\u00b4e, and Aur\u00b4elien Garivier. On the complexity of best arm identifica-  tion in multi-armed bandit models. The Journal of Machine Learning Research, 2015.  Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 767-776, 2015.  Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penal-  ization. arXiv preprint arXiv:0907.3740, 2009.  Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi- armed bandits. In Proceedings of the 25th international conference on Machine learning, pages 784-791. ACM, 2008. SIMCHOWITZ JAMIESON RECHT  Karthik Raman, Pannaga Shivaswamy, and Thorsten Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, KDD \u201912, pages 705-713, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1462-6. doi: 10.1145/2339530.2339642. URL http://doi.acm.org/ 10.1145/2339530.2339642.  Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions.  In Advances in Neural Information Processing Systems, pages 1577-1584, 2009.  Vijay V Vazirani. Approximation algorithms. Springer Science & Business Media, 2013.  Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified  retrieval. In Advances in Neural Information Processing Systems, pages 2483-2491, 2011. BEST-OF-K BANDITS  "}, "Memory, Communication, and Statistical Queries": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Memory, Communication, and Statistical Queries", "abstract": "If a concept class can be represented with a certain amount of memory, can it be efficiently learned with the same amount of memory? What concepts can be efficiently learned by algorithms that extract only a few bits of information from each example? We introduce a formal framework for studying these questions, and investigate the relationship between the fundamental resources of memory or communication and the sample complexity of the learning task. We relate our memory-bounded and communication-bounded learning models to the well-studied statistical query model. This connection can be leveraged to obtain both upper and lower bounds: we show strong lower bounds on learning parity functions with bounded communication, as well as upper bounds on solving sparse linear regression problems with limited memory.", "pdf_url": "http://proceedings.mlr.press/v49/steinhardt16.pdf", "keywords": [], "reference": "A. Agarwal, S. Negahban, and M.J. Wainwright. Stochastic optimization and sparse statistical re- covery: Optimal algorithms for high dimensions. In Advances in Neural Information Processing Systems (NIPS), 2012.  E. Arias-Castro, E.J. Candes, and M.A. Davenport. On the fundamental limits of adaptive sensing.  IEEE Transactions on Information Theory, 59(1):472-481, 2013.  R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In Advances  in Neural Information Processing Systems (NIPS), 2013.  P. Assouad. Deux remarques sur l\u2019estimation. Comptes rendus des s\u00b4eances de l\u2019Acad\u00b4emie des  sciences. S\u00b4erie 1, Math\u00b4ematique, 296(23):1021-1024, 1983.  M.F. Balcan and V. Feldman. Statistical active learning algorithms for noise tolerance and differen-  tial privacy. In Advances in Neural Information Processing Systems (NIPS), 2013.  M.F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity  and privacy. In Conference on Learning Theory (COLT), 2012.  Z. Bar-Yossef, T.S. Jayram, R. Kumar, and D. Sivakumar. An information statistics approach to data stream and communication complexity. Journal of Computer and System Science, 68:702-732, 2004.  S. Ben-David and E. Dichterman. Learning with restricted focus of attention. Journal of Computer  and System Sciences, 56:277-298, 1998.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In Proceedings of the ACM Symposium on Theory of Computing (STOC), pages 253-262. ACM, 1994.  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statisti-  cal query model. Journal of the ACM, 50(4):506-519, 2003.  14   STEINHARDT VALIANT WAGER  In Proposition 16, we use the fact that \u03c1 is a metric to reduce from improper learning to proper learning (by replacing an output concept \u02c6p (cid:54)\u2208 F by some nearby concept in F); note that this reduction is not computationally efficient in general.  Similarly to Proposition 16, if our constraint is on the number of information-theoretic bits of  communication, then there is essentially no separation between PAC-learnability and communication- constrained PAC-learnability:  Proposition 17 If a concept class is (\u03b5, \u03b4)-learnable from poly(n) samples that can each be rep- resented with poly(n) bits, then it is (\u03b5, \u03b4)-learnable from poly(n) samples and one-way communi- cation, with messages each of whose entropies are at most 1 bit.  We thank the anonymous reviewers, who provided many helpful comments to improve the paper. We also thank Ran Raz and Percy Liang for helpful and insightful conversations.  Acknowledgments  References  A. Agarwal, S. Negahban, and M.J. Wainwright. Stochastic optimization and sparse statistical re- covery: Optimal algorithms for high dimensions. In Advances in Neural Information Processing Systems (NIPS), 2012.  E. Arias-Castro, E.J. Candes, and M.A. Davenport. On the fundamental limits of adaptive sensing.  IEEE Transactions on Information Theory, 59(1):472-481, 2013.  R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In Advances  in Neural Information Processing Systems (NIPS), 2013.  P. Assouad. Deux remarques sur l\u2019estimation. Comptes rendus des s\u00b4eances de l\u2019Acad\u00b4emie des  sciences. S\u00b4erie 1, Math\u00b4ematique, 296(23):1021-1024, 1983.  M.F. Balcan and V. Feldman. Statistical active learning algorithms for noise tolerance and differen-  tial privacy. In Advances in Neural Information Processing Systems (NIPS), 2013.  M.F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity  and privacy. In Conference on Learning Theory (COLT), 2012.  Z. Bar-Yossef, T.S. Jayram, R. Kumar, and D. Sivakumar. An information statistics approach to data stream and communication complexity. Journal of Computer and System Science, 68:702-732, 2004.  S. Ben-David and E. Dichterman. Learning with restricted focus of attention. Journal of Computer  and System Sciences, 56:277-298, 1998.  A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In Proceedings of the ACM Symposium on Theory of Computing (STOC), pages 253-262. ACM, 1994.  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statisti-  cal query model. Journal of the ACM, 50(4):506-519, 2003. MEMORY, COMMUNICATION, AND STATISTICAL QUERIES  M. Braverman and A. Rao. Information equals amortized communication. In IEEE Symposium on  Foundations of Computer Science (FOCS), 2011.  M. Braverman, A. Garg, T. Ma, H.L. Nguyen, and D.P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. arXiv preprint arXiv:1506.07216, 2015.  A. Chakrabarti, Y. Shi, A. Wirth, and A. Yao. Informational complexity and the direct sum problem for simultaneous message complexity. In IEEE Symposium on Foundations of Computer Science (FOCS), 2001.  G. Cormode and S Muthukrishnan. An improved data stream summary: The count-min sketch and  its applications. Journal of Algorithms, 55(1):58-75, 2005.  T.M. Cover. Hypothesis testing with finite statistics. The Annals of Mathematical Statistics, 40(3):  828-835, 1969.  J. Duchi, M. Jordan, and M. Wainwright. Local privacy and statistical minimax rates.  In IEEE  Symposium on Foundations of Computer Science (FOCS), 2013.  V. Feldman. A complete characterization of statistical query learning with applications to evolv-  ability. In IEEE Symposium on Foundations of Computer Science (FOCS), 2009.  V. Feldman, E. Grigorescu, L. Reyzin, S. Vempala, and Y. Xiao. Statistical algorithms and a lower bound for detecting planted cliques. In Proceedings of ACM Symposium on Theory of Computing (STOC). ACM, 2013a.  Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisfiability  problems with planted solutions. arXiv preprint arXiv:1311.4821v6, 2013b.  Vitaly Feldman, Cristobal Guzman, and Santosh Vempala. Statistical query algorithms for stochas-  tic convex optimization. arXiv preprint arXiv:1512.09170, 2015.  A. Ganor, G. Kol, and R. Raz. Exponential separation of information and communication. In IEEE  Symposium on Foundations of Computer Science (FOCS), 2014.  A. Garg, T. Ma, and H. Nguyen. On communication cost of distributed statistical estimation and  dimensionality. In Advances in Neural Information Processing Systems (NIPS), 2014.  A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE, 98(6):  937-947, 2010.  A. Gupta, M. Hardt, A. Roth, and J. Ullman. Privately releasing conjunctions and the statistical query barrier. In Proceedings of the ACM Symposium on Theory of Computing (STOC), 2011.  M.E. Hellman and T.M. Cover. Learning with finite memory. The Annals of Mathematical Statistics,  41(3):765-782, 1970.  S.P. Kasiviswanathan, H.K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn  privately? SIAM Journal on Computing, 40(3):793-826, 2011. STEINHARDT VALIANT WAGER  M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):  983-1006, 1998.  NY, USA, 1997.  J. Kivinen and M.K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.  Information and Computation, 132(1):1-63, 1997.  E. Kushilevitz and N. Nisan. Communication Complexity. Cambridge University Press, New York,  J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of Machine  Learning Research, 10:777-801, 2009.  L. Le Cam. Asymptotic methods in statistical theory. Springer-Verlag, New York, 1986.  T. Lee and A. Shraibman. Lower bounds in communication complexity. Foundations and Trends in  Theoretical Computer Science, 3(4):263398, 2009.  I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming PCA. In Advances in Neural  Information Processing Systems (NIPS), 2013.  F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: A lock-free approach to parallelizing stochastic  gradient descent. In Advances in Neural Information Processing Systems (NIPS), 2011.  R. Raz. Fast learning requires good memory: A time-space lower bound for parity learning. Elec-  tronic Colloquium on Computational Complexity (ECCC), 2016.  O. Reingold. Undirected connectivity in log-space. Journal of the ACM, 55(4), 2008.  S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2011.  O. Shamir. Fundamental limits of online and distributed algorithms for statistical learning and  estimation. In Advances in Neural Information Processing Systems (NIPS), 2014.  J. Steinhardt and J. Duchi. Minimax rates for memory-bounded sparse linear regression. In Confer-  ence on Learning Theory (COLT), 2015.  J. Steinhardt, S. Wager, and P. Liang. The statistics of streaming sparse regression. arXiv preprint  arXiv:1412.4182, 2014.  B. Sz\u00a8or\u00b4enyi. Characterizing statistical query learning: simplified notions and proofs. In Algorithmic  Learning Theory, pages 186-200. Springer, 2009.  A.B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Jour-  nal of Machine Learning Research, 11:2543-2596, 2010.  Y. Zhang, J. Duchi, M. Jordan, and M. Wainwright. Information-theoretic lower bounds for dis- tributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems (NIPS), 2013. MEMORY, COMMUNICATION, AND STATISTICAL QUERIES  "}, "benefits of depth in neural networks": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "benefits of depth in neural networks", "abstract": "For any positive integer k, there exist neural networks with \u0398(k^3) layers, \u0398(1) nodes per layer, and \u0398(1) distinct parameters which can not be approximated by networks with O(k) layers unless they are exponentially large \u2014 they must possess \u03a9(2^k) nodes. This result is proved here for a class of nodes termed \\emphsemi-algebraic gates which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: \u03a9(2^k^3) total tree nodes are required).", "pdf_url": "http://proceedings.mlr.press/v49/telgarsky16.pdf", "keywords": ["Neural networks", "representation", "approximation", "depth hierarchy"], "reference": "Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Yoshua Bengio and Olivier Delalleau. Shallow vs. deep sum-product networks. In NIPS, 2011.  Jacek Bochnak, Michal Coste, and Marie-Franc\u00b8oise Roy. Real Algebraic Geometry. Springer, 1998.  Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning  algorithms. pages 161-168, 2006.  John Duchi. Statistics 311/electrical engineering 377: Information theory and statistics. Stanford  University, 2016.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. 2015.  arXiv:1512.03965 [cs.LG].  Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 1980.  Johan H\u02daastad. Computational Limitations of Small Depth Circuits. PhD thesis, Massachusetts  Institute of Technology, 1986.  Daniel Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. 2015. arXiv:1511.07860v1 [cs.CC].  Andrei Kolmogorov. \u00a8Uber die beste ann\u00a8aherung von funktionen einer gegebenen funktionenklasse.  Annals of Mathematics, 37(1):107-110, 1936.  Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition. 114:953-956, 1957.  Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolu-  tional neural networks. In NIPS, 2012.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks.  2015. arXiv:1411.7717v3 [cs.LG].  Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear  regions of deep neural networks. In NIPS, 2014.  Hoifung Poon and Pedro M. Domingos. Sum-product networks: A new deep architecture. In UAI  2011, pages 337-346, 2011.  Benjamin Rossman, Rocco A. Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem  for boolean circuits. In FOCS, 2015.  13   BENEFITS OF DEPTH IN NEURAL NETWORKS  References  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-  bridge University Press, 1999.  Yoshua Bengio and Olivier Delalleau. Shallow vs. deep sum-product networks. In NIPS, 2011.  Jacek Bochnak, Michal Coste, and Marie-Franc\u00b8oise Roy. Real Algebraic Geometry. Springer, 1998.  Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning  algorithms. pages 161-168, 2006.  John Duchi. Statistics 311/electrical engineering 377: Information theory and statistics. Stanford  University, 2016.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. 2015.  arXiv:1512.03965 [cs.LG].  Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 1980.  Johan H\u02daastad. Computational Limitations of Small Depth Circuits. PhD thesis, Massachusetts  Institute of Technology, 1986.  Daniel Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. 2015. arXiv:1511.07860v1 [cs.CC].  Andrei Kolmogorov. \u00a8Uber die beste ann\u00a8aherung von funktionen einer gegebenen funktionenklasse.  Annals of Mathematics, 37(1):107-110, 1936.  Andrey Nikolaevich Kolmogorov. On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition. 114:953-956, 1957.  Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton. Imagenet classification with deep convolu-  tional neural networks. In NIPS, 2012.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks.  2015. arXiv:1411.7717v3 [cs.LG].  Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear  regions of deep neural networks. In NIPS, 2014.  Hoifung Poon and Pedro M. Domingos. Sum-product networks: A new deep architecture. In UAI  2011, pages 337-346, 2011.  Benjamin Rossman, Rocco A. Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem  for boolean circuits. In FOCS, 2015. TELGARSKY  Michael Schmitt. Lower bounds on the complexity of approximating continuous functions by sig-  moidal neural networks. In NIPS, 2000.  Lech Szymanski and Brendan McCane. Deep networks are effective encoders of periodicity. IEEE  Transactions on Neural Networks and Learning Systems, 25(10):1816-1827, 2014.  Matus Telgarsky.  Representation benefits of deep feedforward networks.  2015.  arXiv:1509.08101v2 [cs.LG].  Anatoli Vitushkin. On multidimensional variations. GITTL, 1955. In Russian.  Anatoli Vitushkin. Estimation of the complexity of the tabulation problem. Fizmatgiz., 1959. In  Russian.  Hugh E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the  American Mathematical Society, 133(1):167-178, 1968.  Karl Weierstrass.  \u00a8Uber die analytische darstellbarkeit sogenannter willk\u00a8urlicher functionen einer reellen ver\u00a8anderlichen. Sitzungsberichte der Akademie zu Berlin, pages 633-639, 789-805, 1885.  "}, "A Guide to Learning Arithmetic Circuits": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "A Guide to Learning Arithmetic Circuits", "abstract": "An \\empharithmetic circuit is a directed acyclic graph in which the operations are {+,\\times}. In this paper, we exhibit several connections between learning algorithms for arithmetic circuits and other problems. In particular, we show that: \\beginitemize \\item Efficient learning algorithms for arithmetic circuit classes imply explicit exponential lower bounds. \\item General circuits and formulas can be learned efficiently with membership and equivalence queries iff they can be learned efficiently with membership queries only. \\item Low-query learning algorithms for certain classes of circuits imply explicit rigid matrices. \\item Learning algorithms for multilinear depth-3 and depth-4 circuits must compute square roots. \\enditemize", "pdf_url": "http://proceedings.mlr.press/v49/volkovich16.pdf", "keywords": [], "reference": "M. Agrawal. Proving lower bounds via pseudo-random generators. In Proceedings of the  25th FSTTCS, volume 3821 of LNCS, pages 92-105, 2005.  M. Agrawal and V. Vinay. Arithmetic circuits: A chasm at depth four. In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 67-75, 2008.  M. Agrawal, N. Kayal, and N. Saxena. Primes is in P. Annals of Mathematics, 160(2):  781-793, 2004.  M. Anderson, D. van Melkebeek, and I. Volkovich. Derandomizing polynomial identity testing for multilinear constant-read formulae. Computational Complexity, 24(4):695- 776, 2015.  D. Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):  87-106, 1987.  22:317-330, 1983.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  W. Baur and V. Strassen. The complexity of partial derivatives. Theoretical Comp. Sci.,  D. Beaver and J. Feigenbaum. Hiding instances in multioracle queries.  In STACS, pages 37-48, 1990. doi: 10.1007/3-540-52282-4 30. URL http://dx.doi.org/10.1007/ 3-540-52282-4_30.  A. Beimel, F. Bergadano, N. H. Bshouty, E. Kushilevitz, and S. Varricchio. Learning  functions represented as multiplicity automata. J. ACM, 47(3):506-530, 2000.  A. Blum. Separating distribution-free and mistake-bound learning models over the boolean domain. SIAM J. Comput., 23(5):990-1000, 1994. doi: 10.1137/S009753979223455X. URL http://dx.doi.org/10.1137/S009753979223455X.  17   A Guide to Learning Arithmetic Circuits  (2015); Shpilka and Volkovich (2015), one could show that any reconstruction algorithm for read-twice formulas or even sums of read-once formulas must compute square roots.  Finally, can one prove other consequences from e\ufb03cient learnability of arithmetic cir- cuits? In particular, can one show that certain learning algorithms imply integer factoriza- tion?  Acknowledgment  The author would like to thank Zeev Dvir for pointing out that the techniques used in the paper could show that e\ufb03cient learning algorithm imply explicit rigid matrices, which resulted in Theorem 4. The author would also like to thank Michael Forbes and Rocco Servedio for useful conversations. Finally, the author would like to thank the anonymous referees for useful comments that improved the presentation of the paper.  References  M. Agrawal. Proving lower bounds via pseudo-random generators. In Proceedings of the  25th FSTTCS, volume 3821 of LNCS, pages 92-105, 2005.  M. Agrawal and V. Vinay. Arithmetic circuits: A chasm at depth four. In Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 67-75, 2008.  M. Agrawal, N. Kayal, and N. Saxena. Primes is in P. Annals of Mathematics, 160(2):  781-793, 2004.  M. Anderson, D. van Melkebeek, and I. Volkovich. Derandomizing polynomial identity testing for multilinear constant-read formulae. Computational Complexity, 24(4):695- 776, 2015.  D. Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):  87-106, 1987.  22:317-330, 1983.  D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1988.  W. Baur and V. Strassen. The complexity of partial derivatives. Theoretical Comp. Sci.,  D. Beaver and J. Feigenbaum. Hiding instances in multioracle queries.  In STACS, pages 37-48, 1990. doi: 10.1007/3-540-52282-4 30. URL http://dx.doi.org/10.1007/ 3-540-52282-4_30.  A. Beimel, F. Bergadano, N. H. Bshouty, E. Kushilevitz, and S. Varricchio. Learning  functions represented as multiplicity automata. J. ACM, 47(3):506-530, 2000.  A. Blum. Separating distribution-free and mistake-bound learning models over the boolean domain. SIAM J. Comput., 23(5):990-1000, 1994. doi: 10.1137/S009753979223455X. URL http://dx.doi.org/10.1137/S009753979223455X. Volkovich  N. H. Bshouty, T. R. Hancock, and L. Hellerstein. Learning arithmetic read-once formulas.  SIAM J. on Computing, 24(4):706-735, 1995.  M. L. Carmosino, R. Impagliazzo, V. Kabanets, and A. Kolokolova. Algorithms from natural lower bounds. Electronic Colloquium on Computational Complexity (ECCC), 23:8, 2016. URL http://eccc.hpi-web.de/report/2016/008.  A. Daniely. Complexity theoretic limitations on learning halfspaces. CoRR, abs/1505.05800,  2015. URL http://arxiv.org/abs/1505.05800.  A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning dnf\u2019s.  CoRR, abs/1404.3378, 2014. URL http://arxiv.org/abs/1404.3378.  R. A. DeMillo and R. J. Lipton. A probabilistic remark on algebraic program testing. Inf.  Process. Lett., 7(4):193-195, 1978.  Z. Dvir and A. Shpilka. Locally decodable codes with 2 queries and polynomial identity  testing for depth 3 circuits. SIAM J. on Computing, 36(5):1404-1434, 2006.  M. Forbes and A. Shpilka. Quasipolynomial-time identity testing of non-commutative and read-once oblivious algebraic branching programs. In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 243-252, 2013. Full version at http://eccc.hpi-web.de/report/2012/115.  L. Fortnow and A. R. Klivans. E\ufb03cient learning algorithms yield circuit lower bounds. J.  Comput. Syst. Sci., 75(1):27-36, 2009.  S. Gao, E. Kaltofen, and A. G. B. Lauder. Deterministic distinct-degree factorization of  polynomials over finite fields. J. Symb. Comput., 38(6):1461-1470, 2004.  J. von zur Gathen and J. Gerhard. Modern computer algebra. Cambridge University Press,  1999.  C. Gentry and Sh. Halevi. Fully homomorphic encryption without squashing using depth-3 arithmetic circuits. In Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 107-109, 2011.  Sh. Goldwasser and S. Micali. Probabilistic encryption. J. Comput. Syst. Sci., 28(2):270-  299, 1984. doi: 10.1016/0022-0000(84)90070-9.  A. Gupta, N. Kayal, and S. V. Lokam. Reconstruction of depth-4 multilinear circuits with top fanin 2. In Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC), pages 625-642, 2012. Full version at http://eccc.hpi-web.de/report/2011/153.  J. H\u02daastad. The shrinkage exponent is 2. SIAM J. on Computing, 27:48-64, 1998.  J. Heintz and C. P. Schnorr. Testing polynomials which are easy to compute (extended abstract). In Proceedings of the 12th Annual ACM Symposium on Theory of Computing (STOC), pages 262-272, 1980. A Guide to Learning Arithmetic Circuits  R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press,  1991.  R. Impagliazzo. A personal view of average-case complexity. In Proceedings of the Tenth An- nual Structure in Complexity Theory Conference, Minneapolis, Minnesota, USA, June 19- 22, 1995, pages 134-147, 1995. URL http://dx.doi.org/10.1109/SCT.1995.514853.  V. Kabanets and R. Impagliazzo. Derandomizing polynomial identity tests means proving  circuit lower bounds. Computational Complexity, 13(1-2):1-46, 2004.  Z. S. Karnin and A. Shpilka. Reconstruction of generalized depth-3 arithmetic cir- In Proceedings of the 24th Annual IEEE Confer- Full version at  cuits with bounded top fan-in. ence on Computational Complexity (CCC), pages 274-285, 2009. http://www.cs.technion.ac.il/ shpilka/publications/KarninShpilka09.pdf.  Z. S. Karnin, P. Mukhopadhyay, A. Shpilka, and I. Volkovich. Deterministic identity testing of depth 4 multilinear circuits with bounded top fan-in. SIAM J. on Computing, 42(6): 2114-2131, 2013.  N. Kayal. Derandomizing some number-theoretic and algebraic algorithms. PhD thesis,  Indian Institute of Technology, Kanpur, India, 2007.  M. J. Kearns and L. G. Valiant. Cryptographic limitations on learning boolean formulae  and finite automata. J. ACM, 41(1):67-95, 1994.  A. Klivans and A. Shpilka. Learning restricted models of arithmetic circuits. Theory of  computing, 2(10):185-206, 2006.  A. Klivans and D. Spielman. Randomness e\ufb03cient identity testing of multivariate poly- nomials. In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing (STOC), pages 216-223, 2001.  A. Klivans, P. Kothari, and I. Oliveira. Constructing hard functions from learning algo- rithms. In Proceedings of the 28th Annual IEEE Conference on Computational Complexity (CCC), pages 86-97, 2013.  A. R. Klivans and A. A. Sherstov. Cryptographic hardness for learning intersections of  halfspaces. J. Comput. Syst. Sci., 75(1):2-12, 2009.  O. Lachish and R. Raz. Explicit lower bound of 4.5n \u2212 o(n) for boolean circuits.  In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing (STOC), pages 399-408, 2001.  A.K. Lenstra, H.W. Lenstr, and L. Lov\u00b4asz. Factoring polynomials with rational coe\ufb03cients.  Mathematische Annalen,, 261(4):515-534, 1982.  R. J. Lipton and N. K. Vishnoi. Deterministic identity testing for multivariate polynomi- als. In Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 756-760, 2003. Volkovich  S. V. Lokam. Complexity lower bounds using linear algebra. Foundations and Trends in Theoretical Computer Science, 4(1-2):1-155, 2009. doi: 10.1561/0400000011. URL http://dx.doi.org/10.1561/0400000011.  K. Mulmuley, U. Vazirani, and V. Vazirani. Matching is as easy as matrix inversion. Com-  binatorica, 7(1):105-113, 1987.  M. O. Rabin. How to exchange secrets with oblivious transfer. IACR Cryptology ePrint  Archive, 2005:187, 2005. URL http://eprint.iacr.org/2005/187.  R. Raz. Multi-linear formulas for permanent and determinant are of super-polynomial size.  J. ACM, 56(2), 2009.  R. Raz, A. Shpilka, and A. Yehudayo\ufb00. A lower bound for the size of syntactically multilinear  arithmetic circuits. SIAM J. on Computing, 38(4):1624-1647, 2008.  S. Saraf and I. Volkovich. Blackbox identity testing for depth-4 multilinear circuits.  In Proceedings of the 43rd Annual ACM Symposium on Theory of Computing (STOC), pages 421-430, 2011. Full version at http://eccc.hpi-web.de/report/2011/046.  S. Saraf and S. Yekhanin. Noisy interpolation of sparse polynomials, and applications. In Proceedings of the 26th Annual IEEE Conference on Computational Complexity CCC, pages 86-92, 2011. doi: 10.1109/CCC.2011.38. URL http://dx.doi.org/10.1109/CCC. 2011.38.  J. T. Schwartz. Fast probabilistic algorithms for verification of polynomial identities. J.  ACM, 27(4):701-717, 1980.  V. Shoup. A fast deterministic algorithm for factoring polynomials over finite fields of small  characteristic. In ISSAC, pages 14-21, 1991.  A. Shpilka and I. Volkovich. On the relation between polynomial identity testing and In Automata, Languages and Programming, 37th In- finding variable disjoint factors. ternational Colloquium (ICALP), pages 408-419, 2010. Full version at http://eccc.hpi- web.de/report/2010/036.  A. Shpilka and I. Volkovich. On reconstruction and testing of read-once formulas. Theory  of Computing, 10:465-514, 2014.  plexity, 24(3):477-532, 2015.  A. Shpilka and I. Volkovich. Read-once polynomial identity testing. Computational Com-  A. Shpilka and A. Yehudayo\ufb00. Arithmetic circuits: A survey of recent results and open questions. Foundations and Trends in Theoretical Computer Science, 5(3-4):207-388, 2010.  L. G. Valiant. Graph-theoretic arguments in low-level complexity.  In Lecture notes in  Computer Science, volume 53, pages 162-176. Springer, 1977.  L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142,  1984. A Guide to Learning Arithmetic Circuits  I. Volkovich. On some computations on sparse polynomials. Manuscript, 2015. (submitted).  R. Zippel. Probabilistic algorithms for sparse polynomials. In Proceedings of the Interna-  tional Symposium on Symbolic and Algebraic Computation, pages 216-226, 1979.  "}, "Online learning in repeated auctions": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "Online learning in repeated auctions", "abstract": "Motivated by online advertising auctions, we consider repeated Vickrey auctions where goods of unknown value are sold sequentially and bidders only learn (potentially noisy) information about a good\u2019s value once it is purchased. We adopt an online learning approach with bandit feedback to  model this problem and derive bidding strategies for two models: stochastic and adversarial. In the stochastic model, the observed values of the goods are  random variables centered around the true value of the good. In this case, logarithmic regret is achievable when competing against well behaved adversaries. In the adversarial model, the goods need not be identical. Comparing our performance against that of the best fixed bid in hindsight, we show that sublinear regret is also achievable in this case. For both the stochastic and adversarial models, we prove matching minimax lower bounds showing our strategies to be optimal up to lower-order terms. To our knowledge, this is the first complete set of strategies for bidders participating in auctions of this type.", "pdf_url": "http://proceedings.mlr.press/v49/weed16.pdf", "keywords": ["Second price auctions", "Vickrey auctions", "Repeated auctions", "Bandit problems"], "reference": "Kareem Amin, Michael Kearns, Peter Key, and Anton Schwaighofer. Budget optimization for sponsored search: Censored learning in MDPs. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012, pages 54-63, 2012.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages URL http://papers.nips.cc/paper/ 622-630. Curran Associates, 5589-repeated-contextual-auctions-with-strategic-buyers.pdf.  Inc., 2014.  Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, and Aaron Roth. Online learning and profit maximization from revealed preferences. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 770-776, 2015.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Mach. Learn., 47(2-3):235-256, 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77 (electronic), 2002/03.  Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. Characterizing truthful multi-armed bandit mechanisms: Extended abstract. In Proceedings of the 10th ACM Conference on Elec- tronic Commerce, EC \u201909, pages 79-88, New York, NY, USA, 2009. ACM. ISBN 978-1- 60558-458-4. doi: 10.1145/1566374.1566386. URL http://doi.acm.org/10.1145/ 1566374.1566386.  Moshe Babaioff, Robert D. Kleinberg, and Aleksandrs Slivkins. Truthful mechanisms with implicit payment computation. In Proceedings of the 11th ACM Conference on Electronic Commerce, EC \u201910, pages 43-52, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-822-3. doi: 10.1145/ 1807342.1807349. URL http://doi.acm.org/10.1145/1807342.1807349.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS \u201913, pages 207-216, Washington, DC, USA, 2013. IEEE Computer Society. ISBN 978-0- 7695-5135-7. doi: 10.1109/FOCS.2013.30. URL http://dx.doi.org/10.1109/FOCS. 2013.30.  Santiago R. Balseiro, Omar Besbes, and Gabriel Y. Weintraub. Repeated auctions with budgets in ad exchanges: Approximations and design. Manage. Sci., 61(4):864-884, April 2015. ISSN 0025-1909. doi: 10.1287/mnsc.2014.2022. URL http://dx.doi.org/10.1287/mnsc. 2014.2022.  14   WEED PERCHET RIGOLLET  References  Kareem Amin, Michael Kearns, Peter Key, and Anton Schwaighofer. Budget optimization for sponsored search: Censored learning in MDPs. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012, pages 54-63, 2012.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages URL http://papers.nips.cc/paper/ 622-630. Curran Associates, 5589-repeated-contextual-auctions-with-strategic-buyers.pdf.  Inc., 2014.  Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, and Aaron Roth. Online learning and profit maximization from revealed preferences. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 770-776, 2015.  Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit  problem. Mach. Learn., 47(2-3):235-256, 2002.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL http://dx.doi.org/10.1137/ S0097539701398375.  SIAM J. Comput., 32(1):48-77 (electronic), 2002/03.  Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. Characterizing truthful multi-armed bandit mechanisms: Extended abstract. In Proceedings of the 10th ACM Conference on Elec- tronic Commerce, EC \u201909, pages 79-88, New York, NY, USA, 2009. ACM. ISBN 978-1- 60558-458-4. doi: 10.1145/1566374.1566386. URL http://doi.acm.org/10.1145/ 1566374.1566386.  Moshe Babaioff, Robert D. Kleinberg, and Aleksandrs Slivkins. Truthful mechanisms with implicit payment computation. In Proceedings of the 11th ACM Conference on Electronic Commerce, EC \u201910, pages 43-52, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-822-3. doi: 10.1145/ 1807342.1807349. URL http://doi.acm.org/10.1145/1807342.1807349.  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS \u201913, pages 207-216, Washington, DC, USA, 2013. IEEE Computer Society. ISBN 978-0- 7695-5135-7. doi: 10.1109/FOCS.2013.30. URL http://dx.doi.org/10.1109/FOCS. 2013.30.  Santiago R. Balseiro, Omar Besbes, and Gabriel Y. Weintraub. Repeated auctions with budgets in ad exchanges: Approximations and design. Manage. Sci., 61(4):864-884, April 2015. ISSN 0025-1909. doi: 10.1287/mnsc.2014.2022. URL http://dx.doi.org/10.1287/mnsc. 2014.2022. LEARNING IN AUCTIONS  G\u00b4abor Bart\u00b4ok, Dean P. Foster, D\u00b4avid P\u00b4al, Alexander Rakhlin, and Csaba Szepesv\u00b4ari. Partial monitoring\u2014classification, regret bounds, and algorithms. Math. Oper. Res., 39(4):967-997, 2014. ISSN 0364-765X. doi: 10.1287/moor.2014.0663. URL http://dx.doi.org/10. 1287/moor.2014.0663.  Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu. Online learning in online auctions. In Pro- ceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (Baltimore, MD, 2003), pages 202-204, 2003.  Avrim Blum, Yishay Mansour, and Jamie Morgenstern. Learning valuation distributions from par- tial observation. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 798-804, 2015.  S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-  armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.  S\u00b4ebastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi- In Shai Shalev-Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th armed bandits. Conference on Learning Theory, Princeton, NJ, June 12-14, 2013, volume 30 of JMLR W&CP, pages 122-134, 2013.  Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge Uni- ISBN 978-0-521-84108-5; 0-521-84108-9. doi: 10.1017/  versity Press, Cambridge, 2006. CBO9780511546921. URL http://dx.doi.org/10.1017/CBO9780511546921.  Nicol`o Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Regret minimization for reserve prices in second-price auctions. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201913, pages 1190-1204. SIAM, 2013.  Shuchi Chawla, Jason Hartline, and Denis Nekipelov. Mechanism design for data science.  In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 711-712, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2565-3. doi: 10.1145/2600057. 2602881. URL http://doi.acm.org/10.1145/2600057.2602881.  Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceed- ings of the 46th Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 243-252, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591867. URL http://doi.acm.org/10.1145/2591796.2591867.  Jacques Cr\u00b4emer and Richard P. McLean. Full extraction of the surplus in Bayesian and dominant strategy auctions. Econometrica, 56(6):pp. 1247-1257, 1988. ISSN 00129682. URL http: //www.jstor.org/stable/1913096.  Nikhil R. Devanur and Sham M. Kakade. The price of truthfulness for pay-per-click auctions. In Proceedings of the 10th ACM Conference on Electronic Commerce, EC \u201909, pages 99-106, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-458-4. doi: 10.1145/1566374.1566388. URL http://doi.acm.org/10.1145/1566374.1566388. WEED PERCHET RIGOLLET  Peerapong Dhangwatnotai, Tim Roughgarden, and Qiqi Yan. Revenue maximization with a single sample. Games Econom. Behav., 91:318-333, 2015. ISSN 0899-8256. doi: 10.1016/j.geb.2014. 03.011. URL http://dx.doi.org/10.1016/j.geb.2014.03.011.  Dylan Foster, Alexander Rakhlin, and Karthik Sridharan. Adaptive online learning. In NIPS, 2015.  Hu Fu, Jason Hartline, and Darrell Hoy. Prior-independent auctions for risk-averse agents.  In Proceedings of the Fourteenth ACM Conference on Electronic Commerce, EC \u201913, pages 471- 488, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-1962-1. doi: 10.1145/2482540. 2482551. URL http://doi.acm.org/10.1145/2482540.2482551.  Hu Fu, Nima Haghpanah, Jason Hartline, and Robert Kleinberg. Optimal auctions for correlated buyers with sampling. In Proceedings of the Fifteenth ACM Conference on Economics and Com- putation, EC \u201914, pages 23-36, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2565- 3. doi: 10.1145/2600057.2602895. URL http://doi.acm.org/10.1145/2600057. 2602895.  Ramki Gummadi, Peter Key, and Alexandre Proutiere. Repeated auctions with budget constraints: optimal bidding strategies and equilibria. In Proc. of Informs Applied Probability conf., 2011.  Wei Han, Alexander Rakhlin, and Karthik Sridharan. Competing with strategies. In Shai Shalev- Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th Conference on Learning Theory, Princeton, NJ, June 12-14, 2013, volume 30 of JMLR W&CP, pages 966-992, 2013.  Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. SIGecom Exch., 8 (1):5:1-5:3, July 2009. ISSN 1551-9031. doi: 10.1145/1598780.1598785. URL http://doi. acm.org/10.1145/1598780.1598785.  Elad Hazan and Satyen Kale. Extracting certainty from uncertainty:  ation in costs. Mach. Learn., 80(2-3):165-188, 2010. s10994-010-5175-x. URL http://dx.doi.org/10.1007/s10994-010-5175-x.  ISSN 0885-6125.  regret bounded by vari- doi: 10.1007/  Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Efficient bandit algorithms for online multiclass prediction. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, ICML, volume 307 of ACM International Conference Proceeding Series, pages 440-447. ACM, 2008. ISBN 978-1-60558-205-4.  Yash Kanoria and Hamid Nazerzadeh. Dynamic reserve prices for repeated auctions: Learning from bids. In Tie-Yan Liu, Qi Qi, and Yinyu Ye, editors, Web and Internet Economics, volume 8877 of Lecture Notes in Computer Science, pages 232-232. Springer International Publishing, ISBN 978-3-319-13128-3. doi: 10.1007/978-3-319-13129-0 17. URL http://dx. 2014. doi.org/10.1007/978-3-319-13129-0_17.  Brendan Kitts and Benjamin Leblanc. Optimal bidding on keyword auctions. Electronic Mar- doi: 10.1080/1019678042000245119. URL http://www.  kets, 14(3):186-201, 2004. tandfonline.com/doi/abs/10.1080/1019678042000245119.  Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In Proceedings of the 44th Annual IEEE Symposium on Foundations LEARNING IN AUCTIONS  of Computer Science, FOCS \u201903, pages 594-, Washington, DC, USA, 2003. IEEE Computer So- ciety. ISBN 0-7695-2040-5. URL http://dl.acm.org/citation.cfm?id=946243. 946352.  T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied  Mathematics, 6:4-22, 1985.  1999. ISSN 0090-5364.  E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. Ann. Statist., 27(6):1808-1829,  R.Preston McAfee. The design of advertising exchanges. Review of Industrial Organization, 39(3): 169-185, 2011. ISSN 0889-938X. doi: 10.1007/s11151-011-9300-1. URL http://dx.doi. org/10.1007/s11151-011-9300-1.  Mehryar Mohri and Andres Mu\u02dcnoz Medina. Learning theory and algorithms for revenue optimiza- tion in second price auctions with reserve. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 262-270, 2014.  S. Muthukrishnan. Ad exchanges: Research issues.  In Stefano Leonardi, editor, Internet and Network Economics, volume 5929 of Lecture Notes in Computer Science, pages 1-12. Springer Berlin Heidelberg, 2009. ISBN 978-3-642-10840-2. doi: 10.1007/978-3-642-10841-9 1. URL http://dx.doi.org/10.1007/978-3-642-10841-9_1.  Roger B. Myerson. Optimal auction design. Math. Oper. Res., 6(1):58-73, 1981. ISSN 0364-765X.  doi: 10.1287/moor.6.1.58. URL http://dx.doi.org/10.1287/moor.6.1.58.  Michael Ostrovsky and Michael Schwarz. Reserve prices in internet advertising auctions: A field experiment. In Proceedings of the 12th ACM Conference on Electronic Commerce, EC \u201911, pages 59-60, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0261-6. doi: 10.1145/1993574. 1993585. URL http://doi.acm.org/10.1145/1993574.1993585.  Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. Ann.  Statist., 41(2):693-721, 2013.  Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences.  In Shai Shalev-Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th Conference on Learning Theory, Princeton, NJ, June 12-14, 2013, volume 30 of JMLR W&CP, pages 993-1019, 2013.  John Riley and William F Samuelson. Optimal auctions. American Economic Review, 71(3): 381-92, 1981. URL http://EconPapers.repec.org/RePEc:aea:aecrev:v:71: y:1981:i:3:p:381-92.  Tim Roughgarden, Inbal Talgam-Cohen, and Qiqi Yan. Supply-limiting mechanisms.  In Pro- ceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 844-861, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1415-2. doi: 10.1145/2229012.2229077. URL http://doi.acm.org/10.1145/2229012.2229077.  Aleksandrs Slivkins. Contextual bandits with similarity information. J. Mach. Learn. Res., 15 (1):2533-2568, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation. cfm?id=2627435.2670330. WEED PERCHET RIGOLLET  Long Tran-Thanh, Lampros C. Stavrogiannis, Victor Naroditskiy, Valentin Robu, Nicholas R. Jen- nings, and Peter Key. Efficient regret bounds for online bid optimisation in budget-limited spon- sored search auctions. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27, 2014, pages 809-818, 2014.  Alexandre Tsybakov. Statistique appliqu\u00b4ee. Lecture Notes, 2006.  Alexandre B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. ISBN 978-0-387-79051-0. Revised and extended from the 2004  Springer, New York, 2009. French original, Translated by Vladimir Zaiats.  Chih-Chun Wang, S.R. Kulkarni, and H.V. Poor. Bandit problems with arbitrary side observations. In Decision and Control, 2003. Proceedings. 42nd IEEE Conference on, volume 3, pages 2948- 2953 Vol.3, Dec 2003. doi: 10.1109/CDC.2003.1273074.  Robert Wilson. Game-theoretic analyses of trading processes.  In Truman Fassett Bewley, ed- itor, Advances in Economic Theory, pages 33-70. Cambridge University Press, 1987. ISBN 9781139052054. URL http://dx.doi.org/10.1017/CCOL0521340446.002. Cam- bridge Books Online.  Robert B. Wilson. Competitive bidding with disparate information. Management Science, 15(7): ISSN 00251909, 15265501. URL http://www.jstor.org/stable/  446-448, 1969. 2628640. LEARNING IN AUCTIONS  "}, "The Extended Littlestone\u2019s Dimension for Learning with Mistakes and Abstentions": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "The Extended Littlestone\u2019s Dimension for Learning with Mistakes and Abstentions", "abstract": "This paper studies classification with an abstention option in the online setting. In this setting, examples arrive sequentially, the learner is given a hypothesis class \\mathcalH, and the goal of the learner is to either predict a label on each example or abstain, while ensuring that it does not make more than a pre-specified number of mistakes when it does predict a label. Previous work on this problem has left open two main challenges. First, not much is known about the optimality of algorithms, and in particular, about what an optimal algorithmic strategy is for any individual hypothesis class. Second, while the realizable case has been studied, the more realistic non-realizable scenario is not well-understood. In this paper, we address both challenges. First, we provide a novel measure, called the Extended Littlestone\u2019s Dimension, which captures the number of abstentions needed to ensure a certain number of mistakes. Second, we explore the non-realizable case, and provide upper and lower bounds on the number of abstentions required by an algorithm to guarantee a specified number of mistakes.", "pdf_url": "http://proceedings.mlr.press/v49/zhang16a.pdf", "keywords": [], "reference": "Jacob Abernethy, John Langford, and Manfred K. Warmuth. Continuous experts and the binning algorithm. In 19th Annual Conference on Learning Theory, COLT 2006, pages 544-558, 2006.  15   EXTENDED LITTLESTONE\u2019S DIMENSION  exploits finer structures in H, and also in that we allow the hypothesis class H to be infinite. Demaine and Zadimoghaddam (2013) extends Sayedi et al. (2010) by providing efficient algorithms for the class of disjunctions. Finally, another important line of work for online classification with abstentions is conformal prediction (Shafer and Vovk, 2008), which, given a conformity measure R and an error probability measure \u03b4, shows a strategy for constructing confidence sets in an online manner that contain the correct label with probability 1 \u2212 \u03b4. Our framework differs from this line of work in that the conformity measure for us is not specified.  There is a large volume of literature on online classification when no abstentions are allowed. The mistake bound model, initially proposed by (Littlestone, 1987; Angluin, 1987), considers on- line binary classification in the realizable case. Littlestone (1987) also introduces the standard optimal algorithm and optimal mistake bound (aka Littlestone\u2019s dimension Ldim(H)). There has been much literature on developing algorithms for specific hypothesis classes in the mistake bound model; see Shalev-Shwartz and Ben-David (2014); Cesa-Bianchi and Lugosi (2006) for examples. Ben-David et al. (2009) considers online classification (with no abstentions) in the agnostic case; they show that if the hypothesis class H has finite Littlestone\u2019s dimension, then it is possible to design an online prediction algorithm that makes l + \u02dcO( Ldim(H)T + Ldim(H)) mistakes over T rounds, where l is the minimum error of any hypotheses in H. In follow-up work, (Rakhlin et al., 2010, 2012, 2015a,b) have developed a rich theory of online learning, and defined complexity mea- sures such as sequential Rademacher complexity, and sequential covering number that characterize the complexity of online learning. However, this theory does not apply to online learning with abstentions.  p  In the batch setting, the problem of classification with an abstention option has been both em- pirically and theoretically studied since the pioneering work of Chow (1970). It is however un- clear how to directly apply the results in the batch setting to the online setting, because of the adversarial nature of the examples. Herbei and Wegkamp (2006); Bartlett and Wegkamp (2008); Yuan and Wegkamp (2010) consider classification where the decision to abstain is made based on thresholding a real-valued function that belongs to a fixed function class. Freund et al. (2004) pro- vides an algorithm that performs weighted majority style aggregation over a hypothesis class and abstains when the aggregate is close to zero. Kalai et al. (2012); Kanade and Thaler (2014) study a related problem called reliable learning, and gives a predictor that achieves low error at the expense of abstentions. Balsubramani (2016) considers the problem in transductive setting, where the goal is to make aggregated predictions with abstention based on an ensemble of classifiers, where some error upper bounds on individual classifiers are known. Finally, inspired by the active learning al- gorithm of (Cohn et al., 1994), El-Yaniv and Wiener (2010) proposes a abstention principle in the realizable case, which guarantees a zero error. El-Yaniv and Wiener (2011) shows how to extend the idea to nonrealizable case, where the predictor has zero error with respect to the optimal hyoth- esis and Zhang and Chaudhuri (2014) gives an improved predictor when a nonzero amount of error is allowed.  Acknowledgements. We thank NSF under IIS 1162581 for research support. CZ would like to thank Akshay Balsubramani and Haipeng Luo for helpful discussions.  References  Jacob Abernethy, John Langford, and Manfred K. Warmuth. Continuous experts and the binning algorithm. In 19th Annual Conference on Learning Theory, COLT 2006, pages 544-558, 2006. ZHANG CHAUDHURI  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1987. doi: 10.  1007/BF00116828.  arXiv:1602.08151, 2016.  2008.  SSSR, 206(3):521, 1972.  Akshay Balsubramani.  Learning to abstain from binary prediction.  arXiv preprint  P. L. Bartlett and M. H. Wegkamp. Classification with a reject option using a hinge loss. JMLR, 9,  Ya M Barzdin and RV Freivald. Prediction of general recursive functions. Doklady Akademii Nauk  Shai Ben-David, D\u00b4avid P\u00b4al, and Shai Shalev-Shwartz. Agnostic online learning. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009.  Michael Boardman. The egg drop number. Mathematics Magazine, 77(5):368-372, 2004.  Nicolo Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge university  press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David P. Helmbold, and Manfred K. Warmuth. On-line predic-  tion and conversion strategies. Machine Learning, 25(1):71-110, 1996.  C.K. Chow. On optimum error and reject trade-off. IEEE Trans. on Information Theory, 1970.  D. A. Cohn, L. E. Atlas, and R. E. Ladner. Improving generalization with active learning. Machine  Learning, 15(2), 1994.  Erik D. Demaine and Morteza Zadimoghaddam. Learning disjunctions: Near-optimal trade-off In Proceedings of the Twenty-Fourth Annual ACM- between mistakes and \u201di don\u2019t know\u2019s\u201d. SIAM Symposium on Discrete Algorithms, SODA 2013, pages 1369-1379, 2013. doi: 10.1137/1. 9781611973105.99.  R. El-Yaniv and Y. Wiener. On the foundations of noise-free selective classification. JMLR, 11,  2010.  R. El-Yaniv and Y. Wiener. Agnostic selective classification. In NIPS, 2011.  Y. Freund, Y. Mansour, and R. E. Schapire. Generalization bounds for averaged classifiers. The  Ann. of Stat., 32, 2004.  William Gasarch and Stuart Fletcher. The egg game. 2008.  R. Herbei and M. H. Wegkamp. Classfication with reject option. Canadian J. of Stat., 4, 2006.  Adam Tauman Kalai, Varun Kanade, and Yishay Mansour. Reliable agnostic learning. J. Comput.  Syst. Sci., 78(5):1481-1495, 2012. doi: 10.1016/j.jcss.2011.12.026.  Varun Kanade and Justin Thaler. Distribution-independent reliable learning. In Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014, pages 3-24, 2014. EXTENDED LITTLESTONE\u2019S DIMENSION  Lihong Li, Michael L. Littman, Thomas J. Walsh, and Alexander L. Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning, 82(3):399-443, 2011. doi: 10.1007/ s10994-010-5225-4.  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-  rithm. Machine Learning, 2(4):285-318, 1987. doi: 10.1007/BF00116827.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Sys- tems, pages 1984-1992, 2010.  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize : From value to algorithms. In Advances in Neural Information Processing Systems 25, pages 2150-2158, 2012.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability Theory and Related Fields, 161(1-2):111-153, 2015a.  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complex-  ities. The Journal of Machine Learning Research, 16(1):155-186, 2015b.  Amin Sayedi, Morteza Zadimoghaddam, and Avrim Blum. Trading off mistakes and don\u2019t-know predictions. In Advances in Neural Information Processing Systems 23, pages 2092-2100, 2010.  Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning  Research, 9:371-421, 2008.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in  Machine Learning, 4(2):107-194, 2012. doi: 10.1561/2200000018.  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-  rithms. Cambridge University Press, 2014.  Istv\u00b4an Szita and Csaba Szepesv\u00b4ari. Agnostic kwik learning and efficient approximate reinforcement  learning. In COLT, pages 739-772, 2011.  Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971.  M. Yuan and M. H. Wegkamp. Classification methods with reject option based on convex risk  minimization. JMLR, 11, 2010.  C. Zhang and K. Chaudhuri. Beyond disagreement-based agnostic active learning. In NIPS, 2014.  "}, "First-order Methods for Geodesically Convex Optimization": {"volumn": "v49", "url": "http://proceedings.mlr.press/v49/", "header": "First-order Methods for Geodesically Convex Optimization", "abstract": "Geodesic convexity generalizes the notion of (vector space) convexity to nonlinear metric spaces. But unlike convex optimization, geodesically convex (g-convex) optimization is much less developed. In this paper we contribute to the understanding of g-convex optimization by developing iteration complexity analysis for several first-order algorithms on Hadamard manifolds. Specifically, we prove upper bounds for the global complexity of deterministic and stochastic (sub)gradient methods for optimizing smooth and nonsmooth g-convex functions, both with and without strong g-convexity. Our analysis also reveals how the manifold geometry, especially \\emphsectional curvature, impacts convergence rates. To the best of our knowledge, our work is the first to provide global complexity analysis for first-order algorithms for general g-convex optimization.", "pdf_url": "http://proceedings.mlr.press/v49/zhang16b.pdf", "keywords": ["first-order methods", "geodesic convexity", "manifold optimization", "nonpositively curved spaces", "iteration complexity"], "reference": "Princeton University Press, 2009.  Gruyter GmbH & Co KG, 2014.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.  Miroslav Bac\u00b4ak. Convex analysis and optimization in Hadamard spaces, volume 22. Walter de  Louis J Billera, Susan P Holmes, and Karen Vogtmann. Geometry of the space of phylogenetic  trees. Advances in Applied Mathematics, 27(4):733-767, 2001.  Dario A Bini and Bruno Iannazzo. Computing the Karcher mean of symmetric positive definite  matrices. Linear Algebra and its Applications, 438(4):1700-1710, 2013.  Richard L Bishop and Barrett O\u2019Neill. Manifolds of negative curvature. Transactions of the Amer-  ican Mathematical Society, 145:1-49, 1969.  Silvere Bonnabel. Stochastic gradient descent on Riemannian manifolds. Automatic Control, IEEE  Transactions on, 58(9):2217-2229, 2013.  Stephen Boyd, Seung-Jean Kim, Lieven Vandenberghe, and Arash Hassibi. A tutorial on geometric  programming. Optimization and engineering, 8(1):67-127, 2007.  Martin R Bridson and Andr\u00b4e Hae\ufb02iger. Metric spaces of non-positive curvature, volume 319.  Springer, 1999.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society Providence, 2001.  17   FIRST-ORDER METHODS FOR GEODESICALLY CONVEX OPTIMIZATION  relies on a proximal gradient projection interpretation. In nonlinear space, we have not been able to find an analogy to such a projection. Further study is needed to see if similar anal- ysis can be developed, or a different approach is required, or Nesterov\u2019s algorithms have no nonlinear space counterparts.  \u2022 Another interesting direction is variance reduced stochastic gradient methods for geodesically convex functions. For smooth and convex optimization in Euclidean space, these methods have recently drawn great interests and enjoyed remarkable empirical success. We hypoth- esize that similar algorithms can achieve faster convergence over naive incremental gradient methods on Hadamard manifolds.  \u2022 Finally, since in applications it is often favorable to replace exponential mapping with com- putationally cheap retractions, it is important to understand the effect of this approximation on convergence rate. Analyzing this effect is of both theoretical and practical interests.  Acknowledgments  We thank the anonymous reviewers for helpful suggestions. HZ is generously supported by the Leventhal Graduate Student Fellowship. SS acknowledges partial support from NSF IIS-1409802.  References  Princeton University Press, 2009.  Gruyter GmbH & Co KG, 2014.  P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.  Miroslav Bac\u00b4ak. Convex analysis and optimization in Hadamard spaces, volume 22. Walter de  Louis J Billera, Susan P Holmes, and Karen Vogtmann. Geometry of the space of phylogenetic  trees. Advances in Applied Mathematics, 27(4):733-767, 2001.  Dario A Bini and Bruno Iannazzo. Computing the Karcher mean of symmetric positive definite  matrices. Linear Algebra and its Applications, 438(4):1700-1710, 2013.  Richard L Bishop and Barrett O\u2019Neill. Manifolds of negative curvature. Transactions of the Amer-  ican Mathematical Society, 145:1-49, 1969.  Silvere Bonnabel. Stochastic gradient descent on Riemannian manifolds. Automatic Control, IEEE  Transactions on, 58(9):2217-2229, 2013.  Stephen Boyd, Seung-Jean Kim, Lieven Vandenberghe, and Arash Hassibi. A tutorial on geometric  programming. Optimization and engineering, 8(1):67-127, 2007.  Martin R Bridson and Andr\u00b4e Hae\ufb02iger. Metric spaces of non-positive curvature, volume 319.  Springer, 1999.  Dmitri Burago, Yuri Burago, and Sergei Ivanov. A course in metric geometry, volume 33. American  Mathematical Society Providence, 2001. ZHANG SRA  Yu Burago, Mikhail Gromov, and Gregory Perel\u2019man. A.D. Alexandrov spaces with curvature  bounded below. Russian mathematical surveys, 47(2):1, 1992.  Dario Cordero-Erausquin, Robert J McCann, and Michael Schmuckenschl\u00a8ager. A Riemannian inter- polation inequality `a la Borell, Brascamp and Lieb. Inventiones mathematicae, 146(2):219-257, 2001.  Stephen C Cowin and Guoyu Yang. Averaging anisotropic elastic constant data. Journal of Elastic-  ity, 46(2):151-180, 1997.  Alan Edelman, Tom\u00b4as A Arias, and Steven T Smith. The geometry of algorithms with orthogonality  constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303-353, 1998.  Thomas Fletcher and Sarang Joshi. Riemannian geometry for the statistical analysis of diffusion  tensor data. Signal Processing, 87(2):250-262, 2007.  Mikhail Gromov. Manifolds of negative curvature. J. Differential Geom, 13(2):223-230, 1978.  Mehrtash T Harandi, Conrad Sanderson, Richard Hartley, and Brian C Lovell. Sparse coding and dictionary learning for symmetric positive definite matrices: A kernel approach. In ECCV 2012, pages 216-229. Springer, 2012.  Reshad Hosseini and Suvrit Sra. Matrix manifold optimization for Gaussian mixtures. In Advances  in Neural Information Processing Systems (NIPS), 2015.  Mariya Ishteva, P-A Absil, Sabine Van Huffel, and Lieven De Lathauwer. Best low multilinear rank approximation of higher-order tensors, based on the Riemannian trust-region scheme. SIAM Journal on Matrix Analysis and Applications, 32(1):115-135, 2011.  Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.  Bas Lemmens and Roger Nussbaum. Nonlinear Perron-Frobenius Theory, volume 189. Cambridge  University Press, 2012.  Xin-Guo Liu, Xue-Feng Wang, and Wei-Guo Wang. Maximization of matrix trace function of product Stiefel manifolds. SIAM Journal on Matrix Analysis and Applications, 36(4):1489-1506, 2015.  Bamdev Mishra, Gilles Meyer, Francis Bach, and Rodolphe Sepulchre. Low-rank optimization with  trace norm penalty. SIAM Journal on Optimization, 23(4):2124-2149, 2013.  Maher Moakher. Means and averaging in the group of rotations. SIAM journal on matrix analysis  and applications, 24(1):1-16, 2002.  Maher Moakher. A differential geometric approach to the geometric mean of symmetric positive- definite matrices. SIAM Journal on Matrix Analysis and Applications, 26(3):735-747, 2005.  Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor comput-  ing. International Journal of Computer Vision, 66(1):41-66, 2006. FIRST-ORDER METHODS FOR GEODESICALLY CONVEX OPTIMIZATION  Hao Shen, Stefanie Jegelka, and Arthur Gretton. Fast kernel-based independent component analysis.  Signal Processing, IEEE Transactions on, 57(9):3498-3511, 2009.  Suvrit Sra and Reshad Hosseini. Conic Geometric Optimization on the Manifold of Positive Definite  Matrices. SIAM J. Optimization (SIOPT), 25(1):713-739, 2015.  Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere II: Recovery by  Riemannian trust-region method. arXiv:1511.04777, 2015.  Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. Submitted  to SIAM J. Optim, 2009.  Constantin Udriste. Convex functions and optimization methods on Riemannian manifolds, volume  297. Springer Science & Business Media, 1994.  Bart Vandereycken. Low-rank matrix completion by Riemannian optimization. SIAM Journal on  Optimization, 23(2):1214-1236, 2013.  Ami Wiesel. Geodesic convexity and covariance estimation. Signal Processing, IEEE Transactions  on, 60(12):6182-6189, 2012.  Teng Zhang, Ami Wiesel, and Maria S Greco. Multivariate generalized Gaussian distribution: Con- vexity and graphical models. Signal Processing, IEEE Transactions on, 61(16):4141-4148, 2013.  "}}