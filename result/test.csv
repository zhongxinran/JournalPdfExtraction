test2,"Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 2009.  Markus Baldauf and Joao Santos Silva. On the use of robust regression in econometrics.  Economics Letters, 114(1):124–127, 2012.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. The Annals of Statistics, 33(4):1497–1537, 2005.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.  Ricardo J. Bessa, Vladimiro Miranda, and Joao Gama. Entropy and correntropy against minimum square error in oﬄine and online three-day ahead wind power forecasting. IEEE Transactions on Power Systems, 24(4):1657–1666, 2009.  Eva Cantoni and Elvezio Ronchetti. Resistant selection of the smoothing parameter for  smoothing splines. Statistics and Computing, 11(2):141–146, 2001.  Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, and Jos´e C. Principe. Generalized correntropy for robust adaptive ﬁltering. IEEE Transactions on Signal Processing, 64 (13):3376–3387, 2016a.  Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, and Larry Wasserman. Non-  parametric modal regression. The Annals of Statistics, 44(2):489–514, 2016b.  Herman Chernoﬀ. Estimation of the mode. Annals of the Institute of Statistical Mathemat-  ics, 16(1):31–41, 1964.  Grard Collomb, Wolfgang H¨ardle, and Salima Hassani. A note on prediction via estimation of the conditional mode function. Journal of Statistical Planning and Inference, 15(2): 227–236, 1987.  Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603– 619, 2002.  Felipe Cucker and Ding-Xuan Zhou. Learning Theory: An Approximation Theory View-  point. Cambridge University Press, 2007.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estima- tion. In Advances in Neural Information Processing Systems, pages 2555–2563, 2014.  Krisztina Dearborn and Rafael Frongillo. On the indirect elicitability of the mode and modal interval. Annals of the Institute of Statistical Mathematics, pages 1–14, 2018.  Charles R. Doss and Jon A. Wellner. Global rates of convergence of the MLEs of log-concave  and s-concave densities. The Annals of Statistics, 44(3):954–981, 2016.  32   Feng, Fan, and Suykens  References  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 2009.  Markus Baldauf and Joao Santos Silva. On the use of robust regression in econometrics.  Economics Letters, 114(1):124–127, 2012.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. The Annals of Statistics, 33(4):1497–1537, 2005.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.  Ricardo J. Bessa, Vladimiro Miranda, and Joao Gama. Entropy and correntropy against minimum square error in oﬄine and online three-day ahead wind power forecasting. IEEE Transactions on Power Systems, 24(4):1657–1666, 2009.  Eva Cantoni and Elvezio Ronchetti. Resistant selection of the smoothing parameter for  smoothing splines. Statistics and Computing, 11(2):141–146, 2001.  Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, and Jos´e C. Principe. Generalized correntropy for robust adaptive ﬁltering. IEEE Transactions on Signal Processing, 64 (13):3376–3387, 2016a.  Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, and Larry Wasserman. Non-  parametric modal regression. The Annals of Statistics, 44(2):489–514, 2016b.  Herman Chernoﬀ. Estimation of the mode. Annals of the Institute of Statistical Mathemat-  ics, 16(1):31–41, 1964.  Grard Collomb, Wolfgang H¨ardle, and Salima Hassani. A note on prediction via estimation of the conditional mode function. Journal of Statistical Planning and Inference, 15(2): 227–236, 1987.  Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603– 619, 2002.  Felipe Cucker and Ding-Xuan Zhou. Learning Theory: An Approximation Theory View-  point. Cambridge University Press, 2007.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estima- tion. In Advances in Neural Information Processing Systems, pages 2555–2563, 2014.  Krisztina Dearborn and Rafael Frongillo. On the indirect elicitability of the mode and modal interval. Annals of the Institute of Statistical Mathematics, pages 1–14, 2018.  Charles R. Doss and Jon A. Wellner. Global rates of convergence of the MLEs of log-concave  and s-concave densities. The Annals of Statistics, 44(3):954–981, 2016. Learning for Modal Regression  William F. Eddy. Optimum kernel estimators of the mode. The Annals of Statistics, 8(4):  870–882, 1980.  Jochen Einbeck and Gerhard Tutz. Modelling beyond regression functions: an application of multimodal regression to speed–ﬂow data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 55(4):461–475, 2006.  Jun Fan, Ting Hu, Qiang Wu, and Ding-Xuan Zhou. Consistency analysis of an empirical minimum error entropy algorithm. Applied and Computational Harmonic Analysis, 41 (1):164–189, 2016.  Yunlong Feng and Qiang Wu. Learning under (1 + (cid:15))-moment conditions. Submitted, 2019.  Yunlong Feng and Yiming Ying. Learning with correntropy-induced losses for regression with mixture of symmetric stable noise. Applied and Computational Harmonic Analysis, 48(2):795–810, 2019.  Yunlong Feng, Xiaolin Huang, Lei Shi, Yuning Yang, and Johan A.K. Suykens. Learning with the maximum correntropy criterion induced losses for regression. Journal of Machine Learning Research, 16:993–1034, 2015.  Fr´ed´eric Ferraty, Ali Laksaci, and Philippe Vieu. Functional time series prediction via conditional mode estimation. Comptes Rendus Mathematique, 340(5):389–392, 2005.  Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function, with applications in pattern recognition. IEEE Transactions on Information Theory, 21(1):32–40, 1975.  Ali Gannoun, Jerome Saracco, and Keming Yu. On semiparametric mode regression esti- mation. Communications in Statistics - Theory and Methods, 39(7):1141–1157, 2010.  Ran He, Bao-Gang Hu, Wei-Shi Zheng, and Xiang-Wei Kong. Robust principal compo- nent analysis based on maximum correntropy criterion. IEEE Transactions on Image Processing, 20(6):1485–1494, 2011.  Ran He, Tieniu Tan, Liang Wang, and Wei-Shi Zheng. (cid:96)2,1-regularized correntropy for ro- bust feature selection. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pages 2504–2511. IEEE, 2012.  Claudio Heinrich. The mode functional is not elicitable. Biometrika, 101(1):245–251, 2013.  Eva Herrmann and Klaus Ziegler. Rates of consistency for nonparametric estimation of the mode in absence of smoothness assumptions. Statistics & Probability Letters, 68(4): 359–368, 2004.  Ting Hu, Jun Fan, Qiang Wu, and Ding-Xuan Zhou. Learning theory approach to minimum  error entropy criterion. Journal of Machine Learning Research, 14:377–397, 2013.  Gordon C.R. Kemp and Joao Santos Silva. Regression towards the mode. Journal of  Econometrics, 170(1):92–101, 2012. Feng, Fan, and Suykens  Myoung-Jae Lee. Mode regression. Journal of Econometrics, 42(3):337–349, 1989.  Myoung-Jae Lee. Quadratic mode regression. Journal of Econometrics, 57(1):1–19, 1993.  Myoung-Jae Lee and Hyun Ah Kim. Semiparametric econometric estimators for a truncated regression model: a review with an extension. Statistica Neerlandica, 52(2):200–225, 1998.  Weifeng Liu, Puskal P. Pokharel, and Jose C. Principe. Correntropy: properties and appli- cations in non-Gaussian signal processing. IEEE Transactions on Signal Processing, 55 (11):5286–5298, 2007.  Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, and Zhouchen Lin. Correntropy induced l2 graph for robust subspace clustering. In Proceedings of the IEEE International Conference on Computer Vision, pages 1801–1808, 2013.  Fusheng Lv and Jun Fan. Optimal learning with Gaussians and correntropy loss. Analysis  and Applications, 2019. doi: 10.1142/S0219530519410124.  Zhike Lv, Huiming Zhu, and Keming Yu. Robust variable selection for nonlinear models with diverging number of parameters. Statistics & Probability Letters, 91:90–97, 2014.  Eric Matzner-Løfber, Ali Gannoun, and Jan G. De Gooijer. Nonparametric forecasting: a comparison of three kernel-based methods. Communications in Statistics - Theory and Methods, 27(7):1593–1617, 1998.  Elias Ould-Sa¨ıd. A note on ergodic processes prediction via estimation of the conditional  mode function. Scandinavian Journal of Statistics, 24(2):231–239, 1997.  Emanuel Parzen. On estimation of a probability density function and mode. The Annals  of Mathematical Statistics, 33(3):1065–1076, 1962.  Karl F. Petty, Hisham Noeimi, Kumud Sanwal, Dan Rydzewski, Alexander Skabardonis, Pravin Varaiya, and Haitham Al-Deek. The freeway service patrol evaluation project: Database support programs, and accessibility. Transportation Research Part C: Emerging Technologies, 4(2):71–85, 1996.  Jose C. Principe. Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives.  Springer Science & Business Media, 2010.  Alejandro Quintela-Del-Rio and Philippe Vieu. A nonparametric conditional mode estimate.  Journal of Nonparametric Statistics, 8(3):253–266, 1997.  Tim Robertson and Jonathan D. Cryer. An iterative procedure for estimating the mode.  Journal of the American Statistical Association, 69(348):1012–1016, 1974.  Thomas W. Sager and Ronald A. Thisted. Maximum likelihood estimation of isotonic modal  regression. The Annals of Statistics, 10(3):690–707, 1982.  Khardani Salah and Yao Anne Fran¸coise. Nonlinear parametric mode regression. Commu-  nications in Statistics - Theory and Methods, 46(6):3006–3024, 2016. Learning for Modal Regression  Mrityunjay Samanta and Aerambamoorthy Thavaneswaran. Non-parametric estimation of the conditional mode. Communications in Statistics-Theory and Methods, 19(12):4515– 4524, 1990.  Hiroaki Sasaki, Yurina Ono, and Masashi Sugiyama. Modal regression via direct log-density In International Conference on Neural Information Processing,  eerivative estimation. pages 108–116. Springer, 2016.  Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, New York,  2008.  Matt P. Wand and Chris M. Jones. Kernel Smoothing. Chapman & Hall, London, 1994.  Xueqin Wang, Yunlu Jiang, Mian Huang, and Heping Zhang. Robust variable selection with exponential squared loss. Journal of the American Statistical Association, 108(502): 632–643, 2013.  Qiang Wu, Yiming Ying, and Ding-Xuan Zhou. Multi-kernel regularized classiﬁers. Journal  of Complexity, 23(1):108–134, 2007.  Weixin Yao and Longhai Li. A new regression model: modal linear regression. Scandinavian  Journal of Statistics, 41(3):656–671, 2014.  Weixin Yao and Sijia Xiang. Nonparametric and varying coeﬃcient modal regression. arXiv  preprint arXiv:1602.06609, 2016.  Weixin Yao, Bruce G. Lindsay, and Runze Li. Local modal regression. Journal of Nonpara-  metric Statistics, 24(3):647–663, 2012.  Keming Yu and Katerina Aristodemou. Bayesian mode regression.  arXiv preprint  arXiv:1208.0579, 2012.  Keming Yu, Katerina Aristodemou, Frauke Becker, and Joann Lord. Fast mode regression in big data analysis. In Proceedings of the 2014 International Conference on Big Data Science and Computing, page 24. ACM, 2014.  Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex  risk minimization. The Annals of Statistics, 32(1):56–85, 2004.  Haiming Zhou and Xianzheng Huang. Nonparametric modal regression in the presence of  measurement error. Electronic Journal of Statistics, 10(2):3579–3620, 2016.  Haiming Zhou and Xianzheng Huang. Bandwidth selection for nonparametric modal regres- sion. Communications in Statistics-Simulation and Computation, 48(4):968–984, 2019. "
test3,"Press, 2004.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  Stephen Boyd, Seung-Jean Kim, Lieven Vandenberghe, and Arash Hassibi. A tutorial on  geometric programming. Optimization and Engineering, 8(1):67–127, 2007.  Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction, Learning, and Games. Cambridge  University Press, 2006.  Arpita Ghosh, Preston McAfee, Kishore Papineni, and Sergei Vassilvitskii. Bidding for rep- resentative allocations for display advertising. In Proceedings of International Workshop on Internet and Network Economics (WINE), 2009.  Avi Goldfarb and Catherine Tucker. Online display advertising: targeting and obtrusive-  ness. Marketing Science, 30(3):389–404, 2011.  Elad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of International  Conference on Machine Learning (ICML), 2012.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online  convex optimization. Machine Learning, 69:169–192, 2007.  Toshihide Ibaraki and Naoki Katoh. Resource Allocation Problems: Algorithmic Ap-  proaches. MIT Press, 1988.  Rodolphe Jenatton, Jim Huang, and C´edric Archambeau. Adaptive algorithms for online convex optimization with long-term constraints. In Proceedings of International Confer- ence on Machine learning (ICML), 2016.  Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for eﬃciency: online convex optimization with long term constraints. Journal of Machine Learning Research, 13(1):2503–2528, 2012.  Michael J. Neely. Dynamic Power Allocation and Routing for Satellite and Wireless Net- works with Time Varying Channels. PhD thesis, Massachusetts Institute of Technology, 2003.  Michael J. Neely. Stochastic network optimization with application to communication and  queueing systems. Morgan & Claypool Publishers, 2010.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and  Trends in Machine Learning, 4(2):107–194, 2011.  Hao Yu and Michael J. Neely. A simple parallel algorithm with an O(1/t) convergence rate  for general convex programs. SIAM Journal on Optimization, 27(2):759–783, 2017.  Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic  constraints. In Advances in Neural Information Processing Systems (NIPS), 2017.  Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.  In Proceedings of International Conference on Machine Learning (ICML), 2003.  24   Yu and Neely  References  Press, 2004.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  Stephen Boyd, Seung-Jean Kim, Lieven Vandenberghe, and Arash Hassibi. A tutorial on  geometric programming. Optimization and Engineering, 8(1):67–127, 2007.  Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction, Learning, and Games. Cambridge  University Press, 2006.  Arpita Ghosh, Preston McAfee, Kishore Papineni, and Sergei Vassilvitskii. Bidding for rep- resentative allocations for display advertising. In Proceedings of International Workshop on Internet and Network Economics (WINE), 2009.  Avi Goldfarb and Catherine Tucker. Online display advertising: targeting and obtrusive-  ness. Marketing Science, 30(3):389–404, 2011.  Elad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of International  Conference on Machine Learning (ICML), 2012.  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online  convex optimization. Machine Learning, 69:169–192, 2007.  Toshihide Ibaraki and Naoki Katoh. Resource Allocation Problems: Algorithmic Ap-  proaches. MIT Press, 1988.  Rodolphe Jenatton, Jim Huang, and C´edric Archambeau. Adaptive algorithms for online convex optimization with long-term constraints. In Proceedings of International Confer- ence on Machine learning (ICML), 2016.  Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for eﬃciency: online convex optimization with long term constraints. Journal of Machine Learning Research, 13(1):2503–2528, 2012.  Michael J. Neely. Dynamic Power Allocation and Routing for Satellite and Wireless Net- works with Time Varying Channels. PhD thesis, Massachusetts Institute of Technology, 2003.  Michael J. Neely. Stochastic network optimization with application to communication and  queueing systems. Morgan & Claypool Publishers, 2010.  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and  Trends in Machine Learning, 4(2):107–194, 2011.  Hao Yu and Michael J. Neely. A simple parallel algorithm with an O(1/t) convergence rate  for general convex programs. SIAM Journal on Optimization, 27(2):759–783, 2017.  Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic  constraints. In Advances in Neural Information Processing Systems (NIPS), 2017.  Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.  In Proceedings of International Conference on Machine Learning (ICML), 2003. "
test,"B. Antosik and M. Kurzynski. New measures of classiﬁer competence – heuristics and application to the  design of multiple classiﬁer systems. In Computer Recognition Systems 4, pages 197–206. 2011.  P. R. Cavalin, R. Sabourin, and C. Y. Suen. Dynamic selection approaches for multiple classiﬁer systems.  Neural Computing and Applications, 22(3-4):673–688, 2013.  R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, and Tsang Ing Ren. META-DES: A dynamic ensemble  selection framework using meta-learning. Pattern Recognition, 48(5):1925–1935, 2015a.  R. M. O. Cruz, R. Sabourin, and G. D.C. Cavalcanti. Dynamic classiﬁer selection: Recent advances and  perspectives. Information Fusion, 41:195 – 216, 2018.  4   CRUZ, HAFEMANN, SABOURIN AND CAVALCANTI  4.1. Usage  Each implemented method receives as an input a list of classiﬁers. This list can be either homoge- neous (i.e., all base classiﬁers are of the same type) or heterogeneous (base classiﬁers of different types). The library supports any type of base classiﬁers from scikit-learn.  After instantiation, the method ﬁt(X, y) is used to ﬁt the Dynamic Selection method. Predictions for new examples can then be obtained with predict(X) and predict_proba(X). In the example below, we show how to use the library, with a given Training (X_train, y_train), and Testing (X_test, y_test) datasets. The META-DES (Cruz et al., 2015a) technique is used in this example:  from sklearn.ensemble import RandomForestClassifier from deslib.des.meta_des import METADES  # Train a pool of 10 classifiers pool_classifiers = RandomForestClassifier(n_estimators=10) pool_classifiers.fit(X_train, y_train) # Initialize the DS model metades = METADES(pool_classifiers)  # Fit the dynamic selection model metades.fit(X_dsel, y_dsel)  # Predict new examples: metades.predict(X_test)  As of version 0.3, each implemented method comes with a list of default values, not requiring a trained list of classiﬁers as input. In such case, the pool of classiﬁers is trained together with the DS algorithm inside the ﬁt method. More examples of using different aspects of the library can be found on https://deslib.readthedocs.io/en/latest/auto_examples/ index.html.  5. Conclusion and future plans  In this paper, we introduced the DESlib, a Python library with the implementation of the state-of- the-art dynamic classiﬁer and ensemble selection techniques. The project is fully compatible with the scikit-learn API and is part of the scikit-learn-contrib supported projects. Future work on this library includes the implementation of dynamic selection methods in different contexts, such as One-Class-Classiﬁcation (OCC) and regression.  References  B. Antosik and M. Kurzynski. New measures of classiﬁer competence – heuristics and application to the  design of multiple classiﬁer systems. In Computer Recognition Systems 4, pages 197–206. 2011.  P. R. Cavalin, R. Sabourin, and C. Y. Suen. Dynamic selection approaches for multiple classiﬁer systems.  Neural Computing and Applications, 22(3-4):673–688, 2013.  R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, and Tsang Ing Ren. META-DES: A dynamic ensemble  selection framework using meta-learning. Pattern Recognition, 48(5):1925–1935, 2015a.  R. M. O. Cruz, R. Sabourin, and G. D.C. Cavalcanti. Dynamic classiﬁer selection: Recent advances and  perspectives. Information Fusion, 41:195 – 216, 2018. DESLIB: A DYNAMIC ENSEMBLE SELECTION LIBRARY IN PYTHON  Rafael M. O. Cruz, Robert Sabourin, and George D. C. Cavalcanti. META-DES.H: A dynamic ensemble selection technique using meta-learning and a dynamic weighting approach. In International Joint Con- ference on Neural Networks, pages 1–8, 2015b.  Rafael M. O. Cruz, Dayvid V. R. Oliveira, George D. C. Cavalcanti, and Robert Sabourin. FIRE-DES++: Enhanced online pruning of base classiﬁers for dynamic ensemble selection. Pattern Recognition, 85: 149–160, 2019.  L. Didaci et al. A study on the performances of dynamic classiﬁer selection based on local accuracy estima-  tion. Pattern Recognition, 38(11):2188–2191, 2005.  Salvador García, Zhong-Liang Zhang, Abdulrahman Altalhi, Saleh Alshomrani, and Francisco Herrera. Dy- namic ensemble selection for multi-class imbalanced datasets. Information Sciences, 445:22–37, 2018.  G. Giacinto and F. Roli. Dynamic classiﬁer selection based on multiple classiﬁer behaviour. Pattern Recog-  nition, 34:1879–1881, 2001.  A. H. R. Ko, R. Sabourin, and A. S. Britto Jr. From dynamic classiﬁer selection to dynamic ensemble  selection. Pattern Recognition, 41:1735–1748, 2008.  Ludmila I. Kuncheva. A theoretical study on six classiﬁer fusion strategies. IEEE Transactions on Pattern  Analysis and Machine Intelligence, 24(2):281–286, 2002.  D. V.R. Oliveira, G. D.C. Cavalcanti, and R. Sabourin. Online pruning of base classiﬁers for dynamic ensem-  ble selection. Pattern Recognition, 72:44 – 58, 2017.  F. Pedregosa et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:  2825–2830, 2011.  D. Ruta and B. Gabrys. Classiﬁer selection for majority voting. Inf. Fusion, 6(1):63–81, 2005.  M. Sabourin, A. Mitiche, D. Thomas, and G. Nagy. Classiﬁer combination for handprinted digit recognition.  Intl. Conf. on Document Analysis and Recognition, pages 163–166, 1993.  Paul C Smits. Multiple classiﬁer systems for supervised remote sensing image classiﬁcation based on dy-  namic classiﬁer selection. IEEE Trans. on Geoscience and Remote Sensing, 40(4):801–813, 2002.  M. C. P. Souto et al. Empirical comparison of dynamic classiﬁer selection methods based on diversity and In International Joint Conference on Neural Networks, pages 1480–  accuracy for building ensembles. 1487, 2008.  Mariana A Souza, George DC Cavalcanti, Rafael MO Cruz, and Robert Sabourin. Online local pool genera-  tion for dynamic classiﬁer selection. Pattern Recognition, 85:132–148, 2019.  T. Woloszynski and M. Kurzynski. On a new measure of classiﬁer competence applied to the design of multiclassiﬁer systems. In International Conference on Image Analysis and Processing (ICIAP), pages 995–1004, 2009.  T. Woloszynski and M. Kurzynski. A probabilistic model of classiﬁer competence for dynamic ensemble  selection. Pattern Recognition, 44:2656–2668, October 2011.  T. Woloszynski et al. A measure of competence based on random classiﬁcation for dynamic ensemble selec-  tion. Information Fusion, 13(3):207–213, 2012.  David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.  K. Woods, W. P. Kegelmeyer, and K. Bowyer. Combination of multiple classiﬁers using local accuracy  estimates. IEEE Trans. on PAMI, 19:405–410, April 1997. "
