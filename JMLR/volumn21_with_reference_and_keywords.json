{"A Low Complexity Algorithm with O(\u00e2\u0088\u009aT) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints": {"volumn": 21, "url": "http://jmlr.org/papers/v21/16-494.html", "header": "A Low Complexity Algorithm with O(\u00e2\u0088\u009aT) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints", "author": "Hao Yu, Michael J. Neely", "time": "21(1):1\u221224, 2020.", "abstract": "This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint.  The conventional online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve $O(\\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations for general problems and another algorithm to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints.  A recent extension in Jenatton et al. (2016) can achieve $O(T^{\\max\\{\\theta,1-\\theta\\}})$ regret and $O(T^{1-\\theta/2})$ constraint violations where $\\theta\\in (0,1)$. The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an $O(\\sqrt{T})$ regret bound with $O(1)$ constraint violations.", "pdf_url": "http://jmlr.org/papers/volume21/16-494/16-494.pdf"}, "A Statistical Learning Approach to Modal Regression": {"volumn": 21, "url": "http://jmlr.org/papers/v21/17-068.html", "header": "A Statistical Learning Approach to Modal Regression", "author": "Yunlong Feng, Jun Fan, Johan A.K. Suykens", "time": "21(2):1\u221235, 2020.", "abstract": "This paper studies the nonparametric modal regression problem systematically from a statistical learning viewpoint.  Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that the nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its Bayes rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols such as mean regression, median regression, and quantile regression. On the practical side, the implementation issues of modal regression including the computational algorithm and the selection of the tuning parameters are discussed. Numerical validations on modal regression are also conducted to verify our findings.", "pdf_url": "http://jmlr.org/papers/volume21/17-068/17-068.pdf", "keywords": ["Nonparametric modal regression", "empirical risk minimization", "generalization bounds", "kernel density estimation", "statistical learning theory"], "reference": "Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 2009.  Markus Baldauf and Joao Santos Silva. On the use of robust regression in econometrics.  Economics Letters, 114(1):124-127, 2012.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. The Annals of Statistics, 33(4):1497-1537, 2005.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuli\ufb00e. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  Ricardo J. Bessa, Vladimiro Miranda, and Joao Gama. Entropy and correntropy against minimum square error in o\ufb04ine and online three-day ahead wind power forecasting. IEEE Transactions on Power Systems, 24(4):1657-1666, 2009.  Eva Cantoni and Elvezio Ronchetti. Resistant selection of the smoothing parameter for  smoothing splines. Statistics and Computing, 11(2):141-146, 2001.  Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, and Jos\u00b4e C. Principe. Generalized correntropy for robust adaptive filtering. IEEE Transactions on Signal Processing, 64 (13):3376-3387, 2016a.  Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, and Larry Wasserman. Non-  parametric modal regression. The Annals of Statistics, 44(2):489-514, 2016b.  Herman Cherno\ufb00. Estimation of the mode. Annals of the Institute of Statistical Mathemat-  ics, 16(1):31-41, 1964.  Grard Collomb, Wolfgang H\u00a8ardle, and Salima Hassani. A note on prediction via estimation of the conditional mode function. Journal of Statistical Planning and Inference, 15(2): 227-236, 1987.  Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603- 619, 2002.  Felipe Cucker and Ding-Xuan Zhou. Learning Theory: An Approximation Theory View-  point. Cambridge University Press, 2007.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estima- tion. In Advances in Neural Information Processing Systems, pages 2555-2563, 2014.  Krisztina Dearborn and Rafael Frongillo. On the indirect elicitability of the mode and modal interval. Annals of the Institute of Statistical Mathematics, pages 1-14, 2018.  Charles R. Doss and Jon A. Wellner. Global rates of convergence of the MLEs of log-concave  and s-concave densities. The Annals of Statistics, 44(3):954-981, 2016.  32   Feng, Fan, and Suykens  References  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 2009.  Markus Baldauf and Joao Santos Silva. On the use of robust regression in econometrics.  Economics Letters, 114(1):124-127, 2012.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexi-  ties. The Annals of Statistics, 33(4):1497-1537, 2005.  Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuli\ufb00e. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138-156, 2006.  Ricardo J. Bessa, Vladimiro Miranda, and Joao Gama. Entropy and correntropy against minimum square error in o\ufb04ine and online three-day ahead wind power forecasting. IEEE Transactions on Power Systems, 24(4):1657-1666, 2009.  Eva Cantoni and Elvezio Ronchetti. Resistant selection of the smoothing parameter for  smoothing splines. Statistics and Computing, 11(2):141-146, 2001.  Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, and Jos\u00b4e C. Principe. Generalized correntropy for robust adaptive filtering. IEEE Transactions on Signal Processing, 64 (13):3376-3387, 2016a.  Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, and Larry Wasserman. Non-  parametric modal regression. The Annals of Statistics, 44(2):489-514, 2016b.  Herman Cherno\ufb00. Estimation of the mode. Annals of the Institute of Statistical Mathemat-  ics, 16(1):31-41, 1964.  Grard Collomb, Wolfgang H\u00a8ardle, and Salima Hassani. A note on prediction via estimation of the conditional mode function. Journal of Statistical Planning and Inference, 15(2): 227-236, 1987.  Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603- 619, 2002.  Felipe Cucker and Ding-Xuan Zhou. Learning Theory: An Approximation Theory View-  point. Cambridge University Press, 2007.  Sanjoy Dasgupta and Samory Kpotufe. Optimal rates for k-NN density and mode estima- tion. In Advances in Neural Information Processing Systems, pages 2555-2563, 2014.  Krisztina Dearborn and Rafael Frongillo. On the indirect elicitability of the mode and modal interval. Annals of the Institute of Statistical Mathematics, pages 1-14, 2018.  Charles R. Doss and Jon A. Wellner. Global rates of convergence of the MLEs of log-concave  and s-concave densities. The Annals of Statistics, 44(3):954-981, 2016. Learning for Modal Regression  William F. Eddy. Optimum kernel estimators of the mode. The Annals of Statistics, 8(4):  870-882, 1980.  Jochen Einbeck and Gerhard Tutz. Modelling beyond regression functions: an application of multimodal regression to speed-\ufb02ow data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 55(4):461-475, 2006.  Jun Fan, Ting Hu, Qiang Wu, and Ding-Xuan Zhou. Consistency analysis of an empirical minimum error entropy algorithm. Applied and Computational Harmonic Analysis, 41 (1):164-189, 2016.  Yunlong Feng and Qiang Wu. Learning under (1 + (cid:15))-moment conditions. Submitted, 2019.  Yunlong Feng and Yiming Ying. Learning with correntropy-induced losses for regression with mixture of symmetric stable noise. Applied and Computational Harmonic Analysis, 48(2):795-810, 2019.  Yunlong Feng, Xiaolin Huang, Lei Shi, Yuning Yang, and Johan A.K. Suykens. Learning with the maximum correntropy criterion induced losses for regression. Journal of Machine Learning Research, 16:993-1034, 2015.  Fr\u00b4ed\u00b4eric Ferraty, Ali Laksaci, and Philippe Vieu. Functional time series prediction via conditional mode estimation. Comptes Rendus Mathematique, 340(5):389-392, 2005.  Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function, with applications in pattern recognition. IEEE Transactions on Information Theory, 21(1):32-40, 1975.  Ali Gannoun, Jerome Saracco, and Keming Yu. On semiparametric mode regression esti- mation. Communications in Statistics - Theory and Methods, 39(7):1141-1157, 2010.  Ran He, Bao-Gang Hu, Wei-Shi Zheng, and Xiang-Wei Kong. Robust principal compo- nent analysis based on maximum correntropy criterion. IEEE Transactions on Image Processing, 20(6):1485-1494, 2011.  Ran He, Tieniu Tan, Liang Wang, and Wei-Shi Zheng. (cid:96)2,1-regularized correntropy for ro- bust feature selection. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pages 2504-2511. IEEE, 2012.  Claudio Heinrich. The mode functional is not elicitable. Biometrika, 101(1):245-251, 2013.  Eva Herrmann and Klaus Ziegler. Rates of consistency for nonparametric estimation of the mode in absence of smoothness assumptions. Statistics & Probability Letters, 68(4): 359-368, 2004.  Ting Hu, Jun Fan, Qiang Wu, and Ding-Xuan Zhou. Learning theory approach to minimum  error entropy criterion. Journal of Machine Learning Research, 14:377-397, 2013.  Gordon C.R. Kemp and Joao Santos Silva. Regression towards the mode. Journal of  Econometrics, 170(1):92-101, 2012. Feng, Fan, and Suykens  Myoung-Jae Lee. Mode regression. Journal of Econometrics, 42(3):337-349, 1989.  Myoung-Jae Lee. Quadratic mode regression. Journal of Econometrics, 57(1):1-19, 1993.  Myoung-Jae Lee and Hyun Ah Kim. Semiparametric econometric estimators for a truncated regression model: a review with an extension. Statistica Neerlandica, 52(2):200-225, 1998.  Weifeng Liu, Puskal P. Pokharel, and Jose C. Principe. Correntropy: properties and appli- cations in non-Gaussian signal processing. IEEE Transactions on Signal Processing, 55 (11):5286-5298, 2007.  Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, and Zhouchen Lin. Correntropy induced l2 graph for robust subspace clustering. In Proceedings of the IEEE International Conference on Computer Vision, pages 1801-1808, 2013.  Fusheng Lv and Jun Fan. Optimal learning with Gaussians and correntropy loss. Analysis  and Applications, 2019. doi: 10.1142/S0219530519410124.  Zhike Lv, Huiming Zhu, and Keming Yu. Robust variable selection for nonlinear models with diverging number of parameters. Statistics & Probability Letters, 91:90-97, 2014.  Eric Matzner-L\u00f8fber, Ali Gannoun, and Jan G. De Gooijer. Nonparametric forecasting: a comparison of three kernel-based methods. Communications in Statistics - Theory and Methods, 27(7):1593-1617, 1998.  Elias Ould-Sa\u00a8\u0131d. A note on ergodic processes prediction via estimation of the conditional  mode function. Scandinavian Journal of Statistics, 24(2):231-239, 1997.  Emanuel Parzen. On estimation of a probability density function and mode. The Annals  of Mathematical Statistics, 33(3):1065-1076, 1962.  Karl F. Petty, Hisham Noeimi, Kumud Sanwal, Dan Rydzewski, Alexander Skabardonis, Pravin Varaiya, and Haitham Al-Deek. The freeway service patrol evaluation project: Database support programs, and accessibility. Transportation Research Part C: Emerging Technologies, 4(2):71-85, 1996.  Jose C. Principe. Information Theoretic Learning: Renyi\u2019s Entropy and Kernel Perspectives.  Springer Science & Business Media, 2010.  Alejandro Quintela-Del-Rio and Philippe Vieu. A nonparametric conditional mode estimate.  Journal of Nonparametric Statistics, 8(3):253-266, 1997.  Tim Robertson and Jonathan D. Cryer. An iterative procedure for estimating the mode.  Journal of the American Statistical Association, 69(348):1012-1016, 1974.  Thomas W. Sager and Ronald A. Thisted. Maximum likelihood estimation of isotonic modal  regression. The Annals of Statistics, 10(3):690-707, 1982.  Khardani Salah and Yao Anne Fran\u00b8coise. Nonlinear parametric mode regression. Commu-  nications in Statistics - Theory and Methods, 46(6):3006-3024, 2016. Learning for Modal Regression  Mrityunjay Samanta and Aerambamoorthy Thavaneswaran. Non-parametric estimation of the conditional mode. Communications in Statistics-Theory and Methods, 19(12):4515- 4524, 1990.  Hiroaki Sasaki, Yurina Ono, and Masashi Sugiyama. Modal regression via direct log-density In International Conference on Neural Information Processing,  eerivative estimation. pages 108-116. Springer, 2016.  Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, New York,  2008.  Matt P. Wand and Chris M. Jones. Kernel Smoothing. Chapman & Hall, London, 1994.  Xueqin Wang, Yunlu Jiang, Mian Huang, and Heping Zhang. Robust variable selection with exponential squared loss. Journal of the American Statistical Association, 108(502): 632-643, 2013.  Qiang Wu, Yiming Ying, and Ding-Xuan Zhou. Multi-kernel regularized classifiers. Journal  of Complexity, 23(1):108-134, 2007.  Weixin Yao and Longhai Li. A new regression model: modal linear regression. Scandinavian  Journal of Statistics, 41(3):656-671, 2014.  Weixin Yao and Sijia Xiang. Nonparametric and varying coe\ufb03cient modal regression. arXiv  preprint arXiv:1602.06609, 2016.  Weixin Yao, Bruce G. Lindsay, and Runze Li. Local modal regression. Journal of Nonpara-  metric Statistics, 24(3):647-663, 2012.  Keming Yu and Katerina Aristodemou. Bayesian mode regression.  arXiv preprint  arXiv:1208.0579, 2012.  Keming Yu, Katerina Aristodemou, Frauke Becker, and Joann Lord. Fast mode regression in big data analysis. In Proceedings of the 2014 International Conference on Big Data Science and Computing, page 24. ACM, 2014.  Tong Zhang. Statistical behavior and consistency of classification methods based on convex  risk minimization. The Annals of Statistics, 32(1):56-85, 2004.  Haiming Zhou and Xianzheng Huang. Nonparametric modal regression in the presence of  measurement error. Electronic Journal of Statistics, 10(2):3579-3620, 2016.  Haiming Zhou and Xianzheng Huang. Bandwidth selection for nonparametric modal regres- sion. Communications in Statistics-Simulation and Computation, 48(4):968-984, 2019. "}, "A Model of Fake Data in Data-driven Analysis": {"volumn": 21, "url": "http://jmlr.org/papers/v21/17-360.html", "header": "A Model of Fake Data in Data-driven Analysis", "author": "Xiaofan Li, Andrew B. Whinston", "time": "21(3):1\u221226, 2020.", "abstract": "Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and pictures, can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time game-theoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources and understanding that the sender can be strategic are very helpful to the data receiver. In addition, based on our model, we propose a methodology of detecting fake data that is complementary to the previous studies on this topic, which suggested various approaches on analyzing the data piece by piece. We show that after analyzing each piece of data, understanding a source by looking at the its whole history of pushing data can be helpful.", "pdf_url": "http://jmlr.org/papers/volume21/17-360/17-360.pdf", "keywords": ["data-driven analysis", "fake data", "game theory", "point process"], "reference": "Hunt Allcott and Matthew Gentzkow. Social media and fake news in the 2016 election.  Technical report, National Bureau of Economic Research, 2017.  Axel Anderson and Lones Smith. Dynamic deception. The American Economic Review,  103(7):2811-2847, 2013.  Athos Antonelli, Ra\ufb00aele Cappelli, Dario Maio, and Davide Maltoni. Fake finger detection by skin distortion analysis. IEEE Transactions on Information Forensics and Security, 1 (3):360-373, 2006.  Richard Bellman and Robert E Kalaba. Dynamic Programming and Modern Control The-  ory, volume 81. Citeseer, 1965.  Daryl J Daley and David Vere-Jones. An Introduction to the Theory of Point Processes: Volume II: General Theory and Structure. Springer Science & Business Media, 2007.  Aditya Ganjam, Faisal Siddiqui, Jibin Zhan, Xi Liu, Ion Stoica, Junchen Jiang, Vyas Sekar, In and Hui Zhang. C3: USENIX Symposium on Networked Systems Design and Implementation, pages 131-144, 2015.  Internet-scale control plane for video quality optimization.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.  Junchen Jiang, Shijie Sun, Vyas Sekar, and Hui Zhang. Pytheas: Enabling data-driven qual- ity of experience optimization using group-based exploration-exploitation. In USENIX Symposium on Networked Systems Design and Implementation, 2017.  Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. The American Economic  Review, 101(6):2590-2615, 2011.  Jooyeon Kim, Behzad Tabibian, Alice Oh, Bernhard Sch\u00a8olkopf, and Manuel Gomez- Rodriguez. Leveraging the crowd to detect and reduce the spread of fake news and In ACM International Conference on Web Search and Data Mining, misinformation. pages 324-332. ACM, 2018.  24   Li and Whinston  Proof of Theorem 5 Proof First, it\u2019s obvious that when p0\u2212p(cid:48) a p0\u2212pa than the \u039ba(qe) when p0\u2212p(cid:48) (1 + L). a p0\u2212pa Then, when p0\u2212p(cid:48) < c a p0\u2212pa  < c  c+\u039b0  c+\u039b0  (1 + L), \u039ba(qe) =  \u2265 c  c+\u039b0  (1 + L), \u039ba(qe) equals to c and is larger  p0\u2212p(cid:48) a a\u2212pa\u2212Lpa  Lp0+p(cid:48)  \u039b0 =(L+1) p0\u2212pa p0\u2212p(cid:48) a  \u22121  \u039b0, which  is increasing with respect to p0\u2212p(cid:48) a p0\u2212pa  . Therefore, \u039ba(qe) is an increasing function with respect to p0\u2212p(cid:48) a p0\u2212pa  .  References  Hunt Allcott and Matthew Gentzkow. Social media and fake news in the 2016 election.  Technical report, National Bureau of Economic Research, 2017.  Axel Anderson and Lones Smith. Dynamic deception. The American Economic Review,  103(7):2811-2847, 2013.  Athos Antonelli, Ra\ufb00aele Cappelli, Dario Maio, and Davide Maltoni. Fake finger detection by skin distortion analysis. IEEE Transactions on Information Forensics and Security, 1 (3):360-373, 2006.  Richard Bellman and Robert E Kalaba. Dynamic Programming and Modern Control The-  ory, volume 81. Citeseer, 1965.  Daryl J Daley and David Vere-Jones. An Introduction to the Theory of Point Processes: Volume II: General Theory and Structure. Springer Science & Business Media, 2007.  Aditya Ganjam, Faisal Siddiqui, Jibin Zhan, Xi Liu, Ion Stoica, Junchen Jiang, Vyas Sekar, In and Hui Zhang. C3: USENIX Symposium on Networked Systems Design and Implementation, pages 131-144, 2015.  Internet-scale control plane for video quality optimization.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.  Junchen Jiang, Shijie Sun, Vyas Sekar, and Hui Zhang. Pytheas: Enabling data-driven qual- ity of experience optimization using group-based exploration-exploitation. In USENIX Symposium on Networked Systems Design and Implementation, 2017.  Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. The American Economic  Review, 101(6):2590-2615, 2011.  Jooyeon Kim, Behzad Tabibian, Alice Oh, Bernhard Sch\u00a8olkopf, and Manuel Gomez- Rodriguez. Leveraging the crowd to detect and reduce the spread of fake news and In ACM International Conference on Web Search and Data Mining, misinformation. pages 324-332. ACM, 2018. A Model of Fake Data in Data-driven Analysis  Michael Luca and Georgios Zervas. Fake it till you make it: Reputation, competition, and  yelp review fraud. Management Science, 62(12):3412-3427, 2016.  Justin Malbon. Taking fake online consumer reviews seriously. Journal of Consumer Policy,  36(2):139-157, 2013.  Eric Maskin and Jean Tirole. Markov perfect equilibrium: I. observable actions. Journal  of Economic Theory, 100(2):191-219, 2001.  Dina Mayzlin, Yaniv Dover, and Judith Chevalier. Promotional reviews: An empirical investigation of online review manipulation. The American Economic Review, 104(8): 2421-2455, 2014.  Sendhil Mullainathan and Andrei Shleifer. Media bias. Technical report, National Bureau  of Economic Research, 2002.  Shereen Oraby, Lena Reed, Ryan Compton, Ellen Rilo\ufb00, Marilyn Walker, and Steve Whit- taker. And that\u2019s a fact: Distinguishing factual and emotional argumentation in online dialogue. arXiv preprint arXiv:1709.05295, 2017.  Niels Ott, Ramon Ziai, Michael Hahn, and Detmar Meurers. Comet: Integrating di\ufb00erent levels of linguistic modeling for meaning assessment. In Joint Conference on Lexical and Computational Semantics, Volume 2: International Workshop on Semantic Evaluation, volume 2, pages 608-616, 2013.  Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell. Fake news or truth? using In Workshop on Computational  satirical cues to detect potentially misleading news. Approaches to Deception Detection, pages 7-17, 2016.  Natali Ruchansky, Sungyong Seo, and Yan Liu. Csi: A hybrid deep model for fake news detection. In ACM on Conference on Information and Knowledge Management, pages 797-806. ACM, 2017.  Zhan Shi, Gene Moo Lee, and Andrew B Whinston. Toward a better measure of business  proximity: Topic modeling for industry intelligence. MIS quarterly, 40(4), 2016.  Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. Fakenewsnet: A data repository with news content, social context and dynamic information for studying fake news on social media. arXiv preprint arXiv:1809.01286, 2018.  Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical gen-  erative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.  Scott E Umbaugh. Computer Vision and Image Processing: A Practical Approach Using  CViptools with Cdrom. Prentice Hall PTR, 1997.  Lizhen Xu, Jason A Duan, and Andrew Whinston. Path to purchase: A mutually exciting point process model for online advertising and conversion. Management Science, 60(6): 1392-1412, 2014. Li and Whinston  Hu Zhang, Zhuohua Fan, Jiaheng Zheng, and Quanming Liu. An improving deception detection method in computer-mediated communication. Journal of Networks, 7(11): 1811, 2012.  Xinyan Zhang, Jiangchuan Liu, Bo Li, and Y-SP Yum. Coolstreaming/donet: A data-driven overlay network for peer-to-peer live media streaming. In Annual Joint Conference of the IEEE Computer and Communications Societies, volume 3, pages 2102-2111. IEEE, 2005. "}, "Universal Latent Space Model Fitting for Large Networks with Edge Covariates": {"volumn": 21, "url": "http://jmlr.org/papers/v21/17-470.html", "header": "Universal Latent Space Model Fitting for Large Networks with Edge Covariates", "author": "Zhuang Ma, Zongming Ma, Hongsong Yuan", "time": "21(4):1\u221267, 2020.", "abstract": "Latent space models are effective tools for statistical modeling and visualization of network data. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing the likelihood function for an existing class of inner-product models, and we establish their statistical rates of convergence for these models. In addition, the theory informs us that both methods work simultaneously for a wide range of different latent space models that allow latent positions to affect edge formation in flexible ways, such as distance models. Furthermore, the effectiveness of the methods is demonstrated on a number of real world network data sets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning.", "pdf_url": "http://jmlr.org/papers/volume21/17-470/17-470.pdf", "keywords": ["projected gradient descent"], "reference": "Emmanuel Abbe. Community detection and stochastic block models: recent developments.  The Journal of Machine Learning Research, 18(1):6446-6531, 2017.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block In 2015 IEEE 56th  models: Fundamental limits and e\ufb03cient algorithms for recovery. Annual Symposium on Foundations of Computer Science, pages 670-688. IEEE, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic  block model. IEEE Transactions on Information Theory, 62(1):471-487, 2015.  Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided they blog. In Proceedings of the 3rd International Workshop on Link Discovery, pages 36-43. ACM, 2005.  Deepak Agarwal and Bee-Chung Chen. Regression-based latent factor models.  In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 19-28. ACM, 2009.  Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation In Advances in Neural Information  of a graphon: Theory and consistent estimation. Processing Systems, pages 692-700, 2013.  Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9(Sep):1981-2014, 2008.  Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham M Kakade. A tensor approach to learning mixed membership community models. The Journal of Machine Learning Research, 15(1):2239-2312, 2014.  Dena Asta and Cosma Rohilla Shalizi. Geometric network comparison. In Proceedings of  the 31st Conference on Uncertainty in Artificial Intelligence, pages 102-110, 2015.  61   Universal Latent Space Model Fitting  C.2. The lawyer data example  We now revisit the lawyer data example in Section 6.2. We label attributes practice, gender, o\ufb03ce, and school as node covariate 1 to 4. Then we set edge covariate X p(cid:96)q ji \u201c 1 if i \u2030 j and the ith and the jth lawyers shared the same (cid:96)th node covariate, and X p(cid:96)q ji \u201c 0 otherwise. Applying Algorithm 4 with the aforementioned turning parameter selection scheme suggests that the first three covariates, i.e., indicators for attributes practice, gender, and o\ufb03ce, should be included in the model. The resulting number of misclustered nodes is 9, a 25% improvement compared to the previously reported 12 misclustered nodes when not using any covariate. This error rate is slightly worse than including a single handpicked covariate, indicator for practice, but it enjoys the merit of not having to manually choose the covariates or tune the parameters. Finally, we also note that the error rate does not change if one use latent vectors derived from all non-zero eigenvalues of the fitted pG.  ij \u201c X p(cid:96)q  ij \u201c X p(cid:96)q  References  Emmanuel Abbe. Community detection and stochastic block models: recent developments.  The Journal of Machine Learning Research, 18(1):6446-6531, 2017.  Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block In 2015 IEEE 56th  models: Fundamental limits and e\ufb03cient algorithms for recovery. Annual Symposium on Foundations of Computer Science, pages 670-688. IEEE, 2015.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic  block model. IEEE Transactions on Information Theory, 62(1):471-487, 2015.  Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided they blog. In Proceedings of the 3rd International Workshop on Link Discovery, pages 36-43. ACM, 2005.  Deepak Agarwal and Bee-Chung Chen. Regression-based latent factor models.  In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 19-28. ACM, 2009.  Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation In Advances in Neural Information  of a graphon: Theory and consistent estimation. Processing Systems, pages 692-700, 2013.  Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9(Sep):1981-2014, 2008.  Animashree Anandkumar, Rong Ge, Daniel Hsu, and Sham M Kakade. A tensor approach to learning mixed membership community models. The Journal of Machine Learning Research, 15(1):2239-2312, 2014.  Dena Asta and Cosma Rohilla Shalizi. Geometric network comparison. In Proceedings of  the 31st Conference on Uncertainty in Artificial Intelligence, pages 102-110, 2015. Ma, Ma and Yuan  Peter J Bickel and Aiyou Chen. A nonparametric view of network models and newman- girvan and other modularities. Proceedings of the National Academy of Sciences, 106(50): 21068-21073, 2009.  Eug`ene Bogomolny, Oriol Bohigas, and Charles Schmit. Spectral properties of distance  matrices. Journal of Physics A: Mathematical and General, 36(12):3595, 2003.  Stephen Boyd and Neal Parikh. Proximal algorithms. Foundations and Trends in  optimization, 1(3):123-231, 2013.  S\u00b4ebastien Bubeck. Theory of convex optimization for machine learning. arXiv preprint  arXiv:1405.4980, 2014.  Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank  semidefinite programming. Mathematical Programming, 103(3):427-444, 2005.  T Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. The Annals of Statistics, 43(3):1027-1059, 2015.  Emmanuel Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.  Communications of the ACM, 55(6):111-119, 2012.  Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.  Emmanuel J Cand`es, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase  retrieval via matrix completion. SIAM review, 57(2):225-251, 2015a.  Emmanuel J Cand`es, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger \ufb02ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985- 2007, 2015b.  Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals  of Statistics, 43(1):177-214, 2015.  Kehui Chen and Jing Lei. Network cross-validation for determining the number of communities in network data. Journal of the American Statistical Association, 113(521): 241-251, 2018.  Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.  Yudong Chen, Xiaodong Li, and Jiaming Xu. Convexified modularity maximization for degree-corrected stochastic block models. The Annals of Statistics, 46(4):1573-1602, 2018.  Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices.  Random Matrices: Theory and Applications, 2(04):1350010, 2013. Universal Latent Space Model Fitting  Peter Chin, Anup Rao, and Van Vu. Stochastic block model and community detection in In Conference on  sparse graphs: A spectral algorithm with optimal rate of recovery. Learning Theory, pages 391-423, 2015.  Fan Chung and Linyuan Lu. Connected components in random graphs with given expected  degree sequences. Annals of combinatorics, 6(2):125-145, 2002.  Mark A Davenport, Yaniv Plan, Ewout Van Den Berg, and Mary Wootters. 1-bit matrix  completion. Information and Inference: A Journal of the IMA, 3(3):189-223, 2014.  Richard L Dykstra. An algorithm for restricted least squares regression. Journal of the  American Statistical Association, 78(384):837-842, 1983.  Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics,  38(1):1-50, 2010.  Yingjie Fei and Yudong Chen. Exponential error rates of sdp for block models: Beyond IEEE Transactions on Information Theory, 65(1):551-571,  grothendieck\u2019s inequality. 2018.  Chao Gao and Zongming Ma. Minimax rates in network analysis: Graphon estimation, community detection and hypothesis testing. arXiv preprint arXiv:1811.06055, 2018.  Chao Gao, Yu Lu, and Harrison H Zhou. Rate-optimal graphon estimation. The Annals of  Statistics, 43(6):2624-2652, 2015.  Chao Gao, Yu Lu, Zongming Ma, and Harrison H Zhou. Optimal estimation and completion of matrices with biclustering structures. Journal of Machine Learning Research, 17(1): 5602-5630, 2016.  Chao Gao, Zongming Ma, Anderson Y Zhang, and Harrison H Zhou. Achieving optimal misclassification proportion in stochastic block models. Journal of Machine Learning Research, 18(1):1980-2024, 2017.  Chao Gao, Zongming Ma, Anderson Y Zhang, and Harrison H Zhou. Community detection  in degree-corrected block models. The Annals of Statistics, 46(5):2153-2185, 2018.  Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum.  In Advances in Neural Information Processing Systems, pages 2973-2981, 2016.  Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233-1242, 2017.  Anna Goldenberg, Alice X Zheng, Stephen E Fienberg, and Edoardo M Airoldi. A survey of statistical network models. Foundations and Trends in Machine Learning, 2(2):129-233, 2010.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via grothendieck\u2019s inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016. Ma, Ma and Yuan  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidefinite programming. IEEE Transactions on Information Theory, 62(5):2788-2797, 2016a.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semidefinite programming: Extensions. IEEE Transactions on Information Theory, 62 (10):5918-5937, 2016b.  Mark S Handcock, Adrian E Raftery, and Jeremy M Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301-354, 2007.  Peter D Ho\ufb00. Random e\ufb00ects models for network data. In Dynamic Social Network Modeling  and Analysis: Workshop Summary and Papers. Citeseer, 2003.  Peter D Ho\ufb00. Bilinear mixed-e\ufb00ects models for dyadic data. Journal of the american  Statistical association, 100(469):286-295, 2005.  Peter D Ho\ufb00. Modeling homophily and stochastic equivalence in symmetric relational data.  In Advances in neural information processing systems, pages 657-664, 2008.  Peter D Ho\ufb00, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460):1090-1098, 2002.  Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt.  Stochastic  blockmodels: First steps. Social Networks, 5(2):109-137, 1983.  Sihan Huang and Yang Feng. Pairwise covariates-adjusted block model for community  detection. arXiv preprint arXiv:1807.03469, 2018.  Jiashun Jin. Fast community detection by score. The Annals of Statistics, 43(1):57-89,  2015.  Jiashun Jin, Zheng Tracy Ke, and Shengming Luo. Estimating network memberships by  simplex vertex hunting. arXiv preprint arXiv:1708.07852, 2017.  Jiashun Jin, Zheng Tracy Ke, and Shengming Luo. detection. arXiv preprint arXiv:1811.05927, 2018.  Score+ for network community  Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in  networks. Physical Review E, 83(1):016107, 2011.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from  a few entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010a.  Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from  noisy entries. Journal of Machine Learning Research, 11(Jul):2057-2078, 2010b.  Olga Klopp, Alexandre B Tsybakov, and Nicolas Verzelen. Oracle inequalities for network models and sparse graphon estimation. The Annals of Statistics, 45(1):316-354, 2015. Universal Latent Space Model Fitting  Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov.  Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, pages 2302-2329, 2011.  Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Mari\u00b4an Bogun\u00b4a. Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.  Pavel N Krivitsky and Mark S Handcock. Fitting latent cluster models for networks with  latentnet. Journal of Statistical Software, 24(i05), 2008.  Pavel N Krivitsky, Mark S Handcock, Adrian E Raftery, and Peter D Ho\ufb00. Representing degree distributions, clustering, and homophily in social networks with latent cluster random e\ufb00ects models. Social Networks, 31(3):204-213, 2009.  Emmanuel Lazega. The collegial phenomenon: The social mechanisms of cooperation among  peers in a corporate law partnership. Oxford University Press on Demand, 2001.  Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block  models. The Annals of Statistics, 43(1):215-237, 2015.  Xiaodong Li, Yudong Chen, and Jiaming Xu. Convex relaxation methods for community  detection. arXiv preprint arXiv:1810.00315, 2018.  Zongming Ma. Sparse principal component analysis and iterative thresholding. The Annals  of Statistics, 41(2):772-801, 2013.  Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Information  Automating the construction of internet portals with machine learning. Retrieval, 3(2):127-163, 2000.  M M\u00b4ezard, Giorgio Parisi, and A Zee. Spectra of Euclidean random matrices. Nuclear  Physics B, 559(3):689-701, 1999.  Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of  Machine Learning Research, 7(Dec):2651-2667, 2006.  Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted  partition model. Probability Theory and Related Fields, 162(3-4):431-461, 2015.  Elchanan Mossel, Joe Neeman, Allan Sly, et al. Belief propagation, robust reconstruction and optimal recovery of block models. The Annals of Applied Probability, 26(4):2211- 2256, 2016.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold  conjecture. Combinatorica, 38(3):665-708, 2018.  Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science  & Business Media, 2004. Ma, Ma and Yuan  Art B Owen and Patrick O Perry. Bi-cross-validation of the svd and the nonnegative matrix  factorization. The annals of applied statistics, pages 564-594, 2009.  Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional  stochastic blockmodel. The Annals of Statistics, 39(4):1878-1915, 2011.  Isaac J Schoenberg. On certain metric spaces arising from Euclidean spaces by a change of metric and their imbedding in Hilbert space. Annals of Mathematics, pages 787-793, 1937.  Isaac J Schoenberg. Metric spaces and positive definite functions. Transactions of the  American Mathematical Society, 44(3):522-536, 1938.  Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.  IEEE Transactions on Information Theory, 62(11):6535-6579, 2016.  Daniel L Sussman, Minh Tang, and Carey E Priebe. Consistent latent position estimation and vertex classification for random dot product graphs. IEEE transactions on pattern analysis and machine intelligence, 36(1):48-57, 2014.  Minh Tang, Daniel L Sussman, and Carey E Priebe. Universally consistent vertex classification for latent positions graphs. The Annals of Statistics, 41(3):1406-1430, 2013.  Amanda L Traud, Eric D Kelsic, Peter J Mucha, and Mason A Porter. Comparing community structure to characteristics in online collegiate social networks. SIAM review, 53(3):526-543, 2011.  Amanda L Traud, Peter J Mucha, and Mason A Porter. Social structure of facebook networks. Physica A: Statistical Mechanics and its Applications, 391(16):4165-4180, 2012.  Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear matrix equations via procrustes \ufb02ow. In International Conference on Machine Learning, pages 964-973, 2016.  Patrick J Wolfe and Sofia C Olhede. Nonparametric graphon estimation. arXiv preprint  arXiv:1309.5936, 2013.  Yun-Jhong Wu, Elizaveta Levina, and Ji Zhu. Generalized linear models with low rank  e\ufb00ects for network data. arXiv preprint arXiv:1705.06772, 2017.  Stephen J Young and Edward R Scheinerman. Random dot product graph models for social networks. In International Workshop on Algorithms and Models for the Web-Graph, pages 138-149. Springer, 2007.  Anderson Y Zhang and Harrison H Zhou. Minimax rates of community detection in  stochastic block models. The Annals of Statistics, 44(5):2252-2280, 2016.  Yuan Zhang, Elizaveta Levina, and Ji Zhu. Detecting overlapping communities in networks  using spectral methods. arXiv preprint arXiv:1412.3432, 2014. Universal Latent Space Model Fitting  Yuan Zhang, Elizaveta Levina, and Ji Zhu. Community detection in networks with node  features. Electronic Journal of Statistics, 10(2):3153-3178, 2015.  Qinqing Zheng and John La\ufb00erty.  Convergence analysis  for  completion using burer-monteiro factorization and gradient descent. arXiv:1605.07051, 2016.  rectangular matrix arXiv preprint "}, "Lower Bounds for Parallel and Randomized Convex Optimization": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-771.html", "header": "Lower Bounds for Parallel and Randomized Convex Optimization", "author": "Jelena Diakonikolas, Crist\u00c3\u00b3bal Guzm\u00c3\u00a1n", "time": "21(5):1\u221231, 2020.", "abstract": "We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation and in the high-dimensional regime. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds  with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Another conceptual contribution of our work is in providing a general and streamlined framework for proving lower bounds in the setting of parallel convex optimization. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean ($\\ell_2$) and $\\ell_\\infty$ spaces.", "pdf_url": "http://jmlr.org/papers/volume21/19-771/19-771.pdf", "keywords": ["rithms", "non-Euclidean optimization"], "reference": "E. Balkanski and Y. Singer.  Parallelization does not accelerate convex optimiza- tion: Adaptivity lower bounds for non-smooth convex minimization. arXiv preprint arXiv:1808.03880, 2018.  29   Lower Bounds for Parallel and Randomized Convex Optimization  \u2022 For any f \u2208 F \u03ba (cid:96)T \u221e  (\u00b5), function \u02dcf := f \u25e6 G belongs to F \u03ba (cid:96)d p  function class F \u03ba (cid:96)T \u221e  (\u00b5) can be obtained from F \u03ba (cid:96)d p  (\u00b5). In other words, the whole  (\u00b5) through the linear embedding G.  \u2022 We claim that any local oracle for the class { \u02dcf : f \u2208 F \u03ba (\u00b5)} can be obtained from a (cid:96)T \u221e (\u00b5) (for a proof of this claim, see Guzm\u00b4an and Nemirovski  local oracle for the class F \u03ba (cid:96)d p 2015, Appendix C).  \u2022 From (9), the set Y = GBd  p is such that  1 \u221aT  \u221e \u2286 1 BT  2 BT  2 \u2286 Y \u2286 BT 2 .  From these facts, we can conclude that the oracle complexity over X with function class F \u03ba (1) is at least the one obtained in the embedded space Y with the respective embedded (cid:96)d p function class F \u03ba (cid:96)T \u221e  (1), thus  Compl\u03b3  HP(F \u03ba (cid:96)d p  (1), X , K, \u03b5) \u2265 Compl\u03b3  (1), Y, K, \u03b5)  \u2265 Compl\u03b3  (1), BT  \u221e(0, 1/[2  T ]), K, \u03b5)  \u221a  HP(F \u03ba (cid:96)T \u221e HP(F \u03ba (cid:96)T \u221e  \u221a  Denote \u03b5(cid:48) = 2\u03b5 it is su\ufb03cient to require, as long as T \u2264 \u03bdd, that:  T . By Theorem 14 applied to p = \u221e, together with Remark 4, we get that  (cid:26) 1  ln(T )  (cid:26) 1  (cid:16)  (cid:16)  1 23+4\u03ba\u03b5(cid:48) 1  M = min  = min  ln(T )  24(1+\u03ba)\u03b5  T  (cid:17)1/\u03ba  ,  T ln2(T ) 29 ln(\u03bddK/\u03b3)  (cid:0)23+2\u03ba\u03b5(cid:48)(cid:1)2/\u03ba  (cid:27)  (cid:17)1/\u03ba  ,  \u221a  T ln2(T ) 29 ln(\u03bddK/\u03b3)  \u221a  (cid:0)22(2+\u03ba)\u03b5  T (cid:1)2/\u03ba  (cid:27) .  In the last expression, the left term in the minimum is lower whenever:  T ln2 T \u2265 (29 ln(dK/\u03b3))  2\u03ba 3+2\u03ba  (cid:16) 1 2  (cid:17) 8(2+3\u03ba)  3+2\u03ba (cid:16) 1 \u03b5  (cid:17) 6  3+2\u03ba ,  and it su\ufb03ces to choose:  Under this choice, as long as d \u2265 T /\u03bd, the oracle complexity is lower bounded by:  (cid:24)  T =  2(ln(\u03bddK/\u03b3))  2\u03ba 3+2\u03ba  (cid:16) 1 \u03b5  (cid:17) 6  3+2\u03ba  (cid:25)  .  M =  c\u03ba ln(1/\u03b5) + \u03ba ln ln(dK/\u03b3)  (cid:16) 1 \u03b5  (cid:17) 2  3+2\u03ba ,  where c\u03ba is an absolute constant that only depends on \u03ba, as claimed.  References  E. Balkanski and Y. Singer.  Parallelization does not accelerate convex optimiza- tion: Adaptivity lower bounds for non-smooth convex minimization. arXiv preprint arXiv:1808.03880, 2018. Diakonikolas and Guzm\u00b4an  K. Ball, E. Carlen, and E. Lieb. Sharp uniform convexity and smoothness inequalities for  trace norms. Inventiones mathematicae, 115(1):463-482, 1994.  C. Boyer, P. Weiss, and J. Bigot. An algorithm for variable density sampling with block- constrained acquisition. SIAM Journal on Imaging Sciences, 7(2):1080-1107, May 2014.  G. Braun, C. Guzman, and S. Pokutta. Lower bounds on the oracle complexity of nonsmooth IEEE Trans. Information Theory, 63(7):  convex optimization via information theory. 4709-4724, 2017.  S. Bubeck, Q. Jiang, Y. Lee, Y. Li, and A. Sidford. Complexity of highly parallel non-smooth  convex optimization. arXiv preprint arXiv:1906.10655, 2019.  Y. Carmon, J. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points  I. arXiv preprint arXiv:1710.11606, 2017.  A. d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Op-  timization, 19(3):1171-1183, 2008.  A. d\u2019Aspremont, C. Guzm\u00b4an, and M. Jaggi. Optimal a\ufb03ne-invariant smooth minimization  algorithms. SIAM Journal on Optimization, 28(3):2384-2405, 2018.  J. Duchi, P. Bartlett, and M. Wainwright. Randomized smoothing for stochastic optimiza-  tion. SIAM Journal on Optimization, 22(2):674-701, 2012.  J. Duchi, F. Ruan, and C. Yun. Minimax bounds on stochastic batched convex optimization.  In Proc. COLT\u201918, 2018.  C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth. Preserving statis-  tical validity in adaptive data analysis. In Proc. ACM STOC\u201915, 2015.  V. Feldman, C. Guzman, and S. Vempala. Statistical query algorithms for mean vector estimation and stochastic convex optimization. In Proc. ACM-SIAM SODA\u201917, 2017.  M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics  Quarterly, 3:95-110, 1956.  C. Guzm\u00b4an. Open problem: The oracle complexity of smooth convex optimization in  nonstandard settings. In Proc. COLT\u201915, 2015.  C. Guzm\u00b4an and A. Nemirovski. On lower complexity bounds for large-scale smooth convex  optimization. Journal of Complexity, 31(1):1 - 14, 2015.  U. Haagerup. The best constants in the Khintchine inequality. Studia Mathematica, 70:  231-283, 1981.  J. Kelner, Y. Lee, L. Orecchia, and A. Sidford. An almost-linear-time algorithm for approx- imate max \ufb02ow in undirected graphs, and its multicommodity generalizations. In Proc. ACM-SIAM SODA\u201914, 2014. Lower Bounds for Parallel and Randomized Convex Optimization  Y. Lee, S. Rao, and N. Srivastava. A new approach to computing maximum \ufb02ows using  electrical \ufb02ows. In Proc. ACM STOC\u201913, 2013.  A. Nemirovski. On parallel complexity of nonsmooth convex optimization. Journal of  Complexity, 10(4):451-463, 1994.  A. Nemirovskii and Y. Nesterov. Optimal methods of smooth convex optimization (in  Russian). Zh. Vychisl. Mat. i Mat. Fiz., 25(3):356-369, 1985.  A. Nemirovsky and D. Yudin. Problem complexity and method e\ufb03ciency in optimization.  Wiley-Interscience, 1983. ISBN 0 471 10345 4.  Y. Nesterov and A. Nemirovski. On First-Order Algorithms for (cid:96)1/Nuclear Norm Mini-  mization. Acta Numerica, 22, 4 2013.  G. Pisier. The Volume of Convex Bodies and Banach Space Geometry. Cambridge University  Press, 1 edition, 1989.  Proc. ACM STOC\u201917, 2017.  J. Sherman. Area-convexity, l\u221e regularization, and undirected multicommodity \ufb02ow. In  A. Smith, A. Thakurta, and J. Upadhyay. Is interaction necessary for distributed private  learning? In Proc. IEEE SP\u201917, 2017.  N. Srebro and K. Sridharan. On convex optimization, fat shattering and learning. Unpub-  lished, 2012. URL http://ttic.uchicago.edu/~karthik/optfat.pdf.  T. Steinke and J. Ullman. Interactive fingerprinting codes and the hardness of preventing  false discovery. In Proc. COLT\u201915, 2015.  M. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.  B. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives.  In Proc. NIPS\u201916, 2016.  B. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Proc. NeurIPS\u201918, 2018. "}, "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-085.html", "header": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms", "author": "Anna Little, Mauro Maggioni, James M. Murphy", "time": "21(6):1\u221266, 2020.", "abstract": "We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers.  By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets,  and prove guarantees on the labeling accuracy of the proposed algorithm.  Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric.  We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/18-085/18-085.pdf", "keywords": ["unsupervised learning", "spectral clustering", "manifold learning", "fast al gorithms", "shortest path distance"], "reference": "E. Abbe. Community detection and stochastic block models: Recent developments.  Journal of Machine Learning Research, 18(177):1-86, 2018.  F. Alimoglu and E. Alpaydin. Methods of combining multiple classifiers based on di\ufb00erent representations for pen-based handwritten digit recognition. In TAINN. Citeseer, 1996.  N. Alon and B. Schieber. Optimal preprocessing for answering on-line product queries. Tel-Aviv University. The Moise and Frida Eskenasy Institute of Computer Sciences, 1987.  M. Appel and R. Russo. The maximum vertex degree of a graph on uniform points  in [0, 1]d. Advances in Applied Probability, 29(3):567-581, 1997a.  M. Appel and R. Russo. The minimum vertex degree of a graph on uniform points  in [0, 1]d. Advances in Applied Probability, 29(3):582-594, 1997b.  M. Appel and R. Russo. The connectivity of a graph on uniform points on [0, 1]d.  Statistics & Probability Letters, 60(4):351-357, 2002.  E. Arias-Castro. Clustering based on pairwise distances when the data is of mixed dimensions. IEEE Transactions on Information Theory, 57(3):1692-1706, 2011.  E. Arias-Castro, G. Chen, and G. Lerman. Spectral clustering based on local linear  approximations. Electronic Journal of Statistics, 5:1537-1587, 2011.  E. Arias-Castro, G. Lerman, and T. Zhang. Spectral clustering based on local PCA.  Journal of Machine Learning Research, 18(9):1-57, 2017.  D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In  SODA, pages 1027-1035. SIAM, 2007.  A. Azran and Z. Ghahramani. A new approach to data driven clustering. In ICML,  pages 57-64. ACM, 2006a.  A. Azran and Z. Ghahramani. Spectral methods for automatic multiscale data clus-  tering. In CVPR, volume 1, pages 190-197. IEEE, 2006b.  S. Balakrishnan, M. Xu, A. Krishnamurthy, and A. Singh. Noise thresholds for  spectral clustering. In NIPS, pages 954-962, 2011.  S. Balakrishnan, S. Narayanan, A. Rinaldo, A. Singh, and L. Wasserman. Cluster  trees on manifolds. In NIPS, pages 2679-2687, 2013.  M. Banerjee, M. Capozzoli, L. McSweeney, and D. Sinha. Beyond kappa: A review of interrater agreement measures. Canadian journal of statistics, 27(1):3-23, 1999.  60   Little, Maggioni, Murphy  References  E. Abbe. Community detection and stochastic block models: Recent developments.  Journal of Machine Learning Research, 18(177):1-86, 2018.  F. Alimoglu and E. Alpaydin. Methods of combining multiple classifiers based on di\ufb00erent representations for pen-based handwritten digit recognition. In TAINN. Citeseer, 1996.  N. Alon and B. Schieber. Optimal preprocessing for answering on-line product queries. Tel-Aviv University. The Moise and Frida Eskenasy Institute of Computer Sciences, 1987.  M. Appel and R. Russo. The maximum vertex degree of a graph on uniform points  in [0, 1]d. Advances in Applied Probability, 29(3):567-581, 1997a.  M. Appel and R. Russo. The minimum vertex degree of a graph on uniform points  in [0, 1]d. Advances in Applied Probability, 29(3):582-594, 1997b.  M. Appel and R. Russo. The connectivity of a graph on uniform points on [0, 1]d.  Statistics & Probability Letters, 60(4):351-357, 2002.  E. Arias-Castro. Clustering based on pairwise distances when the data is of mixed dimensions. IEEE Transactions on Information Theory, 57(3):1692-1706, 2011.  E. Arias-Castro, G. Chen, and G. Lerman. Spectral clustering based on local linear  approximations. Electronic Journal of Statistics, 5:1537-1587, 2011.  E. Arias-Castro, G. Lerman, and T. Zhang. Spectral clustering based on local PCA.  Journal of Machine Learning Research, 18(9):1-57, 2017.  D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In  SODA, pages 1027-1035. SIAM, 2007.  A. Azran and Z. Ghahramani. A new approach to data driven clustering. In ICML,  pages 57-64. ACM, 2006a.  A. Azran and Z. Ghahramani. Spectral methods for automatic multiscale data clus-  tering. In CVPR, volume 1, pages 190-197. IEEE, 2006b.  S. Balakrishnan, M. Xu, A. Krishnamurthy, and A. Singh. Noise thresholds for  spectral clustering. In NIPS, pages 954-962, 2011.  S. Balakrishnan, S. Narayanan, A. Rinaldo, A. Singh, and L. Wasserman. Cluster  trees on manifolds. In NIPS, pages 2679-2687, 2013.  M. Banerjee, M. Capozzoli, L. McSweeney, and D. Sinha. Beyond kappa: A review of interrater agreement measures. Canadian journal of statistics, 27(1):3-23, 1999. Path-Based Spectral Clustering  R.E. Bellman. Adaptive control processes: a guided tour. Princeton University Press,  2015.  Business Media, 2010.  J.J. Benedetto and W. Czaja. Integration and modern analysis. Springer Science &  J.L. Bentley. Multidimensional binary search trees used for associative searching.  Communications of the ACM, 18(9):509-517, 1975.  A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In  ICML, pages 97-104. ACM, 2006.  R.B. Bhatt, G. Sharma, A. Dhall, and S. Chaudhury. E\ufb03cient skin region segmen- tation using low complexity fuzzy decision tree model. In INDICON, pages 1-4. IEEE, 2009.  R.L. Bishop and R.J. Crittenden. Geometry of manifolds, volume 15. Academic press,  2011.  P.M. Camerini. The min-max spanning tree problem and some extensions. Informa-  tion Processing Letters, 7(1):10-14, 1978.  H. Chang and D.-Y. Yeung. Robust path-based spectral clustering. Pattern Recog-  nition, 41(1):191-203, 2008.  pages 343-351, 2010.  K. Chaudhuri and S. Dasgupta. Rates of convergence for the cluster tree. In NIPS,  G. Chen and G. Lerman. Foundations of a multi-way spectral clustering framework for hybrid linear modeling. Foundations of Computational Mathematics, 9(5):517-558, 2009a.  G. Chen and G. Lerman. Spectral curvature clustering (SCC). International Journal  of Computer Vision, 81(3):317-330, 2009b.  F. Chung. Spectral graph theory, volume 92. American Mathematical Society, 1997.  R.R. Coifman and S. Lafon. Di\ufb00usion maps. Applied and computational harmonic  analysis, 21(1):5-30, 2006.  R.R. Coifman, S. Lafon, A.B. Lee, M. Maggioni, B. Nadler, F. Warner, and S.W. Zucker. Geometric di\ufb00usions as a tool for harmonic analysis and structure definition of data: Di\ufb00usion maps. Proceedings of the National Academy of Sciences of the United States of America, 102(21):7426-7431, 2005.  E.D. Demaine, G.M. Landau, and O. Weimann. On Cartesian trees and range mini-  mum queries. In ICALP, pages 341-353. Springer, 2009. Little, Maggioni, Murphy  E.D. Demaine, G.M. Landau, and O. Weimann. On Cartesian trees and range mini-  mum queries. Algorithmica, 68(3):610-625, 2014.  E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory, and ap- plications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (11):2765-2781, 2013.  M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for dis- covering clusters in large spatial databases with noise. In Kdd, volume 96, pages 226-231, 1996.  J. Fan, W. Wang, and Y. Zhong. An (cid:96)\u221e eigenvector perturbation bound and its application to robust covariance estimation. Journal of Machine Learning Research, 18(207):1-42, 2018.  H. Federer. Curvature measures. Transactions of the American Mathematical Society,  93(3):418-491, 1959.  B. Fischer and J.M. Buhmann. Path-based clustering for grouping of smooth curves and texture segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(4):513-518, 2003.  B. Fischer, T. Z\u00a8oller, and J. Buhmann. Path based pairwise data clustering with application to texture segmentation. In Energy minimization methods in computer vision and pattern recognition, pages 235-250. Springer, 2001.  B. Fischer, V. Roth, and J.M. Buhmann. Clustering with the connectivity kernel. In  NIPS, pages 89-96, 2004.  J. Friedman, T. Hastie, and R. Tibshirani. The Elements of Statistical Learning,  volume 1. Springer series in Statistics Springer, Berlin, 2001.  H. Gabow and R.E. Tarjan. Algorithms for two bottleneck optimization problems.  Journal of Algorithms, 9:411-417, 1988.  N. Garcia Trillos and D. Slepcev. Continuum limit of total variation on point clouds.  Archive for Rational Mechanics and Analysis, 220(1):193-241, 2016a.  N. Garcia Trillos and D. Slepcev. A variational approach to the consistency of spectral  clustering. Applied and Computational Harmonic Analysis, 2016b.  N. Garcia Trillos, D. Slepcev, J. Von Brecht T. Laurent, and X. Bresson. Consistency of Cheeger and ratio graph cuts. Journal of Machine Learning Research, 17(181): 1-46, 2016. Path-Based Spectral Clustering  N. Garcia Trillos, M. Gerlach, M. Hein, and D. Slep\u02c7cev. Error estimates for spec- tral convergence of the graph Laplacian on random geometric graphs toward the Laplace-Beltrami operator. Foundations of Computational Mathematics, pages 1- 61, 2019a.  N. Garcia Trillos, F. Ho\ufb00mann, and B. Hosseini. Geometric structure of graph Lapla-  cian embeddings. arXiv preprint arXiv:1901.10651, 2019b.  E.N. Gilbert. Random plane networks. Journal of the Society for Industrial and  Applied Mathematics, 9(4):533-543, 1961.  J.M. Gonz\u00b4alez-Barrios and A.J. Quiroz. A clustering procedure based on the com- parison between the k nearest neighbors graph and the minimal spanning tree. Statistics & Probability Letters, 62(1):23-34, 2003.  L. Gy\u00a8orfi, M. Kohler, A. Krzyzak, and H. Walk. A distribution-free theory of non-  parametric regression. Springer Science & Business Media, 2006.  T. Hagerup and C. R\u00a8ub. A guided tour of Cherno\ufb00 bounds. Information Processing  Letters, 33(6):305-308, 1990.  J.A. Hartigan. Consistency of single linkage for high-density clusters. Journal of the  American Statistical Society, 76(374):388-394, 1981.  T. Hastie, R. Tibshirani, and J. Friedman. Elements of Statistical Learning. Springer,  2009.  T.C. Hu. Letter to the editor: The maximum capacity route problem. Operations  Research, 9(6):898-900, 1961.  G. Hughes. On the mean accuracy of statistical pattern recognizers. IEEE Transac-  tions on Information Theory, 14(1):55-63, 1968.  M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.  uci.edu/ml.  M. Maggioni and J.M. Murphy. Learning by unsupervised nonlinear di\ufb00usion. Journal  of Machine Learning Research, 20(160):1-56, 2019.  D. McKenzie and S. Damelin. Power weighted shortest paths for clustering Euclidean  data. Foundations of Data Science, 1(3):307, 2019.  G.J. McLachlan and K.E. Basford. Mixture models: Inference and applications to  clustering, volume 84. Marcel Dekker, 1988.  M. Meila and J. Shi. Learning segmentation by random walks.  In NIPS, pages  873-879, 2001. Little, Maggioni, Murphy  D.G. Mixon, S. Villar, and R. Ward. Clustering subgaussian mixtures by semidefinite programming. Information and Inference: A Journal of the IMA, 6(4):389-415, 2017.  J. Munkres. Algorithms for the assignment and transportation problems. Journal of  the Society for Industrial and Applied Mathematics, 5(1):32-38, 1957.  J.M. Murphy and M. Maggioni. Di\ufb00usion geometric methods for fusion of remotely sensed data. In Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIV, volume 10644, page 106440I. International Society for Optics and Photonics, 2018.  J.M. Murphy and M. Maggioni. Unsupervised clustering and active learning of hy- perspectral images with nonlinear di\ufb00usion. IEEE Transactions on Geoscience and Remote Sensing, 57(3):1829-1845, 2019.  S.A. Nene, S.K. Nayar, and H. Murase. Columbia object image library (COIL-20).  1996.  A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algo-  rithm. In NIPS, pages 849-856, 2002.  R. Ostrovsky, Y. Rabani, L.J. Schulman, and C. Swamy. The e\ufb00ectiveness of Lloyd- type methods for the k-means problem. In FOCS, pages 165-176. IEEE, 2006.  H.S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering.  Expert Systems with Applications, 36(2):3336-3341, 2009.  L. Parsons, E. Haque, and H. Liu. Subspace clustering for high dimensional data: a  review. ACM SIGKDD Explorations Newsletter, 6(1):90-105, 2004.  M. Penrose. The longest edge of the random minimal spanning tree. Annals of Applied  Probability, 7(2):340-361, 1997.  R. Penrose. A strong law for the longest edge of the minimal spanning tree. Annals  of Probability, 27(1):246-260, 1999.  M. Pollack. Letter to the editor: The maximum capacity through a network. Opera-  tions Research, 8(5):733-736, 1960.  A.P. Punnen. A linear time algorithm for the maximum capacity path problem.  European Journal of Operational Research, 53:402-404, 1991.  A. Rinaldo and L. Wasserman. Generalized density clustering. The Annals of Statis-  tics, 38(5):2678-2722, 2010. Path-Based Spectral Clustering  F.D. Roberts and S.H. Storey. A three-dimensional cluster problem. Biometrika, 55  (1):258-260, 1968.  344(6191):1492-1496, 2014.  A. Rodriguez and A. Laio. Clustering by fast search and find of density peaks. Science,  G. Sanguinetti, J. Laidler, and N.D. Lawrence. Automatic determination of the number of clusters using spectral algorithms. In MLSP, pages 55-60. IEEE, 2005.  G. Schiebinger, M.J. Wainwright, and B. Yu. The geometry of kernelized spectral  clustering. Annals of Statistics, 43(2):819-846, 2015.  J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  R. Sibson. SLINK: an optimally e\ufb03cient algorithm for the single-link cluster method.  The Computer Journal, 16(1):30-34, 1973.  M. Soltanolkotabi and E.J. Cand`es. A geometric analysis of subspace clustering with  outliers. The Annals of Statistics, 40(4):2195-2238, 2012.  M. Soltanolkotabi, E. Elhamifar, and E.J. Cand`es. Robust subspace clustering. An-  nals of Statistics, 42(2):669-699, 2014.  B. Sriperumbudur and I. Steinwart. Consistency and rates for clustering with DB-  SCAN. In AISTATS, pages 1090-1098, 2012.  D. Stau\ufb00er and A. Aharony. Introduction to Percolation Theory. CRC Press, 1994.  H. Steinhaus. Sur la division des corps mat\u00b4eriels en parties. Bull. Acad. Polon. Sci.,  4(12):801-804, 1957.  G.W Stewart. Matrix perturbation theory. Citeseer, 1990.  G. Szeg\u00a8o. Inequalities for certain eigenvalues of a membrane of given area. Journal  of Rational Mechanics and Analysis, 3:343-356, 1954.  L.N. Trefethen and D. Bau. Numerical linear algebra, volume 50. SIAM, 1997.  R. Vidal. Subspace clustering. IEEE Signal Processing Magazine, 28(2):52-68, 2011.  U. Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):  395-416, 2007.  V. Vu. A simple SVD algorithm for finding hidden partitions. Combinatorics, Prob-  ability and Computing, 27(1):124-140, 2018. Little, Maggioni, Murphy  X. Wang, K. Slavakis, and G. Lerman. Riemannian multi-manifold modeling. arXiv  preprint arXiv:1410.0095, 2014.  H.F. Weinberger. An isoperimetric inequality for the N -dimensional free membrane  problem. Journal of Rational Mechanics and Analysis, 5(4):633-636, 1956.  X. Xu, M. Ester, H.-P. Kriegel, and J. Sander. A distribution-based clustering al- In ICDE, pages 324-331. IEEE,  gorithm for mining in large spatial databases. 1998.  L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering.  In NIPS, pages  1601-1608, 2004.  T. Zhang, A. Szlam, Y. Wang, and G. Lerman. Hybrid linear modeling via local best-fit \ufb02ats. International Journal of Computer Vision, 100(3):217-240, 2012. "}, "Target Propagation in Recurrent Neural Networks": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-141.html", "header": "Target Propagation in Recurrent Neural Networks", "author": "Nikolay Manchev, Michael Spratling", "time": "21(7):1\u221233, 2020.", "abstract": "Recurrent Neural Networks have been widely used to process sequence data, but have long been criticized for their biological implausibility and training difficulties related to vanishing and exploding gradients. This paper presents a novel algorithm for training recurrent networks, target propagation through time (TPTT), that outperforms standard backpropagation through time (BPTT) on four out of the five problems used for testing. The proposed algorithm is initially tested and compared to BPTT on four synthetic time lag tasks, and its performance is also measured using the sequential MNIST data set. In addition, as TPTT uses target propagation, it allows for discrete nonlinearities and could potentially mitigate the credit assignment problem in more complex recurrent architectures.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/18-141/18-141.pdf", "keywords": [""], "reference": "abs/1511.06464, 2015.  M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. CoRR,  Y. Bengio. Artificial Neural Networks and Their Application to Sequence Recognition. PhD  thesis, McGill University, Montreal, Quebec, Canada, 1991.  Y. Bengio. How auto-encoders could provide credit assignment in deep networks via target  propagation. CoRR, abs/1407.7906, 2014.  Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is di\ufb03cult. Trans. Neur. Netw., 5(2):157-166, March 1994. ISSN 1045-9227. doi: 10.1109/72.279181. URL http://dx.doi.org/10.1109/72.279181.  Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent ICASSP, IEEE International Conference on Acoustics, Speech and Signal networks. Processing - Proceedings, pages 8624-8628, 2013. ISSN 15206149. doi: 10.1109/ICASSP. 2013.6639349.  Y. Bengio, D. H. Lee, J. Bornschein, and Z. Lin. Towards biologically plausible deep  learning. CoRR, abs/1502.04156, 2015.  28   Manchev and Spratling  Network Accuracy  Parameters  %  54.20 54.75  TPTT TPTThx  \u03b1i = 10\u22127, \u03b1f = 10\u22122, \u03b1g = 10\u22128 \u03b1i = 10\u22127, \u03b1f = 10\u22122, \u03b1g = 10\u22128  Table A4: Accuracy on the MNIST sequence of pixels problem achieved by the TPTT and  TPTThx networks.  The TPTThx was also tested on the MNIST sequence of pixels problem (see Section 3.2). The grid search performed over the same parameter space selected a combination of learning rates identical to the TPTT network used on the same problem. In addition, the accuracy results shown in Table A4 appear too close to call, therefore the performance of the two schemes on this specific problem can be considered nearly identical.  In conclusion, the empirical results suggest that in terms of sequence length the alter- native update rule (i.e. a network that is fully optimised using local targets) outperforms both BPTT and a TPTT network that updates Wxh using the global error directly. Simi- larly to the original TPTT network, the TPTThx converges quicker than BPTT, discovers a common model for the synthetic classification problems, and has comparable accuracy on the MNIST sequence of pixels problem.  References  abs/1511.06464, 2015.  M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. CoRR,  Y. Bengio. Artificial Neural Networks and Their Application to Sequence Recognition. PhD  thesis, McGill University, Montreal, Quebec, Canada, 1991.  Y. Bengio. How auto-encoders could provide credit assignment in deep networks via target  propagation. CoRR, abs/1407.7906, 2014.  Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is di\ufb03cult. Trans. Neur. Netw., 5(2):157-166, March 1994. ISSN 1045-9227. doi: 10.1109/72.279181. URL http://dx.doi.org/10.1109/72.279181.  Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent ICASSP, IEEE International Conference on Acoustics, Speech and Signal networks. Processing - Proceedings, pages 8624-8628, 2013. ISSN 15206149. doi: 10.1109/ICASSP. 2013.6639349.  Y. Bengio, D. H. Lee, J. Bornschein, and Z. Lin. Towards biologically plausible deep  learning. CoRR, abs/1502.04156, 2015. Target Propagation in Recurrent Neural Networks  Y. Bengio, B. Scellier, O. Bilaniuk, J. Sacramento, and W. Senn. Feedforward initial- ization for fast inference of deep generative networks is biologically plausible. CoRR, abs/1606.01651, 2016.  R. Caruana. Generalization vs. net size, 1993. Neural Information Processing Systems,  Tutorial, Denver, CO.  J. Chang, P. Kuo, and Y. Chen. Neuroscience-inspired recurrent network for object recog- nition. In 2017 International Symposium on Intelligent Signal Processing and Communi- cation Systems (ISPACS), pages 729-734, Nov 2017. doi: 10.1109/ISPACS.2017.8266572.  F. Crick. The recent excitement about neural networks. Nature, 337:129-132, 01 1989. doi:  10.1038/337129a0. URL http://dx.doi.org/10.1038/337129a0.  W. M. Czarnecki, G. Swirszcz, M. Jaderberg, S. Osindero, O. Vinyals, and K. Kavukcuoglu. In Precup and Understanding synthetic gradients and decoupled neural Teh (2017), pages 904-912. URL http://proceedings.mlr.press/v70/czarnecki17a. html.  interfaces.  W. De Mulder, S. Bethard, and M. F. Moens. A survey on the application of recurrent neural networks to statistical language modeling. Comput. Speech Lang., 30(1):61-98, March 2015. ISSN 0885-2308. doi: 10.1016/j.csl.2014.09.005. URL http://dx.doi.org/ 10.1016/j.csl.2014.09.005.  J. Drewes, G. Goren, W. Zhu, and J. H. Elder. Recurrent processing in the formation of shape percepts. Journal of Neuroscience, 36(1):185-192, 2016. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.2347-15.2016. URL http://www.jneurosci.org/content/36/1/ 185.  J. L. Elman. Finding structure in time. COGNITIVE SCIENCE, 14(2):179-211, 1990.  A. Graves. Sequence transduction with recurrent neural networks. CoRR, abs/1211.3711,  2012. URL http://arxiv.org/abs/1211.3711.  P. J. Grother. NIST special database 19 handprinted forms and character database. Avail- https://www.nist.gov/sites/default/files/documents/srd/nistsd19.pdf.  able: National Institute of Standards and Technology, Gaithersburg, MD., 1995.  D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick. Review Neuroscience-Inspired ISSN 0896-6273. doi: 10.1016/j.  Artificial Intelligence. Neuron, 95(2):245-258, 2017. neuron.2017.06.011. URL http://dx.doi.org/10.1016/j.neuron.2017.06.011.  G. Heigold, I. Moreno, S. Bengio, and N. Shazeer. End-to-end text-dependent speaker  verification. CoRR, abs/1509.08062, 2015.  M. Hena\ufb00, A. Szlam, and Y. LeCun. Orthogonal rnns and long-memory tasks. CoRR,  abs/1602.06662, 2016.  G. E. Hinton, T. J. Sejnowski, and D. H. Ackley. Boltzmann machines: Constraint satis- faction networks that learn. Technical Report CMU-CS-84-119, Computer Science De- partment, Carnegie Mellon University, Pittsburgh, PA, 1984. Manchev and Spratling  S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735- 1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http: //dx.doi.org/10.1162/neco.1997.9.8.1735.  M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and K. Kavukcuoglu. Decoupled neural In Pre- cup and Teh (2017), pages 1627-1635. URL http://proceedings.mlr.press/v70/ jaderberg17a.html.  interfaces using synthetic gradients.  M. I. Jordan. Serial order: A parallel, distributed processing approach. In Je\ufb00rey L. Elman and David E. Rumelhart, editors, Advances in Connectionist Theory: Speech. Erlbaum, Hillsdale, NJ, 1989.  J.H. Kaas and D. C. Lyon. Chapter 18 visual cortex organization in primates: theo- In Vision: From Neurons to Cognition, volume ries of v3 and adjoining visual areas. 134 of Progress in Brain Research, pages 285 - 295. Elsevier, 2001. doi: https://doi. org/10.1016/S0079-6123(01)34019-0. URL http://www.sciencedirect.com/science/ article/pii/S0079612301340190.  A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descrip- tions. IEEE Trans. Pattern Anal. Mach. Intell., 39(4):664-676, April 2017. ISSN 0162- 8828. doi: 10.1109/TPAMI.2016.2598339. URL https://doi.org/10.1109/TPAMI. 2016.2598339.  D. J. Kravitz, K. S. Saleem, C. I. Baker, L. G. Ungerleider, and M. Mishkin. The ventral visual pathway: an expanded neural framework for the processing of object quality. Trends Cogn. Sci. (Regul. Ed.), 17(1):26-49, Jan 2013.  V. Lamme and P. R. Roelfsema. The distinct modes of vision o\ufb00ered by feedforward ISSN and recurrent processing. Trends in Neurosciences, 23(11):571 - 579, 2000. 0166-2236. doi: https://doi.org/10.1016/S0166-2236(00)01657-X. URL http://www. sciencedirect.com/science/article/pii/S016622360001657X.  S. Lawrence, C. Lee Giles, and Ah. Chung Tsoi. Lessons in neural network training: Overfit- ting may be harder than expected. In Proceedings of the National Conference on Artificial Intelligence, pages 540-545, 01 1997.  Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941, 2015. URL http://arxiv.org/abs/1504. 00941.  Y. LeCun. Learning process in an asymmetric threshold network. In E. Bienenstock, F. Fo- gelman Souli\u00b4e, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233-240, Berlin, Heidelberg, 1986. Springer Berlin Heidelberg.  Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner. Gradient-based learning applied to docu-  ment recognition. In Proceedings of the IEEE, pages 2278-2324, 1998. Target Propagation in Recurrent Neural Networks  D. H. Lee, S. Zhang, A. Fischer, and Y. Bengio. Di\ufb00erence target propagation. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9284:498-515, 2015. ISSN 16113349. doi: 10.1007/ 978-3-319-23528-8 31.  T. P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Com- munications, 7, Nov 2016. URL http://dx.doi.org/10.1038/ncomms13276.  P. Liu, X. Qiu, and X. Huang. Recurrent neural network for text classification with multi- In Proceedings of the Twenty-Fifth International Joint Conference on task learning. Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 2873- 2879, 2016. URL http://www.ijcai.org/Abstract/16/408.  W. Lotter, G. Kreiman, and D. D. Cox. Deep predictive coding networks for video prediction  and unsupervised learning. CoRR, abs/1605.08104, 2016.  H. Luo, J. Fu, and J.R. Glass. Bidirectional backpropagation: Towards biologically plausible  error signal transmission in neural networks. CoRR, abs/1702.07097, 2017.  W. Maas. Networks of spiking neurons: The third generation of neural network models. Trans. Soc. Comput. Simul. Int., 14(4):1659-1671, December 1997. ISSN 0740-6797. URL http://dl.acm.org/citation.cfm?id=281543.281637.  J. H. Maunsell and D. C. van Essen. The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey. J. Neurosci., 3(12):2563-2586, Dec 1983.  T. Mikolov, K. Chen, G. Corrado, and J. Dean. E\ufb03cient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. URL http://arxiv.org/abs/1301.3781.  M. Minsky. Steps toward Artificial Intelligence. Proceedings of the IRE, 49(1):8-30, 1961. ISSN 0096-8390. doi: 10.1109/JRPROC.1961.287775. URL http://ieeexplore.ieee. org/document/4066245/.  H. Nakamura, R. Gattass, R. Desimone, and L. G. Ungerleider. The modular organization of projections from areas V1 and V2 to areas V4 and TEO in macaques. J. Neurosci., 13(9):3681-3691, Sep 1993.  Y. Nesterov. A method of solving a convex programming problem with convergence rate  o(1/sqr(k)). Soviet Mathematics Doklady, 27, 1983.  A. G. II Ororbia and A. Mali. Biologically motivated algorithms for propagating local target representations. CoRR, abs/1805.11703, 2018. URL http://arxiv.org/abs/ 1805.11703.  A. G. II Ororbia, A. Mali, D. Kifer, and C. L. Giles. Conducting credit assignment by aligning local representations. CoRR, abs/1803.01834, 2018. URL http://arxiv.org/ abs/1803.01834. Manchev and Spratling  R. Pascanu, T. Mikolov, and Y. Bengio. Understanding the exploding gradient problem.  CoRR, abs/1211.5063, 2012.  P. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 82-90, Bejing, China, 22-24 Jun 2014. PMLR. URL http://proceedings.mlr.press/ v32/pinheiro14.html.  J. B. Pollack.  Language  in Neural  induction by phase  In R. Lippmann, J. E. Moody,  nizers. vances ver, Colorado, USA, November 26-29, mann, 298-language-induction-by-phase-transition-in-dynamical-recognizers.  recog- editors, Ad- [NIPS Conference, Den- 1990], pages 619-626. Morgan Kauf- URL http://papers.nips.cc/paper/  Information Processing Systems 3,  transition in dynamical  and D.S. Touretzky,  ISBN 1-55860-184-8.  1990.  D. Precup and Y.W. Teh, editors. Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, 2017. PMLR. URL http://jmlr.org/ proceedings/papers/v70/.  G. V. Puskorius, L. A. Feldkamp, and L. I. Davis. Dynamic neural network methods applied to on-vehicle idle speed control. Proceedings of the IEEE, 84(10):1407-1420, Oct 1996. ISSN 0018-9219. doi: 10.1109/5.537107.  P. R. Roelfsema and A. Holtmaat. Control of synaptic plasticity in deep cortical networks. Nature Reviews Neuroscience, 19(3):166-180, 2 2018. ISSN 1471-003X. doi: 10.1038/nrn. 2018.6.  P. R. Roelfsema and A. R. Van Ooyen. Attention-gated reinforcement learning of inter- nal representations for classification. Neural Comput., 17(10):2176-2214, October 2005. ISSN 0899-7667. doi: 10.1162/0899766054615699. URL http://dx.doi.org/10.1162/ 0899766054615699.  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Mi- crostructure of Cognition, Vol. 1, pages 318-362. MIT Press, Cambridge, MA, USA, 1986. ISBN 0-262-68053-X. URL http://dl.acm.org/citation.cfm?id=104279.104293.  R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2:207-218, 2014.  R. Soltani and H. Jiang. Higher order recurrent neural networks. CoRR, abs/1605.00064,  2016. URL http://arxiv.org/abs/1605.00064.  C. Spoerer, P. McClure, and N. Kriegeskorte. Recurrent convolutional neural networks: A better model of biological object recognition under occlusion. bioRxiv, 2017. doi: 10. 1101/133330. URL https://www.biorxiv.org/content/early/2017/05/02/133330. Target Propagation in Recurrent Neural Networks  M. Spratling. Cortical region interactions and the functional role of apical dendrites. Be-  havioral and cognitive neuroscience reviews, 1(3):219-228, 2002.  I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks.  In ICML, pages 1017-1024. Omnipress, 2011.  A. Timmaraju and V. Khanna. Sentiment analysis on movie reviews using recursive and  recurrent neural network architectures. 2015.  D. C. Van Essen, W. T. Newsome, J. H. Maunsell, and J. L. Bixby. The projections from striate cortex (V1) to areas V2 and V3 in the macaque monkey: asymmetries, areal boundaries, and patchy connections. J. Comp. Neurol., 244(4):451-480, Feb 1986.  P. J Werbos. Backpropagation through time: what it does and how to do it. Proceedings  of the IEEE, 78(10):1550-1560, 1990.  S. Wiseman, S. Chopra, M. Ranzato, A. Szlam, R. Sun, S. Chintala, and N. Vasilache. Training Language Models Using Target-Propagation. ArXiv e-prints, February 2017.  K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044, 2015.  D. Yogatama, C. Dyer, W. Ling, and P. Blunsom. Generative and Discriminative Text  Classification with Recurrent Neural Networks. ArXiv e-prints, March 2017. "}, "DESlib: A Dynamic ensemble selection library in Python": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-144.html", "header": "DESlib: A Dynamic ensemble selection library in Python", "author": "Rafael M. O. Cruz, Luiz G. Hafemann, Robert Sabourin, George D. C. Cavalcanti", "time": "21(8):1\u22125, 2020.", "abstract": "DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: https://github.com/scikit-learn-contrib/DESlib.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/18-144/18-144.pdf", "keywords": ["Multiple classifier systems", "Ensemble of Classifiers", "Dynamic classifier selection", "Dynamic ensemble selection", "Machine learning", "Python"], "reference": "B. Antosik and M. Kurzynski. New measures of classifier competence - heuristics and application to the  design of multiple classifier systems. In Computer Recognition Systems 4, pages 197-206. 2011.  P. R. Cavalin, R. Sabourin, and C. Y. Suen. Dynamic selection approaches for multiple classifier systems.  Neural Computing and Applications, 22(3-4):673-688, 2013.  R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, and Tsang Ing Ren. META-DES: A dynamic ensemble  selection framework using meta-learning. Pattern Recognition, 48(5):1925-1935, 2015a.  R. M. O. Cruz, R. Sabourin, and G. D.C. Cavalcanti. Dynamic classifier selection: Recent advances and  perspectives. Information Fusion, 41:195 - 216, 2018.  4   CRUZ, HAFEMANN, SABOURIN AND CAVALCANTI  4.1. Usage  Each implemented method receives as an input a list of classifiers. This list can be either homoge- neous (i.e., all base classifiers are of the same type) or heterogeneous (base classifiers of different types). The library supports any type of base classifiers from scikit-learn.  After instantiation, the method fit(X, y) is used to fit the Dynamic Selection method. Predictions for new examples can then be obtained with predict(X) and predict_proba(X). In the example below, we show how to use the library, with a given Training (X_train, y_train), and Testing (X_test, y_test) datasets. The META-DES (Cruz et al., 2015a) technique is used in this example:  from sklearn.ensemble import RandomForestClassifier from deslib.des.meta_des import METADES  # Train a pool of 10 classifiers pool_classifiers = RandomForestClassifier(n_estimators=10) pool_classifiers.fit(X_train, y_train) # Initialize the DS model metades = METADES(pool_classifiers)  # Fit the dynamic selection model metades.fit(X_dsel, y_dsel)  # Predict new examples: metades.predict(X_test)  As of version 0.3, each implemented method comes with a list of default values, not requiring a trained list of classifiers as input. In such case, the pool of classifiers is trained together with the DS algorithm inside the fit method. More examples of using different aspects of the library can be found on https://deslib.readthedocs.io/en/latest/auto_examples/ index.html.  5. Conclusion and future plans  In this paper, we introduced the DESlib, a Python library with the implementation of the state-of- the-art dynamic classifier and ensemble selection techniques. The project is fully compatible with the scikit-learn API and is part of the scikit-learn-contrib supported projects. Future work on this library includes the implementation of dynamic selection methods in different contexts, such as One-Class-Classification (OCC) and regression.  References  B. Antosik and M. Kurzynski. New measures of classifier competence - heuristics and application to the  design of multiple classifier systems. In Computer Recognition Systems 4, pages 197-206. 2011.  P. R. Cavalin, R. Sabourin, and C. Y. Suen. Dynamic selection approaches for multiple classifier systems.  Neural Computing and Applications, 22(3-4):673-688, 2013.  R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, and Tsang Ing Ren. META-DES: A dynamic ensemble  selection framework using meta-learning. Pattern Recognition, 48(5):1925-1935, 2015a.  R. M. O. Cruz, R. Sabourin, and G. D.C. Cavalcanti. Dynamic classifier selection: Recent advances and  perspectives. Information Fusion, 41:195 - 216, 2018. DESLIB: A DYNAMIC ENSEMBLE SELECTION LIBRARY IN PYTHON  Rafael M. O. Cruz, Robert Sabourin, and George D. C. Cavalcanti. META-DES.H: A dynamic ensemble selection technique using meta-learning and a dynamic weighting approach. In International Joint Con- ference on Neural Networks, pages 1-8, 2015b.  Rafael M. O. Cruz, Dayvid V. R. Oliveira, George D. C. Cavalcanti, and Robert Sabourin. FIRE-DES++: Enhanced online pruning of base classifiers for dynamic ensemble selection. Pattern Recognition, 85: 149-160, 2019.  L. Didaci et al. A study on the performances of dynamic classifier selection based on local accuracy estima-  tion. Pattern Recognition, 38(11):2188-2191, 2005.  Salvador Garc\u00eda, Zhong-Liang Zhang, Abdulrahman Altalhi, Saleh Alshomrani, and Francisco Herrera. Dy- namic ensemble selection for multi-class imbalanced datasets. Information Sciences, 445:22-37, 2018.  G. Giacinto and F. Roli. Dynamic classifier selection based on multiple classifier behaviour. Pattern Recog-  nition, 34:1879-1881, 2001.  A. H. R. Ko, R. Sabourin, and A. S. Britto Jr. From dynamic classifier selection to dynamic ensemble  selection. Pattern Recognition, 41:1735-1748, 2008.  Ludmila I. Kuncheva. A theoretical study on six classifier fusion strategies. IEEE Transactions on Pattern  Analysis and Machine Intelligence, 24(2):281-286, 2002.  D. V.R. Oliveira, G. D.C. Cavalcanti, and R. Sabourin. Online pruning of base classifiers for dynamic ensem-  ble selection. Pattern Recognition, 72:44 - 58, 2017.  F. Pedregosa et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:  2825-2830, 2011.  D. Ruta and B. Gabrys. Classifier selection for majority voting. Inf. Fusion, 6(1):63-81, 2005.  M. Sabourin, A. Mitiche, D. Thomas, and G. Nagy. Classifier combination for handprinted digit recognition.  Intl. Conf. on Document Analysis and Recognition, pages 163-166, 1993.  Paul C Smits. Multiple classifier systems for supervised remote sensing image classification based on dy-  namic classifier selection. IEEE Trans. on Geoscience and Remote Sensing, 40(4):801-813, 2002.  M. C. P. Souto et al. Empirical comparison of dynamic classifier selection methods based on diversity and In International Joint Conference on Neural Networks, pages 1480-  accuracy for building ensembles. 1487, 2008.  Mariana A Souza, George DC Cavalcanti, Rafael MO Cruz, and Robert Sabourin. Online local pool genera-  tion for dynamic classifier selection. Pattern Recognition, 85:132-148, 2019.  T. Woloszynski and M. Kurzynski. On a new measure of classifier competence applied to the design of multiclassifier systems. In International Conference on Image Analysis and Processing (ICIAP), pages 995-1004, 2009.  T. Woloszynski and M. Kurzynski. A probabilistic model of classifier competence for dynamic ensemble  selection. Pattern Recognition, 44:2656-2668, October 2011.  T. Woloszynski et al. A measure of competence based on random classification for dynamic ensemble selec-  tion. Information Fusion, 13(3):207-213, 2012.  David H Wolpert. Stacked generalization. Neural networks, 5(2):241-259, 1992.  K. Woods, W. P. Kegelmeyer, and K. Bowyer. Combination of multiple classifiers using local accuracy  estimates. IEEE Trans. on PAMI, 19:405-410, April 1997. "}, "On Mahalanobis Distance in Functional Settings": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-156.html", "header": "On Mahalanobis Distance in Functional Settings", "author": "Jos\u00c3\u00a9 R. Berrendero, Beatriz Bueno-Larraz, Antonio Cuevas", "time": "21(9):1\u221233, 2020.", "abstract": "Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.", "pdf_url": "http://jmlr.org/papers/volume21/18-156/18-156.pdf", "keywords": ["Functional data", "Mahalanobis distance", "reproducing kernel Hilbert spaces", "kernel methods in statistics", "square root operator"], "reference": "1975.  Ana Arribas-Gil and Juan Romo. Shape outlier detection and visualization for functional  data: the outliergram. Biostatistics, 15(4):603-619, 2014.  Robert B. Ash and Melvin F. Gardner. Topics in Stochastic Processes. Academic Press,  Amparo Ba\u00b4\u0131llo, Antonio Cuevas, and Juan Antonio Cuesta-Albertos. Supervised classifica- tion for a family of Gaussian functional models. Scandinavian Journal of Statistics, 38 (3):480-498, 2011.  Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Proba-  bility and Statistics. Kluwer Academic, 2004.  Jos\u00b4e R. Berrendero, Antonio Cuevas, and Jos\u00b4e L. Torrecilla. On the use of reproducing kernel Hilbert spaces in functional classification. Journal of the American Statistical Association, 113(3):1210-1218, 2018.  John B. Conway. A course in functional analysis. Springer, 1990.  Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of  the American Mathematical Society, 39(1):1-49, 2001.  Felipe Cucker and Ding Xuan Zhou. Learning Theory: an Approximation Theory Viewpoint.  Cambridge University Press, 2007.  31   On Mahalanobis Distance in Functional Settings  In principle, to define the Mahalanobis distance one could select other regularization  methods. For instance, the cut-o\ufb00 operator is given by  R\u03b1x =  (cid:104)x, ej(cid:105)ej,  (cid:88)  \u03bbj \u2265\u03b1  1 (cid:112)\u03bbj  which corresponds to q(\u03b1, \u03bbj) = 1, when \u03bbj \u2265 \u03b1, and q(\u03b1, \u03bbj) = 0, when \u03bbj < \u03b1. For this choice, the regularization parameter determines the terms of the series that are retained. Other possibility is the so-called Landweber regularization operator, given by q(m, a, \u03bbj) = 1 \u2212 (1 \u2212 a\u03bbj)m+1, for 0 < a < min{\u03bb\u22121 : \u03bbj > 0}. In this case regularization depends on two parameters, a and m.  j  We believe Tikhonov scheme provides a natural choice with good properties since it depends on just one regularization parameter (unlike Landweber method) and defines a metric in L2[0, 1] (unlike the cut-o\ufb00 approach).  Acknowledgments  This work has been partially supported by Spanish Grant MTM2016-78751-P. The authors are grateful to Daniel Est\u00b4evez and Dmitry Yakubovich for their help with operator the- ory. The constructive comments and suggestions from three anonymous reviewers are also gratefully acknowledged.  References  1975.  Ana Arribas-Gil and Juan Romo. Shape outlier detection and visualization for functional  data: the outliergram. Biostatistics, 15(4):603-619, 2014.  Robert B. Ash and Melvin F. Gardner. Topics in Stochastic Processes. Academic Press,  Amparo Ba\u00b4\u0131llo, Antonio Cuevas, and Juan Antonio Cuesta-Albertos. Supervised classifica- tion for a family of Gaussian functional models. Scandinavian Journal of Statistics, 38 (3):480-498, 2011.  Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Proba-  bility and Statistics. Kluwer Academic, 2004.  Jos\u00b4e R. Berrendero, Antonio Cuevas, and Jos\u00b4e L. Torrecilla. On the use of reproducing kernel Hilbert spaces in functional classification. Journal of the American Statistical Association, 113(3):1210-1218, 2018.  John B. Conway. A course in functional analysis. Springer, 1990.  Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of  the American Mathematical Society, 39(1):1-49, 2001.  Felipe Cucker and Ding Xuan Zhou. Learning Theory: an Approximation Theory Viewpoint.  Cambridge University Press, 2007. Berrendero, Bueno-Larraz and Cuevas  Antonio Cuevas. A partial overview of the theory of statistics with functional data. Journal  of Statistical Planning and Inference, 147:1-23, 2014.  Xiongtao Dai, Hans-Georg M\u00a8uller, and Fang Yao. Optimal Bayes classifiers for functional  data and density ratios. Biometrika, 104(3):545-560, 2017.  Lokenath Debnath and Piotr Mikiusinski. Introduction to Hilbert Spaces with Applications  (3rd Ed.). Elsevier, 2005.  Pedro Galeano, Esdras Joseph, and Rosa E Lillo. The Mahalanobis distance for functional  data with applications to classification. Technometrics, 57:281-291, 2015.  Andrea Ghiglietti and Anna Maria Paganoni. Exact tests for the means of Gaussian stochas-  tic processes. Statistics & Probability Letters, 131:102-107, 2017.  Andrea Ghiglietti, Francesca Ieva, and Anna Maria Paganoni. Statistical inference for stochastic processes: two-sample hypothesis tests. Journal of Statistical Planning and Inference, 180:49-68, 2017.  Israel Gohberg, Seymour Goldberg, and Marinus Kaashoek. Basic Classes of Linear Oper-  ators. Birkh\u00a8auser, 2003.  Tailen Hsing and Randall Eubank. Theoretical Foundations of Functional Data Analysis,  with an Introduction to Linear Operators. John Wiley & Sons, 2015.  Alan J. Izenman. Modern Multivariate Statistical Techniques. Springer, 2008.  Svante Janson. Gaussian Hilbert Spaces. Cambridge University Press, 1997.  Tosio Kato. Perturbation Theory for Linear Operators. Springer, 2013.  Rainer Kress. Linear Integral Equations. Springer, 1989.  Radha G. Laha and Vijay K. Rohatgi. Probability Theory. John Wiley & Sons, 1979.  Milan N. Luki\u00b4c and Jay H. Beder. Stochastic processes with sample paths in reproducing kernel Hilbert spaces. Transactions of the American Mathematical Society, 353:3945- 3969, 2001.  Prasanta C. Mahalanobis. On the generalized distance in statistics. Proceedings of the  National Institute of Sciences (Calcutta), 2:49-55, 1936.  Kanti V. Mardia. Assessment of multinormality and the robustness of Hotelling\u2019s t2-test.  Applied Statistics, pages 163-171, 1975.  Emanuel Parzen. An approach to time series analysis. Journal of the American Statistical  Association, 32:951-989, 1961.  Gert K. Pedersen. Some operator monotone functions. Proceedings of the American Math-  ematical Society, 36:309-310, 1972. On Mahalanobis Distance in Functional Settings  Kay I. Penny. Appropriate critical values when testing for a single multivariate outlier by using the Mahalanobis distance. Journal of the Royal Statistical Society. Series C (Applied Statistics), 45:73-81, 1996.  Alvin C. Rencher. Methods of Multivariate Analysis, volume 3 ed. John Wiley & Sons,  2012.  Peter J. Rousseeuw and Katrien van Driessen. A fast algorithm for the minimum covariance  determinant estimator. Technometrics, 41:212-223, 1999.  Peter J. Rousseeuw and Bert C. van Zomeren. Unmasking multivariate outliers and leverage  points. Journal of the American Statistical Association, 85(411):633-639, 1990.  Bernhard Sch\u00a8olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Ma-  chines, Regularization, Optimization, and Beyond. MIT Press, 2002.  Yijun Zuo and Robert Ser\ufb02ing. General notions of statistical depth function. Annals of  Statistics, 28:461-482, 2000. "}, "Online Sufficient Dimension Reduction Through Sliced Inverse Regression": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-567.html", "header": "Online Sufficient Dimension Reduction Through Sliced Inverse Regression", "author": "Zhanrui Cai, Runze Li, Liping Zhu", "time": "21(10):1\u221225, 2020.", "abstract": "Sliced inverse regression is an effective paradigm that achieves the goal of dimension reduction through replacing  high dimensional covariates with a small number of linear combinations. It does not impose  parametric assumptions on the dependence structure. More importantly, such a reduction of dimension  is sufficient in that it  does  not cause  loss of  information. In this paper, we adapt the stationary sliced inverse regression to cope with the rapidly  changing environments. We propose to implement sliced inverse regression in an online fashion. This online  learner consists of two steps. In the first step we construct an online estimate for the  kernel matrix; in the second step we propose two online algorithms, one is motivated by the perturbation method and the other is originated from the gradient descent optimization, to perform   online singular value decomposition. The  theoretical properties of this   online learner are established. We demonstrate the numerical performance of  this  online learner  through simulations and  real world applications. All numerical studies confirm that this  online learner  performs as well as the batch learner.", "pdf_url": "http://jmlr.org/papers/volume21/18-567/18-567.pdf", "keywords": ["Dimension reduction", "online learning", "perturbation", "singular value decompo sition", "sliced inverse regression", "gradient descent."], "reference": "Raman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic optimiza- tion for PCA and PLS. In 50th Allerton Conference on Communication, Control, and Computing, pages 861-868. IEEE, 2012.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of PCA with capped MSG. In Advances in Neural Information Processing Systems, pages 1815-1823, 2013.  Andreas Artemiou and Bing Li. On principal components and regression: a statistical  explanation of a natural phenomenon. Statistica Sinica, pages 1557-1565, 2009.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incre- mental PCA. In Advances in Neural Information Processing Systems, pages 3174-3182, 2013.  Bernard Bercu, Thi Mong Ngoc Nguyen, and Jerome Saracco. On the asymptotic behaviour of the recursive nadaraya-watson estimator associated with the recursive sliced inverse regression method. Statistics, 49(3):660-679, 2015.  L\u00b4eon Bottou. Online Learning and Stochastic Approximations. Cambridge University Press,  Cambridge, UK, 1998.  Olivier Bousquet and L\u00b4eon Bottou. The tradeo\ufb00s of large scale learning. In Advances in  Neural Information Processing Systems, pages 161-168, 2008.  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal compo- nents analysis. In Proceedings of the Twenty-sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887-901. SIAM, 2015.  Benoit Champagne. Adaptive eigendecomposition of data covariance matrices based on IEEE Transactions on Signal Processing, 42(10):2758-2770,  first-order perturbations. 1994.  Marie Chavent, St\u00b4ephane Girard, Vanessa Kuentz-Simonet, Benoit Liquet, Thi Mong Ngoc Nguyen, and J\u00b4er\u02c6ome Saracco. A sliced inverse regression approach for data stream. Computational Statistics, 29(5):1129-1152, 2014.  R Dennis Cook. Regression Graphics: Ideas for Studying Regressions through Graphics,  volume 482. John Wiley & Sons, 2009.  R Dennis Cook and Sanford Weisberg. Discussion of sliced inverse regression for dimension reduction by Li,K.C. Journal of the American Statistical Association, 86(414):328-332, 1991.  Donald L Fisk. Quasi-martingales. Transactions of the American Mathematical Society,  120(3):369-389, 1965.  23   Online Sufficient Dimension Reduction  References  Raman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic optimiza- tion for PCA and PLS. In 50th Allerton Conference on Communication, Control, and Computing, pages 861-868. IEEE, 2012.  Raman Arora, Andy Cotter, and Nati Srebro. Stochastic optimization of PCA with capped MSG. In Advances in Neural Information Processing Systems, pages 1815-1823, 2013.  Andreas Artemiou and Bing Li. On principal components and regression: a statistical  explanation of a natural phenomenon. Statistica Sinica, pages 1557-1565, 2009.  Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incre- mental PCA. In Advances in Neural Information Processing Systems, pages 3174-3182, 2013.  Bernard Bercu, Thi Mong Ngoc Nguyen, and Jerome Saracco. On the asymptotic behaviour of the recursive nadaraya-watson estimator associated with the recursive sliced inverse regression method. Statistics, 49(3):660-679, 2015.  L\u00b4eon Bottou. Online Learning and Stochastic Approximations. Cambridge University Press,  Cambridge, UK, 1998.  Olivier Bousquet and L\u00b4eon Bottou. The tradeo\ufb00s of large scale learning. In Advances in  Neural Information Processing Systems, pages 161-168, 2008.  Christos Boutsidis, Dan Garber, Zohar Karnin, and Edo Liberty. Online principal compo- nents analysis. In Proceedings of the Twenty-sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887-901. SIAM, 2015.  Benoit Champagne. Adaptive eigendecomposition of data covariance matrices based on IEEE Transactions on Signal Processing, 42(10):2758-2770,  first-order perturbations. 1994.  Marie Chavent, St\u00b4ephane Girard, Vanessa Kuentz-Simonet, Benoit Liquet, Thi Mong Ngoc Nguyen, and J\u00b4er\u02c6ome Saracco. A sliced inverse regression approach for data stream. Computational Statistics, 29(5):1129-1152, 2014.  R Dennis Cook. Regression Graphics: Ideas for Studying Regressions through Graphics,  volume 482. John Wiley & Sons, 2009.  R Dennis Cook and Sanford Weisberg. Discussion of sliced inverse regression for dimension reduction by Li,K.C. Journal of the American Statistical Association, 86(414):328-332, 1991.  Donald L Fisk. Quasi-martingales. Transactions of the American Mathematical Society,  120(3):369-389, 1965. Zhanrui Cai, Runze Li, and Liping Zhu  Anant Hegde, Jose C Principe, Deniz Erdogmus, Umut Ozertem, Yadunandana N Rao, and Hemanth Peddaneni. Perturbation-based eigenvector updates for on-line principal com- ponents analysis and canonical correlation analysis. Journal of VLSI Signal Processing Systems for Signal, Image and Video Technology, 45(1-2):85-95, 2006.  Tosio Kato. Perturbation Theory for Linear Operators, volume 132. Springer Science &  Business Media, 2013.  Harold Joseph Kushner and Dean S Clark. Stochastic Approximation Methods for Con- strained and Unconstrained Systems, volume 26. Springer Science & Business Media, 2012.  Yunwen Lei, Lei Shi, and Zheng-Chu Guo. Convergence of unregularized online learning  algorithms. The Journal of Machine Learning Research, 18(1):6269-6301, 2017.  Bing Li. Su\ufb03cient Dimension Reduction: Methods and Applications with R. CRC Press,  2018.  Bing Li and Shaoli Wang. On directional regression for dimension reduction. Journal of  the American Statistical Association, 102(479):997-1008, 2007.  Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American  Statistical Association, 86(414):316-327, 1991.  Weihua Li, H Henry Yue, Sergio Valle-Cervantes, and S Joe Qin. Recursive PCA for adaptive  process monitoring. Journal of Process Control, 10(5):471-486, 2000.  Yanyuan Ma and Liping Zhu. A semiparametric approach to dimension reduction. Journal  of the American Statistical Association, 107(497):168-179, 2012.  Yanyuan Ma and Liping Zhu. A review on dimension reduction. International Statistical  Review, 81(1):134-150, 2013.  Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11(Jan):19-60, 2010.  David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. e1071: misc functions of the department of statistics, probability theory group (formerly: E1071), TU Wien. R package version 1.6-8. 2017.  Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming PCA. In Advances in Neural Information Processing Systems, pages 2886-2894, 2013.  Jiazhong Nie, Wojciech Kotlowski, and Manfred K. Warmuth. Online PCA with optimal  regret. Journal of Machine Learning Research, 17(173):1-49, 2016.  Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigen- values of the expectation of a random matrix. Journal of Mathematical Analysis and Applications, 106(1):69-84, 1985. Online Sufficient Dimension Reduction  Ohad Shamir. Convergence of stochastic gradient descent for PCA. In International Con-  ference on Machine Learning, pages 257-265, 2016.  Robin Sibson. Studies in the robustness of multidimensional scaling: Perturbational analysis of classical scaling. Journal of the Royal Statistical Society. Series B (Methodological), pages 217-229, 1979.  Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization IEEE Transactions on Information  paths: Optimality and almost-sure convergence. Theory, 60(9):5716-5735, 2014.  Manfred K Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension. Journal of Machine Learning Research, 9 (Oct):2287-2320, 2008.  Yingcun Xia, Howell Tong, Wai Keung Li, and Li-Xing Zhu. An adaptive estimation of dimension reduction space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):363-410, 2002.  Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimiza-  tion. Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.  Wenzhuo Yang and Huan Xu. Streaming sparse principal component analysis. In Interna-  tional Conference on Machine Learning, pages 494-503, 2015.  Zhishen Ye and Robert E Weiss. Using the bootstrap to select one of a new class of dimension reduction methods. Journal of the American Statistical Association, 98(464): 968-979, 2003.  Li-Ping Zhu, Li-Xing Zhu, and Zheng-Hui Feng. Dimension reduction in regressions through cumulative slicing estimation. Journal of the American Statistical Association, 105(492): 1455-1466, 2010.  Li-Xing Zhu and Kai Wang Ng. Asymptotics of sliced inverse regression. Statistica Sinica,  pages 727-736, 1995. "}, "Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-573.html", "header": "Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information", "author": "T. Tony Cai, Tengyuan Liang, Alexander Rakhlin", "time": "21(11):1\u221234, 2020.", "abstract": "We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification.", "pdf_url": "http://jmlr.org/papers/volume21/18-573/18-573.pdf", "keywords": ["weighted message passing", "minimum energy \ufb02ow", "statistical inference"], "reference": "Emmanuel Abbe and Colin Sandon.  fundamental limits and e\ufb03cient recovery algorithms.  Community detection in general stochastic arXiv preprint  block models: arXiv:1503.00609, 2015a.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with mul- tiple clusters: proof of the achievability conjectures, acyclic bp, and the information- computation gap. arXiv preprint arXiv:1512.09080, 2015b.  Emmanuel Abbe and Colin Sandon. Proof of the achievability conjectures for the general stochastic block model. Communications on Pure and Applied Mathematics, 71(7):1334- 1406, 2018.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic  block model. arXiv preprint arXiv:1405.3267, 2014.  Emmanuel Abbe, Francois Baccelli, and Abishek Sankararaman. Community detection on  euclidean random graphs. arXiv preprint arXiv:1706.09942, 2017a.  Emmanuel Abbe, Laurent Massoulie, Andrea Montanari, Allan Sly, and Nikhil Srivastava.  Group synchronization on grids. arXiv preprint arXiv:1706.08561, 2017b.  Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided they blog. In Proceedings of the 3rd international workshop on Link discovery, pages 36-43. ACM, 2005.  Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation In Advances in Neural Information  of a graphon: Theory and consistent estimation. Processing Systems, pages 692-700, 2013.  Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership stochastic blockmodels. Journal of machine learning research, 9(Sep):1981-2014, 2008.  Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Information-theoretic thresholds for community detection in sparse networks. In Conference on Learning The- ory, pages 383-416, 2016.  28   Cai, Liang and Rakhlin  this coupled multi-type Galton Watson tree is \u03bb1(Qdiag(N )) almost surely. Proof is com- pleted.  The authors thank Elchanan Mossel for discussions on block models, and thank one anony- mous referee and the associated editor for valuable suggestions.  Acknowledgments  References  Emmanuel Abbe and Colin Sandon.  fundamental limits and e\ufb03cient recovery algorithms.  Community detection in general stochastic arXiv preprint  block models: arXiv:1503.00609, 2015a.  Emmanuel Abbe and Colin Sandon. Detection in the stochastic block model with mul- tiple clusters: proof of the achievability conjectures, acyclic bp, and the information- computation gap. arXiv preprint arXiv:1512.09080, 2015b.  Emmanuel Abbe and Colin Sandon. Proof of the achievability conjectures for the general stochastic block model. Communications on Pure and Applied Mathematics, 71(7):1334- 1406, 2018.  Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic  block model. arXiv preprint arXiv:1405.3267, 2014.  Emmanuel Abbe, Francois Baccelli, and Abishek Sankararaman. Community detection on  euclidean random graphs. arXiv preprint arXiv:1706.09942, 2017a.  Emmanuel Abbe, Laurent Massoulie, Andrea Montanari, Allan Sly, and Nikhil Srivastava.  Group synchronization on grids. arXiv preprint arXiv:1706.08561, 2017b.  Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided they blog. In Proceedings of the 3rd international workshop on Link discovery, pages 36-43. ACM, 2005.  Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation In Advances in Neural Information  of a graphon: Theory and consistent estimation. Processing Systems, pages 692-700, 2013.  Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. Mixed membership stochastic blockmodels. Journal of machine learning research, 9(Sep):1981-2014, 2008.  Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Information-theoretic thresholds for community detection in sparse networks. In Conference on Learning The- ory, pages 383-416, 2016. Weighted Message Passing for General SBM  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57 (2):764-785, 2011.  Mohsen Bayati, Marc Lelarge, Andrea Montanari, et al. Universality in polytope phase transitions and message passing algorithms. The Annals of Applied Probability, 25(2): 753-822, 2015.  Charles Bordenave, Marc Lelarge, and Laurent Massouli\u00b4e. Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 1347-1357. IEEE, 2015.  T Tony Cai and Xiaodong Li. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. The Annals of Statistics, 43(3):1027-1059, 2015.  T Tony Cai, Tengyuan Liang, and Alexander Rakhlin. Inference via message passing on  partially labeled stochastic block models. arXiv preprint arXiv:1603.06923, 2016.  Yudong Chen, Xiaodong Li, and Jiaming Xu. Convexified modularity maximization for  degree-corrected stochastic block models. arXiv preprint arXiv:1512.08425, 2015.  Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics,  Probability and Computing, 19(02):227-284, 2010.  Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00b4a. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applica- tions. Physical Review E, 84(6):066106, 2011.  Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information  for the two-groups stochastic block model. arXiv preprint arXiv:1507.08685, 2015.  William Evans, Claire Kenyon, Yuval Peres, and Leonard J Schulman. Broadcasting on  trees and the ising model. Annals of Applied Probability, pages 410-433, 2000.  Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, and Barna Saha. The geometric  block model. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 369-376. ACM, 2014.  Olivier Gu\u00b4edon and Roman Vershynin. Community detection in sparse networks via grothendieck\u2019s inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016.  Lennart Gulikers, Marc Lelarge, and Laurent Massouli\u00b4e. A spectral method for community detection in moderately sparse degree-corrected stochastic block models. Advances in Applied Probability, 49(3):686-721, 2017. Cai, Liang and Rakhlin  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via  semidefinite programming. arXiv preprint arXiv:1412.6156, 2014.  Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via  semidefinite programming: Extensions. arXiv preprint arXiv:1502.07738, 2015.  Svante Janson and Elchanan Mossel. Robust reconstruction on trees is determined by the  second eigenvalue. Annals of probability, pages 2630-2649, 2004.  Jiashun Jin. Fast community detection by score. The Annals of Statistics, 43(1):57-89,  2015.  Varun Kanade, Elchanan Mossel, and Tselil Schramm. Global and local information in  clustering labeled block models. arXiv preprint arXiv:1404.6325, 2014.  Harry Kesten and Bernt P Stigum. Additional limit theorems for indecomposable mul- tidimensional galton-watson processes. The Annals of Mathematical Statistics, pages 1463-1481, 1966a.  Harry Kesten and Bernt P Stigum. A limit theorem for multidimensional galton-watson  processes. The Annals of Mathematical Statistics, 37(5):1211-1223, 1966b.  Jon Kleinberg. The small-world phenomenon: An algorithmic perspective. In Proceedings of the thirty-second annual ACM symposium on Theory of computing, pages 163-170. ACM, 2000.  Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zde- borov\u00b4a, and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of Sciences, 110(52):20935-20940, 2013.  Nathan Linial. Locality in distributed graph algorithms. SIAM Journal on Computing, 21  (1):193-201, 1992.  University Press, 2017.  Russell Lyons and Yuval Peres. Probability on trees and networks, volume 42. Cambridge  Laurent Massouli\u00b4e. Community detection thresholds and the weak ramanujan property. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694- 703. ACM, 2014.  Andrea Montanari. Finding one community in a sparse graph. Journal of Statistical Physics,  161(2):273-299, 2015.  Elchanan Mossel. Reconstruction on trees: beating the second eigenvalue. Annals of Applied  Probability, pages 285-300, 2001.  Elchanan Mossel and Yuval Peres. Probability, 13(3):817-844, 2003.  Information \ufb02ow on trees. The Annals of Applied  Elchanan Mossel, Joe Neeman, and Allan Sly. Stochastic block models and reconstruction.  arXiv preprint arXiv:1202.1499, 2012. Weighted Message Passing for General SBM  Elchanan Mossel, Joe Neeman, and Allan Sly. Belief propagation, robust reconstruction,  and optimal recovery of block models. arXiv preprint arXiv:1309.1380, 2013a.  Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold con-  jecture. arXiv preprint arXiv:1311.4115, 2013b.  Huy N Nguyen and Krzysztof Onak. Constant-time approximation algorithms via local improvements. In Foundations of Computer Science, 2008. FOCS\u201908. IEEE 49th Annual IEEE Symposium on, pages 327-336. IEEE, 2008.  Michal Parnas and Dana Ron. Approximating the minimum vertex cover in sublinear time and a connection to distributed algorithms. Theoretical Computer Science, 381(1):183- 196, 2007.  Robin Pemantle and Je\ufb00rey E Steif. Robust phase transitions for heisenberg and other  models on general trees. Annals of Probability, pages 876-912, 1999.  Alaa Saade, Florent Krzakala, Marc Lelarge, and Lenka Zdeborov\u00b4a. Fast randomized semi-  supervised clustering. arXiv preprint arXiv:1605.06422, 2016.  Alexandre B Tsybakov. Introduction to nonparametric estimation, volume 11. Springer  Series in Statistics, 2009.  Bin Yu. Assouad, fano, and le cam.  In Festschrift for Lucien Le Cam, pages 423-435.  Springer, 1997.  block model. 2016.  Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic In Advances in Neural Information Processing Systems, pages 965-973,  Anderson Y Zhang and Harrison H Zhou. Minimax rates of community detection in stochas-  tic block models. arXiv preprint arXiv:1507.05313, 2015. Cai, Liang and Rakhlin  Appendix A. Remaining Proofs  Proof [Proof of Lemma 15] The proof logic here is similar to the k = 2 case. Again, we analyze the message M (u, t) for a particular node u. Use induction on t for the claim  (cid:104)  E  e\u03bbM (u,t)|(cid:96)(u) = l  \u2264 e\u03bb\u00b5t(u,l)e  (cid:105)  \u03bb2\u03c32 t (u) 2  .  The case for t = 0 follows from the assumption about \u00b50(u), \u03c32 Assume that the induction is true for t \u2212 1, and prove the case for t. Note that  0(u) and Cherno\ufb00 bound.  (cid:104)  E  (cid:105)  e\u03bbM (u,t)|(cid:96)(u) = l (cid:104)  (cid:89)  E  e\u03bb\u03b8M (v,t\u22121)|(cid:96)(u) = l  (cid:105)  (cid:104)  (cid:105) e\u03bb\u03b8M (v,t\u22121)|(cid:96)(v) = i  E  Kli  (cid:41)  (cid:41)  (cid:89)  e(\u03bb\u03b8)2  \u03c32  t\u22121(v) 2  e\u03bb\u03b8\u00b5t\u22121(v,i)Kli  (cid:40) k  (cid:88)  i=1  =  =  \u2264  \u2264  v\u2208C(u)  (cid:40) k  (cid:88)  (cid:89)  v\u2208C(u)  i=1  v\u2208C(u)  v\u2208C(u)  (cid:89)  e(\u03bb\u00af\u03b8)2  \u03c32  t\u22121(v) 2  e\u03bb\u03b8[(cid:80)k  i=1 \u00b5t\u22121(v,i)Kli]e(\u03bb\u03b8)2  maxi,j\u2208[k] |\u00b5t\u22121(v,i)\u2212\u00b5t\u22121(v,j)|2 8  ,  where the last step uses the Hoe\ufb00ding\u2019s Lemma. Rearrange the terms, one can see that the above equation implies  (cid:104) e\u03bbM (u,t)|(cid:96)(u) = l  (cid:105)  E  \u2264 e\u03bb (cid:80)  v\u2208C(u) \u03b8(cid:104)Kl\u00b7,\u00b5t\u22121(u)(cid:105)e  \u03bb2\u03b82 (cid:80)  v\u2208C(u)  t\u22121(v)+maxi,j\u2208[k]  (cid:40)  \u03c32  (cid:12) (cid:12) (cid:12) (cid:12)  \u00b5t\u22121(v,+)\u2212\u00b5t\u22121(v,\u2212) 2  2(cid:41)  (cid:12) (cid:12) (cid:12) (cid:12)= e\u03bb\u00b5t(u,l)e  \u03bb2\u03c32 t (u) 2  ,  where Kl\u00b7 denotes the l\u2212row of transition matrix K. Apply the Cherno\ufb00 bound to optimize over \u03bb, one can arrive the exponential concentration bound. Induction completes.  To upper bound the misclassification error, simply plug in  |x| =  mini,j\u2208[k] |\u00b5\u00aft(o, i) \u2212 \u00b5\u00aft(o, j)| 2\u03c3\u00aft(o)  .  Proof [Proof of Proposition 19] Recall that \u03c0((cid:96)\u2202Tt(o)|(cid:96)(o) = i) denotes the probability measure on the leaf labels on depth t, given (cid:96)(o) = i. For the root o, we abbreviate the o . Denote \u00af\u03c0o = 1/k \u00b7 (cid:80)k measure \u03c0((cid:96)\u2202Tt(o)|(cid:96)(o) = i) as \u03c0(i)  Consider (cid:15)\u2217 to be the same as in Theorem 1.2. (iii) of Janson and Mossel (2004) (The- orem 3.3 therein for the general tree case), as our model is the erasure model considered there with erasure probability 1 \u2212 \u03b4 independently for each leaf. Under the condition that  j=1 \u03c0(j) o .  br[T (o)]\u03b82 < 1, Weighted Message Passing for General SBM  we know by the result of Lemma 2.6 in Janson and Mossel (2004), for any i, j \u2208 [k]  lim t\u2192\u221e  d\u03c72  (cid:16)  o , \u03c0(j) \u03c0(i) o  (cid:17)  = lim t\u2192\u221e  (cid:90) (cid:32)  (cid:33)2  d\u03c0(i) o d\u03c0(j) o  d\u03c0(j)  o \u2212 1 \u2264 lim t\u2192\u221e  sup i,j,l  (cid:12) (cid:90) d\u03c0(i) (cid:12) o (cid:12) (cid:12) d\u03c0(j) (cid:12) o  d\u03c0(l) o d\u03c0(j) o  d\u03c0(j)  o \u2212 1  = 0  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  for 1 \u2212 \u03b4 > (cid:15)\u2217. Note that  1 k  k (cid:88)  i=1  (cid:16)  d\u03c72  \u03c0(i) o , \u00af\u03c0o  (cid:17)  =  d\u03c0(i)  o \u2212 1 by convexity of 1/x  d\u03c0(i)  o \u2212 1  1 k  1 k  k (cid:88)  i=1  k (cid:88)  i=1  (cid:90) (cid:32)  (cid:90) 1 k  1 k2  1 k2  k (cid:88)  k (cid:88)  i=1  j=1  k (cid:88)  k (cid:88)  i=1  j=1  \u2264  \u2264  =  (cid:33)  (cid:80)k  1 k  k (cid:88)  j=1 (cid:90) (cid:32)  d\u03c0(i) o j=1 \u03c0(j) o (cid:33) (cid:32) d\u03c0(i) o \u03c0(j) o d\u03c0(i) o \u03c0(j) o  (cid:33)  (cid:16)  d\u03c72  o , \u03c0(j) \u03c0(i) o  (cid:17)  .  d\u03c0(i)  o \u2212 1  lim t\u2192\u221e  1 k  k (cid:88)  i=1  (cid:16)  d\u03c72  \u03c0(i) o , \u00af\u03c0o  (cid:17)  = 0.  Putting things together, under the condition that br[T (o)]\u03b82 < 1, we have  Finally, we invoke the multiple testing argument Theorem 2.6 in Tsybakov (2009)).  Lemma 24 (Tsybakov (2009), Proposition 2.4, Theorem 2.6) Let P0, P1, . . . , Pk be probability measures on (X , A) satisfying  then we have for any selector \u03c8 : X \u2192 [k]  1 k  k (cid:88)  i=1  d\u03c72(Pj, P0) \u2264 k\u03b1\u2217  max i\u2208[k]  Pi(\u03c8 (cid:54)= i) \u2265  (1 \u2212 \u03b1\u2217 \u2212  ).  1 k  1 2  Plugging in the result with P0 = \u00af\u03c0o and Pi = \u03c0(i) choose \u03b1(cid:63)(t) \u2192 0 such that  o , we conclude that as t \u2192 \u221e, we can  lim inf t\u2192\u221e  inf \u03c3  max l\u2208[k]  P(\u03c3(o) (cid:54)= (cid:96)(o)|(cid:96)(o) = l) \u2265  (1 \u2212  ).  1 2  1 k Cai, Liang and Rakhlin  Proof [Proof of Proposition 20] The proof is a standard exercise following the idea from Proposition 4.2 in Mossel et al. (2012). First, let\u2019s recall Bernstein inequality. Consider X \u223c Binom(n, p0), then the following concentration inequality holds  P(X \u2265 np0 + t) \u2264 exp(\u2212  t2 2(np0 + t/3)  ).  Hence if we plug in t = 2  3 log n +  2np0 log n, we know  |\u2202G1|  sto. \u2264 X \u2264 np0 +  log n + (cid:112)2np0 log n \u2264 2np0 + 2 log n  \u221a  2 3  with probability at least 1 \u2212 n\u22121.  Now, through union bound, we can prove that  P (\u2200r \u2264 R, |\u2202Gr| \u2264 (2np0 + 2 log n)r) \u2265 1 \u2212 C \u00b7 (2np0 + 2 log n)Rn\u22121 \u2265 1 \u2212 O(n\u22123/4).  And we know that on the same event,  |\u2202Gr| \u2264 n1/4, \u2200r \u2264 R.  It is clear that bad events that GR is not a tree (with cycles) for each layer is bounded 0|\u2202Gr| + p0|\u2202Gr|2. Take a further union bound over all layers, we know this  above by p2 probability is bounded by O(n\u22121/8) provided p0 = o(n\u22125/8).  Now we need to recursively use the Poisson-Binomial coupling (to achieve Poisson- Multinomial coupling). The following Lemma is taken from Mossel et al. (2012) (Lemma 4.6).  Lemma 25 If m, n are positive integers then  (cid:107)Binom(m,  ) \u2212 Poisson(c)(cid:107)T V \u2264 O(  c n  c2m n2 + c|  m n  \u2212 1|)  Now we condition on all the good events up to layer Gr\u22121, which happens with proba- bility at least 1 \u2212 n\u22121/8 \u2212 n\u22123/4. We can couple the next layer for nodes in \u2202Gr. Take a node v \u2208 \u2202Gr as an example. Assume it is of color i, then the number of color j nodes in his children follows Binom(|V i >r|, pij). Comparing to the Poisson version Poisson(nipij), we know with probability at least  1 \u2212 O(nip2  ij + pij|V i  >r \u2212 ni|),  one can couple the Poisson and Binomial in the same probability space. Note that |V i >r \u2212 ni| \u2264 |\u2202Gr|. Repeat this recursively, and use the union bound, we can couple (GR, (cid:96)GR) = (TR, (cid:96)TR) with probability at least 1 \u2212 O(k maxi(ni)p2  0 + kp0n1/4)n1/4 log n = 1 \u2212 o(1).  Therefore if np0 = no(1) and k (cid:45) log n, we have the bad event (when we cannot couple) happens with probability going to 0 as n \u2192 \u221e. And if p0 = no(1), we can allow R to grow to infinity at a slow rate as R (cid:45) .  log n log[no(1)+log n] "}, "Neyman-Pearson classification: parametrics and sample size requirement": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-577.html", "header": "Neyman-Pearson classification: parametrics and sample size requirement", "author": "Xin Tong, Lucy Xia, Jiacheng Wang, Yang Feng", "time": "21(12):1\u221248, 2020.", "abstract": "The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level $\\alpha$. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng, and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class $0$ observation as class $1$ under the 0-1 coding) upper bound $\\alpha$ with high probability, without specific  distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class $0$, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class $0$ observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class $0$ observations,  we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/18-577/18-577.pdf", "keywords": ["classification", "asymmetric error", "Neyman-Pearson (NP) paradigm", "NP oracle inequalities", "minimum sample size requirement", "linear discriminant analysis (LDA)", "NP umbrella algorithm", "adaptive splitting"], "reference": "A. Bloemendal, L. Erdos, A. Knowles, H.-T. Yau, and J. Yin. Isotropic local laws for sample covariance and generalized wigner matrices. Electronic Journal of Probability, 19:1-53, 2015.  46   Tong, Xia, Wang and Feng  Regarding (II(cid:48)), by Lemma 10 we have  (II(cid:48)) \u2264  rdP0 + C\u03b1  dP0 = P1(Cc) + C\u03b1P0(Cc) \u2264 (1 + C\u03b1) exp{\u2212(n0 \u2227 n1)1/2} .  (cid:90)  Cc  (cid:90)  Cc  Therefore, by the excess type II error decomposition equation (40),  P1( (cid:98)G) \u2212 P1(G\u2217) = (I) + (II) + (I(cid:48)) + (II(cid:48)) + C\u03b1{R0(\u03c6\u2217  \u03b1) \u2212 R0( \u02c6\u03c6k\u2217)} .  Using the upper bounds for (I), (II), (I(cid:48)) and (II(cid:48)) and Lemma 8, With probability at least 1 \u2212 \u03b40 \u2212 \u03b4(cid:48)  0 \u2212 \u03b41 \u2212 \u03b42, we have  P1( (cid:98)G) \u2212 P1(G\u2217) \u2264 c(cid:48)M0  (\u2206R0,C/M1)  (cid:19)1+\u00af\u03b3  1/\u03b3\u2212 + 2T  (cid:18)  (cid:18)  + c(cid:48)(cid:48)M0  (\u2206R0,C/M0)1/\u00af\u03b3 \u2227 (\u2206R0,C/M1)  (cid:19)1+\u00af\u03b3  1/\u03b3\u2212 + 2T  + 2(1 + C\u03b1) exp{\u2212(n0 \u2227 n1)1/2} + C\u03b1 \u00b7 \u03be\u03b1,\u03b40,n(cid:48)  (\u03b4(cid:48) 0)\u2264 \u00afc(cid:48)  1\u2206R  (1+\u00af\u03b3)/\u03b3\u2212 0,C  + \u00afc(cid:48)  2T 1+\u00af\u03b3 + \u00afc(cid:48)  3 exp{\u2212(n0 \u2227 n1)1/2} + C\u03b1 \u00b7 \u03be\u03b1,\u03b40,n(cid:48)  (\u03b4(cid:48)  0) ,for some positive constants \u00afc(cid:48) \u03b3 \u2212  \u2265 \u00af\u03b3. Note that on the event E2 \u2229 E3, Lemma 11 guarantees \u2206R0,C \u2264 2[\u03be\u03b1,\u03b40,n(cid:48) 1\u03d5\u03bbs(n0 \u2227 n1)1/4. Therefore,  exp{\u2212(n0 \u2227 n1)1/2}]. Lemma 10 guarantees that T \u2264 4c(cid:48)3. In the last inequality of the above chain, we used 0) +  (\u03b4(cid:48)  1, \u00afc(cid:48)  2 and \u00afc(cid:48)  P1( (cid:98)G) \u2212 P0(G\u2217) \u2264 \u00afc(cid:48)(cid:48)  1\u03be\u03b1,\u03b40,n(cid:48)(\u03b4(cid:48) 0)  (1+\u00af\u03b3)/\u03b3\u2212\u22271  1\u03d5\u03bbs(n0 \u2227 n1)1/4]1+\u00af\u03b3  + \u00afc(cid:48)(cid:48)  3[exp{\u2212(n0 \u2227 n1)  1 2 }]  + \u00afc(cid:48)(cid:48) 2[4c(cid:48) (1+\u00af\u03b3)/\u03b3\u2212 .  Lemma 7 guarantees that \u03be\u03b1,\u03b40,n(cid:48)(\u03b4(cid:48)  0) \u2264 (5/2)(n(cid:48)  0)\u22121/4. Then the excess type II error is  bounded by  P1( (cid:98)G) \u2212 P0(G\u2217) \u2264 \u00afc1(n(cid:48) 0)  \u2212( 1  4 \u2227 1+\u00af\u03b3 4 \u03b3\u2212  )  + \u00afc2(\u03bbs)1+\u00af\u03b3(n0 \u2227 n1)  1+\u00af\u03b3 4  + \u00afc3 exp  \u2212(n0 \u2227 n1)  \u2227 1)  .  \uf8f1 \uf8f2  \uf8f3  1 2 (  1 + \u00af\u03b3 \u03b3 \u2212  \uf8fc \uf8fd  \uf8fe  References  A. Bloemendal, L. Erdos, A. Knowles, H.-T. Yau, and J. Yin. Isotropic local laws for sample covariance and generalized wigner matrices. Electronic Journal of Probability, 19:1-53, 2015. Neyman-Pearson classification: parametrics and sample size requirement  Leo Breiman. Random forests. Machine Learning, 45(1):5-32, 2001.  Tony Cai and Weidong Liu. A direct estimation approach to sparse linear discriminant  analysis. J. Amer. Statist. Assoc., 106:1566-1577, 2011.  A. Cannon, J. Howse, D. Hush, and C. Scovel. Learning with the neyman-pearson and  min-max criteria. Technical Report LA-UR-02-2951, 2002.  D. Casasent and X. Chen. Radial basis function neural networks for nonlinear fisher dis- crimination and neyman-pearson classification. Neural Networks, 16(5-6):529 - 535, 2003.  Koei Chin, Sandy DeVries, Jane Fridlyand, Paul T Spellman, Ritu Roydasgupta, Wen-Lin Kuo, Anna Lapuk, Richard M Neve, Zuwei Qian, Tom Ryder, Fanqing Chen, Heidi Feiler, Taku Tokuyasu, Chris Kingsley, Shanaz Dairkee, Zhenhang Meng, Karen Chew, Daniel Pinkel, Ajay Jain, Britt Marie Ljung, Laura Esserman, Donna G Albertson, Frederic M Waldman, and Joe W Gray. Genomic and transcriptional aberrations linked to breast cancer pathophysiologies. Cancer Cell, 10(6):529-541, December 2006.  C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the Seventeenth  International Joint Conference on Artificial Intelligence, pages 973-978, 2001.  J. Fan, Y. Feng, and X. Tong. A road to classification in high dimensional space: the regularized optimal a\ufb03ne discriminant. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 74:745-771, 2012.  Gavin J GJ Gordon, Roderick V RV Jensen, Li-Li LL Hsiao, Steven R SR Gullans, Joshua E JE Blumenstock, Sridhar S Ramaswamy, William G WG Richards, David J DJ Sug- arbaker, and Raphael R Bueno. Translation of Microarray Data into Clinically Relevant Cancer Diagnostic Tests Using Gene Expression Ratios in Lung Cancer and Mesothe- lioma. Cancer Research, 62(17):4963-4967, September 2002.  Yaqian Guo, Trevor Hastie, and Robert Tibshirani. Regularized discriminant analysis and  its application in microarrays. Biostatistics, 1:1-18, 2005.  Min Han, Dirong Chen, and Zhaoxu Sun. Analysis to Neyman-Pearson classification with ISSN  convex loss function. Analysis in Theory and Applications, 24(1):18-28, 2008. 1672-4070. doi: 10.1007/s10496-008-0018-3.  Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17(52):1-6, 2012.  Jingyi Jessica Li and Xin Tong. Genomic applications of the neyman-pearson classification  paradigm. In Big Data Analytics in Genomics, pages 145-167. Springer, 2016.  Q. Mai, H. Zou, and M. Yuan. A direct approach to sparse discriminant analysis in ultra-  high dimensions. Biometrika, 99:29-42, 2012.  E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. Annals of Statistics, 27:  1808-1829, 1999. Tong, Xia, Wang and Feng  Rahul Mukerjee and SH Ong. Variance and covariance inequalities for truncated joint nor- mal distribution via monotone likelihood ratio and log-concavity. Journal of Multivariate Analysis, 139:1-6, 2015.  W. Polonik. Measuring mass concentrations and estimating density contour clusters-an  excess mass approach. Annals of Statistics, 23:855-881, 1995.  P. Rigollet and X. Tong. Neyman-pearson classification, convexity and stochastic con-  straints. Journal of Machine Learning Research, 12:2831-2855, 2011.  C. Scott. Comparison and design of neyman-pearson classifiers. Unpublished, 2005.  C. Scott and R. Nowak. A neyman-pearson approach to statistical learning. IEEE Trans-  actions on Information Theory, 51(11):3806-3819, 2005.  Jun Shao, Yazhen Wang, Xinwei Deng, and Sijian Wang. Sparse linear discriminant analysis by thresholding for high dimensional data. Annals of Statistics, 39:1241-1265, 2011.  Xin Tong. A plug-in approach to neyman-pearson classification. Journal of Machine Learn-  ing Research, 14:3011-3040, 2013.  Xin Tong, Yang Feng, and Anqi Zhao. A survey on neyman-pearson classification and sug- gestions for future research. Wiley Interdisciplinary Reviews: Computational Statistics, 8(2):64-81, 2016.  Xin Tong, Yang Feng, and Jingyi Li. Neyman-Pearson (NP) Classification algorithms and NP receiver operating characteristics (NP-ROC). Science Advances, page eaao1659, 2018.  Vladimir Vapnik. The nature of statistical learning theory. Springer, 1999.  Charles Wang, Binsheng Gong, Pierre R Bushel, Jean Thierry-Mieg, Danielle Thierry-Mieg, Joshua Xu, Hong Fang, Huixiao Hong, Jie Shen, Zhenqiang Su, et al. The concordance between rna-seq and microarray data depends on chemical treatment and transcript abun- dance. Nature biotechnology, 32(9):926, 2014.  D. Witten and R. Tibshirani. Penalized classification using fisher\u2019s linear discriminant.  Journal of the Royal Statistical Society Series B, 73:753-772, 2012.  B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive learning by cost-proportionate exam-  ple weighting. IEEE International Conference on Data Mining, page 435, 2003.  Anqi Zhao, Yang Feng, Lie Wang, and Xin Tong. Neyman-Pearson classification under high  dimensional settings. Journal of Machine Learning Research, 17(213):1-39, 2016. "}, "Generalized probabilistic principal component analysis of correlated data": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-595.html", "header": "Generalized probabilistic principal component analysis of correlated data", "author": "Mengyang Gu, Weining Shen", "time": "21(13):1\u221241, 2020.", "abstract": "Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated.  In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA)  by providing the closed-form maximum marginal likelihood estimator of the factor loadings and other parameters.  Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when   other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience.  Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/18-595/18-595.pdf", "keywords": ["Gaussian process", "maximum marginal likelihood estimator", "kernel method", "principal component analysis", "Stiefel manifold"], "reference": "P.-A. Absil, Alan Edelman, and Plamen Koev. On the largest principal angle between  random subspaces. Linear Algebra and its applications, 414(1):288-294, 2006.  Mauricio A Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for vector-valued functions: A review. Foundations and Trends R(cid:13) in Machine Learning, 4(3):195-266, 2012.  38   Gu and Shen  by Equation (7) in Tipping and Bishop (1999). The predictive distribution of the test output by the PPCA was obtained in a similar fashion as the Equation (24) in the GPPCA and the empirical mean for the test output was added back for comparison. The PPCA does not incorporate the temporal correlation and linear trend in the model.  The temporal model is constructed by a GaSP separately for each test location. The Mat\u00b4ern kernel in (10) and the linear trend h(x) = (1, x) are assumed for the temporal model. The spatial model uses a GaSP with a constant mean separately for each test month. The product kernel in (8) is assumed for the two-dimensional input (latitude and longitude) and the Mat\u00b4ern kernel in (10) is used for each subkernel. The range and the nugget parameters in the temporal model and spatial model are estimated using the RobustGaSP R package (Gu et al. (2019)), and the predictions are also obtained by this package.  The temporal regression by the random forest are trained separately for each location. For each test month, the 439 observations of that month are used as the responses and the 439 \u00d7 220 output on the other months for the same locations are used as the covariates. The regression parameters of this temporal regression capture the temporal dependence of the output between the test month and the training months. The 1200 \u00d7 200 matrix of the temperature anomalies at the test locations and observed time points are used as the test input. The spatial regression by the random forest uses 220 observations of a test location as responses and the 220 \u00d7 439 matrix of the temperature anomalies of the observed locations are used as the input. The 20 \u00d7 439 matrix of the temperature anomalies at the observed locations and test time points are used as the test input. The randomForest R package (Liaw and Wiener, 2002) is used for training models and compute predictions.  The spatio-temporal model assumes a 3 dimensional product kernel in (8) for both time points and locations, and the Mat\u00b4ern kernel in (10) is used as the subkernel for each input variables. Note that if we use the whole training output, the computational order of inverting the covariance matrix is O(N 3), where N = 369360 is the total number of inputs, which is computationally challenging. When the output can be written as an n1 \u00d7 n2 matrix, the likelihood corresponds to a matrix normal distribution, where two kernel functions model the correlation between rows and between columns of the output. The computational order of the matrix normal distribution is the maximum of O(n3 2). We choose the 439\u00d7240 temperature anomalies at the locations with the whole observations to estimate the parameters. The constant mean is assumed for each location. The MLE is used for estimating the range parameters in kernel, nugget, mean and variance parameters. After plugging in the parameters, the predictive distribution of the test data is used for predictions. Though only 439\u00d7240 observations are used for estimating the parameters due to the computational convenience, all 369360 training observations are used for computing the predictive distribution of the test output.  1) and O(n3  References  P.-A. Absil, Alan Edelman, and Plamen Koev. On the largest principal angle between  random subspaces. Linear Algebra and its applications, 414(1):288-294, 2006.  Mauricio A Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for vector-valued functions: A review. Foundations and Trends R(cid:13) in Machine Learning, 4(3):195-266, 2012. Generalized probabilistic principal component analysis  Jushan Bai. Inferential theory for factor models of large dimensions. Econometrica, 71(1):  135-171, 2003.  Jushan Bai and Serena Ng. Determining the number of factors in approximate factor models.  Econometrica, 70(1):191-221, 2002.  M. J. Bayarri, James O. Berger, Eliza S. Calder, Keith Dalbey, Simon Lunagomez, Abani K. Patra, E. Bruce Pitman, Elaine T. Spiller, and Robert L. Wolpert. Using statistical and computer models to quantify volcanic hazards. Technometrics, 51:402-413, 2009.  James O Berger, Victor De Oliveira, and Bruno Sans\u00b4o. Objective bayesian analysis of spatially correlated data. Journal of the American Statistical Association, 96(456):1361- 1374, 2001.  James O Berger, Jos\u00b4e M Bernardo, and Dongchu Sun. The formal definition of reference  priors. The Annals of Statistics, 37(2):905-938, 2009.  A. Bj\u00a8orck and Gene H Golub. Numerical methods for computing angles between linear  subspaces. Mathematics of computation, 27(123):579-594, 1973.  Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.  Stefano Conti and Anthony O\u2019Hagan. Bayesian emulation of complex multi-output and dynamic computer models. Journal of statistical planning and inference, 140(3):640-651, 2010.  Marian Farah, Paul Birrell, Stefano Conti, and Daniela De Angelis. Bayesian emulation and calibration of a dynamic epidemic model for A/H1N1 in\ufb02uenza. Journal of the American Statistical Association, 109(508):1398-1411, 2014.  Thomas E Fricker, Jeremy E Oakley, and Nathan M Urban. Multivariate Gaussian process emulators with nonseparable covariance structures. Technometrics, 55(1):47-56, 2013.  Alan E Gelfand, Alexandra M Schmidt, Sudipto Banerjee, and C. F. Sirmans. Nonstationary multivariate process modeling through spatially varying coregionalization. Test, 13(2): 263-312, 2004.  Alan E Gelfand, Peter Diggle, Peter Guttorp, and Montserrat Fuentes. Handbook of spatial  statistics. CRC Press, 2010.  Mengyang Gu. Jointly robust prior for Gaussian stochastic process in emulation, calibration  and variable selection. Bayesian Analysis, 14(3):857-885, 2019.  Mengyang Gu and James O Berger. Parallel partial Gaussian process emulation for com- puter models with massive output. Annals of Applied Statistics, 10(3):1317-1347, 2016.  Mengyang Gu and Yanxun Xu. Fast nonseparable Gaussian stochastic process with ap- plication to methylation level interpolation. Journal of Computational and Graphical Statistics, In Press, 2019. Gu and Shen  Mengyang Gu, Xiaojing Wang, and James O Berger. Robust Gaussian stochastic process  emulation. Annals of Statistics, 46(6A):3038-3066, 2018.  Mengyang Gu, Jes\u00b4us Palomo, and James O Berger. RobustGaSP: Robust Gaussian stochas-  tic process emulation in R. The R Journal, 11(1):112-136, June 2019.  Jouni Hartikainen and Simo Sarkka. Kalman filtering and smoothing solutions to temporal gaussian process regression models. In Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop for Signal Processing, pages 379-384. IEEE, 2010.  Dave Higdon, James Gattiker, Brian Williams, and Maria Rightley. Computer model cali- bration using high-dimensional output. Journal of the American Statistical Association, 103(482):570-583, 2008.  Peter D Ho\ufb00. Bayesian analysis of matrix data with rstiefel. arXiv preprint arXiv:1304.3673,  2013.  2007.  Heiko Ho\ufb00mann. Kernel PCA for novelty detection. Pattern recognition, 40(3):863-874,  Ian Jolli\ufb00e. Principal component analysis. Springer, 2011.  E\ufb00rosini Kokiopoulou, Jie Chen, and Yousef Saad. Trace optimization and eigenproblems in dimension reduction methods. Numerical Linear Algebra with Applications, 18(3): 565-602, 2011.  Cli\ufb00ord Lam and Qiwei Yao. Factor modeling for high-dimensional time series: inference  for the number of factors. The Annals of Statistics, 40(2):694-726, 2012.  Cli\ufb00ord Lam, Qiwei Yao, and Neil Bathia. Estimation of latent factors for high-dimensional  time series. Biometrika, 98(4):901-918, 2011.  Andy Liaw and Matthew Wiener. Classification and regression by randomforest. R news,  2(3):18-22, 2002.  Fei Liu and Mike West. A dynamic modelling strategy for Bayesian computer model emu-  lation. Bayesian Analysis, 4(2):393-411, 2009.  Sebastian Mika, Bernhard Sch\u00a8olkopf, Alex J Smola, Klaus-Robert M\u00a8uller, Matthias Scholz, and Gunnar R\u00a8atsch. Kernel PCA and de-noising in feature spaces. In Advances in neural information processing systems, pages 536-542, 1999.  Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of  computation, 35(151):773-782, 1980.  Jeremy Oakley. Bayesian uncertainty analysis for complex computer codes. PhD thesis,  University of She\ufb03eld, 1999.  Antony M Overstall and David C Woods. Multivariate emulation of computer simulators: model selection and diagnostics with application to a humanitarian relief model. Journal of the Royal Statistical Society: Series C (Applied Statistics), 65(4):483-505, 2016. Generalized probabilistic principal component analysis  Rui Paulo, Gonzalo Garc\u00b4\u0131a-Donato, and Jes\u00b4us Palomo. Calibration of computer models with multivariate output. Computational Statistics and Data Analysis, 56(12):3959-3974, 2012.  Carl Edward Rasmussen. Gaussian Processes for Machine Learning. MIT Press, 2006.  Youcef Saad. Numerical Methods for Large Eigenvalue Problems. Manchester University  Press, 1992.  Jerome Sacks, William J Welch, Toby J Mitchell, and Henry P Wynn. Design and analysis  of computer experiments. Statistical Science, 4(4):409-423, 1989.  Bernhard Sch\u00a8olkopf, Alexander Smola, and Klaus-Robert M\u00a8uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299-1319, 1998.  Matthias Seeger, Yee-Whye Teh, and Michael Jordan. Semiparametric latent factor models.  Technical report, 2005.  Samuel S.P. Shen. R programming for climate data analysis and visualization: computing and plotting for NOAA data applications. San Diego State University, San Diego, USA., 2017.  B Taylor and A Lane. Development of a novel family of military campaign simulation  models. Journal of the Operational Research Society, 55(4):333-339, 2004.  Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611- 622, 1999.  Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality con-  straints. Mathematical Programming, 142(1-2):397-434, 2013.  M. West. Bayesian factor regression models in the \u201clarge p, small n\u201d paradigm. In J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. David, D. Heckerman, A. F. M. Smith, and M. West, editors, Bayesian Statistics 7, pages 723-732. Oxford University Press, 2003. URL http://ftp.isds.duke.edu/WorkingPapers/02-12.html. "}, "On lp-Support Vector Machines and Multidimensional Kernels": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-601.html", "header": "On lp-Support Vector Machines and Multidimensional Kernels", "author": "Victor Blanco, Justo Puerto, Antonio M. Rodriguez-Chia", "time": "21(14):1\u221229, 2020.", "abstract": "In this paper, we extend the  methodology developed for Support Vector Machines (SVM) using the $\\ell_2$-norm  ($\\ell_2$-SVM)  to the more general case of  $\\ell_p$-norms with $p>1$ ($\\ell_p$-SVM).  We derive second order cone formulations for the resulting dual and primal problems.  The concept of kernel function, widely applied in $\\ell_2$-SVM,  is extended to the more general case of $\\ell_p$-norms with $p>1$ by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of $\\ell_p$-SVM with $p>1$.", "pdf_url": "http://jmlr.org/papers/volume21/18-601/18-601.pdf", "keywords": ["Support Vector Machines", "Kernel functions", "(cid:96)p-norms", "Mathematical Opti mization."], "reference": "J. Alcal\u00b4a-Fdez, A. Fernandez, J. Luengo, J. Derrac, S. Garc\u00b4\u0131a, L. S\u00b4anchez, F. Herrera. KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework. Journal of Multiple-Valued Logic and Soft Computing 17:2-3 (2011) 255-287.  C. Bahlmann, B. Haasdonk, and H. Burkhardt (2002). On-Line Handwriting Recognition with Support Vector Machines: A Kernel Approach. In Proc. of the 8th Int. Workshop on Frontiers in Handwriting Recognition.  26   Blanco, Puerto and Rodr\u00b4\u0131guez-Ch\u00b4\u0131a  (cid:96)1 (\u03a6)  (cid:96)1((cid:101)\u03a6)  \u03b7 ACCTr ACCTest Time %NonZ ACCTr ACCTest Time %NonZ cleveland dataset 85.08% 83.83% 0.01 89.23% 84.78% 83.16% 0.01 85.38% 95.12% 80.24% 0.04 57.43% 89.25% 85.15% 0.05 32.38% 99.49% 74.59% 0.68 15.21% 92.59% 79.57% 0.57 10.68% 0.57% 99.49% 74.59% 4.93  3.58% 84.31% 82.16% 3.46  1 2 3 4  housing dataset 88.45% 83.17% 0.01 99.23% 88.45% 83.17% 0.01 99.23% 96.11% 79.22% 0.10 52.29% 91.59% 83.53% 0.09 29.14% 9.43% 99.54% 79.80% 1.03 13.66% 95.43% 82.36% 1.05 1.19% 3.21% 92.47% 80.81% 8.96 99.54% 79.80% 8.43  1 2 3 4  german credit dataset 78.53% 76.20% 0.13 99.58% 78.58% 76.90% 0.09 93.75% 92.99% 68.30% 2.41 81.26% 72.58% 70.30% 1.74 8.22%. 5.05%  1 2 3 100.00% 69.70% 36.76 12.75% 85.17% 75.30% 24.93  1 100.00% 85.48% 0.35  colon dataset 1.63% 100.00% 85.48% 0.36 page blocks 94.78% 93.60% 0.19 77.00% 94.73% 94.19% 0.21 77.78% 96.61% 95.97% 0.77 35.50% 94.26% 95.13% 1.06 24.05% 97.34% 95.85% 5.26 17.62% 95.82% 95.51% 8.90 11.42% 2.59% 97.34% 95.85% 28.05  5.03% 94.41% 94.63% 25.31  1.63%  1 2 3 4  Table 5: Average Results for the (cid:96)1-norm.  Acknowledgements  The first and second authors were partially supported by the project MTM2016-74983-C2-1- R (MINECO, Spain). The third author was partially supported by the project MTM2016- 74983-C2-2-R (MINECO, Spain). The first author was also partially supported by the research project PP2016-PIP06 (Universidad de Granada) and the research group SEJ-534 (Junta de Andaluc\u00b4\u0131a).  References  J. Alcal\u00b4a-Fdez, A. Fernandez, J. Luengo, J. Derrac, S. Garc\u00b4\u0131a, L. S\u00b4anchez, F. Herrera. KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework. Journal of Multiple-Valued Logic and Soft Computing 17:2-3 (2011) 255-287.  C. Bahlmann, B. Haasdonk, and H. Burkhardt (2002). On-Line Handwriting Recognition with Support Vector Machines: A Kernel Approach. In Proc. of the 8th Int. Workshop on Frontiers in Handwriting Recognition. (cid:96)p Support Vector Machines  K.P. Bennett and E.J. Bredensteiner (2000). Duality and Geometry in SVM Classifiers.  ICML 2000: 57-64  D. P. Bertsekas (1995). Nonlinear programming. Belmont, MA: Athena Scientific.  D. Bertsimas and R. Shioda (2007). Classification and Regression via Integer Optimization.  Operations Research 55(2), 252-271.  J. Bi,K. Bennett, M. Embrechts, C. Breneman, and M. Song (2003). Dimensionality reduc- tion via sparse support vector machines. Journal of Machine Learning Research, 3(Mar), 1229-1243.  V. Blanco, J. Puerto, and S. El Haj Ben Ali, Revisiting several problems and algorithms in continuous location with (cid:96)\u03c4 -norms, Computational Optimization and Applications 58 (2014), no. 3, 563-595.  V. Blanco, J. Puerto and R. Salmer\u00b4on (2018). Locating hyperplanes to fitting set of points:  A general framework, Computers & Operations Research, 95, 172-193.  J. Bolte, S. Sabach and M. Teboulle (2014). Proximal alternating linearized minimization  for nonconvex and nonsmooth problems, Math. Program. 146, 459-494.  J. P. Brooks (2011). Support Vector Machines with the Ramp Loss and the Hard Margin  Loss. Operations Research 59(2), 467-479.  Ch.J. Burges (1998). A Tutorial on Support Vector Machines for Pattern Recognition. Data  Min. Knowl. Discov. 2(2), 121-167.  E. Carrizosa and D. Romero-Morales (2013). Supervised classification and mathematical  optimization. Computers & Operations Research, 40(1), 150-165.  E. Carrizosa, A. Nogales-G\u00b4omez, D. Romero-Morales (2017). Clustering categories in sup-  port vector machines. Omega, 66, 28-37.  J. D. Carroll and J. J. Chang (1970). Analysis of individual di\ufb00erences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition, Psychometrika 35 , 283-319.  C.C. Chang and C.J. Lin (2011). LIBSVM - A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology 2(3), 1-27. Available at https:// www.csie.ntu.edu.tw/~cjlin/libsvm/  H. Chen and L. Qi (2015). Positive definiteness and semi-definiteness of even order sym- metric Cauchy tensors. Journal of Industrial and Management Optimization 11(4), 1263- 1274.  H. Chen, G. Li and L. Qi (2016). SOS tensor decomposition: Theory and applications.  Communications in Mathematical Sciences 14 (8), 2073-2100. Blanco, Puerto and Rodr\u00b4\u0131guez-Ch\u00b4\u0131a  P. Comon, G. Golub, L-H. Lim, Lek-Heng and B. Mourrain (2008). Symmetric tensors and symmetric tensor rank. SIAM Journal on Matrix Analysis and Applications 30(3), 1254-1279  C. Cortes and V. Vapnik (1995). Support-Vector Networks. Mach. Learn. 20(3), 273-297.  C. Eckart and G. Young (1939). A principal axis transformation for non-Hermitian matrices.  Bull. Amer. Math. Soc. 4. 118-121.  H. Edelsbrunner (1987). Algorithms in combinatorial geometry. Springer-Verlag, Berlin.  M. Gaudioso, E. Gorgone, M. Labb\u00b4e, and A.M. Rodr\u00b4\u0131guez-Ch\u00b4\u0131a (2017). Lagrangian relax-  ation for SVM feature selection, Computers & Operations Research, 87, 137-145.  L. Gonz\u00b4alez-Abril, F. Velasco, J.A. Ortega, and L. Franco (2011). Support vector machines for classification of input vectors with di\ufb00erent metrics, Computers & Mathematics with Applications 61(9), 2874-2878.  T. Harris (2013). Quantitative credit risk assessment using support vector machines: Broad  versus Narrow default definitions. Expert Syst. Appl. 40(11), 4404-4413.  C. J. Hillar and L.-H. Lim (2013). Most tensor problems are NP-hard. Journal of the ACM  60, 1-39.  J. Jiang, H. Wu, Y. Li, and R. Yu (2000). Three-way data resolution by alternating slice-wise  diagonalization (ASD) method. Journal of Chemometrics 14, 15-36.  V. Kascelan, L. Kascelan, and M. Novovic Buric (2016). A nonparametric data mining approach for risk prediction in car insurance: a case study from the Montenegrin market. Economic Research-Ekonomska Istrazivanja 29(1), 545-558.  E. Kofidis and P. A. Regalia (2002). On the best rank-1 approximation of higher-order supersymmetric tensors. SIAM J. on Matrix Analysis and Applications 23, 863-884.  K. Ikeda and N. Murata (2005). Geometrical Properties of Nu Support Vector Machines  with Di\ufb00erent Norms. Neural Computation 17(11), 2508-2529.  K. Ikeda and N. Murata (2005). E\ufb00ects of norms on learning properties of support vector  machines. ICASSP (5), 241-244  J.B Lasserre (2009). Moments, Positive Polynomials and Their Applications, Imperial Col-  lege Press, London.  J. Lindenstrauss and L. Tzafriri (1977). Classical Banach Spaces I, Sequence Spaces. Ergeb-  nisse der Mathematik und ihrer Grenzgebiete 92, Berlin: Springer-Verlag.  Y. Liu, H.H. Zhang, C. Park, and J. Ahn (2007). Support vector machines with adaptive Lq  penalty. Comput. Stat. Data Anal. 51(12), 6380-6394.  A. Majid, S. Ali, M. Iqbal, and N. Kausar (2014). Prediction of human breast and colon cancers from imbalanced data using nearest neighbor and support vector machines. Comp. Meth. & Progr.in Biomedicine 113(3), 792-808. (cid:96)p Support Vector Machines  O.L. Mangasarian (1999). Arbitrary-norm separating plane. Oper. Res. Lett., 24 (1- 2):15-  23.  Mangasarian, O. L. (2006). Exact 1-norm support vector machines via unconstrained convex  di\ufb00erentiable minimization. Journal of Machine Learning Research 7, 1517-1530.  J. Mercer (1909). Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society A, 209, 415-446.  J.P. Pedroso and N. Murata (2001). Support vector machines with di\ufb00erent norms: moti-  vation, formulations and results. Pattern Recognition Letters 22(12), 1263-1272.  L. Qi and Y. Song (2014). An even order symmetric B tensor is positive definite. Linear  Algebra and its Applications 457, 303-312.  S. Radhimeenakshi (2016). Classification and prediction of heart disease risk using data mining techniques of Support Vector Machine and Artificial Neural Network. 3rd Int. Conf. on Computing for Sustainable Global Development, 3107-3111.  V.N. Vapnik (1995). The Nature of Statistical Learning Theory. Springer-Verlag New York.  V.N. Vapnik (1998). Statistical Learning Theory. Wiley-Interscience.  H. Xu, C. Caramanis, and S. Mannor (2009). Robustness and regularization of support  vector machines. Journal of Machine Learning Research, 10(Jul), 1485-1510.  J. Zhu, S. Rosset, R. Tibshirani and T.J. Hastie (2004). 1\u2212norm support vector machines,  In Advances in neural information processing systems (pp. 49-56). "}, "Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-720.html", "header": "Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning", "author": "Ery Arias-Castro, Adel Javanmard, Bruno Pelletier", "time": "21(15):1\u221237, 2020.", "abstract": "One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data.  Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space.  Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding.  A new perturbation bound for procrustes analysis plays a key role.", "pdf_url": "http://jmlr.org/papers/volume21/18-720/18-720.pdf", "keywords": null, "reference": "Ery Arias-Castro and Thibaut Le Gouic. Unconstrained and curvature-constrained shortest-  path distances and their approximation. arXiv preprint arXiv:1706.09441, 2017.  Ery Arias-Castro and Bruno Pelletier. On the convergence of maximum variance unfolding.  The Journal of Machine Learning Research, 14(1):1747-1770, 2013.  Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods. Journal of Computer and System Sciences, 74(8):1289-1308, 2008.  M. Bernstein, V. De Silva, J.C. Langford, and J.B. Tenenbaum. Graph approximations to geodesics on embedded manifolds. Technical report, Technical report, Department of Psychology, Stanford University, 2000.  Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.  Pratik Biswas, Tzu-Chen Liang, Kim-Chuan Toh, Yinyu Ye, and Ta-Chung Wang. Semidef- inite programming approaches for sensor network localization with noisy distance mea- surements. Transactions on Automation Science and Engineering, 3(4):360-371, 2006.  R.R. Coifman and S. Lafon. Di\ufb00usion maps. Applied and Computational Harmonic Analysis,  21(1):5-30, 2006.  V. de Silva and J.B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. Advances in Neural Information Processing Systems (NIPS), 15:705-712, 2003.  Vin de Silva and Joshua B Tenenbaum. Sparse multidimensional scaling using landmark  points. Technical report, Technical report, Stanford University, 2004.  David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding tech- niques for high-dimensional data. Proceedings of the National Academy of Sciences, 100 (10):5591-5596, 2003.  Christos Faloutsos and King-Ip Lin. Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. In ACM SIGMOD International Conference on Management of Data, volume 24, pages 163-174, 1995.  Herbert Federer. Curvature measures. Transactions of the American Mathematical Society,  93:418-491, 1959.  Christopher R Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Manifold estimation and singular deconvolution under hausdor\ufb00 loss. The Annals of Statistics, 40(2):941-963, 2012.  Evarist Gin\u00b4e and Vladimir Koltchinskii. Empirical graph Laplacian approximation of laplace-beltrami operators: Large sample results. In High dimensional probability, pages 238-259. Institute of Mathematical Statistics, 2006.  Yair Goldberg, Alon Zakai, Dan Kushnir, and Ya\u2019acov Ritov. Manifold learning: The price  of normalization. Journal of Machine Learning Research, 9(Aug):1909-1939, 2008.  35   Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration  References  Ery Arias-Castro and Thibaut Le Gouic. Unconstrained and curvature-constrained shortest-  path distances and their approximation. arXiv preprint arXiv:1706.09441, 2017.  Ery Arias-Castro and Bruno Pelletier. On the convergence of maximum variance unfolding.  The Journal of Machine Learning Research, 14(1):1747-1770, 2013.  Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods. Journal of Computer and System Sciences, 74(8):1289-1308, 2008.  M. Bernstein, V. De Silva, J.C. Langford, and J.B. Tenenbaum. Graph approximations to geodesics on embedded manifolds. Technical report, Technical report, Department of Psychology, Stanford University, 2000.  Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.  Pratik Biswas, Tzu-Chen Liang, Kim-Chuan Toh, Yinyu Ye, and Ta-Chung Wang. Semidef- inite programming approaches for sensor network localization with noisy distance mea- surements. Transactions on Automation Science and Engineering, 3(4):360-371, 2006.  R.R. Coifman and S. Lafon. Di\ufb00usion maps. Applied and Computational Harmonic Analysis,  21(1):5-30, 2006.  V. de Silva and J.B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. Advances in Neural Information Processing Systems (NIPS), 15:705-712, 2003.  Vin de Silva and Joshua B Tenenbaum. Sparse multidimensional scaling using landmark  points. Technical report, Technical report, Stanford University, 2004.  David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding tech- niques for high-dimensional data. Proceedings of the National Academy of Sciences, 100 (10):5591-5596, 2003.  Christos Faloutsos and King-Ip Lin. Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. In ACM SIGMOD International Conference on Management of Data, volume 24, pages 163-174, 1995.  Herbert Federer. Curvature measures. Transactions of the American Mathematical Society,  93:418-491, 1959.  Christopher R Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Manifold estimation and singular deconvolution under hausdor\ufb00 loss. The Annals of Statistics, 40(2):941-963, 2012.  Evarist Gin\u00b4e and Vladimir Koltchinskii. Empirical graph Laplacian approximation of laplace-beltrami operators: Large sample results. In High dimensional probability, pages 238-259. Institute of Mathematical Statistics, 2006.  Yair Goldberg, Alon Zakai, Dan Kushnir, and Ya\u2019acov Ritov. Manifold learning: The price  of normalization. Journal of Machine Learning Research, 9(Aug):1909-1939, 2008. Castro, Javanmard and Pelletier  John C Gower. Some distance properties of latent root and vector methods used in multi-  variate analysis. Biometrika, 53(3-4):325-338, 1966.  Matthias Hein, Jean-Yves Audibert, and Ulrike Von Luxburg. From graphs to manifolds: Weak and strong pointwise consistency of graph Laplacians. In Conference on Compu- tational Learning Theory (COLT), pages 470-485. Springer, 2005.  John T Holodnak and Ilse CF Ipsen. Randomized approximation of the gram matrix: Exact computation and probabilistic bounds. SIAM Journal on Matrix Analysis and Applications, 36(1):110-137, 2015.  Ilse CF Ipsen and Thomas Wentworth. The e\ufb00ect of coherence on sampling from matrices with orthonormal columns, and preconditioned least squares problems. SIAM Journal on Matrix Analysis and Applications, 35(4):1490-1520, 2014.  Adel Javanmard and Andrea Montanari. Localization from incomplete noisy distance mea-  surements. Foundations of Computational Mathematics, 13(3):297-345, 2013.  Arlene KH Kim and Harrison H Zhou. Tight minimax rates for manifold estimation under  hausdor\ufb00 loss. Electronic Journal of Statistics, 9(1):1562-1582, 2015.  Joseph B Kruskal and Judith B Seery. Designing network diagrams. In General Conference  on Social Graphics, pages 22-50, 1980.  Drago\u00b8s Niculescu and Badri Nath. DV based positioning in ad hoc networks. Telecommu-  nication Systems, 22(1-4):267-280, 2003.  Alexander Paprotny and Jochen Garcke. On a connection between maximum variance unfolding, shortest path problems and isomap. In Conference on Artificial Intelligence and Statistics (AISTATS), pages 859-867, 2012.  John Platt. Fastmap, MetricMap, and Landmark MDS are all Nystrom algorithms.  In  Conference on Artificial Intelligence and Statistics (AISTATS), 2005.  George AF Seber. Multivariate observations. John Wiley & Sons, 2004.  Yi Shang, Wheeler Ruml, Ying Zhang, and Markus PJ Fromherz. Localization from mere connectivity. In Symposium on Mobile Ad Hoc Networking & Computing, pages 201-212, 2003.  Robin Sibson. Studies in the robustness of multidimensional scaling: Perturbational analysis of classical scaling. Journal of the Royal Statistical Society. Series B (Methodological), pages 217-229, 1979.  A. Singer. From graph to manifold Laplacian: The convergence rate. Applied and Compu-  tational Harmonic Analysis, 21(1):128-134, 2006.  A.K. Smith, X. Huo, and H. Zha. Convergence and rate of convergence of a manifold-based dimension reduction algorithm. In Advances in Neural Information Processing Systems (NIPS), pages 1529-1536, 2008. Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration  Anthony Man-Cho So and Yinyu Ye. Theory of semidefinite programming for sensor network  localization. Mathematical Programming, 109(2-3):367-384, 2007.  Inge S\u00a8oderkvist. Perturbation analysis of the orthogonal procrustes problem. BIT Numerical  Mathematics, 33(4):687-694, 1993.  G. W. Stewart and Ji Guang Sun. Matrix perturbation theory. Computer Science and Scientific Computing. Academic Press Inc., Boston, MA, 1990. ISBN 0-12-670230-6.  J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for  nonlinear dimensionality reduction. Science, 290(5500):2319-2323, 2000.  Warren S Torgerson. Theory and methods of scaling. Wiley, 1958.  Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of  Computational Mathematics, 12(4):389-434, 2012.  U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of spectral clustering. The  Annals of Statistics, 36(2):555-586, 2008.  Jason Tsong-Li Wang, Xiong Wang, King-Ip Lin, Dennis Shasha, Bruce A Shapiro, and Kaizhong Zhang. Evaluating a class of distance-mapping algorithms for data mining and clustering. In Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 307-311, 1999.  GA Watson. The solution of orthogonal procrustes problems for a family of orthogonally  invariant norms. Advances in Computational Mathematics, 2(4):393-405, 1994.  Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semidefinite programming. International Journal of Computer Vision, 70(1):77-90, 2006a.  Kilian Q Weinberger, Fei Sha, Qihui Zhu, and Lawrence K Saul. Graph Laplacian regu- larization for large-scale semidefinite programming. In Advances in Neural Information Processing Systems (NIPS), pages 1489-1496, 2006.  Killan Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimensional- ity reduction by maximum variance unfolding. In Conference on Artificial Intelligence, volume 2, pages 1683-1686. AAAI, 2006b.  Qiang Ye and Weifeng Zhi. Discrete hessian eigenmaps method for dimensionality reduction.  Journal of Computational and Applied Mathematics, 278:197-212, 2015.  Forrest W Young. Multidimensional scaling: History, theory, and applications. Psychology  Press, 2013.  H. Zha and Z. Zhang. Continuum isomap for manifold learnings. Computational Statistics  & Data Analysis, 52(1):184-200, 2007.  H. Zha and Z. Zhang. Spectral properties of the alignment matrices in manifold learning.  SIAM Review, 51(3):545, 2009. "}, "Practical Locally Private Heavy Hitters": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-786.html", "header": "Practical Locally Private Heavy Hitters", "author": "Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Thakurta", "time": "21(16):1\u221242, 2020.", "abstract": "We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is $\\tilde O(n)$ and user running time is $\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $O(n^{5/2})$ server time and $O(n^{3/2})$ user time. With a typically large number of participants in local algorithms (in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.", "pdf_url": "http://jmlr.org/papers/volume21/18-786/18-786.pdf", "keywords": ["Di\ufb00erential privacy", "local di\ufb00erential privacy", "heavy hitters", "histograms", "sketch ing."], "reference": "Nltk brown corpus. www.nltk.org.  Apple tries to peek at user habits without violating privacy. The Wall Street Journal, 2016.  Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992. ISBN 0-471-  53588-5.  Raef Bassily and Adam Smith. Local, private, e\ufb03cient protocols for succinct histograms. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 127-135. ACM, 2015.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate di\ufb00erential privacy. Theory of Computing, 12(1):1-61, 2016. doi: 10.4086/ toc.2016.v012a001. URL http://dx.doi.org/10.4086/toc.2016.v012a001.  M. Bellare and J. Rompel. Randomness-e\ufb03cient oblivious sampling. In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, SFCS \u201994, pages 276- 287, Washington, DC, USA, 1994. IEEE Computer Society. ISBN 0-8186-6580-7. doi: 10.1109/SFCS.1994.365687. URL http://dx.doi.org/10.1109/SFCS.1994.365687.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Di\ufb00erentially private release and learning of threshold functions. In Venkatesan Guruswami, editor, IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20  37   Practical Locally Private Heavy Hitters  1a. For every (r, (cid:96)) \u2208 [R]\u00d7[log d(cid:48)], invoke Hashtogram(Sr,(cid:96)) with (cid:15) 2 .  1b. For every (r, t) \u2208 [R]\u00d7[T ], for every (cid:96) \u2208 [log d(cid:48)], query Hashtogram(Sr,(cid:96)) to get  (cid:8)ar,(cid:96)(t, b) : b{0, 1}(cid:9).  Now, every one of the steps 1b,2,3,4 of the algorithm Bitstogram contains a loop over every choice of (r, t) \u2208 [R]\u00d7[T ], and we can group all of this steps together into one loop over (r, t) \u2208 [R]\u00d7[T ]. If an iteration of this loop results in a value a(\u02c6vr,t) \u2264 n, we can simply ignore it (recall that the frequencies of domain elements that are not in the list L n is less than the guaranteed bound on the error of the are estimated as zero, and that protocol, so this step does not e\ufb00ect our error bounds). As there could be at most n \u221a elements with frequencies at least \u2248  n, the necessary processing memory is only \u2248  n.  \u221a  \u221a  \u221a  \u221a  Acknowledgments  R. B.\u2019s research is supported by NSF Awards AF-1908281, SHF-1907715, Google Faculty Research Award, and OSU faculty start-up support. Work of K. N. was supported by NSF grant No. 1565387, TWC: Large: Collaborative: Computing Over Distributed Sensi- tive Data. Work of U. S. was supported in part by the Israel Science Foundation (grant No. 1871/19). A. T.\u2019s research is supported by NSF Awards TRIPODS+X-1839317, AF- 1908281, TRIPODS-1740850, and Google Faculty Research Award.  References  Nltk brown corpus. www.nltk.org.  Apple tries to peek at user habits without violating privacy. The Wall Street Journal, 2016.  Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992. ISBN 0-471-  53588-5.  Raef Bassily and Adam Smith. Local, private, e\ufb03cient protocols for succinct histograms. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 127-135. ACM, 2015.  Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate di\ufb00erential privacy. Theory of Computing, 12(1):1-61, 2016. doi: 10.4086/ toc.2016.v012a001. URL http://dx.doi.org/10.4086/toc.2016.v012a001.  M. Bellare and J. Rompel. Randomness-e\ufb03cient oblivious sampling. In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, SFCS \u201994, pages 276- 287, Washington, DC, USA, 1994. IEEE Computer Society. ISBN 0-8186-6580-7. doi: 10.1109/SFCS.1994.365687. URL http://dx.doi.org/10.1109/SFCS.1994.365687.  Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Di\ufb00erentially private release and learning of threshold functions. In Venkatesan Guruswami, editor, IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 Bassily, Nissim, Stemmer, Thakurta  October, 2015, pages 634-649. IEEE Computer Society, 2015. ISBN 978-1-4673-8191-8. doi: 10.1109/FOCS.2015.45. URL http://dx.doi.org/10.1109/FOCS.2015.45.  Mark Bun, Jelani Nelson, and Uri Stemmer. Heavy hitters and the structure of local privacy. In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, Houston, TX, USA, June 10-15, 2018, pages 435-447, 2018. doi: 10.1145/3196959.3196981. URL https://doi.org/10.1145/3196959.3196981.  T.-H. Hubert Chan, Elaine Shi, and Dawn Song. Optimal lower bound for di\ufb00erentially private multi-party aggregation. In Leah Epstein and Paolo Ferragina, editors, Algorithms - ESA 2012 - 20th Annual European Symposium, Ljubljana, Slovenia, September 10- 12, 2012. Proceedings, volume 7501 of Lecture Notes in Computer Science, pages 277- 288. Springer, 2012. ISBN 978-3-642-33089-6. doi: 10.1007/978-3-642-33090-2 25. URL http://dx.doi.org/10.1007/978-3-642-33090-2_25.  Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data  streams. In ICALP, 2002.  Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265- 284. Springer, 2006.  \u00b4Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable  privacy-preserving ordinal response. In CCS, 2014.  Alexandre Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in privacy preserving data mining. In PODS, pages 211-222. ACM, 2003. ISBN 1-58113-670-6.  Giulia Fanti, Vasyl Pihur, and Ulfar Erlingsson. Building a rappor with the un- known: Privacy-preserving learning of associations and data dictionaries. arXiv preprint arXiv:1503.01214, 2015.  Venkatesan Guruswami. List Decoding of Error-Correcting Codes. PhD thesis, Mas-  sachusetts Institute of Technology, 2001. Supervisor-Madhu Sudan.  Justin Hsu, Sanjeev Khanna, and Aaron Roth. Distributed private heavy hitters. In Interna- tional Colloquium on Automata, Languages, and Programming, pages 461-472. Springer, 2012.  Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793- 826, 2011.  Nina Mishra and Mark Sandler. Privacy via pseudorandom sketches.  In Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 143-152. ACM, 2006. Practical Locally Private Heavy Hitters  Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York, NY, USA, 2005. ISBN 0521835402.  A.G. Thakurta, A.H. Vyrros, U.S. Vaishampayan, G. Kapoor, J. Freudiger, V.R. Sridhar,  and D. Davidson. Learning new words. US Patent 9594741, 2017.  Salil P. Vadhan. Pseudorandomness. Foundations and Trends in Theoretical Computer ISSN 1551-305X. doi: 10.1561/0400000010. URL http:  Science, 7(13):1-336, 2012. //dx.doi.org/10.1561/0400000010.  A. Missing Proofs from Section 5  A.1. Proof of Theorem 5.1  It is easy to see that each invocation of LocalRnd is (cid:15)/2-locally di\ufb00erentially private since conditioned on any realization of (cid:96)i, ji, ri, for any pair of possible input items vi, v(cid:48) i \u2208 V to LocalRnd and any output bit b generated in step 7 of Algorithm 1, we have  P [yi = b | vi] \u2264 e(cid:15)/2 P (cid:2)yi = b | v(cid:48)  (cid:3)  i  Note that LocalRnd is invoked only twice for each user: once when Final = 0 (the scanning/pruning phase of TreeHist) and another time when Final = 1 (the final phase of TreeHist). Thus, it follows that protocol TreeHist is (cid:15)-di\ufb00erentially private.  A.2. Proof of Theorem 5.2  Let \u03b2 \u2208 (0, 1) and \u03b7 as defined in the theorem statement. Consider the pruning phase of TreeHist, that is, Steps 1 to 11 in Algorithm 3. Let \u03b3 be as set in Step 2. In this phase, TreeHist invokes FreqOracle once in every iteration of the outer for loop (over the levels of the tree) with the \ufb02ag Final = 0. Consider any such iteration (cid:96). Suppose, for now, that the size of ChildSet(Prefixes) passed to FreqOracle in that iteration is at most 2n/\u03b7. (We will show that with probability at least 1 \u2212 \u03b2 this condition is satisfied at all levels (cid:96) of the tree, i.e., it\u2019s a loop invariant). By invoking Lemma 5.3 with (cid:98)V = ChildSet(Prefixes), (cid:8)\u02dcIj : j \u2208 [t](cid:9) = (cid:8)I(cid:96),j : j \u2208 [t](cid:9), and \u03b3 = t log d = 110 log(n/\u03b2) log d, we have that with probability at least 1 \u2212 \u03b2/ log(d), for every \u02c6v \u2208 ChildSet(Prefixes) : f (\u02c6v) > 3\u03b7, FreqOracle gives an estimate \u02c6f (\u02c6v) \u2265 2\u03b7, and for every \u02c6v \u2208 ChildSet(Prefixes) : f (\u02c6v) \u2264 \u03b7, FreqOracle gives an estimate \u02c6f (\u02c6v) < 2\u03b7. Hence, Step 9 implies, that with probability at least 1\u2212\u03b2/ log d, all \u02c6v \u2208 ChildSet(Prefixes) with true frequencies f (\u02c6v) \u2265 3\u03b7 will proceed to the next iteration (cid:96) + 1 and all those \u02c6v \u2208 ChildSet(Prefixes) with true frequencies f (\u02c6v) < \u03b7 will be pruned out. Since the number of nodes \u02c6v with true frequency f (\u02c6v) \u2265 \u03b7 cannot be more than n/\u03b7, then the number of surviving nodes in the next iteration (cid:96) + 1 cannot be more than 2n/\u03b7. Hence, this condition will be satisfied in the next iteration, and we can proceed in the same fashion. Note that when (cid:96) = 1, the condition is trivially satisfied since there are only 2 < 2n/\u03b7 nodes at that level. This induction argument shows that with probability at least 1 \u2212 \u03b2, for every level (cid:96) \u2208 [log d], the surviving nodes at level (cid:96) correspond to prefixes whose true frequencies are not below \u03b7 and include all prefixes whose true frequencies are above 3\u03b7. In particular, Bassily, Nissim, Stemmer, Thakurta  with probability at least 1 \u2212 \u03b2, all items in SuccHist satisfy these properties. This covers the proof of items 1 and 2 of Theorem 5.2.  Now, consider the final phase of TreeHist, that is, Steps 12 to 14 Algorithm 3. Let \u03b3 be as set in Step 12. In this phase, TreeHist invokes FreqOracle on the surviving nodes at the final level of the tree (the last update of Prefixes) and with input \ufb02ag Final = 1. Now, by invoking Lemma 5.3 with (cid:98)V = Prefixes, (cid:8)\u02dcIj : j \u2208 [t](cid:9) = (cid:8)Ij : j \u2208 [t](cid:9), and \u03b3 = t = 110 log(n/\u03b2), we have that with probability at least 1 \u2212 \u03b2/ log(d), for every \u02c6v \u2208 Prefixes, | \u02c6f (\u02c6v) \u2212 f (\u02c6v)| \u2264 14  . This proves item 3 of the  nt/(cid:15) = O  n log(n/\u03b2)  (cid:18) \u221a  \u221a  (cid:19)  (cid:15)  theorem.  B. Missing Proofs from Section 6  B.1. Proof of Lemma 6.5  Consider the following good event:  Event E1 (over sampling h1, \u00b7 \u00b7 \u00b7, hR): For every query v\u2217 \u2208 V there exists a subset Rv\u2217 r\u2217 \u2208 Rv\u2217 1  1 \u2286 [R] of size |Rv\u2217 it holds that |{v \u2208 S : v (cid:54)= v\u2217 and hr\u2217(v) = hr\u2217(v\u2217)}| \u2264 16n T .  1 | \u2265 7  8 R s.t. for every  Event E1 states that for at least 7R/8 of the hash functions, we have that v\u2217 is mapped into a cell without too many collisions with di\ufb00erent input elements. Informally, for every single hash function hr, algorithm HashHist estimates the number of occurrences of hr(v\u2217) in S. Hence, if event E1 occurs, then most of the estimations result in accurate answers. We start by showing that event E1 happens with high probability. To that end, fix v\u2217 \u2208 V and fix r\u2217 \u2208 [R]. We have that  Ehr\u2217 [| {x \u2208 S : v (cid:54)= v\u2217 and hr\u2217(v) = hr\u2217(v\u2217)} |] =  (cid:88)  Ehr\u2217  (cid:2)1hr\u2217 (v)=hr\u2217 (v\u2217)  (cid:3) \u2264  n T  .  v\u2208S:v(cid:54)=v\u2217  Thus, by Markov\u2019s inequality, we have that  (cid:20)  Pr hr\u2217  | {v \u2208 S : v (cid:54)= v\u2217 and hr\u2217(v) = hr\u2217(v\u2217)} | \u2265  (cid:21)  16n T  \u2264  1 16  .  As the hash functions are independent from each other, for R \u2265 48 ln( d(cid:48) \u03b2 ), by the Cherno\ufb00 bound we get that with probability at least 1 \u2212 \u03b2/d(cid:48) (over sampling h1, . . . , hR) there exists a subset Rv\u2217  8 R s.t. for every r\u2217 \u2208 Rv\u2217  1 \u2286 [R] of size |Rv\u2217  it holds that  1 | \u2265 7|{v \u2208 S : v (cid:54)= v\u2217 and hr\u2217(v) = hr\u2217(v\u2217)}| \u2264  16n T  .  Using the union bound, we have that event E1 happens with probability at least 1 \u2212 \u03b2. We continue the analysis assuming that event E1 occurs. Practical Locally Private Heavy Hitters  For every r \u2208 [R], let Sr = (vj)j\u2208Ir denote a database containing the data of all users j s.t. j \u2208 Ir. Also for v\u2217 \u2208 V and r \u2208 [R] denote |Sr,v\u2217| (cid:44) |{v \u2208 S : hr(v) = hr(v\u2217)}|. That is, |Sr,v\u2217| is the number of users j s.t. hr(vj) = hr(v\u2217). Furthermore, for v\u2217 \u2208 V and r \u2208 [R] denote |I v\u2217 r | (cid:44) |{v \u2208 Sr : hr(v) = hr(v\u2217)}|. That is, |I v\u2217 r | is the number of users j s.t. j \u2208 Ir and hr(vj) = hr(v\u2217). Observe that |Sr,v\u2217| \u2265 fS(v\u2217) and that |I v\u2217  r | \u2265 fSr (v\u2217).  Fix v\u2217 \u2208 V . By the Cherno\ufb00 bound, with probability at least 1 \u2212 \u03b2/d(cid:48) (over partitioning  [n] into subsets I1, . . . , IR), for every r \u2208 [R] we have that  (cid:12) (cid:12) R \u00b7 |I v\u2217 (cid:12)  r | \u2212 |Sr,v\u2217  |  (cid:12) (cid:12) (cid:12) \u2264  (cid:115)  3R \u00b7 |Sr,v\u2217| \u00b7 log(  2Rd(cid:48) \u03b2  ).  (4)  Using the union bound this holds simultaneously for every v\u2217 \u2208 V and r \u2208 [R] with probability at least 1 \u2212 \u03b2. Also, assuming that n \u2265 8R log(2R/\u03b2), by the Cherno\ufb00 bound, with probability at least 1\u2212\u03b2 (over partitioning [n] into subsets I1, . . . , IR), for every r \u2208 [R] we have that n R . We continue the analysis assuming that this is the case, and that Inequality (4) holds.  2R \u2264 |Ir| \u2264 2n  Event E2 (over sampling Z and the coins of the local randomizers): 2 \u2286 [R] of size |Rv\u2217 For every query v\u2217 \u2208 V there exists a subset Rv\u2217 r\u2217 | (cid:12) (cid:12) \u2264 e(cid:15)+1 (cid:12) R \u00b7 ar\u2217(hr\u2217(v\u2217)) \u2212 R \u00b7 |I v\u2217 r\u2217 \u2208 Rv\u2217 11nR. e(cid:15)\u22121 \u00b7 2  it holds that (cid:12)  2 | \u2265 7  \u221a  8 R s.t. for every  For v\u2217 \u2208 V and r \u2208 [R] denote cr(v\u2217) = (cid:80)  Hashtogram answers the query v\u2217 with a(v\u2217) = R \u00b7 e(cid:15)+1 and r \u2208 [R]. We now analyze the expectation of cr(v\u2217):  j\u2208Ir  yj \u00b7 Z[hr(v\u2217), j], and recall that algorithm e(cid:15)\u22121 \u00b7 Medianr\u2208[R]{cr(v\u2217)}. Fix v\u2217 \u2208 V  E[c(v\u2217)] =  E [yj \u00b7 Z[hr(v\u2217), j]]  (cid:88)  j\u2208Ir  (cid:88)  E [yj \u00b7 Z[hr(v\u2217), j]] +  (cid:88)  E [yj \u00b7 Z[hr(v\u2217), j]]  =  =  j\u2208Ir: hr(vj )=hr(v\u2217) (cid:88)  j\u2208Ir: hr(vj )=hr(v\u2217)  = |{v \u2208 Sr : hr(v) = hr(v\u2217)}| \u00b7  j\u2208Ir: hr(vj )(cid:54)=hr(v\u2217) (cid:88)  j\u2208Ir: hr(vj )(cid:54)=hr(v\u2217) e(cid:15) \u2212 1 e(cid:15) + 1  r | \u00b7  (cid:44) |I v\u2217  e(cid:15) \u2212 1 e(cid:15) + 1  E [yj \u00b7 Z[hr(v\u2217), j]] +  E [yj] \u00b7 E [Z[hr(v\u2217), j]]  That is, cr(v) can be expressed as two sums of \u00b11 independent random variables: |I v\u2217 r | r |) variables with expectation 0 (recall that R ). Using the Hoe\ufb00ding bound, with probability at least 43/44 we have that  e(cid:15)+1 , and (|Ir| \u2212 |I v\u2217  variables with expectation e(cid:15)\u22121 n 2R \u2264 |Ir| \u2264 2n (cid:12) (cid:12) cr(v\u2217) \u2212 e(cid:15)\u22121 (cid:12)  e(cid:15)+1 \u00b7 |I v\u2217 r |  (cid:12) (cid:12) (cid:12) \u2264  e(cid:15) + 1 e(cid:15) \u2212 1  \u221a  \u00b7  11nR.  (5)  (cid:12) (cid:12) \u2264 (cid:112)11n/R. That is, (cid:12) (cid:12) (cid:12) R \u00b7 ar(hr(v\u2217)) \u2212 R \u00b7 |I v\u2217 (cid:12) r | Bassily, Nissim, Stemmer, Thakurta  Fix v\u2217 \u2208 V , and observe that the above sums are independent for di\ufb00erent values of r. Hence, using the Cherno\ufb00 bound and asserting that R \u2265 132 ln(d(cid:48)/\u03b2), for that fixed v\u2217 \u2208 V , with probability at least 1\u2212\u03b2/d(cid:48) we have that Inequality (5) holds for at least 7R/8 choices of r \u2208 [R]. Using the union bound, with probability at least 1 \u2212 \u03b2, this is true for every v\u2217 \u2208 V simultaneously. That is, event E2 happens with probability at least 1 \u2212 \u03b2. We continue the analysis assuming that event E2 occurs. For every v\u2217 \u2208 V we denote Rv\u2217 Combining event E2 with Inequality (4), we get that for every r \u2208 Rv\u2217 2  1 \u2229 Rv\u2217 2 .  3 = Rv\u2217  (cid:12) (cid:12) R \u00b7 ar(hr(v\u2217)) \u2212 |Sr,v\u2217 (cid:12)  |  (cid:12) (cid:12) (cid:12) \u2264  \u221a  e(cid:15) + 1 e(cid:15) \u2212 1  \u00b7  11nR +  3R \u00b7 |Sr,v\u2217| \u00b7 log(  (6)  2Rd(cid:48) \u03b2  ).  (cid:115)  Recall that for every v\u2217 \u2208 V and every r \u2208 [R] we have that |Sr,v\u2217| \u2265 fS(v\u2217). Further- T . Hence, for  1 we have that |Sr,v\u2217| \u2264 fS(v\u2217) + 16n  more, for every v\u2217 \u2208 V and every r \u2208 Rv\u2217 every v\u2217 \u2208 V and every r \u2208 Rv\u2217  3 we have that  | R \u00b7 ar(hr(v\u2217)) \u2212 fS(v\u2217) | \u2264  11nR +  3R \u00b7  fS(v\u2217) +  (cid:115)  (cid:18)  (cid:19)  16n T  \u00b7 log(  ) +  2Rd(cid:48) \u03b2  16n T  \u221a  \u00b7  e(cid:15) + 1 e(cid:15) \u2212 1 (cid:44) error(v\u2217).  That is, for every r \u2208 Rv\u2217 |Rv\u2217 {R \u00b7 ar(hr(v\u2217))}, we get that |a(v\u2217) \u2212 fS(v\u2217)| \u2264 error(v\u2217) for every v\u2217 \u2208 V .  3 we have that R \u00b7 ar(hr(v\u2217)) is accurate up to error(v\u2217). As 4 R, and as algorithm Hashtogram answers v\u2217 with a(v\u2217) chosen as the median of  3 | \u2265 3 "}, "Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-817.html", "header": "Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data", "author": "Aki Vehtari, Andrew Gelman, Tuomas Sivula, Pasi Jyl\u00c3\u00a4nki, Dustin Tran, Swupnil Sahai, Paul Blomstedt, John P. Cunningham, David Schiminovich, Christian P. Robert", "time": "21(17):1\u221253, 2020.", "abstract": "A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.", "pdf_url": "http://jmlr.org/papers/volume21/18-817/18-817.pdf", "keywords": ["Bayesian computation", "data partitioning", "expectation propagation", "hierarchi cal models", "statistical computing"], "reference": "Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning, pages 1591-1598, 2012.  Anoop Korattikara Balan, Yutian Chen, and Max Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 31th International Conference on Machine Learning (ICML), pages 181-189, 2014.  R\u00e9mi Bardenet, Arnaud Doucet, and Chris Holmes. On Markov chain Monte Carlo methods  for tall data. Journal of Machine Learning Research, 18(47):1-43, 2017.  Simon Barthelm\u00e9 and Nicolas Chopin. Expectation propagation for likelihood-free inference.  Journal of the American Statistical Association, 109:315-333, 2014.  Michael Betancourt. A general metric for Riemannian manifold Hamiltonian Monte Carlo. In Frank Nielsen and Fr\u00e9d\u00e9ric Barbaresco, editors, Geometric Science of Information - First International Conference, pages 327-334, Berlin, Heidelberg, 2013. Springer.  Michael Betancourt. Adiabatic Monte Carlo. arXiv preprint arXiv:1405.3489, 2014.  Taras Bodnar and Arjun K. Gupta. Estimation of the precision matrix of a multivariate  elliptically contoured stable distribution. Statistics, 45(2):131-142, 2011.  Taras Bodnar, Arjun K Gupta, and Nestor Parolya. Optimal linear shrinkage estimator for large dimensional precision matrix. Contributions in infinite-dimensional statistics and related topics, pages 55-60, 2014.  48   F. Optimizing EP Energy  Consider an EP approximation in the exponential family distribution,  Vehtari et al.  g(\u03b8) =  p(\u03b8)  gk(\u03b8),  1 Z  n Y  k=1  where Z is a normalization constant. The global approximation can be formulated as g(\u03b8) \u221d p(\u03b8) exp(sT \u03bb) and the cavity distribution as g k(\u03b8) \u221d p(\u03b8) exp(sT \u03bb k), where \u03bb \\ \\ and \u03bb k denote natural parameters. A fixed point of the EP algorithm corresponds to a \\ stationary point of the following objective function (Minka, 2001b):  min \u03bb  max \u03bb k  \\  (K \u2212 1) log  p(\u03b8) exp(sT \u03bb) d\u03b8 \u2212  log  p(\u03b8)p(yk|\u03b8) exp(sT \u03bb  k) d\u03b8 \\  Z  Z  K X  k=1  K X  k=1  k.  \u03bb \\  such that(K \u2212 1)\u03bb =  This objective function corresponds to \u2212 log Z and to the expectation-consistent approxi- mation (Opper and Winther, 2005). The correspondence and connection to the Bethe free energy is demonstrated by Heskes et al. (2005).  References  Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning, pages 1591-1598, 2012.  Anoop Korattikara Balan, Yutian Chen, and Max Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 31th International Conference on Machine Learning (ICML), pages 181-189, 2014.  R\u00e9mi Bardenet, Arnaud Doucet, and Chris Holmes. On Markov chain Monte Carlo methods  for tall data. Journal of Machine Learning Research, 18(47):1-43, 2017.  Simon Barthelm\u00e9 and Nicolas Chopin. Expectation propagation for likelihood-free inference.  Journal of the American Statistical Association, 109:315-333, 2014.  Michael Betancourt. A general metric for Riemannian manifold Hamiltonian Monte Carlo. In Frank Nielsen and Fr\u00e9d\u00e9ric Barbaresco, editors, Geometric Science of Information - First International Conference, pages 327-334, Berlin, Heidelberg, 2013. Springer.  Michael Betancourt. Adiabatic Monte Carlo. arXiv preprint arXiv:1405.3489, 2014.  Taras Bodnar and Arjun K. Gupta. Estimation of the precision matrix of a multivariate  elliptically contoured stable distribution. Statistics, 45(2):131-142, 2011.  Taras Bodnar, Arjun K Gupta, and Nestor Parolya. Optimal linear shrinkage estimator for large dimensional precision matrix. Contributions in infinite-dimensional statistics and related topics, pages 55-60, 2014. Expectation Propagation for Partitioned Data  Wilson Y. Chen and Matt P. Wand. Factor graph fragmentization of expectation propa- gation. Journal of the Korean Statistical Society, 2020. doi: https://doi.org/10.1007/ s42952-019-00033-9.  Siddhartha Chib. Marginal likelihood from the Gibbs output. Journal of the American  Statistical Association, 90(432):1313-1321, 1995.  Jean-Marie Cornuet, Jean-Michel Marin, Antonietta Mira, and Christian P. Robert. Adaptive multiple importance sampling. Scandinavian Journal of Statistics, 39(4):798-812, 2012.  Botond Cseke and Tom Heskes. Approximate marginals in latent Gaussian models. Journal  of Machine Learning Research, 12:417-454, 2011.  Botond Cseke, Manfred Opper, and Guido Sanguinetti. Approximate inference in latent In Advances in Neural  Gaussian-Markov models from continuous time observations. Information Processing Systems, pages 971-979, 2013.  John P. Cunningham, Philipp Hennig, and Simon Lacoste-Julien. Gaussian probabilities  and expectation propagation. arXiv preprint arXiv:1111.6832, 2011.  Je\ufb00rey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and An- drew Y. Ng. Large Scale Distributed Deep Networks. In Neural Information Processing Systems, pages 1223-1231, 2012.  Guillaume Dehaene. Expectation propagation performs a smoothed gradient descent. arXiv  preprint arXiv:1612.05053, 2016.  Guillaume Dehaene and Simon Barthelm\u00e9. Expectation propagation in the large data limit.  Journal of the Royal Statistical Society: Series B, 80(1):199-217, 2018.  Guillaume P Dehaene and Simon Barthelm\u00e9. Bounding errors of expectation-propagation.  In Advances in Neural Information Processing Systems 28, pages 244-252. 2015.  Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal  of the Royal Statistical Society B, 68(3):411-436, 2006.  Francesca Dominici, Giovanni Parmigiani, Robert L. Wolpert, and Vic Hasselblad. Meta- analysis of migraine headache treatments: Combining information from heterogeneous designs. Journal of the American Statistical Association, 94(445):16-28, 1999.  Cynthia Dwork and Aaron Roth. The algorithmic foundations of di\ufb00erential privacy.  Foundations and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation  with the graphical lasso. Biostatistics, 9(3):432-441, 2008.  Andrew Gelman, Frederic Bois, and Jiming Jiang. Physiological pharmacokinetic analysis using population modeling and informative prior distributions. Journal of the American Statistical Association, 91:1400-1412, 1996. Vehtari et al.  Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. A weakly informative default prior distribution for logistic and other regression models. Annals of Applied Statistics, 2:1360-1383, 2008.  Andrew Gelman, Bob Carpenter, Michael Betancourt, Marcus Brubaker, and Aki Vehtari. Computationally e\ufb03cient maximum likelihood, penalized maximum likelihood, and hi- erarchical modeling using Stan. Technical report, Department of Statistics, Columbia University, 2014a.  Andrew Gelman, Aki Vehtari, Pasi Jyl\u00e4nki, Christian Robert, Nicolas Chopin, and John P. Cunningham. Expectation propagation as a way of life. arXiv preprint arXiv:1412.4869v1, 2014b.  John Geweke. Bayesian inference in econometric models using Monte Carlo integration.  Econometrica, 57(6):1317-1339, 1989.  Arjun K. Gupta, Tamas Varga, and Taras Bodnar. Elliptically Contoured Models in Statistics  and Portfolio Theory. Springer-Verlag, New York, 2nd edition, 2013.  Erika T. Hamden, David Schiminovich, and Mark Seibert. The di\ufb00use galactic far-ultraviolet  sky. The Astrophysical Journal, 779(180):15, 2013.  Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer, Balaji Lakshmi- narayanan, Charles Blundell, and Yee Whye Teh. Distributed Bayesian learning with stochastic natural gradient expectation propagation and the posterior server. Journal of Machine Learning Research, 18(106):1-37, 2017.  Daniel Hern\u00e1ndez-Lobato and Jose Miguel Hern\u00e1ndez-Lobato. Scalable Gaussian process classification via expectation propagation. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 168-176, 2016.  Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Yingzhen Li, Mark Rowland, Thang Bui, Daniel Hern\u00e1ndez- Lobato, and Richard E. Turner. Black-box \u03b1-divergence minimization. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.  Tom Heskes and Onno Zoeter. Expectation propagation for approximate inference in dynamic Bayesian networks. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI), pages 216-223, 2002.  Tom Heskes, Manfred Opper, Wim Wiegerinck, Ole Winther, and Onno Zoeter. Approximate inference techniques with expectation constraints. Journal of Statistical Mechanics: Theory and Experiment, 2005(11):P11015, 2005.  Julian P. T. Higgins and Anne Whitehead. Borrowing strength from external trials in a  meta-analysis. Statistics in Medicine, 15(24):2733-2749, 1996.  Matthew D. Ho\ufb00man, David M. Blei, Chong Wang, and John William Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):1303-1347, 2013. Expectation Propagation for Partitioned Data  Pasi Jyl\u00e4nki, Jarno Vanhatalo, and Aki Vehtari. Robust Gaussian process regression with a  Student-t likelihood. Journal of Machine Learning Research, 12:3227-3257, 2011.  Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic di\ufb00erentiation variational inference. Journal of Machine Learning Research, 18 (14):1-45, 2017.  Daniel Lewandowski, Dorota Kurowicka, and Harry Joe. Generating random correlation matrices based on vines and extended onion method. Journal of multivariate analysis, 100(9):1989-2001, 2009.  Yingzhen Li, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Richard E. Turner. Stochastic expectation propagation. In Advances in Neural Information Processing Systems, pages 2323-2331, 2015.  Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,  Massachusetts Institute of Technology, Cambridge, MA, USA, 2001a.  Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In J. Breese and D. Koller, editors, Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI), pages 362-369. Morgan Kaufmann, San Francisco, Clif., 2001b.  Thomas P. Minka. Power EP. Technical report, Microsoft Research, Cambridge, 2004.  Thomas P. Minka. Divergence measures and message passing. Technical report, Microsoft  Research, Cambridge, 2005.  Thomas P. Minka and John La\ufb00erty. Expectation-propagation for the generative aspect model. In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI), pages 352-359. Morgan Kaufmann, San Francisco, CA, 2002.  Robb J. Muirhead. Aspects of Multivariate Statistical Theory. John Wiley & Sons, Hoboken,  New Jersey, 2005.  Willie Neiswanger, Chong Wang, and Eric P. Xing. Asymptotically exact, embarrassingly parallel MCMC. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI), pages 623-632, 2014.  Manfred Opper and Ole Winther. Gaussian processes for classification: Mean-field algorithms.  Neural Computation, 12(11):2655-2684, 2000.  Manfred Opper and Ole Winther. Expectation consistent approximate inference. Journal of  Machine Learning Research, 6:2177-2204, 2005.  Judea Pearl. Fusion, propagation, and structuring in belief networks. Artificial Intelligence,  29(3):241-288, 1986.  Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models.  In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd Interna- tional Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 324-333, 2016. Vehtari et al.  Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine  Learning. The MIT Press, 2006.  Jaakko Riihim\u00e4ki, Pasi Jyl\u00e4nki, and Aki Vehtari. Nested expectation propagation for Gaussian process classification with a multinomial probit likelihood. Journal of Machine Learning Research, 14:75-109, 2013.  H\u00e5vard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal statistical Society B, 71(2):319-392, 2009.  Swupnil Sahai. Topics in Computational Bayesian Statistics With Applications to Hierarchical Models in Astronomy and Sociology. PhD thesis, Columbia University Academic Commons, 2018.  Amadou Sarr and Arjun K. Gupta. Estimation of the precision matrix of multivariate Kotz  type model. Journal of Multivariate Analysis, 100(4):742-752, 2009.  Anand D. Sarwate, Sergey M. Plis, Jessica A. Turner, Mohammad R. Arbabshirani, and Vince D. Calhoun. Sharing privacy-sensitive access to neuroimaging and genetics data: A review and preliminary validation. Frontiers in Neuroinformatics, 8(35), 2014. doi: 10.3389/fninf.2014.00035.  Steven L. Scott, Alexander W. Blocker, and Fernando V. Bonassi. Bayes and big data: The consensus Monte Carlo algorithm. International Journal of Management Science and Engineering Management, 11(2):78-88, 2016.  Matthias Seeger. Expectation propagation for exponential families. Technical report, Max  Planck Institute for Biological Cybernetics, Tubingen, 2005.  Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. Journal  of Machine Learning Research, 9:759-813, 2008.  Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classification with multiple  classes. Technical report, University of California, Berkeley, 2004.  Tuomas Sivula. Distributed Bayesian inference using expectation propagation. Master\u2019s  thesis, Aalto University, Espoo, Finland, 2015.  Alexander J. Smola, Vishy Vishwanathan, and Eleazar Eskin. Laplace propagation. In S. Thrun, L.K. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information Processing 16, 2004.  Stan Development Team. Stan modeling language: User\u2019s guide and reference manual, 2017.  Version 2.17.0, http://mc-stan.org/.  Dustin Tran, Rajesh Ranganath, and David M. Blei. The variational Gaussian process. In  International Conference on Learning Representations, 2016. Expectation Propagation for Partitioned Data  Hisayuki Tsukuma and Yoshihiko Konno. On improved estimation of normal precision matrix and discriminant coe\ufb03cients. Journal of Multivariate Analysis, 97(7):1477-1500, 2006.  Marcel van Gerven, Botond Cseke, Robert Oostenveld, and Tom Heskes. Bayesian source localization with the multivariate Laplace prior. In Y. Bengio, D. Schuurmans, J. La\ufb00erty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing 22, pages 1901-1909, 2009.  Jarno Vanhatalo, Jaakko Riihim\u00e4ki, Jouni Hartikainen, Pasi Jyl\u00e4nki, Ville Tolvanen, and Aki Vehtari. GPstu\ufb00: Bayesian modeling with Gaussian processes. Journal of Machine Learning Research, 14:1175-1179, 2013.  Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto  smoothed importance sampling. arXiv:1507.02646, 2019.  Mattias Villani and Rolf Larsson. The multivariate split normal distribution and asymmetric principal components analysis. Communications in Statistics\u2014Theory and Methods, 35 (6):1123-1140, 2006.  Matt P. Wand. Fast approximate inference for arbitrarily large semiparametric regression  models via message passing. Journal of the American Statistical Association, 2017.  Xiangyu Wang and David B. Dunson. Parallelizing MCMC via Weierstrass sampler. arXiv  preprint arXiv:1312.4605, 2013.  John Winn and Christopher M. Bishop. Variational message passing. Journal of Machine  Learning Research, 6:661-694, 2005.  Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, and Bo Zhang. Distributed Bayesian posterior sampling via moment sharing. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3356-3364, 2014.  Onno Zoeter and Tom Heskes. Gaussian quadrature based expectation propagation. In Robert Cowell and Zoubin Ghahramani, editors, International Workshop on Artificial Intelligence and Statistics (AISTATS), volume 10, 2005. "}, "Connecting Spectral Clustering to Maximum Margins and Level Sets": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-850.html", "header": "Connecting Spectral Clustering to Maximum Margins and Level Sets", "author": "David P. Hofmeyr", "time": "21(18):1\u221235, 2020.", "abstract": "We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions.", "pdf_url": "http://jmlr.org/papers/volume21/18-850/18-850.pdf", "keywords": ["sets", "convergence", "asymptotics", "consistency"], "reference": "Daniel Alspach and Harold Sorenson. Nonlinear bayesian estimation using gaussian sum  approximations. IEEE transactions on automatic control, 17(4):439-448, 1972.  Antonio Cuevas, Manuel Febrero, and Ricardo Fraiman. Estimating the number of clusters.  Canadian Journal of Statistics, 28(2):367-382, 2000.  Evarist Gin\u00b4e and Armelle Guillou. Rates of strong uniform consistency for multivariate kernel density estimators. In Annales de l\u2019Institut Henri Poincare (B) Probability and Statistics, volume 38, pages 907-921. Elsevier, 2002.  Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical  Computer Science, 38:293-306, 1985.  Lars Hagen and Andrew Kahng. New spectral methods for ratio cut partitioning and IEEE Transactions On Computer-aided Design of Integrated Circuits and  clustering. Systems, 11(9):1074-1085, 1992.  John A Hartigan. Clustering algorithms. 1975.  David P Hofmeyr. Improving spectral clustering using the asymptotic value of the normal-  ized cut. Journal of Computational and Graphical Statistics, pages 1-13, 2019.  David P Hofmeyr, Nicos G Pavlidis, and Idris A Eckley. Minimum spectral connectivity  projection pursuit. Statistics and Computing, 29(2):391-414, 2019.  James R Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and higher-order cheeger inequalities. Journal of the ACM (JACM), 61(6):1-30, 2014.  Jing Lei, Alessandro Rinaldo, et al. Consistency of spectral clustering in stochastic block  models. The Annals of Statistics, 43(1):215-237, 2015.  Hariharan Narayanan, Mikhail Belkin, and Partha Niyogi. On the relation between low density separation, spectral clustering and graph cuts. In Advances in Neural Information Processing Systems, pages 1025-1032, 2006.  Bruno Pelletier and Pierre Pudlo. Operator norm convergence of spectral clustering on level  sets. Journal of Machine Learning Research, 12(Feb):385-416, 2011.  Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs: Spectral  clustering works! In Conference on Learning Theory, pages 1423-1455, 2015.  Alessandro Rinaldo, Larry Wasserman, et al. Generalized density clustering. The Annals  of Statistics, 38(5):2678-2722, 2010.  Murray Rosenblatt. Stochastic curve estimation. In NSF-CBMS Regional Conference Series,  volume 3. Institute of Mathematical Sciences, 1991.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-  tions on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  34   Hofmeyr  References  Daniel Alspach and Harold Sorenson. Nonlinear bayesian estimation using gaussian sum  approximations. IEEE transactions on automatic control, 17(4):439-448, 1972.  Antonio Cuevas, Manuel Febrero, and Ricardo Fraiman. Estimating the number of clusters.  Canadian Journal of Statistics, 28(2):367-382, 2000.  Evarist Gin\u00b4e and Armelle Guillou. Rates of strong uniform consistency for multivariate kernel density estimators. In Annales de l\u2019Institut Henri Poincare (B) Probability and Statistics, volume 38, pages 907-921. Elsevier, 2002.  Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical  Computer Science, 38:293-306, 1985.  Lars Hagen and Andrew Kahng. New spectral methods for ratio cut partitioning and IEEE Transactions On Computer-aided Design of Integrated Circuits and  clustering. Systems, 11(9):1074-1085, 1992.  John A Hartigan. Clustering algorithms. 1975.  David P Hofmeyr. Improving spectral clustering using the asymptotic value of the normal-  ized cut. Journal of Computational and Graphical Statistics, pages 1-13, 2019.  David P Hofmeyr, Nicos G Pavlidis, and Idris A Eckley. Minimum spectral connectivity  projection pursuit. Statistics and Computing, 29(2):391-414, 2019.  James R Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and higher-order cheeger inequalities. Journal of the ACM (JACM), 61(6):1-30, 2014.  Jing Lei, Alessandro Rinaldo, et al. Consistency of spectral clustering in stochastic block  models. The Annals of Statistics, 43(1):215-237, 2015.  Hariharan Narayanan, Mikhail Belkin, and Partha Niyogi. On the relation between low density separation, spectral clustering and graph cuts. In Advances in Neural Information Processing Systems, pages 1025-1032, 2006.  Bruno Pelletier and Pierre Pudlo. Operator norm convergence of spectral clustering on level  sets. Journal of Machine Learning Research, 12(Feb):385-416, 2011.  Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs: Spectral  clustering works! In Conference on Learning Theory, pages 1423-1455, 2015.  Alessandro Rinaldo, Larry Wasserman, et al. Generalized density clustering. The Annals  of Statistics, 38(5):2678-2722, 2010.  Murray Rosenblatt. Stochastic curve estimation. In NSF-CBMS Regional Conference Series,  volume 3. Institute of Mathematical Sciences, 1991.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-  tions on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000. Large Margins, Level Sets and Spectral Clustering  Nicolas Garcia Trillos, Dejan Slepcev, James Von Brecht, Thomas Laurent, and Xavier Bresson. Consistency of cheeger and ratio graph cuts. The Journal of Machine Learning Research, 17(1):6268-6313, 2016.  Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):  395-416, 2007. ISSN 0960-3174. doi: 10.1007/s11222-007-9033-z.  Ulrike Von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clus-  tering. The Annals of Statistics, pages 555-586, 2008.  Dorothea Wagner and Frank Wagner. Between min cut and graph bisection. Springer, 1993.  Guenther Walther. Granulometric smoothing. The Annals of Statistics, pages 2273-2299,  1997.  Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller dif- ferentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Math- ematische Annalen, 71(4):441-479, 1912. "}, "High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-071.html", "header": "High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix", "author": "Cheng Yong Tang, Ethan X. Fang, Yuexiao Dong", "time": "21(19):1\u221225, 2020.", "abstract": "In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to  combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we  detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interactions effects.  We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios.", "pdf_url": "http://jmlr.org/papers/volume21/19-071/19-071.pdf", "keywords": [""], "reference": "P. J. Bickel and E. Levina. Regularized estimation of large covariance matrices. Annals of  Statistics, 36:199-227, 2008.  Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions.  The Annals of Statistics, 41(3):1111-1141, jun 2013. doi: 10.1214/13-aos1096.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  P. B\u00a8uhlmann and S. van de Geer. Statistics for High-dimensional Data: Methods, Theory  and Applications. New York: Springer, 2011.  Nam Hee Choi, William Li, and Ji Zhu. Variable selection with the strong heredity con- straint and its oracle property. Journal of the American Statistical Association, 105(489): 354-364, mar 2010. doi: 10.1198/jasa.2010.tm08281.  R. Dennis Cook. Principal hessian directions revisited. Journal of the American Statistical  Association, 93(441):84-94, mar 1998. doi: 10.1080/01621459.1998.10474090.  Heather J Cordell. Detecting gene-gene interactions that underlie human diseases. Nature  Reviews Genetics, 10(6):392-404, jun 2009. doi: 10.1038/nrg2579.  Serena Dato, Mette Soerensen, Francesco De Rango, Giuseppina Rose, Kaare Christensen, Lene Christiansen, and Giuseppe Passarino. The genetic component of human longevity: New insights from the analysis of pathway-based SNP-SNP interactions. Aging Cell, 17 (3):e12755, mar 2018. doi: 10.1111/acel.12755.  Chao Ding, Defeng Sun, and Kim-Chuan Toh. An introduction to a class of matrix cone  programming. Mathematical Programming, 144(1-2):141-179, 2014.  Jonathan Eckstein and W Yao. Augmented lagrangian and alternating direction methods for convex optimization: A tutorial and some illustrative computational results. RUTCOR Research Reports, 32:3, 2012.  J. Fan and J. Lv. A selective overview of variable selection in high dimensional feature  space. Statistica Sinica, 20:101-148, 2010.  Yingying Fan, Yinfei Kong, Daoji Li, and Jinchi Lv.  Interaction pursuit with feature  screening and selection. Manuscript. arXiv:1605.08933., 2016.  19   High-Dimensional Interactions Detection  Acknowledgement  We are grateful to the Editor, Action Editor and two reviewers for their helpful comments, which lead to a significant improvement of the earlier version of this paper. Tang is partially supported by NSF grant IIS-1546087 and ES-1533956. Fang is partially supported by NSF grant DMS 1820702.  References  P. J. Bickel and E. Levina. Regularized estimation of large covariance matrices. Annals of  Statistics, 36:199-227, 2008.  Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions.  The Annals of Statistics, 41(3):1111-1141, jun 2013. doi: 10.1214/13-aos1096.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  P. B\u00a8uhlmann and S. van de Geer. Statistics for High-dimensional Data: Methods, Theory  and Applications. New York: Springer, 2011.  Nam Hee Choi, William Li, and Ji Zhu. Variable selection with the strong heredity con- straint and its oracle property. Journal of the American Statistical Association, 105(489): 354-364, mar 2010. doi: 10.1198/jasa.2010.tm08281.  R. Dennis Cook. Principal hessian directions revisited. Journal of the American Statistical  Association, 93(441):84-94, mar 1998. doi: 10.1080/01621459.1998.10474090.  Heather J Cordell. Detecting gene-gene interactions that underlie human diseases. Nature  Reviews Genetics, 10(6):392-404, jun 2009. doi: 10.1038/nrg2579.  Serena Dato, Mette Soerensen, Francesco De Rango, Giuseppina Rose, Kaare Christensen, Lene Christiansen, and Giuseppe Passarino. The genetic component of human longevity: New insights from the analysis of pathway-based SNP-SNP interactions. Aging Cell, 17 (3):e12755, mar 2018. doi: 10.1111/acel.12755.  Chao Ding, Defeng Sun, and Kim-Chuan Toh. An introduction to a class of matrix cone  programming. Mathematical Programming, 144(1-2):141-179, 2014.  Jonathan Eckstein and W Yao. Augmented lagrangian and alternating direction methods for convex optimization: A tutorial and some illustrative computational results. RUTCOR Research Reports, 32:3, 2012.  J. Fan and J. Lv. A selective overview of variable selection in high dimensional feature  space. Statistica Sinica, 20:101-148, 2010.  Yingying Fan, Yinfei Kong, Daoji Li, and Jinchi Lv.  Interaction pursuit with feature  screening and selection. Manuscript. arXiv:1605.08933., 2016. Tang, Fang, and Dong  Ethan X Fang, Bingsheng He, Han Liu, and Xiaoming Yuan. Generalized alternating direction method of multipliers: new theoretical insights and applications. Mathematical Programming Computation, 7(2):149-187, 2015.  Ethan X Fang, Min-Dian Li, Michael I Jordan, and Han Liu. Mining massive amounts of genomic data: a semiparametric topic modeling approach. Journal of the American Statistical Association, 112(519):921-932, 2017.  Roland Glowinski and A Marroco. Sur l\u2019approximation, par \u00b4el\u00b4ements finis d\u2019ordre un, et la r\u00b4esolution, par p\u00b4enalisation-dualit\u00b4e d\u2019une classe de probl`emes de dirichlet non lin\u00b4eaires. Revue fran\u00b8caise d\u2019automatique, informatique, recherche op\u00b4erationnelle. Analyse num\u00b4erique, 9(R2):41-76, 1975.  Ning Hao and Hao Helen Zhang.  Interaction screening for ultrahigh-dimensional data. Journal of the American Statistical Association, 109(507):1285-1301, jul 2014. doi: 10. 1080/01621459.2014.881741.  Ning Hao, Yang Feng, and Hao Helen Zhang. Model selection for high-dimensional quadratic regression via regularization. Journal of the American Statistical Association, 113:615- 625, 2018.  Asad Haris, Daniela Witten, and Noah Simon. Convex modeling of interactions with strong heredity. Journal of Computational and Graphical Statistics, 25(4):981-1004, oct 2016. doi: 10.1080/10618600.2015.1067217.  Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Spar- sity: The Lasso and Generalizations. Taylor & Francis Inc, 2015. ISBN 1498712169. URL http://www.ebook.de/de/product/23642172/trevor_hastie_robert_tibshirani_ martin_wainwright_statistical_learning_with_sparsity.html.  J. J. Jaccard and R. Turrisi. Interaction E\ufb00ects in Multiple Regression. SAGE, 2003. ISBN 0761927425. URL https://www.ebook.de/de/product/4324119/james_j_jaccard_ robert_turrisi_interaction_effects_in_multiple_regression.html.  Binyan Jiang, Xiangyu Wang, and Chenlei Leng. A direct approach for sparse quadratic  discriminant analysis. Journal of Machine Learning Research, 19:1-37, 2018.  Bo Jiang and Jun S. Liu. Variable selection for general index models via sliced inverse regression. The Annals of Statistics, 42(5):1751-1786, oct 2014. doi: 10.1214/14-aos1233.  Mary-Claire King, Joan H Marks, Jessica B Mandell, et al. Breast and ovarian cancer risks due to inherited mutations in BRCA1 and BRCA2. Science, 302(5645):643-646, 2003.  Yinfei Kong, Daoji Li, Yingying Fan, and Jinchi Lv. Interaction pursuit in high-dimensional multi-response regression via distance correlation. The Annals of Statistics, 45(2):897- 922, apr 2017. doi: 10.1214/16-aos1474.  Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Sta- tistical Association, 86(414):316-327, Jun 1991. ISSN 1537-274X. doi: 10.1080/01621459. 1991.10475035. URL http://dx.doi.org/10.1080/01621459.1991.10475035. High-Dimensional Interactions Detection  Ker-Chau Li. On principal hessian directions for data visualization and dimension reduction: Another application of stein\u2019s lemma. Journal of the American Statistical Association, 87(420):1025-1039, dec 1992. doi: 10.1080/01621459.1992.10476258.  Yang Li and Jun S. Liu. Robust variable and interaction selection for logistic regression and general index models. Journal of the American Statistical Association, pages 1-16, nov 2017. doi: 10.1080/01621459.2017.1401541.  Matthew N McCall, Benjamin M Bolstad, and Rafael A Irizarry. Frozen robust multiarray  analysis (fRMA). Biostatistics, 11(2):242-253, 2010.  Sahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, nov 2012. doi: 10.1214/12-sts400.  J. A. Nelder. A reformulation of linear models. Journal of the Royal Statistical Society.  Series A (General), 140(1):48, 1977. doi: 10.2307/2344517.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11(Aug):2241-2259, 2010.  Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Minimax rates of estimation for high- dimensional linear regression over (cid:96)q-balls. IEEE Transactions on Information Theory, 57(10):6976-6994, oct 2011. doi: 10.1109/tit.2011.2165799.  Marylyn D. Ritchie and Kristel Van Steen. The search for gene-gene interactions in genome- wide association studies: challenges in abundance of methods, practical considerations, and biological interpretation. Annals of Translational Medicine, 6(8):157-157, apr 2018. doi: 10.21037/atm.2018.04.05.  Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements.  Manuscript. arXiv:1106.1151., 2011.  Rajen D Shah. Modelling interactions in high-dimensional data with backtracking. Journal  of Machine Learning Research, 17(1):7225-7255, 2016.  Yiyuan She, Zhifeng Wang, and He Jiang. Group regularized estimation under structural hierarchy. Journal of the American Statistical Association, 113(521):445-454, dec 2018. doi: 10.1080/01621459.2016.1260470.  Charles M. Stein. Estimation of the mean of a multivariate normal distribution. The Annals  of Statistics, 9(6):1135-1151, nov 1981. doi: 10.1214/aos/1176345632.  Defeng Sun, Kim-Chuan Toh, and Liuqin Yang. A convergent 3-block semiproximal alter- nating direction method of multipliers for conic programming with 4-type constraints. SIAM Journal on Optimization, 25(2):882-915, 2015.  George Wu, Jason T Yustein, Matthew N McCall, Michael Zilliox, Rafael A Irizarry, Karen Zeller, Chi V Dang, and Hongkai Ji. ChIP-PED enhances the analysis of ChIP-seq and ChIP-chip data. Bioinformatics, 2013. Tang, Fang, and Dong  Ming Yuan, V. Roshan Joseph, and Hui Zou. Structured variable selection and estimation. The Annals of Applied Statistics, 3(4):1738-1757, dec 2009. doi: 10.1214/09-aoas254.  Peng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for grouped and hierarchical variable selection. The Annals of Statistics, 37(6A):3468-3497, dec 2009. doi: 10.1214/07-aos584. High-Dimensional Interactions Detection  Appendix  Proof of Proposition 3  Since Z = \u03a3\u22121X, we have Var(Z) = \u03a3\u22121, and Cov(Z, X) = I. Under model (2),  Y \u2212 E(Y ) =  \u03b2k(cid:96){XkX(cid:96) \u2212 E(XkX(cid:96))} +  \u03b8kXk + (cid:15).  (10)  p (cid:88)  p (cid:88)  k=1  (cid:96)=k  p (cid:88)  k=1  Then, we have from C1 of Condition 2  \u03a8 = E{Y \u2212 E(Y )ZZ T} = E  \u03b2kl{XkX(cid:96) \u2212 E(XkX(cid:96))}ZZ T  .  (cid:35)  (cid:34) p  (cid:88)  p (cid:88)  k=1  (cid:96)=k  (cid:96)=k \u03b2k(cid:96)Cov(XkX(cid:96), ZiZj). Then Proposition 3 follows from C2  This implies \u03c8ij = (cid:80)p of Condition 2.  k=1  (cid:80)p  Proof of Lemma 4  We prove the main results following the framework of penalized M-estimator as in Negahban et al. (2012) with (cid:96)1 penalty function.  Let L(\u03a8) = tr(\u03a8TS\u03a8S) \u2212 2tr(\u03a8Q), and denote by \u03a8\u2217 the truth of \u03a8. By the definition  L( (cid:98)\u03a8) + \u03bb(cid:107) (cid:98)\u03a8(cid:107)1 \u2264 L(\u03a8\u2217) + \u03bb(cid:107) (cid:98)\u03a8\u2217(cid:107)1.  For ease in presentation, we take (cid:98)\u03a8 to be symmetric. Let (cid:98)\u2206 = (cid:98)\u03a8 \u2212 \u03a8\u2217, then we have  L(\u03a8\u2217 + (cid:98)\u2206) \u2212 L(\u03a8\u2217) + \u03bb(cid:107) (cid:98)\u03a8(cid:107)1 \u2264 \u03bb(cid:107) (cid:98)\u03a8\u2217(cid:107)1.  (11)  We note that  Then (11) imples that  tr{(\u03a8\u2217 + (cid:98)\u2206)T S(\u03a8\u2217 + (cid:98)\u2206)S} \u2212 tr(\u03a8\u2217S\u03a8\u2217S) = tr( (cid:98)\u2206S (cid:98)\u2206S) + 2tr{ (cid:98)\u2206S\u03a8\u2217S}.  tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u03a8(cid:107)1 \u2264 \u22122tr{ (cid:98)\u2206(S\u03a8\u2217S \u2212 Q)} + \u03bb(cid:107)\u03a8\u2217(cid:107)1. We observe that with large probability 2(cid:107)S\u03a8\u2217S \u2212 Q(cid:107)\u221e < \u03bb/2. Additionally, let S = supp(\u03a8\u2217),  (12)  (cid:107) (cid:98)\u03a8(cid:107)1 = (cid:107) (cid:98)\u03a8U (cid:107)1 + (cid:107) (cid:98)\u03a8U c(cid:107)1 = (cid:107) (cid:98)\u03a8U + \u03a8\u2217 U (cid:107)1 \u2212 (cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  \u2265 (cid:107)\u03a8\u2217  U (cid:107)1 + (cid:107) (cid:98)\u03a8SC (cid:107)1,  U \u2212 \u03a8\u2217  U (cid:107)1 + (cid:107) (cid:98)\u03a8U c(cid:107)1  (cid:107) (cid:98)\u2206(cid:107)1 = (cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217 These together with (12) imply that  U (cid:107)1 + (cid:107) (cid:98)\u03a8U c(cid:107)1, and (cid:107)\u03a8\u2217(cid:107)1 = (cid:107)\u03a8U (cid:107)1.  2tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u03a8U c(cid:107)1 \u2264 3\u03bb(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107)1.  (13)  Since tr( (cid:98)\u2206S (cid:98)\u2206S) \u2265 0, then with large probability  (cid:107) (cid:98)\u03a8U c(cid:107)1 \u2264 3(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107)1.  This completes the proof of Lemma 4. Tang, Fang, and Dong  Proof of Lemma 5  From the proof of Lemma 4, Lemma 5 follows straightforwardly by noting that  2tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u2206(cid:107)1 = 2tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107)1 + \u03bb(cid:107) (cid:98)\u03a8U c(cid:107)1  \u2264 4\u03bb(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107).  (14)  Proof of Theorem 7  We further note that  tr( (cid:98)\u2206S (cid:98)\u2206S) = vecT( (cid:98)\u2206)(S \u2297 S)vec( (cid:98)\u2206).  Then the oracle inequality (14) and the restricted eigenvalue condition (9) imply that  2tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107)1 + \u03bb(cid:107) (cid:98)\u03a8U c(cid:107)1 \u2264 4\u03bb(cid:107) (cid:98)\u03a8U \u2212 \u03a8\u2217  U (cid:107)1  \u2264 4\u03bb|U|1/2{vecT(\u2206)(S \u2297 S)vec(\u2206)}1/2/\u03b3 \u2264 vecT(\u2206)(S \u2297 S)vec(\u2206) + 4\u03bb2|U|/\u03b32 = tr( (cid:98)\u2206S (cid:98)\u2206S) + 4\u03bb2|U|/\u03b32.  tr( (cid:98)\u2206S (cid:98)\u2206S) + \u03bb(cid:107) (cid:98)\u03a8 \u2212 \u03a8\u2217(cid:107)1 \u2264 4\u03bb2|U|/\u03b32  (15)  That is  This completes the proof of Theorem 7.  5.3. Proof of Theorem 9  Let \u2206\u03a3 = S \u2212 \u03a3X . By its definition  S\u03a8\u2217S \u2212 Q = (\u03a3X + \u2206\u03a3)\u03a8\u2217(\u03a3X + \u2206\u03a3) \u2212 Q  = \u03a3X \u03a8\u2217\u03a3X \u2212 Q + \u2206\u03a3\u03a8\u2217\u03a3X + \u03a3X \u03a8\u2217\u2206\u03a3 + \u2206\u03a3\u03a8\u2217\u2206\u03a3.  Then by Condition 8 and Lemma A.3 of Bickel and Levina (2008), (cid:107)\u2206\u03a3(cid:107)\u221e = Op((cid:112)log p/n). Additionally, since \u03a3X \u03a8\u2217\u03a3X = \u03a3Y XX , and (cid:107)\u03a3Y XX \u2212 Q(cid:107)\u221e = Op((cid:112)log p/n) follows from Condition 8, similar to that of Lemma A.3 of Bickel and Levina (2008). Then Theorem 9 follows by establishing that (cid:107)S\u03a8\u2217S \u2212 Q(cid:107)\u221e = Op((cid:112)log p/n) and applying the triangular inequality and the condition that (cid:107)\u03a8\u2217(cid:107)\u221e = O  , (cid:107)\u03a8\u2217\u03a3X (cid:107)\u221e = O(1).  (cid:17) (cid:16)(cid:112)n/log p  5.4. Proof of Theorem 11  The first order conditon of (5) implies that  0 = S (cid:98)\u03a8S \u2212 Q + \u03bbZ  where Z = sign( (cid:98)\u03a8). Let \u03b7 = vec( (cid:98)\u03a8), \u03b7\u2217 = vec(\u03a8\u2217), \u03be = vec(Q), and \u03c4 = vec(Z). By writing in vector form, we have  (cid:18)\u039b11 \u039b12 \u039b21 \u039b22  (cid:19) (cid:18)\u03b71 \u2212 \u03b7\u2217(cid:19)  \u03b72  \u2212  (cid:19)  (cid:18)\u03be1 \u2212 \u039b11\u03b7\u2217 1 \u03be2 \u2212 \u039b21\u03b7\u2217 1  + \u03bb  = 0  (cid:19)  (cid:18)\u03c41 \u03c42 High-Dimensional Interactions Detection  where (\u03b7T 1 , \u03b7T \u03b72 = 0, we must have  2 )T is the decomposition of vec( (cid:98)\u03a8) corresponding to the set U. Then to ensure  \u03b71 \u2212 \u03b7\u2217  1 = \u039b\u22121 \u03c42 = \u039b21\u039b\u22121  11 (\u03be1 \u2212 \u039b11\u03b7\u2217  1) \u2212 \u03bb\u039b\u22121  11 \u03c41,  11 Z1 + \u03bb\u22121(\u03be2 \u2212 \u039b21\u039b\u22121  11 \u03be1)  where Z1 is the component of Z corresponding to the blocking. Then the irrepresentable condition (8) implies that \u03b72 = 0 with probability tending to 1, and the minimal signal strength condition | min(\u03a8\u2217 11 (cid:107)\u221e ensures that \u03b72 is bounded around from 0 with probability tending to 1.  U )| > (cid:107)\u03bb\u039b\u22121 "}, "Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-083.html", "header": "Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections", "author": "Junhong Lin, Volkan Cevher", "time": "21(20):1\u221244, 2020.", "abstract": "We study the least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space as a special case.  We first investigate regularized algorithms adapted to a projection operator on a closed subspace of the Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function.  As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystr\\\"{o}m regularized algorithms.  Our results provide optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nystr\\\"{o}m regularized algorithms, considering both the attainable and non-attainable cases, in the well-conditioned regimes. We then study stochastic gradient methods with projection over the subspace, allowing multi-pass over the data and minibatches, and we derive similar optimal statistical convergence results.", "pdf_url": "http://jmlr.org/papers/volume21/19-083/19-083.pdf", "keywords": ["kernel methods", "regularized algorithms", "stochastic gradient methods", "random projection", "sketching"], "reference": "Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with In Advances in Neural Information Processing Systems, pages  statistical guarantees. 775-783, 2015.  Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on  Learning Theory, pages 185-209, 2013.  41   Convergences of Regularized Algorithms and SGM with Random Projections  The proof is complete.  With the above lemma, and using a similar argument as that for Lemma 19, we can  prove Lemma 21. We thus skip it.  Appendix B. Learning with Kernel Methods  In this appendix, we review how the regression setting considered in this paper covers non-parametric regression with kernel methods.  Let the input space \u039e be a closed subset of Euclidean space Rd, the output space Y \u2286 R. Let \u00b5 be an unknown but fixed Borel probability measure on \u039e \u00d7 Y . Assume that {(\u03bei, yi)}m i=1 are i.i.d. from the distribution \u00b5. A reproducing kernel K is a symmetric function K : \u039e \u00d7 \u039e \u2192 R such that (K(ui, uj))(cid:96) i,j=1 is positive semidefinite for any finite set of points {ui}(cid:96) i=1 in \u039e. The kernel K defines a reproducing kernel Hilbert space (RKHS) (HK, (cid:107) \u00b7 (cid:107)K) as the completion of the linear span of the set {K\u03be(\u00b7) := K(\u03be, \u00b7) : \u03be \u2208 \u039e} with respect to the inner product (cid:104)K\u03be, Ku(cid:105)K := K(\u03be, u). For any f \u2208 HK, the reproducing property holds: f (\u03be) = (cid:104)K\u03be, f (cid:105)K.  Example B.1 (Sobolev Spaces) Let X = [0, 1] and the kernel  K(x, x(cid:48)) =  (cid:40)  (1 \u2212 y)x, x \u2264 y; (1 \u2212 x)y, x \u2265 y.  (cid:90)  inf f \u2208HK  \u039e\u00d7Y  (f (\u03be) \u2212 y)2d\u00b5(\u03be, y).  (cid:90)  inf f \u2208HK  \u039e\u00d7Y  ((cid:104)f, K\u03be(cid:105)K \u2212 y)2d\u00b5(\u03be, y).  Then the kernel induces a Sobolev Space H = {f : X \u2192 R|f is absolutely continuous , f (0) = f (1) = 0, f \u2208 L2(X)}.  In learning with kernel methods, one considers the following minimization problem  Since f (\u03be) = (cid:104)K\u03be, f (cid:105)K by the reproducing property, the above can be rewritten as  Letting X = {K\u03be : \u03be \u2208 \u039e} and defining another probability measure \u03c1(K\u03be, y) = \u00b5(\u03be, y), the above reduces to the learning setting in Section 1.  References  Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with In Advances in Neural Information Processing Systems, pages  statistical guarantees. 775-783, 2015.  Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on  Learning Theory, pages 185-209, 2013. Lin and Cevher  Francis Bach. On the equivalence between kernel quadrature rules and random feature  expansions. Journal of Machine Learning Research, 18(21):1-38, 2017.  Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28 (3):253-263, 2008.  Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in  learning theory. Journal of Complexity, 23(1):52-72, 2007.  Gilles Blanchard and Nicole M\u00a8ucke. Optimal rates for regularization of statistical inverse learning problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.  Andrea Caponnetto. Optimal learning rates for regularization operators in learning theory.  Technical report, 2006.  Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares  algorithm. Foundations of Computational Mathematics, 7(3):331-368, 2007.  Andrea Caponnetto and Yuan Yao. Adaptation for regularization operators in learning  theory. 2006.  Felipe Cucker and Ding Xuan Zhou. Learning theory: an approximation theory viewpoint,  volume 24. Cambridge University Press, 2007.  Lee H Dicker, Dean P Foster, and Daniel Hsu. Kernel ridge vs. principal component re- gression: Minimax bounds and the qualification of regularization operators. Electronic Journal of Statistics, 11(1):1022-1047, 2017.  Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodru\ufb00. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13(Dec):3475-3506, 2012.  Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse prob-  lems, volume 375. Springer Science & Business Media, 1996.  Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares  algorithm. arXiv 1702.07254v1, 2017.  Junichi Fujii, Masatoshi Fujii, Takayuki Furuta, and Ritsuo Nakamoto. Norm inequalities equivalent to Heinz inequality. Proceedings of the American Mathematical Society, 118 (3):827-830, 1993.  L Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. Neural Computation, 20(7):1873-1897, 2008.  Alex Gittens and Michael W Mahoney. Revisiting the nystr\u00a8om method for improved large- scale machine learning. Journal of Machine Learning Research, 17(1):3977-4041, 2016.  Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression.  Foundations of Computational Mathematics, 14(3):569-600, 2014. Convergences of Regularized Algorithms and SGM with Random Projections  Felix Krahmer and Rachel Ward. New and improved Johnson-Lindenstrauss embeddings via the restricted isometry property. SIAM Journal on Mathematical Analysis, 43(3): 1269-1281, 2011.  Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling techniques for the nystrom  method. In Artificial Intelligence and Statistics, pages 304-311, 2009.  Junhong Lin and Volkan Cevher. Optimal rates of sketched-regularized algorithms for least- squares regression over Hilbert spaces. arXiv preprint arXiv:1803.04371 (Proceedings of the 35th International Conference on Machine Learning), 2018a.  Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochas- tic gradient methods and spectral algorithms. arXiv preprint arXiv:1801.07226, 2018b.  Junhong Lin and Lorenzo Rosasco. Optimal rates for learning with Nystr\u00a8om stochastic  gradient methods. arXiv preprint arXiv:1710.07797, 2017a.  Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient meth-  ods. Journal of Machine Learning Research, 18(97):1-47, 2017b.  Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over Hilbert spaces. Applied and Com- putational Harmonic Analysis, 2018. URL https://doi.org/10.1016/j.acha.2018. 09.009.  Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and  Trends in Machine Learning, 3(2):123-224, 2011.  Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann. Uniform uncertainty principle for bernoulli and subgaussian ensembles. Constructive Approximation, 28(3): 277-289, 2008.  Stanislav Minsker. On some extensions of Bernstein\u2019s inequality for self-adjoint operators.  arXiv preprint arXiv:1112.5448, 2011.  GL Myleiko, S Pereverzyev Jr, and SG Solodky. Regularized Nystr\u00a8om subsampling in  regression and ranking problems under general smoothness assumptions. 2017.  Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochas- tic gradient descent on hard learning problems through multiple passes. In Advances in Neural Information Processing Systems, pages 8114-8124, 2018.  IF Pinelis and AI Sakhanenko. Remarks on inequalities for large deviation probabilities.  Theory of Probability & Its Applications, 30(1):143-148, 1986.  Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.  In  Advances in Neural Information Processing Systems, pages 1177-1184, 2008.  James O Ramsay. Functional data analysis. Wiley Online Library, 2006. Lin and Cevher  Alessandro Rudi, Ra\ufb00aello Camoriano, and Lorenzo Rosasco. Less is more: Nystrom com-  putational regularization. arXiv preprint arXiv:1507.04717, 2015.  John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge  university press, 2004.  Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and  their approximations. Constructive Approximation, 26(2):153-172, 2007.  Bharath Sriperumbudur and Nicholas Sterge. Approximate kernel PCA using random features: Computational vs. statistical trade-o\ufb00. arXiv preprint arXiv:1706.06296, 2017.  Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science &  Business Media, 2008.  Ingo Steinwart, Don R Hush, and Clint Scovel. Optimal rates for regularized least squares  regression. In Conference On Learning Theory, 2009.  Joel A Tropp. User-friendly tools for random matrices: An introduction. Technical report,  DTIC Document, 2012.  Shusen Wang, Alex Gittens, and Michael W Mahoney. Sketched ridge regression: Opti- mization perspective, statistical perspective, and model averaging. Journal of Machine Learning Research, 18(1):8039-8088, 2017.  Christopher KI Williams and Matthias Seeger. Using the Nystr\u00a8om method to speed up kernel machines. In Advances in Neural Information Processing Systems, pages 661-667. MIT press, 2000.  Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr\u00a8om method vs random fourier features: A theoretical and empirical comparison. In Advances in Neural Information Processing Systems, pages 476-484, 2012.  Yun Yang, Mert Pilanci, and Martin J Wainwright. Randomized sketches for kernels: Fast  and optimal nonparametric regression. Annals of Statistics, 45(3):991-1023, 2017.  Tong Zhang. Learning bounds for kernel regression using e\ufb00ective data dimensionality.  Neural Computation, 17(9):2077-2098, 2005.  Yuchen Zhang, John C Duchi, and Martin J Wainwright. Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates. Journal of Machine Learning Research, 16:3299-3340, 2015. "}, "Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-198.html", "header": "Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems", "author": "Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L. Bartlett, Martin J. Wainwright", "time": "21(21):1\u221251, 2020.", "abstract": "We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of these methods when applied to linear-quadratic systems, and study various settings of driving noise and reward feedback.  Our main theoretical result provides an explicit bound on the sample or evaluation complexity: we show that these methods are guaranteed to converge to within any pre-specified tolerance of the optimal policy with a number of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension, and curvature properties of the problem.  Our analysis reveals some interesting differences between the settings of additive driving noise and random initialization, as well as the settings of one-point and two-point reward feedback. Our theory is corroborated by simulations of derivative-free methods in application to these systems. Along the way, we derive convergence rates for stochastic zero-order optimization algorithms when applied to a certain class of non-convex problems.", "pdf_url": "http://jmlr.org/papers/volume21/19-198/19-198.pdf", "keywords": ["Derivative-Free Optimization", "Linear Quadratic Control", "Non-Convex Opti mization."], "reference": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic systems. In Proceedings of the Conference on Learning Theory, pages 1-26, 2011.  Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesv\u00b4ari. Regret bounds for model-free  linear quadratic control. arXiv preprint arXiv:1804.06021, 2018.  Marc Abeille and Alessandro Lazaric.  Improved regret bounds for Thompson sampling in linear quadratic control problems. In Proceedings of the International Conference on Machine Learning, pages 1-9, 2018.  47   Derivative-Free Methods for Policy Optimization  In order to complete the proof, we now demonstrate how to unroll this recursion using strong induction. For each time step i = 1, 2, . . . , T , denote by Ei the event that \u2206i \u2264 10\u22060 and \u2206i \u2264 (cid:0)1 \u2212 \u03b7\u00b5  (cid:1) \u2206i\u22121 + \u03b7 \u00b5(cid:15)16 . We claim that for each t \u2208 N, we have (cid:9) \u2265 1 \u2212 \u03b4t.  Pr (cid:8)\u2229t  i=1Ei  Let us establish this claim via induction.  Base case: Applying Lemma 23 and equation (45), we obtain with probability 1 \u2212 \u03b4 the inequality \u22061 \u2264 (1 \u2212 \u03b7\u00b5 10 , we have \u22061 \u2264 10\u22060, so we have shown the base case that event E1 holds with probability exceeding 1 \u2212 \u03b4.  16 . Further, by our assumption (cid:15) \u2264 min{1, 1  8 )\u22060 + \u03b7 \u00b5(cid:15)  \u00b5 } \u22060  Induction step: Fix an integer t, and assume, by the induction hypothesis, that the event \u2229t i=1Ei holds with probability exceeding 1 \u2212 \u03b4t. Let us condition on this event. In addition, applying Lemma 23 and equation (45) yields, with probability 1\u2212\u03b4, the inequality  \u2206t+1 \u2264  1 \u2212  \u2206t + \u03b7  \u2264  1 \u2212  \u22060 +  (1 \u2212  (cid:16)  (cid:16)  (cid:16)  (cid:16)  (cid:17)  (cid:17)t+1  (cid:17)t+1  \u03b7\u00b5 8 \u03b7\u00b5 8  \u03b7\u00b5 8  \u03b7\u00b5 8  \u00b5(cid:15) 16  t (cid:88)  i=1 \u221e (cid:88)  i=1 (cid:15) 2  .  \u2264  1 \u2212  \u22060 +  (1 \u2212  =  1 \u2212  (cid:17)t+1  \u22060 +  \u03b7\u00b5 8  \u03b7\u00b5 8  )i\u03b7  )i\u03b7  \u00b5(cid:15) 16  \u00b5(cid:15) 16  Once again, by our assumption (cid:15) \u2264 min{1, 1 the pieces with a union bound then implies that the event \u2229t+1 exceeding 1 \u2212 \u03b4(t + 1), thereby establishing the induction hypothesis.  10 , we have \u2206t+1 \u2264 10\u22060. Putting together i=1Ei holds with probability  \u00b5 } \u22060  Finally, at time T , we condition on the event \u2229T  i=1Ei, thereby obtaining the bound  (cid:16)  \u2206T \u2264  1 \u2212  (cid:17)T  \u03b7\u00b5 8  \u22060 +  (cid:15) 2  .  We complete the proof by by substituting our choice of the tuple (\u03b7, T ).  References  Yasin Abbasi-Yadkori and Csaba Szepesv\u00b4ari. Regret bounds for the adaptive control of linear quadratic systems. In Proceedings of the Conference on Learning Theory, pages 1-26, 2011.  Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesv\u00b4ari. Regret bounds for model-free  linear quadratic control. arXiv preprint arXiv:1804.06021, 2018.  Marc Abeille and Alessandro Lazaric.  Improved regret bounds for Thompson sampling in linear quadratic control problems. In Proceedings of the International Conference on Machine Learning, pages 1-9, 2018. Malik, Pananjady, Bhatia, Khamaru, Bartlett, and Wainwright  Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimiza- In Proceedings of the Conference on Learning  tion with multi-point bandit feedback. Theory, pages 28-40, 2010.  Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In Advances in Neural Information Processing Systems, pages 1184-1194, 2017.  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00b4emi Munos. Minimax regret bounds for reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 263-272, 2017.  Dimitri P Bertsekas. Dynamic programming and optimal control. Vol. I. Athena Scientific,  third edition, 2005.  Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control. In Proceedings of the International Conference on Machine Learning, pages 1028-1037, 2018.  Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators e\ufb03- T regret. In Proceedings of the International Conference on Machine  \u221a  ciently with only Learning, pages 1300-1309, 2019.  Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform In Advances in Neural Information  PAC bounds for episodic reinforcement learning. Processing Systems, pages 5713-5723, 2017.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.  Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. arXiv preprint arXiv:1805.09388, 2018.  Mark P Deisenroth, Carl E Rasmussen, and Dieter Fox. Learning to control a low-cost ma- nipulator using data-e\ufb03cient reinforcement learning. In Robotics: Science and Systems, 2012.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61:2788-2806, 2015.  Rick Durrett. Probability: theory and examples. Cambridge university press, 2010.  Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time analysis of optimal adaptive policies for linear-quadratic systems. arXiv preprint arXiv:1711.07230, 2017.  Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In Proceedings of the International Conference on Machine Learning, pages 1466-1475, 2018. Derivative-Free Methods for Policy Optimization  Claude-Nicolas Fiechter. PAC adaptive control of linear systems.  In Proceedings of the  Conference on Computational Learning Theory, pages 72-80, 1997.  Abraham Flaxman, Adam Kalai, and Brendan McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the Symposium on Discrete Algorithms, pages 385-394, 2005.  Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex  stochastic programming. SIAM Journal on Optimization, 23:2341-2368, 2013.  Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Q- learning with model-based acceleration. In Proceedings of the International Conference on Machine Learning, pages 2829-2838, 2016.  David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables. The Annals of Mathematical Statistics, 42(3): 1079-1083, 1971.  Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of  subgaussian random vectors. Electronic Communications in Probability, 17, 2012.  Morteza Ibrahimi, Adel Javanmard, and Benjamin V. Roy. E\ufb03cient reinforcement learn- ing for high dimensional linear quadratic systems. In Advances in Neural Information Processing Systems, pages 2636-2644. 2012.  Kevin G Jamieson, Robert Nowak, and Ben Recht. Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, pages 2672-2680. 2012.  Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.  Rudolf E Kalman. Contributions to the theory of optimal control. Boletin de la Sociedad  Matematica Mexicana, 5:102-119, 1960.  Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795-811, 2016.  Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17:1334-1373, 2016.  Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.  Lennart Ljung. System identification. In Signal analysis and prediction, pages 163-173.  Springer, 1998. Malik, Pananjady, Bhatia, Khamaru, Bartlett, and Wainwright  Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les  \u00b4equations aux d\u00b4eriv\u00b4ees partielles, pages 87-89, 1963.  Volodymyr Mnih et al. Human-level control through deep reinforcement learning. Nature,  518:529-533, 2015.  Yurii Nesterov. Random gradient-free minimization of convex functions. Core discussion pa- pers, Universit\u00b4e catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2011.  Boris T Polyak. Gradient methods for solving equations and inequalities. USSR Computa-  tional Mathematics and Mathematical Physics, 4(6):17-32, 1964.  Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages 6550-6561, 2017.  Jacopo Riccati. Animadversiones in aequationes di\ufb00erentiales secundi gradus. Acta Erudi-  torum Lipsiae, 1724.  Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strate- gies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.  John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the International Conference on Machine Learning, pages 1889-1897, 2015.  Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimiza-  tion. In Proceedings of the Conference on Learning Theory, pages 3-24, 2013.  Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with  two-point feedback. Journal of Machine Learning Research, 18:1703-1713, 2017.  David Silver et al. Mastering the game of Go with deep neural networks and tree search.  Nature, 529:484-489, 2016.  James C Spall. Introduction to stochastic search and optimization: estimation, simulation,  and control, volume 65. John Wiley & Sons, 2005.  Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In Proceedings of the International Conference on Intelligent Robots and Systems, pages 23-30, 2017.  Stephen Tu and Benjamin Recht. Least-squares temporal di\ufb00erence learning for the linear quadratic regulator. In Proceedings of the International Conference on Machine Learning, pages 5012-5021, 2018a.  Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods  on the linear quadratic regulator: An asymptotic viewpoint. CoRR, 2018b. Derivative-Free Methods for Policy Optimization  Yining Wang, Sivaraman Balakrishnan, and Aarti Singh. Optimization of smooth functions with noisy observations: Local minimax rates. arXiv preprint arXiv:1803.08586, 2018a.  Yining Wang, Simon S Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth- order optimization in high dimensions. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 1356-1365, 2018b.  Peter Whittle. Optimal control: Basics and Beyond. Wiley and Sons, 1996.  Farrol Tim Wright. A bound on tail probabilities for quadratic forms in independent random variables whose distributions are not necessarily symmetric. The Annals of Probability, 1:1068-1070, 1973. "}, "A Unified Framework for Structured Graph Learning via Spectral Constraints": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-276.html", "header": "A Unified Framework for Structured Graph Learning via Spectral Constraints", "author": "Sandeep Kumar, Jiaxi Ying, Jos\u00c3\u00a9 Vin\u00c3\u00adcius de M. Cardoso, Daniel P. Palomar", "time": "21(22):1\u221260, 2020.", "abstract": "Graph learning from data is a canonical problem that has received substantial attention in the literature. Learning a structured graph is essential for interpretability and identification of the relationships among data. In general, learning a graph with a specific structure is an NP-hard combinatorial problem and thus designing a general tractable algorithm is challenging. Some useful structured graphs include connected, sparse, multi-component, bipartite, and regular graphs. In this paper, we introduce a unified framework for structured graph learning that combines Gaussian graphical model and spectral graph theory. We propose to convert combinatorial structural constraints into spectral constraints on graph matrices and develop an optimization framework based on block majorization-minimization to solve structured graph learning problem. The proposed algorithms are provably convergent and practically amenable for a number of graph based applications such as data clustering. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms. An open source R package containing the code for all the experiments is available at https://CRAN.R-project.org/package=spectralGraphTopology.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-276/19-276.pdf"}, "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-429.html", "header": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing", "author": "Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi Zhang, Zhongyue Zhang, Shuai Zheng, Yi Zhu", "time": "21(23):1\u22127, 2020.", "abstract": "We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.", "pdf_url": "http://jmlr.org/papers/volume21/19-429/19-429.pdf"}, "Distributed Feature Screening via Componentwise Debiasing": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-537.html", "header": "Distributed Feature Screening via Componentwise Debiasing", "author": "Xingxiang Li, Runze Li, Zhiming Xia, Chen Xu", "time": "21(24):1\u221232, 2020.", "abstract": "Feature screening is a powerful tool in processing high-dimensional data. When the sample size N and the number of features p are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of 'divide-and-conquer', the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that  is insensitive to the number of data segments m. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the centralized estimator in terms of the probability convergence bound and the mean squared error rate; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples.", "pdf_url": "http://jmlr.org/papers/volume21/19-537/19-537.pdf"}, "Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-580.html", "header": "Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models", "author": "Ivona Bez\u00c3\u00a1kov\u00c3\u00a1, Antonio Blanca, Zongchen Chen, Daniel \u00c5\u00a0tefankovi\u00c4\u008d, Eric Vigoda", "time": "21(25):1\u221262, 2020.", "abstract": "We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model $M$ and a sampling oracle for the distribution $\\mu_{M^*}$ of an unknown model $M^*$, can we efficiently determine if the two models $M$ and $M^*$ are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning. For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018) presented a polynomial-time algorithm for identity testing. We prove hardness results in the antiferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. Specifically, for $n$-vertex graphs of maximum degree $d$, we prove that if $|\\beta| d = \\omega(\\log{n})$ (where $\\beta$ is the inverse temperature parameter), then there is no polynomial running time identity testing algorithm unless $RP=NP$. In the hard-constraint setting, we present hardness results for identity testing for proper colorings. Our results are based on the presumed hardness of #BIS, the problem of (approximately) counting independent sets in bipartite graphs.", "pdf_url": "http://jmlr.org/papers/volume21/19-580/19-580.pdf"}, "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes": {"volumn": 21, "url": "http://jmlr.org/papers/v21/15-509.html", "header": "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes", "author": "Anders Ellern Bilgrau, Carel F.W. Peeters, Poul Svante Eriksen, Martin Boegsted, Wessel N. van Wieringen", "time": "21(26):1\u221252, 2020.", "abstract": "We consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. An $\\ell_2$-penalized maximum likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other $\\ell_2$-penalized estimators as special cases. In addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and subtypes of diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 12 large-scale gene expression data sets of diffuse large B-cell lymphoma subtypes. The estimator and its related procedures are incorporated into the R-package rags2ridges.", "pdf_url": "http://jmlr.org/papers/volume21/15-509/15-509.pdf"}, "A New Class of Time Dependent Latent Factor Models with Applications": {"volumn": 21, "url": "http://jmlr.org/papers/v21/16-639.html", "header": "A New Class of Time Dependent Latent Factor Models with Applications", "author": "Sinead A. Williamson, Michael Minyi Zhang, Paul Damien", "time": "21(27):1\u221224, 2020.", "abstract": "In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process \u00e2\u0080\u0094 a probability measure on the space of random, unbounded binary matrices \u00e2\u0080\u0094 finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.", "pdf_url": "http://jmlr.org/papers/volume21/16-639/16-639.pdf"}, "On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms": {"volumn": 21, "url": "http://jmlr.org/papers/v21/17-698.html", "header": "On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms", "author": "Nicolas Garcia Trillos, Zachary Kaplan, Thabo Samakhoana, Daniel Sanz-Alonso", "time": "21(28):1\u221247, 2020.", "abstract": "This paper considers a Bayesian approach to graph-based semi-supervised learning. We show that if the graph parameters are suitably scaled, the graph-posteriors converge to a continuum limit as the size of the unlabeled data set grows. This consistency result has profound algorithmic implications: we prove that when consistency holds, carefully designed Markov chain Monte Carlo algorithms have a uniform spectral gap, independent of the number of unlabeled inputs. Numerical experiments illustrate and complement the theory.", "pdf_url": "http://jmlr.org/papers/volume21/17-698/17-698.pdf"}, "The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response": {"volumn": 21, "url": "http://jmlr.org/papers/v21/17-788.html", "header": "The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response", "author": "Xin Zhang, Qing Mai, Hui Zou", "time": "21(29):1\u221236, 2020.", "abstract": "Sufficient dimension reduction (SDR) is a very useful concept for exploratory analysis and data visualization in regression, especially when the number of covariates is large. Many SDR methods have been proposed for regression with a continuous response, where the central subspace (CS) is the target of estimation. Various conditions, such as the linearity condition and the constant covariance condition, are imposed so that these methods can estimate at least a portion of the CS. In this paper we study SDR for regression and discriminant analysis with categorical response. Motivated by the exploratory analysis and data visualization aspects of SDR,  we propose a new geometric framework to reformulate the SDR problem in terms of manifold optimization and introduce a new concept called Maximum Separation Subspace (MASES). The MASES naturally preserves the \u00e2\u0080\u009csufficiency\u00e2\u0080\u009d in SDR without imposing additional conditions on the predictor distribution, and directly inspires a semi-parametric estimator. Numerical studies show MASES exhibits superior performance as compared with competing SDR methods in specific settings.", "pdf_url": "http://jmlr.org/papers/volume21/17-788/17-788.pdf"}, "Generalized Nonbacktracking Bounds on the Influence": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-112.html", "header": "Generalized Nonbacktracking Bounds on the Influence", "author": "Emmanuel Abbe, Sanjeev Kulkarni, Eun Jee Lee", "time": "21(31):1\u221236, 2020.", "abstract": "This paper develops deterministic upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit r-nonbacktracking walks and Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing algorithms. Further, we provide parameterized versions of the bounds that control the trade-off between efficiency and accuracy. Finally, the tightness of the bounds is illustrated on various network models.", "pdf_url": "http://jmlr.org/papers/volume21/18-112/18-112.pdf"}, "Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-143.html", "header": "Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping", "author": "Mihai Cucuringu, Hemant Tyagi", "time": "21(32):1\u221277, 2020.", "abstract": "Consider an unknown smooth function  $f: [0,1]^d \\rightarrow \\mathbb{R}$, and assume we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) + \\eta_i)  \\bmod 1$, for  $x_i \\in [0,1]^d$, where $\\eta_i$ denotes the noise. Given the samples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust estimates of the clean  samples $f(x_i) \\bmod  1$. We formulate a natural approach for solving this problem, which works with angular embeddings of  the noisy mod 1 samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem -- a quadratically constrained quadratic program (QCQP) -- where the variables are constrained to lie on the unit circle. Our proposed approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, as well as random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our proposed approach via extensive numerical simulations on synthetic data, along with a simple least-squares based solution for the unwrapping stage, that recovers the original samples of $f$ (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo $1$ samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semidefinite programming relaxation of our formulation. For the two-dimensional version of the problem, which has applications in synthetic aperture radar interferometry (InSAR), we are able to solve instances of real-world data with a million sample points in under 10 seconds, on a personal laptop.", "pdf_url": "http://jmlr.org/papers/volume21/18-143/18-143.pdf"}, "On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-425.html", "header": "On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent", "author": "Huan Li, Zhouchen Lin", "time": "21(33):1\u221245, 2020.", "abstract": "Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need $T(\\epsilon^{-2})$ iterations to achieve an $\\epsilon$-optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with $T(\\epsilon^{-1})$ iterations to achieve an $\\epsilon$-optimal dual solution, where $T(x)$ can be $x$ or $\\sqrt{x}$. In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same $O\\left(\\frac{1}{\\sqrt{\\epsilon}}\\right)$ order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of $O\\left(n\\log n+\\sqrt{\\frac{n}{\\epsilon}}\\right)$ in both primal space and dual space, where $n$ is the number of samples. Our result takes out the $\\left(\\log \\frac{1}{\\epsilon}\\right)$ factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal $O\\left(\\sqrt{\\frac{n}{\\epsilon}}\\right)$ iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, e.g., the least absolute deviation and SVM.", "pdf_url": "http://jmlr.org/papers/volume21/18-425/18-425.pdf"}, "Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent": {"volumn": 21, "url": "http://jmlr.org/papers/v21/18-638.html", "header": "Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent", "author": "Dominic Richards, Patrick Rebeschini", "time": "21(34):1\u221244, 2020.", "abstract": "We propose graph-dependent implicit regularisation strategies for synchronised distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, single-machine serial statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in  decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the single-machine serial SGD setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours.", "pdf_url": "http://jmlr.org/papers/volume21/18-638/18-638.pdf"}, "Learning with Fenchel-Young losses": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-021.html", "header": "Learning with Fenchel-Young losses", "author": "Mathieu Blondel, Andr\u00c3\u00a9 F.T. Martins, Vlad Niculae", "time": "21(35):1\u221269, 2020.", "abstract": "Over the past decades, numerous loss functions have been been proposed for a variety of supervised learning tasks, including regression, classification, ranking, and more generally structured prediction. Understanding the core principles and theoretical properties underpinning these losses is key to choose the right loss for the right problem, as well as to create new losses which combine their strengths. In this paper, we introduce Fenchel-Young losses, a generic way to construct a convex loss function for a regularized prediction function. We provide an in-depth study of their properties in a very broad setting, covering all the aforementioned supervised learning tasks, and revealing new connections between sparsity, generalized entropies, and separation margins. We show that Fenchel-Young losses unify many well-known loss functions and allow to create useful new ones easily. Finally, we derive efficient predictive and training algorithms, making Fenchel-Young losses appealing both in theory and practice.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-021/19-021.pdf"}, "Noise Accumulation in High Dimensional Classification and Total Signal Index": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-117.html", "header": "Noise Accumulation in High Dimensional Classification and Total Signal Index", "author": "Miriam R. Elman, Jessica Minnier, Xiaohui Chang, Dongseok Choi", "time": "21(36):1\u221223, 2020.", "abstract": "Great attention has been paid to Big Data in recent years. Such data hold promise for scientific discoveries but also pose challenges to analyses. One potential challenge is noise accumulation. In this paper, we explore noise accumulation in high dimensional two-group classification. First, we revisit a previous assessment of noise accumulation with principal component analyses, which yields a different threshold for discriminative ability than originally identified. Then we extend our scope to its impact on classifiers developed with three common machine learning approaches---random forest, support vector machine, and boosted classification trees. We simulate four scenarios with differing amounts of signal strength to evaluate each method. After determining noise accumulation may affect the performance of these classifiers, we assess factors that impact it. We conduct simulations by varying sample size, signal strength, signal strength proportional to the number predictors, and signal magnitude with random forest classifiers. These simulations suggest that noise accumulation affects the discriminative ability of high-dimensional classifiers developed using common machine learning methods, which can be modified by sample size, signal strength, and signal magnitude. We developed the measure total signal index (TSI) to track the trends of total signal and noise accumulation.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-117/19-117.pdf"}, "Causal Discovery Toolbox: Uncovering causal relationships in Python": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-187.html", "header": "Causal Discovery Toolbox: Uncovering causal relationships in Python", "author": "Diviyan Kalainathan, Olivier Goudet, Ritik Dutta", "time": "21(37):1\u22125, 2020.", "abstract": "This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The cdt package implements an end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the `Bnlearn' and `Pcalg' packages, together with algorithms for pairwise causal discovery such as ANM.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-187/19-187.pdf"}, "Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-239.html", "header": "Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification", "author": "Leo L. Duan", "time": "21(38):1\u221225, 2020.", "abstract": "High dimensional data often contain multiple facets, and several clustering patterns can co-exist under different variable subspaces, also known as the views. While multi-view clustering algorithms were proposed, the uncertainty quantification remains difficult --- a particular challenge is in the high complexity of estimating the cluster assignment probability under each view, and sharing information among views. In this article, we propose an approximate Bayes approach --- treating the similarity matrices generated over the views as rough first-stage estimates for the co-assignment probabilities; in its Kullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by the pairwise product of simplex coordinates. Interestingly, each simplex coordinate directly encodes the cluster assignment uncertainty. For multi-view clustering, we let each view draw a  parameterization from a few candidates, leading to dimension reduction. With high model flexibility, the estimation can be efficiently carried out as a continuous optimization problem, hence enjoys gradient-based computation. The theory establishes the connection of this model to a random partition distribution under multiple views. Compared to single-view clustering approaches, substantially more interpretable results are obtained when clustering brains from a human traumatic brain injury study, using high-dimensional gene expression data.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-239/19-239.pdf"}, "Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-260.html", "header": "Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables", "author": "Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, Kun Zhang", "time": "21(39):1\u221224, 2020.", "abstract": "We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, the inferred causal relationships among the observed variables are often wrong. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among the observed variables. The next question is whether the causal effects can be uniquely identified as well. We show that causal effects among observed variables cannot be identified uniquely under mere assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we are able to propose an efficient method that identifies the set of all possible causal effects that are compatible with the observational data. We present additional structural conditions on the causal graph under which causal effects among observed variables can be determined uniquely.  Furthermore, we provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm for learning causal models.", "pdf_url": "http://jmlr.org/papers/volume21/19-260/19-260.pdf"}, "Optimal Bipartite Network Clustering": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-299.html", "header": "Optimal Bipartite Network Clustering", "author": "Arash A. Amini, Zhixin Zhou", "time": "21(40):1\u221268, 2020.", "abstract": "We study bipartite community detection in networks, or more generally the network  biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood  classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems.  The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of  average degree growth, from sparse networks with average degrees growing arbitrarily slowly  to fairly dense networks with average degrees of order $\\sqrt{n}$. As a special case, we recover the known exact recovery threshold in the $\\log n$ regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable  algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class  by numerical simulations.", "pdf_url": "http://jmlr.org/papers/volume21/19-299/19-299.pdf"}, "Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-407.html", "header": "Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables", "author": "Rune Christiansen, Jonas Peters", "time": "21(41):1\u221246, 2020.", "abstract": "Given a response $Y$ and a vector $X = (X^1, \\dots, X^d)$ of $d$ predictors, we investigate the problem of inferring direct causes of $Y$ among the vector $X$. Models for $Y$ that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of $ Y $ are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-407/19-407.pdf"}, "Branch and Bound for Piecewise Linear Neural Network Verification": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-468.html", "header": "Branch and Bound for Piecewise Linear Neural Network Verification", "author": "Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, M. Pawan Kumar", "time": "21(42):1\u221239, 2020.", "abstract": "The success of Deep Learning and its potential use in many safety-critical applicationshas motivated research on formal verification of Neural Network (NN) models. In thiscontext, verification involves proving or disproving that an NN model satisfies certaininput-output properties. Despite the reputation of learned NN models as black boxes,and the theoretical hardness of proving useful properties about them, researchers havebeen successful in verifying some classes of models by exploiting their piecewise linearstructure and taking insights from formal methods such as Satisifiability Modulo Theory.However, these methods are still far from scaling to realistic neural networks. To facilitateprogress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases.With the help of the BaB framework, we make three key contributions. Firstly, we identifynew methods that combine the strengths of multiple existing approaches, accomplishingsignificant performance improvements over previous state of the art. Secondly, we introducean effective branching strategy on ReLU non-linearities. This branching strategy allows usto efficiently and successfully deal with high input dimensional problems with convolutionalnetwork architecture, on which previous methods fail frequently.  Finally, we proposecomprehensive test data sets and benchmarks which includes a collection of previouslyreleased testcases. We use the data sets to conduct a thorough experimental comparison ofexisting and new algorithms and to provide an inclusive analysis of the factors impactingthe hardness of verification problems.", "pdf_url": "http://jmlr.org/papers/volume21/19-468/19-468.pdf"}, "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-569.html", "header": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "author": "Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, Michael I. Jordan", "time": "21(43):1\u221236, 2020.", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "pdf_url": "http://jmlr.org/papers/volume21/19-569/19-569.pdf"}, "Dynamical Systems as Temporal Feature Spaces": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-589.html", "header": "Dynamical Systems as Temporal Feature Spaces", "author": "Peter Tino", "time": "21(44):1\u221242, 2020.", "abstract": "Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input-driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.", "pdf_url": "http://jmlr.org/papers/volume21/19-589/19-589.pdf"}, "A Convex Parametrization of a New Class of Universal Kernel Functions": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-594.html", "header": "A Convex Parametrization of a New Class of Universal Kernel Functions", "author": "Brendon K. Colbert, Matthew M. Peet", "time": "21(45):1\u221229, 2020.", "abstract": "The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria - e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria - the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth.  Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly.", "pdf_url": "http://jmlr.org/papers/volume21/19-594/19-594.pdf"}, "Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement": {"volumn": 21, "url": "http://jmlr.org/papers/v21/19-985.html", "header": "Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement", "author": "Wouter Kool, Herke van Hoof, Max Welling", "time": "21(47):1\u221236, 2020.", "abstract": "We develop ancestral Gumbel-Top-$k$ sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top $k$ of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size $k$. Our algorithm allows to set the number of parallel processors $m$, to trade off the number of iterations versus the total cost (iterations times $m$) of running the algorithm. For $m = 1$ the algorithm has minimum total cost, whereas for $m = k$ the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of Gumbel-Top-$k$ sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume21/19-985/19-985.pdf"}}