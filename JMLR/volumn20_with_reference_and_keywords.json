{"Adaptation Based on Generalized Discrepancy": {"volumn": 20, "url": "http://jmlr.org/papers/v20/15-192.html", "header": "Adaptation Based on Generalized Discrepancy", "author": "Corinna Cortes, Mehryar Mohri, Andr\u00c3\u00a9s Mu\u00c3\u00b1oz Medina", "time": "20(1):1\u221230, 2019.", "abstract": "We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previously proposed solutions for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, the reweighting depends on the hypothesis sought. The algorithm is derived from a less conservative notion of discrepancy than the DM algorithm called  generalized discrepancy . We present a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also give a detailed theoretical analysis of its learning guarantees which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon discrepancy minimization.", "pdf_url": "http://jmlr.org/papers/volume20/15-192/15-192.pdf", "keywords": ["domain adaptation", "learning theory"], "reference": "Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of  unlabeled target samples. In Proceedings of ALT, pages 139-153, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-  sentations for domain adaptation. In Proceedings of NIPS, pages 137-144, 2006.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain  adaptation. JMLR - Proceedings Track, 9:129-136, 2010.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments.  In Proceedings of ICML, pages 1870-1879, 2015.  Ste\ufb00en Bickel, Michael Br\u00a8uckner, and Tobias Sche\ufb00er. Discriminative learning for di\ufb00ering  training and test distributions. In Proceedings of ICML, pages 81-88, 2007.  28   Cortes, Mohri and Mu\u02dcnoz Medina  Appendix C. \u00b5-admissibility  Lemma 23 Assume that Lp(h(x), y) \u2264 M for all x \u2208 X and y \u2208 Y, then Lp is \u00b5-admissible with \u00b5 = pM p\u22121.  Proof Since x (cid:55)\u2192 xp is p-Lipschitz over [0, 1] we can write  |L(h(x), y) \u2212 L(h(cid:48)(x), y)| = M p  |h(x) \u2212 y| M  p  \u2212  |h(cid:48)(x) \u2212 y| M  \u2264 pM p\u22121|h(x) \u2212 y + y \u2212 h(cid:48)(x)| = pM p\u22121|h(x) \u2212 h(cid:48)(x)|.  (cid:17)  (cid:16)  (cid:16)  (cid:12) (cid:12) (cid:12) (cid:12)  p  (cid:17)  (cid:12) (cid:12) (cid:12) (cid:12)  Lemma 24 Let L be the Lp loss for some p \u2265 1 and let h, h(cid:48), h(cid:48)(cid:48) be functions satisfying Lp(h(x), h(cid:48)(x)) \u2264 M and Lp(h(cid:48)(cid:48)(x), h(cid:48)(x)) \u2264 M for all x \u2208 X , for some M \u2265 0. Then, for any distribution D over X , the following inequality holds:  |LD(h, h(cid:48)) \u2212 LD(h(cid:48)(cid:48), h(cid:48))| \u2264 pM p\u22121[LD(h, h(cid:48)(cid:48))]  1 p .  (32)  Proof Proceeding as in the proof of Lemma 23, we obtain  |LD(h, h(cid:48)) \u2212 LD(h(cid:48)(cid:48), h(cid:48))| = | E x\u2208D \u2264 pM p\u22121 E x\u2208D  (cid:2)  Lp(h(x), h(cid:48)(x)) \u2212 Lp(h(cid:48)(cid:48)(x), h(cid:48)(x)  |  |h(x) \u2212 h(cid:48)(cid:48)(x)|  .  (cid:3)  Since p \u2265 1, by Jensen\u2019s inequality, we can write Ex\u2208D h(cid:48)(cid:48)(x)|p  = [LD(h, h(cid:48)(cid:48))]  1 p .  1/p  (cid:2)  (cid:3) |h(x) \u2212 h(cid:48)(cid:48)(x)|  \u2264 Ex\u2208D  |h(x) \u2212  (cid:2)  (cid:3)  (cid:2)  (cid:3)  References  Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of  unlabeled target samples. In Proceedings of ALT, pages 139-153, 2012.  Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-  sentations for domain adaptation. In Proceedings of NIPS, pages 137-144, 2006.  Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00b4avid P\u00b4al. Impossibility theorems for domain  adaptation. JMLR - Proceedings Track, 9:129-136, 2010.  Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments.  In Proceedings of ICML, pages 1870-1879, 2015.  Ste\ufb00en Bickel, Michael Br\u00a8uckner, and Tobias Sche\ufb00er. Discriminative learning for di\ufb00ering  training and test distributions. In Proceedings of ICML, pages 81-88, 2007. Adaptation Based on Generalized Discrepancy  John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL, 2007.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  Press, Cambridge, 2004.  ALT, 2011.  Corinna Cortes and Mehryar Mohri. Domain adaptation in regression. In Proceedings of  Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory  and algorithm for regression. Theoretical Computer Science, 9474, 2013.  Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection  bias correction theory. In Proceedings of ALT, pages 38-53, 2008.  Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance  weighting. In Proceedings of NIPS, pages 442-450, 2010.  Hal Daum\u00b4e III. Frustratingly easy domain adaptation. In Proceedings of ACL, 2007.  Luc Devroye, L\u00b4azl\u00b4o Gy\u00a8orfi, and G\u00b4abor Lugosi. A Probabilistic Theory of Pattern Recogni-  tion. Springer, 1996.  Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo\u02dcao Gra\u00b8ca, and Fernando Pereira. Frustratingly hard domain adaptation for dependency parsing. In EMNLP-CoNLL, 2007.  Pascal Germain, Amaury Habrard, Fran\u00b8cois Laviolette, and Emilie Morvant. A PAC- Bayesian approach for domain adaptation with specialization to linear classifiers. In Proceedings of ICML, 2013.  Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00a8olkopf, and Alexan-  der J. Smola. A kernel two-sample test. JMLR, 13:723-773, 2012.  Judy Ho\ufb00man, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation  for evolving visual domains. In Proceedings of IEEE CVPR, pages 867-874, 2014.  Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard Sch\u00a8olkopf. Correcting sample selection bias by unlabeled data. In Proceedings of NIPS, volume 19, pages 601-608, 2006.  Jing Jiang and ChengXiang Zhai. Instance Weighting for Domain Adaptation in NLP. In  Proceedings of ACL, pages 264-271, 2007.  Piyush Kumar, Joseph S. B. Mitchell, and E. Alper Yildirim. Computing core-sets and approximate smallest enclosing hyperspheres in high dimensions. In ALENEX, Lecture Notes Comput. Sci, pages 45-55, 2003. Cortes, Mohri and Mu\u02dcnoz Medina  C. J. Leggetter and Philip C. Woodland. Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models. Computer Speech & Language, 9(2):171-185, 1995.  Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning  bounds and algorithms. In Proceedings of COLT. Omnipress, 2009.  Aleix M. Mart\u00b4\u0131nez. Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class. IEEE Trans. Pattern Anal., 24(6), 2002.  Andr\u00b4es Mu\u02dcnoz Medina. Learning Theory and Algorithms for Auctioning and Adaptation  Problems. PhD thesis, New York University, 2015.  Mehryar Mohri and Andres Mu\u02dcnoz. New analysis and algorithm for learning with drifting  distributions. In Proceedings of ALT. Springer, 2012.  Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE Trans. on Neural Networks, 22(2):199-210, 2011.  Carl Edward Rasmussen, Radford M. Neal, Geo\ufb00rey Hinton, Drew van Camp, Michael Revow Zoubin Ghahramani, Rafal Kustra, and Rob Tibshirani. The delve project. http: //www.cs.toronto.edu/~delve/data/datasets.html, 1996. version 1.0.  Sven Sch\u02ddonherr. Quadratic Programming in Geometric Optimization: Theory, Implemen-  tation, and Applications. PhD thesis, Swiss Federal Institute of Technology, 2002.  Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul von B\u00a8unau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In Proceedings of NIPS, pages 1433-1440, 2007.  Tatiana Tommasi, Tinne Tuytelaars, and Barbara Caputo. A testbed for cross-dataset  analysis. CoRR, abs/1402.5923, 2014. URL http://arxiv.org/abs/1402.5923.  Leslie G. Valiant. A Theory of the Learnable. ACM Press New York, NY, USA, 1984.  Vladimir N. Vapnik. Statistical Learning Theory. J. Wiley & Sons, 1998.  E. Alper Yildirim. Two algorithms for the minimum enclosing ball problem. SIAM Journal  on Optimization, 19(3):1368-1391, 2008.  Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain adaptation. In  Proceedings of NIPS, pages 1790-1798. MIT Press, 2012.  Erheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren. Cross valida- tion framework to choose amongst models and datasets for transfer learning. In Proceed- ings of ECML PKDD 2010 Part III, pages 547-562, 2010. "}, "Transport Analysis of Infinitely Deep Neural Network": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-243.html", "header": "Transport Analysis of Infinitely Deep Neural Network", "author": "Sho Sonoda, Noboru Murata", "time": "20(2):1\u221252, 2019.", "abstract": "We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth---why do DNNs perform better than shallow models?---and the interpretation of DNNs---what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation (ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distr", "pdf_url": "http://jmlr.org/papers/volume20/16-243/16-243.pdf", "keywords": ["uum limit", "backward heat equation", "Wasserstein geometry", "ridgelet analysis"], "reference": "Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data generating distribution. Journal of Machine Learning Research, 15(Nov):3743-3773, 2014.  Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: Generative stochastic networks. Information and Inference, 5(2):210-249, 2016.  46  (84)  (85)  (86)  (87)  (88)  (cid:4)   Sonoda and Murata  The second integral in (82) is calculated as follows:  (cid:16)  0;\u221ar2\u2212p2  (cid:90)Bm\u2212  y  2dy = |  (cid:17) |  (cid:90)Sm\u2212  0 (cid:90)  \u221ar2\u2212p2  2\u03c1m\u22122d\u03c1d\u03c9  \u03c1\u03c9 |  |  \u221ar2\u2212p2  \u03c1md\u03c1  (cid:90)Sm\u2212  d\u03c9  0 (cid:90) 1 \u2212 2  m  \u03c0  =  =  (m + 1)\u0393  m\u22121 2  (r2  p2)  m+1 2  .  \u2212  (cid:1) Hence, by combining the first and second integrals, we have  (cid:0)  Rad[Vr,0](u, p) =\u2212  Am 2(m+1) (r2 0 (cid:40)  \u2212  p2)  m\u2212 2  2 m\u22121 p2 + r2  (cid:110)  (cid:111)  |  p | p |  < r  r.  | \u2265  The ridgelet transform R\u03c1(cid:48)[Vr,0] is given by  R\u03c1(cid:48)[Vr,0](u/\u03b1, \u03b2/\u03b1) =  k(p)\u03c1(cid:48)  \u03b1(p  \u03b2)dp,  \u2212  (cid:90)|p|<r  where we define  k(p) := Rad[Vr,0](u, p).  Recall that Rad[Vr,0](u, p) does not depend on the direction u; thus, the definition of k is reasonable. According to (85), k is a compactly supported bump function. Consequently, k is summable; thus, the integral  K :=  k(p)dp  (cid:90)R  always exists. Recall that the convolution results in smoothing, i.e.,  k(p)\u03c1(cid:48)  \u03b1(p  \u03b2)dp  \u2212  K\u03c1(cid:48)  \u03b1(  \u03b2).  \u2212  \u2248  (cid:90)|p|<r  In summary, we have presented the following:  R\u03c1[idr,0](a, b) =  aR\u03c1(cid:48)[Vr,0](a, b)  \u2212  Ka\u03c1(cid:48)(  b).  \u2212  \u2248 \u2212  References  Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data generating distribution. Journal of Machine Learning Research, 15(Nov):3743-3773, 2014.  Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: Generative stochastic networks. Information and Inference, 5(2):210-249, 2016.(84)  (85)  (86)  (87)  (88)  (cid:4)   Transport Analysis of Infinitely Deep Neural Network  Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Gradient Flows in Metric Spaces and  in the Space of Probability Measures. Birkh\u00a8auser, 2008.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein generative adversarial In Proceedings of The 34th International Conference on Machine Learning,  networks. volume 70, pages 214-223, Sydney, Australia, 2017.  Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In The 36th International Conference on Machine Learning, volume 80, pages 244-253, Stockholm, Sweden, 2018.  Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in  Neural Information Processing Systems 27, pages 2654-2662, Montr\u00b4eal, BC, 2014.  Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal  of Machine Learning Research, 18(19):1-53, 2017a.  Francis Bach. On the equivalence between kernel quadrature rules and random feature  expansions. Journal of Machine Learning Research, 18(21):1-38, 2017b.  Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal func-  tion. IEEE Transactions on Information Theory, 39(3):930-945, 1993.  Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Advances in Neural Information Processing Systems 18, pages 123-130, Vancouver, BC, 2006.  Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, pages 153-160, Vancouver, BC, 2007.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, 2013a.  Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto- encoders as generative models. In Advances in Neural Information Processing Systems 26, pages 899-907, Lake Tahoe, USA, 2013b.  Yoshua Bengio, \u00b4Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep gener- ative stochastic networks trainable by backprop. In Proceedings of The 31st International Conference on Machine Learning, volume 32, pages 226-234, Beijing, China, 2014.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  Press, 2004.  versity, 1998.  Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions.  Communications on Pure and Applied Mathematics, 44(4):375-417, 1991.  Emmanuel Jean Cand`es. Ridgelets: Theory and Applications. PhD thesis, Standford Uni- Sonoda and Murata  Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural or- dinary di\ufb00erential equations. In Advances in Neural Information Processing Systems 31, pages 6572\u2014-6583, Montr\u00b4eal, BC, 2018.  Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over- parameterized models using optimal transport. In Advances in Neural Information Pro- cessing Systems 32, Montr\u00b4eal, BC, 2018.  Anna Choromanska, Mikael Hena\ufb00, Michael Mathieu, G\u00b4erard Ben Arous, and Yann Le- Cun. The loss surfaces of multilayer networks. In Proceedings of The 18th International Conference on Artificial Intelligence and Statistics 2015, volume 38, pages 192-204, San Diego, USA, 2015.  Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In 29th Annual Conference on Learning Theory, volume 49, pages 1-31, 2016.  Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional In Advances in Neural Information Processing Systems 27, non-convex optimization. pages 2933-2941, Montr\u00b4eal, BC, 2014.  Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In  29th Annual Conference on Learning Theory, volume 49, pages 1-34, 2016.  Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625-660, 2010.  Lawrence C. Evans and Ronald F. Gariepy. Measure Theory and Fine Properties of Func-  tions. CRC Press, revised edition, 2015.  Edward I. George, Feng Liang, and Xinyi Xu. Improved minimax predictive densities under  Kullback-Leibler loss. Annals of Statistics, 34(1):78-91, 2006.  Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible residual network: Backpropagation without storing activations. In Advances in Neural Information Processing Systems 30, pages 2214-2224, Long Beach, USA, 2017.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pages 2672-2680, Montr\u00b4eal, BC, 2014.  Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse  Problems, 34(1):1-22, 2018.  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, Las Vegas, USA, 2016. Transport Analysis of Infinitely Deep Neural Network  Geo\ufb00rey E. Hinton. Connectionist learning procedures. Artificial Intelligence, 40(1-3):  185-234, 1989.  Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions e\ufb00ectively. In Proceedings of The 22nd International Conference on Artificial Intelligence and Statistics 2019, Okinawa, Japan, 2019.  Sergey Io\ufb00e and Christian Szegedy. Batch normalization: Accelerating deep network train- ing by reducing internal covariate shift. In Proceedings of The 32nd International Con- ference on Machine Learning, volume 37, pages 448-456, Lille, France, 2015.  Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Infor-  mation Processing Systems 29, pages 586-594, Barcelona, Spain, 2016.  Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International  Conference on Learning Representations 2014, pages 1-14, Ban\ufb00, BC, 2014.  Jason M. Klusowski and Andrew R. Barron. Minimax lower bounds for ridge combinations including neural nets. In 2017 IEEE International Symposium on Information Theory, pages 1376-1380, 2017.  Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of ReLU and squared ReLU ridge functions with (cid:96)1 and (cid:96)0 controls. IEEE Transactions on Information Theory, 64(12):7649-7656, 2018.  Alex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1097-1105, Lake Tahoe, USA, 2012.  V\u02d8era K\u02daurkov\u00b4a. Complexity estimates based on integral transforms induced by computa-  tional units. Neural Networks, 33:160-167, 2012.  Honglak Lee. Unsupervised Feature Learning via Sparse Hierarchical Representations. PhD  thesis, Stanford University, 2010.  Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to discrete-weight neural networks. In Proceedings of The 35th International Conference on Machine Learning, volume 80, pages 2985-2994, Stockholm, Sweden, 2018.  Henry W. Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work  so well? Journal of Statistical Physics, 168(6):1223-1247, 2017.  Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems 29, pages 2378-2386, Barcelona, Spain, 2016.  Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural net- works: Bridging deep architectures and numerical di\ufb00erential equations. In Proceedings of The 35th International Conference on Machine Learning, volume 80, pages 3276-3285, Stockholm, Sweden, 2018. Sonoda and Murata  Noboru Murata. An integral representation of functions using three-layered betworks and  their approximation bounds. Neural Networks, 9(6):947-956, 1996.  Behnam Neyshabur. Implicit Regularization in Deep Learning. PhD thesis, Toyota Tech-  nological Institute at Chicago, 2017.  Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings of The 34th International Conference on Machine Learning, volume 70, pages 2603-2612, Sydney, Australia, 2017.  Atsushi Nitanda and Taiji Suzuki. Functional gradient boosting based on residual network perception. In Proceedings of The 35th International Conference on Machine Learning, volume 80, pages 3819-3828, Stockholm, Sweden, 2018.  Kaare Brandt Petersen and Michael Syskind Pedersen. The matrix cookbook, version:  November 15, 2012. Technical report, Technical University of Denmark, 2012.  Gabriel Peyr\u00b4e and Marco Cuturi. Computational Optimal Transport. 2018.  Allan Pinkus. Density in approximation theory. Surveys in Approximation Theory, 1:1-45,  2005.  Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503-519, 2017.  Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, 1996.  Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. In Proceedings of The 32nd International Conference on Machine Learning, volume 37, pages 1530-1538, Lille, France, 2015.  Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of The 28th International Conference on Machine Learning, pages 833-840, Belleview, USA, 2011.  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press,  2012.  activation function. 2017.  University Press, 2004.  Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU  John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge  Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu- pervised learning using nonequilibrium thermodynamics. In Proceedings of The 32nd In- ternational Conference on Machine Learning, volume 37, pages 2256-2265, Lille, France, 2015. Transport Analysis of Infinitely Deep Neural Network  Sho Sonoda and Noboru Murata. Sampling hidden parameters from oracle distribution. In 24th International Conference on Artificial Neural Networks, pages 539-546, Hamburg, Germany, 2014.  Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017a.  Sho Sonoda and Noboru Murata. Double continuum limit of deep neural networks.  In ICML 2017 Workshop on Principled Approaches to Deep Learning, pages 1-5, Sydney, Australia, 2017b.  Sho Sonoda and Noboru Murata. Transportation analysis of denoising autoencoders: A novel method for analyzing deep neural networks. In NIPS 2017 Workshop on Optimal Transport & Machine Learning, pages 1-10, Long Beach, USA, 2017c.  Sho Sonoda, Isao Ishikawa, Masahiro Ikeda, Kei Hagihara, Yoshihiro Sawano, Takuo Mat- Integral representation of shallow neural network that  subara, and Noboru Murata. attains the global minimum. 2018.  Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error  guarantees for multilayer neural networks. 2016.  Taiji Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, volume 84, pages 1397-1406, Playa Blanca, Lanzarote, Canary Islands, 2018.  Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. DeepFace: Closing the gap to human-level performance in face verification. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 1701-1708, Columbus, USA, 2014.  Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics,  48(4):1005-1026, 2011.  Matus Telgarsky. Benefits of depth in neural networks.  In 29th Annual Conference on  Learning Theory, pages 1-23, 2016.  Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag New  York, 2009.  Vladimir Naumovich Vapnik. Statistical Learning Theory. Wiley, 1998.  C\u00b4edric Villani. Optimal Transport: Old and New. Springer-Verlag Berlin Heidelberg, 2009.  Pascal Vincent. A connection between score matching and denoising autoencoders. Neural  Computation, 23(7):1661-1674, 2011.  Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of The 25th International Conference on Machine Learning, pages 1096-1103, Helsinki, Finland, 2008. Sonoda and Murata  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Man- zagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371- 3408, 2010.  Grace Wahba. Spline Models for Observational Data. SIAM, 1990.  Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural  Networks, 94:103-114, 2017.  Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision, pages 818-833, Zurich, Switzerland, 2014.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under- standing deep learning requires rethinking generalization. In International Conference on Learning Representations 2017, Toulon, France, 2017.  Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as Wasserstein gradient \ufb02ows. In Proceedings of The 35th International Conference on Machine Learning, volume 80, pages 5737-5746, Stockholm, Sweden, 2018. "}, "Parsimonious Online Learning with Kernels via Sparse Projections in Function Space": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-585.html", "header": "Parsimonious Online Learning with Kernels via Sparse Projections in Function Space", "author": "Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro", "time": "20(3):1\u221244, 2019.", "abstract": "Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-affordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsification based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoff between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains finite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classification on three canonical data sets: a synthetic Gaussian mixture model, the MNIST hand-written digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-off of objective function evaluation, classification performance, and complexity of the nonparametric regressor extracted by the proposed method.", "pdf_url": "http://jmlr.org/papers/volume20/16-585/16-585.pdf", "keywords": ["kernel methods", "online learning", "stochastic optimization", "supervised learning", "orthogonal matching pursuit", "nonparametric regression"], "reference": "Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.  cambridge university press, 2009.  39   Parsimonious Online Learning with Kernels  Notice that the right-hand side of (73) may be identified as the distance to the subspace HDt in (67) defined in Lemma 10 scaled by a factor of \u03b7|(cid:96)(cid:48)(ft(xt), yt)|. We may upper-bound the right-hand side of (73) as  \u03b7|(cid:96)(cid:48)(ft(xt), yt)|  (cid:13) (cid:13)\u03ba(xt, \u00b7) \u2212 [K\u22121 (cid:13)  Dt,Dt  (cid:13) \u03baDt(xt)]T \u03baDt(\u00b7) (cid:13) (cid:13)H  \u2264 \u03b7|(cid:96)(cid:48)(ft(xt), yt)|dist(\u03ba(xt, \u00b7), HDt) (74)  where we have applied (65) regarding the definition of the subspace distance on the right- hand side of (74) to replace the Hilbert-norm term. Now, when the KOMP stopping criterion is violated, i.e., (68) holds, which implies \u03b3Mt+1 \u2264 (cid:15). Therefore, the right-hand side of (74) is upper-bounded by (cid:15), which we select as (cid:15) = K\u03b73/2.  dist(\u03ba(xt, \u00b7), HDt) \u2264  \u221a  \u03b7  K  |(cid:96)(cid:48)(ft(xt), yt)|  ,  where we have divided both sides by |(cid:96)(cid:48)(ft(xt), yt)| and cancelled out a common factor of \u03b7. Observe that if (75) holds, then \u03b3Mt+1 \u2264 (cid:15) holds, but since \u03b3Mt+1 \u2265 minj \u03b3j, we may conclude that (68) is satisfied. Consequently the model order at the subsequent step does not grow Mt+1 \u2264 Mt whenever (75) is valid.  Now, let\u2019s take the contrapositive of the preceding expressions to observe that growth  in the model order (Mt+1 = Mt + 1) implies that the condition  dist(\u03ba(xt, \u00b7), HDt) >  \u221a  \u03b7  K  |(cid:96)(cid:48)(ft(xt), yt)|  holds. Therefore, each time a new point is added to the model, the corresponding kernel function is guaranteed to be at least a distance of |(cid:96)(cid:48)(ft(xt),yt)| from every other kernel function in the current model.  K  \u221a  \u03b7  By the C-Lipschitz continuity of the instantaneous loss (Assumption 2): specifically  1/|(cid:96)(cid:48)(ft(xt), yt)| \u2265 1/C, we can lower-bound the threshold condition in (76) as  (75)  (76)  (77)  \u221a  \u03b7  K  |(cid:96)(cid:48)(ft(xt), yt)|  \u221a  \u03b7  K  C  \u2265  \u03b7  Therefore, the KOMP stopping criterion is violated for the newest point whenever distinct dictionary points dk and dj for j, k \u2208 {1, . . . , Mt}, satisfy the condition (cid:107)\u03c6(dj) \u2212 \u03c6(dk)(cid:107)2 > \u221a K C . We shall now proceed in a manner similar to the proof of Theorem 3.1 in Engel et al. (2004). Since X is compact and \u03ba is continuous, the range \u03c6(X ) (where \u03c6(x) = \u03ba(x, \u00b7) for x \u2208 X ) of the kernel transformation of feature space X is compact. Therefore, the number \u221a of balls of radius \u03b4 (here, \u03b4 = K C ) needed to cover the set \u03c6(X ) is finite (see, e.g., Anthony and Bartlett (2009)). Therefore, for some finite M \u221e, if Mt = M \u221e, the left-hand side of (cid:4) (??) holds, which implies (68) is true for all t. We conclude that Mt \u2264 M \u221e for all t.  \u03b7  References  Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.  cambridge university press, 2009. Koppel, Warnell, Stump, and Ribeiro  Antoine Bordes, Seyda Ertekin, Jason Weston, and L\u00b4eon Bottou. Fast kernel classifiers with online and active learning. Journal of Machine Learning Research, 6(Sep):1579- 1619, 2005.  Antoine Bordes, L\u00b4eon Bottou, and Patrick Gallinari. Sgd-qn: Careful quasi-newton stochas- tic gradient descent. The Journal of Machine Learning Research, 10:1737-1754, 2009.  L\u00b4eon Bottou. Online algorithms and stochastic approximations.  In David Saad, editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.  S. Boyd and L. Vanderberghe. Convex Programming. Wiley, New York, NY, 2004.  P. Brodatz. Textures: A Photographic Album for Artists and Designers. Dover, 1966.  Emmanuel J Candes. The restricted isometry property and its implications for compressed  sensing. Comptes Rendus Mathematique, 346(9):589-592, 2008.  Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1-27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.  Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Informa- tion Processing Systems, pages 3041-3049, 2014.  Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646-1654, 2014.  Aymeric Dieuleveut and Francis Bach. Non-parametric stochastic approximation with large  step sizes. arXiv preprint arXiv:1408.0361, 2014.  Y. Engel, S. Mannor, and R. Meir. The kernel recursive least-squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275-2285, Aug 2004. ISSN 1053-587X. doi: 10.1109/TSP.2004.830985.  Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio. Regularization networks and support vector machines. Advances in computational mathematics, 13(1):1-50, 2000.  W. J. Fu. Penalized regressions: the bridge versus the lasso. Journal of Computational and  Graphical Statistics, 7(3):397-416, 1998.  Simon Haykin. Neural networks: A comprehensive foundation. Macmillan College Publish-  ing Company, 1994.  P. Honeine. Online kernel principal component analysis: A reduced-order model. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 34(9):1814-1826, 2012.  Thorsten Joachims and Chun-Nam John Yu. Sparse kernel svms via cutting-plane training.  Machine Learning, 76(2-3):179-193, 2009. Parsimonious Online Learning with Kernels  Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315- 323, 2013.  S Sathiya Keerthi, Olivier Chapelle, and Dennis DeCoste. Building support vector machines with reduced classifier complexity. Journal of Machine Learning Research, 7(Jul):1493- 1515, 2006.  George Kimeldorf and Grace Wahba. Some results on tchebyche\ufb03an spline functions. Jour-  nal of mathematical analysis and applications, 33(1):82-95, 1971.  J. Kivinen, A. J. Smola, and R. C. Williamson. Online Learning with Kernels.  IEEE Transactions on Signal Processing, 52:2165-2176, August 2004. doi: 10.1109/TSP.2004. 830991.  A. Koppel, J. Fink, G. Warnell, E. Stump, and A. Ribeiro. Online learning for characterizing unknown environments in ground robotic vehicle models. In Proc. Int. Conf. Intelligent Robots and Systems, volume (to appear). Daejeon, Korea, Oct 9 - 14 2016.  Alec Koppel, Garrett Warnell, Ethan Stump, and A. Ribeiro. Parsimonious online kernel learning via sparse projections in function space. In 2017 IEEE Proc. Int. Conf. Acoust. Speech Signal Process., March 5-9 2017.  Alex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E Hinton. Imagenet classification with deep In Advances in neural information processing systems,  convolutional neural networks. pages 1097-1105, 2012.  Trung Le, Tu Nguyen, Vu Nguyen, and Dinh Phung. Dual space gradient descent for online learning. In Advances in Neural Information Processing Systems, pages 4583-4591, 2016a.  Trung Le, Vu Nguyen, Tu Dinh Nguyen, and Dinh Phung. Nonparametric budgeted stochas- tic gradient descent. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 654-662, 2016b.  Yann Lecun and Corinna Cortes. The MNIST database of handwritten digits. Database,  2009. URL http://yann.lecun.com/exdb/mnist/.  T. Leung and J. Malik. Representing and Recognizing the Visual Appearence of Materials using Three-dimensional Textons. International Journal of Computer Vision, 43(1):29- 44, 1999.  Jun-Bao Li, Shu-Chuan Chu, and Jeng-Shyang Pan. Kernel Learning Algorithms for Face  Recognition. Springer, 2014.  Weifeng Liu, Puskal P Pokharel, and Jose C Principe. The kernel least-mean-square algo-  rithm. Signal Processing, IEEE Transactions on, 56(2):543-554, 2008.  Jing Lu, Steven CH Hoi, Jialei Wang, Peilin Zhao, and Zhi-Yong Liu. Large scale online kernel learning. The Journal of Machine Learning Research, 17(1):1613-1655, 2016. Koppel, Warnell, Stump, and Ribeiro  Julien Mairal, Julien Mairal, Michael Elad, Michael Elad, Guillermo Sapiro, and Guillermo Sapiro. Sparse representation for color image restoration. In the IEEE Trans. on Image Processing, pages 53-69. ITIP, 2007.  Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic bfgs algorithm. Signal  Processing, IEEE Transactions on, 62(23):6089-6104, 2014.  Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. Journal of Machine Learning Research, 16:3151-3181, 2015. URL http://jmlr.org/ papers/v16/mokhtari15a.html.  Shayan Mukherjee and Shree K Nayar. Automatic generation of rbf networks using wavelets.  Pattern Recognition, 29(8):1369-1383, 1996.  Klaus-Robert M\u00a8uller, T\u00a8ulay Adali, Kenji Fukumizu, Jose C. Principe, and Sergios Theodor- idis. Special issue on advances in kernel-based learning for signal processing [from the guest editors]. IEEE Signal Process. Mag., 30(4):14-15, 2013. doi: 10.1109/MSP.2013. 2253031. URL http://dx.doi.org/10.1109/MSP.2013.2253031.  K. Murphy. Machine Learning: A Probabilistic Perspective. MIT press, 2012.  Deanna Needell, Joel Tropp, and Roman Vershynin. Greedy signal recovery review.  In Signals, Systems and Computers, 2008 42nd Asilomar Conference on, pages 1048-1050. IEEE, 2008.  Jorge Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of  Computation, 35(151):773-773, 1980. ISSN 0025-5718.  Vladimir Norkin and Michiel Keyzer. On stochastic optimization and statistical learning in reproducing kernel hilbert spaces by support vector machines (svm). Informatica, 20(2): 273-292, 2009.  Y. Pati, R. Rezaiifar, and P.S. Krishnaprasad. Orthogonal Matching Pursuit: Recursive Function Approximation with Applications to Wavelet Decomposition. In Proceedings of the Asilomar Conference on Signals, Systems and Computers, 1993.  Massimiliano Pontil. A note on di\ufb00erent covering numbers in learning theory. Journal of  Complexity, 19(5):665-671, 2003.  Massimiliano Pontil, Yiming Ying, and Ding xuan Zhou. Error analysis for online gradient descent algorithms in reproducing kernel hilbert spaces. Technical report, University College London, 2005.  Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.  In  Advances in neural information processing systems, pages 1177-1184, 2008.  Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing min- imization with randomization in learning. In Advances in neural information processing systems, pages 1313-1320, 2009. Parsimonious Online Learning with Kernels  C\u00b4edric Richard, Jos\u00b4e Carlos M Bermudez, and Paul Honeine. Online prediction of time series  data with kernels. IEEE Transactions on Signal Processing, 57(3):1058-1067, 2009.  Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math.  Statist., 22(3):400-407, 09 1951. doi: 10.1214/aoms/1177729586.  Geo\ufb00rey Sampson, Robin Haigh, and Eric Atwell. Natural language analysis by stochastic optimization: A progress report on project april. J. Exp. Theor. Artif. Intell., 1(4): 271-287, October 1990. ISSN 0952-813X. doi: 10.1080/09528138908953710. URL http: //dx.doi.org/10.1080/09528138908953710.  Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochas-  tic average gradient. arXiv preprint arXiv:1309.2388, 2013.  Bernhard Sch\u00a8olkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. Subseries of Lecture Notes in Computer Science Edited by JG Carbonell and J. Siekmann, page 416, 2001.  Nicol N Schraudolph, Jin Yu, and Simon G\u00a8unter. A stochastic quasi-newton method for In International Conference on Artificial Intelligence and  online convex optimization. Statistics, pages 436-443, 2007.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635- 2670, 2010.  Konstantinos Slavakis, Pantelis Bouboulis, and Sergios Theodoridis. Online learning in reproducing kernel hilbert spaces. Signal Processing Theory and Machine Learning, pages 883-987, 2013.  Victor Solo and X. Kong. Adaptive Signal Processing Algorithms: Stability and Perfor- mance. Prentice-Hall information and system sciences series. Prentice Hall, 1995. ISBN 9780135012635. URL https://books.google.com/books?id=3AkfAQAAIAAJ.  Murat Ta\u00b8san, Gabriel Musso, Tong Hao, Marc Vidal, Calum A MacRae, and Frederick P Roth. selecting causal genes from genome-wide association studies via functionally co- herent subnetworks. Nature methods, 2014.  Ambuj Tewari and Peter L. Bartlett. Learning theory. In Paulo S.R. Diniz, Johan A.K. Suykens, Rama Chellappa, and Sergios Theodoridis, editors, Academic Press Library in Signal Processing: Volume 1 Signal Processing Theory and Machine Learning, volume 1 of Academic Press Library in Signal Processing, chapter 14, pages 775-816. Elsevier, 2014. URL http://dx.doi.org/10.1016/B978-0-12-396502-8.00014-0.  Sergios Theodoridis. Machine learning: a Bayesian and optimization perspective. Academic  P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning, 48(1):165-187,  Press, 2015.  2002. Koppel, Warnell, Stump, and Ribeiro  Zhuang Wang and Slobodan Vucetic. Online passive-aggressive algorithms on a budget. In  International Conference on Artificial Intelligence and Statistics, 2010a.  Zhuang Wang, Koby Crammer, and Slobodan Vucetic. Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training. The Journal of Machine Learning Research, 13(1):3103-3131, 2012.  R. Wheeden, R.L. Wheeden, and A. Zygmund. Measure and Integral: An Introduction to Real Analysis. Chapman & Hall/CRC Pure and Applied Mathematics. Taylor & Fran- cis, 1977. ISBN 9780824764999. URL https://books.google.com/books?id=YDkDmQ_ hdmcC.  Christopher KI Williams and Matthias Seeger. Using the nystr\u00a8om method to speed up kernel machines. In Advances in neural information processing systems, pages 682-688, 2001.  Y. Ying and D. X. Zhou. Online regularized classification algorithms. IEEE Transactions on Information Theory, 52(11):4775-4788, Nov 2006. ISSN 0018-9448. doi: 10.1109/TIT. 2006.883632.  Kai Zhang, Ivor W Tsang, and James T Kwok. Improved nystr\u00a8om low-rank approxima- tion and error analysis. In Proceedings of the 25th international conference on Machine learning, pages 1232-1239. ACM, 2008.  Lijun Zhang, Jinfeng Yi, Rong Jin, Ming Lin, and Xiaofei He. Online kernel learning with  a near optimal sparsity bound. In ICML (3), pages 621-629, 2013.  Peilin Zhao and Steven Hoi. Bduol: Double updating online learning on a fixed budget.  Machine Learning and Knowledge Discovery in Databases, pages 810-826, 2012.  Shilei Zhao, Peng Wu, and Yupeng Liu. An online kernel learning algorithm based on  orthogonal matching pursuit. Journal of Software, 7(Sep):2076-2082, 2012.  Ding-Xuan Zhou. The covering number in learning theory. Journal of Complexity, 18(3):  739-767, 2002.  J. Zhu and T. Hastie. Kernel Logistic Regression and the Import Vector Machine. Journal  of Computational and Graphical Statistics, 14(1):185-205, 2005.  Zeyuan Allen Zhu, Weizhu Chen, Gang Wang, Chenguang Zhu, and Zheng Chen. P- packsvm: Parallel primal gradient descent kernel svm. In Data Mining, 2009. ICDM\u201909. Ninth IEEE International Conference on, pages 677-686. IEEE, 2009.  Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301-320, 2005. "}, "Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-588.html", "header": "Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations", "author": "Cl\u00c3\u00a9ment Bouttier, Ioana Gavra", "time": "20(4):1\u221245, 2019.", "abstract": "In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.", "pdf_url": "http://jmlr.org/papers/volume20/16-588/16-588.pdf", "keywords": ["gence Rate", "Aircraft Trajectory Optimization"], "reference": "E. Aarts and J. Korst. Simulated annealing and Boltzmann machines. New York, NY; John  Wiley and Sons Inc., 1988.  Emile HL Aarts and Jan HM Korst. Boltzmann machines for travelling salesman problems.  European Journal of Operational Research, 39(1):79-95, 1989.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed  bandit problem. Machine learning, 47(2-3):235-256, 2002.  43   Convergence rate of a noisy simulated annealing  Appendix B. Definition of m(cid:63)  In this section we prove that the definition of m(cid:63), i.e. Equation (4), is equivalent to the definition provided by Holley and Stroock (1988). (cid:27)  (cid:27)  (cid:26)  (cid:26)  Lemma 3 Let, m(cid:63)  HS := max x,y\u2208E  min p\u2208Pxy  max z\u2208p  J(z)  J(y)  J(x) + minu J(u)  .  \u2212  \u2212  Then  m(cid:63) = m(cid:63)  HS  (cid:27)  (cid:26)  u  Proof. Let x, y  E and denote Hxy := min p\u2208Pxy  max z\u2208p  J(z)  .  \u2208  First it can be noticed that if x is a global minimum of J then we have  Hx,y  J(y)  J(x) + min  J(u) = Hx,y  J(y)  (40)  \u2212  \u2212  \u2212  Thus m(cid:63)  HS \u2265  Hx(cid:63),y  \u2212  J(y) for any y in E, where x(cid:63) is a global minimum of J.  Recall that m(cid:63) = max  max (J(y), J(x)) . As the set of paths going from x to } y containing a global minimum x(cid:63) is a subset of the paths going from x to y, we have:  x,y\u2208E {  Hxy  \u2212  Let x, y  E such that m(cid:63) = Hxy  max (J(y), J(x)),  \u2208  \u2212  Hxy  max (Hx(cid:63)x, Hx(cid:63)y)  \u2264  On the other hand, as  E, we have  min (J(y), J(x)) + minu J(u)  0, so  \u2264  m(cid:63)  \u2264  \u2264  max (Hx(cid:63)x, Hx(cid:63)y) max (Hx(cid:63)x m(cid:63)  \u2212  HS  \u2212 J(x), Hx(cid:63)y  max (J(y), J(x))  J(y))  \u2212  \u2264 x, y  \u2200 J(y)  \u2208  \u2212 J(u)  Hxy  J(x) + min  Hxy  max (J(y), J(x))  \u2212  \u2212 m(cid:63), which completes the proof.  \u2264  u  \u2212  This implies m(cid:63)  HS \u2264  References  E. Aarts and J. Korst. Simulated annealing and Boltzmann machines. New York, NY; John  Wiley and Sons Inc., 1988.  Emile HL Aarts and Jan HM Korst. Boltzmann machines for travelling salesman problems.  European Journal of Operational Research, 39(1):79-95, 1989.  Jean-Yves Audibert and S\u00b4ebastien Bubeck. Best arm identification in multi-armed bandits.  In COLT-23th Conference on Learning Theory-2010, pages 13-p, 2010.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed  bandit problem. Machine learning, 47(2-3):235-256, 2002. Bouttier and Gavra  Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and geometry of Markov  di\ufb00usion operators, volume 348. Springer Science & Business Media, 2013.  John T. Betts. Survey of numerical methods for trajectory optimization. Journal of  Guidance,Control, and Dynamics, 21(2):193-207, March 1998.  St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. Concentration inequalities: A  nonasymptotic theory of independence. Oxford university press, 2013.  J. Branke, S. Meisel, and C. Schmidt. Simulated annealing in the presence of noise. Journal  of Heuristics, 14(6):627-654, 2008.  S\u00b4ebastien Bubeck, R\u00b4emi Munos, Gilles Stoltz, and Csaba Szepesv\u00b4ari. X-armed bandits.  Journal of Machine Learning Research, 12:1587-1627, 2011.  Adam D Bull. Convergence rates of e\ufb03cient global optimization algorithms. The Journal  of Machine Learning Research, 12:2879-2904, 2011.  Adam D Bull et al. Adaptive-treed bandits. Bernoulli, 21(4):2289-2307, 2015.  Laurence Charles Ward Dixon and Giorgio Philip Szeg\u00a8o. Towards global optimisation 2.  North-Holland Amsterdam, 1978.  T.MA. Fink. Inverse protein folding, hierarchical optimisation and tie knots. PhD thesis,  University of Cambridge, 1998.  S. B. Gelfand and S. K. Mitter. Simulated annealing with noisy or imprecise energy ISSN 0022-3239. doi:  measurements. 10.1007/BF00939629. URL http://dx.doi.org/10.1007/BF00939629.  J. Optim. Theory Appl., 62(1):49-62, 1989.  W.J. Gutjahr and G.Ch. P\ufb02ug. Simulated annealing for noisy cost functions. Journal of  Global Optimization, 8(1):1-13, 1996.  B. Hajek. Cooling schedules for optimal annealing. Mathematics of operations research, 13  (2):311-329, 1988.  R. Holley and D. Stroock. Simulated annealing via Sobolev inequalities. Comm. Math. ISSN 0010-3616. URL http://projecteuclid.org/  Phys., 115(4):553-569, 1988. euclid.cmp/1104161084.  Tito Homem-de Mello. Variable-sample methods and simulated annealing for discrete  stochastic optimization. 2000.  Reiner Horst and Panos M Pardalos. Handbook of global optimization, volume 2. Springer  Science & Business Media, 2013.  Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization without the lipschitz constant. Journal of Optimization Theory and Applications, 79(1): 157-181, 1993. Convergence rate of a noisy simulated annealing  Donald R Jones, Matthias Schonlau, and William J Welch. E\ufb03cient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.  John DC Little, Katta G Murty, Dura W Sweeney, and Caroline Karel. An algorithm for  the traveling salesman problem. Operations research, 11(6):972-989, 1963.  R\u00b4emi Munos. From bandits to monte-carlo tree search: The optimistic principle applied to  optimization and planning. 2014.  Arkadi Nemirovski, D-B Yudin, and E-R Dawson. Problem complexity and method e\ufb03-  ciency in optimization. 1982.  Michael Woodroofe. Normal approximation and large deviations for the robbins-monro  process. Probability Theory and Related Fields, 21(4):329-338, 1972. "}, "Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-607.html", "header": "Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression", "author": "Han Chen, Garvesh Raskutti, Ming Yuan", "time": "20(5):1\u221237, 2019.", "abstract": "In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for non-convex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set $\\Theta$ in terms of its localized Gaussian width (due to Gaussian design). We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical error bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen.", "pdf_url": "http://jmlr.org/papers/volume20/16-607/16-607.pdf", "keywords": [""], "reference": "Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation  with norm regularization. Technical Report arXiv:1505.02294, November 2015.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals  of Statistics, 33(4):1497-1537, 2005.  S. Basu and G. Michailidis. Regularized estimation in sparse high-dimensional time series  models. Annals of Statistics, 43(4):1535-1567, 2015.  35   Projected Low-rank Tensor Regression  Therefore for all Y \u2208 \u03983(r(1)  1 , r(1)  2 , r(1) 3 )  (cid:107) (cid:98)P  \u03983(r(2)  1 ,r(2)  2 ,r(2) 3 )  (Z) \u2212 Z(cid:107)F = (cid:107) (cid:98)P  \u03983(r(2)  1 ,r(2)  2 ,r(2) 3 )  (Z) \u2212 Z(2)(cid:107)F + (cid:107)Z(2) \u2212 Z(1)(cid:107)F + (cid:107)Z(1) \u2212 Z(cid:107)F  \u2264 [(\u03b21 + 1)(\u03b22 + 1)(\u03b23 + 1) \u2212 1](cid:107)Y \u2212 Z(cid:107)F.  Lemma 2 guarantees the approximate projection (cid:98)P\u03983(r1,r2,r3) fulfills the contractive projection property CPP(\u03b4) with \u03b4 = 7. And hence via setting t1 = (r(cid:48) 3) and t0 = (r(cid:48) 3 + r3), Theorem 3 directly implies the linear convergence rate re- sult with statistical error of order n\u22121/2wG[\u03983(t0) \u2229 BF(1)]. To upper bound the Gaussian width, we define the following nuclear norms:  1 + r1, r(cid:48)  2 + r2, r(cid:48)  2, r(cid:48)  1, r(cid:48)  where 1 \u2264 i \u2264 3 and (cid:107).(cid:107)\u2217 is the standard nuclear norm. Then it clearly follows that  R(i)(A) = (cid:107)Mi(A)(cid:107)\u2217,  \u03983(t0) \u2229 BF(1) \u2282 \u22293  BR(i)(  i=1  r(cid:48) i + ri).  (cid:113)  Lemma 5 in Raskutti and Yuan (2015) then implies that  wG[\u03983(t0) \u2229 BF(1)] \u2264 wG[\u2229iBR(i)(  (cid:113)  r(cid:48) i + ri)] (cid:113)  r(cid:48) i + ri)]  wG[BR(i)( (cid:113)  i + riwG[BR(i)(1)] r(cid:48)  \u2264 min  i  \u2264 min  i (cid:113)  which completes the proof.  \u2264  6 min{(r(cid:48)  1 + r1)(d1 + d2d3), (r(cid:48)  2 + r2)(d2 + d1d3), (r(cid:48)  3 + r3)(d3 + d1d2)}  Acknowledgement. Han Chen and Ming Yuan was supported in part by NSF FRG Grant DMS-1265202, and NIH Grant 1-U54AI117924-01. Ming Yuan was also supported by NSF Grant DMS-1803450 Garvesh Raskutti was supported in part by NSF Grants DMS- 1407028, DMS-1811767, NIH Grant R01 GM131381-01, ARO Grant W911NF-17-1-0357 and NGA Grant HM0476-17-1-2003.  References  Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation  with norm regularization. Technical Report arXiv:1505.02294, November 2015.  P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals  of Statistics, 33(4):1497-1537, 2005.  S. Basu and G. Michailidis. Regularized estimation in sparse high-dimensional time series  models. Annals of Statistics, 43(4):1535-1567, 2015. Chen, Raskutti, Yuan  P. Buhlmann and S. van de Geer. Statistical for High-Dimensional Data. Springer Series  in Statistics. Springer, New York, 2011.  V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex algebraic geometry of linear inverse problems. Foundations of Computational Mathematics, 12: 805-849, 2012.  S. Cohen and M. Collins. Tensor decomposition for fast parsing with latent-variable pcfgs.  In Advances in Neural Information Processing Systems, 2012.  J. Fan and R.Z. Li. Variable selection via nonconcave penalized likelihood and its oracle  properties. Journal of American Statistical Association, (32):407-499, 2001.  S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n rank tensor recovery via  convex optimization. Inverse Problems, 27, 2011.  Y. Gordon. On milmans inequality and random subspaces which escape through a mesh in Rn. Geometric aspects of functional analysis, Israel Seminar 1986-87, Lecture Notes, 1317:84-106, 1988.  T. Hastie, R. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity: The Lasso and Generalizations. Monographs on Statistics and Applied Probability 143. CRC Press, New York, 2015.  P. Jain, A. Tewari, A. Nanopoulos, and P. Kar. On iterative hard thresholding methods for  high-dimensional m-estimation. In Proceedings of NIPS, 2014.  P. Jain, N. Rao, and I. Dhillon. Structured sparse regression via greedy hard-thresholding.  Technical Report arXiv:1602.06042, February 2016.  T. G. Koldar and B. W. Bader. Tensor decompositions and applications. SIAM Review,  51:455-500, 2009.  N. Li and B. Li. Tensor completion for on-board compression of hyperspectral images. In 17th IEEE International Conference on Image Processing (ICIP), pages 517-520, 2010.  P. Loh and M. J. Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima. Journal of Machine Learning Research, 15:559-616, 2015.  Shahar Mendelson. Upper bounds on product and multiplier empirical processes. Technical  Report arXiv:1410.8003, Technion, I.I.T, 2014.  C. Mu, B. Huang, J. Wright, and D. Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In International Conference on Machine Learning, 2014.  S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high- dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, 2012. Projected Low-rank Tensor Regression  G. Raskutti and M. Yuan. Convex regularization for high-dimensional tensor regression. Technical Report arXiv:1512.01215, University of Wisconsin-Madison, December 2015.  G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.  O. Semerci, N. Hao, M. Kilmer, and E. Miller. Tensor based formulation and nuclear norm regularizatin for multienergy computed tomography. IEEE Transactions on Image Processing, 23:1678-1693, 2014.  N.D. Sidiropoulos and N. Nion. Tensor algebra and multi-dimensional harmonic retrieval in signal processing for mimo radar. IEEE Transactions on Signal Processing, 58:5693-5705, 2010.  R. Tomioka, T. Suzuki, K. Hayashi, and H. Kashima. Statistical performance of convex In Advances in Neural Information Processing Systems, pages  tensor decomposition. 972-980, 2013.  M. Yuan and C-H. Zhang. On tensor completion via nuclear norm minimization. Foundation  of Computational Mathematics, to appear, 2014. "}, "Scalable Approximations for Generalized Linear Problems": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-279.html", "header": "Scalable Approximations for Generalized Linear Problems", "author": "Murat Erdogdu, Mohsen Bayati, Lee H. Dicker", "time": "20(7):1\u221245, 2019.", "abstract": "In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical risk minimization may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations $n$ is much larger than the dimension of the parameter $p$ ($n \\gg p \\gg 1$). We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a quadratic convergence rate, and that are computationally cheaper than any batch optimization algorithm by at least a factor of $\\mathcal{O}(p)$. We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.", "pdf_url": "http://jmlr.org/papers/volume20/17-279/17-279.pdf", "keywords": ["Generalized Linear Problems", "Stochastic optimization", "Subsampling", "Dimen sion reduction in optimization."], "reference": "Pierre Alquier and G\u00b4erard Biau. Sparse single-index model. Journal of Machine Learning  Research, 14(Jan):243-280, 2013.  Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-  energy physics with deep learning. Nature communications, 5, 2014.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. IEEE Trans-  actions on Information Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Murat A Erdogdu, and Andrea Montanari. Estimating lasso risk and noise  level. In Advances in Neural Information Processing Systems, pages 944-952, 2013.  Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press,  1995.  Press, 2004.  Jock A. Blackard and Denis J. Dean. Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables. Comput. Electron. Agr., 24:131-151, 1999.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  David R Brillinger. A generalized linear model with \u201dGaussian\u201d regressor variables. In A  Festschrift For Erich L. Lehmann, pages 97-114. CRC Press, 1982.  Charles G Broyden. The convergence of a class of double-rank minimization algorithms 2.  the new algorithm. IMA Journal of Applied Mathematics, 6(3):222-231, 1970.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability  estimation and classification: Structure and applications. 2005.  Louis H. Y. Chen, Larry Goldstein, and Qi-Man Shao. Normal approximation by Stein\u2019s  method. Springer, 2010.  Sheng Chen and Arindam Banerjee. Robust structured estimation with single-index models.  In International Conference on Machine Learning, pages 712-721, 2017.  Paramveer Dhillon, Yichao Lu, Dean P Foster, and Lyle Ungar. New subsampling algo- rithms for fast least squares regression. In Advances in Neural Information Processing Systems, pages 360-368, 2013.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914- 18919, 2009.  42   Erdogdu, Bayati, Dicker  References  Pierre Alquier and G\u00b4erard Biau. Sparse single-index model. Journal of Machine Learning  Research, 14(Jan):243-280, 2013.  Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-  energy physics with deep learning. Nature communications, 5, 2014.  Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.  Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. IEEE Trans-  actions on Information Theory, 58(4):1997-2017, 2012.  Mohsen Bayati, Murat A Erdogdu, and Andrea Montanari. Estimating lasso risk and noise  level. In Advances in Neural Information Processing Systems, pages 944-952, 2013.  Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press,  1995.  Press, 2004.  Jock A. Blackard and Denis J. Dean. Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables. Comput. Electron. Agr., 24:131-151, 1999.  Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University  David R Brillinger. A generalized linear model with \u201dGaussian\u201d regressor variables. In A  Festschrift For Erich L. Lehmann, pages 97-114. CRC Press, 1982.  Charles G Broyden. The convergence of a class of double-rank minimization algorithms 2.  the new algorithm. IMA Journal of Applied Mathematics, 6(3):222-231, 1970.  Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability  estimation and classification: Structure and applications. 2005.  Louis H. Y. Chen, Larry Goldstein, and Qi-Man Shao. Normal approximation by Stein\u2019s  method. Springer, 2010.  Sheng Chen and Arindam Banerjee. Robust structured estimation with single-index models.  In International Conference on Machine Learning, pages 712-721, 2017.  Paramveer Dhillon, Yichao Lu, Dean P Foster, and Lyle Ungar. New subsampling algo- rithms for fast least squares regression. In Advances in Neural Information Processing Systems, pages 360-368, 2013.  David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914- 18919, 2009. Scalable Approximations for Generalized Linear Problems  Petros Drineas, Michael W Mahoney, S Muthukrishnan, and Tam\u00b4as Sarl\u00b4os. Faster least  squares approximation. Numerische Mathematik, 117(2):219-249, 2011.  Naihua Duan and Ker-Chau Li. Slicing regression: a link-free regression method. The  Annals of Statistics, pages 505-530, 1991.  Murat A Erdogdu. Newton-stein method: A second order method for glms via stein\u2019s lemma. In Advances in Neural Information Processing Systems, pages 1216-1224, 2015.  Murat A. Erdogdu. Newton-stein method: an optimization method for glms via stein\u2019s  lemma. The Journal of Machine Learning Research, 17(1):7565-7616, 2016.  Murat A Erdogdu. Stein\u2019s lemma and subsampling in large-scale optimization. PhD thesis,  Stanford University, 2017.  Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton meth- ods. In Advances in Neural Information Processing Systems, pages 3052-3060, 2015.  Murat A Erdogdu, Mohsen Bayati, and Lee H Dicker. Scaled least squares estimator for In Advances in Neural Information Processing Systems,  glms in large-scale problems. 2016.  Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals  Eugenic, 7:179-188, 1936.  (3):317-322, 1970.  Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13  Donald Goldfarb. A family of variable-metric methods derived by variational means. Math-  ematics of computation, 24(109):23-26, 1970.  Larry Goldstein.  l1 bounds in normal approximation. Annals Probability, 35:1888-1930,  2007.  Larry Goldstein and Gesine Reinert. Stein\u2019s method and the zero bias transformation with application to simple random sampling. Annals of Applied Probability, 7:935-952, 1997.  Jackson Gorham and Lester Mackey. Measuring sample quality with stein\u2019s method. In  Advances in Neural Information Processing Systems, pages 226-234, 2015.  Peter Hall and Ker-Chau Li. On almost linearity of low dimensional projections from high  dimensional data. The annals of Statistics, pages 867-889, 1993.  David P Helmbold, Jyrki Kivinen, and Manfred K Warmuth. Relative loss bounds for single  neurons. IEEE Transactions on Neural Networks, 10(6):1291-1304, 1999.  Magnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear  systems\u2019. Journal of Research of the National Bureau of Standards, 49(6), 1952.  Marian Hristache, Anatoli Juditsky, and Vladimir Spokoiny. Direct estimation of the index  coe\ufb03cient in a single-index model. Annals of Statistics, pages 595-623, 2001. Erdogdu, Bayati, Dicker  Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. E\ufb03cient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927-935, 2011.  Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.  Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Tech-  niques. MIT press, 2009.  Bing Li and Yuexiao Dong. Dimension reduction for nonelliptically distributed predictors.  The Annals of Statistics, pages 1272-1298, 2009.  Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American  Statistical Association, 86(414):316-327, 1991.  Ker-Chau Li and Naihua Duan. Regression analysis under link violation. Annals of Statis-  tics, 17:1009-1052, 1989.  Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pages 2370- 2378, 2016.  Qiang Liu, Jason D Lee, and Michael I Jordan. A kernelized stein discrepancy for goodness-  of-fit tests and model evaluation. arXiv preprint arXiv:1602.03253, 2016.  James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th  International Conference on Machine Learning (ICML-10), pages 735-742, 2010.  Peter McCullagh and John A. Nelder. Generalized Linear Models. Chapman and Hall, 2nd  edition, 1989.  John A Nelder and R. Jacob Baker. Generalized linear models. Wiley Online Library, 1972.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate  O(1/k2). Soviet Math. Dokl., 27:372-376, 1983.  Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer,  2004.  Christopher C Paige and Michael A Saunders. Solution of sparse indefinite systems of linear  equations. SIAM journal on numerical analysis, 12(4):617-629, 1975.  Mert Pilanci and Martin J Wainwright. Newton sketch: A linear-time optimization algo-  rithm with linear-quadratic convergence. arXiv preprint arXiv:1505.02250, 2015.  Yaniv Plan and Roman Vershynin. The generalized lasso with non-linear observations.  IEEE Transactions on information theory, 62(3):1528-1537, 2016.  Mark D Reid and Robert C Williamson. Composite binary losses. Journal of Machine  Learning Research, 11(Sep):2387-2422, 2010. Scalable Approximations for Generalized Linear Problems  Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-squares regression. Proceedings of the National Academy of Sciences, 105(36):13212- 13217, 2008.  Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods i: glob-  ally convergent algorithms. arXiv preprint arXiv:1601.04737, 2016a.  Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii: Local  convergence rates. arXiv preprint arXiv:1601.04738, 2016b.  Mark J Schervish. A general method for comparing probability assessors. The Annals of  Statistics, pages 1856-1879, 1989.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic  convex optimization. In Conference on Learning Theory (COLT), 2009.  David F Shanno. Conditioning of quasi-newton methods for function minimization. Math-  ematics of computation, 24(111):647-656, 1970.  Weijie Su and Emmanuel Candes. Slope is adaptive to unknown sparsity and asymptotically  minimax. The Annals of Statistics, 44(3):1038-1068, 2016.  Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Lasso with non-linear measure- ments is equivalent to one with linear measurements. In Advances in Neural Information Processing Systems, pages 3420-3428, 2015.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2010.  arXiv:1011.3027.  Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and  variational inference. Foundations and Trends in Machine Learning, 1:1-305, 2008. "}, "Forward-Backward Selection with Early Dropping": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-334.html", "header": "Forward-Backward Selection with Early Dropping", "author": "Giorgos Borboudakis, Ioannis Tsamardinos", "time": "20(8):1\u221239, 2019.", "abstract": "Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive performance. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about 1-2 orders of magnitude, while selecting fewer or the same number of variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists.", "pdf_url": "http://jmlr.org/papers/volume20/17-334/17-334.pdf", "keywords": ["Feature Selection", "Forward Selection", "Markov Blanket Discovery", "Bayesian Networks", "Maximal Ancestral Graphs"], "reference": "Alan Agresti. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley-  Interscience, 2nd edition, 2002.  Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle.  In Second International Symposium on Information Theory, pages 267-281, 1973.  Constantin F. Aliferis, Ioannis Tsamardinos, and Alexander Statnikov. HITON: a novel Markov blanket algorithm for optimal variable selection. In AMIA Annual Symposium Proceedings, volume 2003, page 21. American Medical Informatics Association, 2003.  Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D. Koutsoukos. Local causal and Markov blanket induction for causal discov- ery and feature selection for classification part i: Algorithms and empirical evaluation. Journal of Machine Learning Research, 11(Jan):171-234, 2010.  Thomas Blumensath and Mike E. Davies. On the di\ufb00erence between Orthogonal Matching Pursuit and Orthogonal Least Squares. Technical report, University of Edinburgh, 2007.  Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.  Gavin Brown, Adam Pocock, Ming-Jie Zhao, and Mikel Luj\u00b4an. Conditional likelihood maximisation: A unifying framework for information theoretic feature selection. Journal of Machine Learning Research, 13:27-66, January 2012.  Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines.  ACM transactions on intelligent systems and technology, 2(3):27, 2011.  Jiahua Chen and Zehua Chen. Extended Bayesian information criteria for model selection  with large model spaces. Biometrika, 95(3):759-771, 2008.  Sheng Chen, Stephen A Billings, and Wan Luo. Orthogonal least squares methods and their application to non-linear system identification. International Journal of control, 50 (5):1873-1896, 1989.  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):  273-297, 1995.  Samuel A. Danziger, S. Joshua Swamidass, Jue Zeng, Lawrence R. Dearth, Qiang Lu, Jonathan H. Chen, Jianlin Cheng, Vinh P. Hoang, Hiroto Saigo, Ray Luo, et al. Func- tional census of mutation sequence spaces: the example of p53 cancer rescue mutants. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 3(2):114-125, 2006.  Geo\ufb00rey M. Davis, Stephane G. Mallat, and Zhifeng Zhang. Adaptive time-frequency  decompositions. Optical engineering, 33(7):2183-2192, 1994.  Thomas G. Dietterich, Ajay N. Jain, Richard H. Lathrop, and Tomas Lozano-Perez. A com- parison of dynamic reposing and tangent distance for drug activity prediction. Advances in Neural Information Processing Systems, pages 216-216, 1994.  35   Forward-Backward Selection with Early Dropping  References  Alan Agresti. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley-  Interscience, 2nd edition, 2002.  Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle.  In Second International Symposium on Information Theory, pages 267-281, 1973.  Constantin F. Aliferis, Ioannis Tsamardinos, and Alexander Statnikov. HITON: a novel Markov blanket algorithm for optimal variable selection. In AMIA Annual Symposium Proceedings, volume 2003, page 21. American Medical Informatics Association, 2003.  Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D. Koutsoukos. Local causal and Markov blanket induction for causal discov- ery and feature selection for classification part i: Algorithms and empirical evaluation. Journal of Machine Learning Research, 11(Jan):171-234, 2010.  Thomas Blumensath and Mike E. Davies. On the di\ufb00erence between Orthogonal Matching Pursuit and Orthogonal Least Squares. Technical report, University of Edinburgh, 2007.  Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.  Gavin Brown, Adam Pocock, Ming-Jie Zhao, and Mikel Luj\u00b4an. Conditional likelihood maximisation: A unifying framework for information theoretic feature selection. Journal of Machine Learning Research, 13:27-66, January 2012.  Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines.  ACM transactions on intelligent systems and technology, 2(3):27, 2011.  Jiahua Chen and Zehua Chen. Extended Bayesian information criteria for model selection  with large model spaces. Biometrika, 95(3):759-771, 2008.  Sheng Chen, Stephen A Billings, and Wan Luo. Orthogonal least squares methods and their application to non-linear system identification. International Journal of control, 50 (5):1873-1896, 1989.  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):  273-297, 1995.  Samuel A. Danziger, S. Joshua Swamidass, Jue Zeng, Lawrence R. Dearth, Qiang Lu, Jonathan H. Chen, Jianlin Cheng, Vinh P. Hoang, Hiroto Saigo, Ray Luo, et al. Func- tional census of mutation sequence spaces: the example of p53 cancer rescue mutants. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 3(2):114-125, 2006.  Geo\ufb00rey M. Davis, Stephane G. Mallat, and Zhifeng Zhang. Adaptive time-frequency  decompositions. Optical engineering, 33(7):2183-2192, 1994.  Thomas G. Dietterich, Ajay N. Jain, Richard H. Lathrop, and Tomas Lozano-Perez. A com- parison of dynamic reposing and tangent distance for drug activity prediction. Advances in Neural Information Processing Systems, pages 216-216, 1994. Borboudakis and Tsamardinos  Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regres-  sion. The Annals of Statistics, 32(2):407-499, 2004.  Jianqing Fan, Yang Feng, Yichao Wu, et al. High-dimensional variable selection for Cox\u2019s proportional hazards model. In Borrowing Strength: Theory Powering Applications-A Festschrift for Lawrence D. Brown, pages 70-86. Institute of Mathematical Statistics, 2010.  Yingying Fan and Cheng Yong Tang. Tuning parameter selection in high dimensional pe- nalized likelihood. Journal of the Royal Statistical Society: Series B (Statistical Method- ology), 75(3):531-552, 2013.  Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Frank Hutter, Lars Kottho\ufb00, and Joaquin Vanschoren, editors, AutoML: Methods, Sytems, Challenges, chap- ter 1, pages 3-37. Springer, December 2018. To appear.  Livio Finos, Chiara Brombin, and Luigi Salmaso. Adjusting stepwise p-values in generalized linear models. Communications in Statistics-Theory and Methods, 39(10):1832-1846, 2010.  Peter L. Flom and David L. Cassell. Stopping stepwise: Why stepwise and similar selection methods are bad, and what you should use. In NorthEast SAS Users Group Inc 20th Annual Conference, 2007.  Robert V. Foutz and R. C. Srivastava. The performance of the likelihood ratio test when  the model is incorrect. The Annals of Statistics, 5(6):1183-1194, 1977.  Andreas Groll and Gerhard Tutz. Variable selection for generalized linear mixed models by  L1-penalized estimation. Statistics and Computing, 24(2):137-154, 2014.  Max Grazier G\u2019Sell, Stefan Wager, Alexandra Chouldechova, and Robert Tibshirani. Se- quential selection procedures and false discovery rate control. Journal of the Royal Sta- tistical Society: Series B (Statistical Methodology), 78(2):423-444, 2016.  Isabelle Guyon and Andr\u00b4e Elissee\ufb00. An introduction to variable and feature selection.  Journal of machine learning research, 3(Mar):1157-1182, 2003.  Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. Result analysis of the NIPS 2003 feature selection challenge. In Advances in neural information processing systems, pages 545-552, 2004.  Isabelle Guyon, Amir Reza Sa\ufb00ari Azar Alamdari, Gideon Dror, and Joachim M. Buhmann. Performance prediction challenge. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, pages 1649-1656. IEEE, 2006a.  Isabelle Guyon, Jiwen Li, Theodor Mader, Patrick A Pletscher, Georg Schneider, and  Markus Uhr. Feature selection with the CLOP package. Technical report, 2006b.  Mazin Abdulrasool Hameed. Comparative analysis of orthogonal matching pursuit and least angle regression. Master\u2019s thesis, Michigan State University, Electrical Engineering, 2012. Forward-Backward Selection with Early Dropping  Frank Harrell. Regression Modeling Strategies. Springer, corrected edition, January 2001.  Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learn-  ing: data mining, inference and prediction. Springer, 2nd edition, 2009.  Trevor Hastie, Robert Tibshirani, and Ryan J Tibshirani. Extended comparisons of arXiv preprint  forward stepwise selection, and the lasso.  best subset selection, arXiv:1707.08692, 2017.  Jing-Shiang Hwang and Tsuey-Hwa Hu. A stepwise regression algorithm for high- dimensional variable selection. Journal of Statistical Computation and Simulation, 85 (9):1793-1806, 2015.  St\u00b4ephane Ivano\ufb00, Franck Picard, and Vincent Rivoirard. Adaptive lasso and group-lasso for functional Poisson regression. Journal of Machine Learning Research, 17(Jan):1903-1948, 2016.  George H. John, Ron Kohavi, and Karl P\ufb02eger. Irrelevant features and the subset selection In Machine learning: Proceedings of the Eleventh International Conference,  problem. pages 121-129. Morgan Kaufmann, 1994.  Markus Kalisch and Peter B\u00a8uhlmann. Estimating high-dimensional directed acyclic graphs with the PC-algorithm. Journal of Machine Learning Research, 8(Mar):613-636, 2007.  Yongdai Kim, Sunghoon Kwon, and Hosik Choi. Consistent model selection criteria on high  dimensions. Journal of Machine Learning Research, 13(Apr):1037-1057, 2012.  Michael H. Kutner, Christopher J. Nachtsheim, John Neter, and William Li. Applied Linear  Statistical Models. McGraw-Hill/Irwin, 5 edition, August 2004.  Vincenzo Lagani, Giorgos Athineou, Alessio Farcomeni, Michail Tsagris, and Ioannis Tsamardinos. Feature selection with the R package MXM: Discovering statistically equiv- alent feature subsets. Journal of Statistical Software, 80(7), 2017.  Jinchi Lv and Jun S. Liu. Model selection principles in misspecified models. Journal of the  Royal Statistical Society: Series B (Statistical Methodology), 76(1):141-167, 2014.  Dimitris Margaritis. Toward provably correct feature selection in arbitrary domains. In  Advances in Neural Information Processing Systems, pages 1240-1248, 2009.  Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighbor- hoods. In S. A. Solla, T. K. Leen, and K. M\u00a8uller, editors, Advances in Neural Information Processing Systems 12, pages 505-511. MIT Press, 2000.  Lukas Meier, Sara Van De Geer, and Peter B\u00a8uhlmann. The group Lasso for logistic re- gression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2008.  Andrew Y. Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM, 2004. Borboudakis and Tsamardinos  Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Conference Record of The Twenty-Seventh Asilomar Confer- ence on Signals, Systems and Computers, pages 40-44. IEEE, 1993.  Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.  Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.  Judea Pearl. Causality, Models, Reasoning, and Inference. Cambridge University Press,  Cambridge, U.K., 2000.  Peter Peduzzi, John Concato, Elizabeth Kemper, Theodore R. Holford, and Alvan R. Fe- instein. A simulation study of the number of events per variable in logistic regression analysis. Journal of clinical epidemiology, 49(12):1373-1379, 1996.  Junyang Qian, Ttrevor Hastie, Jerome Friedman, Rob Tibshirani, and Noah Simon. Glmnet  for Matlab, 2013.  Thomas Richardson. Markov properties for acyclic directed mixed graphs. Scandinavian  Journal of Statistics, 30(1):145-157, 2003.  Thomas Richardson and Peter Spirtes. Ancestral graph Markov models. Annals of Statistics,  pages 962-1030, 2002.  J\u00a8urg Schelldorfer, Peter B\u00a8uhlmann, and Sara Van De Geer. Estimation for high-dimensional linear mixed-e\ufb00ects models using L1-penalization. Scandinavian Journal of Statistics, 38 (2):197-214, 2011.  Gideon Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):  461-464, 1978.  MIT press, 2 edition, 2000.  Peter Spirtes, Clark N. Glymour, and Richard Scheines. Causation, prediction, and search.  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal  Statistical Society: Series B (Statistical Methodology), pages 267-288, 1996.  Ryan J. Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. Exact post-selection inference for sequential regression procedures. Journal of the American Statistical Association, 111(514):600-620, 2016.  Michail Tsagris, Vincenzo Lagani, and Ioannis Tsamardinos. Feature selection for high-  dimensional temporal data. BMC bioinformatics, 19(1):17, 2018.  Ioannis Tsamardinos and Constantin F. Aliferis. Towards principled feature selection: rel- In Proceedings of the Ninth International Workshop on  evancy, filters and wrappers. Artificial Intelligence and Statistics, 2003. Forward-Backward Selection with Early Dropping  Ioannis Tsamardinos, Constantin F. Aliferis, and Alexander Statnikov. Time and sample e\ufb03cient discovery of Markov blankets and direct causal relations. In Proceedings of the Ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 673-678. ACM, 2003a.  Ioannis Tsamardinos, Constantin F. Aliferis, and Alexander R. Statnikov. Algorithms for  large scale Markov blanket discovery. In FLAIRS conference, volume 2, 2003b.  Ioannis Tsamardinos, Giorgos Borboudakis, Pavlos Katsogridakis, Polyvios Pratikakis, and Vassilis Christophides. A greedy feature selection algorithm for Big Data of high dimen- sionality. Machine Learning, Aug 2018a.  Ioannis Tsamardinos, Elissavet Greasidou, and Giorgos Borboudakis. Bootstrapping the out-of-sample predictions for e\ufb03cient and accurate cross-validation. Machine Learning, May 2018b.  Thomas S. Verma and Judea Pearl. Causal Networks: Semantics and Expressiveness. In Proceedings, 4th Workshop on Uncertainty in Artificial Intelligence, pages 352-359, Au- gust 1988.  Eric Vittingho\ufb00 and Charles E. McCulloch. Relaxing the rule of ten events per variable in logistic and Cox regression. American journal of epidemiology, 165(6):710-718, 2007.  Quang H. Vuong. Likelihood ratio tests for model selection and non-nested hypotheses.  Econometrica: Journal of the Econometric Society, pages 307-333, 1989.  Sanford Weisberg. Applied linear regression, volume 528. John Wiley & Sons, 2005.  Halbert White. Maximum likelihood estimation of misspecified models. Econometrica, 50  (1):1-25, 1982.  Samuel S. Wilks. The large-sample distribution of the likelihood ratio for testing composite  hypotheses. The Annals of Mathematical Statistics, 9(1):60-62, March 1938.  Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00a8olkopf. Kernel-based condi- tional independence test and application in causal discovery. In Proceedings of the Twenty- Seventh Conference on Uncertainty in Artificial Intelligence, pages 804-813, 2011.  Yongli Zhang and Xiaotong Shen. Model selection procedure for high-dimensional data. Statistical Analysis and Data Mining: The ASA Data Science Journal, 3(5):350-358, 2010.  Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67:301-320, 2005. "}, "Dynamic Pricing in High-dimensions": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-357.html", "header": "Dynamic Pricing in High-dimensions", "author": "Adel Javanmard, Hamid Nazerzadeh", "time": "20(9):1\u221249, 2019.", "abstract": "We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasing decisions according to a general choice model that includes products features and customers' characteristics, encoded as $d$-dimensional numerical vectors, as well as the price offered. The parameters of the choice model are a priori unknown to the firm, but can be learned as the (binary-valued) sales data accrues over time. The firm's objective is to maximize its revenue. We benchmark the performance using the classic regret minimization framework where the regret is defined as the expected revenue loss against a clairvoyant policy that knows the parameters of the choice model in advance, and always offers the revenue-maximizing price. This setting is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We assume a structured choice model, parameters of which depend on $s_0$ out of the $d$ product features. Assuming that the market noise distribution is known, we propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of the high-dimensional model and obtains a logarithmic regret in $T$. More specifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$. Furthermore, we show that no policy can obtain regret better than $O(s_0 (\\log d + \\log T))$. {In addition, we propose a generalization of our policy to a setting that the market noise distribution is unknown but belongs to a parametrized family of distributions. This policy obtains regret of $O(\\sqrt{(\\log d)T})$. We further show that no policy can obtain regret better than $\\Omega(\\sqrt{T})$ in such environments.}", "pdf_url": "http://jmlr.org/papers/volume20/17-357/17-357.pdf", "keywords": ["Revenue Management", "Dynamic Pricing", "High-dimensional Regression", "Max imum Likelihood", "Regret", "Sparsity"], "reference": "Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conver-  sions and application to sparse stochastic bandits. In AISTATS, pages 1-9, 2012.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knap- sacks. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 989-1006, 2014.  Airbnb Documentation. Smart pricing: Set prices based on demand. https://www.airbnb.  com/help/article/1168/smart-pricing--set-prices-based-on-demand, 2015.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. In Advances in Neural Information Processing Systems, pages 622-630, 2014.  Victor F Araman and Ren\u00b4e Caldentey. Dynamic pricing for nonperishable products with  demand learning. Operations research, 57(5):1169-1188, 2009.  Moshe Babaio\ufb00, Shaddin Dughmi, Robert Kleinberg, and Aleksandrs Slivkins. Dynamic pricing with limited supply. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 74-91, 2012. ISBN 978-1-4503-1415-2.  45   Dynamic Pricing in High-dimensions  Applying triangle inequality and rearranging the terms, this yields to the following inequal- ity:  2(cid:96)F \u03c4k\u22121  (cid:107) (cid:101)X (k\u22121)(\u03b70,\u2227 \u2212 \u03b7\u2227,\u2227)(cid:107)2 \u2264 4\u03bbk(cid:107)\u03b7\u2227,\u2227 \u2212 \u03b70,\u2227(cid:107)1 \u2264 8W \u03bbk .  (145)  By substituting for \u03b70,\u2227 and \u03b7\u2227,\u2227, we get Equation (79).  Likewise, we can write (145) with \u03b7\u2227,0 instead of \u03b70,\u2227:  2(cid:96)F \u03c4k\u22121  (cid:107) (cid:101)X (k\u22121)(\u03b7\u2227,0 \u2212 \u03b7\u2227,\u2227)(cid:107)2 \u2264 8W \u03bbk .  (146)  By substituting for \u03b7\u2227,0 and \u03b70,0 we obtain  2(cid:96)F \u03c4k\u22121  2(cid:96)F \u03c4k\u22121  (\u03b20 \u2212 (cid:98)\u03b2k)2(cid:107)p(k\u22121)(cid:107)2 =  (cid:107) (cid:101)X (k\u22121)(\u03b7\u2227,0 \u2212 \u03b7\u2227,\u2227)(cid:107)2 \u2264 8W \u03bbk ,  (147)  where p(k) = (p\u03c4k\u22121, . . . , p\u03c4k\u22121)T is the vector of prices o\ufb00ered in episode (k \u2212 1). Hence, in order to prove (79), it su\ufb03ces to show that pt \u2265 \u02dccW for all t. To see this, note that since g(v) > 0 it attains its minimum over any bounded interval. Therefore, there exists a constant c(cid:48) W for v \u2208 [\u2212W, W ]. Further, by the constraint in the optimization (28) we have | (cid:98)\u03b2(k\u22121)| \u2264 W . Combining these two facts, we get  W > 0, such that g(v) > c(cid:48)  The result follows from (147) and setting cW = 4W/\u02dcc2  pt =  1 (cid:98)\u03b2(k\u22121)  g(xt \u00b7 (cid:98)\u00b5(k\u22121)) \u2265  \u2261 \u02dccW .  (148)  c(cid:48) W W  W .  References  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conver-  sions and application to sparse stochastic bandits. In AISTATS, pages 1-9, 2012.  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knap- sacks. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC \u201914, pages 989-1006, 2014.  Airbnb Documentation. Smart pricing: Set prices based on demand. https://www.airbnb.  com/help/article/1168/smart-pricing--set-prices-based-on-demand, 2015.  Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. In Advances in Neural Information Processing Systems, pages 622-630, 2014.  Victor F Araman and Ren\u00b4e Caldentey. Dynamic pricing for nonperishable products with  demand learning. Operations research, 57(5):1169-1188, 2009.  Moshe Babaio\ufb00, Shaddin Dughmi, Robert Kleinberg, and Aleksandrs Slivkins. Dynamic pricing with limited supply. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC \u201912, pages 74-91, 2012. ISBN 978-1-4503-1415-2. Javanmard and Nazerzadeh  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Sym- posium on, pages 207-216. IEEE, 2013.  Mark Bagnoli and Ted Bergstrom. Log-concave probability and its applications. Economic  theory, 26(2):445-469, 2005.  Santiago R Balseiro, Jon Feldman, Vahab Mirrokni, and S Muthukrishnan. Yield optimiza- tion of display advertising with ad exchange. Management Science, 60(12):2886-2907, 2014.  Hamsa Bastani and Mohsen Bayati. Online decision-making with high-dimensional covari-  ates. Working Paper, 2016.  Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: risk bounds and near-optimal algorithms. Operations Research, 57:1407-1420, 2009.  Sonia A Bhaskar and Adel Javanmard. 1-bit matrix completion under exact low-rank constraint. In Information Sciences and Systems (CISS), 2015 49th Annual Conference on, pages 1-6. IEEE, 2015.  Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,  2004.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning, 3(1):1-122, 2011.  Josef Broder and Paat Rusmevichientong. Dynamic pricing under a general parametric  choice model. Operations Research, 60(4):965-980, 2012.  Peter B\u00a8uhlmann and Sara van de Geer. Statistics for high-dimensional data. Springer-  Verlag, 2011.  Emmanuel Candes and Terence Tao. The dantzig selector: Statistical estimation when p is  much larger than n. The Annals of Statistics, pages 2313-2351, 2007.  Xi Chen, Zachary Owen, Clark Pixton, and David Simchi-Levi. A statistical learning  approach to personalization in revenue management. Working Paper, 2015.  Wang Chi Cheung, Will Ma, David Simchi-Levi, and Xinshang Wang. Inventory balancing  with online learning. Working Paper, 2018.  Maxime C Cohen, Ilan Lobel, and Renato Paes Leme. Feature-based dynamic pricing. ACM  Conference on Economics and Computation, 2016.  Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under  bandit feedback. In COLT, pages 355-366, 2008.  A. V. den Boer and A. P. Zwart. Mean square convergence rates for maximum(quasi)  likelihood estimation. Stochastic systems, 4:1 - 29, 2014. ISSN 1946-5238. Dynamic Pricing in High-dimensions  Arnoud V den Boer. Dynamic pricing and learning: historical origins, current research, and new directions. Surveys in operations research and management science, 20(1):1-18, 2015.  Arnoud V den Boer and Bert Zwart. Simultaneously learning and optimizing using con-  trolled variance pricing. Management Science, 60(3):770-783, 2013.  Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz.  Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords. The American economic review, 97(1):242-259, 2007.  Ossama Elshiewy, Daniel Guhl, and Yasemin Boztug. Multinomial  logit models in  marketing-from fundamentals to state-of-the-art. Marketing ZFP, 39(3):32-49, 2017.  Vivek F Farias, Srikanth Jagabathula, and Devavrat Shah. A nonparametric approach to  modeling choice with limited data. Management Science, 59(2):305-322, 2013.  Kris Johnson Ferreira, David Simchi-Levi, and He Wang. Online network revenue manage-  ment using thompson sampling. Operations research, 2018.  Dennis H Gensch and Wilfred W Recker. The multinomial, multiattribute logit choice  model. Journal of Marketing Research, pages 124-132, 1979.  Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic  Systems, 3(1):230-261, 2013.  Negin Golrezaei, Hamid Nazerzadeh, and Paat Rusmevichientong. Real-time optimization  of personalized assortments. Management Science, 60(6):1532-1551, 2014.  Negin Golrezaei, Adel Javanmard, and Vahab Mirrokni. Dynamic incentive-aware learning: Robust pricing in contextual auctions. http://dx.doi.org/10.2139/ssrn.3144034, 2018.  Peter M Guadagni and John DC Little. A logit model of brand choice calibrated on scanner  data. Marketing Science, 27(1):29-48, 2008.  J Michael Harrison, Bora Keskin, and Assaf Zeevi. Bayesian dynamic pricing policies: Learning and earning under a binary prior distribution. Management Science, 58(3): 570-586, 2012.  John R Hauser and Frank S Koppelman. Alternative perceptual mapping techniques: Rel- ative accuracy and usefulness. Journal of marketing Research, pages 495-506, 1979.  Adel Javanmard. Perishability of data: Dynamic pricing under varying-coe\ufb03cient mod- els. Journal of Machine Learning Research, 18(53):1-31, 2017. URL http://jmlr.org/ papers/v18/17-061.html.  Nathan Kallus and Madeleine Udell. Dynamic assortment personalization in high dimen-  sions. Working Paper, 2016.  Godfrey Keller and Sven Rady. Optimal experimentation in a changing environment. The  review of economic studies, 66(3):475-507, 1999. Javanmard and Nazerzadeh  Bora Keskin. Optimal dynamic pricing with demand model uncertainty: A squared-  coe\ufb03cient-of-variation rule for learning and earning. Working Paper, 2014.  Bora Keskin and Assaf Zeevi. Dynamic pricing with an unknown demand model: Asymp- totically optimal semi-myopic policies. Operations Research, 62(5):1142-1167, 2014.  N Bora Keskin and Assaf Zeevi. Chasing demand: Learning and earning in a changing  environment. Mathematics of Operations Research, 2016.  Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In Proceedings of 44th Annual IEEE Symposium on Foundations of Computer Science, pages 594-605. IEEE, 2003.  S\u00b4ebastien Lahaie and David M Pennock. Revenue analysis of a family of ranking rules for keyword auctions. In Proceedings of the 8th ACM conference on Electronic commerce, pages 50-56. ACM, 2007.  Jordan J Louviere and George Woodworth. Design and analysis of simulated consumer choice or allocation experiments: an approach based on aggregate data. Journal of marketing research, pages 350-367, 1983.  Daniel McFadden. Economic choices. American economic review, 91(3):351-378, 2001.  Roger B. Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):  58-73, 1981.  Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming.  Communications on Pure and Applied Mathematics, 66(8):1275-1297, 2013.  Stanislav P Ponomarev. Submersions and preimages of sets of measure zero. Siberian  Mathematical Journal, 28(1):153-163, 1987.  Sheng Qiang and Mohsen Bayati. Dynamic pricing with demand covariates. Working Paper,  2016.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high- dimensional linear regression over-balls. IEEE Transactions on Information Theory, 57 (10):6976-6994, 2011.  Michael Rothschild. A two-armed bandit theory of market pricing. Journal of Economic  Theory, 9(2):185-202, 1974.  Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements.  IEEE Trans. on Inform. Theory, 59(6):3434-3447, 2013.  Kalyan T Talluri and Garrett J Van Ryzin. The theory and practice of revenue management,  volume 68. Springer Science & Business Media, 2006.  A.B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. ISBN 9780387790527. URL https://books.google.com/  Springer New York, 2008. books?id=mwB8rUBsbqoC. Dynamic Pricing in High-dimensions  Sara A Van de Geer. High-dimensional generalized linear models and the lasso. The Annals  of Statistics, pages 614-645, 2008.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv  preprint arXiv:1011.3027, 2010.  Zizhuo Wang, Shiming Deng, and Yinyu Ye. Close the gaps: A learning-while-doing al- gorithm for single-product revenue management problems. Operations Research, 62(2): 318-331, 2014.  Baichun Xiao, Wei Yang, and Jun Li. Optimal reserve price for the generalized second-price auction in sponsored search advertising. Journal of Electronic Commerce Research, 10 (3):114, 2009.  Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates  of convergence. Annals of Statistics, pages 1564-1599, 1999.  Bin Yu. Assouad, fano and le, cam.  In Research Papers in Probability and Statistics:  Festschrift in Honor of Lucien Le Cam, pages 423-435. Springer-Verlag, 1997. "}, "Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-501.html", "header": "Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions", "author": "Salar Fattahi, Somayeh Sojoudi", "time": "20(10):1\u221244, 2019.", "abstract": "Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an $l_1$ regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL method---as a conic optimization problem---has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as $80,000\\times 80,000$ (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours", "pdf_url": "http://jmlr.org/papers/volume20/17-501/17-501.pdf", "keywords": ["Graphical Lasso", "Graphical Model", "Sparse Graphs", "Optimization"], "reference": "Caltrans Performance Management System (PeMS), 2017. URL http://pems.dot.ca.gov.  Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network \ufb02ows: theory,  algorithms, and applications. Pearson, 1993.  Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine Learning, 4(1): 1-106, 2012.  Onureena Banerjee, Laurent El Ghaoui, and Alexandre d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research, 9:485-516, 2008.  Steven J. Benson, Yinyu Ye, and Xiong Zhang. Solving large-scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000.  Peter B\u00a8uhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods,  theory and applications. Springer Science & Business Media, 2011.  Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling.  Inverse Problems, 23(3):969-985, 2007.  Thomas Frederick Coleman and Yuying Li, editors. Large-scale numerical optimization,  volume 46. SIAM, 1990.  41   Graphical Lasso and Thresholding  Next, we prove the inequality (19). The following chain of inequalities hold  \u2212 log det(A) + trace( \u02c6\u03a3A) + \u03bb(cid:107)A(cid:107)1,o\ufb00 = \u2212 log det(A) + trace(\u03a3A) + \u03bb(cid:107)A(cid:107)1,o\ufb00 (cid:125)  (cid:124)  (cid:123)(cid:122) f (A)  + trace(( \u02c6\u03a3 \u2212 \u03a3)A)  (a) \u2264 \u2212 log det(Sopt) + trace( \u02c6\u03a3Sopt) + \u03bb(cid:107)Sopt(cid:107)1,o\ufb00 = \u2212 log det(Sopt) + trace(\u03a3Sopt) + \u03bb(cid:107)Sopt(cid:107)1,o\ufb00 (cid:123)(cid:122) (cid:125) f \u2217  (cid:124)  + trace(( \u02c6\u03a3 \u2212 \u03a3)Sopt),  where (a) is due to the fact that A is optimal for the GL with the perturbed sample covariances. This implies that  f (A) \u2212 f \u2217 \u2264 trace(( \u02dc\u03a3 \u2212 \u03a3)(Sopt \u2212 A)) \u2264 (cid:107) \u02dc\u03a3 \u2212 \u03a3(cid:107)2((cid:107)Sopt(cid:107)2 + (cid:107)A(cid:107)2) \u2264 (cid:0)\u00b5max(A) + \u00b5max(Sopt)(cid:1) dmax(A)  (cid:18)  1 \u00b5min(A)  (cid:19)  + 1  (cid:15).  (cid:4)  References  Caltrans Performance Management System (PeMS), 2017. URL http://pems.dot.ca.gov.  Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network \ufb02ows: theory,  algorithms, and applications. Pearson, 1993.  Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine Learning, 4(1): 1-106, 2012.  Onureena Banerjee, Laurent El Ghaoui, and Alexandre d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research, 9:485-516, 2008.  Steven J. Benson, Yinyu Ye, and Xiong Zhang. Solving large-scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000.  Peter B\u00a8uhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods,  theory and applications. Springer Science & Business Media, 2011.  Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling.  Inverse Problems, 23(3):969-985, 2007.  Thomas Frederick Coleman and Yuying Li, editors. Large-scale numerical optimization,  volume 46. SIAM, 1990. Fattahi and Sojoudi  Patrick Danaher, Pei Wang, and Daniela M Witten. The joint graphical lasso for inverse covariance estimation across multiple classes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(2):373-397, 2014.  Jianqing Fan and Jinchi Lv. A selective overview of variable selection in high dimensional  feature space. Statistica Sinica, 20(1):101, 2010.  Makan Fardad, Fu Lin, and Mihailo R. Jovanovi\u00b4c. Sparsity-promoting optimal control for a class of distributed systems. American Control Conference, pages 2050-2055, 2011.  Salar Fattahi and Javad Lavaei. On the convexity of optimal decentralized control problem In American Control Conference (ACC), 2017, pages 3359-3366.  and sparsity path. IEEE, 2017.  Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Mind the duality gap: safer rules  for the lasso. arXiv preprint arXiv:1505.03410, 2015.  Lino Figueiredo, Isabel Jesus, JA Tenreiro Machado, Jose Rui Ferreira, and JL Martins De Carvalho. Towards the development of intelligent transportation systems. IEEE Intelli- gent Transportation Systems, pages 1206-1211, 2001.  Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing,  volume 1. Basel: Birkh\u00a8auser, 2013.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estima-  tion with the graphical lasso. Biostatistics, 9(3):432-441, 2008.  Jochen Garcke, Michael Griebel, and Michael Thess. Data mining with sparse grids. Com-  puting, 67(3):225-253, 2001.  Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature elimination for the lasso and sparse supervised learning problems. arXiv preprint arXiv:1009.4219, 2010.  Cho-Jui Hsieh, M\u00b4aty\u00b4as A. Sustik, Inderjit S. Dhillon, and Pradeep Ravikumar. Quic: quadratic approximation for sparse inverse covariance estimation. Journal of Machine Learning Research, 15(1):2911-2947, 2014.  Nicole Kr\u00a8amer, Juliane Sch\u00a8afer, and Anne-Laure Boulesteix. Regularized estimation of large-scale gene association networks using graphical gaussian models. BMC bioinfor- matics, 10(1):384, 2009.  Han Liu, Kathryn Roeder, and Larry Wasserman. Stability approach to regularization se- lection (stars) for high dimensional graphical models. In Advances in Neural Information Processing Systems, pages 1432-1440, 2010.  Rahul Mazumder and Trevor Hastie. Exact covariance thresholding into connected compo- nents for large-scale graphical lasso. Journal of Machine Learning Research, 13:781-794, 2012.  Shanmugavelayutham Muthukrishnan. Data streams: Algorithms and applications. Foun-  dations and Trends R(cid:13) in Theoretical Computer Science, 1(2):117-236, 2005. Graphical Lasso and Thresholding  Habibollah Nassiri and Rafegh Aghamohammadi. A new analytic neuro-fuzzy model for work zone capacity estimation. Transportation Research Board 96th Annual Meeting, 17 (06061), 2017.  Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Gap safe screening rules for sparse multi-task and multi-class models. In Advances in Neural Information Processing Systems, pages 811-819, 2015.  Lishan Qiao, Songcan Chen, and Xiaoyang Tan. Sparsity preserving projections with ap-  plications to face recognition. Pattern Recognition, 43(1):331-341, 2010.  Pradeep Ravikumar, Martin J Wainwright, Garvesh Raskutti, and Bin Yu. High- dimensional covariance estimation by minimizing l1-penalized log-determinant diver- gence. Electronic Journal of Statistics, 5:935-980, 2011.  Somayeh Sojoudi. Equivalence of graphical lasso and thresholding for sparse graphs. Journal  of Machine Learning Research, 17(115):1-21, 2016.  Somayeh Sojoudi and John Doyle. Study of the brain functional network using synthetic data. 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 350-357, 2014.  Shiliang Sun, Rongqing Huang, and Ya Gao. Network-scale tra\ufb03c modeling and forecasting with graphical lasso and neural networks. Journal of Transportation Engineering, 138(11): 1358-1367, 2012.  Robert Tibshirani, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah Simon, Jonathan Taylor, and Ryan J Tibshirani. Strong rules for discarding predictors in lasso-type prob- lems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(2): 245-266, 2012.  Lieven Vandenberghe and Martin S. Andersen. Chordal graphs and semidefinite optimiza-  tion. Foundations and Trends R(cid:13) in Optimization, 1(4):241-433, 2015.  Petra E. V\u00b4ertes, Aaron F. Alexander-Bloch, Nitin Gogtay, Jay N. Giedd, Judith L. Rapoport, and Edward T. Bullmore. Simple models of human brain functional networks. Proceedings of the National Academy of Sciences, 109(15):5868-5873, 2012.  Daniela M. Witten, Jerome H. Friedman, and Noah Simon. New insights and faster com- putations for the graphical lasso. Journal of Computational and Graphical Statistics, 20 (4):892-900, 2011.  John Wright, Yi Ma, Julien Mairal, Guillermo Sapiro, Thomas S. Huang, and Shuicheng Yan. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE, 98(6):1031-1044, 2010.  Xindong Wu, Xingquan Zhu, Gong Qing Wu, and Wei Ding. Data mining with big data.  IEEE Transactions on Knowledge and Data Engineering, 26(1):97-107, 2014. Fattahi and Sojoudi  Eunho Yang, Aur\u00b4elie C Lozano, and Pradeep K Ravikumar. Elementary estimators for In Advances in neural information processing systems, pages 2159-  graphical models. 2167, 2014.  Hongbin Yin, S. C Wong, Jianmin Xu, and C. K. Wong. Urban tra\ufb03c \ufb02ow prediction using a fuzzy-neural approach. Transportation Research Part C: Emerging Technologies, 10(2): 85-98, 2017.  Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model.  Biometrika, 94(1):19-35, 2007.  Sangwoon Yun and Kim-Chuan Toh. A coordinate gradient descent method for l1- regularized convex minimization. Computational Optimization and Applications, 48(2): 273-307, 2011.  Richard Y Zhang and Javad Lavaei. Modified interior-point method for large-and-sparse low-rank semidefinite programs. In Decision and Control (CDC), 2017 IEEE 56th Annual Conference on, pages 5640-5647. IEEE, 2017. "}, "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-504.html", "header": "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory", "author": "Mehmet Eren Ahsen, Mathukumalli Vidyasagar", "time": "20(11):1\u221223, 2019.", "abstract": "In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in $\\R^n$ generated by $k$-sparse vectors is bounded below by $k ( \\lfloor\\lg (n/k) \\rfloor +1 )$ and above by $\\lfloor 2k \\lg (en) \\rfloor $. By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a $k$-sparse vector with $O(k \\lg n)$ measurements, given only the signs of the measurement vector. This result holds for \\textit{all} probability measures on $\\R^n$. The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability.", "pdf_url": "http://jmlr.org/papers/volume20/17-504/17-504.pdf", "keywords": null, "reference": "Mehmet Eren Ahsen and Mathukumalli Vidyasagar. An approach to one-bit compressed sensing based on probably approximately correct learning theory. arXiv:1710.07973va1, 2017.  A. Ai, A. Lapanowski, Y. Plan, and R. Vershynin. One-bit compressed sensing with non-  Gaussian measurements. Linear Algebra and Its Applications, 441:222-239, 2014.  Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2:  343-370, 1988.  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 1999.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the vapnik-  chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  P. T. Boufounos and R. G. Baraniuk. 1-bit compressive sensing.  In Proceedings of the  Conference on Information Sciences and Systems, 2008.  Petros T. Boufounos. Greedy sparse signal reconstruction from sign measurements.  In  Proceedings of the Asilomar Conference on Signals, Systems, and Computation, 2009.  20   Ahsen and Vidyasagar  6. Discussion  In this paper, the problem of one-bit compressed sensing (OBCS) has been formulated as a problem in probably approximately correct (PAC) learning theory. In particular, it has been shown that the VC-dimension of the set of half-spaces in Rn generated by k-sparse vectors is bounded by O(k lg n). Therefore, in principle at least, the OBCS problem can be solved using only O(k lg n) samples. This is possible in principle even when the measurements are corrupted by noise, except that as the mislabelling probability \u03b1 approaches 0.5, the constant under the O symbol becomes larger and larger. However, in general, it is NP- hard to find a consistent algorithm when measurements are free from noise, and to find an algorithm that minimizes empirical risk when measurements are noisy.  One of the main advantages of formulating OBCS as a problem in PAC learning is that extending these results to the case where the samples {ai} (or {(ai, bi)} as the case may be) are not i.i.d. essentially \u201ccomes for free.\u201d It is now known that, if a concept class has finite VC-dimension, then empirical means converge to their true values not only for i.i.d. samples {ai} (or {(ai, bi)} as the case may be), but also when the samples come from a \u03b2-mixing stochastic process, e.g., from a Markov process. The convergence result is established in (Nobel and Dembo, 1993), and explicit rates of convergence are proved in (Karandikar and Vidyasagar, 2002). As it is fairly straight-forward to adapt the various theorems given here to the case of \u03b2-mixing processes using the above-mentioned results, the details are omitted.  The authors thank the reviewers for very helpful comments.  Acknowledgement  References  Mehmet Eren Ahsen and Mathukumalli Vidyasagar. An approach to one-bit compressed sensing based on probably approximately correct learning theory. arXiv:1710.07973va1, 2017.  A. Ai, A. Lapanowski, Y. Plan, and R. Vershynin. One-bit compressed sensing with non-  Gaussian measurements. Linear Algebra and Its Applications, 441:222-239, 2014.  Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2:  343-370, 1988.  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.  Cambridge University Press, 1999.  A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the vapnik-  chervonenkis dimension. Journal of the ACM, 36(4):929-965, 1989.  P. T. Boufounos and R. G. Baraniuk. 1-bit compressive sensing.  In Proceedings of the  Conference on Information Sciences and Systems, 2008.  Petros T. Boufounos. Greedy sparse signal reconstruction from sign measurements.  In  Proceedings of the Asilomar Conference on Signals, Systems, and Computation, 2009. One-Bit Compressed Sensing: A PAC Learning Approach  Emmanuel J. Cand`es and Terence Tao. Decoding by linear programming. IEEE Transac-  tions on Information Theory, 51(12):4203-4215, December 2005.  Emmanuel J. Cand`es, J. Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications in Pure and Applied Mathematics, 59(8): 1207-1223, August 2006.  Mark A. Davenport, Marco F. Duarte, Yonina C. Eldar, and Gitta Kutyniok. Introduction to compressed sensing. In Yonina C. Eldar and Gitta Kutyniok, editors, Compressed Sensing: Theory and Applications, pages 1-68. Cambridge University Press, Cambridge, UK, 2012.  D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):  1289-1306, April 2006a.  D. L. Donoho. For most large underdetermined systems of linear equations, the minimal (cid:96)1-norm solution is also the sparsest solution. Communications in Pure and Applied Mathematics, 59(6):797-829, 2006b.  R. M. Dudley. Central limit theorems for empirical measures. The Annals of Probability,  6(6):899-929, 1978.  A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the Information and Computation, 82:247-261,  number of examples needed for learning. 1989.  Michael Elad. Sparse and Redundant Representations: From Theory to Applications in  Signal and Image Processing. Springer-Verlag, New York, 2010.  Yonina C. Eldar and Gitta Kutyniok, editors. Compressed Sensing: Theory and Applica-  tions. Cambridge University Press, Cambridge, UK, 2012.  Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing.  Springer-Verlag, 2013.  M. X. Goemans and D. P. Williamson. Improved algorithms for maximum cut and satis- fiability problems using semidefinite programming. Journal of the ACM, 42:1115-1145, 1995.  Ankit Gupta, Robert Nowak, and Benjamin Recht. Sample complexity for 1-bit compres- sive sensing and sparse classification. In Proceedings of the International Symposium on Information Theory, 2010.  Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Spar- sity: The Lasso and Generalizations. CRC Press, Taylor & Francis Group, Boca Raton, FL, 2015.  Laurent Jacques, Jason N. Laska, Petros T. Boufounos, and Richard G. Baraniuk. Ro- IEEE  bust 1-bit compressive sensing via binary stable embeddings of sparse vectors. Transactions on Information Theory, 59(4):2082-2102, April 2013. Ahsen and Vidyasagar  Rajeeva L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means  with mixingprocesses. Statistics & Probability Letters, 58:297-307, 2002.  Karin Knudson, Rayan Saab, and Rachel Ward. One-bit compressive sensing with norm  estimation. IEEE Transactions on Information Theory, 62(5):2748-2758, 2016.  Philip D. Laird. Learning from Good and Bad Data. Kluwer Academic Press, 1988.  B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Com-  puting, 24:227-234, 1995.  N. Natarajan, I. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. Neural  Information Processig Systems, 26, 2013.  Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301-321, 2009.  S. Negabhan, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high- dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, December 2012.  Tyler Neylon. Sparse solutions for linear prediction problems. Ph.D. thesis, Department of  Mathematics, New York University, 2006.  A. Nobel and A. Dembo. Rates of uniform convergence of empirical means with mixing-  processes. Statistics & Probability Letters, 17:169-172, 1993.  Yaniv Plan and Ronan Vershynin. One-bit compressed sensing by linear programming.  Communications on Pure and Applied Mathematics, 66:1275-1297, 2013a.  Yaniv Plan and Ronan Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, January 2013b.  Irina Rish and Genady Grabarnik. Sparse Modeling: Theory, Algorithms, and Applications.  CRC Press, Taylor & Francis Group, Boca Raton, FL, 2015.  N. Sauer. On the densities of families of sets. Journal of Combinatorial Theory, Series A,  13:145-147, 1972.  Hans Ulrich Simon. General bounds on the number of examples needed for learning prob-  abilistic concepts. Journal of Computer and System Sciences, 52(2):239-254, 1996.  L. G. Valiant. Learning conjuctions of disjunctions. In Proceedings of the Ninth International  Joint Conference on Artificial Intelligence, pages 560-566, 1985.  Leslie G. Valiant. A theory of the learnable. Journal of the ACM, 29(11):1134-1142, 1984.  V. N. Vapnik. Statistical Learning Theory. John Wiley, New York, 1998.  M. Vidyasagar. A Theory of Learning and Generalization. Springer-Verlag, London, 1997. One-Bit Compressed Sensing: A PAC Learning Approach  M. Vidyasagar. Learning and Generalization: With Applications to Neural Networks and  Control Systems. Springer-Verlag, London, 2003. "}, "Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-517.html", "header": "Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds", "author": "Shusen Wang, Alex Gittens, Michael W. Mahoney", "time": "20(12):1\u221249, 2019.", "abstract": "Kernel $k$-means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear $k$-means clustering algorithm. However, kernel $k$-means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points. Kernel approximation, e.g., the Nystrom method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel $k$-means clustering, and shows that applying the linear $k$-means clustering algorithm to $\\frac{k}{\\epsilon} (1 + o(1))$ features constructed using a so-called rank-restricted Nystrom approximation results in cluster assignments that satisfy a $1 + \\epsilon$ approximation ratio in terms of the kernel $k$-means cost function, relative to the guarantee provided by the same algorithm without the use of the Nystrom method. As part of the analysis, this work establishes a novel $1 + \\epsilon$ relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nystrom approximation. Empirical evaluations on the $8.1$ million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel $k$-means clustering with Nystrom approximation. This work argues that spectral clustering using Nystrom approximation---a popular and computationally efficient, but theoretically unsound approach to non-linear clustering---should be replaced with the efficient and theoretically sound combination of kernel $k$-means clustering with Nystrom approximation. The superior performance of the latter approach is empirically verified.", "pdf_url": "http://jmlr.org/papers/volume20/17-517/17-517.pdf", "keywords": ["kernel k-means clustering", "the Nystr\u00a8om method", "randomized linear algebra"], "reference": "Ahmed Alaoui and Michael W. Mahoney. Fast Randomized Kernel Ridge Regression with Statistical Guarantees. In Advances in Neural Information Processing Systems (NIPS). 2015.  Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of  Euclidean sum-of-squares clustering. Machine Learning, 75(2):245-248, 2009.  Okan Arikan. Compression of motion capture databases. In ACM Transactions on Graphics  (TOG), volume 25, pages 890-897. ACM, 2006.  David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In  Annual ACM-SIAM Symposium on Discrete Algorithms, 2007.  Pranjal Awasthi, Moses Charikar, Ravishankar Krishnaswamy, and Ali Kemal Sinop. The hardness of approximation of Euclidean k-means. arXiv preprint arXiv:1502.03316, 2015.  Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In International  Conference on Learning Theory (COLT), 2013.  Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and David A. Forsyth. Who\u2019s in the  picture. Advances in Neural Information Processing Systems (NIPS), 2004.  Christos Boutsidis, Petros Drineas, and Michael W. Mahoney. Unsupervised Feature In Advances in Neural Information  Selection for the k-means Clustering Problem. Processing Systems (NIPS). 2009.  Christos Boutsidis, Anastasios Zouzias, and Petros Drineas. Random projections for k- means clustering. In Advances in Neural Information Processing Systems (NIPS), 2010.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-Optimal Column-Based  Matrix Reconstruction. SIAM Journal on Computing, 43(2):687-717, 2014.  Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, and Petros Drineas. Randomized Dimensionality Reduction for k-Means Clustering. IEEE Transactions on Information Theory, 61(2):1045-1062, 2015.  Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey.  ACM Computing Surveys, 41(3):1-58, 2009.  Anil Chaturvedi, J. Douglas Carroll, Paul E. Green, and John A. Rotondo. A feature- based approach to market segmentation via overlapping k-centroids clustering. Journal of Marketing Research, 34(3):370-377, 1997.  Je\ufb00 Cheeger. A lower bound for the smallest eigenvalue of the Laplacian.  In Problems in Analysis, Papers dedicated to Salomon Bochner, pages 195-199. Princeton University Press, 1969.  Ke Chen. On coresets for k-median and k-means clustering in metric and Euclidean spaces  and their applications. SIAM Journal on Computing, 39(3):923-947, 2009.  43   Scalable Kernel K-Means Clustering with Nystr\u00a8om Approximation  References  Ahmed Alaoui and Michael W. Mahoney. Fast Randomized Kernel Ridge Regression with Statistical Guarantees. In Advances in Neural Information Processing Systems (NIPS). 2015.  Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. NP-hardness of  Euclidean sum-of-squares clustering. Machine Learning, 75(2):245-248, 2009.  Okan Arikan. Compression of motion capture databases. In ACM Transactions on Graphics  (TOG), volume 25, pages 890-897. ACM, 2006.  David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In  Annual ACM-SIAM Symposium on Discrete Algorithms, 2007.  Pranjal Awasthi, Moses Charikar, Ravishankar Krishnaswamy, and Ali Kemal Sinop. The hardness of approximation of Euclidean k-means. arXiv preprint arXiv:1502.03316, 2015.  Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In International  Conference on Learning Theory (COLT), 2013.  Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and David A. Forsyth. Who\u2019s in the  picture. Advances in Neural Information Processing Systems (NIPS), 2004.  Christos Boutsidis, Petros Drineas, and Michael W. Mahoney. Unsupervised Feature In Advances in Neural Information  Selection for the k-means Clustering Problem. Processing Systems (NIPS). 2009.  Christos Boutsidis, Anastasios Zouzias, and Petros Drineas. Random projections for k- means clustering. In Advances in Neural Information Processing Systems (NIPS), 2010.  Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-Optimal Column-Based  Matrix Reconstruction. SIAM Journal on Computing, 43(2):687-717, 2014.  Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, and Petros Drineas. Randomized Dimensionality Reduction for k-Means Clustering. IEEE Transactions on Information Theory, 61(2):1045-1062, 2015.  Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey.  ACM Computing Surveys, 41(3):1-58, 2009.  Anil Chaturvedi, J. Douglas Carroll, Paul E. Green, and John A. Rotondo. A feature- based approach to market segmentation via overlapping k-centroids clustering. Journal of Marketing Research, 34(3):370-377, 1997.  Je\ufb00 Cheeger. A lower bound for the smallest eigenvalue of the Laplacian.  In Problems in Analysis, Papers dedicated to Salomon Bochner, pages 195-199. Princeton University Press, 1969.  Ke Chen. On coresets for k-median and k-means clustering in metric and Euclidean spaces  and their applications. SIAM Journal on Computing, 39(3):923-947, 2009. Wang, Gittens, and Mahoney  Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen Lin, and Edward Y. Chang. Parallel Spectral Clustering in Distributed Systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):568-586, 2011.  Radha Chitta, Rong Jin, Timothy C. Havens, and Anil K. Jain. Approximate Kernel In ACM SIGKDD International  k-means: Solution to Large Scale Kernel Clustering. Conference on Knowledge Discovery and Data Mining (KDD), 2011.  Radha Chitta, Rong Jin, and Anil K. Jain. E\ufb03cient Kernel Clustering Using Random Fourier Features. In IEEE International Conference on Data Mining (ICDM), 2012.  Jiawei Chiu and Laurent Demanet.  Sublinear Randomized Algorithms for Skeleton Decompositions. SIAM Journal on Matrix Analysis and Applications, 34(3):1361-1383, 2013.  Kenneth L. Clarkson and David P. Woodru\ufb00. Low rank approximation and regression in input sparsity time. In Annual ACM Symposium on Theory of Computing (STOC), 2013.  Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality reduction for k-means clustering and low rank approximation. In Annual ACM Symposium on Theory of Computing (STOC), 2015.  Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2017.  Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on learning accuracy. In Conference on Artificial Intelligence and Statistics (AISTATS), 2010.  Sanjoy Dasgupta and Yoav Freund. Random projection trees for vector quantization. IEEE  Transactions on Information Theory, 55(7):3229-3242, 2009.  Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and normalized cuts. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2004.  Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In SIAM International Conference on Data Mining (SDM), 2005.  William E Donath and Alan J Ho\ufb00man. Algorithms for partitioning graphs and computer logic based on eigenvectors of connection matrices. IBM Technical Disclosure Bulletin, 15(3):938-944, 1972.  William E Donath and Alan J Ho\ufb00man. Lower bounds for the partitioning of graphs. IBM  Journal of Research and Development, 17(5):420-425, 1973.  Petros Drineas and Michael W. Mahoney. On the Nystr\u00a8om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, 2005. Scalable Kernel K-Means Clustering with Nystr\u00a8om Approximation  Petros Drineas and Michael W Mahoney. RandNLA: randomized numerical linear algebra.  Communications of the ACM, 59(6):80-90, 2016.  Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication. SIAM Journal on Computing, 36(1): 132-157, 2006.  Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, September 2008.  Petros Drineas, Michael W. Mahoney, S. Muthukrishnan, and Tam\u00b4as Sarl\u00b4os. Faster least  squares approximation. Numerische Mathematik, 117(2):219-249, 2011.  Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodru\ufb00. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13:3441-3472, 2012.  Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: constant-size coresets for k-means, PCA and projective clustering. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2013.  Miroslav Fiedler. Algebraic connectivity of graphs. Czechoslovak mathematical journal, 23  (2):298-305, 1973.  Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping using the Nystr\u00a8om method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214-225, 2004.  A. Frank and A. Asuncion. UCI machine learning repository, 2010. URL http://archive.  ics.uci.edu/ml.  Michael R. Garey, D. S. Johnson, and Hans S. Witsenhausen. The complexity of the generalized Lloyd-Max problem. IEEE Transactions on Information Theory, 28(2):255- 256, 1982.  Alex Gittens and Michael W. Mahoney. Revisiting the Nystr\u00a8om method for improved large-scale machine learning. Journal of Machine Learning Research, 17(1):3977-4041, 2016.  Alex Gittens, Jey Kottalam, Jiyan Yang, Michael F. Ringenburg, Jatin Chhugani, Evan Racah, Mohitdeep Singh, Yushu Yao, Curt Fischer, Oliver Ruebel, Benjamin Bowen, Norman G. Lewis, Michael W. Mahoney, Venkat Krishnamurthy, and Prabhat. A Multi- Platform Evaluation of the Randomized CX Low-Rank Matrix Factorization in Spark. In IEEE International Parallel and Distributed Processing Symposium Workshops, 2016.  Stephen Guattery and Gary L. Miller. On the performance of spectral graph partitioning methods. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 1995. Wang, Gittens, and Mahoney  Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp.  Finding structure for constructing approximate matrix  with randomness: decompositions. SIAM Review, 53(2):217-288, 2011.  Probabilistic algorithms  Mark S. Handcock, Adrian E. Raftery, and Jeremy M. Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301-354, 2007.  Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering.  In Annual ACM Symposium on Theory of Computing (STOC), 2004.  Robert M. Haralick and Linda G. Shapiro.  Image segmentation techniques. Computer  Vision, Graphics, and Image Processing, 29(1):100-132, 1985.  Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Y Li, and Z Zhou. the Nystr\u00a8om method with application to kernel classification. Information Theory, 59(10):6939-6949, 2013.  Improved bounds for IEEE Transactions on  William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a  Hilbert space. Contemporary mathematics, 26(189-206), 1984.  Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. A local search approximation algorithm for k-means clustering. In Annual Symposium on Computational Geometry, 2002.  Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1 + (cid:15))- In Annual IEEE  approximation algorithm for k-means clustering in any dimensions. Symposium on Foundations of Computer Science (FOCS), 2004.  Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling methods for the Nystr\u00a8om  method. Journal of Machine Learning Research, 13:981-1006, 2012.  Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.  Mu Li, Wei Bi, J.T. Kwok, and Bao-Liang Lu.  approximation using randomized SVD. Learning Systems, 26(1):152-164, Jan 2015.  Large-scale Nystr \u00a8m kernel matrix IEEE Transactions on Neural Networks and  Yingyu Liang, Maria-Florina F Balcan, Vandana Kanchanapally, and David Woodru\ufb00. Improved distributed principal component analysis. In Advances in Neural Information Processing Systems (NIPS), 2014.  Stuart Lloyd. Least squares quantization in PCM.  IEEE Transactions on Information  Theory, 28(2):129-137, 1982.  Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the subsampled randomized Hadamard transform. In Advances in Neural Information Processing Systems (NIPS), 2013. Scalable Kernel K-Means Clustering with Nystr\u00a8om Approximation  Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-means problem is NP-hard. In International Workshop on Algorithms and Computation, pages 274-285. Springer, 2009.  Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and  Trends in Machine Learning, 3(2):123-224, 2011.  Michael W. Mahoney and Petros Drineas. Structural properties underlying high-quality randomized numerical linear algebra algorithms. In P. B\u00a8uhlmann, P. Drineas, M. Kane, and M. van de Laan, editors, Handbook of Big Data, pages 137-154. CRC Press, 2016.  Jiri Matousek. On approximate geometric k-clustering. Discrete & Computational  Geometry, 24(1):61-84, 2000.  Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input- sparsity time and applications to robust linear regression. In Annual ACM Symposium on Theory of Computing (STOC), 2013.  Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Mateh Zaharia, and Ameet Talwalkar. MLlib: Machine Learning in Apache Spark. Journal of Machine Learning Research, 17(34):1-7, 2016.  Cameron Musco and Christopher Musco. Recursive sampling for the Nystrom method. In  Advances in Neural Information Processing Systems (NIPS), 2017.  John Nelson and Huy L Nguy\u02c6en. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2013.  Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: analysis and an  algorithm. Advances in Neural Information Processing Systems (NIPS), 2002.  Evert J. Nystr\u00a8om. \u00a8Uber die praktische au\ufb02\u00a8osung von integralgleichungen mit anwendungen  auf randwertaufgaben. Acta Mathematica, 54(1):185-204, 1930.  Farhad Pourkamali-Anaraki and Stephen Becker. Randomized Clustered Nystr\u00a8om for Large-  Scale Kernel Machines. arXiv preprint arXiv:1612.06470, 2016.  Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.  In  Advances in Neural Information Processing Systems (NIPS), 2007.  Mark Rudelson and Roman Vershynin. The Littlewood-O\ufb00ord problem and invertibility of  random matrices. Advances in Mathematics, 218(2):600-633, 2008.  Bernhard Sch\u00a8olkopf and Alexander J. Smola. Learning with Kernels: Support Vector  Machines, Regularization, Optimization, and Beyond. MIT Press, 2002.  Bernhard Sch\u00a8olkopf, Alexander Smola, and Klaus-Robert M\u00a8uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299-1319, 1998. Wang, Gittens, and Mahoney  Roded Sharan, Rani Elkon, and Ron Shamir. Cluster analysis and its applications to gene expression data. In Bioinformatics and Genome Analysis, pages 83-108. Springer, 2002.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation.  IEEE  Transactions on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  Si Si, Cho-Jui Hsieh, and Inderjit Dhillon. Memory e\ufb03cient kernel approximation.  In  International Conference on Machine Learning (ICML), 2014.  Mingjun Song and Sanguthevar Rajasekaran. Fast algorithms for constant approximation k-means clustering. Transactions on Machine Learning and Data Mining, 3(2):67-79, 2010.  Daniel A. Spielman and Shang-Hua Teng. Spectral partitioning works: Planar graphs and finite element meshes. In Annual IEEE Symposium on Foundations of Computer Science (FOCS), 1996.  Alexander Strehl and Joydeep Ghosh. Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 3(Dec):583-617, 2002.  Terence Tao and Van Vu. Random matrices: The distribution of the smallest singular  values. Geometric And Functional Analysis, 20(1):260-297, 2010.  Joel A Tropp.  Improved analysis of the subsampled randomized hadamard transform.  Advances in Adaptive Data Analysis, 3(01n02):115-126, 2011.  Joel A Tropp, Alp Yurtsever, Madeleine Udell, and Volkan Cevher. approximation of a positive-semidefinite matrix from streaming data. Neural Information Processing Systems (NIPS), 2017.  Fixed-rank In Advances in  L. H. Ungar and D. P. Foster. Clustering Methods for Collaborative Filtering. In Proc. Recommender Systems, Papers from 1998 Workshop, 1998. Technical Report WS-98-08.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv  preprint arXiv:1011.3027, 2010.  Shusen Wang and Zhihua Zhang. Improving CUR matrix decomposition and the Nystr\u00a8om approximation via adaptive sampling. Journal of Machine Learning Research, 14:2729- 2769, 2013.  Shusen Wang, Luo Luo, and Zhihua Zhang. SPSD matrix approximation vis column selection: Theories, algorithms, and extensions. Journal of Machine Learning Research, 17(49):1-49, 2016a.  Shusen Wang, Zhihua Zhang, and Tong Zhang. Towards more e\ufb03cient SPSD matrix approximation and CUR matrix decomposition. Journal of Machine Learning Research, 17(210):1-49, 2016b. Scalable Kernel K-Means Clustering with Nystr\u00a8om Approximation  Shusen Wang, Alex Gittens, and Michael W Mahoney.  Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging. In International Conference on Machine Learning (ICML), 2017.  Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. In Advances in Neural  Information Processing Systems (NIPS), 2009.  Christopher Williams and Matthias Seeger. Using the Nystr\u00a8om method to speed up kernel  machines. In Advances in Neural Information Processing Systems (NIPS), 2001.  David P Woodru\ufb00. Sketching as a tool for numerical linear algebra. Foundations and  Trends R(cid:13) in Theoretical Computer Science, 10(1-2):1-157, 2014.  Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr\u00a8om method vs random Fourier features: A theoretical and empirical comparison. In Advances in Neural Information Processing Systems (NIPS), 2012.  Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica.  Spark: Cluster computing with working sets. HotCloud, 10(10-10):95, 2010.  Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. Resilient distributed In USENIX datasets: A fault-tolerant abstraction for in-memory cluster computing. Conference on Networked Systems Design and Implementation (NSDI), 2012.  Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D Simon. Spectral relaxation for k-means clustering. In Advances in Neural Information Processing Systems (NIPS), 2002.  Kai Zhang and James T. Kwok. Clustered Nystr\u00a8om method for large scale manifold learning IEEE Transactions on Neural Networks, 21(10):1576-1587,  singular value decomposition,  applications and beyond.  and dimension reduction. 2010.  Zhihua Zhang.  The arXiv:1510.08532, 2015. "}, "Train and Test Tightness of LP Relaxations in Structured Prediction": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-535.html", "header": "Train and Test Tightness of LP Relaxations in Structured Prediction", "author": "Ofer Meshi, Ben London, Adrian Weller, David Sontag", "time": "20(13):1\u221234, 2019.", "abstract": "Structured prediction is used in areas including computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation for the striking observation that approximations based on linear programming (LP) relaxations are often tight (exact) on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that this training tightness generalizes to test data.", "pdf_url": "http://jmlr.org/papers/volume20/17-535/17-535.pdf", "keywords": null, "reference": "53-68, 1993.  tion. In CVPR, 2004.  G. H. Bakir, T. Hofmann, B. Sch\u00a8olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan.  Predicting Structured Data. The MIT Press, 2007.  F. Barahona. On cuts and matchings in planar graphs. Mathematical Programming, 60:  E. Borenstein, E. Sharon, and S. Ullman. Combining top-down and bottom-up segmenta-  C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and approximation algorithms for the metric labeling problem. SIAM J. on Discrete Mathematics, 18(3):608-625, 2004.  L.-C. Chen, A. Schwing, A. Yuille, and R. Urtasun. Learning deep structured models. In  International Conference on Machine Learning, 2015.  M. Collins. Discriminative training methods for hidden Markov models: Theory and  experiments with perceptron algorithms. In EMNLP, 2002.  Hal Daum\u00b4e III, John Langford, and Daniel Marcu. Search-based structured prediction.  Machine Learning, 75(3):297-325, 2009.  J. Domke. Learning graphical model parameters with approximate marginal inference.  Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(10), 2013.  M. Donsker and S. Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. Communications on Pure and Applied Mathematics, 28(1):1-47, 1975.  T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In Proceedings of the 25th International Conference on Machine learning, pages 304-311, 2008.  A. Globerson, T. Roughgarden, D. Sontag, and C. Yildirim. How hard is inference for  structured prediction? In ICML, 2015.  J. Kleinberg and E. Tardos. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields. Journal of the ACM, 49(5):616-639, 2002.  T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for  parsing with non-projective head automata. In EMNLP, 2010.  A. Koster, S.P.M. van Hoesel, and A.W.J. Kolen. The partial constraint satisfaction  problem: Facets and lifting theorems. Operations Research Letters, 23:89-97, 1998.  A. Kulesza and F. Pereira. Structured learning with approximate inference. In Advances  in Neural Information Processing Systems 20, pages 785-792, 2007.  31   Train and Test Tightness of LP Relaxations in Structured Prediction  References  53-68, 1993.  tion. In CVPR, 2004.  G. H. Bakir, T. Hofmann, B. Sch\u00a8olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan.  Predicting Structured Data. The MIT Press, 2007.  F. Barahona. On cuts and matchings in planar graphs. Mathematical Programming, 60:  E. Borenstein, E. Sharon, and S. Ullman. Combining top-down and bottom-up segmenta-  C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and approximation algorithms for the metric labeling problem. SIAM J. on Discrete Mathematics, 18(3):608-625, 2004.  L.-C. Chen, A. Schwing, A. Yuille, and R. Urtasun. Learning deep structured models. In  International Conference on Machine Learning, 2015.  M. Collins. Discriminative training methods for hidden Markov models: Theory and  experiments with perceptron algorithms. In EMNLP, 2002.  Hal Daum\u00b4e III, John Langford, and Daniel Marcu. Search-based structured prediction.  Machine Learning, 75(3):297-325, 2009.  J. Domke. Learning graphical model parameters with approximate marginal inference.  Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(10), 2013.  M. Donsker and S. Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. Communications on Pure and Applied Mathematics, 28(1):1-47, 1975.  T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In Proceedings of the 25th International Conference on Machine learning, pages 304-311, 2008.  A. Globerson, T. Roughgarden, D. Sontag, and C. Yildirim. How hard is inference for  structured prediction? In ICML, 2015.  J. Kleinberg and E. Tardos. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields. Journal of the ACM, 49(5):616-639, 2002.  T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for  parsing with non-projective head automata. In EMNLP, 2010.  A. Koster, S.P.M. van Hoesel, and A.W.J. Kolen. The partial constraint satisfaction  problem: Facets and lifting theorems. Operations Research Letters, 23:89-97, 1998.  A. Kulesza and F. Pereira. Structured learning with approximate inference. In Advances  in Neural Information Processing Systems 20, pages 785-792, 2007. Meshi, London, Weller, and Sontag  S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jordan. Word alignment via quadratic assignment. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 112-119, 2006.  S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe  optimization for structural SVMs. In ICML, pages 53-61, 2013.  Hunter Lang, David Sontag, and Aravindan Vijayaraghavan. Optimality of approximate inference algorithms on stable instances. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (AISTATS). JMLR: W&CP, 2018.  D. Khu\u02c6e L\u02c6e-Huu and Nikos Paragios. Continuous Relaxation of MAP Inference: A In CVPR 2018 - IEEE Conference on Computer Vision and  Nonconvex Perspective. Pattern Recognition, pages 1-19, 2018.  B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction.  Journal of Machine Learning Research, 17, 2016.  Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Bilu-Linial stable instances of max cut and minimum multiway cut. Proc. 22nd Symposium on Discrete Algorithms (SODA), 2014.  A. Martins, N. Smith, and E. P. Xing. Concise integer linear programming formulations for  dependency parsing. In ACL, 2009a.  A. Martins, N. Smith, and E. P. Xing. Polyhedral outer approximations with application to natural language parsing. In Proceedings of the 26th International Conference on Machine Learning, 2009b.  D. McAllester. Generalization bounds and consistency for structured labeling. In G. Bakir, T. Hofmann, B. Sch\u00a8olkopf, A. Smola, B. Taskar, and S. Vishwanathan, editors, Predicting Structured Data. MIT Press, 2007.  O. Meshi, E. Eban, G. Elidan, and A. Globerson. Learning max-margin tree predictors. In  UAI, 2013.  Sebastian Nowozin, Peter V. Gehler, Jeremy Jancsary, and Christoph Lampert. Advanced  Structured Prediction. MIT Press, 2014.  Manfred Padberg. The boolean quadric polytope: Some characteristics, facets and relatives.  Mathematical Programming, 45(1):139-172, 1989.  P. Raghavan and C. Tompson. Randomized rounding: A technique for provably good  algorithms and algorithmic proofs. Combinatorica, 7(4):365-374, 1987.  P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured linear programs: Proximal methods and rounding schemes. JMLR, 11:1043-1080, 2010.  D. Roth. On the hardness of approximate reasoning. Artificial Intelligence, 82, 1996. Train and Test Tightness of LP Relaxations in Structured Prediction  D. Roth and W. Yih. A linear programming formulation for global inference in natural language tasks. In CoNLL, The 8th Conference on Natural Language Learning, 2004.  D. Roth and W. Yih. Integer linear programming inference for conditional random fields.  In ICML, pages 736-743. ACM, 2005.  M. Rowland, A. Pacchiano, and A. Weller. Conditions beyond treewidth for tightness of higher-order LP relaxations. In In Artificial Intelligence and Statistics (AISTATS), 2017.  Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP, 2010.  M. I. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions.  Kibernetika, 4:113\u2014130, 1976.  Y. Seldin, F. Laviolette, N. Cesa-Bianchi, J. Shawe-Taylor, and P. Auer. PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory, 58(12):7086- 7093, 2012.  H. D. Sherali and W. P. Adams. A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems. SIAM J. on Disc. Math., 3(3):411-430, 1990.  Y. Shimony. Finding the MAPs for belief networks is NP-hard. Aritifical Intelligence, 68(2):  399-410, 1994.  D. Sontag and T. Jaakkola. New outer bounds on the marginal polytope. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1393-1400. MIT Press, Cambridge, MA, 2008.  D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations  for MAP using message passing. In UAI, pages 503-510, 2008.  D. Sontag, O. Meshi, T. Jaakkola, and A. Globerson. More data means less inference: A  pseudo-max approach to structured learning. In NIPS, 2010.  B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks.  In Advances in  Neural Information Processing Systems. MIT Press, 2003.  B. Taskar, V. Chatalbashev, and D. Koller. Learning associative Markov networks. In Proc.  ICML. ACM Press, 2004.  2012.  J. Thapper and S. \u02c7Zivn\u00b4y. The power of linear programming for valued CSPs. In FOCS,  I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning  for interdependent and structured output spaces. In ICML, pages 104-112, 2004.  M. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational  Inference. Now Publishers Inc., Hanover, MA, USA, 2008. Meshi, London, Weller, and Sontag  M. Wainwright, T. Jaakkola, and A. Willsky. MAP estimation via agreement on trees: message-passing and linear programming. IEEE Transactions on Information Theory, 51 (11):3697-3717, 2005.  D. Weiss and B. Taskar. Structured Prediction Cascades. In AISTATS, 2010.  A. Weller. Characterizing tightness of LP relaxations by forbidding signed minors.  In  Uncertainty in Artificial Intelligence (UAI), 2016.  Adrian Weller. Bethe and related pairwise entropy approximations.  In Uncertainty in  Artificial Intelligence (UAI), 2015.  Adrian Weller, Mark Rowland, and David Sontag. Tightness of LP relaxations for almost  balanced models. In AISTATS, 2016.  Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Greed is good if randomized:  New inference for dependency parsing. In EMNLP, 2014. "}, "Approximations of the Restless Bandit Problem": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-547.html", "header": "Approximations of the Restless Bandit Problem", "author": "Steffen Gr\u00c3\u00bcnew\u00c3\u00a4lder, Azadeh Khaleghi", "time": "20(14):1\u221237, 2019.", "abstract": "The multi-armed restless bandit problem is studied in the case where the pay-off distributions are stationary $\\varphi$-mixing. This version of the problem provides a more realistic model for most real-world applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The objective of this paper is to characterize a sub-class of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that under some conditions on the $\\varphi$-mixing coefficients, a modified version of UCB can prove effective. The main challenge is that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same characteristics as those of the original bandit arms. In particular, the $\\varphi$-mixing property does not necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on the pay-off distributions. Some of the proof techniques developed in this paper can be more generally used in the context of online sampling under dependence. Proposed algorithms are accompanied with corresponding regret analysis.", "pdf_url": "http://jmlr.org/papers/volume20/17-547/17-547.pdf", "keywords": null, "reference": "J. Audiffren and L. Ralaivola. Cornering stationary and restless mixing bandits with remix-ucb. In  Advances in Neural Information Processing Systems, 2015.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-armed bandit problem.  Machine Learning, 2002.  P. Auer and R. Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed bandit  problem. Periodica Mathematica Hungarica, 61(1-2):55-65, 2010.  R. C. Bradley. Basic properties of strong mixing conditions. A survey and some open questions.  Probability surveys, 2(2):107-144, 2005.  R. C. Bradley. Introduction to Strong Mixing Conditions, Vols. 1, 2 and 3. Kendrick Press, 2007.  G. E. Cho and C. D. Meyer. Comparison of perturbation bounds for the stationary distribution of a  markov chain. Linear Algebra and its Applications, 2001.  P. Doukhan. Mixing: Properties and Examples. Springer Lecture Notes, 1994.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, 2002.  R. M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, second edition, 2014.  D. H. Fremlin. Measure Theory. Torres Fremlin, Colchester, England, 2010. Vol. 1 and 2.  E. Gin\u00e9 and R. Nickl. Mathematical Foundations of Infinite-dimensional Statistical Models. Cam-  bridge University Press, 2016.  of the ACM (JACM), 58(1):3, 2010.  Society, 2008.  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. Journal  D. A. Levin, Y. Peres, and E. L. Wilmer. Markov Chains and Mixing Times. American Mathematical  R. Ortner, D. Ryabko, P. Auer, and R. Munos. Regret bounds for restless Markov bandits. Theoretical  Computer Science, 558:62-76, 2014.  C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of optimal queuing network control.  Mathematics of Operations Research, 1999.  E. Rio. Th\u00e9orie asymptotique des processus al\u00e9atoires faiblement d\u00e9pendants. Springer, 1999.  A. N. Shiryaev. Probability. Springer, 1991.  P. Whittle. Restless bandits: Activity allocation in a changing world. Journal of Applied Probability,  1988.  37   APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM  References  J. Audiffren and L. Ralaivola. Cornering stationary and restless mixing bandits with remix-ucb. In  Advances in Neural Information Processing Systems, 2015.  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-armed bandit problem.  Machine Learning, 2002.  P. Auer and R. Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed bandit  problem. Periodica Mathematica Hungarica, 61(1-2):55-65, 2010.  R. C. Bradley. Basic properties of strong mixing conditions. A survey and some open questions.  Probability surveys, 2(2):107-144, 2005.  R. C. Bradley. Introduction to Strong Mixing Conditions, Vols. 1, 2 and 3. Kendrick Press, 2007.  G. E. Cho and C. D. Meyer. Comparison of perturbation bounds for the stationary distribution of a  markov chain. Linear Algebra and its Applications, 2001.  P. Doukhan. Mixing: Properties and Examples. Springer Lecture Notes, 1994.  R. M. Dudley. Real Analysis and Probability. Cambridge University Press, 2002.  R. M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, second edition, 2014.  D. H. Fremlin. Measure Theory. Torres Fremlin, Colchester, England, 2010. Vol. 1 and 2.  E. Gin\u00e9 and R. Nickl. Mathematical Foundations of Infinite-dimensional Statistical Models. Cam-  bridge University Press, 2016.  of the ACM (JACM), 58(1):3, 2010.  Society, 2008.  S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. Journal  D. A. Levin, Y. Peres, and E. L. Wilmer. Markov Chains and Mixing Times. American Mathematical  R. Ortner, D. Ryabko, P. Auer, and R. Munos. Regret bounds for restless Markov bandits. Theoretical  Computer Science, 558:62-76, 2014.  C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of optimal queuing network control.  Mathematics of Operations Research, 1999.  E. Rio. Th\u00e9orie asymptotique des processus al\u00e9atoires faiblement d\u00e9pendants. Springer, 1999.  A. N. Shiryaev. Probability. Springer, 1991.  P. Whittle. Restless bandits: Activity allocation in a changing world. Journal of Applied Probability,  1988. "}, "Smooth neighborhood recommender systems": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-629.html", "header": "Smooth neighborhood recommender systems", "author": "Ben Dai, Junhui Wang, Xiaotong Shen, Annie Qu", "time": "20(16):1\u221224, 2019.", "abstract": "Recommender systems predict users' preferences over a large number of items by pooling similar information from other users and/or items in the presence of sparse observations. One major challenge is how to utilize user-item specific covariates and networks describing user-item interactions in a high-dimensional situation, for accurate personalized prediction. In this article, we propose a smooth neighborhood recommender in the framework of the latent factor models. A similarity kernel is utilized to borrow neighborhood information from continuous covariates over a user-item specific network, such as a user's social network, where the grouping information defined by discrete covariates is also integrated through the network. Consequently, user-item specific information is built into the recommender to battle the `cold-start\u00e2\u0080\u009d issue in the absence of observations in collaborative and content-based filtering. Moreover, we utilize a \u00e2\u0080\u009cdivide-and-conquer\u00e2\u0080\u009d version of the alternating least squares algorithm to achieve scalable computation, and establish asymptotic results for the proposed method, demonstrating that it achieves superior prediction accuracy. Finally, we illustrate that the proposed method improves substantially over its competitors in simulated examples and real benchmark data--Last.fm music data.", "pdf_url": "http://jmlr.org/papers/volume20/17-629/17-629.pdf", "keywords": ["Blockwise coordinate decent", "Cold-start", "Kernel smoothing", "Neighborhood", "Personalized prediction", "Singular value decomposition", "Social networks."], "reference": "Deepak Agarwal, Liang Zhang, and Rahul Mazumder. Modeling item-item similarities for personalized recommendations on Yahoo! front page. Annals of Applied Statistics, 5(3): 1839-1875, 2011.  Stephen H Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. Hinge-loss markov random fields and probabilistic soft logic. Journal of Machine Learning Research, 18 (109):1-67, 2017.  Robert M Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neigh- borhood interpolation weights. In 7th IEEE International Conference on Data Mining (ICDM 2007), pages 43-52. IEEE, 2007.  Xuan Bi, Annie Qu, Junhui Wang, and Xiaotong Shen. A group-specific recommender  system. Journal of the American Statistical Association, 112(519):1344-1353, 2017.  Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.  Daniel Billsus and Michael J Pazzani. User modeling for adaptive news access. User  Modeling and User-Adapted Interaction, 10(2-3):147-180, 2000.  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal  of Machine Learning Research, 3(Jan):993-1022, 2003.  Robin Burke. Hybrid recommender systems: Survey and experiments. User Modeling and  User-Adapted Interaction, 12(4):331-370, 2002.  J Douglas Carroll and Jih-Jie Chang. Analysis of individual di\ufb00erences in multidimensional scaling via an n-way generalization of \u201cEckart-Young\u201d decomposition. Psychometrika, 35 (3):283-319, 1970.  20   Dai, Wang, Shen and Qu  where the last inequality follows from Assumptions C and D, and  (cid:88)  (u(cid:48),i(cid:48))\u2208\u2126  K2  h((cid:107)xui \u2212 xu(cid:48)i(cid:48)(cid:107)2)Sui  u(cid:48)i(cid:48) \u2264 2|\u2126|E(cid:0)K2  h(U ui)|Sui = 1, \u2206 = 1(cid:1) \u2264 a9|\u2126|h.  Combing the above inequalities yields that  \u03c9(x0, xu(cid:48)i(cid:48))(cid:107)x0, xu(cid:48)i(cid:48)(cid:107)\u03b1  2 =  (cid:80)  (u(cid:48),i(cid:48))\u2208\u2126 Kh((cid:107)x0 \u2212 xu(cid:48)i(cid:48)(cid:107)2)(cid:107)x0 \u2212 xu(cid:48)i(cid:48)(cid:107)\u03b1  2 Sui u(cid:48)i(cid:48)  (cid:80)  (u(cid:48),i(cid:48))\u2208\u2126 Kh((cid:107)x \u2212 xu(cid:48)i(cid:48)(cid:107)2)Sui u(cid:48)i(cid:48)  \u2264 a8h\u03b1.  (cid:88)  (u(cid:48),i(cid:48))\u2208\u2126  \u03c92(x0, xu(cid:48)i(cid:48)) \u2264  (cid:80)  (cid:0) (cid:80)  (u(cid:48),i(cid:48))\u2208\u2126 K2 h((cid:107)x0 \u2212 xu(cid:48)i(cid:48)(cid:107)2)Sui u(cid:48)i(cid:48) (u(cid:48),i(cid:48))\u2208\u2126 Kh((cid:107)x \u2212 xu(cid:48)i(cid:48)(cid:107)2)Sui u(cid:48)i(cid:48)  (cid:1)2 \u2264  a9 a2 7|\u2126|h  .  Consequently, \u03ba1 = h\u03b1 and \u03ba2 = (|\u2126|h)\u22121, then the desired result follows immediately. (cid:3)  (cid:88)  (u(cid:48),i(cid:48))\u2208\u2126  Furthermore,  References  Deepak Agarwal, Liang Zhang, and Rahul Mazumder. Modeling item-item similarities for personalized recommendations on Yahoo! front page. Annals of Applied Statistics, 5(3): 1839-1875, 2011.  Stephen H Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. Hinge-loss markov random fields and probabilistic soft logic. Journal of Machine Learning Research, 18 (109):1-67, 2017.  Robert M Bell and Yehuda Koren. Scalable collaborative filtering with jointly derived neigh- borhood interpolation weights. In 7th IEEE International Conference on Data Mining (ICDM 2007), pages 43-52. IEEE, 2007.  Xuan Bi, Annie Qu, Junhui Wang, and Xiaotong Shen. A group-specific recommender  system. Journal of the American Statistical Association, 112(519):1344-1353, 2017.  Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.  Daniel Billsus and Michael J Pazzani. User modeling for adaptive news access. User  Modeling and User-Adapted Interaction, 10(2-3):147-180, 2000.  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal  of Machine Learning Research, 3(Jan):993-1022, 2003.  Robin Burke. Hybrid recommender systems: Survey and experiments. User Modeling and  User-Adapted Interaction, 12(4):331-370, 2002.  J Douglas Carroll and Jih-Jie Chang. Analysis of individual di\ufb00erences in multidimensional scaling via an n-way generalization of \u201cEckart-Young\u201d decomposition. Psychometrika, 35 (3):283-319, 1970. Smooth neighborhood recommender systems  Bilian Chen, Simai He, Zhening Li, and Shuzhong Zhang. Maximum block improvement  and polynomial optimization. SIAM Journal on Optimization, 22(1):87-107, 2012.  Hsin Chen and Alan F Murray. Continuous restricted Boltzmann machine with an imple- mentable training algorithm. IEE Proceedings-Vision, Image and Signal Processing, 150 (3):153-158, 2003.  Tianle Chen, Yuanjia Wang, Huaihou Chen, Karen Marder, and Donglin Zeng. Targeted local support vector machine for age-dependent classification. Journal of the American Statistical Association, 109(507):1174-1187, 2014.  Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: A  survey. Internet Mathematics, 3(1):79-127, 2006.  Aurore Delaigle and Peter Hall. Defining probability density for a distribution of random  functions. Annals of Statistics, 38(2):1171-1193, 2010.  \u00a8Ozg\u00a8ur Demir, Alexey Rodriguez Yakushev, Rany Keddo, and Ursula Kallio.  Item-item music recommendations with side information. CoRR, abs/1706.00218, 2017. URL http: //arxiv.org/abs/1706.00218.  Jianqing Fan and Irene Gijbels. Local polynomial modelling and its applications: monographs  on statistics and applied probability, volume 66. CRC Press, 1996.  Andrey Feuerverger, Yu He, and Shashi Khatri. Statistical significance of the Net\ufb02ix chal-  lenge. Statistical Science, 27(2):202-231, 2012.  Peter Forbes and Mu Zhu. Content-boosted matrix factorization for recommender systems: Experiments with recipe recommendation. In Proceedings of the 5th ACM conference on Recommender Systems, pages 261-264. ACM, 2011.  Jerome H Friedman and Werner Stuetzle. Projection pursuit regression. Journal of the  American Statistical Association, 76(376):817-823, 1981.  Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In Proceedings of the 2010 SIAM International Conference on Data Mining, pages 199-210. SIAM, 2010.  Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd via fast alternating least squares. Journal of Machine Learning Research, 16(1):3367-3402, 2015.  Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions  on Information Systems (TOIS), 22(1):89-115, 2004.  Yehuda Koren. Factorization meets the neighborhood: A multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 426-434. ACM, 2008. Dai, Wang, Shen and Qu  Pigi Kouki, Shobeir Fakhraei, James Foulds, Magdalini Eirinaki, and Lise Getoor. Hyper: A \ufb02exible and extensible probabilistic framework for hybrid recommender systems. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 99-106. ACM, 2015.  Oluwasanmi Koyejo and Joydeep Ghosh. A kernel-based approach to exploiting interaction- networks in heterogeneous information sources for improved recommender systems. In Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems, pages 9-16. ACM, 2011.  Ken Lang. Newsweeder: Learning to filter netnews. In Proceedings of the 12th International  Conference on Machine Learning, pages 331-339, 1995.  Huazhen Lin, Lixian Pan, Shaogao Lv, and Wenyang Zhang. E\ufb03cient estimation and computation for the generalised additive models with unknown link function. Journal of Econometrics, 2017.  Greg Linden, Brent Smith, and Jeremy York. Amazon.com recommendations: Item-to-item  collaborative filtering. IEEE Internet Computing, 7(1):76-80, 2003.  Yufeng Liu and Xiaotong Shen. Multicategory \u03c8-learning. Journal of the American Statis-  tical Association, 101(474):500-509, 2006.  Shujie Ma and Jian Huang. A concave pairwise fusion approach to subgroup analysis.  Journal of the American Statistical Association, 112(517):410-423, 2017.  JS Marron and WJ Padgett. Asymptotically optimal bandwidth selection for kernel density estimators from randomly right-censored samples. Annals of Statistics, pages 1520-1535, 1987.  Judith Mastho\ufb00. Group recommender systems: Combining individual models. In Recom-  mender systems handbook, pages 677-702. Springer, 2011.  Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collabo-  rative filtering for improved recommendations. AAAI/IAAI, 23:187-192, 2002.  Stuart E Middleton, Nigel R Shadbolt, and David C De Roure. Ontological user profiling in recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):54-88, 2004.  Bradley N Miller, Istvan Albert, Shyong K Lam, Joseph A Konstan, and John Riedl. Movielens unplugged: Experiences with an occasionally connected recommender system. In Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 263-266. ACM, 2003.  A Mukherjee, K Chen, N Wang, and J Zhu. On the degrees of freedom of reduced-rank  estimators in multivariate regression. Biometrika, 102(2):457-477, 2015.  Jennifer Nguyen and Mu Zhu. Content-boosted matrix factorization techniques for recom-  mender systems. Statistical Analysis and Data Mining, 6(4):286-301, 2013. Smooth neighborhood recommender systems  Trong T Nguyen and Hady W Lauw. Representation learning for homophilic preferences. In Proceedings of the 10th ACM Conference on Recommender Systems, pages 317-324. ACM, 2016.  Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative  filtering. In Proceedings of KDD Cup and Workshop, pages 5-8, 2007.  Michael J Pazzani, Jack Muramatsu, Daniel Billsus, et al. Syskill & Webert: Identifying  interesting web sites. In AAAI/IAAI, volume 1, pages 54-61, 1996.  Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62  (1-2):107-136, 2006.  Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization us- ing markov chain monte carlo. In Proceedings of the 25th International Conference on Machine Learning, pages 880-887. ACM, 2008.  Ruslan Salakhutdinov, Andriy Mnih, and Geo\ufb00rey Hinton. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th International Conference on Machine learning, pages 791-798. ACM, 2007.  David W Scott. Multivariate density estimation: theory, practice, and visualization. John  Wiley & Sons, 2015.  Xiaotong Shen, George C Tseng, Xuegong Zhang, and Wing Hung Wong. On \u03c8-learning.  Journal of the American Statistical Association, 98(463):724-734, 2003.  Nathan Srebro, Noga Alon, and Tommi S Jaakkola. Generalization error bounds for collab- orative prediction with low-rank matrices. In Advances in Neural Information Processing Systems, pages 1321-1328, 2005.  Charles J Stone. An asymptotically optimal window selection rule for kernel density esti-  mates. Annals of Statistics, pages 1285-1297, 1984.  Robert Tibshirani and Trevor Hastie. Local likelihood estimation. Journal of the American  Statistical Association, 82(398):559-567, 1987.  Philippe Vieu. Nonparametric regression: optimal local bandwidth choice. Journal of the  Royal Statistical Society. Series B (Methodological), pages 453-464, 1991.  Larry Wassermann. All of nonparametric statistics. New York: Springer, 2006.  Wing Hung Wong and Xiaotong Shen. Probability inequalities for likelihood ratios and  convergence rates of sieve MLEs. Annals of Statistics, 23(2):339-362, 1995.  Shuang-Hong Yang, Bo Long, Alex Smola, Narayanan Sadagopan, Zhaohui Zheng, and Hongyuan Zha. Like like alike: joint friendship and interest propagation in social net- works. In Proceedings of the 20th International Conference on World Wide Web, pages 537-546. ACM, 2011. Dai, Wang, Shen and Qu  Feipeng Zhao and Yuhong Guo. Learning discriminative recommendation systems with side information. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 3469-3475, 2017.  Feipeng Zhao, Min Xiao, and Yuhong Guo. Predictive collaborative filtering with side information. In Proceedings of the 25th International Joint Conference on Artificial In- telligence, pages 2385-2391, 2016.  Tinghui Zhou, Hanhuai Shan, Arindam Banerjee, and Guillermo Sapiro. Kernelized prob- abilistic matrix factorization: Exploiting graphs and side information. In Proceedings of the 2012 SIAM International Conference on Data Mining, pages 403-414. SIAM, 2012.  Yunzhang Zhu, Xiaotong Shen, and Changqing Ye. Personalized prediction and sparsity pursuit in latent factor models. Journal of the American Statistical Association, 111(513): 241-252, 2016. "}, "Delay and Cooperation in Nonstochastic Bandits": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-631.html", "header": "Delay and Cooperation in Nonstochastic Bandits", "author": "Nicol\u00c3\u00b2 Cesa-Bianchi, Claudio Gentile, Yishay Mansour", "time": "20(17):1\u221238, 2019.", "abstract": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than $d$ hops to arrive, where $d$ is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with $K$ actions and $N$ agents the average per-agent regret after $T$ rounds is at most of order $\\sqrt{\\bigl(d+1 + \\tfrac{K}{N}\\alpha_{\\le d}\\bigr)(T\\ln K)}$, where $\\alpha_{\\le d}$ is the independence number of the $d$-th power of the communication graph $G$. We then show that for any connected graph, for $d=\\sqrt{K}$ the regret bound is $K^{1/4}\\sqrt{T}$, strictly better than the minimax regret $\\sqrt{KT}$ for noncooperating agents. More informed choices of $d$ lead to bounds which are arbitrarily close to the full information minimax regret $\\sqrt{T\\ln K}$ when $G$ is dense. When $G$ has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in $G$, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.", "pdf_url": "http://jmlr.org/papers/volume20/17-631/17-631.pdf", "keywords": ["Multi-armed bandits", "distributed learning", "cooperative multi-agent systems", "regret minimization", "LOCAL communication"], "reference": "Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In J. Shawe- Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 873-881. Curran Associates, Inc., 2011.  Noga Alon, Nicol`o Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Jour- nal of Computing, 46(6):1785-1826, 2017.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed  bandit problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002b.  Baruch Awerbuch and Robert Kleinberg. Competitive collaborative learning. Journal of  Computer and System Sciences, 74(8):1271-1288, 2008.  Samuel Barrett and Peter Stone. Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards. In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents, pages 9-14, 2011.  Nicol`o Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and cooperation in nonstochastic bandits. In Conference on Learning Theory, pages 605-622, 2016.  John Duchi, Michael I Jordan, and Brendan McMahan. Estimation, optimization, and parallelism when data is sparse. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2832-2840. Curran Associates, Inc., 2013.  John C Duchi, Sorathan Chaturapruek, and Christopher R\u00b4e. Asynchronous stochastic  convex optimization. arXiv preprint arXiv:1508.00882, 2015.  Miroslav Dud\u00b4\u0131k, Daniel J. Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. E\ufb03cient optimal learning for contextual bandits. In UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, pages 169-178, 2011.  35   Delay and Cooperation in Nonstochastic Bandits  so that  E[(III)] \u2264  T e 1 \u2212 e\u22121  (cid:0)N \u00b5D + K \u00af\u03b1D  (cid:1) .  Putting pieces together, dividing by N , and further overapproximating gives  coop R T  \u2264 3d +  ln K \u03b7  +  e2\u03b7 T 1 \u2212 e\u22121  (cid:18)  1 + \u00b5D +  (cid:19)  \u00af\u03b1D  ,  2K N  thereby concluding the proof.  References  Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In J. Shawe- Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 873-881. Curran Associates, Inc., 2011.  Noga Alon, Nicol`o Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Jour- nal of Computing, 46(6):1785-1826, 2017.  Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed  bandit problem. Machine learning, 47(2-3):235-256, 2002a.  Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic  multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002b.  Baruch Awerbuch and Robert Kleinberg. Competitive collaborative learning. Journal of  Computer and System Sciences, 74(8):1271-1288, 2008.  Samuel Barrett and Peter Stone. Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards. In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents, pages 9-14, 2011.  Nicol`o Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and cooperation in nonstochastic bandits. In Conference on Learning Theory, pages 605-622, 2016.  John Duchi, Michael I Jordan, and Brendan McMahan. Estimation, optimization, and parallelism when data is sparse. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2832-2840. Curran Associates, Inc., 2013.  John C Duchi, Sorathan Chaturapruek, and Christopher R\u00b4e. Asynchronous stochastic  convex optimization. arXiv preprint arXiv:1508.00882, 2015.  Miroslav Dud\u00b4\u0131k, Daniel J. Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. E\ufb03cient optimal learning for contextual bandits. In UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, pages 169-178, 2011. Cesa-Bianchi, Gentile, and Mansour  P. Firby and J. Haviland. Independence and average distance in graphs. Discrete Applied  Mathematics, 75:27-37, 1997.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online learning under delayed feed- back. In Proceedings of the 30th International Conference on Machine Learning (ICML- 13), pages 1453-1461, 2013.  Pooria Joulani, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Delay-tolerant online convex op- timization: Unified analysis and adaptive-gradient algorithms. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pages 1744-1750, 2016.  Soummya Kar, H Vincent Poor, and Shuguang Cui. Bandit problems in networks: Asymp- totically e\ufb03cient distributed allocation rules. In 50th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC), pages 1771-1778. IEEE, 2011.  Robert Kleinberg, Georgios Piliouras, and \u00b4Eva Tardos. Multiplicative updates outperform generic no-regret learning in congestion games. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 533-542. ACM, 2009.  Tom\u00b4a\u02c7s Koc\u00b4ak, Gergely Neu, Michal Valko, and Remi Munos. E\ufb03cient learning by implicit exploration in bandit problems with side observations. In Advances in Neural Information Processing Systems 27, pages 613-621. 2014.  Ravi Kumar Kolla, Krishna Jagannathan, and Aditya Gopalan. Collaborative learning In 54th Annual Allerton Conference on  of stochastic bandits over a social network. Communication, Control, and Computing, pages 1228-1235. IEEE, 2016.  Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. On distributed coopera- tive decision-making in multiarmed bandits. arXiv preprint arXiv:1512.06888, 2015.  Mu Li, David G. Andersen, and Alexander Smola. Distributed delayed proximal gradient  methods. In NIPS Workshop on Optimization for Machine Learning, 2013.  Nathan Linial. Locality in distributed graph algorithms. SIAM J. Comput., 21(1):193-201,  1992.  Ji Liu, Stephen J Wright, Christopher R\u00b4e, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. The Journal of Machine Learning Research, 16(1):285-322, 2015.  Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchan- dran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic opti- mization. arXiv preprint arXiv:1507.06970, 2015.  Brendan McMahan and Matthew Streeter. Delay-tolerant algorithms for asynchronous distributed online learning. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2915-2923. Curran Associates, Inc., 2014. Delay and Cooperation in Nonstochastic Bandits  Chris Mesterharm. On-line learning with delayed label feedback. In Algorithmic Learning  Theory, pages 399-413. Springer, 2005.  Chris Mesterharm.  Improving Online Learning. PhD thesis, Department of Computer  Science, Rutgers University, 2007.  Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic  bandits. In Advances in Neural Information Processing Systems 28 (NIPS), 2015.  Gergely Neu, Andras Antos, Andr\u00b4as Gy\u00a8orgy, and Csaba Szepesv\u00b4ari. Online Markov decision processes under bandit feedback. In Advances in Neural Information Processing Systems 23, pages 1804-1812. Curran Associates, Inc., 2010.  Gergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos. Online markov decision processes under bandit feedback. Automatic Control, IEEE Transactions on, 59(3):676- 691, 2014.  Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchan- dran, and Michael I Jordan. Parallel correlation clustering on big graphs. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 82-90. Curran Associates, Inc., 2015.  Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Ste\ufb00en Grunewalder. Bandits with delayed, aggregated anonymous feedback. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018, pages 4102-4110, 2018.  Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. In C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1270-1278. Curran Associates, Inc., 2015.  Jonathan Rosenski, Ohad Shamir, and Liran Szlak. Multi-player bandits - a musical chairs  approach. CoRR, abs/1512.02866, 2015.  Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited advice and multiarmed bandits with paid observations. In Proceedings of The 31st International Conference on Machine Learning, pages 280-287, 2014.  Ruben Stranders, Long Tran-Thanh, Francesco M Delle Fave, Alex Rogers, and Nicholas R Jennings. Dcops and bandits: Exploration and exploitation in decentralised coordina- tion. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1, pages 289-296. International Foundation for Autonomous Agents and Multiagent Systems, 2012.  Jukka Suomela. Survey of local algorithms. ACM Computing Surveys, 45(2):24, 2013.  Balazs Szorenyi, R\u00b4obert Busa-Fekete, Istv\u00b4an Heged\u00a8us, R\u00b4obert Orm\u00b4andi, M\u00b4ark Jelasity, and Bal\u00b4azs K\u00b4egl. Gossip-based distributed stochastic bandit algorithms. In 30th International Conference on Machine Learning (ICML 2013), volume 28, pages 19-27. ACM Press, 2013. Cesa-Bianchi, Gentile, and Mansour  Cem Tekin and Mihaela van der Schaar. Distributed online learning via cooperative con-  textual bandits. IEEE Transactions on Signal Processing, 63(14):3700-3714, 2015.  Cem Tekin, Simpson Z. Zhang, and Mihaela van der Schaar. Distributed online learning in  social recommender systems. J. Sel. Topics Signal Processing, 8(4):638-652, 2014.  Klaus Wehmuth and Artur Ziviani. Daccer: Distributed assessment of the closeness cen-  trality ranking in complex networks. Computer Networks, 57(13):2536-2548, 2013.  Marcelo J Weinberger and Erik Ordentlich. On delayed prediction of individual sequences.  IEEE Transactions on Information Theory, 48(7):1959-1976, 2002.  Martin Zinkevich, John Langford, and Alex J. Smola. Slow learners are fast. In Y. Bengio, D. Schuurmans, J.D. La\ufb00erty, C.K.I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2331-2339. Curran Associates, Inc., 2009. "}, "Multiplicative local linear hazard estimation and best one-sided cross-validation": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-663.html", "header": "Multiplicative local linear hazard estimation and best one-sided cross-validation", "author": "Maria Luz G\u00c3\u00a1miz, Mar\u00c3\u00ada Dolores  Mart\u00c3\u00adnez-Miranda, Jens Perch Nielsen", "time": "20(18):1\u221229, 2019.", "abstract": "This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance.", "pdf_url": "http://jmlr.org/papers/volume20/17-663/17-663.pdf", "keywords": ["Aalen\u2019s multiplicative model", "multiplicative bias correction", "bandwidth", "in direct cross-validation"], "reference": "Statist., 6, 701-726.  Aalen, O. O. (1978). Non-parametric inference for a family of counting processes. Ann.  Andersen, P., Borgan, O., Gill, R. and Keiding, N. (1993). Statistical Models Based on  Counting Processes. New York: Springer.  G\u00b4amiz, M. L., Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2016). Double one-sided cross-validation of local linear hazards. J. Royal Statist. Soc. B, 78, 755-779.  G\u00b4amiz, M. L., Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2017). DOvalidation: Kernel Hazard Estimation with Best One-Sided and Double One-Sided Cross-Validation. R package version 1.1.0.  G\u00b4amiz, M. L., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2013a). Smoothing survival  densities in practice. Comput. Statist. Data Anal., 58, 368-382.  G\u00b4amiz, M. L., Janys, L., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2013b). Bandwidth selection in marker dependent kernel hazard estimation. Comput. Statist. Data Anal., 68, 155-169.  Hall, P. and Johnstone, I. (1992). Empirical Functionals and E\ufb03cient Smoothing Parameter  Selection. J. Royal Statist. Soc. B, 54(2), 475-530.  Hall, P. and Marron, J. S. (1987). Extent to which Least-Squares Cross-Validation Minimises Integrated Square Error in Nonparametric Density Estimation. Probab. Theory Rel., 74, 567-581.  28   G\u00b4amiz, Mart\u00b4\u0131nez-Miranda and Nielsen  Here  (cid:90) Th(t) = \u03b1(t)  (cid:18) d2 dt2  (cid:0)\u03b1(cid:48)(cid:48)(t)/\u03b1(t)(cid:1)  = c\u03b24\u03bb  (cid:19)  et\u03b2 (\u03bb + cet\u03b2)2  (cid:16)  \u03bb \u2212 cet\u03b2(cid:17)  ,  and we have made the change of variable y = \u03b1(t), dy = c\u03b2e\u03b2tdt = \u03b2(y \u2212 \u03bb) dt. Similarly  h2(t)\u03b1(t) dt =  (cid:90) \u03bb+c exp(\u03b2T )  c\u03b27\u03bb2 e\u03b2t  (cid:90) \u03bb+c exp(\u03b2T )  \u03b27\u03bb2  =  \u03bb+c  \u03bb+c  = \u03b27\u03bb2  (cid:20) 2\u03bb3  y2 \u2212  8\u03bb2 y  y3 (2\u03bb \u2212 y)2dy = (cid:18) 8 5 4 y3 \u03bb3 \u2212 y2 \u03bb2 \u2212 y  (cid:90) \u03bb+c exp(\u03b2T )  \u03bb+c  (cid:19)  \u03bb + 1  dy  \u2212 5\u03bb ln(y) + y  (cid:21)y=\u03bb+c exp(\u03b2T )  y=\u03bb+c  \u03b27\u03bb  y \u2212 \u03bb y3  (2\u03bb \u2212 y)2dy  We then substitute the above results in the expression of RMBC and take limits for T \u2192 \u221e. To this goal we only look at the leading terms in the numerator and the denominator and we get that RM BC \u2192 \u221e as T \u2192 \u221e. And the ratio increases to \u221e as log(\u03bb+c exp(\u03b2T )), this is, at the linear rate \u03b2T . The limit for T \u2192 0 is (8R(\u0393K))\u22121/6, which for the Epanechnikov kernel is about 0.13.  References  Statist., 6, 701-726.  Aalen, O. O. (1978). Non-parametric inference for a family of counting processes. Ann.  Andersen, P., Borgan, O., Gill, R. and Keiding, N. (1993). Statistical Models Based on  Counting Processes. New York: Springer.  G\u00b4amiz, M. L., Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2016). Double one-sided cross-validation of local linear hazards. J. Royal Statist. Soc. B, 78, 755-779.  G\u00b4amiz, M. L., Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2017). DOvalidation: Kernel Hazard Estimation with Best One-Sided and Double One-Sided Cross-Validation. R package version 1.1.0.  G\u00b4amiz, M. L., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2013a). Smoothing survival  densities in practice. Comput. Statist. Data Anal., 58, 368-382.  G\u00b4amiz, M. L., Janys, L., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2013b). Bandwidth selection in marker dependent kernel hazard estimation. Comput. Statist. Data Anal., 68, 155-169.  Hall, P. and Johnstone, I. (1992). Empirical Functionals and E\ufb03cient Smoothing Parameter  Selection. J. Royal Statist. Soc. B, 54(2), 475-530.  Hall, P. and Marron, J. S. (1987). Extent to which Least-Squares Cross-Validation Minimises Integrated Square Error in Nonparametric Density Estimation. Probab. Theory Rel., 74, 567-581. Best one-sided cross-validation  Hart, J. D. and Yi, S. (1998) One-Sided Cross-Validation. J. Am. Statist. Ass., 93, 620-631.  Hiabu, M., Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D. and Nielsen, J. P. (2016) In-sample  forecasting with local linear survival densities. Biometrika, 103, 843-859.  Jones, M. C., Linton, O. B. and Nielsen, J. P. (1995). A simple bias reduction method for  density estimation. Biometrika, 82, 327-38.  Jones, M. C. and Signorini, D. F. (1997). A comparison of higher-order bias kernel density  estimators. J. Am. Statist. Ass., 92, 1063-1073.  Kapotufe, S. and Verma, N. (2017). Time-Accuracy Tradeo\ufb00s in Kernel Prediction: Con-  trolling Prediction Quality. J. Mach. Learn. Res., 18, 1-29.  Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D., Nielsen, J. P. and Sperlich, S. (2011). Do-validation  for kernel density estimation. J. Am. Statist. Ass., 106, 651-660.  Mammen, E., Mart\u00b4\u0131nez-Miranda, M. D., Nielsen, J. P. and Sperlich, S. (2014). Further theoretical and practical insight to the do-validated bandwidth selector. J. Korean Statist. Soc., 43, 355-365.  Mammen, E. and Nielsen, J. P. (2007). A general approach to the predictability issue in  survival analysis with applications. Biometrika, 94, 873-892.  Mart\u00b4\u0131nez-Miranda, M. D., Nielsen, J. P., Sperlich, S. and Verrall, R. (2013). Continuous Chain Ladder: Reformulating and generalizing a classical insurance problem. Expert Syst. Appl., 40(14), 5588-5603.  Mu\u02dcnoz, I. D. and van der Laan, M. J. (2012). Super Learner Based Conditional Density Estimation with Application to Marginal Structural Models. Int. J. Biostat., 7(1), article 38.  Nielsen, J. P. (1998). Multiplicative bias correction in kernel hazard estimation. Scand. J.  Statist., 25, 541-553.  Nielsen, J. P. and Tanggaard, C. (2001). Boundary and bias correction in kernel hazard  estimation. Scand. J. Statist., 28, 675-698.  Savchuk, O. Y., Hart, J. D., Sheather S. J. (2010). estimation. J. Am. Statist. Ass., 105, 415-423.  Indirect crossvalidation for density  Sheather, S. J. and Jones, M. C. (1991). A reliable data-based bandwidth selection method  for kernel density estimation. J. Royal Statist. Soc. B, 53, 683-690.  van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2007). Super Learner. Statistical  Applications of Genetics and Molecular Biology, 6(25), article 25. "}, "Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-030.html", "header": "Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics", "author": "Yanning Shen, Tianyi Chen, Georgios B. Giannakis", "time": "20(22):1\u221236, 2019.", "abstract": "Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function `on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms.", "pdf_url": "http://jmlr.org/papers/volume20/18-030/18-030.pdf"}, "Determining the Number of Latent Factors in Statistical Multi-Relational Learning": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-037.html", "header": "Determining the Number of Latent Factors in Statistical Multi-Relational Learning", "author": "Chengchun Shi, Wenbin Lu, Rui Song", "time": "20(23):1\u221238, 2019.", "abstract": "Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs.  Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer $s$, RESCAL computes an $s$-dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties.", "pdf_url": "http://jmlr.org/papers/volume20/18-037/18-037.pdf"}, "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-190.html", "header": "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations", "author": "Alberto Bietti, Julien Mairal", "time": "20(25):1\u221249, 2019.", "abstract": "The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.", "pdf_url": "http://jmlr.org/papers/volume20/18-190/18-190.pdf"}, "Iterated Learning in Dynamic Social Networks": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-539.html", "header": "Iterated Learning in Dynamic Social Networks", "author": "Bernard Chazelle, Chu Wang", "time": "20(29):1\u221228, 2019.", "abstract": "A classic finding by (Kalish et al., 2007) shows that no language can be learned iteratively by rational agents in a self-sustained manner. In other words, if $A$ teaches a foreign language to $B$, who then teaches what she learned to $C$, and so on, the language will quickly get lost and agents will wind up teaching their own common native language. If so, how can linguistic novelty ever be sustained? We address this apparent paradox by considering the case of iterated learning in a social network: we show that by varying the lengths of the learning sessions over time or by keeping the networks dynamic, it is possible for iterated learning to endure forever with arbitrarily small loss.", "pdf_url": "http://jmlr.org/papers/volume20/18-539/18-539.pdf"}, "Exact Clustering of Weighted Graphs via Semidefinite Programming": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-128.html", "header": "Exact Clustering of Weighted Graphs via Semidefinite Programming", "author": "Aleksis Pirinen, Brendan Ames", "time": "20(30):1\u221234, 2019.", "abstract": "As a model problem for clustering, we consider the densest $k$-disjoint-clique problem of partitioning a weighted complete graph into $k$ disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of \\(k\\) large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of nodes not belonging to any cluster. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size $n$ of the graph tends to infinity, we can recover clusters of size polylogarithmic in $n$ under certain conditions on the distribution of edge weights. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure.", "pdf_url": "http://jmlr.org/papers/volume20/16-128/16-128.pdf"}, "Kernels for Sequentially Ordered Data": {"volumn": 20, "url": "http://jmlr.org/papers/v20/16-314.html", "header": "Kernels for Sequentially Ordered Data", "author": "Franz J. Kiraly, Harald Oberhauser", "time": "20(31):1\u221245, 2019.", "abstract": "We present a novel framework for learning with sequential data of any kind, such as multivariate time series, strings, or sequences of graphs. The main result is a \u00e2\u0080\u009dsequentialization\u00e2\u0080\u009d that transforms any kernel on a given domain into a kernel for sequences in that domain. This procedure preserves properties such as positive definiteness, the associated kernel feature map is an ordered variant of sample (cross-)moments, and this sequentialized kernel is consistent in the sense that it converges to a kernel for paths if sequences converge to paths (by discretization). Further, classical kernels for sequences arise as special cases of this method. We use dynamic programming and low-rank techniques for tensors to provide efficient algorithms to compute this sequentialized kernel.", "pdf_url": "http://jmlr.org/papers/volume20/16-314/16-314.pdf"}, "NetSDM: Semantic Data Mining with Network Analysis": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-066.html", "header": "NetSDM: Semantic Data Mining with Network Analysis", "author": "Jan Kralj, Marko Robnik-Sikonja, Nada Lavrac", "time": "20(32):1\u221250, 2019.", "abstract": "Semantic data mining (SDM) is a form of relational data mining that uses annotated data together with complex semantic background knowledge to learn rules that can be easily interpreted. The drawback of SDM is a high computational complexity of existing SDM algorithms, resulting in long run times even when applied to relatively small data sets. This paper proposes an effective SDM approach, named NetSDM, which first transforms the available semantic background knowledge into a network format, followed by network analysis based node ranking and pruning to significantly reduce the size of the original background knowledge. The experimental evaluation of the NetSDM methodology on acute lymphoblastic leukemia and breast cancer data demonstrates that NetSDM achieves radical time efficiency improvements and that learned rules are comparable or better than the rules obtained by the original SDM algorithms.", "pdf_url": "http://jmlr.org/papers/volume20/17-066/17-066.pdf"}, "The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-147.html", "header": "The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient", "author": "Roei Gelbhart, Ran El-Yaniv", "time": "20(33):1\u221238, 2019.", "abstract": "A selective classifier $(f,g)$ comprises a classification function $f$ and a binary selection function $g$, which determines if the classifier abstains from prediction, or uses $f$ to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A \u00e2\u0080\u009cfast\u00e2\u0080\u009d rejection rate is achieved if the rejection mass is bounded from above by $\\tilde{O}(1/m)$ where $m$ is the number of labeled examples used to train the classifier (and $\\tilde{O}$ hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke\u00c3\u00a2\u00c2\u0080\u00c2\u0099s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke's disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke's disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called {\\ActiveiLESS}.", "pdf_url": "http://jmlr.org/papers/volume20/17-147/17-147.pdf"}, "Matched Bipartite Block Model with Covariates": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-153.html", "header": "Matched Bipartite Block Model with Covariates", "author": "Zahra S. Razaee, Arash A. Amini, Jingyi Jessica Li", "time": "20(34):1\u221244, 2019.", "abstract": "Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models.", "pdf_url": "http://jmlr.org/papers/volume20/17-153/17-153.pdf"}, "Optimal Policies for Observing Time Series and Related Restless Bandit Problems": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-185.html", "header": "Optimal Policies for Observing Time Series and Related Restless Bandit Problems", "author": "Christopher R. Dance, Tomi Silander", "time": "20(35):1\u221293, 2019.", "abstract": "The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance. It is based on a new verification theorem by Nino-Mora that guarantees threshold structure for Markov decision processes, and on the relation between binary sequences known as Christoffel words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology. This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index policy. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies.", "pdf_url": "http://jmlr.org/papers/volume20/17-185/17-185.pdf"}, "A New Approach to Laplacian Solvers and Flow Problems": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-286.html", "header": "A New Approach to Laplacian Solvers and Flow Problems", "author": "Patrick Rebeschini, Sekhar Tatikonda", "time": "20(36):1\u221237, 2019.", "abstract": "This paper investigates the behavior of the Min-Sum message passing scheme to solve systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. Voltage and flow problems involve the minimization of quadratic functions and are fundamental primitives that arise in several domains. Algorithms that have been proposed are typically centralized and involve multiple graph-theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze Min-Sum to solve voltage and flow problems. We characterize the error committed by the algorithm on general weighted graphs in terms of hitting times of random walks defined on the computation trees that support the operations of the algorithms with time. For $d$-regular graphs with equal weights, we show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks defined on the original graph that start from neighboring nodes. The framework that we introduce extends the analysis of Min-Sum to settings where the contraction arguments previously considered in the literature (based on the assumption of walk summability or scaled diagonal dominance) can not be used, possibly in the presence of constraints.", "pdf_url": "http://jmlr.org/papers/volume20/17-286/17-286.pdf"}, "A Well-Tempered Landscape for Non-convex Robust Subspace Recovery": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-324.html", "header": "A Well-Tempered Landscape for Non-convex Robust Subspace Recovery", "author": "Tyler Maunu, Teng Zhang, Gilad Lerman", "time": "20(37):1\u221259, 2019.", "abstract": "We present a mathematical analysis of a non-convex energy landscape for robust subspace recovery. We prove that an underlying subspace is the only stationary point and local minimizer in a specified neighborhood under a deterministic condition on a dataset. If the deterministic condition is satisfied, we further show that a geodesic gradient descent method over the Grassmannian manifold can exactly recover the underlying subspace when the method is properly initialized. Proper initialization by principal component analysis is guaranteed with a simple deterministic condition. Under slightly stronger assumptions, the gradient descent method with a piecewise constant step-size scheme achieves linear convergence. The practicality of the deterministic condition is demonstrated on some statistical models of data, and the method achieves almost state-of-the-art recovery guarantees on the Haystack Model for different regimes of sample size and ambient dimension. In particular, when the ambient dimension is fixed and the sample size is large enough, we show that our gradient method can exactly recover the underlying subspace for any fixed fraction of outliers (less than 1).", "pdf_url": "http://jmlr.org/papers/volume20/17-324/17-324.pdf"}, "Approximation Hardness for A Class of Sparse Optimization Problems": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-373.html", "header": "Approximation Hardness for A Class of Sparse Optimization Problems", "author": "Yichen Chen, Yinyu Ye, Mengdi Wang", "time": "20(38):1\u221227, 2019.", "abstract": "In this paper, we consider three typical optimization problems with a convex loss function and a nonconvex sparse penalty or constraint. For the sparse penalized problem, we prove that finding an $\\mathcal{O}(n^{c_1}d^{c_2})$-optimal solution to an $n\\times d$ problem is strongly NP-hard for any $c_1, c_2\\in [0,1)$ such that $c_1+c_2<1$. For two constrained versions of the sparse optimization problem, we show that it is intractable to approximately compute a solution path associated with increasing values of some tuning parameter. The hardness results apply to a broad class of loss functions and sparse penalties. They suggest that one cannot even approximately solve these three problems in polynomial time, unless P $=$ NP.", "pdf_url": "http://jmlr.org/papers/volume20/17-373/17-373.pdf"}, "A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-451.html", "header": "A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication", "author": "Miles E. Lopes, Shusen Wang, Michael W. Mahoney", "time": "20(39):1\u221240, 2019.", "abstract": "In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random app", "pdf_url": "http://jmlr.org/papers/volume20/17-451/17-451.pdf"}, "Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-526.html", "header": "Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations", "author": "Qianxiao Li, Cheng Tai, Weinan E", "time": "20(40):1\u221247, 2019.", "abstract": "We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.", "pdf_url": "http://jmlr.org/papers/volume20/17-526/17-526.pdf"}, "Decontamination of Mutual Contamination Models": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-576.html", "header": "Decontamination of Mutual Contamination Models", "author": "Julian Katz-Samuels, Gilles Blanchard, Clayton Scott", "time": "20(41):1\u221257, 2019.", "abstract": "Many machine learning problems can be characterized by \\emph{mutual contamination models}. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees.", "pdf_url": "http://jmlr.org/papers/volume20/17-576/17-576.pdf"}, "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-608.html", "header": "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization", "author": "Lin Xiao, Adams Wei Yu, Qihang Lin, Weizhu Chen", "time": "20(43):1\u221258, 2019.", "abstract": "Machine learning with big data often involves large optimization models. For distributed optimization over a cluster of machines, frequent communication and synchronization of all model parameters (optimization variables) can be very costly. A promising solution is to use parameter servers to store different subsets of the model parameters, and update them asynchronously at different machines using local datasets. In this paper, we focus on distributed optimization of large linear models with convex loss functions, and propose a family of randomized primal-dual block coordinate algorithms that are especially suitable for asynchronous distributed implementation with parameter servers. In particular, we work with the saddle-point formulation of such problems which allows simultaneous data and model partitioning, and exploit its structure by doubly stochastic coordinate optimization with variance reduction (DSCOVR). Compared with other first-order distributed algorithms, we show that DSCOVR may require less amount of overall computation and communication, and less or no synchronization. We discuss the implementation details of the DSCOVR algorithms, and present numerical experiments on an industrial distributed computing system.", "pdf_url": "http://jmlr.org/papers/volume20/17-608/17-608.pdf"}, "Robust Frequent Directions with Application in Online Learning": {"volumn": 20, "url": "http://jmlr.org/papers/v20/17-773.html", "header": "Robust Frequent Directions with Application in Online Learning", "author": "Luo Luo, Cheng Chen, Zhihua Zhang, Wu-Jun Li, Tong Zhang", "time": "20(45):1\u221241, 2019.", "abstract": "The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deficient matrices. To overcome the rank deficiency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameter-free online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms.", "pdf_url": "http://jmlr.org/papers/volume20/17-773/17-773.pdf"}, "Analysis of spectral clustering algorithms for community detection: the general bipartite setting": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-170.html", "header": "Analysis of spectral clustering algorithms for community detection: the general bipartite setting", "author": "Zhixin Zhou, Arash A.Amini", "time": "20(47):1\u221247, 2019.", "abstract": "We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a kmeans type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.", "pdf_url": "http://jmlr.org/papers/volume20/18-170/18-170.pdf"}, "Efficient augmentation and relaxation learning for individualized treatment rules using observational data": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-191.html", "header": "Efficient augmentation and relaxation learning for individualized treatment rules using observational data", "author": "Ying-Qi Zhao, Eric B. Laber, Yang Ning, Sumona Saha, Bruce E. Sands", "time": "20(48):1\u221223, 2019.", "abstract": "Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome.", "pdf_url": "http://jmlr.org/papers/volume20/18-191/18-191.pdf"}, "Using Simulation to Improve Sample-Efficiency of Bayesian Optimization for Bipedal Robots": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-196.html", "header": "Using Simulation to Improve Sample-Efficiency of Bayesian Optimization for Bipedal Robots", "author": "Akshara Rai, Rika Antonova, Franziska Meier, Christopher G. Atkeson", "time": "20(49):1\u221224, 2019.", "abstract": "Learning for control can acquire controllers for novel robotic tasks, paving the path for autonomous agents. Such controllers can be expert-designed policies, which typically require tuning of parameters for each task scenario. In this context, Bayesian optimization (BO) has emerged as a promising approach for automatically tuning controllers. However, sample-efficiency can still be an issue for high-dimensional policies on hardware. Here, we develop an approach that utilizes simulation to learn structured feature transforms that map the original parameter space into a domain-informed space. During BO, similarity between controllers is now calculated in this transformed space. Experiments on the ATRIAS robot hardware and simulation show that our approach succeeds at sample-efficiently learning controllers for multiple robots. Another question arises: What if the simulation significantly differs from hardware? To answer this, we create increasingly approximate simulators and study the effect of increasing simulation-hardware mismatch on the performance of Bayesian optimization. We also compare our approach to other approaches from literature, and find it to be more reliable, especially in cases of high mismatch. Our experiments show that our approach succeeds across different controller types, bipedal robot models and simulator fidelity levels, making it applicable to a wide range of bipedal locomotion problems.", "pdf_url": "http://jmlr.org/papers/volume20/18-196/18-196.pdf"}, "No-Regret Bayesian Optimization with Unknown Hyperparameters": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-213.html", "header": "No-Regret Bayesian Optimization with Unknown Hyperparameters", "author": "Felix Berkenkamp, Angela P. Schoellig, Andreas Krause", "time": "20(50):1\u221224, 2019.", "abstract": "Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.", "pdf_url": "http://jmlr.org/papers/volume20/18-213/18-213.pdf"}, "Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures": {"volumn": 20, "url": "http://jmlr.org/papers/v20/18-241.html", "header": "Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures", "author": "Gregor Pir\u00c5\u00a1, Erik \u00c5\u00a0trumbelj", "time": "20(51):1\u221218, 2019.", "abstract": "Ensemble methods are a powerful tool, often outperforming individual prediction models. Existing Bayesian ensembles either do not model the correlations between sources, or they are only capable of combining non-probabilistic predictions. We propose a new model, which overcomes these disadvantages. Transforming the probabilistic predictions with the inverse additive logistic transformation allows us to model the correlations with multivariate normal mixtures. We derive an efficient Gibbs sampler for the proposed model and implement a regularization method to make it more robust. We compare our method to related work and the classical linear opinion pool. Empirical evaluation on several toy and real-world data sets, including a case study on air-pollution forecasting, shows that the method outperforms other methods, while being robust and easy to use.", "pdf_url": "http://jmlr.org/papers/volume20/18-241/18-241.pdf"}}