{"Numerical Analysis near Singularities in RBF Networks": {"volumn": 19, "url": "http://jmlr.org/papers/v19/16-210.html", "header": "Numerical Analysis near Singularities in RBF Networks", "author": "Weili Guo, Haikun Wei (Corresponding author), Yew-Soon Ong, Jaime Rubio Hervas, Junsheng Zhao, Hai Wang, Kanjian Zhang", "time": "19(1):1\u221239, 2018.", "abstract": "The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks.", "pdf_url": "http://jmlr.org/papers/volume19/16-210/16-210.pdf", "reference": "276, 1998.  S. Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):251\u2013  S. Amari and H. Nagaoka. Methods of Information Geometry. AMS and Oxford University,  New York, USA, 2000.  S. Amari and T. Ozeki. Di\ufb00erential and algebraic geometry of multilayer perceptrons. IEICE Transactions on Fundamentals of Electronics, Communications and Computer System, E84(A(1)):31\u201338, 2001.  S. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning  for multilayer perceptrons. Neural Computation, 12(6):1399\u20131409, 2000.  S. Amari, H. Park, and T. Ozeki. Singularities a\ufb00ect dynamics of learning in neuromanifolds.  Neural Computation, 18(5):1007\u20131065, 2006.  S. Amari, T. Ozeki, F. Cousseau, and H. Wei. Dynamics of learning in hierarchical models\u2014 singularity and milnor attractor. Second International Conference on Cognitive Neuro- dynamics, pages 3\u20139, 2009.  M. Aoyagi. Stochastic complexity and generalization error of a restricted Boltzmann ma- chine in Bayesian estimation. Journal of Machine Learning Research, 11:1243\u20131272, 2010.  Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient  descent is di\ufb03cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.  M. Biehl and H. Schwarze. Learning by on-line gradient descent. Journal of Physics A:  Mathematical and General, 28(3):643\u2013656, 1995.  A. Borji and L. Itti. State-of-the-art in visual attention modeling. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 35(1):185\u2013207, 2013.  A. Choromanska, M. Hena\ufb00, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surface of multilayer networks. 18th International Conference on Arti\ufb01cial Intelligence and Statis- tics(AISTATS 2015), pages 192\u2013204, 2015.  F. Cousseau, T. Ozeki, and S. Amari. Dynamics of learning in multilayer perceptrons near  singularities. IEEE Transactions on Neural Networks, 19(8):1313\u20131328, 2008.  Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in Neural Information Processing Systems (NIPS 2014), pages 2933\u20132941, 2014.  D. Erhan, P. A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent. The di\ufb03culty of training deep architectures and the e\ufb00ect of unsupervised pre-training. 12th International Con- ference on Arti\ufb01cial Intelligence and Statistics (AISTATS 2009), pages 153\u2013160, 2009.  K. Fukumizu. A regularity condition of information matrix of a multilayer perceptron  network. Neural Networks, 9(5):871\u2013879, 1996.  36   GUO, WEI, ONG, HERVAS, ZHAO, WANG and ZHANG  References  276, 1998.  S. Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):251\u2013  S. Amari and H. Nagaoka. Methods of Information Geometry. AMS and Oxford University,  New York, USA, 2000.  S. Amari and T. Ozeki. Di\ufb00erential and algebraic geometry of multilayer perceptrons. IEICE Transactions on Fundamentals of Electronics, Communications and Computer System, E84(A(1)):31\u201338, 2001.  S. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning  for multilayer perceptrons. Neural Computation, 12(6):1399\u20131409, 2000.  S. Amari, H. Park, and T. Ozeki. Singularities a\ufb00ect dynamics of learning in neuromanifolds.  Neural Computation, 18(5):1007\u20131065, 2006.  S. Amari, T. Ozeki, F. Cousseau, and H. Wei. Dynamics of learning in hierarchical models\u2014 singularity and milnor attractor. Second International Conference on Cognitive Neuro- dynamics, pages 3\u20139, 2009.  M. Aoyagi. Stochastic complexity and generalization error of a restricted Boltzmann ma- chine in Bayesian estimation. Journal of Machine Learning Research, 11:1243\u20131272, 2010.  Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient  descent is di\ufb03cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.  M. Biehl and H. Schwarze. Learning by on-line gradient descent. Journal of Physics A:  Mathematical and General, 28(3):643\u2013656, 1995.  A. Borji and L. Itti. State-of-the-art in visual attention modeling. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 35(1):185\u2013207, 2013.  A. Choromanska, M. Hena\ufb00, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surface of multilayer networks. 18th International Conference on Arti\ufb01cial Intelligence and Statis- tics(AISTATS 2015), pages 192\u2013204, 2015.  F. Cousseau, T. Ozeki, and S. Amari. Dynamics of learning in multilayer perceptrons near  singularities. IEEE Transactions on Neural Networks, 19(8):1313\u20131328, 2008.  Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in Neural Information Processing Systems (NIPS 2014), pages 2933\u20132941, 2014.  D. Erhan, P. A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent. The di\ufb03culty of training deep architectures and the e\ufb00ect of unsupervised pre-training. 12th International Con- ference on Arti\ufb01cial Intelligence and Statistics (AISTATS 2009), pages 153\u2013160, 2009.  K. Fukumizu. A regularity condition of information matrix of a multilayer perceptron  network. Neural Networks, 9(5):871\u2013879, 1996. Numerical Analysis near Singularities in RBF Networks  K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structure of multi-  layer perceptrons. Neural Networks, 13(3):317\u2013327, 2000.  I. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network  optimization problems. URL: http://arxiv. org/abs/1412.6544, 2014.  I. Goodfellow, Y. Bengio, and A. Courville.  Deep learning. MIT Press URL:  http://www.deeplearningbook.org, 2016.  C. Gulcehre, J. Sotelo, M. Moczulski, and Y. Bengio. A robust adaptive stochastic gradient method for deep learning. International Joint Conference on Neural Networks (IJCNN 2017), pages 125\u2013132, 2017.  W. Guo, H. Wei, J. Zhao, and K. Zhang. Averaged learning equations of error-function- based multilayer perceptrons. Neural Computing & Applications, 25(3-4):825\u2013832, 2014.  W. Guo, H. Wei, J. Zhao, and K. Zhang. Theoretical and numerical analysis of learning dynamics near singularity in multilayer perceptrons. Neurocomputing, 151:390\u2013400, 2015.  T. Heskes. On \u201dnatural\u201d learning and pruning in multilayered perceptrons. Neural Com-  putation, 12(4):881\u2013901, 2000.  G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets.  Neural Computation, 18(7):1527\u20131554, 2006.  G. Lee, Y. Tai, and J. Kim. Deep saliency with encoded low level distance map and high level features. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016), pages 660\u2013668, 2016.  Z. C. Lipton.  Stuck in a what?  adventures  in weight  space.  URL: http-  s://arxiv.org/abs/1602.07320, 2016.  T. Mononen. A case study of the widely applicable Bayesian information criterion and its  optimality. Statistics and Computing, 25(5):929\u2013940, 2015.  T. Nitta. Local minima in hierarchical structures of complex-valued neural networks. Neural  Networks, 43:1\u20137, 2013.  T. Nitta. Learning dynamics of a single polar variable complex-valued neuron. Neural  Computation, 27(5):1120\u20131141, 2015.  H. Park and T. Ozeki. Singularity and slow convergence of the EM algorithm for Gaussian  mixtures. Neural Processing Letters, 29(1):45\u201359, 2009.  H. Park, S. Amari, and K. Fukumizu. Adaptive natural gradient learning algorithms for  various stochastic models. Neural Networks, 13(7):755\u2013764, 2000.  H. Park, M. Inoue, and M. Okada. Online learning dynamics of multilayer perceptrons with unidenti\ufb01able parameters. Journal of Physics A: Mathematical and General, 36 (47):11753\u201311764, 2003. GUO, WEI, ONG, HERVAS, ZHAO, WANG and ZHANG  R. Pascanu and Y. Bengio. Revisiting natural gradient for deep networks. URL:  http://arxiv.org/abs/1301.3584v7, 2014.  M. Rattray, D. Saad, and S. Amari. Natural gradient descent for on-line learning. Physical  Review Letters, 81(24):5461\u20135464, 1998.  D. Saad and A.Solla. Exact solution for online learning in multilayer neural networks.  Physical Review Letters, 74(21):4337\u20134340, 1995.  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. URL: http://arXiv preprint arXiv:1312.6120, 2014.  J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:  85\u2013117, 2015.  H. van Hasselt, A. Guez, M. Hessel, and D. Silver. Learning functions across many orders  of magnitudes. URL: http://arXiv preprint arXiv:1602.07714, 2016.  S. Watanabe. Algebraic analysis for non-identi\ufb01able learning machines. Neural Computa-  tion, 13(4):899\u2013933, 2001a.  S. Watanabe. Algebraic geometrical methods for hierarchical learning machines. Neural  Works, 14(8):1049\u20131060, 2001b.  S. Watanabe. Almost all learning machines are singular. IEEE Symposium on Foundations  of Computational Intelligence (FOCI 2007), pages 383\u2013388, 2007.  S. Watanabe. Asymptotic equivalence of Bayes cross validation and widely applicable in- formation criterion in singular learning theory. Journal of Machine Learning Research, 11:3571\u20133594, 2010.  S. Watanabe. A widely applicable Bayesian information criterion. Journal of Machine  Learning Research, 14:867\u2013897, 2013.  H. Wei and S. Amari. Dynamics of learning near singularities in radial basis function  networks. Neural Networks, 21(7):989\u20131005, 2008.  H. Wei, Q. Li, and W. Song. Gradient learning dynamics of radial basis function networks.  Control Theory & Applications, 24(3):356\u2013360, 2007 (in Chinese).  H. Wei, J. Zhang, F. Cousseau, T. Ozeki, and S. Amari. Dynamics of learning near singu-  larities in layered networks. Neural Computation, 20(34):813\u2013843, 2008.  Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detection. IEEE Conference on  Computer Vision and Pattern Recognition (CVPR 2013), pages 1155\u20131162, 2013.  J. Zhang, J. Ding, and J. Yang. Exploiting global rarity, local contrast and central bias for  salient region learning. Neurocomputing, 144:569\u2013580, 2014. Numerical Analysis near Singularities in RBF Networks  J. Zhang, K. A. Ehinger, H. Wei, K. Zhang, and J. Yang. A novel graph-based optimization  framework for salient object detection. Pattern Recognition, 64:39\u201350, 2017.  J. Zhao, H. Wei, C. Zhang, W. Li, W. Guo, and K. Zhang. Natural gradient learning  algorithms for RBF networks. Neural Computation, 27(2):481\u2013505, 2015a.  R. Zhao, W. Ouyang, H. Li, and X. Wang. Daliency detection by multi-context deep learning. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015), pages 1265\u20131274, 2015b. "}, "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference": {"volumn": 19, "url": "http://jmlr.org/papers/v19/16-656.html", "header": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference", "author": "Ahmed M. Alaa, Mihaela van der Schaar", "time": "19(4):1\u221262, 2018.", "abstract": "Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the  Hidden Absorbing Semi-Markov Model  (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel   forward-filtering backward-sampling  Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time  embedded Markov chain  that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22$\\%$ AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology.", "pdf_url": "http://jmlr.org/papers/volume19/16-656/16-656.pdf", "reference": "Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je\ufb00rey Dean, Matthieu Devin, Sanjay Ghemawat, Geo\ufb00rey Irving, Michael Isard, et al. Tensor\ufb02ow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.  Ahmed M. Alaa and Mihaela van der Schaar. Balancing suspense and surprise: Timely deci- sion making with endogenous information acquisition. In Advances in Neural Information Processing Systems, pages 2910\u20132918, 2016.  Ahmed M Alaa, Jinsung Yoon, Scott Hu, and Mihaela van der Schaar. Personalized risk scoring for critical care prognosis using mixtures of gaussian processes. arXiv preprint arXiv:1610.08853, 2016.  Ahmed M. Alaa, Scott Hu, and Mihaela van der Schaar. Learning from clinical judgments: Semi-markov-modulated marked hawkes processes for risk prognosis. Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.  Je\ufb00rey A Bakal, Finlay A McAlister, Wei Liu, and Justin A Ezekowitz. Heart failure re-admission: measuring the ever shortening gap between repeat heart failure hospital- izations. PloS one, 9(9):e106494, 2014.  Jirina Bartkova, Zuzana Ho\u02c7rej\u02c7s\u00b4\u0131, Karen Koed, Alwin Kr\u00a8amer, Frederic Tort, Karsten Zieger, Per Guldberg, Maxwell Sehested, Jahn M Nesland, Claudia Lukas, et al. Dna damage response as a candidate anti-cancer barrier in early human tumorigenesis. Nature, 434 (7035):864\u2013870, 2005.  Kevin Beier, Sabitha Eppanapally, Heidi S Bazick, Domingo Chang, Karthik Mahadevappa, Fiona K Gibbons, and Kenneth B Christopher. Elevation of bun is predictive of long- term mortality in critically ill patients independent of\u2019normal\u2019creatinine. Critical care medicine, 39(2):305, 2011.  Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction. In Advances in neural information processing systems, pages 153\u2013160, 2007.  James G Booth and James P Hobert. Maximizing generalized linear mixed model likelihoods with an automated monte carlo em algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(1):265\u2013285, 1999.  Brian S Ca\ufb00o, Wolfgang Jank, and Galin L Jones. Ascent-based monte carlo expectation\u2013 maximization. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):235\u2013251, 2005.  Lucienne TQ Cardoso, Cintia MC Grion, Tiemi Matsuo, Elza HT Anami, Ivanil AM Kauss, Ludmila Seko, and Ana M Bonametti. Impact of delayed admission to intensive care units on mortality of critically ill patients: a cohort study. Critical Care, 15(1):R28, 2011.  Chris K Carter and Robert Kohn. On gibbs sampling for state space models. Biometrika,  81(3):541\u2013553, 1994.  54   Alaa and van der Schaar  References  Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je\ufb00rey Dean, Matthieu Devin, Sanjay Ghemawat, Geo\ufb00rey Irving, Michael Isard, et al. Tensor\ufb02ow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.  Ahmed M. Alaa and Mihaela van der Schaar. Balancing suspense and surprise: Timely deci- sion making with endogenous information acquisition. In Advances in Neural Information Processing Systems, pages 2910\u20132918, 2016.  Ahmed M Alaa, Jinsung Yoon, Scott Hu, and Mihaela van der Schaar. Personalized risk scoring for critical care prognosis using mixtures of gaussian processes. arXiv preprint arXiv:1610.08853, 2016.  Ahmed M. Alaa, Scott Hu, and Mihaela van der Schaar. Learning from clinical judgments: Semi-markov-modulated marked hawkes processes for risk prognosis. Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.  Je\ufb00rey A Bakal, Finlay A McAlister, Wei Liu, and Justin A Ezekowitz. Heart failure re-admission: measuring the ever shortening gap between repeat heart failure hospital- izations. PloS one, 9(9):e106494, 2014.  Jirina Bartkova, Zuzana Ho\u02c7rej\u02c7s\u00b4\u0131, Karen Koed, Alwin Kr\u00a8amer, Frederic Tort, Karsten Zieger, Per Guldberg, Maxwell Sehested, Jahn M Nesland, Claudia Lukas, et al. Dna damage response as a candidate anti-cancer barrier in early human tumorigenesis. Nature, 434 (7035):864\u2013870, 2005.  Kevin Beier, Sabitha Eppanapally, Heidi S Bazick, Domingo Chang, Karthik Mahadevappa, Fiona K Gibbons, and Kenneth B Christopher. Elevation of bun is predictive of long- term mortality in critically ill patients independent of\u2019normal\u2019creatinine. Critical care medicine, 39(2):305, 2011.  Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction. In Advances in neural information processing systems, pages 153\u2013160, 2007.  James G Booth and James P Hobert. Maximizing generalized linear mixed model likelihoods with an automated monte carlo em algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(1):265\u2013285, 1999.  Brian S Ca\ufb00o, Wolfgang Jank, and Galin L Jones. Ascent-based monte carlo expectation\u2013 maximization. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):235\u2013251, 2005.  Lucienne TQ Cardoso, Cintia MC Grion, Tiemi Matsuo, Elza HT Anami, Ivanil AM Kauss, Ludmila Seko, and Ana M Bonametti. Impact of delayed admission to intensive care units on mortality of critically ill patients: a cohort study. Critical Care, 15(1):R28, 2011.  Chris K Carter and Robert Kohn. On gibbs sampling for state space models. Biometrika,  81(3):541\u2013553, 1994. An HASMM Model for Informatively Censored Temporal Data  Dustin Charles, Meghan Gabriel, and JaWanna Henry. Electronic capabilities for patient engagement among us non-federal acute care hospitals: 2012-2014. The O\ufb03ce of the National Coordinator for Health Information Technology, 2015.  Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Re- current neural networks for multivariate time series with missing values. arXiv preprint arXiv:1606.01865, 2016.  Baojiang Chen and Xiao-Hua Zhou. Non-homogeneous markov process models with infor- mative observations with an application to alzheimer\u2019s disease. Biometrical Journal, 53 (3):444\u2013463, 2011.  Jill M Cholette, Kelly F Henrichs, George M Al\ufb01eris, Karen S Powers, Richard Phipps, Sherry L Spinelli, Michael Swartz, Francisco Gensini, L Eugene Daugherty, Emily Nazar- ian, et al. Washing red blood cells and platelets transfused in cardiac surgery reduces post-operative in\ufb02ammation and number of transfusions: Results of a prospective, ran- domized, controlled clinical trial. Pediatric critical care medicine: a journal of the Society of Critical Care Medicine and the World Federation of Pediatric Intensive and Critical Care Societies, 13(3), 2012.  Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning, pages 233\u2013240. ACM, 2006.  Zelalem Getahun Dessie. Multi-state models of hiv/aids by homogeneous semi-markov  process. American Journal of Biostatistics, 4(2):21, 2014.  Michael Dewar, Chris Wiggins, and Frank Wood. Inference in hidden markov models with explicit state duration distributions. IEEE Signal Processing Letters, 19(4):235\u2013238, 2012.  Rick Durrett. Probability: theory and examples. Cambridge university press, 2010.  Allison A Eddy and Eric G Neilson. Chronic kidney disease progression. Journal of the  American Society of Nephrology, 17(11):2964\u20132966, 2006.  Yohann Foucher, Eve Mathieu, Philippe Saint-Pierre, J Durand, and J Daures. A semi- markov model based on generalized weibull distribution with an illustration for hiv dis- ease. Biometrical journal, 47(6):825, 2005.  Yohann Foucher, Magali Giral, Jean-Paul Soulillou, and Jean-Pierre Daures. A semi-markov model for multistate and interval-censored data with multiple terminal events. application in renal transplantation. Statistics in medicine, 26(30):5381\u20135393, 2007.  Yohann Foucher, M Giral, JP Soulillou, and JP Daures. A \ufb02exible semi-markov model for interval-censored data and goodness-of-\ufb01t testing. Statistical methods in medical research, 2008.  Emily Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. Bayesian nonparametric inference of switching dynamic linear models. IEEE Transactions on Signal Processing, 59(4):1569\u20131585, 2011a. Alaa and van der Schaar  Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. A sticky hdp- hmm with application to speaker diarization. The Annals of Applied Statistics, pages 1020\u20131056, 2011b.  Mitchell H Gail and Phuong L Mai. Comparing breast cancer risk assessment models.  Journal of the National Cancer Institute, 102(10):665\u2013668, 2010.  Valentine Genon-Catalot, Thierry Jeantheau, Catherine Lar\u00b4edo, et al. Stochastic volatility models as hidden markov models and statistical applications. Bernoulli, 6(6):1051\u20131079, 2000.  Konstantinos Georgatzis, Christopher KI Williams, and Christopher Hawthorne.  Input- output non-linear dynamical systems applied to physiological condition monitoring. Jour- nal of Machine Learning Research, 2016.  Zoubin Ghahramani and Michael I Jordan. Factorial hidden markov models. Machine  learning, 29(2-3):245\u2013273, 1997.  Marzyeh Ghassemi, Marco AF Pimentel, Tristan Naumann, Thomas Brennan, David A Clifton, Peter Szolovits, and Mengling Feng. A multivariate timeseries modeling ap- proach to severity of illness assessment and forecasting in icu with sparse, heterogeneous clinical data. In Proceedings of the... AAAI Conference on Arti\ufb01cial Intelligence. AAAI Conference on Arti\ufb01cial Intelligence, volume 2015, page 446. NIH Public Access, 2015.  Giacomo Giampieri, Mark Davis, and Martin Crowder. Analysis of default data using  hidden markov models. Quantitative Finance, 5(1):27\u201334, 2005.  Florence Gillaizeau, Etienne Dantan, Magali Giral, and Yohann Foucher. A multistate additive relative survival semi-markov model. Statistical methods in medical research, page 0962280215586456, 2015.  Simon J Godsill, Arnaud Doucet, and Mike West. Monte carlo smoothing for nonlinear  time series. Journal of the american statistical association, 99(465):156\u2013168, 2004.  Peter J Green and David I Hastie. Reversible jump mcmc. Genetics, 155(3):1391\u20131403,  2009.  She\ufb03eld ML group. Gpy: A gaussian process framework in python. 2012.  Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. Hidden topic markov models. In AISTATS,  volume 7, pages 163\u2013170, 2007.  Yann Gu\u00b4edon. Exploring the state sequence space for hidden markov and semi-markov  chains. Computational Statistics & Data Analysis, 51(5):2379\u20132409, 2007.  Chantal Guihenneuc-Jouyaux, Sylvia Richardson, and Ira M Longini. Modeling markers of disease progression by a hidden markov process: application to characterizing cd4 cell decline. Biometrics, 56(3):733\u2013741, 2000. An HASMM Model for Informatively Censored Temporal Data  Tracy D Gunter and Nicolas P Terry. The emergence of national electronic health record architectures in the united states and australia: models, costs, and questions. Journal of medical Internet research, 7(1):e3, 2005.  Alan G Hawkes and David Oakes. A cluster process representation of a self-exciting process.  Journal of Applied Probability, pages 493\u2013503, 1974.  CharlesO Hershey and Linda Fisher. Why outcome of cardiopulmonary resuscitation in  general wards is poor. The Lancet, 319(8262):31\u201334, 1982.  Asger Hobolth and Jens Ledet Jensen.  Summary statistics for endpoint-conditioned  continuous-time markov chains. Journal of Applied Probability, pages 911\u2013924, 2011.  Helen Hogan, Frances Healey, Graham Neale, Richard Thomson, Charles Vincent, and Nick Black. Preventable deaths due to problems in care in english acute hospitals: a retrospective case record review study. BMJ quality & safety, pages bmjqs\u20132012, 2012.  William Hoiles and Mihaela van der Schaar. A non-parametric learning method for con\ufb01- dently estimating patient\u2019s clinical state and dynamics. In Advances in Neural Informa- tion Processing Systems, pages 2020\u20132028, 2016.  George Hripcsak, David J Albers, and Adler Perotte. Parameterizing time in electronic health record studies. Journal of the American Medical Informatics Association, 22(4): 794\u2013804, 2015.  Xuelin Huang and Robert A Wolfe. A frailty model for informative censoring. Biometrics,  58(3):510\u2013520, 2002.  2004.  Aparna V Huzurbazar. Multistate models, \ufb02owgraph models, and semi-markov processes.  Christopher H Jackson, Linda D Sharples, Simon G Thompson, Stephen W Du\ufb00y, and Elisabeth Couto. Multistate markov models for disease progression with classi\ufb01cation error. Journal of the Royal Statistical Society: Series D (The Statistician), 52(2):193\u2013 209, 2003.  Jacques Janssen and R De Dominicis. Finite non-homogeneous semi-markov processes: Theoretical and computational aspects. Insurance: Mathematics and Economics, 3(3): 157\u2013165, 1984.  Daniel W Johnson, Ulrich H Schmidt, Edward A Bittner, Benjamin Christensen, Retsef Levi, and Richard M Pino. Delay of transfer from the intensive care unit: a prospective observational study of incidence, causes, and \ufb01nancial impact. Critical Care, 17(4):R128, 2013.  Matthew J Johnson and Alan S Willsky. Bayesian nonparametric hidden semi-markov  models. Journal of Machine Learning Research, 14(Feb):673\u2013701, 2013.  Pierre Joly and Daniel Commenges. A penalized likelihood approach for a progressive three- state model with censored and truncated data: Application to aids. Biometrics, 55(3): 887\u2013890, 1999. Alaa and van der Schaar  Jared Katzman, Uri Shaham, Jonathan Bates, Alexander Cloninger, Tingting Jiang, and Yuval Kluger. Deep survival: A deep cox proportional hazards network. arXiv preprint arXiv:1606.00931, 2016.  Juliane Kause, Gary Smith, David Prytherch, Michael Parr, Arthas Flabouris, Ken Hillman, et al. A comparison of antecedents to cardiac arrests, deaths and emergency intensive care admissions in australia and new zealand, and the united kingdom\u2014the academia study. Resuscitation, 62(3):275\u2013282, 2004.  Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv  preprint arXiv:1412.6980, 2014.  Lisa L Kirkland, Michael Malinchoc, Megan O\u2019Byrne, Joanne T Benson, Deanne T Kashi- wagi, M Caroline Burton, Prathibha Varkey, and Timothy I Morgenthaler. A clinical deterioration prediction tool for internal medicine patients. American Journal of Medical Quality, 28(2):135\u2013142, 2013.  William A Knaus, Elizabeth A Draper, Douglas P Wagner, and Jack E Zimmerman. Apache ii: a severity of disease classi\ufb01cation system. Critical care medicine, 13(10):818\u2013829, 1985.  William A Knaus, Douglas P Wagner, Elizabeth A Draper, Jack E Zimmerman, Marilyn Bergner, Paulo G Bastos, Carl A Sirio, Donald J Murphy, Ted Lotring, and Anne Dami- ano. The apache iii prognostic system. risk prediction of hospital mortality for critically ill hospitalized adults. Chest Journal, 100(6):1619\u20131636, 1991.  Vidyadhar G Kulkarni. Modeling and analysis of stochastic systems. CRC Press, 1996.  Stephan W Lagakos, Charles J Sommer, and Marvin Zelen. Semi-markov models for par-  tially censored data. Biometrika, 65(2):311\u2013317, 1978.  David Lando. On cox processes and credit risky securities. Review of Derivatives research,  2(2-3):99\u2013120, 1998.  2015.  Laura Landro. Hospitals \ufb01nd new ways to monitor patients 24/7. The Wall Street Journal,  Jose Leiva-Murillo, AA Rodrguez, and E Baca-Garca. Visualization and prediction of disease interactions with continuous-time hidden markov models. In NIPS 2011 Workshop on Personalized Medicine, 2011.  H Lehman Li-wei, Shamim Nemati, Ryan P Adams, George Moody, Atul Malhotra, and Roger G Mark. Tracking progression of patient state of health in critical care using inferred shared dynamics in physiological time series. In 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 7072\u20137075. IEEE, 2013.  William A Link. A model for informative censoring. Journal of the American Statistical  Association, 84(407):749\u2013752, 1989. An HASMM Model for Informatively Censored Temporal Data  Zachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences with rnns: Improved classi\ufb01cation of clinical time series. In Machine Learning for Healthcare Conference, pages 253\u2013270, 2016.  Yu-Ying Liu, Shuang Li, Fuxin Li, Le Song, and James M Rehg. E\ufb03cient learning of continuous-time hidden markov models for disease progression. In Advances in neural information processing systems, pages 3600\u20133608, 2015.  Sergio Matos, Surinder S Birring, Ian D Pavord, and H Evans. Detection of cough signals IEEE Transactions on  in continuous audio recordings using hidden markov models. Biomedical Engineering, 53(6):1078\u20131083, 2006.  Raina M Merchant, Lin Yang, Lance B Becker, Robert A Berg, Vinay Nadkarni, Graham Nichol, Brendan G Carr, Nandita Mitra, Steven M Bradley, Benjamin S Abella, et al. Incidence of treated cardiac arrest in hospitalized patients in the united states. Critical care medicine, 39(11):2401, 2011.  Philipp Metzner, Illia Horenko, and Christof Sch\u00a8utte. Generator estimation of markov jump processes based on incomplete observations nonequidistant in time. Physical Review E, 76(6):066702, 2007.  Rui P Moreno, Philipp GH Metnitz, Eduardo Almeida, Barbara Jordan, Peter Bauer, Ri- cardo Abizanda Campos, Gaetano Iapichino, David Edbrooke, Maurizia Capuzzo, Jean- Roger Le Gall, et al. Saps 3-from evaluation of the patient to evaluation of the intensive care unit. part 2: Development of a prognostic model for hospital mortality at icu admis- sion. Intensive care medicine, 31(10):1345\u20131355, 2005.  RJM Morgan, F Williams, and MM Wright. An early warning scoring system for detecting  developing critical illness. Clin Intensive Care, 8(2):100, 1997.  DR Mould. Models for disease progression: new approaches and uses. Clinical Pharmacology  & Therapeutics, 92(1):125\u2013131, 2012.  Kevin Murphy et al. The bayes net toolbox for matlab. Computing science and statistics,  33(2):1024\u20131034, 2001.  Kevin P Murphy. Hidden semi-markov models (hsmms). unpublished notes, 2, 2002.  Uri Nodelman, Christian R Shelton, and Daphne Koller. Expectation maximization and complex duration distributions for continuous time bayesian networks. arXiv preprint arXiv:1207.1402, 2012.  Zdzis(cid:32)law Opial. Weak convergence of the sequence of successive approximations for nonex- pansive mappings. Bulletin of the American Mathematical Society, 73(4):591\u2013597, 1967.  Mari Ostendorf, Vassilios V Digalakis, and Owen A Kimball. From hmm\u2019s to segment models: A uni\ufb01ed view of stochastic modeling for speech recognition. IEEE Transactions on speech and audio processing, 4(5):360\u2013378, 1996. Alaa and van der Schaar  Soren Erik Pedersen, Suzanne S Hurd, Robert F Lemanske, Allan Becker, Heather J Zar, Peter D Sly, Manuel Soto-Quiroz, Gary Wong, and Eric D Bateman. Global strategy for the diagnosis and management of asthma in children 5 years and younger. Pediatric pulmonology, 46(1):1\u201317, 2011.  Fabian Pedregosa, Ga\u00a8el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(Oct):2825\u20132830, 2011.  Romain Pirracchio, Maya L Petersen, Marco Carone, Matthieu Resche Rigon, Sylvie Chevret, and Mark J van der Laan. Mortality prediction in intensive care units with the super icu learner algorithm (sicula): a population-based study. The Lancet Respira- tory Medicine, 3(1):42\u201352, 2015.  Andrei D Polyanin and Alexander V Manzhirov. Handbook of integral equations. CRC  press, 2008.  Ross L Prentice, John D Kalb\ufb02eisch, Arthur V Peterson Jr, Nancy Flournoy, Vern T Farewell, and Norman E Breslow. The analysis of failure times in the presence of com- peting risks. Biometrics, pages 541\u2013554, 1978.  Zhen Qin and Christian R Shelton. Auxiliary gibbs sampling for inference in piecewise- constant conditional intensity models. In Proceedings of the Thirty-First Conference on Uncertainty in Arti\ufb01cial Intelligence, 2015.  Lawrence R Rabiner. A tutorial on hidden markov models and selected applications in  speech recognition. Proceedings of the IEEE, 77(2):257\u2013286, 1989.  Rajesh Ranganath, Adler Perotte, No\u00b4emie Elhadad, and David Blei. Deep survival analysis.  In Machine Learning for Healthcare Conference, pages 101\u2013114, 2016.  Carl Edward Rasmussen. Gaussian processes for machine learning. 2006.  Santiago Romero-Brufau, Jeanne M Huddleston, Gabriel J Escobar, and Mark Liebow. Why the c-statistic is not informative to evaluate early warning scores and what metrics to use. Critical Care, 19(1):285, 2015.  Michael J Rothman, Steven I Rothman, and Joseph Beals. Development and validation of a continuous measure of patient condition using the electronic medical record. Journal of biomedical informatics, 46(5):837\u2013848, 2013.  Mohammed Saeed, Christine Lieu, Greg Raber, and Roger G Mark. Mimic ii: a massive temporal icu patient database to support research in intelligent patient monitoring. In Computers in Cardiology, 2002, pages 641\u2013644. IEEE, 2002.  Daniel O Scharfstein and James M Robins. Estimation of the failure time distribution in  the presence of informative censoring. Biometrika, 89(3):617\u2013634, 2002. An HASMM Model for Informatively Censored Temporal Data  Peter Schulam and Suchi Saria. A framework for individualizing predictions of disease trajectories by exploiting multi-resolution structure. In Advances in Neural Information Processing Systems, pages 748\u2013756, 2015.  Padhraic Smyth. Hidden markov models for fault detection in dynamic systems. Pattern  recognition, 27(1):149\u2013164, 1994.  Henry T Stelfox, Brenda R Hemmelgarn, Sean M Bagshaw, Song Gao, Christopher J Doig, Cheri Nijssen-Jordan, and Braden Manns. Intensive care unit bed availability and out- comes for hospitalized patients with sudden clinical deterioration. Archives of internal medicine, 172(6):467\u2013474, 2012.  CP Subbe, M Kruger, P Rutherford, and L Gemmel. Validation of a modi\ufb01ed early warning  score in medical admissions. Qjm, 94(10):521\u2013526, 2001.  MJ Sweeting, VT Farewell, and D De Angelis. Multi-state markov models for disease progression in the presence of informative examination times: An application to hepatitis c. Statistics in medicine, 29(11):1161\u20131174, 2010.  S Taghipour, D Banjevic, AB Miller, N Montgomery, AKS Jardine, and BJ Harvey. Pa- rameter estimates for invasive breast cancer progression in the canadian national breast screening study. British journal of cancer, 108(3):542\u2013548, 2013.  Hale F Trotter and John W Tukey. Conditional monte carlo for normal samples. In Sym-  posium on Monte Carlo Methods, pages 64\u201379. Wiley, 1956.  John Varga, Christopher P Denton, and Fredrick M Wigley. Scleroderma: From pathogenesis  to comprehensive management. Springer Science & Business Media, 2012.  J-L Vincent, Rui Moreno, Jukka Takala, Sheila Willatts, Arnaldo De Mendon\u00b8ca, Hajo Bruining, CK Reinhart, PeterM Suter, and LG Thijs. The sofa (sepsis-related organ failure assessment) score to describe organ dysfunction/failure. Intensive care medicine, 22(7):707\u2013710, 1996.  Xiang Wang, David Sontag, and Fei Wang. Unsupervised learning of disease progression models. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 85\u201394. ACM, 2014.  Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings  of the IEEE, 78(10):1550\u20131560, 1990.  J Yoon, A Alaa, S Hu, and M van der Schaar. Forecasticu: A prognostic decision support system for timely prediction of intensive care unit admission. pages 1680\u20131689, 2016.  Lei Yu and Huan Liu. Feature selection for high-dimensional data: A fast correlation-based  \ufb01lter solution. In ICML, volume 3, pages 856\u2013863, 2003.  Shun Yu, Sharon Leung, Moonseong Heo, Graciela J Soto, Ronak T Shah, Sampath Gunda, and Michelle Ng Gong. Comparison of risk prediction scoring systems for ward patients: a retrospective nested case-control study. Critical Care, 18(3):1, 2014. Alaa and van der Schaar  Shun-Zheng Yu. Hidden semi-markov models. Arti\ufb01cial Intelligence, 174(2):215\u2013243, 2010.  Yongyue Zhang, Michael Brady, and Stephen Smith. Segmentation of brain mr images through a hidden markov random \ufb01eld model and the expectation-maximization algo- rithm. IEEE transactions on medical imaging, 20(1):45\u201357, 2001. "}, "Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-006.html", "header": "Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models", "author": "Noureddine El Karoui, Elizabeth Purdom", "time": "19(5):1\u221266, 2018.", "abstract": "We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where $p\\lt n$ but $p/n$ is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of $\\beta$ (where $\\beta$ is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression\u2013residual bootstrap and pairs bootstrap\u2013give very poor inference on $\\beta$ as the ratio $p/n$ grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio $p/n$ grows. We also show that the jackknife resampling technique for estimating the variance of $\\hat{\\beta}$ severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods.", "pdf_url": "http://jmlr.org/papers/volume19/17-006/17-006.pdf", "reference": "M. ApS. The MOSEK optimization toolbox for MATLAB manual. Version 7.1 (Revision  28)., 2015. URL http://docs.mosek.com/7.1/toolbox/index.html.  D. Bean, P. J. Bickel, N. El Karoui, and B. Yu. Optimal M-estimation in high-dimensional regression. Proceedings of the National Academy of Sciences, 110(36):14563\u201314568, 2013.  A. Belloni, V. Chernozhukov, and K. Kato. Uniform post-selection inference for least absolute deviation regression and other Z-estimation problems. Biometrika, 102(1):77\u201394, March 2015.  R. Beran and M. S. Srivastava. Bootstrap tests and con\ufb01dence regions for functions of a  covariance matrix. Ann. Statist., 13(1):95\u2013115, 1985.  P. J. Bickel and D. A. Freedman. Bootstrapping regression models with many parameters. In A Festschrift for Erich L. Lehmann, Wadsworth Statist./Probab. Ser., pages 28\u201348. Wadsworth, Belmont, Calif., 1983.  P. J. Bickel, F. G\u00a8otze, and W. R. van Zwet. Resampling fewer than n observations: gains, losses, and remedies for losses. Statist. Sinica, 7(1):1\u201331, 1997. Empirical Bayes, se- quential analysis and related topics in statistics and probability (New Brunswick, NJ, 1995).  P. J. Bickel and D. A. Freedman. Some asymptotic theory for the bootstrap. Ann. Statist.,  9(6):1196\u20131217, 1981.  O. Chapelle, E. Manavoglu, and R. Rosales. Simple and scalable response prediction for display advertising. ACM Trans. Intell. Syst. Technol., 5(4):61:1\u201361:34, December 2014.  A. Chatterjee and S. N. Lahiri. ASYMPTOTIC PROPERTIES OF THE RESIDUAL BOOTSTRAP FOR LASSO ESTIMATORS. Proceedings of the American Mathematical Society, 138(12):4497\u20134509, December 2010.  A. Chatterjee and S. N. Lahiri. Rates of convergence of the Adaptive LASSO estimators to the Oracle distribution and higher order re\ufb01nements by the bootstrap. The Annals of Statistics, 41(3):1232\u20131259, June 2013.  A. Chatterjee and S. Lahiri. Bootstrapping lasso estimators. Journal of the American  Statistical Association, 106(494):608\u2013625, 2011.  M. R. Chernick. Bootstrap Methods: A Practitioner\u2019s Guide. Wiley, 1999.  Criteo. Criteo publicly available datasets, 2017. URL http://research.criteo.com/  outreach/.  A. C. Davison and D. V. Hinkley. Bootstrap methods and their application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 1997.  62   El Karoui and Purdom  References  M. ApS. The MOSEK optimization toolbox for MATLAB manual. Version 7.1 (Revision  28)., 2015. URL http://docs.mosek.com/7.1/toolbox/index.html.  D. Bean, P. J. Bickel, N. El Karoui, and B. Yu. Optimal M-estimation in high-dimensional regression. Proceedings of the National Academy of Sciences, 110(36):14563\u201314568, 2013.  A. Belloni, V. Chernozhukov, and K. Kato. Uniform post-selection inference for least absolute deviation regression and other Z-estimation problems. Biometrika, 102(1):77\u201394, March 2015.  R. Beran and M. S. Srivastava. Bootstrap tests and con\ufb01dence regions for functions of a  covariance matrix. Ann. Statist., 13(1):95\u2013115, 1985.  P. J. Bickel and D. A. Freedman. Bootstrapping regression models with many parameters. In A Festschrift for Erich L. Lehmann, Wadsworth Statist./Probab. Ser., pages 28\u201348. Wadsworth, Belmont, Calif., 1983.  P. J. Bickel, F. G\u00a8otze, and W. R. van Zwet. Resampling fewer than n observations: gains, losses, and remedies for losses. Statist. Sinica, 7(1):1\u201331, 1997. Empirical Bayes, se- quential analysis and related topics in statistics and probability (New Brunswick, NJ, 1995).  P. J. Bickel and D. A. Freedman. Some asymptotic theory for the bootstrap. Ann. Statist.,  9(6):1196\u20131217, 1981.  O. Chapelle, E. Manavoglu, and R. Rosales. Simple and scalable response prediction for display advertising. ACM Trans. Intell. Syst. Technol., 5(4):61:1\u201361:34, December 2014.  A. Chatterjee and S. N. Lahiri. ASYMPTOTIC PROPERTIES OF THE RESIDUAL BOOTSTRAP FOR LASSO ESTIMATORS. Proceedings of the American Mathematical Society, 138(12):4497\u20134509, December 2010.  A. Chatterjee and S. N. Lahiri. Rates of convergence of the Adaptive LASSO estimators to the Oracle distribution and higher order re\ufb01nements by the bootstrap. The Annals of Statistics, 41(3):1232\u20131259, June 2013.  A. Chatterjee and S. Lahiri. Bootstrapping lasso estimators. Journal of the American  Statistical Association, 106(494):608\u2013625, 2011.  M. R. Chernick. Bootstrap Methods: A Practitioner\u2019s Guide. Wiley, 1999.  Criteo. Criteo publicly available datasets, 2017. URL http://research.criteo.com/  outreach/.  A. C. Davison and D. V. Hinkley. Bootstrap methods and their application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 1997. Can We Trust the Bootstrap in High-dimensions?  A. Delaigle and I. Gijbels. Estimation of integrated squared density derivatives from a contaminated sample. Journal of the Royal Statistical Society, B, 64:869\u2013886, 2002.  A. Delaigle and I. Gijbels. Practical bandwidth selection in deconvolution kernel density  estimation. Computational Statistics and Data Analysis, 45:249 \u2013 267, 2004.  A. Delaigle. Nonparametric kernel methods with errors-in-variables: constructing esti- mators, computing them, and avoiding common mistakes. Aust. N. Z. J. Stat., 56(2): 105\u2013124, 2014.  R. Dezeure, P. B\u00a8uhlmann, and C.-H. Zhang. High-dimensional simultaneous inference with  the bootstrap. TEST, 26(4):685\u2013719, October 2017.  P. Diaconis and D. Freedman. Asymptotics of graphical projection pursuit. Ann. Statist.,  12(3):793\u2013815, 1984.  D. Donoho and A. Montanari. High dimensional robust m-estimation: Asymptotic variance  via approximate message passing. arXiv:1310.7320, 2013.  M. L. Eaton and D. E. Tyler. On Wielandt\u2019s inequality and its application to the asymptotic distribution of the eigenvalues of a random symmetric matrix. Ann. Statist., 19(1):260\u2013 271, 1991.  B. Efron. Bootstrap methods: another look at the jackknife. Ann. Statist., 7(1):1\u201326, 1979.  B. Efron. The jackknife, the bootstrap and other resampling plans, volume 38 of CBMS-NSF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, Pa., 1982.  B. Efron and C. Stein. The jackknife estimate of variance. Ann. Statist., 9(3):586\u2013596,  1981.  B. Efron and R. J. Tibshirani. An introduction to the bootstrap, volume 57 of Monographs  on Statistics and Applied Probability. Chapman and Hall, New York, 1993.  N. El Karoui. Concentration of measure and spectra of random matrices: Applications to correlation matrices, elliptical distributions and beyond. The Annals of Applied Proba- bility, 19(6):2362\u20132405, December 2009.  N. El Karoui. High-dimensionality e\ufb00ects in the Markowitz problem and other quadratic programs with linear constraints: risk underestimation. Ann. Statist., 38(6):3487\u20133566, 2010.  N. El Karoui. On the realized risk of high-dimensional markowitz portfolios. SIAM Journal  in Financial Mathematics, 4(1), 2013.  N. El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results. arXiv:1311.2445, 2013. ArXiv:1311.2445. El Karoui and Purdom  N. El Karoui. On the impact of predictor geometry on the performance on high-dimensional ridge-regularized generalized robust regression estimators. Probability Theory and Related Fields, 2017.  N. El Karoui and H. Koesters. Geometric sensitivity of random matrix results: conse- quences for shrinkage estimators of covariance and related statistical methods. Submitted to Bernoulli, 2011. Available at arXiv:1105.1404 (68 pages).  N. El Karoui, D. Bean, P. Bickel, C. Lim, and B. Yu. On robust regression with high- dimensional predictors. Technical Report 811, UC, Berkeley, Department of Statistics, 2011. Originally submitted as manuscript AoS1111-009. Not under consideration any- more.  N. El Karoui, D. Bean, P. J. Bickel, C. Lim, and B. Yu. On robust regression with high-  dimensional predictors. Proceedings of the National Academy of Sciences, 2013.  J. Fan. On the optimal rates of convergence for nonparametric deconvolution problems.  Ann. Statist., 19(3):1257\u20131272, 1991.  M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In V. Blon- del, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95\u2013110. Springer-Verlag Limited, 2008. http://stanford.edu/~boyd/graph_dcp.html.  M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version  2.1. http://cvxr.com/cvx, March 2014.  L. R. Ha\ufb00. An identity for the Wishart distribution with applications. J. Multivariate  Anal., 9(4):531\u2013544, 1979.  Verlag, New York, 1992.  P. Hall. The bootstrap and Edgeworth expansion. Springer Series in Statistics. Springer-  P. Hall and S. Lahiri. Estimation of distributions, moments and quantiles in deconvolution  problems. The Annals of Statistics, 36(5):2110\u20132134, 2008.  P. Hall, J. S. Marron, and A. Neeman. Geometric representation of high dimension, low  sample size data. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(3):427\u2013444, 2005.  J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Fundamentals of convex analysis. Grundlehren Text Editions. Springer-Verlag, Berlin, 2001. Abridged version of \u0131t Convex analysis and minimization algorithms. I [Springer, Berlin, 1993; MR1261420 (95m:90001)] and \u0131t II [ibid.; MR1295240 (95m:90002)].  R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press, Cambridge,  1990. Corrected reprint of the 1985 original.  P. J. Huber. Robust regression: asymptotics, conjectures and Monte Carlo. Ann. Statist.,  1:799\u2013821, 1973. Can We Trust the Bootstrap in High-dimensions?  P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley Series in Probability and Statis-  tics. John Wiley & Sons Inc., Hoboken, NJ, second edition, 2009.  I. Johnstone. On the distribution of the largest eigenvalue in principal component analysis.  Ann. Statist., 29(2):295\u2013327, 2001.  T. Kato. Perturbation theory for linear operators. Classics in Mathematics. Springer-Verlag,  Berlin, 1995. Reprint of the 1980 edition.  A. Kleiner, A. Talwalkar, P. Sarkar, and M. I. Jordan. A scalable bootstrap for massive data. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(4): 795\u2013816, 2014.  R. Koenker. Quantile regression, volume 38 of Econometric Society Monographs. Cambridge  University Press, Cambridge, 2005.  R. Koenker. quantreg: Quantile Regression, 2013. URL http://CRAN.R-project.org/  package=quantreg. R package version 5.05.  J. Langford, L. Li, and A. Strehl, 2007. URL https://github.com/JohnLangford/vowpal_  wabbit/wiki.  M. Ledoux. The concentration of measure phenomenon, volume 89 of Mathematical Surveys  and Monographs. American Mathematical Society, Providence, RI, 2001.  M. Lopes. A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs. In Advances in Neural Information Processing Systems NIPS, pages 3239\u20133247, 2014.  E. Mammen. Asymptotics with increasing dimension for robust regression with applications  to the bootstrap. Ann. Statist., 17(1):382\u2013400, 1989.  E. Mammen. Bootstrap, wild bootstrap, and asymptotic normality. Probab. Theory Related  Fields, 93(4):439\u2013455, 1992.  Statist., 21(1):255\u2013285, 1993.  E. Mammen. Bootstrap and wild bootstrap for high-dimensional linear models. Ann.  K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate analysis. Academic Press [Harcourt Brace Jovanovich Publishers], London, 1979. Probability and Mathematical Statistics: A Series of Monographs and Textbooks.  J. W. McKean, S. J. Sheather, and T. P. Hettmansperger. The Use and Interpretation of Residuals Based on Robust Estimation. Journal of the American Statistical Association, 88(424):1254\u20131263, December 1993.  P. D. Miller. Applied asymptotic analysis, volume 75 of Graduate Studies in Mathematics.  American Mathematical Society, Providence, RI, 2006.  J.-J. Moreau. Proximit\u00b4e et dualit\u00b4e dans un espace hilbertien. Bull. Soc. Math. France, 93:  273\u2013299, 1965. El Karoui and Purdom  MOSEK. Rmosek: The R to MOSEK Optimization Interface, 2014. URL http://rmosek.  r-forge.r-project.org/,http://www.mosek.com/. R package version 7.0.5.  A. Pajor and L. Pastur. On the limiting empirical measure of eigenvalues of the sum of  rank one matrices with log-concave distribution. Studia Math., 195(1):11\u201329, 2009.  M. I. Parzen, L. J. Wei, and Z. Ying. A resampling method based on pivotal estimating  functions. Biometrika, 81(2):341\u2013350, 1994.  D. N. Politis, J. P. Romano, and M. Wolf. Subsampling. Springer Series in Statistics.  Springer-Verlag, New York, 1999.  S. Portnoy. Asymptotic behavior of M -estimators of p regression parameters when p2/n is  large. I. Consistency. Ann. Statist., 12(4):1298\u20131309, 1984.  S. Portnoy. Asymptotic behavior of M estimators of p regression parameters when p2/n is  large. II. Normal approximation. Ann. Statist., 13(4):1403\u20131417, 1985.  S. Portnoy. Asymptotic behavior of the empiric distribution of M -estimated residuals from  a regression model with many parameters. Ann. Statist., 14(3):1152\u20131170, 1986.  S. Portnoy. A central limit theorem applicable to robust regression estimators. J. Multi-  variate Anal., 22(1):24\u201350, 1987.  G. R. Shorack. Bootstrapping robust regression. Comm. Statist. A\u2014Theory Methods, 11  (9):961\u2013972, 1982.  J. W. Silverstein. Strong convergence of the empirical distribution of eigenvalues of large-  dimensional random matrices. J. Multivariate Anal., 55(2):331\u2013339, 1995.  D. W. Stroock. Probability theory, an analytic view. Cambridge University Press, Cam-  bridge, 1993.  A. W. van der Vaart. Asymptotic statistics. Cambridge Series in Statistical and Probabilistic  Mathematics. Cambridge University Press, Cambridge, 1998.  W. N. Venables and B. D. Ripley. Modern Applied Statistics with S. Springer, New York,  fourth edition, 2002. ISBN 0-387-95457-0.  K. W. Wachter. The strong limits of random matrix spectra for sample matrices of inde-  pendent elements. Annals of Probability, 6(1):1\u201318, 1978.  X. Wang and B. Wang. Deconvolution estimation in measurement error models: The r  package decon. Journal of Statistical Software, 39(10):1\u201324, 2011.  S. Weisberg. Applied linear regression. Wiley Series in Probability and Statistics. John  Wiley & Sons, Inc., Hoboken, NJ, fourth edition, 2014.  C.-F. J. Wu. Jackknife, bootstrap and other resampling methods in regression analysis. Ann. Statist., 14(4):1261\u20131350, 1986. With discussion and a rejoinder by the author.  S. Zheng, D. Jiang, Z. Bai, and X. He. Inference on multiple correlation coe\ufb03cients with  moderately high dimensional data. Biometrika, 101:748\u2013754, 2014. "}, "RSG: Beating Subgradient Method without Smoothness and Strong Convexity": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-016.html", "header": "RSG: Beating Subgradient Method without Smoothness and Strong Convexity", "author": "Tianbao Yang, Qihang Lin", "time": "19(6):1\u221233, 2018.", "abstract": "In this paper, we study the efficiency of a  R estarted  S ub G radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\\epsilon$-optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\\epsilon$-level set and the optimal set {multiplied by a logarithmic factor}. Moreover, we show the advantages of RSG over SG in solving a broad family of problems that satisfy a local error bound condition, and also demonstrate its advantages for three specific families of convex optimization problems with different power constants in the local error bound condition. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the $\\epsilon$-sublevel set, RSG has an $O(\\frac{1}{\\epsilon}\\log(\\frac{1}{\\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-{\\L}ojasiewicz property with a power constant of $\\beta\\in[0,1)$, RSG has an $O(\\frac{1}{\\epsilon^{2\\beta}}\\log(\\frac{1}{\\epsilon}))$ iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\\L ojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion.", "pdf_url": "http://jmlr.org/papers/volume19/17-016/17-016.pdf", "reference": "Francisco J Aragon Artacho and Michel H Geo\ufb00roy. Characterization of metric regularity  of subdi\ufb00erentials. Journal of Convex Analysis, 15:365\u2013380, 2008.  H\u00b4edy Attouch, J\u00b4er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-lojasiewicz inequality. Math. Oper. Res., 35:438\u2013457, 2010.  Hedy Attouch, J\u00b4er\u02c6ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods. Math. Program., 137(1-2):91\u2013129, 2013.  Francis R. Bach. Learning with submodular functions: A convex optimization perspective.  Foundations and Trends in Machine Learning, 6(2-3):145\u2013373, 2013.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation In Advances in Neural Information Processing Systems  with convergence rate o(1/n). (NIPS), pages 773\u2013781, 2013.  Dimitris Bertsimas and Martin S. Copenhaver. Characterization of the equivalence of ro- busti\ufb01cation and regularization in linear, median, and matrix regression. arXiv, 2014.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, and Adrian Lewis. The (cid:32)lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM J. on Optimization, 17:1205\u20131223, 2006.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, Adrian S. Lewis, and Masahiro Shiota. Clarke subgradients  of strati\ufb01able functions. SIAM Journal on Optimization, 18:556\u2013572, 2007.  J\u00b4er\u02c6ome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized min- imization for nonconvex and nonsmooth problems. Mathematical Programming, 146: 459\u2013494, 2014.  J\u00b4er\u02c6ome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W. Suter. From error bounds to the complexity of \ufb01rst-order descent methods for convex functions. Mathemat- ical Programming, 165(2):471\u2013507, Oct 2017. doi: 10.1007/s10107-016-1091-6.  James V. Burke and Sien Deng. Weak sharp minima revisited part i: Basic theory. Control  and Cybernetics, 31:439?469, 2002.  James V. Burke and Sien Deng. Weak sharp minima revisited, part II: application to linear  regularity and error bounds. Math. Program., 104(2-3):235\u2013261, 2005.  28   Yang and Lin  Proposition 1 (Bolte et al., 2017, Theorem 5) Let f (x) be an extended-valued, proper, convex and lower semicontinuous function that satis\ufb01es the KL inequality (20) at x ) < f (x) < f (x ) for all x arg min f ( U \u2208 \u00b7 )) then dist(x, arg min f ( \u00b7  , where U is a neighborhood of x ) + \u03b7  x : f (x \u03d5(f (x)  ) < f (x) < f (x  \u2217 )) for all x  \u2217 \u2208 , \u2217 . }  )+\u03b7 } U  x : f (x  \u2229{ \u2264  \u2217 \u2212  f (x  \u2229 {  \u2208  \u2217  \u2217  \u2217  References  Francisco J Aragon Artacho and Michel H Geo\ufb00roy. Characterization of metric regularity  of subdi\ufb00erentials. Journal of Convex Analysis, 15:365\u2013380, 2008.  H\u00b4edy Attouch, J\u00b4er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-lojasiewicz inequality. Math. Oper. Res., 35:438\u2013457, 2010.  Hedy Attouch, J\u00b4er\u02c6ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods. Math. Program., 137(1-2):91\u2013129, 2013.  Francis R. Bach. Learning with submodular functions: A convex optimization perspective.  Foundations and Trends in Machine Learning, 6(2-3):145\u2013373, 2013.  Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation In Advances in Neural Information Processing Systems  with convergence rate o(1/n). (NIPS), pages 773\u2013781, 2013.  Dimitris Bertsimas and Martin S. Copenhaver. Characterization of the equivalence of ro- busti\ufb01cation and regularization in linear, median, and matrix regression. arXiv, 2014.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, and Adrian Lewis. The (cid:32)lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM J. on Optimization, 17:1205\u20131223, 2006.  J\u00b4er\u02c6ome Bolte, Aris Daniilidis, Adrian S. Lewis, and Masahiro Shiota. Clarke subgradients  of strati\ufb01able functions. SIAM Journal on Optimization, 18:556\u2013572, 2007.  J\u00b4er\u02c6ome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized min- imization for nonconvex and nonsmooth problems. Mathematical Programming, 146: 459\u2013494, 2014.  J\u00b4er\u02c6ome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W. Suter. From error bounds to the complexity of \ufb01rst-order descent methods for convex functions. Mathemat- ical Programming, 165(2):471\u2013507, Oct 2017. doi: 10.1007/s10107-016-1091-6.  James V. Burke and Sien Deng. Weak sharp minima revisited part i: Basic theory. Control  and Cybernetics, 31:439?469, 2002.  James V. Burke and Sien Deng. Weak sharp minima revisited, part II: application to linear  regularity and error bounds. Math. Program., 104(2-3):235\u2013261, 2005. RSG: Beating Subgradient Method without Smoothness and Strong Convexity  James V. Burke and Sien Deng. Weak sharp minima revisited, part III: error bounds for  di\ufb00erentiable convex inclusions. Math. Program., 116(1-2):37\u201356, 2009.  James V. Burke and Michael C. Ferris. Weak sharp minima in mathematical pro- gramming. SIAM Journal on Control and Optimization, 31(5):1340\u20131359, 1993. doi: 10.1137/0331063.  Xi Chen, Qihang Lin, and Javier Pena. Optimal regularized dual averaging methods for stochastic optimization. In Advances in Neural Information Processing Systems (NIPS), pages 395\u2013403. 2012.  Dmitriy Drusvyatskiy and Courtney Kempton. An accelerated algorithm for minimizing  convex compositions. arXiv:1605.00125, 2016.  Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear  convergence of proximal methods. Mathematics of Operations Research, 2018.  Dmitriy Drusvyatskiy, Boris Mordukhovich, and Nghia T.A. Tran. Second-order growth, tilt stability, and metric regularity of the subdi\ufb00erential. Journal of Convex Analysis, 21: 1165\u20131192, 2014.  I. I. Eremin. The relaxation method of solving systems of inequalities with convex functions  on the left-hand side. Dokl. Akad. Nauk SSSR, 160:994 \u2013 996, 1965.  Michael C. Ferris. Finite termination of the proximal point algorithm. Mathematical Pro-  gramming, 50(1):359\u2013366, Mar 1991.  Robert M. Freund and Haihao Lu. New computational guarantees for solving convex op- timization problems with \ufb01rst order methods, via a function growth condition measure. Mathematical Programming, 2017.  Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estima-  tion with the graphical lasso. Biostatistics, 9, 2008.  Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, II: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 23(4):2061\u20132089, 2013.  Andrew Gilpin, Javier Pe\u02dcna, and Tuomas Sandholm.  First-order algorithm with log(1/epsilon) convergence for epsilon-equilibrium in two-person zero-sum games. Math. Program., 133(1-2):279\u2013298, 2012.  R. Goebel and R. T. Rockafellar. Local strong convexity and local lipschitz continuity of  the gradient of convex functions. Journal of Convex Analysis, 2007.  Pinghua Gong and Jieping Ye. Linear convergence of variance-reduced projected stochastic  gradient without strong convexity. CoRR, abs/1406.1102, 2014.  Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory (COLT), pages 421\u2013436, 2011. Yang and Lin  Ke Hou, Zirui Zhou, Anthony Man-Cho So, and Zhi-Quan Luo. On the linear convergence of the proximal gradient method for trace norm regularization. In Advances in Neural Information Processing Systems (NIPS), pages 710\u2013718, 2013.  Anatoli Juditsky and Yuri Nesterov. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems, 4(1):44\u201380, 2014.  Hamed Karimi, Julie Nutini, and Mark W. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-(cid:32)lojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases - European Conference (ECML-PKDD), pages 795\u2013811, 2016.  Seyoung Kim, Kyung-Ah Sohn, and Eric P. Xing. A multivariate regression approach to  association analysis of a quantitative trait network. Bioinformatics, 25(12), 2009.  Alexander Y. Kruger. Error bounds and h\u00a8older metric subregularity. Set-Valued and Vari-  ational Analysis, 23:705\u2013736, 2015.  Simon Lacoste-Julien, Mark W. Schmidt, and Francis R. Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method. CoRR, abs/1212.2002, 2012. URL http://arxiv.org/abs/1212.2002.  Guoyin Li. On the asymptotically well behaved functions and global error bound for convex  polynomials. SIAM Journal on Optimization, 20(4):1923\u20131943, 2010.  Guoyin Li. Global error bounds for piecewise convex polynomials. Math. Program., 137  (1-2):37\u201364, 2013.  Guoyin Li and Boris S. Mordukhovich. H\u00a8older metric subregularity with applications to  proximal point method. SIAM Journal on Optimization, 22(4):1655\u20131684, 2012.  Zhi-Quan Luo and Paul Tseng. On the convergence of coordinate descent method for convex di\ufb00erentiable minization. Journal of Optimization Theory and Applications, 72(1):7\u201335, 1992a.  Zhi-Quan Luo and Paul Tseng. On the linear convergence of descent methods for convex essenially smooth minization. SIAM Journal on Control and Optimization, 30(2):408\u2013425, 1992b.  Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent  methods: a general approach. Annals of Operations Research, 46:157\u2013178, 1993.  Boris Mordukhovich and Wei Ouyang. Higher-order metric subregularity and its applica- tions. Journal of Global Optimization- An International Journal Dealing with Theoretical and Computational Aspects of Seeking Global Optima and Their Applications in Science, Management and Engineering, 63(4):777\u2013795, 2015.  Ion Necoara and Dragos Clipici. Parallel random coordinate descent method for composite minimization: Convergence analysis and error bounds. SIAM Journal on Optimization, 26(1):197\u2013226, 2016. doi: 10.1137/130950288. RSG: Beating Subgradient Method without Smoothness and Strong Convexity  Ion Necoara, Yurri Nesterov, and Francois Glineur. Linear convergence of \ufb01rst order meth-  ods for non-strongly convex optimization. CoRR, abs/1504.06298, 2015.  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Op- timization, 19:1574\u20131609, 2009.  Arkadii Semenovich. Nemirovsky A.S. and D. B Yudin. Problem complexity and method ef- \ufb01ciency in optimization. Wiley-Interscience series in discrete mathematics. Wiley, Chich- ester, New York, 1983. ISBN 0-471-10345-4. A Wiley-Interscience publication.  Yurii Nesterov.  Introductory lectures on convex optimization : a basic course. Applied  optimization. Kluwer Academic Publ., 2004. ISBN 1-4020-7553-7.  Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,  103(1):127\u2013152, 2005.  Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Pro-  gramming, 120:221\u2013259, 2009.  Hua Ouyang, Niao He, Long Tran, and Alexander G. Gray. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 80\u201388, 2013.  Jong-Shi Pang. A posteriori error bounds for the linearly-constrained variational inequality problem. Mathematics of Operations Research, 12(3):474\u2013484, August 1987. ISSN 0364- 765X.  Jong-Shi Pang. Error bounds in mathematical programming. Mathematical Programming,  79(1):299\u2013332, October 1997.  B. T. Polyak. Sharp minima. In Proceedings of the IIASA Workshop on Generalized La- grangians and Their Applications, Institute of Control Sciences Lecture Notes, Moscow. 1979.  B. T. Polyak. Introduction to Optimization. Optimization Software Inc, New York, 1987.  B.T. Polyak. Minimization of unsmooth functionals. USSR Computational Mathematics  and Mathematical Physics, 9:509 \u2013 521, 1969.  James Renegar. E\ufb03cient \ufb01rst-order methods for linear programming and semide\ufb01nite pro-  gramming. ArXiv e-prints, 2014.  James Renegar. A framework for applying subgradient methods to conic optimization  problems. ArXiv e-prints, 2015.  James Renegar. \u201ce\ufb03cient\u201d subgradient methods for general convex optimization. SIAM  Journal on Optimization, 26(4):2649\u20132676, 2016. Yang and Lin  Jasson D. M. Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22Nd International Conference on Machine Learning, pages 713\u2013719, New York, NY, USA, 2005. ACM. ISBN 1-59593-180-5. doi: 10.1145/1102351.1102441. URL http://doi.acm.org/10.1145/1102351.1102441.  R.T. Rockafellar. Convex Analysis. Princeton mathematical series. Princeton University  Press, 1970.  Reinhold Schneider and Andr\u00b4e Uschmajew. Convergence results for projected line-search methods on varieties of low-rank matrices via (cid:32)lojasiewicz inequality. SIAM Journal on Optimization, 25(1):622\u2013646, 2015.  Anthony Man-Cho So and Zirui Zhou. Non-asymptotic convergence analysis of inexact gradient methods for machine learning without strong convexity. Optimization Methods and Software, 32:963 \u2013 992, 2017.  Marcin Studniarski and Doug E. Ward. Weak sharp minima: Characterizations and su\ufb03- cient conditions. SIAM Journal on Control and Optimization, 38(1):219\u2013236, 1999. doi: 10.1137/S0363012996301269.  P. Tseng and S. Yun. A block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. Journal of Optimization Theory Application, 140: 513\u2013535, 2009a.  P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable  minimization. Mathematical Programming, 117:387\u2013423, 2009b.  Po-Wei Wang and Chih-Jen Lin. Iteration complexity of feasible descent methods for convex  optimization. Journal of Machine Learning Research, 15(1):1523\u20131548, 2014.  Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. IEEE  Trans. Information Theory, 56(7):3561\u20133574, 2010.  Yi Xu, Yan Yan, Qihang Lin, and Tianbao Yang. Homotopy smoothing for non-smooth problems with lower complexity than 1/epsilon. In Advances in Neural Information Pro- cessing Systems, 2016.  Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies faster global convergence. In Proceedings of the 34th International Conference on Machine Learning, (ICML), pages 3821\u20133830, 2017.  Tianbao Yang and Qihang Lin. Rsg: Beating sgd without smoothness and/or strong con-  vexity. CoRR, abs/1512.03107, 2016.  Tianbao Yang, Mehrdad Mahdavi, Rong Jin, and Shenghuo Zhu. An e\ufb03cient primal-dual  prox method for non-smooth optimization. Machine Learning, 2014.  W. H. Yang. Error bounds for convex polynomials. SIAM Journal on Optimization, 19(4):  1633\u20131647, 2009. RSG: Beating Subgradient Method without Smoothness and Strong Convexity  Yuning Yang, Yunlong Feng, and Johan A. K. Suykens. Robust low-rank tensor recovery with regularized redescending m-estimator. IEEE Trans. Neural Netw. Learning Syst., 27(9):1933\u20131946, 2016.  Hui Zhang. Characterization of linear convergence of gradient descent. arXiv:1606.00269,  2016.  Hui Zhang and Lizhi Cheng. Restricted strong convexity and its applications to convergence analysis of gradient-type methods in convex optimization. Optimization Letter, 9:961\u2013979, 2015.  Zirui Zhou and Anthony Man-Cho So. A uni\ufb01ed approach to error bounds for structured convex optimization problems. Mathematical Programming, 165(2):689\u2013728, Oct 2017.  Zirui Zhou, Qi Zhang, and Anthony Man-Cho So. L1p-norm regularization: Error bounds and convergence rate analysis of \ufb01rst-order methods. In Proceedings of the 32nd Inter- national Conference on Machine Learning, (ICML), pages 1501\u20131510, 2015.  Martin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In Proceedings of the International Conference on Machine Learning (ICML), pages 928\u2013 936, 2003. "}, "Scalable Bayes via Barycenter in Wasserstein Space": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-084.html", "header": "Scalable Bayes via Barycenter in Wasserstein Space", "author": "Sanvesh Srivastava, Cheng Li, David B. Dunson", "time": "19(8):1\u221235, 2018.", "abstract": "Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database.", "pdf_url": "http://jmlr.org/papers/volume19/17-084/17-084.pdf", "reference": "Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal  on Mathematical Analysis, 43(2):904\u2013924, 2011.  Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. Proceedings of the 29th International Conference on Machine Learning, 2012.  Pierre Alquier, Nial Friel, Richard Everitt, and Aidan Boland. Noisy Monte Carlo: Conver- gence of Markov chains with approximate transition kernels. Statistics and Computing, 26(1-2):29\u201347, 2016.  Pedro C \u00b4Alvarez-Esteban, E del Barrio, JA Cuesta-Albertos, and C Matr\u00b4an. A \ufb01xed-point approach to barycenters in Wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):744\u2013762, 2016.  Ethan Anderes, Ste\ufb00en Borgwardt, and Jacob Miller. Discrete Wasserstein barycenters: optimal transport for discrete data. Mathematical Methods of Operations Research, 84 (2):389409, 2016. ISSN 1432-5217. doi: 10.1007/s00186-016-0549-x. URL http://dx. doi.org/10.1007/s00186-016-0549-x.  R\u00b4emi Bardenet, Arnaud Doucet, and Chris Holmes. On Markov chain Monte Carlo methods  for tall data. Journal of Machine Learning Research, 18(47):1\u201343, 2017.  Peter J Bickel and David A Freedman. Some asymptotic theory for the bootstrap. The  Annals of Statistics, 9(6):1196\u20131217, 1981.  C. M. Bishop. Pattern recognition and machine learning, volume 4. Springer New York,  2006.  Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael Jordan. In Advances in Neural Information Processing Systems,  Streaming variational Bayes. pages 1727\u20131735, 2013.  Guillaume Carlier, Adam Oberman, and Edouard Oudet. Numerical methods for matching for teams and Wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):1621\u20131642, 2015.  Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte  Carlo. In International Conference on Machine Learning, pages 1683\u20131691, 2014.  Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Pro- ceedings of the 31st International Conference on Machine Learning, JMLR W&CP, pages 685\u2013693, 2014.  David B Dunson and Chuanhua Xing. Nonparametric Bayes modeling of multivariate categorical data. Journal of the American Statistical Association, 104(487):1042\u20131051, 2009.  32   Srivastava, Li, and Dunson  References  Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal  on Mathematical Analysis, 43(2):904\u2013924, 2011.  Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. Proceedings of the 29th International Conference on Machine Learning, 2012.  Pierre Alquier, Nial Friel, Richard Everitt, and Aidan Boland. Noisy Monte Carlo: Conver- gence of Markov chains with approximate transition kernels. Statistics and Computing, 26(1-2):29\u201347, 2016.  Pedro C \u00b4Alvarez-Esteban, E del Barrio, JA Cuesta-Albertos, and C Matr\u00b4an. A \ufb01xed-point approach to barycenters in Wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):744\u2013762, 2016.  Ethan Anderes, Ste\ufb00en Borgwardt, and Jacob Miller. Discrete Wasserstein barycenters: optimal transport for discrete data. Mathematical Methods of Operations Research, 84 (2):389409, 2016. ISSN 1432-5217. doi: 10.1007/s00186-016-0549-x. URL http://dx. doi.org/10.1007/s00186-016-0549-x.  R\u00b4emi Bardenet, Arnaud Doucet, and Chris Holmes. On Markov chain Monte Carlo methods  for tall data. Journal of Machine Learning Research, 18(47):1\u201343, 2017.  Peter J Bickel and David A Freedman. Some asymptotic theory for the bootstrap. The  Annals of Statistics, 9(6):1196\u20131217, 1981.  C. M. Bishop. Pattern recognition and machine learning, volume 4. Springer New York,  2006.  Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael Jordan. In Advances in Neural Information Processing Systems,  Streaming variational Bayes. pages 1727\u20131735, 2013.  Guillaume Carlier, Adam Oberman, and Edouard Oudet. Numerical methods for matching for teams and Wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):1621\u20131642, 2015.  Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte  Carlo. In International Conference on Machine Learning, pages 1683\u20131691, 2014.  Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Pro- ceedings of the 31st International Conference on Machine Learning, JMLR W&CP, pages 685\u2013693, 2014.  David B Dunson and Chuanhua Xing. Nonparametric Bayes modeling of multivariate categorical data. Journal of the American Statistical Association, 104(487):1042\u20131051, 2009. Scalable Bayes via Barycenter in Wasserstein Space  Christel Faes, John T Ormerod, and Matt P Wand. Variational Bayesian inference for parametric and nonparametric regression with missing data. Journal of the American Statistical Association, 106(495):959\u2013971, 2012.  Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density estimation. Journal of the American statistical Association, 97(458):611\u2013631, 2002.  Andrew Gelman, Aki Vehtari, Pasi Jyl\u00a8anki, Christian Robert, Nicolas Chopin, and John P Cunningham. Expectation propagation as a way of life. arXiv preprint arXiv:1412.4869, 2014.  Subhashis Ghosal and Aad van der Vaart. Convergence rates of posterior distributions for  noniid observations. The Annals of Statistics, 35(1):192\u2013223, 2007.  Ryan Giordano, Tamara Broderick, and Michael I Jordan. Covariances, robustness, and  variational bayes. arXiv preprint arXiv:1709.02536, 2017.  Gurobi Optimization Inc. Gurobi Optimizer Reference Manual Version 6.0.0, 2014.  Matthew D. Ho\ufb00man, David M. Blei, Chong Wang, and John Paisley. Stochastic variational  inference. Journal of Machine Learning Research, 14:1303\u20131347, 2013.  Ildar Abdulovi\u02c7c Ibragimov and Rafail Zalmanovich Has\u2019 Minskii. Statistical Estimation:  Asymptotic Theory, volume 16. Springer Science & Business Media, 2013.  James E. Johndrow, Jonathan C. Mattingly, Sayan Mukherjee, and David B. Dunson. Ap- proximations of Markov chains and High-Dimensional Bayesian Inference. arXiv preprint arXiv:1508.03387v1, 2015.  Yoonsang Kim, Young-Ku Choi, and Sherry Emery. Logistic regression with multiple ran- dom e\ufb00ects: a simulation study of estimation methods and statistical packages. The American Statistician, 67(3):171\u2013182, 2013.  Anoop Korattikara, Yutian Chen, and Max Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 31st International Conference on Machine Learning, page 181189, 2014.  Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David Blei. Automatic variational inference in Stan. In Advances in Neural Information Processing Systems, pages 568\u2013576, 2015.  Shiwei Lan, Bo Zhou, and Babak Shahbaba. Spherical Hamiltonian Monte Carlo for con- strained target distributions. In JMLR workshop and conference proceedings, volume 32, page 629. NIH Public Access, 2014.  Cathy Yuen Yi Lee and Matt P. Wand. Streamlined mean \ufb01eld variational Bayes for longi- tudinal and multilevel data analysis. Biometrical Journal, 58(4):868\u2013895, 2016. ISSN 1521-4036. doi: 10.1002/bimj.201500007. URL http://dx.doi.org/10.1002/bimj. 201500007. Srivastava, Li, and Dunson  Cheng Li, Sanvesh Srivastava, and David B Dunson. Simple, scalable and accurate posterior  interval estimation. Biometrika, 104:665\u2013680, 2017.  Dougal Maclaurin and Ryan Prescott Adams. Fire\ufb02y Monte Carlo: Exact MCMC with Sub- sets of Data. In Twenty-Fourth International Joint Conference on Arti\ufb01cial Intelligence, 2015.  Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, and David Dunson. Scalable and robust In Proceedings of the 31st International  Bayesian inference via the median posterior. Conference on Machine Learning (ICML-14), pages 1656\u20131664, 2014.  Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, and David B Dunson. Robust and scal- able Bayes via a median of subset posterior measures. The Journal of Machine Learning Research, 18(1):4488\u20134527, 2017.  Alexey Miroshnikov and Erin Conlon. parallelMCMCcombine: Methods for combining in- dependent subset Markov chain Monte Carlo posterior samples to estimate a posterior density given the full data set, 2014. URL https://CRAN.R-project.org/package= parallelMCMCcombine. R package version 1.0.  Willie Neiswanger, Chong Wang, and Eric Xing. Asymptotically exact, embarrassingly parallel MCMC. In Proceedings of the 30th International Conference on Uncertainty in Arti\ufb01cial Intelligence, pages 623\u2013632, 2014.  Patrick O Perry. Fast moment-based estimation for hierarchical models. Journal of the  Royal Statistical Society: Series B (Statistical Methodology), 79(1):267\u2013291, 2017.  Maxim Rabinovich, Elaine Angelino, and Michael I Jordan. Variational consensus Monte Carlo. In Advances in Neural Information Processing Systems, pages 1207\u20131215, 2015.  Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models.  In  International Conference on Machine Learning, pages 324\u2013333, 2016.  Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine  learning. MIT Press, 2006.  Danilo Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows.  In Proceedings of The 32nd International Conference on Machine Learning, pages 1530\u2013 1538, 2015.  H\u02daavard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(2):319\u2013392, 2009.  Steven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. Bayes and big data: the consensus Monte Carlo algorithm. International Journal of Management Science and Engineering Management, 11(2):78\u201388, 2016. Scalable Bayes via Barycenter in Wasserstein Space  Babak Shahbaba, Shiwei Lan, Wesley O Johnson, and Radford M Neal. Split Hamiltonian  Monte Carlo. Statistics and Computing, 24(3):339\u2013349, 2014.  Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. WASP: Scalable Bayes via barycenters of subset posteriors. In Proceedings of the 18th International Conference on Arti\ufb01cial Intelligence and Statistics, pages 912\u2013920, 2015.  Stan Development Team. Stan: A C++ library for probability and sampling, version 2.5.0,  2014. URL http://mc-stan.org/.  Linda SL Tan and David J Nott. Variational inference for generalized linear mixed models using partially noncentered parametrizations. Statistical Science, 28(2):168\u2013188, 2013.  Aad W van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.  Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Variational Inference. Found. Trends Mach. Learn., 1:1\u2013305, January 2008. doi: 10.1561/ 2200000001. URL http://portal.acm.org/citation.cfm?id=1498840.1498841.  Matt Wand. KernSmooth: Functions for Kernel Smoothing Supporting Wand & Jones (1995), 2015. URL http://CRAN.R-project.org/package=KernSmooth. R package ver- sion 2.23-14.  Xiangyu Wang and David B Dunson. Parallel MCMC via Weierstrass sampler. arXiv  preprint arXiv:1312.4605, 2013.  Xiangyu Wang, Fangjian Guo, Katherine A Heller, and David B Dunson. Parallelizing In Advances in Neural Information Processing  MCMC with random partition trees. Systems, pages 451\u2013459, 2015.  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pages 681\u2013688, 2011.  Wing Hung Wong and Xiaotong Shen. Probability inequalities for likelihood ratios and  convergence rates of sieve MLEs. The Annals of Statistics, 23(2):339\u2013362, 1995. "}, "Experience Selection in Deep Reinforcement Learning for Control": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-131.html", "header": "Experience Selection in Deep Reinforcement Learning for Control", "author": "Tim de Bruin, Jens Kober, Karl Tuyls, Robert Babu\u00c5\u00a1ka", "time": "19(9):1\u221256, 2018.", "abstract": "Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience  retention  and experience  sampling . We refer to the combination as experience  selection . We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.", "pdf_url": "http://jmlr.org/papers/volume19/17-131/17-131.pdf", "reference": "Eduard Alibekov, Jiri Kubal\u00b4\u0131k, and Robert Babu\u02c7ska. Policy derivation methods for critic- only reinforcement learning in continuous action spaces. Engineering Applications of Arti\ufb01cial Intelligence, 69:178\u2013187, 2018.  David Andre, Nir Friedman, and Ronald Parr. Generalized prioritized sweeping. In Ad- vances In Neural Information Processing Systems (NIPS), pages 1001\u20131007. MIT Press, 1997.  John Aslanides, Jan Leike, and Marcus Hutter. Universal reinforcement learning algorithms: In International Joint Conference on Arti\ufb01cial Intelligence  Survey and experiments. (IJCAI), pages 1403\u20131410, 2017.  Jimmy Lei Ba, Jamie Ryan Kiros, and Geo\ufb00rey E Hinton. Layer normalization. arXiv  preprint arXiv:1607.06450, 2016.  Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In World Congress on Computational Intelligence (WCCI), volume 4, pages 2448\u20132453, 1994.  Bikramjit Banerjee and Jing Peng. Performance bounded reinforcement learning in strategic interactions. In AAAI National Conference on Arti\ufb01cial Intelligence (AAAI), volume 4, pages 2\u20137, 2004.  Samuel Barrett, Matt Taylor, and Peter Stone. Transfer learning for reinforcement learning on a physical robot. Adaptive Learning Agents Workshop, International Conference on Autonomous Agents and Multiagent Systems (AAMAS - ALA), 2010.  Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems (NIPS), pages 1471\u20131479, 2016.  Yoshua Bengio, J\u00b4er\u02c6ome Louradour, Ronan Collobert, and Jason Weston. Curriculum learn-  ing. In International Conference on Machine Learning (ICML), pages 41\u201348, 2009.  Lucian Bu\u00b8soniu, Damien Ernst, Robert Babu\u02c7ska, and Bart De Schutter. Approximate dynamic programming with a fuzzy parameterization. Automatica, 46(5):804\u2013814, 2010.  Wouter Caarls and Erik Schuitema. Parallel online temporal di\ufb00erence learning for motor control. IEEE Transactions on Neural Networks and Learning Systems, 27(7):1457\u20131468, 2016.  Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh.  Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 1281\u20131288, 2004.  Kamil Ciosek and Shimon Whiteson. OFFER: o\ufb00-environment reinforcement learning. In  AAAI Conference on Arti\ufb01cial Intelligence (AAAI), 2017.  52   de Bruin, Kober, Tuyls and Babu\u02c7ska  References  Eduard Alibekov, Jiri Kubal\u00b4\u0131k, and Robert Babu\u02c7ska. Policy derivation methods for critic- only reinforcement learning in continuous action spaces. Engineering Applications of Arti\ufb01cial Intelligence, 69:178\u2013187, 2018.  David Andre, Nir Friedman, and Ronald Parr. Generalized prioritized sweeping. In Ad- vances In Neural Information Processing Systems (NIPS), pages 1001\u20131007. MIT Press, 1997.  John Aslanides, Jan Leike, and Marcus Hutter. Universal reinforcement learning algorithms: In International Joint Conference on Arti\ufb01cial Intelligence  Survey and experiments. (IJCAI), pages 1403\u20131410, 2017.  Jimmy Lei Ba, Jamie Ryan Kiros, and Geo\ufb00rey E Hinton. Layer normalization. arXiv  preprint arXiv:1607.06450, 2016.  Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In World Congress on Computational Intelligence (WCCI), volume 4, pages 2448\u20132453, 1994.  Bikramjit Banerjee and Jing Peng. Performance bounded reinforcement learning in strategic interactions. In AAAI National Conference on Arti\ufb01cial Intelligence (AAAI), volume 4, pages 2\u20137, 2004.  Samuel Barrett, Matt Taylor, and Peter Stone. Transfer learning for reinforcement learning on a physical robot. Adaptive Learning Agents Workshop, International Conference on Autonomous Agents and Multiagent Systems (AAMAS - ALA), 2010.  Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems (NIPS), pages 1471\u20131479, 2016.  Yoshua Bengio, J\u00b4er\u02c6ome Louradour, Ronan Collobert, and Jason Weston. Curriculum learn-  ing. In International Conference on Machine Learning (ICML), pages 41\u201348, 2009.  Lucian Bu\u00b8soniu, Damien Ernst, Robert Babu\u02c7ska, and Bart De Schutter. Approximate dynamic programming with a fuzzy parameterization. Automatica, 46(5):804\u2013814, 2010.  Wouter Caarls and Erik Schuitema. Parallel online temporal di\ufb00erence learning for motor control. IEEE Transactions on Neural Networks and Learning Systems, 27(7):1457\u20131468, 2016.  Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh.  Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 1281\u20131288, 2004.  Kamil Ciosek and Shimon Whiteson. OFFER: o\ufb00-environment reinforcement learning. In  AAAI Conference on Arti\ufb01cial Intelligence (AAAI), 2017. Experience Selection in Deep RL for Control  Ronan Collobert, Koray Kavukcuoglu, and Cl\u00b4ement Farabet. Torch7: A Matlab-like en- vironment for machine learning. BigLearn Workshop, Advances in Neural Information Processing Systems (NIPS - BLWS), 2011.  Tim de Bruin, Jens Kober, Karl Tuyls, and Robert Babu\u02c7ska. The importance of experience replay database composition in deep reinforcement learning. Deep Reinforcement Learn- ing Workshop, Advances in Neural Information Processing Systems (NIPS - DRLWS), 2015.  Tim de Bruin, Jens Kober, Karl Tuyls, and Robert Babu\u02c7ska. Improved deep reinforcement learning for robotics through distribution-based experience retention. In International Conference on Intelligent Robots and Systems (IROS), 2016a.  Tim de Bruin, Jens Kober, Karl Tuyls, and Robert Babu\u02c7ska. O\ufb00 policy experience retention for deep actor critic learning. Deep Reinforcement Learning Workshop, Advances in Neural Information Processing Systems (NIPS - DRLWS), 2016b.  Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. OpenAI Baselines. https: //github.com/openai/baselines, 2017.  Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In International Joint Conference on Arti\ufb01cial Intelligence (IJCAI), volume 15, pages 3460\u20133468, 2015.  Bradley Efron. Bootstrap methods: Another look at the jackknife. In Breakthroughs in  Statistics, pages 569\u2013593. Springer, 1992.  Vincent Fran\u00b8cois-Lavet, Raphael Fonteneau, and Damien Ernst. How to discount deep re- inforcement learning: Towards new dynamic strategies. arXiv preprint arXiv:1512.02011, 2015.  Gene F Franklin, David J Powell, and Michael L Workman. Digital Control of Dynamic  Systems, volume 3. Addison-Wesley Menlo Park, 1998.  Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction to boosting. Journal  of Japanese Society for Arti\ufb01cial Intelligence, 14(771-780):1612, 1999.  Javier Garc\u0131a and Fernando Fern\u00b4andez. A comprehensive survey on safe reinforcement  learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014.  Ian J Goodfellow, Mehdi Mirza, Xiao Da, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgeting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. de Bruin, Kober, Tuyls and Babu\u02c7ska  Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Q-  learning with model-based acceleration. arXiv preprint arXiv:1603.00748, 2016.  Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop: sample-e\ufb03cient policy gradient with an o\ufb00-policy critic. In International Con- ference on Learning Representations (ICLR), 2017.  Geo\ufb00rey E Hinton. To recognize shapes, \ufb01rst learn to generate images. Progress in Brain  Research, 165:535\u2013547, 2007.  Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: Variational information maximizing exploration. In Advances in Neural Informa- tion Processing Systems (NIPS), pages 1109\u20131117, 2016.  Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Inter-  national Conference for Learning Representations (ICLR), 2015.  Oleg Klimov. OpenAI Roboschool. https://github.com/openai/roboschool, 2017.  Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: a survey. International Journal of Robotics Research (IJRR), 32(11):1238\u20131274, 2013.  Ivan Koryakovskiy, Heike Vallery, Robert Babu\u02c7ska, and Wouter Caarls. Evaluation of physical damage associated with action selection strategies in reinforcement learning. In IFAC World Congress, 2017.  Leonid Kuvayev and Richard S Sutton. Model-based reinforcement learning with an ap-  proximate, learned model. Yale Workshop on Adaptive Learning Systems, 1996.  Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1\u201340, 2016.  Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.  Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and  teaching. Machine Learning, 8(3-4):293\u2013321, 1992.  Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. E\ufb03cient exploration for dialogue policy learning with BBQ networks & replay bu\ufb00er spiking. arXiv preprint arXiv:1608.05081, 2016.  Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural  networks. arXiv preprint arXiv:1511.06343, 2015.  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. Experience Selection in Deep RL for Control  Gr\u00b4egoire Montavon, Genevi`eve B. Orr, and Klaus-Robert M\u00a8uller, editors. Neural Networks: Tricks of the Trade. Lecture Notes in Computer Science (LNCS). Springer, 2nd edition, 2012.  Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 2924\u20132932, 2014.  Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: reinforcement learning  with less data and less time. Machine Learning, 13(1):103\u2013130, 1993.  Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Empirical Methods in Natural Language Processing (EMNLP), 2015.  Deanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm. Mathematical Programming, 155 (1-2):549\u2013573, 2016.  Brendan O\u2019Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient and Q-learning. In International Conference on Learning Representations (ICLR), 2017.  Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In Advances In Neural Information Processing Systems (NIPS), pages 4026\u20134034, 2016.  Mathijs Pieters and Marco A Wiering. Q-learning with experience replay in a dynamic In Symposium Series on Computational Intelligence (SSCI), pages 1\u20138.  environment. IEEE, 2016.  Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations (ICLR), 2018.  Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances In Neural Information Processing Systems (NIPS), pages 6550\u20136561, 2017.  Andrei A Rusu, Matej Vecerik, Thomas Roth\u00a8orl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016.  Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive  Sciences, 3(6):233\u2013242, 1999.  Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience  replay. In International Conference on Learning Representations (ICLR), 2016. de Bruin, Kober, Tuyls and Babu\u02c7ska  J\u00a8urgen Schmidhuber. A possibility for implementing curiosity and boredom in model- building neural controllers. In From Animals to Animats: International Conference on Simulation of Adaptive Behavior (SAB), 1991.  Young-Woo Seo and Byoung-Tak Zhang. Learning user\u2019s preferences by analyzing web- browsing behaviors. In International Conference on Autonomous Agents (ICAA), pages 381\u2013387, 2000.  David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried- miller. Deterministic policy gradient algorithms. In International Conference on Machine Learning (ICML), 2014.  David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.  David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Master- ing the game of Go without human knowledge. Nature, 550(7676):354\u2013359, 2017.  Burrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.  Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting.  ACM SIGART Bulletin, 2(4):160\u2013163, 1991.  Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Sequential de-  cision making with coherent risk. IEEE Transactions on Automatic Control, 2016.  George E Uhlenbeck and Leonard S Ornstein. On the theory of the Brownian motion.  Physical Review, 36(5):823, 1930.  Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142,  1984.  Je\ufb00rey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical  Software (TOMS), 11(1):37\u201357, 1985.  Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample e\ufb03cient actor-critic with experience replay. In International Conference on Learning Representations (ICLR), 2017. "}, "Model-Free Trajectory-based Policy Optimization with Monotonic Improvement": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-329.html", "header": "Model-Free Trajectory-based Policy Optimization with Monotonic Improvement", "author": "Riad Akrour, Abbas Abdolmaleki, Hany Abdulsamad, Jan Peters, Gerhard Neumann", "time": "19(14):1\u221225, 2018.", "abstract": "Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update. One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies. These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems. However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy. In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement. The algorithm backpropagates a local, quadratic and time-dependent \\qfunc learned from trajectory data instead of a model of the system dynamics. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics. We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics. In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations.", "pdf_url": "http://jmlr.org/papers/volume19/17-329/17-329.pdf", "reference": "A. Abdolmaleki, R. Lioutikov, J. Peters, N. Lau, L. Pualo Reis, and G. Neumann. Model- based relative entropy stochastic search. In Advances in Neural Information Processing Systems (NIPS). 2015.  R. Akrour, A. Abdolmaleki, H. Abdulsamad, and G. Neumann. Model-free trajectory op- timization for reinforcement learning. In International Conference on Machine Learning (ICML), 2016.  D. P. Bertsekas. Dynamic programming and optimal control. Athena Scienti\ufb01c, 1995.  S. Bhojanapalli, A. T. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-  de\ufb01nite optimization. CoRR, 2015.  C. Daniel, G. Neumann, and J. Peters. Hierarchical Relative Entropy Policy Search. In  International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2012.  M. Deisenroth and C. Rasmussen. PILCO: A Model-Based and Data-E\ufb03cient Approach to  Policy Search. In International Conference on Machine Learning (ICML), 2011.  M. P. Deisenroth, G. Neumann, and J. Peters. A Survey on Policy Search for Robotics.  Foundations and Trends in Robotics, 2013.  P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.  A. Ijspeert and S. Schaal. Learning attractor landscapes for learning motor primitives. In  Advances in Neural Information Processing Systems (NIPS). 2003.  S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University  College London, 2003.  S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning.  In International Conference on Machine Learning (ICML), 2002.  A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-e\ufb03cient generalization of robot skills with contextual policy search. In The Conference on Arti\ufb01cial Intelligence (AAAI), 2013.  S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems (NIPS). 2014.  S. Levine and V. Koltun. Learning complex neural network policies with trajectory opti-  mization. In International Conference on Machine Learning (ICML), 2014.  T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.  Continuous control with deep reinforcement learning. CoRR, 2015.  24   Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann  References  A. Abdolmaleki, R. Lioutikov, J. Peters, N. Lau, L. Pualo Reis, and G. Neumann. Model- based relative entropy stochastic search. In Advances in Neural Information Processing Systems (NIPS). 2015.  R. Akrour, A. Abdolmaleki, H. Abdulsamad, and G. Neumann. Model-free trajectory op- timization for reinforcement learning. In International Conference on Machine Learning (ICML), 2016.  D. P. Bertsekas. Dynamic programming and optimal control. Athena Scienti\ufb01c, 1995.  S. Bhojanapalli, A. T. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-  de\ufb01nite optimization. CoRR, 2015.  C. Daniel, G. Neumann, and J. Peters. Hierarchical Relative Entropy Policy Search. In  International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2012.  M. Deisenroth and C. Rasmussen. PILCO: A Model-Based and Data-E\ufb03cient Approach to  Policy Search. In International Conference on Machine Learning (ICML), 2011.  M. P. Deisenroth, G. Neumann, and J. Peters. A Survey on Policy Search for Robotics.  Foundations and Trends in Robotics, 2013.  P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.  A. Ijspeert and S. Schaal. Learning attractor landscapes for learning motor primitives. In  Advances in Neural Information Processing Systems (NIPS). 2003.  S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University  College London, 2003.  S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning.  In International Conference on Machine Learning (ICML), 2002.  A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-e\ufb03cient generalization of robot skills with contextual policy search. In The Conference on Arti\ufb01cial Intelligence (AAAI), 2013.  S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems (NIPS). 2014.  S. Levine and V. Koltun. Learning complex neural network policies with trajectory opti-  mization. In International Conference on Machine Learning (ICML), 2014.  T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.  Continuous control with deep reinforcement learning. CoRR, 2015. Model-Free Trajectory-based Policy Optimization  V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015.  K. M\u00a8ulling, J. Kober, and J. Peters. A biomimetic approach to robot table tennis. Adaptive  Behavior Journal, 2011.  Y. Pan and E. Theodorou. Probabilistic di\ufb00erential dynamic programming. In Advances in  Neural Information Processing Systems (NIPS). 2014.  J. Peters, K. M\u00a8ulling, and Y. Altun. Relative entropy policy search.  In Conference on  Arti\ufb01cial Intelligence (AAAI), 2010.  M. Pirotta, M. Restelli, and L. Bascetta. Adaptive step-size for policy gradient methods.  In Advances in Neural Information Processing Systems (NIPS). 2013a.  M. Pirotta, M. Restelli, A. Pecorino, and D. Calandriello. Safe policy iteration. In Inter-  national Conference on Machine Learning (ICML), 2013b.  E. A. R\u00a8uckert, M. Mindt, J. Peters, and G. Neumann. Robust policy updates for stochastic optimal control. In International Conference on Humanoid Robots (Humanoids), 2014.  J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy opti-  mization. In International Conference on Machine Learning (ICML), 2015.  D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrit- twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016.  C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.  E. Theodorou, J. Buchli, and S. Schaal. Path integral stochastic optimal control for rigid body dynamics. In International Symposium on Approximate Dynamic Programming and Reinforcement Learning (ADPRL), 2009.  E. Theodorou, Y. Tassa, and E. Todorov. Stochastic di\ufb00erential dynamic programming. In  American Control Conference (ACC), 2010.  E. Todorov. Optimal control theory. Bayesian Brain, 2006.  E. Todorov and Y. Tassa. Iterative local dynamic programming. In International Symposium  on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2009.  M. Toussaint. Robot trajectory optimization using approximate inference. In International  Conference on Machine Learning (ICML), 2009.  P. Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy  iteration. In Advances in Neural Information Processing Systems (NIPS). 2011. "}, "Regularized Optimal Transport and the Rot Mover's Distance": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-361.html", "header": "Regularized Optimal Transport and the Rot Mover's Distance", "author": "Arnaud Dessein, Nicolas Papadakis, Jean-Luc Rouas", "time": "19(15):1\u221253, 2018.", "abstract": "This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classification.", "pdf_url": "http://jmlr.org/papers/volume19/17-361/17-361.pdf", "reference": "Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network Flows: Theory, Algorithms and Applications. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1993.  Ibrahim Alabdulmohsin, Xin Gao, and Xiangliang Zhang. Support vector machines with inde\ufb01nite kernels. In Asian Conference on Machine Learning (ACML), pages 32\u201347, 2014.  Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, USA, 2000.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein GAN. Technical report,  arXiv:1701.07875, 2017.  Heinz H. Bauschke and Adrian S. Lewis. Dykstra\u2019s algorithm with Bregman projections:  A convergence proof. Optimization, 48(4):409\u2013427, 2000.  Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00b4e. Iterative Bregman projections for regularized transportation problems. SIAM Journal on Scienti\ufb01c Computing, 37(2):A1111\u2013A1138, 2015.  Espen Bernton, Pierre E. Jacob, Mathieu Gerber, and Christian P. Robert. Inference in generative models using the Wasserstein distance. Technical report, arXiv:1701.05146, 2017.  J\u00b4er\u00b4emie Bigot, Ra\u00b4ul Gouet, Thierry Klein, and Alfredo L\u00b4opez. Geodesic PCA in the Wasser-  stein space. Technical report, arXiv:1307.7721, 2013.  Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport.  Technical report, arXiv:1710.06276, 2017.  Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. ACM Transactions on Graphics, 30(6): 158:1\u2013158:12, 2011.  Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bern- hard Sch\u00a8olkopf. From optimal transport to generative modeling: the VEGAN cookbook. Technical report, arXiv:1705.07642, 2017.  50   Dessein, Papadakis and Rouas  Acknowledgments  This study has been carried out with \ufb01nancial support from the French State, managed by the French National Research Agency (ANR) in the frame of the \u201cInvestments for the future\u201d Program IdEx Bordeaux (ANR-10-IDEX-03-02), Cluster of excellence CPU, and the GOTMI project (ANR-16-CE33-0010-01). The authors would like to thank Annamaria Mesaros for her kind help with the evaluation on the DCASE 2016 IEEE AASP chal- lenge, Charles-Alban Deledalle for his valuable advice on the use of the computing platform PlaFRIM, and Marco Cuturi for the insightful discussions about this work.  References  Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network Flows: Theory, Algorithms and Applications. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1993.  Ibrahim Alabdulmohsin, Xin Gao, and Xiangliang Zhang. Support vector machines with inde\ufb01nite kernels. In Asian Conference on Machine Learning (ACML), pages 32\u201347, 2014.  Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, USA, 2000.  Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein GAN. Technical report,  arXiv:1701.07875, 2017.  Heinz H. Bauschke and Adrian S. Lewis. Dykstra\u2019s algorithm with Bregman projections:  A convergence proof. Optimization, 48(4):409\u2013427, 2000.  Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00b4e. Iterative Bregman projections for regularized transportation problems. SIAM Journal on Scienti\ufb01c Computing, 37(2):A1111\u2013A1138, 2015.  Espen Bernton, Pierre E. Jacob, Mathieu Gerber, and Christian P. Robert. Inference in generative models using the Wasserstein distance. Technical report, arXiv:1701.05146, 2017.  J\u00b4er\u00b4emie Bigot, Ra\u00b4ul Gouet, Thierry Klein, and Alfredo L\u00b4opez. Geodesic PCA in the Wasser-  stein space. Technical report, arXiv:1307.7721, 2013.  Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport.  Technical report, arXiv:1710.06276, 2017.  Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. ACM Transactions on Graphics, 30(6): 158:1\u2013158:12, 2011.  Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bern- hard Sch\u00a8olkopf. From optimal transport to generative modeling: the VEGAN cookbook. Technical report, arXiv:1705.07642, 2017. Rot Mover\u2019s Distance  Elsa Cazelles, Vivien Seguy, J\u00b4er\u00b4emie Bigot, Marco Cuturi, and Nicolas Papadakis. Log- PCA versus geodesic PCA of histograms in the Wasserstein space. Technical report, arXiv:1708.08143, 2017.  Roberto Cominetti and Jaime San Mart\u00b4\u0131n. Asymptotic analysis of the exponential penalty trajectory in linear programming. Mathematical Programming, 67(1\u20133):169\u2013187, 1994.  Nicolas Courty, R\u00b4emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853\u20131865, 2015.  Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Inter- national Conference on Neural Information Processing Systems (NIPS), pages 2292\u20132300, 2013.  Marco Cuturi and David Avis. Ground metric learning. Journal of Machine Learning  Research, 15(1):533\u2013564, 2014.  Marco Cuturi and Gabriel Peyr\u00b4e. A smoothed dual approach for variational Wasserstein  problems. SIAM Journal on Imaging Sciences, 9(1):320\u2013343, 2016.  Arnaud Dessein, Nicolas Papadakis, and Charles-Alban Deledalle. Parameter estimation in \ufb01nite mixture models by regularized optimal transport: A uni\ufb01ed framework for hard and soft clustering. Technical report, arXiv:1711.04366, 2017.  Inderjit S. Dhillon and Joel A. Tropp. Matrix nearness problems with Bregman divergences.  SIAM Journal on Matrix Analysis and Applications, 29(4):1120\u20131146, 2007.  Sira Ferradans, Nicolas Papadakis, Gabriel Peyr\u00b4e, and Jean-Fran\u00b8cois Aujol. Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3):1853\u20131882, 2014.  Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Pog- gio. Learning with a Wasserstein loss. In International Conference on Neural Information Processing Systems (NIPS), pages 2053\u20132061, 2015.  Alfred Galichon and Bernard Salani\u00b4e. Cupid\u2019s invisible hand: Social surplus and identi\ufb01-  cation in matching models. Technical report, SSRN:1804623, 2015.  Aude Genevay, Gabriel Peyr\u00b4e, and Marco Cuturi. GAN and VAE from an optimal transport  point of view. Technical report, arXiv:1706.01807, 2017.  Kristen Grauman and Trevor Darrell. Fast contour matching using approximate earth In IEEE Computer Vision and Pattern Recognition (CVPR), pages  mover\u2019s distance. 220\u2013227, 2004.  Joachim Gudmundsson, Oliver Klein, Christian Knauer, and Michiel Smid. Small Manhat- tan networks and algorithmic applications for the earth mover\u2019s distance. In European Workshop on Computational Geometry (EuroCG), pages 174\u2013177, 2007.  Bernard Haasdonk. Feature space interpretation of SVMs with inde\ufb01nite kernels. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 27(4):482\u2013492, 2005. Dessein, Papadakis and Rouas  Martin Idel. A review of matrix scaling and Sinkhorn\u2019s normal form for matrices and  positive maps. Technical report, arXiv:1609.06349, 2016.  Piotr Indyk and Nitin Thaper. Fast image retrieval via embeddings.  In International  Workshop on Statistical and Computational Theories of Vision (SCTV), 2003.  Sven Kurras. Symmetric iterative proportional \ufb01tting.  In International Conference on  Arti\ufb01cial Intelligence and Statistics (AISTATS), pages 526\u2013534, 2015.  Hsuan-Tien Lin and Chih-Jen Lin. A study on sigmoid kernels for SVM and the training of non-PSD kernels by SMO-type methods. Technical report, National Taiwan University, 2003.  Haibin Ling and Kazunori Okada. An e\ufb03cient earth mover\u2019s distance algorithm for robust histogram comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(5):840\u2013853, 2007.  Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. TUT database for acoustic scene classi\ufb01cation and sound event detection. In European Signal Processing Conference (EUSIPCO), pages 1128\u20131132, 2016.  Gr\u00b4egoire Montavon, Klaus-Robert M\u00a8uller, and Marco Cuturi. Wasserstein training of re- stricted Boltzmann machines. In International Conference on Neural Information Pro- cessing Systems (NIPS), pages 3718\u20133726, 2016.  Boris Muzellec, Richard Nock, Giorgio Patrini, and Frank Nielsen. Tsallis regularized optimal transport and ecological inference. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI), pages 2387\u20132393, 2018.  Assaf Naor and Gideon Schechtman. Planar earthmover is not in l1. SIAM Journal on  Computing, 37(3):804\u2013826, 2007.  Adam M. Oberman and Yuanlong Ruan. An e\ufb03cient linear programming method for  optimal transportation. Technical report, arXiv:1509.03668, 2015.  O\ufb01r Pele and Michael Werman. A linear time histogram metric for improved SIFT matching.  In European Conference on Computer Vision (ECCV), pages 495\u2013508, 2008.  O\ufb01r Pele and Michael Werman. Fast and robust earth mover\u2019s distances. In IEEE Inter-  national Conference on Computer Vision (ICCV), pages 460\u2013467, 2009.  Julien Rabin, Julie Delon, and Yann Gousseau. A statistical approach to the matching of  local features. SIAM Journal on Imaging Sciences, 2(3):931\u2013958, 2009.  Antoine Rolet, Marco Cuturi, and Gabriel Peyr\u00b4e. Fast dictionary learning with a smoothed In International Conference on Arti\ufb01cial Intelligence and Statistics  Wasserstein loss. (AISTATS), pages 630\u2013638, 2016.  Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. The earth mover\u2019s distance as a metric  for image retrieval. International Journal of Computer Vision, 40(2):99\u2013121, 2000. Rot Mover\u2019s Distance  Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngol`e, David Coeurjolly, Marco Cuturi, Peyr\u00b4e Gabriel, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised non-linear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643\u2013678, 2018.  Bernhard Schmitzer. A sparse multi-scale algorithm for dense optimal transport. Journal  of Mathematical Imaging and Vision, 56(2):238\u2013259, 2016a.  Bernhard Schmitzer. Stabilized sparse scaling algorithms for entropy regularized transport  problems. Technical report, arXiv:1610.06519, 2016b.  Vivien Seguy and Marco Cuturi. Principal geodesic analysis for probability measures un- In International Conference on Neural Information  der the optimal transport metric. Processing Systems (NIPS), pages 3312\u20133320, 2015.  Fei Sha, Yuanqing Lin, Lawrence K. Saul, and Daniel D. Lee. Multiplicative updates for  nonnegative quadratic programming. Neural Computation, 19(8):2004\u20132031, 2007.  Sameer Shirdhonkar and David W. Jacobs. Approximate earth mover\u2019s distance in linear time. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1\u20138, 2008.  Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic  matrices. Paci\ufb01c Journal of Mathematics, 21(2):343\u2013348, 1967.  Justin Solomon, Raif M. Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation for semi-supervised learning. In International Conference on Machine Learn- ing (ICML), pages 306\u2013314, 2014.  Justin Solomon, Fernando de Goes, Gabriel Peyr\u00b4e, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional Wasserstein distances: E\ufb03cient optimal transportation on geometric domains. ACM Transactions on Graphics, 34(4): 66:1\u201366:11, 2015.  Alexis Thibault, L\u00b4e\u00b4ena\u00a8\u0131c Chizat, Charles Dossal, and Nicolas Papadakis. Overre- laxed sinkhorn-knopp algorithm for regularized optimal transport. Technical report, arXiv:1711.01851, 2017.  Lars Thorlund-Petersen. Global convergence of Newton\u2019s method on an interval. Mathe-  matical Methods of Operations Research, 59(1):91\u2013110, 2004.  Paul Tseng. Dual coordinate ascent methods for non-strictly convex minimization. Mathe-  matical Programming, 59(1\u20133):231\u2013247, 1993.  C\u00b4edric Villani. Optimal Transport: Old and New, volume 338 of Comprehensive Studies in  Mathematics. Springer, Berlin Heidelberg, Germany, 2009.  Gloria Zen, Elisa Ricci, and Nicu Sebe. Simultaneous ground metric learning and ma- trix factorization with earth mover\u2019s distance. In International Conference on Pattern Recognition (ICPR), pages 3690\u20133695, 2014. "}, "Streaming kernel regression with provably adaptive mean, variance, and regularization": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-404.html", "header": "Streaming kernel regression with provably adaptive mean, variance, and regularization", "author": "Audrey Durand, Odalric-Ambrym Maillard, Joelle Pineau", "time": "19(17):1\u221234, 2018.", "abstract": "We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we ta", "pdf_url": "http://jmlr.org/papers/volume19/17-404/17-404.pdf", "reference": "Y. Abbasi-Yadkori, D. P\u00b4al, and C. Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems (NIPS), pages 2312\u20132320, 2011.  M. Abeille and A. Lazaric. Linear Thompson sampling revisited. In International Confer-  ence on Arti\ufb01cial Intelligence and Statistics (AISTATS), pages 176\u2013184, 2017.  M. Abramowitz and I. A. Stegun. Handbook of mathematical functions: with formulas,  graphs, and mathematical tables, volume 55. Courier Corporation, 1964.  S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payo\ufb00s.  In International Conference on Machine Learning (ICML), pages 127\u2013135, 2013.  32   Durand, Maillard, and Pineau  we deduce that with probability higher than 1  3\u03b4,  \u2212  RT (cid:54) (4\u221a\u03c0e + 1)  s\u03bb,t\u22121(xt)gt(\u03b4)  + R\u03b44\u03c0e + R  (cid:19)  (cid:19)  (cid:118) (cid:117) (cid:117) (cid:116)8\u03c0e  T (cid:88)  t=1  (1 + \u03b4  \u221a4\u03c0e t(t + 1)  )2 ln(1/\u03b4)  (cid:113)  8\u03c0e(1 + \u03b4\u221a4\u03c0e)2R  T ln(1/\u03b4)  (cid:112)  (cid:54) (4\u221a\u03c0e + 1)  s\u03bb,t\u22121(xt)gt(\u03b4)  + R\u03b44\u03c0e +  = (4\u221a\u03c0e + 1)  k\u03bbt,t\u22121(xt, xt) \u03bbt  B\u03bbt,t\u22121(\u03b4/4)(1 + ct,\u03b4)  (cid:19)  +R\u03b44\u03c0e +  8\u03c0e(1 + \u03b4\u221a4\u03c0e)2R  T ln(1/\u03b4) .  (cid:112)  This concludes the proof of the main result, since ct,\u03b4 (cid:54) cT,\u03b4.  Final bound Then, using Lemma 2 we can rewrite the regret as  (cid:18) T  (cid:88)  t=1 (cid:18) T  (cid:88)  t=1 (cid:18) T  (cid:88)  (cid:115)  t=1  (cid:113)  RT = (4\u221a\u03c0e + 1)(1 + cT,\u03b4)\u03c3+  1 +  2 ln(4/\u03b4) + 2\u03b3T (\u03c32  \u2212/C2)  (cid:16)  (cid:113)  +R\u03b44\u03c0e +  8\u03c0e(1 + \u03b4\u221a4\u03c0e)2R  T ln(1/\u03b4) .  (cid:112)  (cid:113)  (cid:115)  (cid:17) T (cid:88)  t=1  k\u03bbt,t\u22121(xt, xt) \u03bbt  Using Lemma 1 together with a Cauchy-Schwarz inequality, we \ufb01nally obtain  RT = (4\u221a\u03c0e + 1)(1 + cT,\u03b4)\u03c3+  1 +  2 ln(4/\u03b4) + 2\u03b3T (\u03c32  (cid:16)  (cid:113)  (cid:17) \u2212/C2)  (cid:118) (cid:117) (cid:117) (cid:116)  2T C2/\u03c32 (cid:16) 1 + C2 \u03c32  ln  (cid:17) \u03b3T (\u03c32/C2)  +R\u03b44\u03c0e +  8\u03c0e(1 + \u03b4\u221a4\u03c0e)2R  T ln(1/\u03b4) .  (cid:112)  (cid:113)  References  Y. Abbasi-Yadkori, D. P\u00b4al, and C. Szepesv\u00b4ari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems (NIPS), pages 2312\u20132320, 2011.  M. Abeille and A. Lazaric. Linear Thompson sampling revisited. In International Confer-  ence on Arti\ufb01cial Intelligence and Statistics (AISTATS), pages 176\u2013184, 2017.  M. Abramowitz and I. A. Stegun. Handbook of mathematical functions: with formulas,  graphs, and mathematical tables, volume 55. Courier Corporation, 1964.  S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payo\ufb00s.  In International Conference on Machine Learning (ICML), pages 127\u2013135, 2013. Streaming kernel regression with unknown variance  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit  problem. Machine learning, 47(2-3):235\u2013256, 2002.  E. Brochu, N. De Freitas, and A. Ghosh. Active preference learning with discrete choice In Advances in Neural Information Processing Systems (NIPS), pages 409\u2013416,  data. 2008.  S. Canu, X. Mary, and A. Rakotomamonjy. Functional learning through kernels. arXiv  preprint arXiv:0910.1013, 2009.  W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandits with linear payo\ufb00 func- tions. In International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), volume 15, pages 208\u2013214, 2011.  A. Cotter, J. Keshet, and N. Srebro. Explicit approximations of the Gaussian kernel. arXiv  preprint arXiv:1109.4603, 2011.  T. M Cover and J. A. Thomas. Elements of information theory. 1991.  A. Krause and C. S. Ong. Contextual Gaussian process bandit optimization. In Advances  in Neural Information Processing Systems (NIPS), pages 2447\u20132455, 2011.  L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person- alized news article recommendation. In International Conference on World Wide Web (WWW), pages 661\u20130670, 2010.  S. Loustau. Penalized empirical risk minimization over besov spaces. Electronic Journal of  Statistics, 3:824\u2013850, 2009.  O.-A. Maillard. Self-normalization techniques for streaming con\ufb01dent regression. work- URL https://hal.archives-ouvertes.fr/  ing paper or preprint, May 2016. hal-01349727.  R. Marchant and F. Ramos. Bayesian optimisation for informative continuous path plan- ning. In International Conference on Robotics and Automation (ICRA), pages 6136\u20136143. IEEE, 2014.  A. Maurer and M Pontil. Empirical Bernstein bounds and sample variance penalization. In  Annual Conference on Learning Theory (COLT), 2009.  C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT  Press, 2006.  J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine In Advances in Neural Information Processing Systems (NIPS),  learning algorithms. pages 2951\u20132959, 2012.  N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning (ICML), 2010. Durand, Maillard, and Pineau  N. Srinivas, A. Krause, S. M. Kakade, and M. W. Seeger.  Information-theoretic regret bounds for Gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 58(5):3250\u20133265, 2012.  W. R. Thompson. On the likelihood that one unknown probability exceeds another in view  of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.  M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of In Conference on Uncertainty In Arti\ufb01cial Intelligence  kernelised contextual bandits. (UAI), pages 654\u2013665, 2013.  Z. Wang and N. de Freitas. Theoretical analysis of Bayesian optimisation with unknown  Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014.  L. Wasserman. 10-702 Statistical Machine Learning, Function Spaces, Spring 2017 (Carnegie http://www.stat.cmu.edu/~larry/=sml/functionspaces.pdf,  Mellon University). 2017. (Accessed June 26, 2018).  A. Wilson, A. Fern, and P. Tadepalli. Using trajectory data to improve Bayesian opti- mization for reinforcement learning. Journal of Machine Learning Research, 15:253\u2013282, 2014. "}, "Dual Principal Component Pursuit": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-436.html", "header": "Dual Principal Component Pursuit", "author": "Manolis C. Tsakiris, Ren\u00c3\u00a9 Vidal", "time": "19(18):1\u221250, 2018.", "abstract": "We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications.", "pdf_url": "http://jmlr.org/papers/volume19/17-436/17-436.pdf"}, "Refining the Confidence Level for Optimistic Bandit Strategies": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-513.html", "header": "Refining the Confidence Level for Optimistic Bandit Strategies", "author": "Tor Lattimore", "time": "19(20):1\u221232, 2018.", "abstract": "This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is present", "pdf_url": "http://jmlr.org/papers/volume19/17-513/17-513.pdf"}, "Robust Synthetic Control": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-777.html", "header": "Robust Synthetic Control", "author": "Muhammad Amjad, Devavrat Shah, Dennis Shen", "time": "19(22):1\u221251, 2018.", "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method cf. \\cite{abadie3}, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors for the synthetic control, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. We posit that the setting can be viewed as an instance of the Latent Variable Model and provide the first finite sample analysis (coupled with asymptotic results) for the estimation of the counterfactual. Our algorithm accurately imputes missing entries and filters corrupted observations in producing a consistent estimator of the underlying signal matrix, provided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same proportion of observations, we demonstrate that the mean-squared error in our counterfactual estimation scales as $\\mathcal{O}(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the variance of the inherent noise. Additionally, we introduce a Bayesian framework to quantify the estimation uncertainty. Our experiments, using both synthetic and real-world datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.", "pdf_url": "http://jmlr.org/papers/volume19/17-777/17-777.pdf"}, "Reverse Iterative Volume Sampling for Linear Regression": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-781.html", "header": "Reverse Iterative Volume Sampling for Linear Regression", "author": "Micha\u00c5\u0082 Derezi\u00c5\u0084ski, Manfred K. Warmuth", "time": "19(23):1\u221239, 2018.", "abstract": "We study the following basic machine learning task: Given a fixed set of input points in $\\mathbb{R}^d$ for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension $d$ many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all $n$ responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.", "pdf_url": "http://jmlr.org/papers/volume19/17-781/17-781.pdf"}, "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems": {"volumn": 19, "url": "http://jmlr.org/papers/v19/18-020.html", "header": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems", "author": "Lyudmila Grigoryeva, Juan-Pablo Ortega", "time": "19(24):1\u221240, 2018.", "abstract": "A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family.", "pdf_url": "http://jmlr.org/papers/volume19/18-020/18-020.pdf"}, "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations": {"volumn": 19, "url": "http://jmlr.org/papers/v19/18-046.html", "header": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations", "author": "Maziar Raissi", "time": "19(25):1\u221224, 2018.", "abstract": "We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations.", "pdf_url": "http://jmlr.org/papers/volume19/18-046/18-046.pdf"}, "A Direct Approach for Sparse Quadratic Discriminant Analysis": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-285.html", "header": "A Direct Approach for Sparse Quadratic Discriminant Analysis", "author": "Binyan Jiang, Xiangyu Wang, Chenlei Leng", "time": "19(31):1\u221237, 2018.", "abstract": "Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets.", "pdf_url": "http://jmlr.org/papers/volume19/17-285/17-285.pdf"}, "Distribution-Specific Hardness of Learning Neural Networks": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-537.html", "header": "Distribution-Specific Hardness of Learning Neural Networks", "author": "Ohad Shamir", "time": "19(32):1\u221229, 2018.", "abstract": "Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the \u00e2\u0080\u009cniceness\u00e2\u0080\u009d of the input distribution, or \u00e2\u0080\u009cniceness\u00e2\u0080\u009d of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of \u00e2\u0080\u009cnice\u00e2\u0080\u009d target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is \u00e2\u0080\u009cnice\u00e2\u0080\u009d. To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions.", "pdf_url": "http://jmlr.org/papers/volume19/17-537/17-537.pdf"}, "Kernel Density Estimation for Dynamical Systems": {"volumn": 19, "url": "http://jmlr.org/papers/v19/16-349.html", "header": "Kernel Density Estimation for Dynamical Systems", "author": "Hanyuan Hang, Ingo Steinwart, Yunlong Feng, Johan A.K. Suykens", "time": "19(35):1\u221249, 2018.", "abstract": "We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the $\\mathcal{C}$-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under $L_1$-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under $L_1$-norm. In the analysis, the density function $f$ is only assumed to be H\\\"{o}lder continuous or pointwise H\\\"{o}lder controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under $L_\\infty$-norm and $L_1$-norm can be achieved when the density function is H\\\"{o}lder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations.", "pdf_url": "http://jmlr.org/papers/volume19/16-349/16-349.pdf"}, "Invariant Models for Causal Transfer Learning": {"volumn": 19, "url": "http://jmlr.org/papers/v19/16-432.html", "header": "Invariant Models for Causal Transfer Learning", "author": "Mateo Rojas-Carulla, Bernhard Sch\u00c3\u00b6lkopf, Richard Turner, Jonas Peters", "time": "19(36):1\u221234, 2018.", "abstract": "Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a  subset  of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.", "pdf_url": "http://jmlr.org/papers/volume19/16-432/16-432.pdf"}, "The xyz algorithm for fast interaction search in high-dimensional data": {"volumn": 19, "url": "http://jmlr.org/papers/v19/16-515.html", "header": "The xyz algorithm for fast interaction search in high-dimensional data", "author": "Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah", "time": "19(37):1\u221242, 2018.", "abstract": "When performing regression on a data set with $p$ variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complex", "pdf_url": "http://jmlr.org/papers/volume19/16-515/16-515.pdf"}, "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden {M}arkov Models": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-345.html", "header": "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden {M}arkov Models", "author": "Luc Leh\u00c3\u00a9ricy", "time": "19(39):1\u221246, 2018.", "abstract": "In this paper, we introduce a new estimator for the emission densities of a nonparametric hidden Markov model. It is adaptive and minimax with respect to each state's regularity\u2013as opposed to globally minimax estimators, which adapt to the worst regularity among the emission densities. Our method is based on Goldenshluger and Lepski's methodology. It is computationally efficient and only requires a family of preliminary estimators, without any restriction on the type of estimators considered. We present two such estimators that allow to reach minimax rates up to a logarithmic term: a spectral estimator and a least squares estimator. We show how to calibrate it in practice and assess its performance on simulations and on real data.", "pdf_url": "http://jmlr.org/papers/volume19/17-345/17-345.pdf"}, "Learning from Comparisons and Choices": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-607.html", "header": "Learning from Comparisons and Choices", "author": "Sahand Negahban, Sewoong Oh, Kiran K. Thekumparampil, Jiaming Xu", "time": "19(40):1\u221295, 2018.", "abstract": "When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions.", "pdf_url": "http://jmlr.org/papers/volume19/17-607/17-607.pdf"}, "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models": {"volumn": 19, "url": "http://jmlr.org/papers/v19/17-704.html", "header": "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models", "author": "Bin Dai, Yu Wang, John Aston, Gang Hua, David Wipf", "time": "19(41):1\u221242, 2018.", "abstract": "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.", "pdf_url": "http://jmlr.org/papers/volume19/17-704/17-704.pdf"}, "An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach": {"volumn": 19, "url": "http://jmlr.org/papers/v19/18-117.html", "header": "An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach", "author": "Julien Ah-Pine", "time": "19(42):1\u221243, 2018.", "abstract": "We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based A", "pdf_url": "http://jmlr.org/papers/volume19/18-117/18-117.pdf"}}