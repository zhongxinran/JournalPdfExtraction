{"Links Between Multiplicity Automata, Observable Operator Models and Predictive State Representations -- a Unified Learning Framework": {"volumn": 16, "url": "http://jmlr.org/papers/v16/thon15a.html", "header": "Links Between Multiplicity Automata, Observable Operator Models and Predictive State Representations -- a Unified Learning Framework", "author": "Michael Thon, Herbert Jaeger", "time": "16(4):103\u2212147, 2015.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume16/thon15a/thon15a.pdf", "keywords": ["multiplicity automata", "hidden Markov models", "observable operator models", "predictive state representations", "spectral learning algorithms"], "reference": "Naoki Abe and Manfred K. Warmuth. On the computational complexity of approximating  distributions by probabilistic automata. Machine Learning, 9:205-260, 1992.  Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture models and hidden markov models. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, Proceedings of the 25th Annual Conference on Learning  140   Thon and Jaeger  Proof [of Proposition 40] First, we calculate:  Var[C \u02c6F I,J ]  (cid:104)  (\u2217) = E  CF I,J  (cid:105)  2 F ||  cki \u02c6f (xjxi)  ckif (xjxi)  (cid:33)2\uf8f9 \uf8fb  (cid:88)  i\u2208I  \u2212  (cid:35)  C \u02c6F I,J || \uf8ee d (cid:88)  E  \uf8f0  \u2212 (cid:32)  (cid:88)  =  j\u2208J  k=1  (cid:88)  d (cid:88)  (cid:88)  i\u2208I  (cid:34)  (cid:88)  j\u2208J  k=1  i\u2208I  (cid:88)  d (cid:88)  (cid:88)  (\u2217) =  (\u2217\u2217) =  k=1  i\u2208I  j\u2208J (cid:88)  (cid:107)  i\u2208I  =  (C)i  2 F  (cid:107)  (cid:88)  j\u2208J  Var  cki \u02c6f (xjxi)  kiVar[ \u02c6f (xjxi)] c2  Var[ \u02c6f (xjxi)] =  (cid:88)  i\u2208I  vi  (C)i  (cid:107)  2 F ,  (cid:107)  ) and uncorrelatedness in (  where (C)i is the i-th column of C, and vi = (cid:80) j\u2208J Var[ \u02c6f (xjxi)]. Note that we have used ). unbiasedness in ( \u2217  Our goal is now to minimize J(G) = Var[(C\u2217 + G) \u02c6F I,J ] = (cid:80)  2 F subject || to the constraints hk,l(G) = [G\u03a0]k,l = 0 for k, l = 1 . . . d. Note that if vi = 0 for some i, then the i-th column of G does not in\ufb02uence the value of J(G), and we may w.l.o.g. fix (G)i = 0 and replace the equality constraints by \u02dchk,l(G) = [GDD\u2020\u03a0]k,l = 0, where D = diag[(vi)i\u2208I ]. This is a convex quadratic programming problem, therefore G = 0 will be a solution if and only if it satisfies the KKT conditions  (C\u2217 + G)i ||  i\u2208I vi  \u2217\u2217  \u2202J \u2202G  (G) +  d (cid:88)  k,l=1  \u03bbk,l  \u2202hk,l \u2202G  k, l = 1 . . . d : \u02dchk,l(G) = 0, \u2200  (G) = 0, and  for some Lagrange multipliers \u03bbk,l isfied for all k, l by G = 0. We can calculate (cid:80)d Rd\u00d7d, [\u03bb]k,l = \u03bbk,l, as well as \u2202J \u03bb condition is then satisfied by G = 0 with \u03bb = of DI .  R. Clearly, the latter condition \u02dchk,l(G) = 0 is sat- \u2202\u02dchk,l \u2202G (G) = \u03bb\u03a0(cid:62)D\u2020D, where I + G)D. The first I D = \u03a0(cid:62)D\u2020D by definition  \u2202G (G) = 2(C\u2217 + G)D = 2(\u03a0(cid:62)D2  2I, since \u03a0(cid:62)D2  k,l=1 \u03bbk,l  \u2212  \u2208  \u2208  References  Naoki Abe and Manfred K. Warmuth. On the computational complexity of approximating  distributions by probabilistic automata. Machine Learning, 9:205-260, 1992.  Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture models and hidden markov models. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, Proceedings of the 25th Annual Conference on Learning Links Between MA, OOMs and PSRs  Theory (COLT 2012), volume 23 of JMLR Workshop & Conference Proceedings, pages 33.1-33.34, 2012.  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1987.  Rapha\u00a8el Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maxi- In Chun-Nan Hsu and Wee Sun Lee, editors, Proceedings of the 3rd Asian mization. Conference on Machine Learning (ACML 2011), volume 20 of JMLR Workshop & Con- ference Proceedings, pages 147-163, 2011.  Rapha\u00a8el Bailly, Fran\u00b8cois Denis, and Liva Ralivola. Grammatical inference as a princi- pal component analysis problem. In Andrea Pohoreckyj Danyluk, L\u00b4eon Bottou, and Michael L. Littman, editors, Proceedings of the 26th International Conference on Ma- chine Learning (ICML 2009), volume 382 of ACM Proceedings, pages 33-40, 2009.  Raphael Bailly, Xavier Carreras, and Ariadna Quattoni. Unsupervised spectral learning of finite state transducers. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 800-808. Curran Associates, Inc., 2013.  Borja Balle and Mehryar Mohri. Spectral learning of general weighted automata via con- strained matrix completion. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Infor- mation Processing Systems 25 (NIPS 2012), pages 2168-2176, 2012.  Borja Balle, Ariadna Quattoni, and Xavier Carreras. A spectral learning algorithm for finite state transducers. In Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis Vazirgiannis, editors, Machine Learning and Knowledge Discovery in Databases - European Conference (ECML/PKDD 2011), Proceedings, Part I, volume 6911 of Lecture Notes in Computer Science, pages 156-171. Springer, 2011.  Borja Balle, Ariadna Quattoni, and Xavier Carreras. Local loss optimization in operator models: A new insight into spectral learning. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012). icml.cc / Omnipress, 2012.  Borja Balle, Xavier Carreras, Franco M. Luque, and Ariadna Quattoni. Spectral learning of weighted automata - a forward-backward perspective. Machine Learning, 96(1-2):33-63, 2014.  Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Var- In Proceedings of ricchio. On the applications of multiplicity automata in learning. the 37th Annual Symposium on Foundations of Computer Science (FOCS 1996), pages 349-358. IEEE Computer Society, 1996.  Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Var- ricchio. Learning functions represented as multiplicity automata. Journal of the ACM, 47(3):506-530, 2000. Thon and Jaeger  Francesco Bergadano and Stefano Varricchio. Learning behaviors of automata from mul- tiplicity and equivalence queries. In Maurizio A. Bonuccelli, Pierluigi Crescenzi, and Rossella Petreschi, editors, Proceedings of the 2nd Italian conference on Algorithms and Complexity (CIAC 1994), volume 778 of Lecture Notes in Computer Science, pages 54-62. Springer, 1994.  Francesco Bergadano, Dario Catalano, and Stefano Varricchio. Learning sat-k-DNF formu- las from membership queries. In Proceedings of the 28th Annual ACM Symposium on Theory of Computing (STOC 1996), pages 126-130. ACM, 1996.  Jean Berstel, Jr. and Christophe Reutenauer. Rational Series and Their Languages, vol-  ume 12 of EATCS Monographs on Theoretical Computer Science. Springer, 1988.  Byron Boots and Geo\ufb00rey J. Gordon. Predictive state temporal di\ufb00erence learning.  In J. La\ufb00erty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23 (NIPS 2010), pages 271-279. MIT Press, 2010.  Byron Boots and Geo\ufb00rey J. Gordon. An online spectral learning algorithm for partially observable nonlinear dynamical systems. In Wolfram Burgard and Dan Roth, editors, Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI 2011). AAAI Press, 2011.  Byron Boots, Sajid M. Siddiqi, and Geo\ufb00rey J. Gordon. Closing the learning-planning loop In Proceedings of the 9th International Confer- with predictive state representations. ence on Autonomous Agents and Multiagent Systems (AAMAS 2010), pages 1369-1370. IFAAMAS, 2010.  Byron Boots, Geo\ufb00rey J. Gordon, and Arthur Gretton. Hilbert space embeddings of pre- dictive state representations. In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013), pages 92-101. AUAI Press, 2013.  Michael Bowling, Peter McCracken, Michael James, James Neufeld, and Dana F. Wilkinson. Learning predictive state representations using non-blind policies. In William W. Cohen and Andrew Moore, editors, Proceedings of the 23rd International Conference on Machine Learning (ICML 2006), volume 148 of ACM Proceedings, pages 129-136, 2006.  Jack W. Carlyle and Azaria Paz. Realizations by stochastic finite automata. Journal of  Computer and System Sciences, 5(1):26-40, 1971.  Corinna Cortes and Mehryar Mohri. Context-free recognition with weighted automata.  Grammars, 3(2/3):133-150, 2000.  Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39 (1):1-38, 1977.  Fran\u00b8cois Denis and Yann Esposito. Learning classes of probabilistic automata. In Proceed- ings of the 17th Annual Conference on Learning Theory (COLT 2004), volume 3120 of Lecture Notes in Computer Science, pages 124-139. Springer, 2004. Links Between MA, OOMs and PSRs  Fran\u00b8cois Denis and Yann Esposito. On rational stochastic languages. Fundamenta Infor-  maticae, 86(1):41-77, 2008.  Fran\u00b8cois Denis, Yann Esposito, and Amaury Habrard. Learning rational stochastic lan- guages. In G\u00b4abor Lugosi and Hans-Ulrich Simon, editors, Proceedings of the 19th Annual Conference on Learning Theory (COLT 2006), volume 4005 of Lecture Notes in Computer Science, pages 274-288. Springer, 2006.  Manfred Droste, Werner Kuich, and Heiko Vogler. Handbook of Weighted Automata.  Springer, 2009.  Pierre Dupont, Fran\u00b8cois Denis, and Yann Esposito. Links between probabilistic automata and hidden Markov models: probability distributions, learning models and induction algorithms. Pattern Recognition, 38(9):1349-1371, 2005.  Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.  Psychometrika, 1(3):211-218, 1936.  Michel Fliess. Matrices de Hankel. Journal de Math\u00b4ematiques Pures et Appliqu\u00b4ees, 53:  197-222, 1974.  Edgar J. Gilbert. On the identifiability problem for functions of finite Markov chains. The  Annals of Mathematical Statistics, 30(3):688-697, 1959.  Robert M. Gray. Probability, Random Processes, and Ergodic Properties. Springer, 1988.  William L. Hamilton, Mahdi M. Fard, and Joelle Pineau. Modelling sparse dynamical sys- tems with compressed predictive state representations. In Sanjoy Dasgupta and David Mcallester, editors, Proceedings of the 30th International Conference on Machine Learn- ing (ICML 2013), volume 28 of JMLR Workshop & Conference Proceedings, pages 178- 186, 2013.  Per Christian Hansen. Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1998.  Alex Heller. On stochastic processes derived from Markov chains. The Annals of Mathe-  matical Statistics, 36(4):1286-1291, 1965.  Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden In Proceedings of the 22nd Annual Conference on Learning Theory  Markov models. (COLT 2009), 2009.  Hisashi Ito. An Algebraic Study of Discrete Stochastic Systems. Unpublished doctoral  dissertation, University of Tokyo, Bunkyo-ku, Tokyo, 1992.  Hisashi Ito, Shun ichi Amari, and Kingo Kobayashi. Identifiability of hidden Markov infor- mation sources and their minimum degrees of freedom. IEEE Transactions on Informa- tion Theory, 38(2):324-333, 1992. Thon and Jaeger  Herbert Jaeger. Observable operator models and conditioned continuation representations. Arbeitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Au- gustin, Germany, 1997.  Herbert Jaeger. Discrete-time, discrete-valued observable operator models: a tutorial. Tech- nical Report 42, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Ger- many, 1998.  Herbert Jaeger. Modeling and learning continuous-valued stochastic processes with OOMs. GMD Report 102, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Ger- many, 2000a.  Herbert Jaeger. Observable operator models for discrete stochastic time series. Neural  Computation, 12(6):1371-1398, 2000b.  Herbert Jaeger, MingJie Zhao, and Andreas Kolling. E\ufb03cient estimation of ooms.  In Y. Weiss, B. Sch\u00a8olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18 (NIPS 2005), pages 555-562. MIT Press, 2006a.  Herbert Jaeger, MingJie Zhao, Klaus Kretzschmar, Tobias Oberstein, Dan Popovici, and Andreas Kolling. Learning observable operator models via the ES algorithm. In Simon Haykin, Jos\u00b4e C. Pr\u00b4\u0131ncipe, Terrence J. Sejnowski, and John McWhirter, editors, New Directions in Statistical Signal Processing: From Systems to Brains, Neural Information Processing, chapter 14, pages 417-464. MIT Press, Cambridge, MA, USA, 2006b.  Michael R. James and Satinder P. Singh. Learning and discovery of predictive state rep- resentations in dynamical systems with reset. In Carla E. Brodley, editor, Proceedings of the 21st International Conference on Machine Learning (ICML 2004), volume 69 of ACM Proceedings, pages 53-60, 2004.  Michael R. James and Satinder P. Singh. Planning in models that combine memory with predictive representations of state. In Manuela M. Veloso and Subbarao Kambhampati, editors, Proceedings of the 20th National Conference on Artificial Intelligence (AAAI 2005), pages 987-992. AAAI Press, 2005.  Michael R. James, Satinder Singh, and Michael L. Littman. Planning with predictive state representations. In Proceedings of the 3rd International Conference on Machine Learning and Applications (ICMLA 2004), pages 304-311. IEEE Computer Society, 2004.  Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99-134, 1998.  Attila Kondacs and John Watrous. On the power of quantum finite state automata. In Proceedings of the 37th Annual Symposium on Foundations of Computer Science (FOCS 1996), pages 66-75. IEEE Computer Society, 1997.  Klaus Kretzschmar. Learning symbol sequences with observable operator models. GMD Report 161, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Germany, 2001. Links Between MA, OOMs and PSRs  Michael L. Littman, Richard S. Sutton, and Satinder P. Singh. Predictive representations of state. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14 (NIPS 2001), pages 1555-1561. MIT Press, 2001.  Ivan Markovsky and Sabine Van Hu\ufb00el. Left vs right representations for solving weighted low-rank approximation problems. Linear Algebra and its Applications, 422(2-3):540-552, 2007a.  Ivan Markovsky and Sabine Van Hu\ufb00el. Overview of total least-squares methods. Signal  Processing, 87(10):2283-2302, 2007b.  Peter McCracken and Michael H. Bowling. Online discovery and learning of predictive In Y. Weiss, B. Sch\u00a8olkopf, and J. Platt, editors, Advances in  state representations. Neural Information Processing Systems 18 (NIPS 2005). MIT Press, 2006.  Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted finite-state transducers in  speech recognition. Computer Speech & Language, 16(1):69-88, 2002.  Cristopher Moore and James P. Crutchfield. Quantum automata and quantum grammars.  Theoretical Computer Science, 237(1-2):275-306, 2000.  Hiroyuki Ohnishi, Hiroyuki Seki, and Tadao Kasami. A polynomial time learning algorithm IEICE Transactions on Information and Systems, E77-D(10):  for recognizable series. 1077-1085, 1994.  Lawrence R. Rabiner. A tutorial on hidden Markov models and selected applications in  speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989.  Adri`a Recasens and Ariadna Quattoni. Spectral learning of sequence taggers over continuous sequences. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Zelezn\u00b4y, editors, Machine Learning and Knowledge Discovery in Databases - European Conference (ECML/PKDD 2013), Proceedings, Part I, volume 8188 of Lecture Notes in Computer Science, pages 289-304. Springer, 2013.  Matthew Rosencrantz, Geo\ufb00rey J. Gordon, and Sebastian Thrun. Learning low dimensional predictive representations. In Carla E. Brodley, editor, Proceedings of the 21st Interna- tional Conference on Machine Learning (ICML 2004), volume 69 of ACM Proceedings, pages 695-702, 2004.  Sam Roweis. EM algorithms for PCA and SPCA. In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances in Neural Information Processing Systems 10 (NIPS 1997), pages 626-632. MIT Press, 1998.  Matthew Rudary and Satinder P. Singh. Predictive linear-Gaussian models of controlled stochastic dynamical systems. In William W. Cohen and Andrew Moore, editors, Pro- ceedings of the 23rd International Conference on Machine Learning (ICML 2006), volume 148 of ACM Proceedings, pages 777-784, 2006. Thon and Jaeger  Matthew Rudary and Satinder P. Singh. Predictive linear-Gaussian models of stochastic dynamical systems with vector-value actions and observations. In Proceedings of the 10th International Symposium on Artificial Intelligence and Mathematics (ISAIM 2008), 2008.  Matthew Rudary, Satinder P. Singh, and David Wingate. Predictive linear-Gaussian mod- els of stochastic dynamical systems. In Fahiem Bacchus and Tommi Jaakkola, editors, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence (UAI 2005), pages 501-508. AUAI Press, 2005.  Matthew R. Rudary and Satinder Singh. A nonlinear predictive state representation. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Process- ing Systems 15 (NIPS 2002), pages 855-862. MIT Press, 2003.  Arto Salomaa and Matti Soittola. Automata-Theoretic Aspects of Formal Power Series.  Texts and Monographs in Computer Science. Springer, 1978.  Marcel Paul Sch\u00a8utzenberger. On the definition of a family of automata. Information and  Control, 4(2-3):245-270, 1961.  Sajid M. Siddiqi, Byron Boots, and Geo\ufb00rey J. Gordon. Reduced-rank hidden markov In Yee Whye Teh and D. Mike Titterington, editors, Proceedings of the 13th models. International Conference on Artificial Intelligence and Statistics (AISTATS 2010), vol- ume 9 of JMLR Workshop & Conference Proceedings, pages 741-748, 2010.  Satinder Singh, Michael R. James, and Matthew R. Rudary. Predictive state representa- tions: A new theory for modeling dynamical systems. In Joseph Halpern, editor, Proceed- ings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI 2004), pages 512-519. AUAI Press, 2004.  Le Song, Byron Boots, Sajid M. Siddiqi, Geo\ufb00rey J. Gordon, and Alex J. Smola. Hilbert space embeddings of hidden Markov models. In Johannes F\u00a8urnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pages 991-998. Omnipress, 2010.  Daniel R. Upper. Theory and Algorithms for Hidden Markov Models and Generalized Hidden  Markov Models. PhD thesis, University of California at Berkeley, 1997.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-  1142, 1984.  Eric W. Wiewiora. Modeling Probability Distributions with Predictive State Representations.  PhD thesis, University of California, San Diego, 2008.  David Wingate and Satinder P. Singh. Kernel predictive linear Gaussian models for non- linear stochastic dynamical systems. In William W. Cohen and Andrew Moore, editors, Proceedings of the 23rd International Conference on Machine Learning (ICML 2006), volume 148 of ACM Proceedings, pages 1017-1024, 2006a. Links Between MA, OOMs and PSRs  David Wingate and Satinder P. Singh. Mixtures of predictive linear Gaussian models for nonlinear, stochastic dynamical systems. In Anthony Cohn, editor, Proceedings of the 21st National Conference on Artificial Intelligence (AAAI 2006). AAAI Press, 2006b.  David Wingate and Satinder P. Singh. Exponential family predictive representations of state. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS 2007), pages 1617-1624. MIT Press, 2008a.  David Wingate and Satinder P. Singh. E\ufb03ciently learning linear-linear exponential family predictive representations of state. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the 25th International Conference on Machine Learning (ICML 2008), volume 307 of ACM Proceedings, pages 1176-1183, 2008b.  David Wingate, Vishal Soni, Britton Wolfe, and Satinder P. Singh. Relational knowledge with predictive state representations. In Manuela M. Veloso, editor, Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007), pages 2035- 2040. AAAI Press, 2007.  Britton Wolfe and Satinder P. Singh. Predictive state representations with options.  In William W. Cohen and Andrew Moore, editors, Proceedings of the 23rd International Conference on Machine Learning (ICML 2006), volume 148 of ACM Proceedings, pages 1025-1032, 2006.  Lotfi Asker Zadeh. The concept of system, aggregate, and state in system theory.  In Lotfi Asker Zadeh and Elijah Polak, editors, System Theory, volume 8 of Inter-University Electronics Series, pages 3-42. McGraw-Hill, New York, 1969.  MingJie Zhao and Herbert Jaeger. Norm observable operator models. Neural Computation,  22(7):1927-1959, 2010.  MingJie Zhao, Herbert Jaeger, and Michael Thon. A bound on modeling error in observable operator models and an associated learning algorithm. Neural Computation, 21(9):2687- 2712, 2009a.  MingJie Zhao, Herbert Jaeger, and Michael Thon. Making the error-controlling algorithm of observable operator models constructive. Neural Computation, 21(12):3460-3486, 2009b. "}, "Multimodal Gesture Recognition via Multiple Hypotheses Rescoring": {"volumn": 16, "url": "http://jmlr.org/papers/v16/pitsikalis15a.html", "header": "Multimodal Gesture Recognition via Multiple Hypotheses Rescoring", "author": "Vassilis Pitsikalis, Athanasios Katsamanis, Stavros Theodorakis, Petros Maragos", "time": "16(9):255\u2212284, 2015.", "abstract": "We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature descriptors of the hands' movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further refined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%.", "pdf_url": "http://jmlr.org/papers/volume16/pitsikalis15a/pitsikalis15a.pdf", "keywords": ["multimodal gesture recognition", "HMMs", "speech recognition", "multimodal fusion", "activity detection"], "reference": "U. Agris, J. Zieren, U. Canzler, B. Bauer, and K.-F. Kraiss. Recent developments in visual sign language recognition. Universal Access in the Information Society, 6:323-362, 2008.  J. Alon, V. Athitsos, Q. Yuan, and S. Sclaro\ufb00. A unified framework for gesture recognition and spatiotemporal gesture segmentation. IEEE Trans. on Patt. Anal. and Mach. Intell., 31(9):1685-1699, 2009.  A. Argyros and M. Lourakis. Real time tracking of multiple skin-colored objects with a  possibly moving camera. In Proc. Europ. Conf. on Computer Vision, 2004.  B. Bauer and K. F. Kraiss. Towards an automatic sign language recognition system using  subunits. In Proc. of Int\u2019l Gest. Wrksp, volume 2298, pages 64-75, 2001.  I. Bayer and S. Thierry. A multi modal approach to gesture recognition from audio and video data. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 461-466. ACM, 2013.  P. Bernardis and M. Gentilucci. Speech and gesture share the same communication system.  Neuropsychologia, 44(2):178-190, 2006.  278   Pitsikalis, Katsamanis, Theodorakis and Maragos  decoding, fusion of the hypotheses, and then parallel fusion. Apart from the experimen- tal evidence, these features render it appealing for extensions and exploitation in multiple directions: First, the method itself can be advanced by generalizing the approach towards an iterative fusion scheme, that gives feedback back to the training/refinement stage of the statistical models. Moreover in the current generative framework, we ignore statistical dependencies across cues/modalities. These could further be examined. Second, it can be advanced by incorporating in the computational modeling specific gesture theories, e.g., from linguistics, for the gesture per se or in its multimodal version; taxonomies of gestures, e.g., that describe deictic, motor, iconic and metaphoric cases. Such varieties of cases can be systematically studied with respect to their role. This could be achieved via automatic processing of multitudes of existing data sets, which elaborate more complex speech-gesture issues, leading to valuable analysis results. Then, apart from the linguistic role of gesture, its relation to other aspects, such as, psychological, behavioral socio-cultural, or commu- nicative, to name but a few, could further be exploited. To conclude, given the potential of the proposed approach, the acute interdisciplinary interest in multimodal gesture calls for further exploration and advancements.  Acknowledgements  This research work was supported by the project \u201cCOGNIMUSE\u201d which is implemented under the \u201cARISTEIA\u201d Action of the Operational Program Education and Lifelong Learn- ing and is co-funded by the European Social Fund and Greek National Resources. It was also partially supported by the European Union under the project \u201cMOBOT\u201d with grant FP7-ICT-2011-9 2.1 - 600796. The authors want to gratefully thank Georgios Pavlakos for his contribution in previous, earlier stages, of this work.  References  U. Agris, J. Zieren, U. Canzler, B. Bauer, and K.-F. Kraiss. Recent developments in visual sign language recognition. Universal Access in the Information Society, 6:323-362, 2008.  J. Alon, V. Athitsos, Q. Yuan, and S. Sclaro\ufb00. A unified framework for gesture recognition and spatiotemporal gesture segmentation. IEEE Trans. on Patt. Anal. and Mach. Intell., 31(9):1685-1699, 2009.  A. Argyros and M. Lourakis. Real time tracking of multiple skin-colored objects with a  possibly moving camera. In Proc. Europ. Conf. on Computer Vision, 2004.  B. Bauer and K. F. Kraiss. Towards an automatic sign language recognition system using  subunits. In Proc. of Int\u2019l Gest. Wrksp, volume 2298, pages 64-75, 2001.  I. Bayer and S. Thierry. A multi modal approach to gesture recognition from audio and video data. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 461-466. ACM, 2013.  P. Bernardis and M. Gentilucci. Speech and gesture share the same communication system.  Neuropsychologia, 44(2):178-190, 2006. Multimodal Gesture Recognition via Multiple Hypotheses Rescoring  N. D. Binh, E. Shuichi, and T. Ejima. Real-time hand tracking and gesture recognition system. In Proc. of Int\u2019l Conf. on Graphics, Vision and Image Processing (GVIP), pages 19-21, 2005.  A. F. Bobick and J. W. Davis. The recognition of human movement using temporal tem-  plates. IEEE Trans. on Patt. Anal. and Mach. Intell., 23(3):257-267, 2001.  R. A. Bolt. \u201cPut-that-there\u201d: Voice and gesture at the graphics interface. In Proc. of the 7th Annual Conf. on Computer Graphics and Interactive Techniques, volume 14. ACM, 1980.  H. Bourlard and S. Dupont. Subband-based speech recognition. In Proc. Int\u2019l Conf. on  Acoustics, Speech and Sig. Proc., volume 2, pages 1251-1254. IEEE, 1997.  K. Bousmalis, L. Morency, and M. Pantic. Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition. In Proc. Int\u2019l Conf. on Autom. Face & Gest. Rec., pages 746-752. IEEE, 2011.  P. Buehler, M. Everingham, and A. Zisserman. Learning sign language by watching TV (using weakly aligned subtitles). In Proc. Int\u2019l Conf. on Comp. Vis. & Patt. Rec., 2009.  S. Celebi, A. S. Aydin, T. T. Temiz, and T. Arici. Gesture recognition using skeleton data with weighted dynamic time warping. Computer Vision Theory and Applications, 2013.  F.-S. Chen, C.-M. Fu, and C.-L. Huang. Hand gesture recognition using a real-time tracking method and hidden Markov models. Image and Vis. Computing, 21(8):745-758, 2003.  X. Chen and M. Koskela. Online rgb-d gesture recognition with extreme learning machines. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 467-474. ACM, 2013.  Y. L. Chow and R. Schwartz. The n-best algorithm: An e\ufb03cient procedure for finding top n sentence hypotheses. In Proc. of the Workshop on Speech and Natural Language, pages 199-202. Association for Computational Linguistics, 1989.  S. Conseil, S. Bourennane, and L. Martin. Comparison of Fourier descriptors and Hu moments for hand posture recognition. In Proc. European Conf. on Signal Processing, 2007.  Y. Cui and J. Weng. Appearance-based hand sign recognition from intensity image se-  quences. Comp. Vis. and Im. Undrst., 78(2):157-176, 2000.  N. Dalal and B. Triggs. Histogram of oriented gradients for human detection. In Proc. Int\u2019l  Conf. on Comp. Vis. & Patt. Rec., 2005.  W. Du and J. Piater. Hand modeling and tracking for video-based sign language recognition by robust principal component analysis. In Proc. ECCV Wksp on Sign, Gest. and Activity, September 2010. Pitsikalis, Katsamanis, Theodorakis and Maragos  S. Escalera, J. Gonz`alez, X. Bar\u00b4o, M. Reyes, I. Guyon, V. Athitsos, H. Escalante, L. Sigal, A. Argyros, C. Sminchisescu, R. Bowden, and S. Sclaro\ufb00. ChaLearn multi-modal gesture recognition 2013: grand challenge and workshop summary. In Proc. of the 15th ACM on Int\u2019l Conf. on Multimodal Interaction, pages 365-368. ACM, 2013a.  S. Escalera, J. Gonzlez, X. Bar, M. Reyes, O. Lopes, I. Guyon, V. Athitsos, and H.J. Escalante. Multi-modal Gesture Recognition Challenge 2013: Dataset and Results. In 15th ACM Int\u2019l Conf. on Multimodal Interaction (ICMI), ChaLearn Challenge and Wrksp on Multi-modal Gesture Recognition. ACM, 2013b.  J. Foote. An overview of audio information retrieval. Multimedia Systems, 7(1):2-10, 1999.  URL http://link.springer.com/article/10.1007/s005300050106.  L. Gillick and S.J. Cox. Some statistical issues in the comparison of speech recognition algorithms. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., pages 532-535 vol.1, may 1989.  H. Glotin, D. Vergyr, C. Neti, G. Potamianos, and J. Luettin. Weighting schemes for audio- visual fusion in speech recognition. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., volume 1, pages 173-176. IEEE, 2001.  B. Habets, S. Kita, Z. Shao, A. \u00a8Ozyurek, and P. Hagoort. The role of synchrony and ambiguity in speech-gesture integration during comprehension. Journal of Cognitive Neuroscience, 23(8):1845-1854, 2011.  J. Han, G. Awad, and A. Sutherland. Modelling and segmenting subunits for sign language  recognition based on hand motion analysis. Patt. Rec. Letters, 30:623-633, 2009.  H. Hermansky. Perceptual linear predictive (plp) analysis of speech. Journal of the Acous-  tical Society of America, 87(4):1738-1752, 1990.  A. Hern\u00b4andez-Vela, M. \u00b4A. Bautista, X. Perez-Sala, V. Ponce-L\u00b4opez, S. Escalera, X. Bar\u00b4o, O. Pujol, and C. Angulo. Probability-based dynamic time warping and bag-of-visual- and-depth-words for human gesture recognition in rgb-d. Patt. Rec. Letters, 2013.  C.-L. Huang and S.-H. Jeng. A model-based hand gesture recognition system. Machine  Vision and Application, 12(5):243-258, 2001.  M. Isard and A. Blake. Condensation-conditional density propagation for visual tracking.  Int\u2019l Journal of Computer Vision, 29(1):5-28, 1998.  M. Iverson, J. and S. Goldin-Meadow. Why people gesture when they speak. Nature, 396  (6708):228-228, 1998.  A. Jaimes and N. Sebe. Multimodal human-computer interaction: A survey. Comp. Vis.  and Im. Undrst., 108(1):116-134, 2007.  S. D. Kelly, A. \u00a8Ozy\u00a8urek, and E. Maris. Two sides of the same coin speech and gesture mutually interact to enhance comprehension. Psychological Science, 21(2):260-267, 2010. Multimodal Gesture Recognition via Multiple Hypotheses Rescoring  A. Kendon. Gesture: Visible Action as Utterance. Cambridge University Press, 2004.  W. Kong and S. Ranganath. Sign language phoneme transcription with rule-based hand  trajectory segmentation. J. Signal Proc. Sys., 59:211-222, 2010.  I. Laptev, M. Marszalek, and B. Schmid, C.and Rozenfeld. Learning realistic human actions from movies. In Proc. Int\u2019l Conf. on Comp. Vis. & Patt. Rec., pages 1-8. IEEE, 2008.  H-K. Lee and J-H. Kim. An HMM-based threshold model approach for gesture recognition.  IEEE Trans. on Patt. Anal. and Mach. Intell., 21(10):961-973, 1999.  J. Li and N. M. Allinson. Simultaneous gesture segmentation and recognition based on forward spotting accumulative hmms. Pattern Recognition, 40(11):3012-3026, 2007.  J. Li and N. M. Allinson. A comprehensive review of current local features for computer  vision. Neurocomputing, 71(10):1771-1787, 2008.  D. G. Lowe. Object recognition from local scale-invariant features. In Proc. Int\u2019l Conf. on  Comp. Vis., pages 1150-1157, 1999.  P. Maragos, P. Gros, A. Katsamanis, and Papandreou G. Cross-modal integration for performance improving in multimedia: A review. In P. Maragos, A. Potamianos, and P. Gros, editors, Multimodal Processing and Interaction: Audio, Video, Text, chapter 1, pages 3-48. Springer-Verlag, New York, 2008.  D. McNeill. Hand and Mind: What Gestures Reveal About Thought. University of Chicago  Press, 1992.  M. Miki, N. Kitaoka, C. Miyajima, T. Nishino, and K. Takeda. Improvement of multimodal gesture and speech recognition performance using time intervals between gestures and accompanying speech. EURASIP Journal on Audio, Speech, and Music Processing, 2014 (1):17, 2014. URL http://link.springer.com/article/10.1186/1687-4722-2014-2.  D. Morris, P. Collett, P. Marsh, and O\u2019Shaughnessy M. Gestures: Their Origins and  Distribution. Stein and Day, 1979.  Y. Nam and K. Wohn. Recognition of space-time hand-gestures using hidden Markov model. In ACM Symposium on Virtual Reality Software and Technology, pages 51-58, 1996.  K. Nandakumar, K. W. Wan, S. Chan, W. Ng, J. G. Wang, and W. Y. Yau. A multi-modal gesture recognition system using audio, video, and skeletal joint data. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 475-482. ACM, 2013.  N. Neverova, C. Wolf, G. Paci, G. Sommavilla, G. Taylor, and F. Nebout. A multi-scale In Proc. of the IEEE Int\u2019l Conf. on  approach to gesture detection and recognition. Computer Vision Wrksp, pages 484-491, 2013.  E.-J. Ong and R. Bowden. A boosted classifier tree for hand shape detection. In Proc. Int\u2019l  Conf. on Autom. Face & Gest. Rec., pages 889-894. IEEE, 2004. Pitsikalis, Katsamanis, Theodorakis and Maragos  M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. M. Schwartz, and J. R. Rohlicek. Inte- gration of Diverse Recognition Methodologies Through Reevaluation of N-Best Sentence Hypotheses. In HLT, 1991.  S. Oviatt and P. Cohen. Perceptual user interfaces: multimodal interfaces that process  what comes naturally. Communications of the ACM, 43(3):45-53, 2000.  G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 17(3):423-435, 2009.  V. Pitsikalis, S. Theodorakis, C. Vogler, and P. Maragos. Advances in phonetics-based sub-unit modeling for transcription alignment and sign language recognition. In IEEE CVPR Wksp on Gest. Rec., 2011.  I. Poddar, Y. Sethi, E. Ozyildiz, and R. Sharma. Toward natural gesture/speech HCI: A case study of weather narration. In Proc. Wrksp on Perceptual User Interfaces, 1998.  V. Ponce-L\u00b4opez, S. Escalera, and X. Bar\u00b4o. Multi-modal social signal analysis for predicting agreement in conversation settings. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 495-502. ACM, 2013.  G. Potamianos, C. Neti, J. Luettin, and I. Matthews. Audio-visual automatic speech recog- nition: An overview. Issues in Visual and Audio-Visual Speech Processing, 22:23, 2004.  L.R. Rabiner and B.H. Juang. Fundamentals of Speech Recognition. Prentice Hall, 1993.  Z. Ren, J. Yuan, and Z. Zhang. Robust hand gesture recognition based on finger-earth mover\u2019s distance with a commodity depth camera. In Proc. of the 19th ACM Int\u2019l Conf. on Multimedia, pages 1093-1096. ACM, 2011.  R. C. Rose. Discriminant wordspotting techniques for rejecting non-vocabulary utterances in unconstrained speech. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., volume 2, pages 105-108. IEEE, 1992. URL http://ieeexplore.ieee.org/xpls/abs_all.jsp? arnumber=226109.  R. C. Rose and D. B. Paul. A hidden Markov model based keyword recognition system. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., pages 129-132, 1990. URL http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=115555.  A. Roussos, S. Theodorakis, V. Pitsikalis, and P. Maragos. Dynamic a\ufb03ne-invariant shape- appearance handshape features and classification in sign language videos. Journal of Machine Learning Research, 14(1):1627-1663, 2013.  S. Ru\ufb03eux, D. Lalanne, and E. Mugellini. ChAirGest: A Challenge for Multimodal Mid-air Gesture Recognition for Close HCI. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, ICMI \u201913, pages 483-488, New York, NY, USA, 2013. ACM.  S. Ru\ufb03eux, D. Lalanne, E. Mugellini, and O. A. Khaled. A Survey of Datasets for Human Gesture Recognition. In Human-Computer Interaction. Advanced Interaction Modalities and Techniques, pages 337-348. Springer, 2014. Multimodal Gesture Recognition via Multiple Hypotheses Rescoring  R. Sharma, M. Yeasin, N. Krahnstoever, I. Rauschert, G. Cai, I. Brewer, A. M MacEachren, and K. Sengupta. Speech-gesture driven multimodal interfaces for crisis management. Proc. of the IEEE, 91(9):1327-1354, 2003.  S. Shimojo and L. Shams. Sensory modalities are not separate modalities: plasticity and  interactions. Current Opinion in Neurobiology, 11(4):505-509, 2001.  J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finocchio, A. Blake, M. Cook, and R. Moore. Real-time human pose recognition in parts from single depth images. Com- munications of the ACM, 56(1):116-124, 2013.  R. Shwartz and S. Austin. A comparison of several approximate algorithms for finding multiple N-Best sentence hypotheses. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., 1991.  Y. C. Song, H. Kautz, J. Allen, M. Swift, Y. Li, J. Luo, and C. Zhang. A Markov logic framework for recognizing complex events from multimodal data. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 141-148. ACM, 2013.  T. Starner, J. Weaver, and A. Pentland. Real-time american sign language recognition using desk and wearable computer based video. IEEE Trans. on Patt. Anal. and Mach. Intell., 20(12):1371-1375, Dec. 1998.  L. N. Tan, B. J. Borgstrom, and A. Alwan. Voice activity detection using harmonic fre- quency components in likelihood ratio test. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., pages 4466-4469. IEEE, 2010.  N. Tanibata, N. Shimada, and Y. Shirai. Extraction of hand features for recognition of sign  language words. In Proc. Int\u2019l Conf. on Vision Interface, pages 391-398, 2002.  S. Theodorakis, V. Pitsikalis, and P. Maragos. Dynamic-Static Unsupervised Sequential- ity, Statistical Subunits and Lexicon for Sign Language Recognition. Imave and Vision Computing, 32(8):533549, 2014.  M. Turk. Multimodal interaction: A review. Patt. Rec. Letters, 36:189-195, 2014.  C. Vogler and D. Metaxas. A framework for recognizing the simultaneous aspects of amer-  ican sign language. Comp. Vis. and Im. Undrst., 81:358, 2001.  S. B. Wang, A. Quattoni, L. Morency, D. Demirdjian, and T. Darrell. Hidden conditional random fields for gesture recognition. In Proc. Int\u2019l Conf. on Comp. Vis. & Patt. Rec., volume 2, pages 1521-1527. IEEE, 2006.  D. Weimer and S. Ganapathy. A synthetic visual environment with hand gesturing and  voice input. In ACM SIGCHI Bulletin, volume 20, pages 235-240. ACM, 1989.  L. D Wilcox and M. Bush. Training and search algorithms for an interactive wordspotting system. In Proc. Int\u2019l Conf. on Acoustics, Speech and Sig. Proc., volume 2, pages 97-100. IEEE, 1992. Pitsikalis, Katsamanis, Theodorakis and Maragos  J. Wilpon, L. R. Rabiner, C.-H. Lee, and E. R. Goldman. Automatic recognition of keywords in unconstrained speech using hidden Markov models. IEEE Trans. on Acoustics, Speech and Signal Processing, 38(11):1870-1878, 1990.  A. Wilson and A. Bobick. Parametric hidden Markov models for gesture recognition. IEEE  Trans. on Patt. Anal. and Mach. Intell., 21:884-900, 1999.  J. Wu, J. Cheng, C. Zhao, and H. Lu. Fusing multi-modal features for gesture recognition. In Proc. of the 15th ACM Int\u2019l Conf. on Multimodal Interaction, pages 453-460. ACM, 2013.  M.-H. Yang, N. Ahuja, and M. Tabb. Extraction of 2d motion trajectories and its application to hand gesture recognition. IEEE Trans. on Patt. Anal. and Mach. Intell., 24(8):1061- 1074, Aug. 2002.  S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. The HTK Book. Entropic Cambridge Research Labora- tory, Cambridge, United Kingdom, 2002. "}, "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm": {"volumn": 16, "url": "http://jmlr.org/papers/v16/liu15a.html", "header": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm", "author": "Ji Liu, Stephen J. Wright, Christopher R\u00c3\u00a9, Victor Bittorf, Srikrishna Sridhar", "time": "16(10):285\u2212322, 2015.", "abstract": "We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate ($1/K$) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is $O(n^{1/2})$ in unconstrained optimization and $O(n^{1/4})$ in the separable- constrained case, where $n$ is the number of variables. We describe results from implementation on 40-core processors.", "pdf_url": "http://jmlr.org/papers/volume16/liu15a/liu15a.pdf", "keywords": ["asynchronous parallel optimization", "stochastic coordinate descent"], "reference": "A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems 24, pages 873-881. 2011. URL http://papers. nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf.  H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable  convergence rate through randomization. IPDPS, 2014.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM J. Imaging Sciences, 2(1):183-202, 2009.  A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods.  SIAM Journal on Optimization, 23(4):2037-2060, 2013.  D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical  Methods. Pentice Hall, 1989.  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines, 2011. URL  http://www.csie.ntu.edu.tw/~cjlin/libsvm/.  C. Cortes and V. Vapnik. Support vector networks. Machine Learning, pages 273-297,  1995.  A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms In Advances in Neural Information Process- URL http://papers.nips.cc/paper/  via accelerated gradient methods. ing Systems 24, pages 1647-1655. 2011. 4432-better-mini-batch-algorithms-via-accelerated-gradient-methods.pdf.  J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57 (3):592-606, 2012.  M. C. Ferris and O. L. Mangasarian. Parallel variable distribution. SIAM Journal on  Optimization, 4(4):815-832, 1994.  320   Liu, Wright, R\u00b4e, Bittorf, and Sridhar  we have  (cid:19) \u221a  (cid:18)  \u2212  1 \u2212  nLmax 4Lres\u03c4 \u03c1\u03c4 = Lmax 2Lres\u03c4 \u03c1\u03c4 \u2265 1 \u2212 We can thus set \u03b3 = 1/2, and by substituting this choice into (21), we obtain (25). We obtain (26) by making the same substitution into (22).  1 2 \u221a \u03c1 n 4e\u03c4 Lres 4Lres\u03c4 \u03c1\u03c4 +1 \u2212  nLmax 4Lres\u03c4 \u03c1\u03c4  2 \u221a n 1 2  1 2  =  \u2212  =  .  (cid:18) \u03c1 \u2212 1 \u03c1  (cid:19) \u221a  References  A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems 24, pages 873-881. 2011. URL http://papers. nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf.  H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable  convergence rate through randomization. IPDPS, 2014.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM J. Imaging Sciences, 2(1):183-202, 2009.  A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods.  SIAM Journal on Optimization, 23(4):2037-2060, 2013.  D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical  Methods. Pentice Hall, 1989.  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines, 2011. URL  http://www.csie.ntu.edu.tw/~cjlin/libsvm/.  C. Cortes and V. Vapnik. Support vector networks. Machine Learning, pages 273-297,  1995.  A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms In Advances in Neural Information Process- URL http://papers.nips.cc/paper/  via accelerated gradient methods. ing Systems 24, pages 1647-1655. 2011. 4432-better-mini-batch-algorithms-via-accelerated-gradient-methods.pdf.  J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57 (3):592-606, 2012.  M. C. Ferris and O. L. Mangasarian. Parallel variable distribution. SIAM Journal on  Optimization, 4(4):815-832, 1994. AsySCD  D. Goldfarb and S. Ma. Fast multiple-splitting algorithms for convex optimization. SIAM  Journal on Optimization, 22(2):533-556, 2012.  Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent  methods. Technical Report arXiv:1305.4723, Simon Fraser University, 2013.  Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex di\ufb00erentiable minimization. Journal of Optimization Theory and Applications, 72:7-35, 1992.  O. L. Mangasarian. Parallel gradient distribution in unconstrained optimization. SIAM  Journal on Optimization, 33(1):916-1925, 1995.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19:1574-1609, 2009.  Y. Nesterov.  Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  Academic Publishers, 2004.  Y. Nesterov. E\ufb03ciency of coordinate descent methods on huge-scale optimization problems.  SIAM Journal on Optimization, 22(2):341-362, 2012.  F. Niu, B. Recht, C. R\u00b4e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems 24, pages 693-701, 2011.  Z. Peng, M. Yan, and W. Yin. Parallel and distributed sparse optimization. Preprint, 2013.  P. Richt\u00b4arik and M. Tak\u00b4a\u02c7c. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematrical Programming, 144:1-38, 2012a.  P. Richt\u00b4arik and M. Tak\u00b4a\u02c7c. Parallel coordinate descent methods for big data optimization.  Technical Report arXiv:1212.0873, 2012b.  C. Scherrer, A. Tewari, M. Halappanavar, and D. Haglin. Feature clustering for accelerating parallel coordinate descent. Advances in Neural Information Processing Systems 25, pages 28-36, 2012.  S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent.  Advances in Neural Information Processing Systems 26, pages 378-385, 2013.  O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Con- vergence results and optimal averaging schemes. In Proceedings of the 30th International Conference on Machine Learning, 2013.  R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal  Statistical Society, Series B, 58:267-288, 1996. Liu, Wright, R\u00b4e, Bittorf, and Sridhar  P. Tseng. Convergence of a block coordinate descent method for nondi\ufb00erentiable mini-  mization. Journal of Optimization Theory and Applications, 109:475-494, 2001.  P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable  minimization. Mathematical Programming, Series B, 117:387-423, June 2009.  P. Tseng and S. Yun. A coordinate gradient descent method for linearly constrained smooth optimization and support vector machines training. Computational Optimization and Applications, 47(2):179-206, 2010.  P.-W. Wang and C.-J. Lin.  Iteration complexity of feasible descent methods for convex  optimization. Journal of Machine Learning Research, 15:1523-1548, 2014.  S. J. Wright. Accelerated block-coordinate relaxation for regularized optimization. SIAM  Journal on Optimization, 22(1):159-186, 2012.  T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. Advances in Neural Information Processing Systems 26, pages 629-637, 2013. "}, "Composite Self-Concordant Minimization": {"volumn": 16, "url": "http://jmlr.org/papers/v16/trandihn15a.html", "header": "Composite Self-Concordant Minimization", "author": "Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher", "time": "16(12):371\u2212416, 2015.", "abstract": "We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data.", "pdf_url": "http://jmlr.org/papers/volume16/trandihn15a/trandihn15a.pdf", "keywords": ["proximal-gradient/Newton method", "composite minimization", "self-concordance", "sparse convex optimization", "graph learning"], "reference": "K. Banaszek, G. M. D\u2019Ariano, M. G. A. Paris, and M. F. Sacchi. Maximum-likelihood  estimation of the density matrix. Phys. Rev. A., 61(010304):1-4, 1999.  O. Banerjee, L. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485-516, 2008.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM J. Imaging Sciences, 2(1):183-202, 2009a.  A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. IEEE Trans Image Process., 18(11):2419-2434, 2009b.  S. Becker and M.J. Fadili. A quasi-Newton proximal splitting method. In Proceedings of  Neutral Information Processing Systems Foundation, 2012.  S. Becker, E. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical Programming Computation, 3:165-218, 2011. ISSN 1867-2949.  A. Ben-Tal and A.K. Nemirovski. Lectures on Modern Convex Optimization: Analysis,  Algorithms, and Engineering Applications. SIAM, 2001.  412   Tran-Dinh, Kyrillidis, and Cevher  \u03b1k) + \u03b1k 1\u2212\u03b3  \u2212  (cid:18)  \u03b3 + (cid:107) 1\u2212 (cid:107)  xk\u2212x\u2217  xk\u2212x\u2217  (cid:107)x\u2217 (cid:107)x\u2217  (cid:19)  . Then, for \u03b3 < 1/2, \u02dc\u03c8k < 1 if  (cid:13) x\u2217(cid:13) (cid:13)x\u2217 < 1\u22122\u03b3 (cid:13)x0 2(1\u2212\u03b3) . Therefore, by induction, if we choose 2(1\u2212\u03b3) , then \u2212 (cid:13) (cid:13) (cid:13)x\u2217 < 1\u22122\u03b3 \u02dc\u03c8k (cid:13)xk+1 (cid:13)xk 2(1\u2212\u03b3) for all k 0. Moreover, 0 (cid:13)x\u2217 \u2264 [0, 1). This implies that (cid:8)(cid:13) x\u2217(cid:13) (cid:9) (cid:13)xk k\u22650 linearly converges to zero with the (cid:13)x\u2217  (cid:13)x\u2217 < 1\u22122\u03b3 x\u2217(cid:13)  (cid:13)x\u2217 for k  x\u2217(cid:13)  \u2265  \u2265  \u2212  \u2212  \u2212  \u2212  x\u2217(cid:13) x\u2217(cid:13)  Let us define \u02dc\u03c8k := (1 (cid:13) (cid:13)xk (cid:13) (cid:13)xk \u2212 and \u02dc\u03c8k \u2208 factor \u02dc\u03c8k.  Finally, we assume that Dk := LkI, the quantity in (72) satisfies  N\u2217 :=  \u2207  2f (x\u2217)\u22121/2 (cid:0)  2f (x\u2217)  \u2207  (cid:1)  Hk  \u2212  \u2207  2f (x\u2217)\u22121/2 = I  Lk  \u2212  \u2207  2f (x\u2217)\u22121.  Then, we can easily observe that:  N\u2217 (cid:107)  (cid:107)2 =  (cid:13) (cid:13)I  Lk  \u2212  \u2207  2f (x\u2217)\u22121(cid:13)  max  (cid:13)2 \u2264  (cid:110)(cid:12) (cid:12) (cid:12)1  \u2212  Lk \u03c3\u2217  min  (cid:12) (cid:12) (cid:12),  (cid:12) (cid:12) (cid:12)1  \u2212  Lk \u03c3\u2217  max  (cid:12) (cid:111) (cid:12) (cid:12)  := \u03b3\u2217,  (91)  where \u03c3\u2217 Using the estimate (91), we can derive  min (respectively, \u03c3\u2217  max) is the smallest (respectively, largest) eigenvalue of  2f (x\u2217).  \u2207  (cid:13) (cid:13)  (cid:0)Dk  \u2212 \u2207  2f (x\u2217)(cid:1) dk g  (cid:13) \u2217 (cid:13) x\u2217  (91)  N\u2217  sk  2 (cid:107)  (cid:107)  \u2212  xk  x\u2217  (cid:107)  \u2264  \u03b3\u2217  dk g(cid:107) (cid:107)  x\u2217,  \u2264 (cid:107)  which proves the last conclusion of Theorem 14.  (cid:4)  References  K. Banaszek, G. M. D\u2019Ariano, M. G. A. Paris, and M. F. Sacchi. Maximum-likelihood  estimation of the density matrix. Phys. Rev. A., 61(010304):1-4, 1999.  O. Banerjee, L. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485-516, 2008.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM J. Imaging Sciences, 2(1):183-202, 2009a.  A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. IEEE Trans Image Process., 18(11):2419-2434, 2009b.  S. Becker and M.J. Fadili. A quasi-Newton proximal splitting method. In Proceedings of  Neutral Information Processing Systems Foundation, 2012.  S. Becker, E. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical Programming Computation, 3:165-218, 2011. ISSN 1867-2949.  A. Ben-Tal and A.K. Nemirovski. Lectures on Modern Convex Optimization: Analysis,  Algorithms, and Engineering Applications. SIAM, 2001. Composite Self-concordant Minimization  D.P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Meth-  ods. Prentice Hall, 1989.  J.F. Bonnans. Local analysis of Newton-type methods for variational inequalities and non-  linear programming. Appl. Math. Optim, 29:161-186, 1994.  S. Boyd and L. Vandenberghe. Convex Optimization. University Press, Cambridge, 2004.  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  L.M. Briceno-Arias and P.L. Combettes. A monotone + skew splitting model for composite  monotone inclusions in duality. SIAM J. Optim., 21(4):1230-1250, 2011.  R. H. Byrd and J. Nocedal. A tool for the analysis of quasi-Newton methods with application  to unconstrained minimization. SIAM J. Numer. Anal., 26(3):727-739, 1989.  E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger  than n. Annals of Statistics, 35(6):2313-2351, 2007.  A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120-145, 2011.  E. Chouzenoux, J.-C. Pesquet, and A. Repetti. Variable metric forward-backward algorithm for minimizing the sum of a di\ufb00erentiable function and a convex function. J. Optim. Theory Appl., DOI 10.1007/s10957-013-0465-7:1-22, 2013.  P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting.  Multiscale Model. Simul., 4:1168-1200, 2005.  A. S. Dalalyan, M. Hebiri, K. Meziani, and J. Salmon. Learning heteroscedastic models by convex programming under group sparsity. Proc. of the International conference on Machine Learning, pages 1-8, 2013.  A. P. Dempster. Covariance selection. Biometrics, 28:157-175, 1972.  J.E. Dennis and J. J. Mor\u00b4e. A characterisation of superlinear convergence and its application  to quasi-Newton methods. Mathemathics of Computation, 28:549-560, 1974.  J. Eckstein and D. Bertsekas. On the Douglas - Rachford splitting method and the proximal point algorithm for maximal monotone operators. Math. Program., 55:293-318, 1992.  F. Facchinei and J.-S. Pang. Finite-Dimensional Variational Inequalities and Complemen-  tarity Problems, volume 1-2. Springer-Verlag, 2003.  D. Goldfarb and S. Ma. Fast alternating linearization methods of minimization of the sum  of two convex functions. Math. Program., Ser. A, pages 1-34, 2012. Tran-Dinh, Kyrillidis, and Cevher  T. Goldstein and S. Osher. The split Bregman method for (cid:96)1-pegularized problems. SIAM  J. Imaging Sciences, 2(2):323-343, 2009.  T. Goldstein, B. ODonoghue, and S. Setzer. Fast alternating direction optimization meth- ods. Tech. report., Department of Mathematics, University of California, Los Angeles, USA, May 2012.  M. Grant, S. Boyd, and Y. Ye. Disciplined convex programming. In L. Liberti and N. Mac- ulan, editors, Global Optimization: From Theory to Implementation, Nonconvex Opti- mization and its Applications, pages 155-210. Springer, 2006.  Z.T. Harmany, R.F. Marcia, and R. M. Willett. This is SPIRAL-TAP: Sparse poisson intensity reconstruction algorithms - theory and practice,. IEEE Transactions on Image Processing, 21(3):1084-1096, 2012.  Jean-Baptiste Hiriart-Urruty and Claude Lemar\u00b4echal. Fundamentals of Convex Analysis.  Springer, 2001.  C. J. Hsieh, M.A. Sustik, I.S. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. Advances in Neutral Information Processing Systems (NIPS), 24:1-18, 2011.  J. Kim and H. Park. Fast active-set-type algorithms for (cid:96)1-regularized linear regression. In Proceedings of the 13th International Conference on Artificial Intelligience and Statistics (AISTATS), volume 9, pages 397-404, Sardinia, Italy, 2010.  A. Kyrillidis and V. Cevher. Fast proximal algorithms for self-concordant function min- imization with application to sparse graph selection. Proc. of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 1-5, 2013.  A. Kyrillidis, R. Karimi Mahabadi, Q. Tran-Dinh, and V. Cevher. Scalable sparse covari- ance estimation via self-concordance. In Proc. of the 28th International Conference on Artificial Intelligence (AAAI-14), pages 1-9. 2014.  J.D. Lee, Y. Sun, and M.A. Saunders. Proximal Newton-type methods for convex optimiza- tion. Advances in Neural Information Processing Systems (NIPS), 25:827-835, 2012.  J. L\u00a8ofberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In Proceedings  of the CACSD Conference, Taipei, Taiwan, 2004.  Z. Lu. Adaptive first-order methods for general sparse inverse covariance selection. SIAM  Journal on Matrix Analysis and Applications, 31(4):2000-2016, 2010.  H. Mine and M. Fukushima. A minimization method for the sum of a convex function and  a continuously di\ufb00erentiable function. J. Optim. Theory Appl., 33:9-23, 1981.  A.S. Nemirovskii and M.J. Todd. Interior-point methods for optimization. Acta Numerica,  pages 191-234, 2008.  Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87  of Applied Optimization. Kluwer Academic Publishers, 2004. Composite Self-concordant Minimization  Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127-  Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM J. Opti-  152, 2005a.  mization, 16(1):235-249, 2005b.  gram., 140(1):125-161, 2007.  Y. Nesterov. Gradient methods for minimizing composite objective function. Math. Pro-  Y. Nesterov. Barrier subgradient method. Math. Program., Ser. B, 127:31-56, 2011.  Y. Nesterov and A. Nemirovski. Interior-point Polynomial Algorithms in Convex Program-  ming. Society for Industrial Mathematics, 1994.  Y. Nesterov and M.J. Todd. Self-scaled barriers and interior-point methods for convex  programming. Math. Oper. Research, 22(1):1-42, 1997.  J. Nocedal and S.J. Wright. Numerical Optimization. Springer Series in Operations Research  and Financial Engineering. Springer, 2 edition, 2006.  P.A. Olsen, F. Oztoprak, J. Nocedal, and S.J. Rennie. Newton-like methods for sparse in- verse covariance estimation. Advances in Neural Information Processing Systems (NIPS), pages 1-9, 2012.  P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing (cid:96)1-penalized log-determinant divergence. Electron. J. Statist., 5:935-988, 2011.  S. M. Robinson. Strongly Regular Generalized Equations. Mathematics of Operations  Research, Vol. 5, No. 1 (Feb., 1980), pp. 43-62, 5:43-62, 1980.  R. T. Rockafellar. Convex Analysis, volume 28 of Princeton Mathematics Series. Princeton  University Press, 1970.  Opt., 14:877-898, 1976.  R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM J. Control  B. Rolfs, B. Rajaratnam, D. Guillot, I. Wong, and A. Maleki. Iterative thresholding al- In Advances in Neural Information  gorithm for sparse inverse covariance estimation. Processing Systems 25, pages 1583-1591, 2012.  K. Scheinberg and I. Rish. SINCO-a greedy coordinate ascent method for sparse inverse  covariance selection problem. Tech. Report, IBM RC24837:1-21, 2009.  K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse covariance selection via alternating linearization methods. Neural Information Processing Systems (NIPS), pages 1-9, 2010.  M. Schmidt, N.L. Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. Neural Information Processing Systems (NIPS), 2011. Tran-Dinh, Kyrillidis, and Cevher  N. St\u00a8adler, P. B\u00a8ulmann, and S. Van de Geer. (cid:96)1-Penalization for Mixture Regression Models.  Tech. Report., pages 1-35, 2012.  Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. A proximal newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions. International Conference on Machine Learning (ICML), 28(2):271-279, 2013a.  Q. Tran-Dinh, I. Necoara, C. Savorgnan, and M. Diehl. An inexact perturbed path-following method for Lagrangian decomposition in large-scale separable convex optimization. SIAM J. Optim., 23(1):95-125, 2013b.  Q. Tran-Dinh, C. Savorgnan, and M. Diehl. Combining Lagrangian decomposition and excessive gap smoothing technique for solving large-scale separable convex optimization problems. Compt. Optim. Appl., 55(1):75-111, 2013c.  Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. An inexact proximal path-following algorithm  for constrained convex minimization. SIAM J. Optimization (accepted), 2014a.  Q. Tran-Dinh, Y. H. Li, and V. Cevher. Barrier smoothing for nonsmooth convex mini- mization. In Proc. of the 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 1-4, 2014b.  E. van den Berg and M. P. Friedlander. Probing the pareto frontier for basis pursuit  solutions. SIAM J. Sci. Comput., 31(2):890-912, 2008.  S. J. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approxi-  mation. IEEE Trans. Signal Processing, 57:2479-2493, 2009.  X. Yuan. Alternating direction method for covariance selection models. Journal of Scientific  Computing, 51(2):261-273, 2012.  W.I. Zangwill. Nonlinear Programming. Prentice Hall, 1969. "}, "Network Granger Causality with Inherent Grouping Structure": {"volumn": 16, "url": "http://jmlr.org/papers/v16/basu15a.html", "header": "Network Granger Causality with Inherent Grouping Structure", "author": "Sumanta Basu, Ali Shojaie, George Michailidis", "time": "16(13):417\u2212453, 2015.", "abstract": "The problem of estimating high-dimensional network models arises naturally in the analysis of many biological and socio-economic systems. In this work, we aim to learn a network structure from temporal panel data, employing the framework of Granger causal models under the assumptions of sparsity of its edges and inherent grouping structure among its nodes. To that end, we introduce a group lasso regression regularization framework, and also examine a thresholded variant to address the issue of group misspecification. Further, the norm consistency and variable selection consistency of the estimates are established, the latter under the novel concept of direction consistency. The performance of the proposed methodology is assessed through an extensive set of simulation studies and comparisons with existing techniques. The study is illustrated on two motivating examples coming from functional genomics and financial econometrics.", "pdf_url": "http://jmlr.org/papers/volume16/basu15a/basu15a.pdf", "keywords": ["Granger causality", "high dimensional networks", "panel vector autoregression model", "group lasso", "thresholding"], "reference": "F. R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn.  Res., 9:1179-1225, 2008. ISSN 1532-4435.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig  selector. The Annals of Statistics, 37(4):1705-1732, 2009.  M. Binder, C. Hsiao, and M.H. Pesaran. Estimation and inference in short panel vector autoregressions with unit roots and cointegration. Econometric Theory, 21:795-837, 2005. ISSN 1469-4360. doi: 10.1017/S0266466605050413.  O. Blanchard and R. Perotti. An empirical characterization of the dynamic e\ufb00ects of changes in government spending and taxes on output. The Quarterly Journal of Economics, 117 (4):1329-1368, 2002.  P. Breheny and J. Huang. Penalized methods for bi-level variable selection. Stat. Interface,  2(3):369-380, 2009. ISSN 1938-7989.  B. Cao and Y. Sun. Asymptotic distributions of impulse response functions in short panel vector autoregressions. Journal of Econometrics, 163(2):127 - 143, 2011. ISSN 0304-4076.  N. Friedman. Inferring cellular networks using probabilistic graphical models. Science\u2019s  STKE, 303(5659):799, 2004.  451   NGC with Inherent Grouping  Appendix E. Thresholding Group Lasso Estimates.  Proof [Proof of Theorem 4.2] We use the notations developed in the proof of Proposi- tion C.1. First note that, (ii) follows directly from Theorem 4.1. For (i), since the falsely selected groups are present after the initial thresholding, we get (cid:107) \u02c6\u03b2[g](cid:107) > 4\u03bb for every such group. Next, we obtain an upper bound for the number of such groups. Specifically, denoting \u2206 = \u02c6\u03b2 \u2212 \u03b20, we get  (cid:12) \u02c6S\\S (cid:12) (cid:12)  (cid:12) (cid:12) (cid:12) \u2264  (cid:107) \u02c6\u03b2Sc(cid:107)2,1 4\u03bb  =  (cid:80)  g /\u2208S (cid:107)\u2206[g](cid:107) 4\u03bb  .  (54)  Next, note that from the sparsity oracle inequality (37), the following holds on the event  A,  It readily follows that  (cid:88)  g /\u2208S  (cid:107)\u2206[g](cid:107) \u2264 3  (cid:107)\u2206[g](cid:107).  (cid:88)  g\u2208S  (cid:88)g /\u2208S  (cid:107)\u2206[g](cid:107) \u2264 3(cid:107)\u2206(cid:107)2,1 \u2264  48 \u03c62 s\u03bb,  where the last inequality follows from the (cid:96)2,1-error bound of (34). Using this inequality together with (54) gives the result.  References  F. R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn.  Res., 9:1179-1225, 2008. ISSN 1532-4435.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig  selector. The Annals of Statistics, 37(4):1705-1732, 2009.  M. Binder, C. Hsiao, and M.H. Pesaran. Estimation and inference in short panel vector autoregressions with unit roots and cointegration. Econometric Theory, 21:795-837, 2005. ISSN 1469-4360. doi: 10.1017/S0266466605050413.  O. Blanchard and R. Perotti. An empirical characterization of the dynamic e\ufb00ects of changes in government spending and taxes on output. The Quarterly Journal of Economics, 117 (4):1329-1368, 2002.  P. Breheny and J. Huang. Penalized methods for bi-level variable selection. Stat. Interface,  2(3):369-380, 2009. ISSN 1938-7989.  B. Cao and Y. Sun. Asymptotic distributions of impulse response functions in short panel vector autoregressions. Journal of Econometrics, 163(2):127 - 143, 2011. ISSN 0304-4076.  N. Friedman. Inferring cellular networks using probabilistic graphical models. Science\u2019s  STKE, 303(5659):799, 2004. Basu, Shojaie and Michailidis  A. Fujita, J. Sato, H. Garay-Malpartida, R. Yamaguchi, S. Miyano, M. Sogayar, and C. Fer- reira. Modeling gene expression regulatory networks with the sparse vector autoregressive model. BMC Systems Biology, 1(1):39, 2007. ISSN 1752-0509.  G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, Baltimore, MD, third edition, 1996. ISBN 0-8018-5413-X; 0-8018-5414-8.  C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral  methods. Econometrica, 37(3):424-438, 1969.  C. Hiemstra and J. D. Jones. Testing for linear and nonlinear granger causality in the stock  price-volume relation. Journal of Finance, pages 1639-1664, 1994.  J. Huang and T. Zhang. The benefit of group sparsity. Ann. Statist., 38(4):1978-2004,  2010. ISSN 0090-5364.  J. Huang, S. Ma, H. Xie, and C-H. Zhang. A group bridge approach for variable selection.  Biometrika, 96(2):339-355, 2009. ISSN 0006-3444.  K. Kim, J.H. Kim, J. Lee, H.M. Jin, S.H. Lee, D.E. Fisher, H. Kook, K.K. Kim, Y. Choi, and N. Kim. Nuclear factor of activated t cells c1 induces osteoclast-associated receptor gene expression during tumor necrosis factor-related activation-induced cytokine-mediated os- teoclastogenesis. Journal of Biological Chemistry, 280(42):35209-35216, 2005.  M. Ledoux and M. Talagrand. Probability in Banach Spaces, volume 23 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Springer-Verlag, Berlin, 1991. ISBN 3-540-52013-9. Isoperimetry and processes.  K. Lounici, M. Pontil, S. van de Geer, and A. B. Tsybakov. Oracle inequalities and optimal  inference under group sparsity. Ann. Statist., 39(4):2164-2204, 2011.  A. Lozano, N. Abe, Y. Liu, and S. Rosset. Grouped graphical granger modeling for gene  expression regulatory networks discovery. Bioinformatics, 25(12):i110, 2009.  H. L\u00a8utkepohl. New Introduction to Multiple Time Series Analysis. Springer, 2005.  G. Michailidis. Statistical challenges in biological networks. Journal of Computational and  Graphical Statistics, 21(4):840-855, 2012. doi: 10.1080/10618600.2012.738614.  S. V. Parter. Extreme eigenvalues of Toeplitz forms and applications to elliptic di\ufb00erence  equations. Trans. Amer. Math. Soc., 99:153-192, 1961. ISSN 0002-9947.  J. Pearl. Causality: Models, Reasoning, and Inference, volume 47. Cambridge, 2000.  M. B. Priestley. Spectral Analysis and Time Series. Vol. 2. Academic Press Inc. [Harcourt Brace Jovanovich Publishers], London, 1981. ISBN 0-12-564902-9. Multivariate series, prediction and control, Probability and Mathematical Statistics. NGC with Inherent Grouping  C. Rangel, J. Angus, Z. Ghahramani, M. Lioumi, E. Sotheran, A. Gaiba, D.L. Wild, and F. Falciani. Modeling t-cell activation using gene expression profiling and state-space models. Bioinformatics, 20(9):1361, 2004.  M. Rudelson and R. Vershynin. Hanson-wright inequality and sub-gaussian concentration. Electron. Commun. Probab., 18:no. 82, 1-9, 2013. ISSN 1083-589X. doi: 10.1214/ECP. v18-2865.  A. Shojaie and G. Michailidis. Penalized likelihood methods for estimation of sparse high  dimensional directed acyclic graphs. Biometrika, 97(3):519-538, 2010a.  A. Shojaie and G. Michailidis. Discovering graphical granger causality using a truncating  lasso penalty. Bioinformatics, 26(18):i517-i523, 2010b.  C.A. Sims. Money, income, and causality. The American Economic Review, 62(4):540-552,  1972.  S. van de Geer and P. B\u00a8uhlmann. On the conditions used to prove oracle results for the  Lasso. Electron. J. Stat., 3:1360-1392, 2009. ISSN 1935-7524.  S. van de Geer, P. B\u00a8uhlmann, and S. Zhou. The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the Lasso). Electron. J. Stat., 5: 688-749, 2011. ISSN 1935-7524.  R. Vershynin. Lectures in Geometric Functional Analysis.  available at http://www-  personal.umich.edu/ romanv/papers/GFA-book/GFA-book.pdf, 2009.  M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery us- ing (cid:96)1-constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5):2183 -2202, May 2009. ISSN 0018-9448.  M. L. Waterman, K. A. Jones, et al. Purification of tcf-1 alpha, a t-cell-specific transcription factor that activates the t-cell receptor c alpha gene enhancer in a context-dependent manner. The New Biologist, 2(7):621, 1990.  F. Wei and J. Huang. Consistent group selection in high-dimensional linear regression.  Bernoulli, 16(4):1369-1384, 2010. ISSN 1350-7265.  P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:  2541-2563, December 2006. ISSN 1532-4435.  S. Zhou. Thresholded lasso for high dimensional variable selection and statistical estimation.  Arxiv preprint arXiv:1002.1583, 2010. "}, "Iterative and Active Graph Clustering Using Trace Norm Minimization Without Cluster Size Constraints": {"volumn": 16, "url": "http://jmlr.org/papers/v16/ailon15a.html", "header": "Iterative and Active Graph Clustering Using Trace Norm Minimization Without Cluster Size Constraints", "author": "Nir Ailon, Yudong Chen, Huan Xu", "time": "16(14):455\u2212490, 2015.", "abstract": "This paper investigates graph clustering under the planted partition model in the presence of  small clusters . Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large---in particular, the cluster sizes need to be $\\tilde{\\Omega}(\\sqrt{n})$, where $n$ is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover  almost all clusters  via a \u00c3\u00a2\u00c2\u0080\u00c2\u009cpeeling strategy\u00c3\u00a2\u00c2\u0080\u00c2\u009d: we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an  active  learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling---that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one---is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper.", "pdf_url": "http://jmlr.org/papers/volume16/ailon15a/ailon15a.pdf", "keywords": ["tion", "planted partition model", "stochastic block model"], "reference": "Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxima-  tions. Journal of the ACM, 54(2):9, 2007.  Nir Ailon, Yudong Chen, and Huan Xu. Breaking the small cluster barrier of graph clus- tering. In Proceedings of International Conference on Machine Learning (ICML), pages 995-1003, 2013.  Nir Ailon, Ron Begleiter, and Esther Ezra. Active learning using smooth relative regret approximations with applications. Journal of Machine Learning Research, 15:885-920, 2014.  Brendan P. W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted  clique and biclique problems. Mathematical Programming, 129(1):69-89, 2011.  Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral approach to learning mixed membership community models. Journal of Machine Learning Research, 15:2239-2312, June 2014.  487   Iterative and Active Clustering Without Size Constraints  We turn to part 2 of the lemma. Let A(cid:48) be an independent copy of A, and define  \u00afA :=  (cid:20) 0 A A(cid:48) 0  (cid:21)  .  Note that \u00afA is an 2N \u00d7 2N random matrix with i.i.d. entries. If \u03c3 \u2265 c1 for some su\ufb03ciently large absolute constant c1 > 0, then by Theorem 3.1 in Achlioptas and Mcsherry (2007) we know that with probability at least 1 \u2212 n\u22126, \u03bb1( \u00afA) \u2264 10\u03c3 N . The lemma follows from noting that \u03bb1(A) \u2264 \u03bb1( \u00afA).  \u221a  B log2 n\u221a N  A.2 Standard Bernstein Inequality for the Sum of Independent Variables  Lemma 18 ( Bernstein inequality) Let Y1, . . . , YN be independent random variables, each of which has variance bounded by \u03c32 and is bounded in absolute value by B a.s.. Then we have that  Pr  (cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  N (cid:88)  i=1  Yi \u2212 E  Yi  > t  \u2264 2 exp  (cid:35)  (cid:34) N (cid:88)  i=1  (cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  (cid:26)  t2/2 N \u03c32 + Bt/3  (cid:27)  .  The following lemma is an immediate consequence of Lemma 18.  Lemma 19 Let Y1, . . . , YN be independent random variables, each of which has variance bounded by \u03c32 and is bounded in absolute value by B a.s. Then we have  (cid:12) N (cid:12) (cid:88) (cid:12) (cid:12) (cid:12)  i=1  (cid:34) N (cid:88)  i=1  (cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  Yi \u2212 E  Yi  \u2264 10  (cid:16)  \u03c3(cid:112)N log n + B log n  (cid:17)  with probability at least 1 \u2212 2n\u22128.  References  Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxima-  tions. Journal of the ACM, 54(2):9, 2007.  Nir Ailon, Yudong Chen, and Huan Xu. Breaking the small cluster barrier of graph clus- tering. In Proceedings of International Conference on Machine Learning (ICML), pages 995-1003, 2013.  Nir Ailon, Ron Begleiter, and Esther Ezra. Active learning using smooth relative regret approximations with applications. Journal of Machine Learning Research, 15:885-920, 2014.  Brendan P. W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted  clique and biclique problems. Mathematical Programming, 129(1):69-89, 2011.  Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral approach to learning mixed membership community models. Journal of Machine Learning Research, 15:2239-2312, June 2014. Ailon, Chen and Xu  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning,  56:89-113, 2004.  B\u00b4ela Bollob\u00b4as and Alex D. Scott. Max cut for random graphs with a planted partition.  Combinatorics, Probability and Computing, 13(4-5):451-474, 2004.  Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis. In Proceedings of Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 280- 285, 1987.  Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component  analysis? Journal of the ACM, 58:1-37, 2011.  Ted Carson and Russell Impagliazzo. Hill-climbing finds random planted bisections.  In Proceedings of the 12th Annual Symposium on Discrete Algorithms, pages 903-909, 2001.  Venkat Chandrasekaran, Sujay Sanghavi, Pablo Parrilo, and Alan Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572-596, 2011.  Kamalika Chaudhuri, Fan Chung, and Alexander Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), pages 35.1-35.23, 2012.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in Neural Information Processing Systems 25, pages 2204-2212. Curran Associates, Inc., 2012.  Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex optimization. Journal of Machine Learning Research, 15:2213-2238, June 2014a.  Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transac-  tions on Information Theory, 60(10):6440-6455, 2014b.  Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted  partition model. Random Structures and Algorithms, 18(2):116-140, 2001.  Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active clustering: ro- bust and e\ufb03cient hierarchical clustering using adaptively selected similarities. In Proceed- ings of International Conference on Artificial Intelligence and Statistics, pages 260-268, 2011.  Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. A database interface for clustering in In Proceedings of 1st International Conference on Knowledge  large spatial databases. Discovery and Data Mining (KDD), 1995.  Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a fixed number of  clusters. Theory of Computing, 2(1):249-266, 2006. Iterative and Active Clustering Without Size Constraints  Paul W. Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: some  first steps. Social networks, 5(2):109-137, 1983.  Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex optimization. In Proceedigns of the 28th International Conference on Machine Learning, pages 1001-1008, 2011.  Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adap- tive sampling. In Advances in Neural Information Processing Systems 26, pages 836-844, 2013.  Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, and Aarti Singh. E\ufb03cient active algorithms for hierarchical clustering. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 887-894, 2012.  Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In Proceedings of 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 299-308, 2010.  Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized alternating direction method with adaptive penalty for low-rank representation. In Advances in Neural Information Pro- cessing Systems 24, pages 612-620, 2011.  Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 712- 728. SIAM, 2010.  Frank McSherry. Spectral partitioning of random graphs. In Proceedings of 42nd IEEE  Symposium on Foundations of Computer Science, pages 529-537, 2001.  Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert E. Tarjan. Clustering social networks. In Algorithms and Models for the Web-Graph, pages 56-67. Springer, 2007.  Samet Oymak and Babak Hassibi. Finding dense clusters via low rank + sparse decompo-  sition. arXiv:1104.5186v1, 2011.  Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional  stochastic block model. Annals of Statistics, 39:1878-1915, 2011.  Ohad Shamir and Naftali Tishby. Spectral clustering on a budget. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 661-669, 2011.  Ron Shamir and Dekel Tsur. Improved algorithms for the random cluster graph model.  Random Structure and Algorithm, 31(4):418-449, 2007.  Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of  Computational Mathematics, 12(4):389-434, 2012.  Konstantin Voevodski, Maria-Florina Balcan, Heiko R\u00a8oglin, Shang-Hua Teng, and Yu Xia. Active clustering of biological sequences. Journal of Machine Learning Research, 13: 203-225, 2012. Ailon, Chen and Xu  Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit.  IEEE Transactions on Information Theory, 58(5):3047-3064, 2012.  Yahoo!-Inc. Graph partitioning. http://research.yahoo.com/project/2368, 2009.  Yunpeng Zhao, Elizaveta Levina, and Ji Zhu. Community extraction for social networks.  Proceedings of the National Academy of Sciences, 108(18):7321-7326, 2011. "}, "AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models": {"volumn": 16, "url": "http://jmlr.org/papers/v16/martins15a.html", "header": "AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models", "author": "Andr\u00c3\u00a9 F. T. Martins, M\u00c3\u00a1rio A. T. Figueiredo, Pedro M. Q. Aguiar, Noah A. Smith, Eric P. Xing", "time": "16(16):495\u2212545, 2015.", "abstract": "We present AD$^3$, a new algorithm for approximate  maximum a posteriori  (MAP) inference on factor graphs, based on the alternating directions method of multipliers. Like other dual decomposition algorithms, AD$^3$ has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update. The key characteristic of AD$^3$ is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice. We provide closed-form solutions for these AD$^3$ subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD$^3$ applicable to a wide range of problems. Experiments on synthetic and real-world problems show that AD$^3$ compares favorably with the state-of-the-art.", "pdf_url": "http://jmlr.org/papers/volume16/martins15a/martins15a.pdf", "keywords": ["MAP inference", "graphical models", "dual decomposition", "alternating directions method of multipliers."], "reference": "M. B. Almeida and A. F. T. Martins. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proc. of the Annual Meeting of the Association for Computational Linguistics, 2013.  S. Barman, X. Liu, S. Draper, and B. Recht. Decomposition methods for large scale LP decoding. In 49th Annual Allerton Conference on Communication, Control, and Com- puting, pages 253-260. IEEE, 2011.  D. Batra, S. Nowozin, and P. Kohli. Tighter relaxations for MAP-MRF inference: A local primal-dual gap based separation algorithm. In International Conference on Artificial Intelligence and Statistics, pages 146-154, 2011.  D. Bertsekas, W. Hager, and O. Mangasarian. Nonlinear Programming. Athena Scientific,  D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. Convex Analysis and Optimization. Athena  1999.  Scientific, 2003.  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Now Publishers, 2011.  S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J.P. Boyle and R.L. Dykstra. A method for finding projections onto the intersections of convex sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, pages 28-47. Springer Verlag, 1986.  P. Brucker. An O(n) algorithm for quadratic knapsack problems. Operations Research  Letters, 3(3):163-166, 1984.  M. Chang, L. Ratinov, and D. Roth. Constraints as prior knowledge.  In International Conference of Machine Learning: Workshop on Prior Knowledge for Text and Language Processing, July 2008.  Y. J. Chu and T. H. Liu. On the shortest arborescence of a directed graph. Science Sinica,  14:1396-1400, 1965.  D. Das. Semi-Supervised and Latent-Variable Models of Natural Language Semantics. PhD  thesis, Carnegie Mellon University, 2012.  D. Das, A.F.T. Martins, and N.A. Smith. An exact dual decomposition algorithm for shallow semantic parsing with constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM), 2012.  J. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using combinatorial optimization within max-product belief propagation. Advances in Neural Information Processing Systems, 19, 2007.  540   Martins, Figueiredo, Aguiar, Smith, and Xing  References  M. B. Almeida and A. F. T. Martins. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proc. of the Annual Meeting of the Association for Computational Linguistics, 2013.  S. Barman, X. Liu, S. Draper, and B. Recht. Decomposition methods for large scale LP decoding. In 49th Annual Allerton Conference on Communication, Control, and Com- puting, pages 253-260. IEEE, 2011.  D. Batra, S. Nowozin, and P. Kohli. Tighter relaxations for MAP-MRF inference: A local primal-dual gap based separation algorithm. In International Conference on Artificial Intelligence and Statistics, pages 146-154, 2011.  D. Bertsekas, W. Hager, and O. Mangasarian. Nonlinear Programming. Athena Scientific,  D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. Convex Analysis and Optimization. Athena  1999.  Scientific, 2003.  S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Now Publishers, 2011.  S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J.P. Boyle and R.L. Dykstra. A method for finding projections onto the intersections of convex sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, pages 28-47. Springer Verlag, 1986.  P. Brucker. An O(n) algorithm for quadratic knapsack problems. Operations Research  Letters, 3(3):163-166, 1984.  M. Chang, L. Ratinov, and D. Roth. Constraints as prior knowledge.  In International Conference of Machine Learning: Workshop on Prior Knowledge for Text and Language Processing, July 2008.  Y. J. Chu and T. H. Liu. On the shortest arborescence of a directed graph. Science Sinica,  14:1396-1400, 1965.  D. Das. Semi-Supervised and Latent-Variable Models of Natural Language Semantics. PhD  thesis, Carnegie Mellon University, 2012.  D. Das, A.F.T. Martins, and N.A. Smith. An exact dual decomposition algorithm for shallow semantic parsing with constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM), 2012.  J. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using combinatorial optimization within max-product belief propagation. Advances in Neural Information Processing Systems, 19, 2007. Alternating Directions Dual Decomposition  J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. E\ufb03cient projections onto the L1-ball for learning in high dimensions. In Proc. of International Conference of Machine Learning, 2008.  J. Eckstein and D. Bertsekas. On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Programming, 55(1): 293-318, 1992.  J. Edmonds. Optimum branchings. Journal of Research of the National Bureau of Standards,  71B:233-240, 1967.  J.M. Eisner. Three new probabilistic models for dependency parsing: An exploration. In Proc. of International Conference on Computational Linguistics, pages 340-345, 1996.  F. Facchinei and J.S. Pang. Finite-Dimensional Variational Inequalities and Complemen-  tarity Problems, volume 1. Springer Verlag, 2003.  P. F. Felzenszwalb and D. P. Huttenlocher. E\ufb03cient belief propagation for early vision.  International Journal of Computer Vision, 70(1):41-54, 2006.  C.J. Fillmore. Frame semantics and the nature of language. Annals of the New York  Academy of Sciences, 280(1):20-32, 1976.  Q. Fu, H. Wang, and A. Banerjee. Bethe-ADMM for tree decomposition based parallel  MAP inference. In Proc. of Uncertainty in Artificial Intelligence, 2013.  D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathematics with Applications, 2(1): 17-40, 1976.  A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms  for MAP LP-relaxations. Neural Information Processing Systems, 20, 2008.  R. Glowinski and A. Marroco. Sur l\u2019approximation, par \u00b4el\u00b4ements finis d\u2019ordre un, et la r\u00b4esolution, par penalisation-dualit\u00b4e, d\u2019une classe de probl`emes de Dirichlet non lin\u00b4eaires. Rev. Franc. Automat. Inform. Rech. Operat., 9:41-76, 1975.  T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. IEEE Transactions on Information Theory, 56(12):6294-6316, 2010.  B.S. He and X.M. Yuan. On the O(1/t) convergence rate of alternating direction method.  SIAM Journal of Numerical Analysis (to appear), 2011.  M. Hestenes. Multiplier and gradient methods. Journal of Optimization Theory and Appli-  cations, 4:302-320, 1969.  J.K. Johnson, D.M. Malioutov, and A.S. Willsky. Lagrangian relaxation for MAP estimation in graphical models. In 45th Annual Allerton Conference on Communication, Control and Computing, 2007. Martins, Figueiredo, Aguiar, Smith, and Xing  V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In  International Conference of Machine Learning, 2010.  J. Kappes, B. Savchynskyy, and C. Schnorr. A bundle approach to e\ufb03cient MAP-inference by Lagrangian relaxation. In IEEE Conference on Computer Vision and Pattern Recog- nition, 2012.  D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. The  MIT Press, 2009.  V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 28:1568-1583, 2006.  N. Komodakis, N. Paragios, and G. Tziritas. MRF optimization via dual decomposition: In Proc. of International Conference on Computer Vision,  Message-passing revisited. 2007.  T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for parsing with non-projective head automata. In Proc. of Empirical Methods for Natural Language Processing, 2010.  V.A. Kovalevsky and V.K. Koval. A di\ufb00usion algorithm for decreasing energy of max-sum labeling problem. Technical report, Glushkov Institute of Cybernetics, Kiev, USSR, 1975.  F. R. Kschischang, B. J. Frey, and H. A. Loeliger. Factor graphs and the sum-product  algorithm. IEEE Transactions on Information Theory, 47, 2001.  A. Kulesza and F. Pereira. Structured learning with approximate inference. Neural Infor-  mation Processing Systems, 2007.  S. Lauritzen. Graphical Models. Clarendon Press, Oxford, 1996. ISBN 0-19-852219-3.  Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J.M. Hellerstein. Graphlab: A new parallel framework for machine learning. In International Conference on Uncertainty in Artificial Intelligence, 2010.  M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus  of English: the Penn treebank. Computational Linguistics, 19(2):313-330, 1993.  A. F. T. Martins. The Geometry of Constrained Structured Prediction: Applications to Inference and Learning of Natural Language Syntax. PhD thesis, Carnegie Mellon Uni- versity and Instituto Superior T\u00b4ecnico, 2012.  A. F. T. Martins, N. A. Smith, and E. P. Xing. Polyhedral outer approximations with application to natural language parsing. In Proc. of International Conference of Machine Learning, 2009.  A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo. In Neural Information Processing  Augmented dual decomposition for MAP inference. Systems: Workshop in Optimization for Machine Learning, 2010. Alternating Directions Dual Decomposition  A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. An augmented Lagrangian approach to constrained MAP inference. In Proc. of International Conference on Machine Learning, 2011a.  A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. Dual decom- position with many overlapping components. In Proc. of Empirical Methods for Natural Language Processing, 2011b.  A. F. T. Martins, M. B. Almeida, and N. A. Smith. Turning on the turbo: Fast third- order non-projective turbo parsers. In Proc. of the Annual Meeting of the Association for Computational Linguistics, 2013.  R. McDonald and G. Satta. On the complexity of non-projective data-driven dependency  parsing. In Proc. of International Conference on Parsing Technologies, 2007.  R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. Non-projective dependency parsing In Proc. of Empirical Methods for Natural Language  using spanning tree algorithms. Processing, 2005.  O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2011.  C. Michelot. A finite algorithm for finding the projection of a point onto the canonical simplex of Rn. Journal of Optimization Theory and Applications, 50(1):195-200, 1986.  Y. Nesterov. A method of solving a convex programming problem with convergence rate  O(1/k2). Soviet Math. Doklady, 27:372-376, 1983.  J. Nocedal and S.J. Wright. Numerical Optimization. Springer verlag, 1999.  S. Nowozin and C.H. Lampert. Global connectivity potentials for random field models. In IEEE Conference on Computer Vision and Pattern Recognition, pages 818-825. IEEE, 2009.  P. M. Pardalos and N. Kovoor. An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46(1):321-328, 1990.  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.  Morgan Kaufmann, 1988.  P. Pletscher and S. Wul\ufb00. LPQP for MAP: Putting LP Solvers to Better Use. In Proc. of  International Conference on Machine Learning, 2012.  H. Poon and P. Domingos. Unsupervised semantic parsing. In Proc. of Empirical Methods  in Natural Language Processing, 2009.  M. Powell. A method for nonlinear constraints in minimization problems. In R. Fletcher,  editor, Optimization, pages 283-298. Academic Press, 1969. Martins, Figueiredo, Aguiar, Smith, and Xing  V. Punyakanok, D. Roth, W. Yih, and D. Zimak. Learning and inference over constrained  output. In Proc. of International Joint Conference on Artificial Intelligence, 2005.  P. Ravikumar, A. Agarwal, and M. Wainwright. Message-passing for graph-structured linear programs: Proximal methods and rounding schemes. Journal of Machine Learning Research, 11:1043-1080, 2010.  M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1):107-136,  2006.  2008.  T.J. Richardson and R.L. Urbanke. Modern Coding Theory. Cambridge University Press,  R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970.  D. Roth and W. Yih. A linear programming formulation for global inference in natural  language tasks. In International Conference on Natural Language Learning, 2004.  A.M. Rush and M. Collins. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45:305-362, 2012.  B. Savchynskyy, S. Schmidt, J. Kappes, and C. Schn\u00a8orr. A study of Nesterov\u2019s scheme for Lagrangian decomposition and MAP labeling. In IEEE Conference on Computer Vision and Pattern Recognition, 2011.  M. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions.  Kibernetika, 4:113-130, 1976.  A. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Distributed message passing for In IEEE Conference on Computer Vision and Pattern  large scale graphical models. Recognition, pages 1833-1840, 2011.  A. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally convergent dual MAP LP relaxation solvers using Fenchel-Young margins. In Advances in Neural Information Processing Systems 25, pages 2393-2401, 2012.  D. Smith and J. Eisner. Dependency parsing by belief propagation. In Proc. of Empirical  Methods for Natural Language Processing, 2008.  D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and T Jaakkola. Tightening LP relaxations for MAP using message-passing. In Proc. of Uncertainty in Artificial Intelligence, 2008.  D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference.  In Optimization for Machine Learning. MIT Press, 2011.  M. Sun, M. Telaprolu, H. Lee, and S. Savarese. An e\ufb03cient branch-and-bound algorithm for optimal human pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1616-1623, 2012.  R. Tanner. A recursive approach to low complexity codes. IEEE Transactions on Informa-  tion Theory, 27(5):533-547, 1981. Alternating Directions Dual Decomposition  D. Tarlow, I. E. Givoni, and R. S. Zemel. HOP-MAP: E\ufb03cient message passing with high  order potentials. In AISTATS, 2010.  M. Wainwright and M. Jordan. Graphical Models, Exponential Families, and Variational  Inference. Now Publishers, 2008.  M. Wainwright, T. Jaakkola, and A. Willsky. MAP estimation via agreement on trees: message-passing and linear programming. IEEE Transactions on Information Theory, 51 (11):3697-3717, 2005.  H. Wang and A. Banerjee. Online alternating direction method. In Proc. of International  Conference on Machine Learning, 2012.  T. Werner. A linear programming approach to max-sum problem: A review. IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, 29:1165-1179, 2007.  H. Yamada and Y. Matsumoto. Statistical dependency analysis with support vector ma-  chines. In Proc. of International Conference on Parsing Technologies, 2003.  C. Yanover, T. Meltzer, and Y. Weiss.  Linear programming relaxations and belief propagation\u2014an empirical study. Journal of Machine Learning Research, 7:1887-1907, 2006.  J.S. Yedidia, Y. Wang, and S.C. Draper. Divide and concur and di\ufb00erence-map BP decoders  for LDPC codes. IEEE Transactions on Information Theory, 57(2):786-802, 2011. "}, "Discrete Restricted Boltzmann Machines": {"volumn": 16, "url": "http://jmlr.org/papers/v16/montufar15a.html", "header": "Discrete Restricted Boltzmann Machines", "author": "Guido Mont\u00c3\u00bafar, Jason Morton", "time": "16(21):653\u2212672, 2015.", "abstract": "We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete na\u00c3\u0083\u00c2\u00afve Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension.", "pdf_url": "http://jmlr.org/papers/volume16/montufar15a/montufar15a.pdf"}, "Evolving GPU Machine Code": {"volumn": 16, "url": "http://jmlr.org/papers/v16/dasilva15a.html", "header": "Evolving GPU Machine Code", "author": "Cleomar Pereira da Silva, Douglas Mota Dias, Cristiana Bentes, Marco Aur\\'{e}lio Cavalcanti Pacheco, Le, ro Fontoura Cupertino", "time": "16(22):673\u2212712, 2015.", "abstract": "Parallel Graphics Processing Unit (GPU) implementations of GP have appeared in the literature using three main methodologies: (i)  compilation , which generates the individuals in GPU code and requires compilation; (ii)  pseudo-assembly , which generates the individuals in an intermediary assembly code and also requires compilation; and (iii)  interpretation , which interprets the codes. This paper proposes a new methodology that uses the concepts of quantum computing and directly handles the GPU machine code instructions. Our methodology utilizes a probabilistic representation of an individual to improve the global search capability. In addition, the evolution in machine code eliminates both the overhead of compiling the code and the cost of parsing the program during evaluation. We obtained up to 2.74 trillion GP operations per second for the 20-bit Boolean Multiplexer benchmark. We also compared our approach with the other three GPU-based acceleration methodologies implemented for quantum-inspired linear GP. Significant gains in performance were obtained.", "pdf_url": "http://jmlr.org/papers/volume16/dasilva15a/dasilva15a.pdf"}, "A Compression Technique for Analyzing Disagreement-Based Active Learning": {"volumn": 16, "url": "http://jmlr.org/papers/v16/wiener15a.html", "header": "A Compression Technique for Analyzing Disagreement-Based Active Learning", "author": "Yair Wiener, Steve Hanneke, Ran El-Yaniv", "time": "16(23):713\u2212745, 2015.", "abstract": "We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the  version space compression set size . This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms.", "pdf_url": "http://jmlr.org/papers/volume16/wiener15a/wiener15a.pdf"}, "Strong Consistency of the Prototype Based Clustering in Probabilistic Space": {"volumn": 16, "url": "http://jmlr.org/papers/v16/nikulin15a.html", "header": "Strong Consistency of the Prototype Based Clustering in Probabilistic Space", "author": "Vladimir Nikulin", "time": "16(25):775\u2212785, 2015.", "abstract": "In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering. This approach was motivated by the Divisive Information- Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence, which may be regarded as a special case within the Clustering Minimisation framework.", "pdf_url": "http://jmlr.org/papers/volume16/nikulin15a/nikulin15a.pdf"}, "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm": {"volumn": 16, "url": "http://jmlr.org/papers/v16/germain15a.html", "header": "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm", "author": "Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario March, Jean-Francis Roy", "time": "16(26):787\u2212860, 2015.", "abstract": "We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback- Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine.", "pdf_url": "http://jmlr.org/papers/volume16/germain15a/germain15a.pdf"}, "Learning with the Maximum Correntropy Criterion Induced Losses for Regression": {"volumn": 16, "url": "http://jmlr.org/papers/v16/feng15a.html", "header": "Learning with the Maximum Correntropy Criterion Induced Losses for Regression", "author": "Yunlong Feng, Xiaolin Huang, Lei Shi, Yuning Yang, Johan A.K. Suykens", "time": "16(30):993\u22121034, 2015.", "abstract": "Within the statistical learning framework, this paper studies the regression model associated with the correntropy induced losses. The correntropy, as a similarity measure, has been frequently employed in signal processing and pattern recognition. Motivated by its empirical successes, this paper aims at presenting some theoretical understanding towards the maximum correntropy criterion in regression problems. Our focus in this paper is two-fold: first, we are concerned with the connections between the regression model associated with the correntropy induced loss and the least squares regression model. Second, we study its convergence property. A learning theory analysis which is centered around the above two aspects is conducted. From our analysis, we see that the scale parameter in the loss function balances the convergence rates of the regression model and its robustness. We then make some efforts to sketch a general view on robust loss functions when being applied into the learning for regression problems. Numerical experiments are also implemented to verify the effectiveness of the model.", "pdf_url": "http://jmlr.org/papers/volume16/feng15a/feng15a.pdf"}, "Joint Estimation of Multiple Precision Matrices with Common Structures": {"volumn": 16, "url": "http://jmlr.org/papers/v16/lee15a.html", "header": "Joint Estimation of Multiple Precision Matrices with Common Structures", "author": "Wonyul Lee, Yufeng Liu", "time": "16(31):1035\u22121062, 2015.", "abstract": "Estimation of inverse covariance matrices, known as precision matrices, is important in various areas of statistical analysis. In this article, we consider estimation of multiple precision matrices sharing some common structures. In this setting, estimating each precision matrix separately can be suboptimal as it ignores potential common structures. This article proposes a new approach to parameterize each precision matrix as a sum of common and unique components and estimate multiple precision matrices in a constrained $l_1$ minimization framework. We establish both estimation and selection consistency of the proposed estimator in the high dimensional setting. The proposed estimator achieves a faster convergence rate for the common structure in certain cases. Our numerical examples demonstrate that our new estimator can perform better than several existing methods in terms of the entropy loss and Frobenius loss. An application to a glioblastoma cancer data set reveals some interesting gene networks across multiple cancer subtypes.", "pdf_url": "http://jmlr.org/papers/volume16/lee15a/lee15a.pdf"}, "Fast Cross-Validation via Sequential Testing": {"volumn": 16, "url": "http://jmlr.org/papers/v16/krueger15a.html", "header": "Fast Cross-Validation via Sequential Testing", "author": "Tammo Krueger, Danny Panknin, Mikio Braun", "time": "16(33):1103\u22121155, 2015.", "abstract": "With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the power of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy.", "pdf_url": "http://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf"}, "Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data": {"volumn": 16, "url": "http://jmlr.org/papers/v16/honorio15a.html", "header": "Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data", "author": "Jean Honorio, Luis Ortiz", "time": "16(34):1157\u22121210, 2015.", "abstract": "We consider learning, from  strictly  behavioral data, the structure and parameters of  linear influence games (LIGs) , a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate  causal strategic inference (CSI) : Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by  pure-strategy Nash equilibria (PSNE) . Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including  convex loss minimization (CLM)  and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games.", "pdf_url": "http://jmlr.org/papers/volume16/honorio15a/honorio15a.pdf"}, "Local Identification of Overcomplete Dictionaries": {"volumn": 16, "url": "http://jmlr.org/papers/v16/schnass15a.html", "header": "Local Identification of Overcomplete Dictionaries", "author": "Karin Schnass", "time": "16(35):1211\u22121242, 2015.", "abstract": "This paper presents the first theoretical results showing that stable identification of overcomplete $\\mu$-coherent dictionaries $\\Phi \\in \\mathbb{R}^{d\\times K}$ is locally possible from training signals with sparsity levels $S$ up to the order $O(\\mu^{-2})$ and signal to noise ratios up to $O(\\sqrt{d})$. In particular the dictionary is recoverable as the local maximum of a new maximization criterion that generalizes the K-means criterion. For this maximization criterion results for asymptotic e", "pdf_url": "http://jmlr.org/papers/volume16/schnass15a/schnass15a.pdf"}, "Perturbed Message Passing for Constraint Satisfaction Problems": {"volumn": 16, "url": "http://jmlr.org/papers/v16/ravanbakhsh15a.html", "header": "Perturbed Message Passing for Constraint Satisfaction Problems", "author": "Siamak Ravanbakhsh, Russell  Greiner", "time": "16(37):1249\u22121274, 2015.", "abstract": "We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called  Perturbed Belief Propagation , smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver,  Perturbed Survey Propagation . Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (w.r.t. the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes.", "pdf_url": "http://jmlr.org/papers/volume16/ravanbakhsh15a/ravanbakhsh15a.pdf"}, "Rationality, Optimism and Guarantees in General Reinforcement Learning": {"volumn": 16, "url": "http://jmlr.org/papers/v16/sunehag15a.html", "header": "Rationality, Optimism and Guarantees in General Reinforcement Learning", "author": "Peter Sunehag, Marcus Hutter", "time": "16(40):1345\u22121390, 2015.", "abstract": "In this article, we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis- generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.", "pdf_url": "http://jmlr.org/papers/volume16/sunehag15a/sunehag15a.pdf"}, "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion": {"volumn": 16, "url": "http://jmlr.org/papers/v16/kiraly15a.html", "header": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion", "author": "Franz J.Kir\u00c3\u00a1ly, Louis Theran, Ryota Tomioka", "time": "16(41):1391\u22121436, 2015.", "abstract": "We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition.", "pdf_url": "http://jmlr.org/papers/volume16/kiraly15a/kiraly15a.pdf"}, "Second-Order Non-Stationary Online Learning for Regression": {"volumn": 16, "url": "http://jmlr.org/papers/v16/moroshko15a.html", "header": "Second-Order Non-Stationary Online Learning for Regression", "author": "Edward Moroshko, Nina Vaits, Koby Crammer", "time": "16(43):1481\u22121517, 2015.", "abstract": "The goal of a learner in standard online learning, is to have the cumulative loss not much larger compared with the best- performing function from some fixed class. Numerous algorithms were shown to have this gap arbitrarily close to zero, compared with the best function that is chosen off-line. Nevertheless, many real-world applications, such as adaptive filtering, are non-stationary in nature, and the best prediction function may drift over time. We introduce two novel algorithms for online regression, designed to work well in non-stationary environment. Our first algorithm performs adaptive resets to forget the history, while the second is last-step min-max optimal in context of a drift. We analyze both algorithms in the worst-case regret framework and show that they maintain an average loss close to that of the best slowly changing sequence of linear functions, as long as the cumulative drift is sublinear. In addition, in the stationary case, when no drift occurs, our algorithms suffer logarithmic regret, as for previous algorithms. Our bounds improve over existing ones, and simulations demonstrate the usefulness of these algorithms compared with other state-of-the-art approaches.", "pdf_url": "http://jmlr.org/papers/volume16/moroshko15a/moroshko15a.pdf"}, "A Finite Sample Analysis of the Naive Bayes Classifier": {"volumn": 16, "url": "http://jmlr.org/papers/v16/berend15a.html", "header": "A Finite Sample Analysis of the Naive Bayes Classifier", "author": "Daniel Berend, Aryeh Kontorovich", "time": "16(44):1519\u22121545, 2015.", "abstract": "We revisit, from a statistical learning perspective, the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Naive Bayes weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. We derive optimality results for our estimates and also establish some structural characterizations. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. Several challenging open problems are posed, and experimental results are provided to illustrate the theory.", "pdf_url": "http://jmlr.org/papers/volume16/berend15a/berend15a.pdf"}, "Flexible High-Dimensional Classification Machines and Their Asymptotic Properties": {"volumn": 16, "url": "http://jmlr.org/papers/v16/qiao15a.html", "header": "Flexible High-Dimensional Classification Machines and Their Asymptotic Properties", "author": "Xingye Qiao, Lingsong Zhang", "time": "16(45):1547\u22121572, 2015.", "abstract": "Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high- dimensional data sets by showing the undesired data-piling phenomenon. The DWD method was proposed to improve SVM in the high-dimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and provides a trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate theoretical findings.", "pdf_url": "http://jmlr.org/papers/volume16/qiao15a/qiao15a.pdf"}, "Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery": {"volumn": 16, "url": "http://jmlr.org/papers/v16/liu15b.html", "header": "Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery", "author": "Han Liu, Lie Wang, Tuo Zhao", "time": "16(47):1579\u22121606, 2015.", "abstract": "We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst- case numerical rate of convergence $\\cO(1/\\epsilon)$, where $\\epsilon$ is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts. The R package  camel  implementing the proposed method is available on the Comprehensive R Archive Network  cran.r-project.org/web/ packages/camel .", "pdf_url": "http://jmlr.org/papers/volume16/liu15b/liu15b.pdf"}, "Bayesian Nonparametric Crowdsourcing": {"volumn": 16, "url": "http://jmlr.org/papers/v16/moreno15a.html", "header": "Bayesian Nonparametric Crowdsourcing", "author": "Pablo G. Moreno, Antonio Artes-Rodriguez, Yee Whye Teh, Fern, o Perez-Cruz", "time": "16(48):1607\u22121627, 2015.", "abstract": "Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the ex", "pdf_url": "http://jmlr.org/papers/volume16/moreno15a/moreno15a.pdf"}, "Preface to this Special Issue": {"volumn": 16, "url": "http://jmlr.org/papers/v16/gammerman15a.html", "header": "Preface to this Special Issue", "author": "Alex Gammerman, Vladimir Vovk", "time": "16(50):1677\u22121681, 2015.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume16/gammerman15a/gammerman15a.pdf"}, "Optimal Estimation of Low Rank Density Matrices": {"volumn": 16, "url": "http://jmlr.org/papers/v16/koltchinskii15a.html", "header": "Optimal Estimation of Low Rank Density Matrices", "author": "Vladimir Koltchinskii, Dong Xia", "time": "16(53):1757\u22121792, 2015.", "abstract": "The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten $p$-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances.", "pdf_url": "http://jmlr.org/papers/volume16/koltchinskii15a/koltchinskii15a.pdf"}, "Fast Rates in Statistical and Online Learning": {"volumn": 16, "url": "http://jmlr.org/papers/v16/vanerven15a.html", "header": "Fast Rates in Statistical and Online Learning", "author": "Tim van Erven, Peter D. Gr\u00c3\u00bcnwald, Nishant A. Mehta, Mark D. Reid, Robert C. Williamson", "time": "16(54):1793\u22121861, 2015.", "abstract": "The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning --- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the  central condition  for `proper' learning algorithms that always output a hypothesis in the given model, and  stochastic mixability  for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the  Bernstein  condition, itself a generalization of the  Tsybakov margin condition , both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a  stochastic exp-concavity  condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of  mixability . Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.", "pdf_url": "http://jmlr.org/papers/volume16/vanerven15a/vanerven15a.pdf"}, "Sharp Oracle Bounds for Monotone and Convex Regression Through Aggregation": {"volumn": 16, "url": "http://jmlr.org/papers/v16/bellec15a.html", "header": "Sharp Oracle Bounds for Monotone and Convex Regression Through Aggregation", "author": "Pierre C. Bellec, Alexandre B. Tsybakov", "time": "16(56):1879\u22121892, 2015.", "abstract": "We derive oracle inequalities for the problems of isotonic and convex regression using the combination of $Q$-aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation.", "pdf_url": "http://jmlr.org/papers/volume16/bellec15a/bellec15a.pdf"}, "Exceptional Rotations of Random Graphs: A VC Theory": {"volumn": 16, "url": "http://jmlr.org/papers/v16/addarioberry15a.html", "header": "Exceptional Rotations of Random Graphs: A VC Theory", "author": "Louigi Addario-Berry, Shankar Bhamidi, S\u00c3\u00a9bastien Bubeck, Luc Devroye, G\u00c3\u00a1bor Lugosi, Roberto Imbuzeiro Oliveira", "time": "16(57):1893\u22121922, 2015.", "abstract": "In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how \"rich\" does the process have to be so that one sees atypical behavior. In particular, we study a natural process of Erd\u00c3\u0085\u00c2\u0091s-R\u00c3\u0083\u00c2\u00a9nyi random graphs indexed by unit vectors in $\\R^d$. We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension $d$ that guarantees the existence of \"exceptional directions\" in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes.", "pdf_url": "http://jmlr.org/papers/volume16/addarioberry15a/addarioberry15a.pdf"}, "Semi-Supervised Interpolation in an Anticausal Learning Scenario": {"volumn": 16, "url": "http://jmlr.org/papers/v16/janzing15a.html", "header": "Semi-Supervised Interpolation in an Anticausal Learning Scenario", "author": "Dominik Janzing, Bernhard Sch\u00c3\u00b6lkopf", "time": "16(58):1923\u22121948, 2015.", "abstract": "According to a recently stated 'independence postulate', the distribution $P_{\\rm cause}$ contains no information about the conditional $P_{\\rm effect | cause}$ while $P_{\\rm effect}$ may contain information about $P_{\\rm cause | effect}$. Since semi- supervised learning (SSL) attempts to exploit information from $P_X$ to assist in predicting $Y$ from $X$, it should on", "pdf_url": "http://jmlr.org/papers/volume16/janzing15a/janzing15a.pdf"}, "Towards an Axiomatic Approach to Hierarchical Clustering of Measures": {"volumn": 16, "url": "http://jmlr.org/papers/v16/thomann15a.html", "header": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures", "author": "Philipp Thomann, Ingo Steinwart, Nico Schmid", "time": "16(59):1949\u22122002, 2015.", "abstract": "We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density.", "pdf_url": "http://jmlr.org/papers/volume16/thomann15a/thomann15a.pdf"}, "Predicting a Switching Sequence of Graph Labelings": {"volumn": 16, "url": "http://jmlr.org/papers/v16/herbster15a.html", "header": "Predicting a Switching Sequence of Graph Labelings", "author": "Mark Herbster, Stephen Pasteris, Massimiliano Pontil", "time": "16(60):2003\u22122022, 2015.", "abstract": "We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of $K$ labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions.", "pdf_url": "http://jmlr.org/papers/volume16/herbster15a/herbster15a.pdf"}, "Learning Using Privileged Information: Similarity Control and Knowledge Transfer": {"volumn": 16, "url": "http://jmlr.org/papers/v16/vapnik15b.html", "header": "Learning Using Privileged Information: Similarity Control and Knowledge Transfer", "author": "Vladimir Vapnik, Rauf Izmailov", "time": "16(61):2023\u22122049, 2015.", "abstract": "This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.", "pdf_url": "http://jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf"}, "Alexey Chervonenkis's Bibliography: Introductory Comments": {"volumn": 16, "url": "http://jmlr.org/papers/v16/gammerman15b.html", "header": "Alexey Chervonenkis's Bibliography: Introductory Comments", "author": "Alex Gammerman, Vladimir Vovk", "time": "16(62):2051\u22122066, 2015.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume16/gammerman15b/gammerman15b.pdf"}, "Alexey Chervonenkis's Bibliography": {"volumn": 16, "url": "http://jmlr.org/papers/v16/gammerman15c.html", "header": "Alexey Chervonenkis's Bibliography", "author": "Alex Gammerman, Vladimir Vovk", "time": "16(63):2067\u22122080, 2015.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume16/gammerman15c/gammerman15c.pdf"}, "Photonic Delay Systems as Machine Learning Implementations": {"volumn": 16, "url": "http://jmlr.org/papers/v16/hermans15a.html", "header": "Photonic Delay Systems as Machine Learning Implementations", "author": "Michiel Hermans, Miguel C. Soriano, Joni Dambre, Peter Bienstman, Ingo Fischer", "time": "16(64):2081\u22122097, 2015.", "abstract": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.", "pdf_url": "http://jmlr.org/papers/volume16/hermans15a/hermans15a.pdf"}, "On Linearly Constrained Minimum Variance Beamforming": {"volumn": 16, "url": "http://jmlr.org/papers/v16/zhang15b.html", "header": "On Linearly Constrained Minimum Variance Beamforming", "author": "Jian Zhang, Chao Liu", "time": "16(65):2099\u22122145, 2015.", "abstract": "Beamforming is a widely used technique for source localization in signal processing and neuroimaging. A number of vector- beamformers have been introduced to localize neuronal activity by using magnetoencephalography (MEG) data in the literature. However, the existing theoretical analyses on these beamformers have been limited to simple cases, where no more than two sources are allowed in the associated model and the theoretical sensor covariance is also assumed known. The information about the effects of the MEG spatial and temporal dimensions on the consistency of vector-beamforming is incomplete. In the present study, we consider a class of vector-beamformers defined by thresholding the sensor covariance matrix, which include the standard vector-beamformer as a special case. A general asymptotic theory is developed for these vector-beamformers, which shows the extent of effects to which the MEG spatial and temporal dimensions on estimating the neuronal activity index. The performances of the proposed beamformers are assessed by simulation studies. Superior performances of the proposed beamformers are obtained when the signal-to-noise ratio is low. We apply the proposed procedure to real MEG data sets derived from five sessions of a human face-perception experiment, finding several highly active areas in the brain. A good agreement between these findings and the known neurophysiology of the MEG response to human face perception is shown.", "pdf_url": "http://jmlr.org/papers/volume16/zhang15b/zhang15b.pdf"}, "Existence and Uniqueness of Proper Scoring Rules": {"volumn": 16, "url": "http://jmlr.org/papers/v16/ovcharov15a.html", "header": "Existence and Uniqueness of Proper Scoring Rules", "author": "Evgeni Y. Ovcharov", "time": "16(67):2207\u22122230, 2015.", "abstract": "To discuss the existence and uniqueness of proper scoring rules one needs to extend the associated entropy functions as sublinear functions to the conic hull of the prediction set. In some natural function spaces, such as the Lebesgue $L^p$-spaces over $\\mathbb{R}^d$, the positive cones have empty interior. Entropy functions defined on such cones have directional derivatives only, which typically exist on large subspaces and behave similarly to gradients. Certain entropies may be further extended continuously to open cones in normed spaces containing signed densities. The extended densities are Gateaux differentiable except on a negligible set and have everywhere continuous subgradients due to the supporting hyperplane theorem. We introduce the necessary framework from analysis and algebra that allows us to give an affirmative answer to the titular question of the paper. As a result of this, we give a formal sense in which entropy functions have uniquely associated proper scoring rules. We illustrate our framework by studying the derivatives and subgradients of the following three prototypical entropies: Shannon entropy, Hyvarinen entropy, and quadratic entropy.", "pdf_url": "http://jmlr.org/papers/volume16/ovcharov15a/ovcharov15a.pdf"}, "Adaptive Strategy for Stratified Monte Carlo Sampling": {"volumn": 16, "url": "http://jmlr.org/papers/v16/carpentier15a.html", "header": "Adaptive Strategy for Stratified Monte Carlo Sampling", "author": "Alexandra Carpentier, Remi Munos, Andr\u00c3\u00a1s Antos", "time": "16(68):2231\u22122271, 2015.", "abstract": "We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a $K$-armed bandit, where the arms represent the $K$ strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable $n$ times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distribution-dependent bound of order $\\text{poly}(\\lambda_{\\min}^{-1})\\tilde{O}(n^{-3/2})$ (The notation $a_n=\\text{poly}(b_n)$ means that there exist $C$,$\\alpha>0$ such that $a_n\\le Cb_n^\\alpha$ for $n$ large enough. Moreover, $a_n=\\tilde{O}(b_n)$ means that $a_n/b_n=\\text{poly}(\\log n)$ for $n$ large enough.) that depends on a measure of the disparity $\\lambda_{\\min}$ of the per stratum variances and a distribution-free bound $\\text{poly}(K)\\tilde{O}(n^{-7/6})$ that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problem- independent bound for this proxy matches its recent minimax lower bound in terms of $n$ up to a $\\log n$ factor.", "pdf_url": "http://jmlr.org/papers/volume16/carpentier15a/carpentier15a.pdf"}}