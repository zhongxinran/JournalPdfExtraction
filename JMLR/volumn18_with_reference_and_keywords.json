{"Averaged Collapsed Variational Bayes Inference": {"volumn": 18, "url": "http://jmlr.org/papers/v18/14-249.html", "header": "Averaged Collapsed Variational Bayes Inference", "author": "Katsuhiko Ishiguro, Issei Sato, Naonori Ueda", "time": "18(1):1\u221229, 2017.", "abstract": "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study th", "pdf_url": "http://jmlr.org/papers/volume18/14-249/14-249.pdf", "keywords": ["nonparametric Bayes", "collapsed variational Bayes inference", "averaged CVB"], "reference": "Collapsed variational Bayesian inference for hidden Markov models. In Proc. AISTATS, 2013.  Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed Membership  Stochastic Blockmodels. Journal of Machine Learning Research, 9:1981-2014, 2008.  Kristo\ufb00er Jon Albers, Andreas Leon Aagard Moth, Morten M\u00f8rup, and Mikkel N. Schmidt. Large scale inference in the infinite relational model: Gibbs sampling is not enough. In Proc. MLSP, 2013.  Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference  for topic models. In Proc. UAI, 2009.  Hagai Attias. A variational Bayesian framework for graphical models. In Proc. NIPS, 2000.  Jullian Besag. Statistical Analysis of Non-Lattice Data. Journal of the Royal Statistical Society:  Series D, 24(3):179-195, 1975.  Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York,  2006.  David Blackwell and James B. MacQueen. Ferguson Distributions via Polya urn schemes. The  Annals of Statistics, 1(2):353-355, 1973.  David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of  Machine Learning Research, 3:993-1022, 2003.  David M. Blei, Alp Kucukelbir, and Jon D. McAuli\ufb00e. Variational Inference: A Review for Statis-  ticians. arXiv, page arXiv:1601.00670v1 [stat.CO], 2016.  Arnim Bleier. Practical collapsed stochastic variational inference for the HDP.  In Proc. NIPS  workshop on topic models, 2013.  Statistics, 1(2):353-355, 1973.  Thomas S. Ferguson. A Bayesian Analysis of Some Nonparametric Problems. The Annals of  James Foulds, Levi Boyles, Christopher DuBois, Padhraic Symyth, and Max Welling. Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation. In Proc. KDD, 2013.  Wenjie Fu, Le Song, and Eric P. Xing. Dynamic mixed membership blockmodel for evolving  networks. In Proc. ICML, 2009.  Thomas L. Gri\ufb03ths and Zoubin Ghahramani. The Indian Bu\ufb00et Process : An Introduction and  Review. Journal of Machine Learning Research, 12:1185-1224, 2011.  Toke Jansen Hansen, Morten M\u00f8rup, and Lars Kai Hanse. Non-parametric co-clustering of large  scale sparse nipartite networks on the GPU. In Proc. MLSP, 2011.  Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, and Joshua B. Tenenbaum. Dynamic infinite  relational model for time-varying relational data analysis. In Proc. NIPS, 2010.  28   Ishiguro, Sato, and Ueda  References  Collapsed variational Bayesian inference for hidden Markov models. In Proc. AISTATS, 2013.  Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed Membership  Stochastic Blockmodels. Journal of Machine Learning Research, 9:1981-2014, 2008.  Kristo\ufb00er Jon Albers, Andreas Leon Aagard Moth, Morten M\u00f8rup, and Mikkel N. Schmidt. Large scale inference in the infinite relational model: Gibbs sampling is not enough. In Proc. MLSP, 2013.  Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference  for topic models. In Proc. UAI, 2009.  Hagai Attias. A variational Bayesian framework for graphical models. In Proc. NIPS, 2000.  Jullian Besag. Statistical Analysis of Non-Lattice Data. Journal of the Royal Statistical Society:  Series D, 24(3):179-195, 1975.  Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York,  2006.  David Blackwell and James B. MacQueen. Ferguson Distributions via Polya urn schemes. The  Annals of Statistics, 1(2):353-355, 1973.  David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of  Machine Learning Research, 3:993-1022, 2003.  David M. Blei, Alp Kucukelbir, and Jon D. McAuli\ufb00e. Variational Inference: A Review for Statis-  ticians. arXiv, page arXiv:1601.00670v1 [stat.CO], 2016.  Arnim Bleier. Practical collapsed stochastic variational inference for the HDP.  In Proc. NIPS  workshop on topic models, 2013.  Statistics, 1(2):353-355, 1973.  Thomas S. Ferguson. A Bayesian Analysis of Some Nonparametric Problems. The Annals of  James Foulds, Levi Boyles, Christopher DuBois, Padhraic Symyth, and Max Welling. Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation. In Proc. KDD, 2013.  Wenjie Fu, Le Song, and Eric P. Xing. Dynamic mixed membership blockmodel for evolving  networks. In Proc. ICML, 2009.  Thomas L. Gri\ufb03ths and Zoubin Ghahramani. The Indian Bu\ufb00et Process : An Introduction and  Review. Journal of Machine Learning Research, 12:1185-1224, 2011.  Toke Jansen Hansen, Morten M\u00f8rup, and Lars Kai Hanse. Non-parametric co-clustering of large  scale sparse nipartite networks on the GPU. In Proc. MLSP, 2011.  Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, and Joshua B. Tenenbaum. Dynamic infinite  relational model for time-varying relational data analysis. In Proc. NIPS, 2010. Averaged CVB  Katushiko Ishiguro, Naonori Ueda, and Hiroshi Sawada. Subset infinite relational models. In Proc.  AISTATS, 2012.  Charles Kemp, Joshua B. Tenenbaum, Thomas L. Gri\ufb03ths, Takeshi Yamada, and Naonori Ueda.  Learning systems of concepts with an infinite relational model. In Proc. AAAI, 2006.  Bryan Klimt and Yiming Yang. The Enron corpus : a new dataset for Email classification Research.  In Proc. ECML, 2004.  Takuya Konishi, Takatomi Kubo, Kazuho Watanabe, and Kazushi Ikeda. Variational Bayesian In- ference Algorithms for Infinite Relational Model of Network Data. IEEE Transactions on Neural Networks and Learning Systems, 26(9):2176-2181, 2014.  Kenichi Kurihara, Max Welling, and Yee Whye Teh. Collapsed variational Dirichlet process mixture  models. In Proc. IJCAI, 2007.  Aaron Q. Li, Amr Ahmed, Sujith Ravi, and Alexander J. Smola. Reducing the sampling complexity  of topic models. In Proc. KDD, 2014.  Andrew McCallum, Andres Corrada-Emmanuel, and Xuerui Wang. Topic and role discovery in  social networks. In Proc. IJCAI, 2005.  Kurt T. Miller, Thomas L. Gri\ufb03ths, and Michael I. Jordan. Nonparametric latent feature models for  link prediction. In Proc. NIPS, 2009.  Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-  ical Statistics, pages 400-407, 1951.  Issei Sato and Hiroshi Nakagawa. Rethinking collapsed variational Bayes inference for LDA. In  Proc. ICML, 2012.  Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa. Practical collapsed variational Bayes inference  for hierarchical dirichlet process. In Proc. KDD, 2012.  Jayaram Sethuraman. A Constructive definition of Dirichlet process. Statistica Sinica, 4:639-650,  1994.  Lei Tang, Huan Liu, Jianping Zhang, and Zohreh Nazeri. Community evolution in dynamic multi-  mode networks. In Proc. KDD, 2008.  Yee Whye Teh, David Newman, Max Welling, and D. Neaman. A collapsed variational Bayesian  inference algorithm for latent Dirichlet allocation. In Proc. NIPS, 2007.  Yee Whye Teh, Kenichi Kurihara, and Max Welling. Collapsed variational inference for HDP. In  Pengyu Wang and Phil Blunsom. Collapsed variational Bayesian inference for PCFGs. In Proc.  Proc. NIPS, 2008.  ACL, 2013. "}, "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks": {"volumn": 18, "url": "http://jmlr.org/papers/v18/14-400.html", "header": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks", "author": "Nan Du, Yingyu Liang, Maria-Florina Balcan, Manuel Gomez-Rodriguez, Hongyuan Zha, Le Song", "time": "18(2):1\u221245, 2017.", "abstract": "A typical viral marketing model identifies influential u", "pdf_url": "http://jmlr.org/papers/volume18/14-400/14-400.pdf"}, "Local algorithms for interactive clustering": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-085.html", "header": "Local algorithms for interactive clustering", "author": "Pranjal Awasthi, Maria Florina Balcan, Konstantin Voevodski", "time": "18(3):1\u221235, 2017.", "abstract": "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn - changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice.", "pdf_url": "http://jmlr.org/papers/volume18/15-085/15-085.pdf", "keywords": null, "reference": "COLT, 2005.  NIPS, 2014.  AAAI, 2012.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  Margareta Ackerman and Sajoy Dasgupta. Incremental clustering: The case for extra clusters. In  Margareta Ackerman, Shai Ben-David, Simina Br\u02c6anzei, and David Loker. Weighted clustering. In  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1998.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In STOC, 2001.  Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries.  CoRR, abs/1606.02404, 2016.  Pranjal Awasthi and Maria-Florina Balcan. Center based clustering: A foundational perspective. In Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto Rocci, editors, Handbook of Cluster Analysis. CRC Press, 2015.  Pranjal Awasthi and Reza Bosagh Zadeh. Supervised clustering. In NIPS, 2010.  Maria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In ALT, 2008.  Maria-Florina Balcan and Pramod Gupta. Robust hierarchical clustering. In COLT, 2010.  Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for cluster-  ing via similarity functions. In STOC, 2008.  Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta. Robust hierarchical clustering. Journal  of Machine Learning Research, 15:3831-3871, 2014.  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning, 56  (1-3):89-113, 2004.  ICML, 2002.  Sugato Basu, Arindam Banerjee, and Raymond Mooney. Semi-supervised clustering by seeding. In  Sugato Basu, Arindam Banerjee, and Raymond J. Mooney. Active semi-supervision for pairwise  constrained clustering. In SDM, 2004.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, 2010.  29   LOCAL ALGORITHMS FOR INTERACTIVE CLUSTERING  7. Acknowledgments  This work was supported in part by grants NSF CCF-0953192, NSF-CCF 1535967, NSF CCF- 1422910, NSF IIS1618714, ONR grant N00014-09-1-0751, AFOSR grant FA9550-09-1-0538, a Google Research Award, and a Microsoft Research Fellowship.  References  COLT, 2005.  NIPS, 2014.  AAAI, 2012.  Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.  In  Margareta Ackerman and Sajoy Dasgupta. Incremental clustering: The case for extra clusters. In  Margareta Ackerman, Shai Ben-David, Simina Br\u02c6anzei, and David Loker. Weighted clustering. In  Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319-342, 1998.  Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In STOC, 2001.  Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries.  CoRR, abs/1606.02404, 2016.  Pranjal Awasthi and Maria-Florina Balcan. Center based clustering: A foundational perspective. In Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto Rocci, editors, Handbook of Cluster Analysis. CRC Press, 2015.  Pranjal Awasthi and Reza Bosagh Zadeh. Supervised clustering. In NIPS, 2010.  Maria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In ALT, 2008.  Maria-Florina Balcan and Pramod Gupta. Robust hierarchical clustering. In COLT, 2010.  Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for cluster-  ing via similarity functions. In STOC, 2008.  Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta. Robust hierarchical clustering. Journal  of Machine Learning Research, 15:3831-3871, 2014.  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning, 56  (1-3):89-113, 2004.  ICML, 2002.  Sugato Basu, Arindam Banerjee, and Raymond Mooney. Semi-supervised clustering by seeding. In  Sugato Basu, Arindam Banerjee, and Raymond J. Mooney. Active semi-supervision for pairwise  constrained clustering. In SDM, 2004.  Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, 2010. AWASTHI, BALCAN AND VOEVODSKI  Constantinos Boulis and Mari Ostendorf. Combining multiple clustering systems. In PKDD, 2004.  S. Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. CoRR,  abs/0804.3575, 2008.  David Bryant and Vincent Berry. A structured family of clustering and tree construction methods.  Advances in Applied Mathematics, 27(4):705-732, 2001.  D. Chaudhuri, B. B. Chaudhuri, and C. A. Murthy. A new split-and-merge clustering technique.  Pattern Recognition Letters, 13(6):399-409, 1992.  Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.  In NIPS,  2010.  Bo Dai, Baogang Hu, and Gang Niu. Bayesian maximum margin clustering. In ICDM, 2010.  Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS, 1999.  Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In ICML, 2008.  Chris Ding and Xiaofeng He. Cluster merging and splitting in hierarchical clustering algorithms. In  ICDM, 2002.  Brian Erikkson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active clustering: robust and  efficient hierarchical clustering using adaptively selected similarities. In AISTATS, 2011.  Katherine A. Heller and Zoubin Ghahramani. Bayesian hierarchical clustering. In ICML, 2005.  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two  Gaussians. In STOC, 2010.  models. In COLT, 2005.  Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture  Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, and Aarti Singh. Efficient active algo-  rithms for hierarchical clustering. In ICML, 2012.  Juho Lee, Suha Kwak, Bohyung Han, and Seungjin Choi. Online video segmentation by bayesian  split-merge clustering. In ECCV, 2012.  Marina Meil\u02d8a. Comparing clusterings - an information based distance. Journal of Multivariate  Analysis, 98(5):873-895, 2007.  Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians.  In FOCS, 2010.  training method. 2012.  38(5):2678-2722, 2010.  Feiping Nie, Dong Xu, and Xuelong Li. Initialization independent clustering with actively self- IEEE Transactions on Systems, Man and Cybernetics, Part B, 42(1):17-27,  Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. The Annals of Statistics, LOCAL ALGORITHMS FOR INTERACTIVE CLUSTERING  Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval.  Information processing and management, 24(5):513-523, 1988.  Alexander Strehl, Joydeep Ghosh, and Raymond Mooney. Impact of similarity measures on web-  page clustering. In AAAI, 2000.  Matus Telgarsky and Sanjoy Dasgupta. Agglomerative bregman clustering. In ICML, 2012.  Konstantin Voevodski, Maria-Florina Balcan, Heiko R\u00a8oglin, Shang-Hua Teng, and Yu Xia. Active clustering of biological sequences. Journal of Machine Learning Research, 13:203-225, 2012.  Qiaoliang Xiang, Qi Mao, Kian Ming A. Chai, Hai Leong Chieu, Ivor Wai-Hung Tsang, and Zhen-  dong Zhao. A split-merge framework for comparing clusterings. In ICML, 2012.  Shi Zhong. Generative model-based document clustering: a comparative study. Knowledge and  Information Systems, 8:374-384, 2005.  Appendix A. Other feedback models  Besides the cluster split/merge feedback considered here, two other kinds of feedback have also been studied in the literature: cluster-assignment feedback (Basu et al., 2002; Nie et al., 2012) and must-link/cannot-link feedback (Basu et al., 2004; Ashtiani et al., 2016). The former reveals the ground-truth cluster assignment of a single data point; the latter reveals whether or not a pair of points are in the same ground-truth cluster.  Claim 28 Cluster-assignment feedback and must-link/cannot link feedback can be converted to cluster split/merge feedback in the unrestricted-merge model.  Proof We first observe that cluster-assignment feedback can be converted to must-link/cannot-link feedback as follows. For every two known cluster assignments for points x and y s.t. x and y belong to the same ground-truth cluster, output a must-link feedback for (x, y). For every two known cluster assignments for points x and y s.t. x and y belong to different ground-truth clusters, output a cannot-link feedback for (x, y).  We next observe that must-link/cannot-link feedback can be converted to cluster split/merge feedback in the unrestricted merge model as follows. For every must-link feedback for points x and y, if x and y belong to different clusters in the proposed clustering, request to merge these two clus- ters. For every cannot-link feedback for points x and y, if x and y belong to the same cluster in the proposed clustering, request to split this cluster. Observe that by definition, the output split/merge requests satisfy the assumptions of the unrestricted-merge model.  However, the conversion in Claim 28 is sometimes vacuous: cluster split/merge feedback is only output when the cluster-assignment or must-link/cannot-link feedback disagrees with the current clustering. Still, it is reasonable to assume that while the proposed clustering is not equivalent to the ground truth, a constant fraction of such feedback will disagree with the proposed clustering. AWASTHI, BALCAN AND VOEVODSKI  Appendix B. Complete experimental results  The following figures show the complete experimental results for all the algorithms. Figure 12 shows the results in the \u03b7-merge model. Figure 13 shows the results in the \u03b7-merge model for the algorithms in Figure 2 and Figure 4 (for the correlation-clustering objective). Figure 14 shows the results in the unrestricted-merge model.  Figure 12: Experimental results in the \u03b7-merge model.0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set A eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set B eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set C eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set D eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set E eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0  LOCAL ALGORITHMS FOR INTERACTIVE CLUSTERING  Figure 13: Experimental results in the \u03b7-merge model for algorithms for correlation-clustering ob-  jective.0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set A eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set B eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set C eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set D eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set E eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0  AWASTHI, BALCAN AND VOEVODSKI  Figure 14: Experimental results in the unrestricted-merge model.0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set A eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set B eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set C eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set D eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 no pruning 2 per cluster 4 per cluster Number Edit Requests Pruned Points Data Set E eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0  LOCAL ALGORITHMS FOR INTERACTIVE CLUSTERING  Appendix C. Experiments with robust average-linkage tree  When we investigate instances where our algorithms are unable to find the target clustering, we observe that there are outlier points that are attached near the root of the average-linkage tree, which are incorrectly split off and re-merged by the algorithm without making any progress towards finding the target clustering.  We can address these outliers by constructing the average-linkage tree in a more robust way: first find groups (\u201cblobs\u201d) of similar points of some minimum size, compute an average-linkage tree for each group, and then combine these trees using average-linkage. The tree constructed in such fashion may then be used by our algorithms.  We tried this approach, using Algorithm 2 from Balcan and Gupta (2010) to compute the blobs. We find that using the robust average-linkage tree gives better performance for the unpruned data sets, but gives no gains for the pruned data sets, as expected. Figure 15 displays the comparison in the unrestricted merge model for the five unpruned data sets. For the pruned data sets, we expect the robust tree to be very similar to the standard tree, which explains why there is little difference in performance (results not shown).  Figure 15: Experimental results in the unrestricted-merge model using a standard versus robust  average-linkage tree. Results presented for unpruned data sets.0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 A B C D E Number Edit Requests Data Set Average-Linkage Tree eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 A B C D E Number Edit Requests Data Set Robust Average-Linkage Tree eta = 0.5 eta = 0.6 eta = 0.7 eta = 0.8 eta = 0.9 eta = 1.0  "}, "Communication-efficient Sparse Regression": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-002.html", "header": "Communication-efficient Sparse Regression", "author": "Jason D. Lee, Qiang Liu, Yuekai Sun, Jonathan E. Taylor", "time": "18(5):1\u221230, 2017.", "abstract": "We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average  debiased  or  desparsified  lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models.", "pdf_url": "http://jmlr.org/papers/volume18/16-002/16-002.pdf", "keywords": ["Distributed Sparse Regression", "Averaging", "Debiasing", "lasso", "high-dimensional statistics"], "reference": "Alekh Agarwal, Sahand Negahban, Martin J Wainwright, et al. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40 (5):2452-2482, 2012.  Heather Battey, Jianqing Fan, Han Liu, and Junwei Lu. Splitotic analysis for distributed  estimation and hypothesis testing. preprint (personal communication), 2015.  Alexandre Belloni, Victor Chernozhukov, and Christian Hansen.  Inference for high-  dimensional sparse econometric models. arXiv preprint arXiv:1201.0220, 2011.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodru\ufb00. Commu- nication lower bounds for statistical estimation problems via a distributed data processing inequality. arXiv preprint arXiv:1506.07216, 2015.  28   Lee et al.  By Vershynin (2010), Proposition 5.16 and Lemma 23, it is possible to show that  (cid:13) (cid:13)  1 N  \u02c6\u0398X T (cid:15)(cid:13)  (cid:13)\u221e \u223c OP  (cid:16)(cid:16) log p  (cid:17) 1  2 (cid:17)  .  N  Thus the second term in (12) is of order (cid:0) log p N the stated conclusion.  (cid:1) 1  2 . We put all the pieces together to obtain  Proof [Proof of Theorem 20] Since \u02dc\u03b2ht \u2212 \u03b2\u2217 is 2s0-sparse,  or, equivalently,  By the triangle inequality,  (cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)2 2  (cid:46) s0(cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)2  (\u221e,c(cid:48)s0)  (cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)2 (cid:46)  s0(cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)(\u221e,c(cid:48)s0).  \u221a  (cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)(\u221e,c(cid:48)s0) \u2264 (cid:107) \u02dc\u03b2ht \u2212 \u02dc\u03b2(cid:107)(\u221e,c(cid:48)s0) + (cid:107) \u02dc\u03b2 \u2212 \u03b2\u2217(cid:107)(\u221e,c(cid:48)s0) \u2264 2(cid:107) \u02dc\u03b2 \u2212 \u03b2\u2217(cid:107)(\u221e,c(cid:48)s0),  where the second inequality is by the fact that thresholding at t = | \u02dc\u03b2|(c(cid:48)s0) minimizes (cid:107)\u03b2 \u2212 \u03b2\u2217(cid:107)(\u221e,c(cid:48)s0) over c(cid:48)s0-sparse points \u03b2. Thus  (cid:107) \u02dc\u03b2ht \u2212 \u03b2\u2217(cid:107)2 = OP  (cid:16)(cid:16) s0 log p  (cid:17) 1  2 +  s0 log p n  (cid:17)  .  N  To complete the proof, we observe that the estimation error bound of \u02dc\u03b2ht in the (cid:96)1 norm follows by the fact that \u02dc\u03b2ht \u2212 \u03b2\u2217 is 2s0-sparse.  References  Alekh Agarwal, Sahand Negahban, Martin J Wainwright, et al. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40 (5):2452-2482, 2012.  Heather Battey, Jianqing Fan, Han Liu, and Junwei Lu. Splitotic analysis for distributed  estimation and hypothesis testing. preprint (personal communication), 2015.  Alexandre Belloni, Victor Chernozhukov, and Christian Hansen.  Inference for high-  dimensional sparse econometric models. arXiv preprint arXiv:1201.0220, 2011.  Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1-122, 2011.  Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodru\ufb00. Commu- nication lower bounds for statistical estimation problems via a distributed data processing inequality. arXiv preprint arXiv:1506.07216, 2015. Communication-efficient Sparse Regression  Peter B\u00a8uhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods,  theory and applications. Springer, 2011.  Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. The Journal of Machine Learning Research, 13(1):165-202, 2012.  John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimization: convergence analysis and network scaling. Automatic Control, IEEE Trans- actions on, 57(3):592-606, 2012.  John C Duchi, Michael I Jordan, Martin J Wainwright, and Yuchen Zhang. Optimality guarantees for distributed statistical estimation. arXiv preprint arXiv:1405.0782, 2014.  Ankit Garg, Tengyu Ma, and Huy L Nguyen. Lower bound for high-dimensional statistical  learning problem via direct-sum theorem. arXiv preprint arXiv:1405.1665, 2014.  Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity:  the lasso and its generalizations. CRC Press, 2015.  Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for  high-dimensional regression. arXiv preprint arXiv:1306.3171, 2013a.  Adel Javanmard and Andrea Montanari. Nearly optimal sample size in hypothesis testing  for high-dimensional regression. arXiv preprint arXiv:1311.0274, 2013b.  Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S Mann. In E\ufb03cient large-scale distributed training of conditional maximum entropy models. Advances in Neural Information Processing Systems, pages 1231-1239, 2009.  Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538-557, 2012.  Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for  correlated gaussian designs. J. Mach. Learn. Res., 11:2241-2259, 2010.  Jonathan Rosenblatt and Boaz Nadler. On the optimality of averaging in distributed sta-  tistical learning. arXiv preprint arXiv:1407.2724, 2014.  Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements.  Information Theory, IEEE Transactions on, 59(6):3434-3447, 2013.  Sara van de Geer, Peter B\u00a8uhlmann, Ya\u2019acov Ritov, and Ruben Dezeure. On asymptoti- cally optimal confidence regions and tests for high-dimensional models. arXiv preprint arXiv:1303.0518, 2013.  Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv  preprint arXiv:1011.3027, 2010. Lee et al.  Cun-Hui Zhang and Stephanie S Zhang. Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):217-242, 2014.  Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-e\ufb03cient algo- rithms for statistical optimization. Journal of Machine Learning Research, 14:3321-3363, 2013.  Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2595- 2603, 2010. "}, "Improving Variational Methods via Pairwise Linear Response Identities": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-070.html", "header": "Improving Variational Methods via Pairwise Linear Response Identities", "author": "Jack Raymond, Federico Ricci-Tersenghi", "time": "18(6):1\u221236, 2017.", "abstract": "Inference methods are often formulated as variational approximations: these approxima- tions allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima.", "pdf_url": "http://jmlr.org/papers/volume18/16-070/16-070.pdf", "keywords": ["variational inference", "graphical models", "message passing algorithms", "statisti cal physics", "linear response"], "reference": "C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for  machine learning. Machine Learning, 50(1):5-43, 2003.  M. Chertkov and V.Y. Chernyak. Loop calculus in statistical physics and information  science. Phys. Rev. E, 73:065102, 2006.  P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks  is NP-hard. Artif. Intell., 60(1):141-153, March 1993.  R. Dickman and G. Stell. Self-consistent Ornstein-Zernike approximation for lattice gases.  Physical review letters, 77(6):996, 1996.  E. Dom\u00b4\u0131nguez, A. Lage-Castellanos, R. Mulet, F. Ricci-Tersenghi, and T. Rizzo. Char- acterizing and improving generalized belief propagation algorithms on the 2d Edwards- Anderson model. Journal of Statistical Mechanics: Theory and Experiment, 2011(12): P12007, 2011.  R.J. Giordano, T. Broderick, and M.I. Jordan. Linear response methods for accurate co- variance estimates from mean field variational Bayes. In Advances in Neural Information Processing Systems 28, pages 1441-1449. Curran Associates, Inc., Red Hook, NY, 2015.  T. Heskes, K. Albers, and B. Kappen. Approximate inference and constrained optimization. In Uncertainty in Artificial Intelligence: Proceedings of the Nineteenth Conference (UAI- 2003), pages 313-320, San Francisco, CA, 2003. Morgan Kaufmann Publishers.  33   Variational Methods and Linear Response  An exact expression for the correlation matrix \u03c7 as a function of the variational pa- rameters was derived in Raymond and Ricci-Tersenghi (2013b) and Raymond and Ricci- Tersenghi (2013a), the special case for homogeneous graphs is made explicit (Appendix A). The inverse covariance matrix \u03c7\u22121 is, in the basis where variational parameters are beliefs q, a linear function of \u03bb. Updating a single \u03bb can be achieved by the Shermann-Morrison formula, which in its linearized form becomes  \u03c7i,i \u03b4\u03bbi,j \u03c7j,j = (C\u2217  i,j \u2212 \u03c7i,j) .  (29)  It is also possible to write a similar expression blockwise (over \u03bb\u03b1). This scheme and variations were considered, but were found to require more iterations or stronger damping (for convergence) than the linearized cavity formula (20).  In our approach we associate each constraint to a unique region, but some statistics such as Vp(xi, xi) may be approximated by di\ufb00erent \u03b1 for the same variational approach (for example by qij or qik if i participates in two interactions). Though these estimates agree finally at the solution point, they disagree at intermediate stages of CLBP. Since updates for \u03bb may be correlated (consider Figure 8 where diagonal and o\ufb00-diagonal are strongly anti-correlated), the grouping of \u03bb may be very important. Updating non-disjoint sets and averaging over results may be more stable than our scheme where closely correlated \u03bb (for example \u03bbi,i and \u03bbi,j) may be updated on di\ufb00erent regions.  References  C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for  machine learning. Machine Learning, 50(1):5-43, 2003.  M. Chertkov and V.Y. Chernyak. Loop calculus in statistical physics and information  science. Phys. Rev. E, 73:065102, 2006.  P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks  is NP-hard. Artif. Intell., 60(1):141-153, March 1993.  R. Dickman and G. Stell. Self-consistent Ornstein-Zernike approximation for lattice gases.  Physical review letters, 77(6):996, 1996.  E. Dom\u00b4\u0131nguez, A. Lage-Castellanos, R. Mulet, F. Ricci-Tersenghi, and T. Rizzo. Char- acterizing and improving generalized belief propagation algorithms on the 2d Edwards- Anderson model. Journal of Statistical Mechanics: Theory and Experiment, 2011(12): P12007, 2011.  R.J. Giordano, T. Broderick, and M.I. Jordan. Linear response methods for accurate co- variance estimates from mean field variational Bayes. In Advances in Neural Information Processing Systems 28, pages 1441-1449. Curran Associates, Inc., Red Hook, NY, 2015.  T. Heskes, K. Albers, and B. Kappen. Approximate inference and constrained optimization. In Uncertainty in Artificial Intelligence: Proceedings of the Nineteenth Conference (UAI- 2003), pages 313-320, San Francisco, CA, 2003. Morgan Kaufmann Publishers. Raymond and Ricci-Tersenghi  J. S. H\u00f8ye and G. Stell. Thermodynamics of the MSA for simple \ufb02uids. J. Chem. Phys.,  67:439-445, July 1977. doi: 10.1063/1.434887.  H. Huang and Y. Kabashima. Adaptive Thouless-Anderson-Palmer approach to inverse  Ising problems with quenched random fields. Phys. Rev. E, 87:062129, Jun 2013.  H.J. Kappen and F.B. Rodriguez. E\ufb03cient learning in Boltzmann machines using linear  response theory. Neural Comput., 20:1137, 1998.  E. Kierlik, M.L. Rosinberg, and G. Tarjus. A self-consistent Ornstein-Zernike approxima- tion for the random field Ising model. Journal of Statistical Physics, 94(5-6):805-836, 1999.  A. Lage-Castellanos, R. Mulet, F. Ricci-Tersenghi, and T. Rizzo. Replica cluster variational method: the replica symmetric solution for the 2d random bond Ising model. Journal of Physics A: Mathematical and Theoretical, 46(13):135001, 2013.  P.M. Long and R.A. Servedio. Restricted Boltzmann machines are hard to approximately evaluate or simulate. In Proceedings of the 27th International Conference on Machine Learning, pages 703-710. Omnipress, 2010.  D.J. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge Univer-  sity Press, Cambridge, UK, 2004.  M. Mezard and A. Montanari. Information, Physics, and Computation. Oxford University  Press, Inc., New York, NY, USA, 2009.  M. Mezard and T. Mora. Constraint satisfaction problems and neural networks: a statistical  physics perspective. arXiv:0803.3061, 2008.  J. Miskin and D. J. C. MacKay. Ensemble Learning for Blind Image Separation and De-  convolution, pages 123-141. Springer London, London, 2000.  A. Montanari and T. Rizzo. How to compute loop corrections to the Bethe approximation.  J. Stat. Mech., page P10011, 2005.  A. Montanari, F. Ricci-Tersenghi, and G. Semerjian. Solving constraint satisfaction prob- lems through belief propagation-guided decimation. In 45th Annual Allerton Conference on Communication, Control and Computing, Red Hook, NY, USA, 2007. Curran Asso- ciates, Inc.  J.M. Mooij. libDAI: A free and open source c++ library for discrete approximate inference in graphical models. Journal of Machine Learning Research, 11:2169-2173, August 2010.  J.M. Mooij, B. Wemmenhove, B. Kappen, and T. Rizzo. Loop corrected belief propagation. In International Conference on Artificial Intelligence and Statistics, pages 331-338, 2007.  M. Opper and O. Winther. Tractable approximations for probabilistic models: The adaptive Thouless-Anderson-Palmer mean field approach. Phys. Rev. Lett., 86:3695-3699, 2001. Variational Methods and Linear Response  M. Opper and O. Winther. Variational linear response. In Advances in Neural Information  Processing Systems 16. MIT Press, Cambridge, MA, 2003.  M. Opper and O. Winther. Expectation consistent free energies for approximate inference. In Advances in Neural Information Processing Systems 17. MIT Press, Cambridge, MA, 2005.  M. Opper, U. Paquet, and O. Winther. Improving on expectation propagation. In Advances  in Neural Information Processing Systems 21. MIT Press, Cambridge, MA, 2009.  M. Opper, U. Paquet, and O. Winther. Perturbative corrections for approximate inference in Gaussian latent variable models. Journal of Machine Learning Research, 14:2857-2898, 2013.  U. Paquet, O. Winther, and M. Opper. Perturbation corrections in approximate inference: Mixture model applications. Journal of Machine Learning Research, 10:1263-1304, 2009.  G. Parisi. Statistical Field Theory. Addison-Wesley, Boston, MA, USA, 1987.  A. Pelizzola. Cluster variation method in statistical physics and probabilistic graphical  models. J. Phys. A, 38(33):R309, 2005.  J. Raymond and F. Ricci-Tersenghi. Correcting beliefs in the mean-field and Bethe ap- proximations using linear response. In Communications Workshops (ICC), 2013 IEEE International Conference on, pages 1429-1433, 2013a.  J. Raymond and F. Ricci-Tersenghi. Mean-field method with correlations determined by  linear response. Phys. Rev. E, 87:052111, 2013b.  T. Richardson and R. Urbanke. Modern Coding Theory. Cambridge University Press,  Cambridge, UK, 2008.  M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational  inference. Found. Trends Mach. Learn., 1(1-2):1-305, 2008.  M. Welling and Y.W. Teh. Linear response algorithms for approximate inference in graphical  models. Neural Comput., 16:197-221, 2004.  O. Winther and M. Opper. Expectation consistent approximate inference. Journal of  Machine Learning Research, 6:2177-2204, 2005.  M. Yasuda. An generalization of improved susceptibility propagation. In Inference, Com-  putation, and Spin Glasses (ICSG2013), 2013.  M. Yasuda and K. Tanaka. Susceptibility propagation by using diagonal consistency. Phys.  Rev. E, 87:012134, Jan 2013.  J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. Information Theory, IEEE Transactions on, 51(7):2282-2312, 2005. Raymond and Ricci-Tersenghi  A. L. Yuille. CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent  alternatives to belief propagation. Neural Computation, 14:2002, 2002. "}, "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-270.html", "header": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks", "author": "Adam S. Charles, Dong Yin, Christopher J. Rozell", "time": "18(7):1\u221237, 2017.", "abstract": "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low- dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.", "pdf_url": "http://jmlr.org/papers/volume18/16-270/16-270.pdf", "keywords": ["covery", "restricted isometry property"], "reference": "M. Aharon, M. Elad, A. Bruckstein, and Y. Katz. K-SVD: An algorithm for designing of overcom- plete dictionaries for sparse representations. IEEE Transactions on Signal Processing, 54(11): 4311-4322, 2006.  A. Ahmed and J. Romberg. Compressive multiplexing of correlated signals. IEEE Transactions on  Information Theory, 61(1):479-498, 2015.  32   CHARLES, YIN AND ROZELL  Since we wish to bound the square of the sum of terms, we calculate the square values of the  two terms in the Bernstein inequality. The first term is bounded by  t\u03c32  X \u2264 c\u03b2\u03ba  \u00b52  k\u22121  ||ql||2  2 + 2R\u00b52 0  log(LN ),  R M  (cid:18) N M  (cid:19)  and the second term is bounded by  t2U 2  \u03b1 log2  (cid:19)  (cid:18) |\u0393k|U 2 \u03b1 \u03c32 X  \u2264 t2U 2  \u03b1 log2  \uf8edc  \uf8eb  |\u0393k|M \u03ba2R\u00b52  k\u22121  M 2\u03baR\u00b52  k\u22121  (cid:16) N 2  M ||ql||2 (cid:16) N M ||ql||2  2 + 2R2\u00b54 0 (cid:17)  2 + 2R\u00b52 0  (cid:17)  \uf8f6  \uf8f8  \u2264 t2c\u03ba2 R  M 2 \u00b52  k\u22121  \u2264 c\u03b22\u03ba2 R  M 2 \u00b52  k\u22121  (cid:18) N 2 M (cid:18) N 2 M  ||ql||2  2 + 2R2\u00b54 0  log2  (cid:32) c  N 2 ||ql||2 N ||ql||2  2 + 2M R2\u00b54 0 2 + 2RM \u00b52 0  (cid:33)  (cid:19)  (cid:19)  ||ql||2  2 + 2R2\u00b54 0  log4 (LN ) ,  where the last step assumes L > 1 and LN > R\u00b54 0. Each summand is then bounded by the maximum of these two quantities with probability 1 \u2212 O(|\u0393k|(LN )\u2212\u03b2), the |\u0393k| term coming from the union bound over all terms in each inner sum.  Using this bound on each summand, we obtain the total bound by taking a union bound, sum-  ming over l \u2208 [1, \u00b7 \u00b7 \u00b7 , L], and dividing by R, yielding a bound of the maximum of  t\u03c32  X \u2264 c\u03b2\u03ba  \u00b52  k\u22121  + 2L\u00b54 0  log(LN ),  R M  (cid:18) N M  (cid:19)  and  t2U 2  \u03b1 log2  (cid:19)  (cid:18) |\u0393k|U 2 \u03b1 \u03c32 X  \u2264 \u03b22c\u03ba2 R  M 2 \u00b52  k\u22121  (cid:18) N M  (cid:19)  + 2RL\u00b52 0  log4 (LN ) ,  with probability 1 \u2212 O(M (N L)\u2212\u03b2). To complete the proof, we note that if we have  M \u2265 c\u03b2\u03baR (cid:0)N + L\u00b52 0  (cid:1) log2(LN ),  then both terms in this bound are less than \u00b52  k\u22121.  References  M. Aharon, M. Elad, A. Bruckstein, and Y. Katz. K-SVD: An algorithm for designing of overcom- plete dictionaries for sparse representations. IEEE Transactions on Signal Processing, 54(11): 4311-4322, 2006.  A. Ahmed and J. Romberg. Compressive multiplexing of correlated signals. IEEE Transactions on  Information Theory, 61(1):479-498, 2015. SEQUENCE MEMORY IN RECURRENT NETWORKS  A. Ahmed, B. Recht, and J. Romberg. Blind deconvolution using convex programming.  IEEE  Transactions on Information Theory, 60(3):1711-1732, 2014.  S.-I. Amari. Characteristics of random nets of analog neuron-like elements. IEEE Transactions on  Systems, Man and Cybernetics, (5):643-657, 1972.  N. J. Apthorpe, A. J. Riordan, R. E. Aguilar, J. Homann, Y. Gu, D. W. Tank, and S. H. Seung. Automatic neuron detection in calcium imaging data using convolutional networks. Advances in neural information processing systems (NIPS), pages 3270-3278, Dec. 2016.  A. Balavoine, J. Romberg, and C. J. Rozell. Convergence and rate analysis of neural networks for sparse approximation. IEEE Transactions on Neural Networks and Learning Systems, 23: 1377-1389, Sept. 2012.  P. Bashivan, I. Rish, M. Yeasin, and N. Codella. Learning representations from eeg with deep recurrent-convolutional neural networks. International Conference on Learning Representations, 2016.  A. Ber\u00b4enyi, Z. Somogyv\u00b4ari, A. J. Nagy, L. Roux, J. D. Long, S. Fujisawa, E. Stark, A. Leonardo, T. D. Harris, and G. Buzs\u00b4aki. Large-scale, high-density (up to 512 channels) recording of local circuits in behaving animals. Journal of neurophysiology, 111(5):1132-1149, 2014.  M. Buehner and P. Young. A tighter bound for the echo state property. IEEE Transactions on Neural  Networks, 17(3):820-824, 2006.  D. V Buonomano and W. Maass. State-dependent computations: spatiotemporal processing in  cortical networks. Nature Reviews Neuroscience, 10(2):113-125, 2009.  L. B\u00a8using, B. Schrauwen, and R. Legenstein. Connectivity, dynamics, and memory in reservoir  computing with binary and analog neurons. Neural computation, 22(5):1272-1311, 2010.  E. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925-936,  2010.  E. J. Cand`es and Y. Plan. A probabilistic and ripless theory of compressed sensing. IEEE Transac-  tions on Information Theory, 57(11):7235-7254, 2011a.  E. J. Cand`es and Y. Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57(4):2342- 2359, 2011b.  E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE  Transactions on Information Theory, 56(5):2053-2080, 2010.  E. J. Cand`es, J. Romberg, and Tao T. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52 (2):489-509, 2006.  A. S. Charles, P. Garrigues, and C. J. Rozell. A common network architecture efficiently implements a variety of sparsity-based inference problems. Neural Computation, 24(12):3317-3339, 2012. CHARLES, YIN AND ROZELL  A. S. Charles, H. L. Yap, and C. J. Rozell. Short-term memory capacity in networks via the restricted  isometry property. Neural computation, 26(6):1198-1235, 2014.  P. Chen and D. Suter. Recovering the missing components in a large noisy low-rank matrix: Applica- tion to sfm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1051-1063, 2004.  Y. Chen, Z. Lin, X. Zhao, G. Wang, and Y. Gu. Deep learning-based classification of hyperspectral data. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7(6): 2094-2107, 2014.  Y. Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi. Deep feature extraction and classification of hyperspectral images based on convolutional neural networks. IEEE Transactions on Geoscience and Remote Sensing, 54(10):6232-6251, 2016.  C.-K. Chiang, C.-H. Liu, C.-H. Duan, and S.-H. Lai. Learning component-level sparse represen- IEEE Transactions on Image Processing, 22(12):  tation for image and video categorization. 4775-4787, 2013.  C. Christopoulos, A. Skodras, and T. Ebrahimi. The jpeg2000 still image coding system: an  overview. IEEE transactions on consumer electronics, 46(4):1103-1127, 2000.  M. E. Davies and L. Daudet. Sparse audio representations using the mclt. Signal processing, 86(3):  457-470, 2006.  Science, 248(4951):73-6, 1990.  W. Denk, J. H. Strickler, and W. W. Webb. Two-photon laser scanning \ufb02uorescence microscopy.  J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar- rell. Long-term recurrent convolutional networks for visual recognition and description. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2625-2634, 2015.  M. Elad, M. A. T. Figueiredo, and Y. Ma. On the role of sparse and redundant representations in  image processing. Proceedings of the IEEE, 98(6)972-982, 2010.  O. Faugeras, J. Touboul, and B. Cessac. A constructive mean-field analysis of multi-population neural networks with random synaptic weights and stochastic inputs. Frontiers in computational neuroscience, 3(1):1-26, Feb. 2009.  M. Fazel. Matrix rank minimization with applications. PhD Thesis, Stanford University, 2002.  M. Galtier and G. Wainrib. A local echo state property through the largest Lyapunov exponent.  Neural Networks, 76:39-45, 2016.  S. Ganguli and H. Sompolinsky. Short-term memory in neuronal networks through dynamical compressed sensing. Advances in Neural Information Processing Systems (NIPS), pages 667- 675, 2010.  S. Ganguli, D. Huh, and H. Sompolinsky. Memory traces in dynamical systems. Proceedings of the  National Academy of Sciences, 105(48):18970, 2008. SEQUENCE MEMORY IN RECURRENT NETWORKS  Y. Gao, E. Archer, L. Paninski, and J. P. Cunningham. Linear dynamical neural population models through nonlinear embeddings. Advances in Neural Information Processing Systems (NIPS), pages 163-171, Dec. 2016.  F. Gobet, P. C. R. Lane, S. Croker, P. C. H. Cheng, G. Jones, I. Oliver, and J. M. Pine. Chunking  mechanisms in human learning. Trends in Cognitive Sciences, 5(6):236-243, 2001.  A. Graves, A.-R. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net- works. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645-6649, 2013.  K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image  generation. arXiv preprint arXiv:1502.04623, 2015.  D. Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on  Information Theory, 57(3):1548-1566, 2011.  T. Guha and R. K. Ward. Learning sparse representations for human action recognition. Transactions on Pattern Analysis and Machine Intelligence, 34(8):1576-1588, 2012.  IEEE  X. Hinaut, M. Petit, G. Pointeau, and P. F. Dominey. Exploring the acquisition and production of grammatical constructions through human-robot interaction with echo state networks. Frontiers in Neurorobotics, 8(16):1-17, 2014.  H. Jaeger. Short term memory in echo state networks. GMD Report 152 German National Research  Center for Information Technology, 5:1-60, 2001.  H. Jaeger and H. Haas. Harnessing nonlinearity: predicting chaotic systems and saving energy in  wireless communication. Science, 304(5667):78-80, 2004.  M. Jaggi, M. Sulovsk, et al. A simple algorithm for nuclear norm regularized problems. Interna-  tional Conference on Machine Learning (ICML), pages 471-478, 2010.  N. Kalchbrenner, A. Graves, and I. Danihelka. Grid long short-term memory. International Con-  ference on Learning Representations, 2016.  K. Kavukcuoglu, P. Sermanet, Y. L. Boureau, K. Gregor, M. Mathieu, and Y. LeCun. Learning con- volutional feature hierarchies for visual recognition. Advances in Neural Information Processing Systems (NIPS), pages 1090-1098, 2010.  Y. LeCun, K. Kavukcuoglu, C. Farabet, et al. Convolutional networks and applications in vision.  IEEE International Symposium on Circuits and Systems ISCAS, pages 253-256, 2010.  R. Legenstein and W. Maass. Edge of chaos and prediction of computational performance for neural  circuit models. Neural Networks, 20(3):323-334, 2007.  Z. Lipton, D. Kale, C. Elkan, and R. Wetzel. Learning to diagnose with lstm recurrent neural  networks. International Conference on Learning Representations, 2016. CHARLES, YIN AND ROZELL  Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application to system identification. SIAM Journal on Matrix Analysis and Applications, 31(3):1235-1256, 2009.  M. Luko\u02c7sevi\u02c7cius. A practical guide to applying echo state networks. Neural Networks: Tricks of  the Trade, pages 659-686. 2012.  M. Luko\u02c7sevi\u02c7cius and H. Jaeger. Reservoir computing approaches to recurrent neural network train-  ing. Computer Science Review, 3(3):127-149, 2009.  W. Maass, T. Natschl\u00a8ager, and H. Markram. Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Computation, 14(11):2531- 2560, 2002.  S. Mallat. Understanding deep convolutional networks. Philosophical Transactions of the Royal  Society A, 374(2065):20150203, 2016.  G. Manjunath and H. Jaeger. Echo state property linked to an input: Exploring a fundamental  characteristic of recurrent neural networks. Neural Computation, 25(3):671-696, 2013.  R. Maruyama, K. Maeda, H. Moroda, I. Kato, M. Inoue, H. Miyakawa, and T. Aonishi. Detecting cells using non-negative matrix factorization on calcium imaging data. Neural Networks, 55: 11-19, 2014.  M. Massar and S. Massar. Mean-field theory of echo state networks. Physical Review E, 87(4):  042809, 2013.  B. A. Olshausen and D. Field. Emergence of simple-cell receptive field properties by learning a  sparse code for natural images. Nature, 381(13):607-609, Jun 1996.  A. B. Patel, T. Nguyen, and R. G. Baraniuk. A probabilistic theory of deep learning. arXiv preprint  arXiv:1504.00641, 2015.  K. Rajan, L. F. Abbott, and H. Sompolinsky. Stimulus-dependent suppression of chaos in recurrent  neural networks. Physical Review E, 82(1):011903, 2010.  H. Rauhut. Compressive sensing and structured random matrices. Theoretical Found. and Numeri-  cal Methods for Sparse Recovery, 9:1-92, 2010.  B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equa-  tions via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.  C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural Computation, 20(10):2526-2563, Oct 2010.  H. Sak, A. Senior, and F. Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. Conference of International Speech Communication Associa- tion (INTERSPEECH), 2014.  S. Shapero, M. Zhu, P. Hasler, and C. J. Rozell. Optimal sparse approximation with integrate and  fire neurons. International Journal of Neural Systems, 24(05):1440001, Aaug 2014. SEQUENCE MEMORY IN RECURRENT NETWORKS  A. Singer and M. Cucuringu. Uniqueness of low-rank matrix completion by rigidity theory. SIAM  Journal on Matrix Analysis and Applications, 31(4):1621-1641, 2010.  H. Sompolinsky, A. Crisanti, and H. J. Sommers. Chaos in random neural networks. Physical  Review Letters, 61(3):259, 1988.  S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. Advances in Neural  Information Processing Systems (NIPS), pages 2431-2439.  K.-C. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized  linear least squares problems. Pacific Journal of Optimization, 6(615-640):15, 2010.  J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational  Mathematics, 12(4):389-434, 2012.  J.A. Tropp, J.N. Laska, M.F. Duarte, J.K. Romberg, and R.G. Baraniuk. Beyond Nyquist: efficient sampling of sparse bandlimited signals. IEEE Transactions on Information Theory, 56(1), Jan. 2009. ISSN 0018-9448.  V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events  to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971.  P. Vardan, Y. Romano, and M. Elad. Convolutional neural networks analyzed via convolutional  sparse coding. arXiv preprint arXiv:1607.08194, 2016.  M. A. Veganzones, M. Simoes, G. Licciardi, N. Yokoya, J.M. Bioucas-Dias, and J. Chanussot. Hyperspectral super-resolution of locally low rank images from complementary multisource data. IEEE Transactions on Image Processing, 25(1):274-288, 2016.  D. Verstraeten, B. Schrauwen, M. dHaene, and D. Stroobandt. An experimental unification of  reservoir computing methods. Neural Networks, 20(3):391-403, 2007.  D. Verstraeten, J. Dambre, X. Dutoit, and B. Schrauwen. Memory versus non-linearity in reservoirs. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1-8, 2010.  G. Wainrib. Context dependent representation in recurrent neural networks.  arXiv preprint  http://arxiv.org/pdf/1506.06602.pdf, 2015.  E. Wallace, R. M. Hamid, and P. E. Latham. Randomly connected networks have short temporal  memory. Neural Computation, 25:1408-1439, 2013.  O. L. White, D. D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural networks.  Physical Review Letters, 92(14):148102, 2004.  H. R. Wilson and J. D. Cowan. Excitatory and inhibitory interactions in localized populations of  model neurons. Biophysical Journal, 12(1):1, 1972.  I. B. Yildiz, H. Jaeger, and S. J. Kiebel. Re-visiting the echo state property. Neural networks, 35:  1-9, 2012.  H. Zhang, W. He, L. Zhang, H. Shen, and Q. Yuan. Hyperspectral image restoration using low- rank matrix recovery. IEEE Transactions on Geoscience and Remote Sensing, 52(8):4729-4743, 2014. "}, "Persistence Images: A Stable Vector Representation of Persistent Homology": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-337.html", "header": "Persistence Images: A Stable Vector Representation of Persistent Homology", "author": "Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, Lori Ziegelmeier", "time": "18(8):1\u221235, 2017.", "abstract": "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a  persistence diagram  (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite- dimensional vector representation which we call a  persistence image  (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the  linked twist map ) and a partial differential equation (the  anisotropic Kuramoto-Sivashinsky equation ) provide a novel application of the discriminatory power of PIs.", "pdf_url": "http://jmlr.org/papers/volume18/16-337/16-337.pdf", "keywords": ["topological data analysis", "persistent homology", "persistence images", "machine learning", "dynamical systems"], "reference": "Aaron Adcock, Erik Carlsson, and Gunnar Carlsson. The ring of algebraic functions on  persistence bar codes. Homology, Homotopy and Applications, 18(1):381-402, 2016.  Paul Bendich. Analyzing Stratified Spaces Using Persistent Versions of Intersection and  Local Homology. PhD thesis, Duke University, 2009.  Paul Bendich, James S. Marron, Ezra Miller, Alex Pieloch, and Sean Skwerer. Persistent  homology analysis of brain artery trees. Ann. Appl. Stat., 10(1):198-218, 2016.  Paul S. Bradley and Olvi L. Mangasarian. Feature selection via concave minimization and In Proceedings of the Fifteenth International Conference on  support vector machines. Machine Learning, pages 82-90, San Francisco, CA, 1998.  Peter Bubenik. Statistical topological data analysis using persistence landscapes. The Jour-  nal of Machine Learning Research, 16(1):77-102, 2015.  31   Persistence Images  Figure 11: SSVM-based feature (pixel) selection for H1 PIs from the six classes of the synthetic data. The parameters used are resolution 20 \u00d7 20 and variance 0.0001, for noise level 0.05. Selected pixels are marked by blue crosses. (a) A noisy solid cube with the two selected pixels (indices 59 and 79 out of 400). (b) A noisy torus with the two selected pixels (indices 59 and 98 out of 400). (c) A noisy sphere with the five selected pixels (indices 58, 59, 60, 79, and 98 out of 400). (d) Noisy three clusters with the one selected pixel (index 20 out of 400). (e) Noisy three clusters within three clusters with the seven selected pixels (indices 20, 40, 59, 60, 79, 80, and 98 out of 400). (f) A noisy circle with the two selected pixels (indices 21 and 22 out of 400).  References  Aaron Adcock, Erik Carlsson, and Gunnar Carlsson. The ring of algebraic functions on  persistence bar codes. Homology, Homotopy and Applications, 18(1):381-402, 2016.  Paul Bendich. Analyzing Stratified Spaces Using Persistent Versions of Intersection and  Local Homology. PhD thesis, Duke University, 2009.  Paul Bendich, James S. Marron, Ezra Miller, Alex Pieloch, and Sean Skwerer. Persistent  homology analysis of brain artery trees. Ann. Appl. Stat., 10(1):198-218, 2016.  Paul S. Bradley and Olvi L. Mangasarian. Feature selection via concave minimization and In Proceedings of the Fifteenth International Conference on  support vector machines. Machine Learning, pages 82-90, San Francisco, CA, 1998.  Peter Bubenik. Statistical topological data analysis using persistence landscapes. The Jour-  nal of Machine Learning Research, 16(1):77-102, 2015. Adams, et al.  Peter Bubenik and Pawel Dlotko. A persistence landscapes toolbox for topological statistics.  Journal of Symbolic Computation, 2016. Accepted.  Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46  (2):255-308, 2009.  Mathieu Carri\u00e8re, Steve Y. Oudot, and Maks Ovsjanikov. Stable topological signatures for  points on 3d shapes. In Computer Graphics Forum, volume 34, pages 1-12, 2015.  Fr\u00e9d\u00e9ric Chazal, Vin de Silva, and Steve Oudot. Persistence stability for geometric com-  plexes. Geometriae Dedicata, 173(1):193-214, 2014.  Yen-Chi Chen, Daren Wang, Alessandro Rinaldo, and Larry Wasserman. Statistical analysis  of persistence intensity functions. arXiv preprint arXiv:1510.02502, 2015.  Sofya Chepushtanova, Christopher Gittins, and Michael Kirby. Band selection in hyper- spectral imagery using sparse support vector machines. In Proceedings SPIE DSS 2014, volume 9088, pages 90881F-90881F15, 2014.  Moo K. Chung, Peter Bubenik, and Peter T. Kim. Persistence diagrams of cortical surface  data. In Information Processing in Medical Imaging, pages 386-397. Springer, 2009.  David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence dia-  grams. Discrete & Computational Geometry, 37(1):103-120, 2007.  David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz functions have Lp-stable persistence. Foundations of computational mathematics, 10(2): 127-139, 2010.  Rodolfo Cuerno and Albert-L\u00e1sl\u00f3 Barab\u00e1si. Dynamic scaling of ion-sputtered surfaces.  Physical Review Letters, 74:4746, 1995.  Yu Dabaghian, Facundo Memoli, Loren Frank, and Gunnar Carlsson. A topological paradigm for hippocampal spatial map formation using persistent homology. PLoS Com- putational Biology, 8(8):e1002581, 2012.  Barbara Di Fabio and Massimo Ferri. Comparing persistence diagrams through complex In International Conference on Image Analysis and Processing 2015 Part I;  vectors. Editors V. Murino, E. Puppo, LNCS 9279, pages 294-305, 2015.  Pietro Donatini, Patrizio Frosini, and Alberto Lovato. Size functions for signature recog- nition. In SPIE\u2019s International Symposium on Optical Science, Engineering, and Instru- mentation, pages 178-183, 1998.  Herbert Edelsbrunner and John Harer. Persistent homology - a survey. Contemporary  Mathematics, 453:257-282, 2008.  Herbert Edelsbrunner and John Harer. Computational topology: An introduction. American  Mathematical Society, 2010. Persistence Images  Herbert Edelsbrunner, Alexandr Ivanov, and Roman Karasev. Current open problems in discrete and computational geometry. Modelirovanie i Analiz Informats. Sistem, 19(5): 5-17, 2012.  Tegan Emerson, Michael Kirby, Kelly Bethel, Anand Kolatkar, Madelyn Luttgen, Stephen O\u2019Hara, Paul Newton, and Peter Kuhn. Fourier-ring descriptor to characterize rare circu- lating cells from images generated using immuno\ufb02uorescence microscopy. Computerized Medical Imaging and Graphics, 40:70-87, 2015.  Massimo Ferri and Claudia Landi. Representing size functions by complex polynomials.  Proc. Math. Met. in Pattern Recognition, 9:16-19, 1999.  Massimo Ferri, Patrizio Frosini, Alberto Lovato, and Chiara Zambelli. Point selection: A new comparison scheme for size functions (with an application to monogram recognition). In Computer Vision ACCV\u201998, pages 329-337. Springer, 1997.  Glenn M. Fung and Olvi L. Mangasarian. A feature selection newton method for support vector machine classification. Computational Optimization and Applications, 28(2):185- 202, 2004.  Robert Ghrist. Barcodes: The persistent topology of data. Bulletin of the American Math-  ematical Society, 45(1):61-75, 2008.  Alexander A. Golovin and Stephen H. Davis. E\ufb00ect of anisotropy on morphological instabil- ity in the freezing of a hypercooled melt. Physica D: Nonlinear Phenomena, 116:363-391, 1998.  Zhenhua Guo, Lei Zhang, and David Zhang. A completed modeling of local binary pattern operator for texture classification. IEEE Transactions on Image Processing, 19(6):1657- 1663, 2010.  Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002.  Kyle Heath, Natasha Gelfand, Maks Ovsjanikov, Mridul Aanjaneya, and Leonidas J Guibas. Image webs: Computing and exploiting connectivity in image collections. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.  Jan-Martin Hertzsch, Rob Sturman, and Stephen Wiggins. DNA microarrays: Design prin-  ciples for maximizing ergodic, chaotic mixing. Small, 3(2):202-218, 2007.  Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, 20(8):832-844, 1998.  Leonard Kaufman and Peter Rousseeuw. Clustering by means of medoids. North-Holland,  1987.  Michael Kerber, Dmitriy Morozov, and Arnur Nigmetov. Geometry helps to compare persis- tence diagrams. In 2016 Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments (ALENEX), pages 103-112, 2016. Adams, et al.  Yuriy Mileyko, Sayan Mukherjee, and John Harer. Probability measures on the space of  persistence diagrams. Inverse Problems, 27(12):124007, 2011.  Francis C. Motta, Patrick D. Shipman, and R. Mark Bradley. Highly ordered nanoscale surface ripples produced by ion bombardment of binary compounds. Journal of Physics D: Applied Physics, 45(12):122001, 2012.  Deepti Pachauri, Christian Hinrichs, Moo K. Chung, Sterling C. Johnson, and Vikas Singh. Topology-based kernels with application to inference problems in Alzheimer\u2019s disease. IEEE Transactions on Medical Imaging, 30(10):1760-1770, 2011.  Hae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for k-medoids clustering.  Expert Systems with Applications, 36(2):3336-3341, 2009.  Daniel A. Pearson, R. Mark Bradley, Francis C. Motta, and Patrick D. Shipman. Pro- ducing nanodot arrays with improved hexagonal order by patterning surfaces before ion sputtering. Phys. Rev. E, 92:062401, Dec 2015.  Jose A. Perea and John Harer. Sliding windows and persistence: An application of topolog- ical methods to signal analysis. Foundations of Computational Mathematics, pages 1-40, 2013.  Jan Reininghaus, Stefan Huber, Ulrich Bauer, and Roland Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4741-4748, 2015.  Martin Rost and Joachim Krug. Anisotropic Kuramoto-Sivashinsky equation for surface  growth and erosion. Physical Review Letters, 75:3894, 1995.  David Rouse, Adam Watkins, David Porter, John Harer, Paul Bendich, Nate Strawn, Eliz- abeth Munch, Jonathan DeSena, Jesse Clarke, Je\ufb00rey Gilbert, Peter Chin, and Andrew Newman. Feature-aided multiple hypothesis tracking using topological and statistical behavior classifiers. In SPIE Proceedings, volume 9474, page 94740L, 2015.  Gurjeet Singh, Facundo Memoli, Tigran Ishkhanov, Guillermo Sapiro, Gunnar Carlsson, and Dario L. Ringach. Topological analysis of population activity in visual cortex. Journal of Vision, 8(8):11, 2008.  Chad M. Topaz, Lori Ziegelmeier, and Tom Halverson. Topological data analysis of biological  aggregation models. PloS One, 10(5):e0126383, 2015.  Katharine Turner, Yuriy Mileyko, Sayan Mukherjee, and John Harer. Fr\u00e9chet means for distributions of persistence diagrams. Discrete & Computational Geometry, 52(1):44-70, 2014.  Sara Kali\u0161nik Verov\u0161ek. Tropical coordinates on the space of persistence barcodes. arXiv  preprint arXiv:1604.00113, 2016.  J. Villain. Continuum models of crystal growth from atomic beams with and without des-  orption. J. Phys. I France, 1:19-42, 1991. Persistence Images  Dietrich E. Wolf. Kinetic roughening of vicinal surfaces. Physical Review Letters, 67:1783,  1991.  Matthias Zeppelzauer, Bartosz Zieli\u0144ski, Mateusz Juda, and Markus Seidl. Topological In Computational Topology in Image Context: 6th  descriptors for 3d surface analysis. International Workshop Proceedings, pages 77-87, Marseille, France, 2016.  Li Zhang and Weida Zhou. On the sparseness of 1-norm support vector machines. Neural  Networks, 23(3):373-385, 2010.  Ji Zhu, Saharon Rosset, Trevor Hastie, and Rob Tibshirani. 1-norm support vector machines.  Advances in neural information processing systems, 16(1):49-56, 2004.  Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Com-  putational Geometry, 33(2):249-274, 2005. "}, "On Perturbed Proximal Gradient Algorithms": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-038.html", "header": "On Perturbed Proximal Gradient Algorithms", "author": "Yves F. Atchad\u00c3\u00a9, Gersende Fort, Eric Moulines", "time": "18(10):1\u221233, 2017.", "abstract": "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non- asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.", "pdf_url": "http://jmlr.org/papers/volume18/15-038/15-038.pdf", "keywords": ["Proximal Gradient Methods; Stochastic Optimization; Monte Carlo approximations; Perturbed Majorization-Minimization algorithms."], "reference": "S. Allassonni`ere and E. Kuhn. Convergent Stochastic Expectation Maximization algorithm with e\ufb03cient sampling in high dimension. Application to deformable template model estimation. Comput. Stat. Data An., 91:4-19, 2015.  C. Andrieu and E. Moulines. On the ergodicity properties of some adaptive MCMC  algorithms. Ann. Appl. Probab., 16(3):1462-1505, 2006.  C. Andrieu, E. Moulines, and P. Priouret. Stability of stochastic approximation under  verifiable conditions. SIAM J. Control Optim., 44(1):283-312, 2005.  O. Banerjee, L. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. J. Mach. Learn. Res., 9:485-516, 2008.  H. Bauschke and P.L. Combettes. Convex analysis and monotone operator theory in Hilbert spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011. ISBN 978-1-4419-9466-0. With a foreword by H\u00b4edy Attouch.  A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal- recovery problems. In Convex optimization in signal processing and communica- tions, pages 42-88. Cambridge Univ. Press, Cambridge, 2010.  A. Benveniste, M. M\u00b4etivier, and P. Priouret. Adaptive algorithms and stochastic approximations, volume 22 of Applications of Mathematics (New York). Springer- Verlag, Berlin, 1990.  P. Biane, J. Pitman, and M. Yor. Probability laws related to the Jacobi theta and Riemann zeta functions, and Brownian excursions. Bull. Amer. Math. Soc. (N.S.), 38(4):435-465 (electronic), 2001. ISSN 0273-0979.  H.M. Choi and J. P. Hobert. The polya-gamma gibbs sampler for bayesian logistic regression is uniformly ergodic. Electronic Journal of Statistics, 7:2054-2064, 2013.  P.L. Combettes. Inherently parallel Algorithms in Feasibility and Optimization and their Applications, chapter Quasi-Fejerian analysis of some optimization algorithms, pages 115-152. Elsevier Science, 2001.  P.L. Combettes and J.C. Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, volume 49 of Springer Optim. Appl., pages 185-212. Springer, New York, 2011.  P.L. Combettes and J.C. Pesquet. Stochastic Quasi-Fejer block-coordinate fixed point  iterations with random sweeping. SIAM J. Optim., 25(2):1221-1248, 2015a.  P.L. Combettes and J.C. Pesquet.  Stochastic Approximations and Perturba- tions in Forward-Backward Splitting for Monotone Operators. Technical report, arXiv:1507.07095v1, 2015b.  30   Atchad\u00b4e, Fort and Moulines  References  S. Allassonni`ere and E. Kuhn. Convergent Stochastic Expectation Maximization algorithm with e\ufb03cient sampling in high dimension. Application to deformable template model estimation. Comput. Stat. Data An., 91:4-19, 2015.  C. Andrieu and E. Moulines. On the ergodicity properties of some adaptive MCMC  algorithms. Ann. Appl. Probab., 16(3):1462-1505, 2006.  C. Andrieu, E. Moulines, and P. Priouret. Stability of stochastic approximation under  verifiable conditions. SIAM J. Control Optim., 44(1):283-312, 2005.  O. Banerjee, L. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. J. Mach. Learn. Res., 9:485-516, 2008.  H. Bauschke and P.L. Combettes. Convex analysis and monotone operator theory in Hilbert spaces. CMS Books in Mathematics/Ouvrages de Math\u00b4ematiques de la SMC. Springer, New York, 2011. ISBN 978-1-4419-9466-0. With a foreword by H\u00b4edy Attouch.  A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal- recovery problems. In Convex optimization in signal processing and communica- tions, pages 42-88. Cambridge Univ. Press, Cambridge, 2010.  A. Benveniste, M. M\u00b4etivier, and P. Priouret. Adaptive algorithms and stochastic approximations, volume 22 of Applications of Mathematics (New York). Springer- Verlag, Berlin, 1990.  P. Biane, J. Pitman, and M. Yor. Probability laws related to the Jacobi theta and Riemann zeta functions, and Brownian excursions. Bull. Amer. Math. Soc. (N.S.), 38(4):435-465 (electronic), 2001. ISSN 0273-0979.  H.M. Choi and J. P. Hobert. The polya-gamma gibbs sampler for bayesian logistic regression is uniformly ergodic. Electronic Journal of Statistics, 7:2054-2064, 2013.  P.L. Combettes. Inherently parallel Algorithms in Feasibility and Optimization and their Applications, chapter Quasi-Fejerian analysis of some optimization algorithms, pages 115-152. Elsevier Science, 2001.  P.L. Combettes and J.C. Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, volume 49 of Springer Optim. Appl., pages 185-212. Springer, New York, 2011.  P.L. Combettes and J.C. Pesquet. Stochastic Quasi-Fejer block-coordinate fixed point  iterations with random sweeping. SIAM J. Optim., 25(2):1221-1248, 2015a.  P.L. Combettes and J.C. Pesquet.  Stochastic Approximations and Perturba- tions in Forward-Backward Splitting for Monotone Operators. Technical report, arXiv:1507.07095v1, 2015b. On Perturbed Proximal Gradient Algorithms  P.L. Combettes and V. Wajs. Signal recovery by proximal forward-backward splitting.  Multiscale Modeling and Simulation, 4(4):1168-1200, 2005.  A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gradient methods. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Pro- cessing Systems 24, pages 1647-1655. Curran Associates, Inc., 2011.  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning ISSN  and stochastic optimization. J. Mach. Learn. Res., 12:2121-2159, 2011. 1532-4435.  M. Ekeberg, C. L\u00a8ovkvist, Y. Lan, M. Weigt, and E. Aurell. Improved contact predic- tion in proteins: Using pseudolikelihoods to infer potts models. Phys. Rev. E, 87: 012707, 2013.  G. Fort and E. Moulines. Convergence of the Monte Carlo expectation maximization for curved exponential families. Ann. Statist., 31(4):1220-1259, 2003. ISSN 0090- 5364.  G. Fort, E. Moulines, and P. Priouret. Convergence of adaptive and interacting Markov chain Monte Carlo algorithms. Ann. Statist., 39(6):3262-3289, 2011. ISSN 0090-5364.  G. Fort, E. Moulines, M. Vihola, and A. Schreck. Convergence of Markovian Stochas- tic Approximation with discontinuous dynamics. Technical report, arXiv math.ST 1403.6803, 2014.  G. Fort, B. Jourdain, E. Kuhn, T. Leli`evre, and G. Stoltz. Convergence of the Wang-  Landau algorithm. Mathematics of Computation, 84:2297-2327, 2015.  C.J. Geyer. On the convergence of Monte Carlo maximum likelihood calculations. J.  Roy. Statist. Soc. Ser. B, 56(1):261-274, 1994.  J. Guo, E. Levina, G. Michailidis, and J. Zhu. Joint structure estimation for categor-  ical Markov networks. Technical report, Univ. of Michigan, 2010.  P. Hall and C.C. Heyde. Martingale Limit Theory and its Application. Academic  Press, 1980.  H. H\u00a8o\ufb02ing and R. Tibshirani. Estimation of sparse binary pairwise Markov networks  using pseudo-likelihoods. J. Mach. Learn. Res., 10:883-906, 2009.  C. Hu, W. Pan, and J.T. Kwok. Accelerated gradient methods for stochastic opti- mization and online learning. In Y. Bengio, D. Schuurmans, J. La\ufb00erty, C. K. I Williams, and A. Culotta, editors, Advances in Neural Information Processing Sys- tems, pages 781-789, 2009.  A. Juditsky and A. Nemirovski. First-order methods for nonsmooth convex large-scale optimization, i: General purpose methods. In S. Sra, S. Nowozin, and S. Wright, editors, Oxford Handbook of Innovation, pages 121-146. MIT Press, Boston, 2012a. Atchad\u00b4e, Fort and Moulines  A. Juditsky and A. Nemirovski. First-order methods for nonsmooth convex large- scale optimization, ii: Utilizing problem\u2019s structure. In S. Sra, S. Nowozin, and S. Wright, editors, Oxford Handbook of Innovation, pages 149-181. MIT Press, Boston, 2012b.  H. Kamisetty, S. Ovchinnikov, and D. Baker. Assessing the utility of coevolution- based residue-residue contact predictions in a sequence-and structure-rich era. Pro- ceedings of the National Academy of Sciences, 2013. doi: 10.1073/pnas.1314045110.  G. Lan. An optimal method for stochastic composite optimization. Math. Program.,  133(1-2, Ser. A):365-397, 2012. ISSN 0025-5610.  J. Lin, L. Rosasco, S. Villa, and D.X. Zhou. Modified Fejer Sequences and Applica-  tions. Technical report, arXiv:1510:04641v1 math.OC, 2015.  G.J. McLachlan and T. Krishnan. The EM algorithms and Extensions. Wiley-  Interscience; 2 edition, 2008.  S. Meyn and R.L. Tweedie. Markov chains and stochastic stability. Cambridge Uni- versity Press, Cambridge, second edition, 2009. ISBN 978-0-521-73182-9. With a prologue by Peter W. Glynn.  J. Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien.  CR Acad. Sci. Paris S\u00b4er. A Math, 255:2897-2899, 1962.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):1574-1609, 2008. ISSN 1052-6234.  Y.E. Nesterov. Introductory Lectures on Convex Optimization, A basic course. Kluwer  Academic Publishers, 2004.  A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1574-1582. Curran Associates, Inc., 2014.  H.E. Ogden. A sequential reduction method for inference in generalized linear mixed  models. Electron. J. Statist., 9(1):135-152, 2015.  N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimiza-  tion, 1(3):123-231, 2013.  N. G. Polson, J. G. Scott, and J. Windle. Bayesian inference for logistic models using Polya-Gamma latent variables. J. Am. Stat. Assoc., 108(504):1339-1349, 2013.  B.T. Polyak. Introduction to Optimization. xx, 1987.  P. Ravikumar, M.J. Wainwright, and J.D. La\ufb00erty. High-dimensional Ising model selection using (cid:96)1-regularized logistic regression. Ann. Statist., 38(3):1287-1319, 2010. On Perturbed Proximal Gradient Algorithms  C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Texts in  Statistics. Springer; 2nd edition, 2005.  L. Rosasco, S. Villa, and B.C. Vu. Convergence of a Stochastic Proximal Gradient  Algorithm. Technical report, arXiv:1403.5075v3, 2014.  L. Rosasco, S. Villa, and B.C. Vu. A Stochastic Inertial Forward-Backward Splitting Algorithm for multi-variate monotone inclusions. Technical report, arXiv:1507.00848v1, 2015.  E. Saksman and M. Vihola. On the ergodicity of the adaptive Metropolis algorithm  on unbounded domains. Ann. Appl. Probab., 20(6):2178-2203, 2010.  J. Schelldorfer, L. Meier, and P. B\u00a8uhlmann. GLMMLasso: an algorithm for high- dimensional generalized linear mixed models using (cid:96)1-penalization. J. Comput. Graph. Statist., 23(2):460-477, 2014.  M.W. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal- gradient methods for convex optimization. In NIPS, pages 1458-1466, 2011. see also the technical report INRIA-00618152.  A. Schreck, G. Fort, and E. Moulines. Adaptive Equi-energy sampler : conver- gence and illustration. ACM Transactions on Modeling and Computer Simulation (TOMACS), 23(1):Art 5., 2013.  J. Shao. Mathematical Statistics. Springer texts in Statistics, 2003.  L. Xiao. Dual averaging methods for regularized stochastic learning and online opti-  mization. J. Mach. Learn. Res., 11:2543-2596, 2010. ISSN 1532-4435.  L. Xiao and T. Zhang. A Proximal Stochastic Gradient Method with Progressive  Variance Reduction. SIAM J. Optim., 24:2057-2075, 2014.  L. Xue, H. Zou, and T. Cai. Non-concave penalized composite likelihood estimation  of sparse ising models. Ann. Statist., 40:1403-1429, 2012. "}, "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-449.html", "header": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns", "author": "Herbert Jaeger", "time": "18(13):1\u221243, 2017.", "abstract": "Biological brains can learn, recognize, organize, and re- generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called  conceptors , which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and  focussed . Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content- addressed in ways that are analog to recalling static patterns in Hopfield networks.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume18/15-449/15-449.pdf", "keywords": ["Recurrent neural network", "temporal pattern learning", "neural long-term mem ory", "neural dynamics"], "reference": "S.-I. Amari. Learning patterns and pattern sequences by self-organizing nets of threshold  elements. IEEE Trans. on Computers, C-21(11):1197-1207, 1972.  A. Billard and G. Hayes. DRAMA, a connectionist architecture for control and learning in  autonomous robots. Adaptive Behavior, 7(1):35-63, 1999.  K. J. Bostr\u00a8om, H. Wagner, M. Prieske, and M. de Lussanet. Model for a \ufb02exible motor memory based on a self-active recurrent neural network. Human Movement Science, 32: 880-898, 2013.  B. Burger and P. Toiviainen. MoCap Toolbox - A Matlab toolbox for computational analysis of movement data. In Roberto Bresin, editor, 10th Sound and Music Com- puting Conference, pages 172-178, Stockholm, Sweden, 2013. KTH Royal Institute of Technology. URL https://www.jyu.fi/hum/laitokset/musiikki/en/research/coe/ materials/mocaptoolbox.  CMU Graphics Lab. Motion Capture Database. URL http://mocap.cs.cmu.edu/. re-  trieved Feb 2013.  A. Coates, P. Abbeel, and A. Y. Ng. Learning for control from multiple demonstrations. In  Proc. 25th ICML, Helsinki, 2008.  L. N. Cooper. A possible organization of animal memory and learning. In B. Lundqvist and S. Lundqvist, editors, Collective properties of physical systems: Medicine and Natural Sciences, Nobel Symposia on Medicine and Natural Sciences, pages 252-264. Academic Press, New York and London, 1973.  R. Douglas and T. Sejnowski. Future challenges for the sciene and engineering of learning: Final workshop report. Technical report, National Science Foundation, 2008. URL http: //www.nsf.gov/sbe/SLCWorkshopReportjan08.pdf.  D. Durstewitz, J. K. Seamans, and T. J. Sejnowski. Neurocomputational models of working  memory. Nature Neuroscience, 3:1184-91, 2000.  C. Eliasmith. A unified approach to building and controlling spiking attractor networks.  Neural Computation, 17:1276-1314, 2005.  R. M. French. Catastrophic interference in connectionist networks.  In L. Nadel, editor, Encyclopedia of Cognitive Science, volume 1, pages 431-435. Nature Publishing Group, 2003.  S. Fusi and X.-J. Wang. Long-term, short-term and working memory. In M. Arbib and J. Bonaiuto, editors, From Neuron to Cognition via Computational Neuroscience, chap- ter 11, pages 319-344. MIT Press, 2016.  M. Gillies and B. Spanlang. Comparing and evaluating real time character engines for  virtual environments. Presence, 19(2):95-117, 2010.  40   Jaeger  References  S.-I. Amari. Learning patterns and pattern sequences by self-organizing nets of threshold  elements. IEEE Trans. on Computers, C-21(11):1197-1207, 1972.  A. Billard and G. Hayes. DRAMA, a connectionist architecture for control and learning in  autonomous robots. Adaptive Behavior, 7(1):35-63, 1999.  K. J. Bostr\u00a8om, H. Wagner, M. Prieske, and M. de Lussanet. Model for a \ufb02exible motor memory based on a self-active recurrent neural network. Human Movement Science, 32: 880-898, 2013.  B. Burger and P. Toiviainen. MoCap Toolbox - A Matlab toolbox for computational analysis of movement data. In Roberto Bresin, editor, 10th Sound and Music Com- puting Conference, pages 172-178, Stockholm, Sweden, 2013. KTH Royal Institute of Technology. URL https://www.jyu.fi/hum/laitokset/musiikki/en/research/coe/ materials/mocaptoolbox.  CMU Graphics Lab. Motion Capture Database. URL http://mocap.cs.cmu.edu/. re-  trieved Feb 2013.  A. Coates, P. Abbeel, and A. Y. Ng. Learning for control from multiple demonstrations. In  Proc. 25th ICML, Helsinki, 2008.  L. N. Cooper. A possible organization of animal memory and learning. In B. Lundqvist and S. Lundqvist, editors, Collective properties of physical systems: Medicine and Natural Sciences, Nobel Symposia on Medicine and Natural Sciences, pages 252-264. Academic Press, New York and London, 1973.  R. Douglas and T. Sejnowski. Future challenges for the sciene and engineering of learning: Final workshop report. Technical report, National Science Foundation, 2008. URL http: //www.nsf.gov/sbe/SLCWorkshopReportjan08.pdf.  D. Durstewitz, J. K. Seamans, and T. J. Sejnowski. Neurocomputational models of working  memory. Nature Neuroscience, 3:1184-91, 2000.  C. Eliasmith. A unified approach to building and controlling spiking attractor networks.  Neural Computation, 17:1276-1314, 2005.  R. M. French. Catastrophic interference in connectionist networks.  In L. Nadel, editor, Encyclopedia of Cognitive Science, volume 1, pages 431-435. Nature Publishing Group, 2003.  S. Fusi and X.-J. Wang. Long-term, short-term and working memory. In M. Arbib and J. Bonaiuto, editors, From Neuron to Cognition via Computational Neuroscience, chap- ter 11, pages 319-344. MIT Press, 2016.  M. Gillies and B. Spanlang. Comparing and evaluating real time character engines for  virtual environments. Presence, 19(2):95-117, 2010. Managing neural memory  B. Goodrich and I. Arel. Unsupervised neuron selection for mitigating catastrophic for- getting in neural networks. In Proc. IEEE 57th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 997-1000. IEEE, 2014.  S. Grossberg. Linking attention to learning, expectation, competition, and consciousness. In L. Itti, G. Rees, and J. Tsotsos, editors, Neurobiology of attention, chapter 107, pages 652-662. San Diego: Elsevier, 2005.  X. Hinaut and P. F. Dominey. A three-layered model of primate prefrontal cortex encodes identity and abstract categorical structure of behavioral sequences. J. Physiology - Paris, 105(1-3):16-24, 2011.  J. J. Hopfield. Neural networks and physical systems with emergent collective computational  abilities. Proc. Natl. Acad. Sci. USA, 79:2554-2558, 1982.  J. Huang and M. Hagiwara. A combined multi-winner multidirectional associative memory.  Neurocomputing, 48:369-389, 2002.  H. Jaeger. The \u201decho state\u201d approach to analysing and training recurrent neural networks. GMD Report 148, GMD - German National Research Institute for Computer Science, 2001. URL http://minds.jacobs-university.de/pubs.  H. Jaeger. Reservoir self-control for achieving invariance against slow input distortions.  technical report 23, Jacobs University Bremen, 2010.  H. Jaeger. Long short-term memory in echo state networks: Details of a simulation study. Technical Report 27, Jacobs University Bremen, 2012. URL http://minds. jacobs-university.de/pubs.  H. Jaeger. Controlling recurrent neural networks by conceptors. Technical Report 31, Jacobs  University Bremen, 2014. arXiv:1403.3369.  X. Jiang, V. Gripon, C. Berrou, and M. Rabbat. Storing sequences in binary tournament- based neural networks. IEEE Trans. on Neural Networks and Learning Systems, 27(5): 913-925, 2016.  M. I. Jordan. Serial order: a parallel distributed processing approach. In J. W. Donahoe and V. Packard Dorsel, editors, Neural-Networks Models of Cognition, chapter 25, pages 471-495. Elsevier, 1997. Abridged reprint of a technical report from 1986.  T. Kohonen. An adaptive associative memory principle. IEEE Transactions on Computers,  23(4):444-445, 1974.  J. F. Kolen and J. B. Pollack. Multiassociative memory. In Proc. of the Thirteenth Annual  Conference of the Cognitive Science Society, pages 785-789, 1991.  A. F. Krause, V. D\u00a8urr, B. Bl\u00a8(a)sing, and T. Schack. Evolutionary optimization of echo state networks: multiple motor pattern learning. In ANNIIP - 6th International Workshop on Artificial Neural Networks and Intelligent Information Processing, 2010. Jaeger  R. Laje and D. V. Buonomano. Robust timing and motor patterns by taming chaos in  recurrent neural networks. Nature Neuroscience, 16(7):925-933, 2013.  L. Lukic, J. Santos-Victor, and A. Billard. Learning coupled dynamical systems from human demonstration for robotic eye-arm-hand coordination. In IEEE-RAS International Conference on Humanoid Robots, Osaka 2012, 2012.  M. Lukosevicius. A practical guide to applying echo state networks.  In K.-R. M\u00a8uller, G. Montavon, and G. Orr, editors, Neural Networks Tricks of the Trade, Reloaded, LNCS, pages 659-686. Springer Verlag, 2012.  G. Manjunath and H. Jaeger. Echo state property linked to an input: Exploring a funda- mental characteristic of recurrent neural networks. Neural Computation, 25(3):671-696, 2013.  N. M. Mayer and M. Browne. Echo state networks and self-prediction.  In Biologically Inspired Approaches to Advanced Information Technology, volume 3141 of LNCS, pages 40-48. Springer Verlag Berlin / Heidelberg, 2004.  E. Oja. A simplified neuron model as a principal component analyzer. J. Math. Biol., 15:  267-273, 1982.  R. W. Paine and J. Tani. How hierarchical control self-organizes in artificial adaptive  systems. Adaptive Behaviour, 13(3):211-225, 2005.  G. Palm. On associative memory. Biol. Cybernetics, 36(1):19-31, 1980.  C. Pehlevan, T. Hu, and D. B. Chklovskii. A Hebbian/anti-Hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data. Neural Computation, 27(7):1461-1495, 2015.  J. B. Pollack. Recursive distributed representations. Artificial Intelligence, 46(1-2):77-105,  1990.  F. R. Reinhart and J. J. Steil. Recurrent neural associative learning of forward and inverse kinematics for movement generation of the redundant pa-10 robot. In A. Stoica, E. Tunsel, T. Huntsberger, T. Arslan, S. Vijayakumar, and A. O. El-Rayis, editors, LAB-RS 2008, vol. 1, pages 35-40, 2008.  R. F. Reinhart and J. J. Steil. A constrained regularization approach for input-driven recurrent neural networks. Di\ufb00erential Equations and Dynamical Systems, 19(1-2):27- 46, 2011. DOI 10.1007/s12591-010-0067-x is an 2010 online pre-publication.  G. J. Rinkus. A combinatorial neural network exhibiting episodic and semantic memory  properties for spatiotemporal patterns. Phd thesis, Boston University, 1996.  H. Ritter and T. Kohonen. Self-organizing semantic maps. Biological Cybernetics, 61:  241-254, 1989. Managing neural memory  L. Shastri. Advances in Shruti - a neurally motivated model of relational knowledge repre- sentation and rapid inference using temporal synchrony. Artificial Intelligence, 11:79-108, 1999.  H. Sompolinsky and I. Kanter. Temporal association in asymmetric neural networks. Phys-  ical Review Letters, 57(22):2861-2864, 1986.  D. Sussillo and L. Abbott. Transferring learning from external to internal weights in echo-  state networks with sparse connectivity. PLoS ONE, 7(5):e37372, 2012.  I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal restricted boltzmann machine. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21 (NIPS 08), pages 1601-1608, 2009.  I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks.  In ICML 2011 (online), 2011. URL http://www.icml-2011.org/papers.php.  G. W. Taylor, G. E. Hinton, and S. T. Roweis. Two distributed-state models for generating high-dimensional time series. Journal of Machine Learning Research, 12(March):1025- 1068, 2011.  D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins. Non-holographic associative  memory. Nature, 222(June 7):960-962, 1969.  J. Wright and I. Jordanov.  In WCCI 2012 IEEE World Congress on Computational Intelligence, pages 1-8, 2012. doi: dx.doi.org/10. 1109/IJCNN.2012.6252537.  Intelligent approaches in locomotion.  F. wy\ufb00els and B. Schrauwen. Design of a central pattern generator using reservoir computing for learning human motion. In Advanced Technologies for Enhanced Quality of Life, 2009. AT-EQUAL \u201909, pages 118-122. IEEE, 2009. "}, "Automatic Differentiation Variational Inference": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-107.html", "header": "Automatic Differentiation Variational Inference", "author": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei", "time": "18(14):1\u221245, 2017.", "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new mo", "pdf_url": "http://jmlr.org/papers/volume18/16-107/16-107.pdf", "keywords": ["Bayesian inference", "approximate inference", "probabilistic programming"], "reference": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je\ufb00rey Dean, Matthieu Devin, Sanjay Ghemawat, Geo\ufb00rey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensor\ufb02ow: a system for large-scale machine learning. arXiv:1605.08695, 2016.  Shun-Ichi Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):251-276,  1998.  Charles E Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric  problems. The Annals of Statistics, 2(6):1152-1174, 1974.  David Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.  Atilim Gunes Baydin, Barak A Pearlmutter, and Alexey Andreyevich Radul. Automatic di\ufb00erentiation  in machine learning: a survey. arXiv:1502.05767, 2015.  Jos\u00e9 M Bernardo and Adrian FM Smith. Bayesian Theory. John Wiley & Sons, 2009.  Christopher M Bishop. Pattern Recognition and Machine Learning. Springer New York, 2006.  Christopher M Bishop, David Spiegelhalter, and John Winn. VIBES: a variational inference engine  for Bayesian networks. In Neural Information Processing Systems, 2002.  David M Blei. Build, compute, critique, repeat: data analysis with latent variable models. Annual  Review of Statistics and Its Application, 1:203-232, 2014.  David M Blei, Alp Kucukelbir, and Jon D McAuli\ufb00e. Variational inference: a review for statisticians.  arXiv:1601.00670, 2016.  Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In Proceedings of The 32nd International Conference on Machine Learning, pages 1613-1622, 2015.  L\u00e9on Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pages  421-436. Springer, 2012.  John Canny. GaP: a factor model for discrete data. In SIGIR Conference on Research and Development  in Information Retrieval, 2004.  Bob Carpenter, Matthew D Ho\ufb00man, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betancourt. The Stan math library: reverse-mode automatic di\ufb00erentiation in C++. arXiv:1509.07164, 2015.  Ali Taylan Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational  Intelligence and Neuroscience, 2009.  Edward Challis and David Barber. A\ufb03ne independent variational inference. In Neural Information  Processing Systems, 2012.  41   Automatic Differentiation Variational Inference  References  Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je\ufb00rey Dean, Matthieu Devin, Sanjay Ghemawat, Geo\ufb00rey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensor\ufb02ow: a system for large-scale machine learning. arXiv:1605.08695, 2016.  Shun-Ichi Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):251-276,  1998.  Charles E Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric  problems. The Annals of Statistics, 2(6):1152-1174, 1974.  David Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.  Atilim Gunes Baydin, Barak A Pearlmutter, and Alexey Andreyevich Radul. Automatic di\ufb00erentiation  in machine learning: a survey. arXiv:1502.05767, 2015.  Jos\u00e9 M Bernardo and Adrian FM Smith. Bayesian Theory. John Wiley & Sons, 2009.  Christopher M Bishop. Pattern Recognition and Machine Learning. Springer New York, 2006.  Christopher M Bishop, David Spiegelhalter, and John Winn. VIBES: a variational inference engine  for Bayesian networks. In Neural Information Processing Systems, 2002.  David M Blei. Build, compute, critique, repeat: data analysis with latent variable models. Annual  Review of Statistics and Its Application, 1:203-232, 2014.  David M Blei, Alp Kucukelbir, and Jon D McAuli\ufb00e. Variational inference: a review for statisticians.  arXiv:1601.00670, 2016.  Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In Proceedings of The 32nd International Conference on Machine Learning, pages 1613-1622, 2015.  L\u00e9on Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pages  421-436. Springer, 2012.  John Canny. GaP: a factor model for discrete data. In SIGIR Conference on Research and Development  in Information Retrieval, 2004.  Bob Carpenter, Matthew D Ho\ufb00man, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betancourt. The Stan math library: reverse-mode automatic di\ufb00erentiation in C++. arXiv:1509.07164, 2015.  Ali Taylan Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational  Intelligence and Neuroscience, 2009.  Edward Challis and David Barber. A\ufb03ne independent variational inference. In Neural Information  Processing Systems, 2012. Kucukelbir, Tran, Ranganath, Gelman and Blei  Edward Challis and David Barber. Gaussian Kullback-Leibler approximate inference. The Journal of  Machine Learning Research, 14(1):2239-2286, 2013.  Erhan \u00c7\u0131nlar. Probability and Stochastics. Springer, 2011.  Samantha R Cook, Andrew Gelman, and Donald B Rubin. Validation of software for Bayesian models using posterior quantiles. Journal of Computational and Graphical Statistics, pages 675-692, 2006.  Persi Diaconis and Donald Ylvisaker. Conjugate priors for exponential families. The Annals of  Statistics, 7(2):269-281, 1979.  estimation. arXiv:1410.8516, 2014.  Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components  Jan Drugowitsch. Variational Bayesian inference for linear and logistic regression. arXiv:1310.5438,  2013.  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.  Kai Fan, Ziteng Wang, Je\ufb00 Beck, James Kwok, and Katherine A Heller. Fast second order stochastic backpropagation for variational inference. In Neural Information Processing Systems, 2015.  Andrew Gelman and Jennifer Hill. Data Analysis using Regression and Multilevel/Hierarchical  Models. Cambridge University Press, 2006.  Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.  Bayesian Data Analysis. CRC Press, 2013.  Noah D Goodman and Andreas Stuhlm\u00fcller. The Design and Implementation of Probabilistic  Programming Languages, 2014.  Noah D Goodman, Vikash K Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B Tenenbaum.  Church: a language for generative models. In Uncertainty in Artificial Intelligence, 2008.  Wolfgang H\u00e4rdle and L\u00e9opold Simar. Applied Multivariate Statistical Analysis. Springer, 2012.  Matthew D Ho\ufb00man and Andrew Gelman. The No-U-Turn sampler. The Journal of Machine  Learning Research, 15(1):1593-1623, 2014.  Matthew D Ho\ufb00man, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.  The Journal of Machine Learning Research, 14(1):1303-1347, 2013.  Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to  variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.  Mohammad E Khan, Pierre Baqu\u00e9, Fran\u00e7ois Fleuret, and Pascal Fua. Kullback-Leibler proximal  variational inference. In Neural Information Processing Systems, 2015.  Sangjoon Kim, Neil Shephard, and Siddhartha Chib. Stochastic volatility: likelihood inference and  comparison with ARCH models. The Review of Economic Studies, 65(3):361-393, 1998. Automatic Differentiation Variational Inference  Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference  on Learning Representations, 2014.  Diederik P Kingma and Jimmy Ba Adam. A method for stochastic optimization. In International  Conference on Learning Representation, 2015.  David A Knowles. Stochastic gradient variational Bayes for Gamma approximating distributions.  arXiv:1509.01631, 2015.  Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David Blei. Automatic variational inference  in Stan. In Neural Information Processing Systems, 2015.  D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401  (6755):788-791, 1999.  Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: reverse-mode di\ufb00erentiation of  native Python. In ICML Workshop on Automatic Machine Learning, 2015.  Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. In  Neural Information Processing Systems, 2015.  Vikash Mansinghka, Daniel Selsam, and Yura Perov. Venture: a higher-order probabilistic program-  ming platform with programmable inference. arXiv:1404.0099, 2014.  Renate Meyer, David A Fournier, and Andreas Berg. Stochastic volatility: Bayesian computation using automatic di\ufb00erentiation and the extended Kalman filter. The Econometrics Journal, 6(2): 408-420, 2003.  T. Minka, J.M. Winn, J.P. Guiver, S. Webster, Y. Zaykov, B. Yangel, A. Spengler, and J. Bronskill. Infer.NET 2.6, 2014. Microsoft Research Cambridge. http://research.microsoft.com/infernet.  Kevin P Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012.  Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:  113-162, 2011.  David J Olive. Statistical Theory and Inference. Springer, 2014.  Manfred Opper and C\u00e9dric Archambeau. The variational Gaussian approximation revisited. Neural  Computation, 21(3):786-792, 2009.  Avi Pfe\ufb00er. Figaro: an object-oriented probabilistic programming language. Charles River Analytics  Technical Report, 137, 2009.  Jos\u00e9 C Pinheiro and Douglas M Bates. Unconstrained parametrizations for variance-covariance  matrices. Statistics and Computing, 6(3):289-296, 1996.  Martyn Plummer. JAGS: a program for analysis of Bayesian graphical models using Gibbs sampling.  In International Workshop on Distributed Statistical Computing, 2003. Kucukelbir, Tran, Ranganath, Gelman and Blei  Thomas Pock, Michael Pock, and Horst Bischof. Algorithmic di\ufb00erentiation: application to variational problems in computer vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(7):1180-1193, 2007.  Rajesh Ranganath, Chong Wang, Blei David, and Eric Xing. An adaptive learning rate for stochastic  variational inference. In International Conference on Machine Learning, 2013.  Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference. In Artificial  Intelligence and Statistics, 2014.  Rajesh Ranganath, Dustin Tran, and David M Blei. Hierarchical variational models. In International  Conference on Machine Learning, 2016.  Danilo Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. In International  Conference on Machine Learning, 2015.  Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, 2014.  Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical  Statistics, 1951.  Christian P Robert and George Casella. Monte Carlo Statistical Methods. Springer, 1999.  Francisco JR Ruiz, Michalis K Titsias, and David M Blei. The generalized reparameterization  gradient. In Neural Information Processing Systems, 2016a.  Francisco JR Ruiz, Michalis K Titsias, and David M Blei. Overdispersed black-box variational  inference. In Uncertainty in Artificial Intelligence, 2016b.  Tim Salimans and David Knowles. On using control variates with stochastic approximation for  variational Bayes. arXiv:1401.1022, 2014.  John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming in Python  using PyMC3. PeerJ Computer Science, 2:e55, 2016.  Matthias Seeger. Gaussian covariance and scalable variational inference. In International Conference  on Machine Learning, 2010.  David J Spiegelhalter, Andrew Thomas, Nicky G Best, and Wally R Gilks. BUGS: Bayesian inference  using Gibbs sampling, version 0.50. MRC Biostatistics Unit, Cambridge, 1995.  Stan Development Team. Stan Modeling Language Users Guide and Reference Manual, 2016.  Theano Development Team. Theano: a Python framework for fast computation of mathematical  expressions. arXiv:1605.02688, 2016.  Tijmen Tieleman and Geo\ufb00rey Hinton. Lecture 6.5-rmsprop: divide the gradient by a running average  of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.  Michalis Titsias and Miguel L\u00e1zaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate  inference. In International Conference on Machine Learning, 2014. Automatic Differentiation Variational Inference  Dustin Tran, Alp Kucukelbir, Adji B Dieng, Maja Rudolph, Dawen Liang, and David M Blei. Edward:  a library for probabilistic modeling, inference, and criticism. arXiv:1610.09787, 2016a.  Dustin Tran, Rajesh Ranganath, and David M Blei. The variational Gaussian process. In International  Conference on Learning Representations, 2016b.  Richard E Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. In Workshop on Inference and Estimation in Probabilistic Time-Series Models, 2008.  Mauricio Villegas, Roberto Paredes, and Bart Thomee. Overview of the ImageCLEF 2013 Scalable  Concept Image Annotation Subtask. In CLEF Evaluation Labs and Workshop, 2013.  Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational  inference. Foundations and Trends in Machine Learning, 1(1-2):1-305, 2008.  Bernard Widrow and Michael A Lehr. 30 years of adaptive neural networks: perceptron, madaline,  and backpropagation. Proceedings of the IEEE, 78(9):1415-1442, 1990.  David Wingate and Theophane Weber. Automated variational inference in probabilistic programming.  arXiv:1301.1299, 2013.  Research, 6:661-694, 2005.  John Winn and Christopher M Bishop. Variational message passing. Journal of Machine Learning  Frank Wood, Jan Willem van de Meent, and Vikash Mansinghka. A new approach to probabilistic  programming inference. In Artificial Intelligence and Statistics, 2014. "}, "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles": {"volumn": 18, "url": "http://jmlr.org/papers/v18/14-467.html", "header": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles", "author": "Yann Ollivier, Ludovic Arnold, Anne Auger, Nikolaus Hansen", "time": "18(18):1\u221265, 2017.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume18/14-467/14-467.pdf", "keywords": ["black-box optimization", "stochastic optimization", "randomized optimization", "natural gradient", "invariance", "evolution strategy", "information-geometric optimization"], "reference": "2008.  D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for Boltzmann machines.  Cognitive Science, 9(1):147-169, 1985.  R.P. Agarwal and D. O\u2019Regan. An Introduction to Ordinary Differential Equations. Springer,  Y. Akimoto and Y. Ollivier. Objective improvement in information-geometric optimization. In F. Neumann and K. DeJong, editors, Foundations of Genetic Algorithms XII (FOGA 2013), Adelaide, Australia, 2013.  Y. Akimoto, Y. Nagata, I. Ono, and S. Kobayashi. Bidirectional relation between CMA evolution strategies and natural evolution strategies. In Proceedings of Parallel Problem  59   Information-Geometric Optimization  \ud835\udf15\ud835\udc56 ln \ud835\udc43\ud835\udf03(\ud835\udc65). So the covariance matrix of \u02dc\u2207 ln \ud835\udc43\ud835\udf03(\ud835\udc65) is E\ud835\udc65\u223c\ud835\udc43\ud835\udf03 (\ud835\udf15\ud835\udc56 ln \ud835\udc43\ud835\udf03(\ud835\udc65)\ud835\udf15\ud835\udc57 ln \ud835\udc43\ud835\udf03(\ud835\udc65)), which is equal to the Fisher matrix by definition. So this covariance matrix is the identity, whose largest eigenvalue is 1. This proves Proposition 20.  For Corollary 21, by the relationship (2) between Fisher matrix and Kullback-Leibler divergence, if \ud835\udc63 is the speed of the IGO flow then the Kullback-Leibler divergence between \ud835\udc43\ud835\udf03\ud835\udc61 and \ud835\udc43\ud835\udf03\ud835\udc61+\ud835\udeff\ud835\udc61 (where \ud835\udc43\ud835\udf03\ud835\udc61+\ud835\udeff\ud835\udc61 is the trajectory of the IGO flow after a time \ud835\udeff\ud835\udc61) is equal to the square norm of \ud835\udeff\ud835\udc61.\ud835\udc63 in Fisher metric up to an \ud835\udc42(\u2016\ud835\udeff\ud835\udc61.\ud835\udc63\u20163) term. Now if \ud835\udc43\ud835\udf03\ud835\udc61+\ud835\udeff\ud835\udc61 is obtained by a finite-population IGO algorithm, by Theorem 6 the actual \ud835\udc63 from the IGO algorithm differs from the speed of the IGO flow by an \ud835\udc5c(1)\ud835\udc41 \u2192\u221e term. Collecting terms, we find the expression in Corollary 21.  D.7 Proof of Proposition 22 (Noisy IGO)  On the one hand, let \ud835\udc43\ud835\udf03 be a family of distributions on \ud835\udc4b. The IGO algorithm (16) applied to a random function \ud835\udc53 (\ud835\udc65) = \u02dc\ud835\udc53 (\ud835\udc65, \ud835\udf14) where \ud835\udf14 is a random variable uniformly distributed in [0, 1] reads  \ud835\udf03\ud835\udc61+\ud835\udeff\ud835\udc61 = \ud835\udf03\ud835\udc61 + \ud835\udeff\ud835\udc61  ^\ud835\udc64\ud835\udc56 \u0303\ufe00\u2207\ud835\udf03 ln \ud835\udc43\ud835\udf03(\ud835\udc65\ud835\udc56)  \ud835\udc41 \u2211\ufe01  (77)  \ud835\udc56=1 where \ud835\udc65\ud835\udc56 \u223c \ud835\udc43\ud835\udf03 and ^\ud835\udc64\ud835\udc56 is according to (14) where ranking is applied to the values \u02dc\ud835\udc53 (\ud835\udc65\ud835\udc56, \ud835\udf14\ud835\udc56), with \ud835\udf14\ud835\udc56 uniform variables in [0, 1] independent from \ud835\udc65\ud835\udc56 and from each other.  On the other hand, for the IGO algorithm using \ud835\udc43\ud835\udf03 \u2297\ud835\udc48[0,1] and applied to the deterministic function \u02dc\ud835\udc53 , ^\ud835\udc64\ud835\udc56 is computed using the ranking according to the \u02dc\ud835\udc53 values of the sampled points \u02dc\ud835\udc65\ud835\udc56 = (\ud835\udc65\ud835\udc56, \ud835\udf14\ud835\udc56), and thus coincides with the one in (77).  Besides,  \ud835\udf15\ud835\udf03 ln \ud835\udc43\ud835\udf03\u2297\ud835\udc48[0,1](\u02dc\ud835\udc65\ud835\udc56) = \ud835\udf15\ud835\udf03 ln \ud835\udc43\ud835\udf03(\ud835\udc65\ud835\udc56) + \ud835\udf15\ud835\udf03 ln \ud835\udc48[0,1](\ud835\udf14\ud835\udc56)    \u23df   \u23de =0  and thus, both the vanilla gradients and the Fisher matrix \ud835\udc3c (given by the tensor square of the vanilla gradients) coincide. This proves that the IGO algorithm update on space \ud835\udc4b \u00d7 [0, 1], using the family of distributions \u02dc\ud835\udc43\ud835\udf03 = \ud835\udc43\ud835\udf03 \u2297 \ud835\udc48[0,1], applied to the deterministic function \u02dc\ud835\udc53 , coincides with (77).  References  2008.  D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for Boltzmann machines.  Cognitive Science, 9(1):147-169, 1985.  R.P. Agarwal and D. O\u2019Regan. An Introduction to Ordinary Differential Equations. Springer,  Y. Akimoto and Y. Ollivier. Objective improvement in information-geometric optimization. In F. Neumann and K. DeJong, editors, Foundations of Genetic Algorithms XII (FOGA 2013), Adelaide, Australia, 2013.  Y. Akimoto, Y. Nagata, I. Ono, and S. Kobayashi. Bidirectional relation between CMA evolution strategies and natural evolution strategies. In Proceedings of Parallel Problem Ollivier, Arnold, Auger and Hansen  Solving from Nature - PPSN XI, volume 6238 of Lecture Notes in Computer Science, pages 154-163. Springer, 2010.  Y. Akimoto, A. Auger, and N. Hansen. Convergence of the continuous time trajectories of isotropic evolution strategies on monotonic \ud835\udc9e2-composite functions. In C.A. Coello Coello, V. Cutello, K. Deb, S. Forrest, G. Nicosia, and M. Pavone, editors, PPSN (1), volume 7491 of Lecture Notes in Computer Science, pages 42-51. Springer, 2012. ISBN 978-3-642-32936-4.  S.-I. Amari. Natural gradient works efficiently in learning. Neural Comput., 10:251-276, February 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL http://portal. acm.org/citation.cfm?id=287476.287477.  S.-I. Amari and H. Nagaoka. Methods of information geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, 2000. ISBN 0-8218-0531-2. Translated from the 1993 Japanese original by Daishi Harada.  D.V. Arnold. Weighted multirecombination evolution strategies. Theoretical computer  science, 361(1):18-37, 2006.  S. Baluja. Population based incremental learning: A method for integrating genetic search based function optimization and competitve learning. Technical Report CMU-CS-94-163, Carnegie Mellon Report, 1994.  S. Baluja and R. Caruana. Removing the genetics from the standard genetic algorithm. In  Proceedings of ICML\u201995, pages 38-46, 1995.  Y. Bengio, P. Lamblin, V. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 153-160. MIT Press, Cambridge, MA, 2007.  Y. Bengio, A.C. Courville, and P. Vincent. Unsupervised feature learning and deep learning:  A review and new perspectives. ArXiv preprints, arXiv:1206.5538, 2012.  J. Bensadon. Black-box optimization using geodesics in statistical manifolds. Entropy, 17  (1):304-345, 2015.  A. Berny. Selection and reinforcement learning for combinatorial optimization. In M. Schoe- nauer, K. Deb, G. Rudolph, X. Yao, E. Lutton, J. Merelo, and H.-P. Schwefel, editors, Parallel Problem Solving from Nature PPSN VI, volume 1917 of Lecture Notes in Computer Science, pages 601-610. Springer Berlin Heidelberg, 2000a.  A. Berny. An adaptive scheme for real function optimization acting as a selection operator. In Combinations of Evolutionary Computation and Neural Networks, 2000 IEEE Symposium on, pages 140 -149, 2000b. doi: 10.1109/ECNN.2000.886229.  A. Berny. Boltzmann machine for population-based incremental learning. In ECAI, pages  198-202, 2002. Information-Geometric Optimization  H.-G. Beyer. The Theory of Evolution Strategies. Natural Computing Series. Springer-Verlag,  2001.  H.-G. Beyer and H.-P. Schwefel. Evolution strategies\u2014a comprehensive introduction. Natural  computing, 1(1):3-52, 2002. ISSN 1567-7818.  P. Billingsley. Probability and measure. Wiley Series in Probability and Mathematical Statistics. John Wiley & Sons Inc., New York, third edition, 1995. ISBN 0-471-00710-2. A Wiley-Interscience Publication.  J. Branke, C. Lode, and J.L. Shapiro. Addressing sampling errors and diversity loss in UMDA. In Proceedings of the 9th annual conference on Genetic and evolutionary computation, pages 508-515. ACM, 2007.  T.M. Cover and J.A. Thomas. Elements of information theory. Wiley-Interscience [John Wiley & Sons], Hoboken, NJ, second edition, 2006. ISBN 978-0-471-24195-9; 0-471-24195-4.  S. Das, S. Maity, B.-Y. Qu, and P.N. Suganthan. Real-parameter evolutionary multimodal optimization - a survey of the state-of-the-art. Swarm and Evolutionary Computation, 1 (2):71-88, 2011.  P.-T. de Boer, D.P. Kroese, S. Mannor, and R.Y. Rubinstein. A tutorial on the cross-entropy  method. Annals OR, 134(1):19-67, 2005.  G. Desjardins, A. Courville, Y. Bengio, P. Vincent, and O. Dellaleau. Parallel tempering for training of restricted Boltzmann machines. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.  P. Deuflhard. Newton methods for nonlinear problems: affine invariance and adaptive  algorithms, volume 35. Springer, 2011.  E.D. Dolan and J.J. Mor\u00e9. Benchmarking optimization software with performance profiles.  Mathematical programming, 91(2):201-213, 2002.  S. Bhatnagar E. Zhou. Gradient-based adaptive stochastic search for simulation optimization  over continuous space. ArXiv preprints, arXiv:1608.00663, 2016.  M. Gallagher and M. Frean. Population-based continuous optimization, probabilistic mod- elling and mean shift. Evol. Comput., 13(1):29-42, January 2005. ISSN 1063-6560. doi: 10.1162/1063656053583478. URL http://dx.doi.org/10.1162/1063656053583478.  Z. Ghahramani. Unsupervised learning. In O. Bousquet, U. von Luxburg, and G. R\u00e4tsch, ed- itors, Advanced Lectures on Machine Learning, volume 3176 of Lecture Notes in Computer Science, pages 72-112. Springer Berlin / Heidelberg, 2004.  T. Glasmachers, T. Schaul, Y. Sun, D. Wierstra, and J. Schmidhuber. Exponential nat- ural evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolutionary computation GECCO\u201910, pages 393-400. ACM, 2010. Ollivier, Arnold, Auger and Hansen  J. Grahl, S. Minner, and F. Rothlauf. Behaviour of umda c with truncation selection on monotonous functions. In Evolutionary Computation, 2005. The 2005 IEEE Congress on, volume 3, pages 2553-2559. IEEE, 2005.  N. Hansen. An analysis of mutative \ud835\udf0e-self-adaptation on linear fitness functions. Evolutionary  Computation, 14(3):255-275, 2006a.  N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano, P. Larranaga, I. Inza, and E. Bengoetxea, editors, Towards a new evolutionary computation. Advances on estimation of distribution algorithms, pages 75-102. Springer, 2006b.  N. Hansen. Benchmarking a BI-population CMA-ES on the BBOB-2009 function testbed. In Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, GECCO \u201909, pages 2389-2396, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-505-5. doi: http://doi.acm.org/10.1145/ 1570256.1570333. URL http://doi.acm.org/10.1145/1570256.1570333.  N. Hansen and A. Auger. Principled design of continuous stochastic search: From theory to practice. In Y. Borenstein and A. Moraglio, editors, Theory and Principled Methods for the Design of Metaheuristics, Natural Computing Series, pages 145-180. Springer Berlin Heidelberg, 2014. ISBN 978-3-642-33205-0. doi: 10.1007/978-3-642-33206-7_8. URL http://dx.doi.org/10.1007/978-3-642-33206-7_8.  N. Hansen and S. Kern. Evaluating the CMA evolution strategy on multimodal test functions. In X. Yao et al., editors, Parallel Problem Solving from Nature PPSN VIII, volume 3242 of LNCS, pages 282-291. Springer, 2004.  N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. In ICEC96, pages 312-317. IEEE Press, 1996.  N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution  strategies. Evolutionary Computation, 9(2):159-195, 2001.  N. Hansen, S.D. M\u00fcller, and P. Koumoutsakos. Reducing the time complexity of the deran- domized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11(1):1-18, 2003. ISSN 1063-6560.  G.R. Harik, F.G. Lobo, and D.E. Goldberg. The compact genetic algorithm. Evolutionary  Computation, IEEE Transactions on, 3(4):287-297, 1999.  G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural  Computation, 14:1771-1800, 2002.  G.E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets.  Neural Conputation, 18:1527-1554, 2006.  R. Hooke and T.A. Jeeves. \u201cDirect search\u201d solution of numerical and statistical problems.  Journal of the ACM, 8:212-229, 1961. Information-Geometric Optimization  G.A. Jastrebski and D.V. Arnold. Improving evolution strategies through active covariance matrix adaptation. In Evolutionary Computation, 2006. CEC 2006. IEEE Congress on, pages 2814-2821. IEEE, 2006. ISBN 0780394879.  M. Jebalia and A. Auger. Log-linear convergence of the scale-invariant (\ud835\udf07/\ud835\udf07\ud835\udc64, \ud835\udf06)-ES and optimal \ud835\udf07 for intermediate recombination for large population sizes. In R. Schae- fer et al., editor, Parallel Problem Solving from Nature (PPSN XI), volume 6239, pages 52-61. Springer, 2010. URL http://hal.inria.fr/docs/00/49/44/78/PDF/ ppsn2010JebaliaAuger.pdf.  H. Jeffreys. An invariant form for the prior probability in estimation problems. Proc. Roy.  Soc. London. Ser. A., 186:453-461, 1946. ISSN 0962-8444.  H.K. Khalil. Nonlinear Systems. Nonlinear Systems. Prentice-Hall, Inc., second edition,  1996.  P.E. Kloeden and E. Platen. Numerical solution of stochastic differential equations, volume 23 of Applications of Mathematics (New York). Springer-Verlag, Berlin, 1992. ISBN 3-540- 54062-8.  S. Kullback. Information theory and statistics. Dover Publications Inc., Mineola, NY, 1997.  ISBN 0-486-69684-7. Reprint of the second (1968) edition.  P. Larranaga and J.A. Lozano. Estimation of distribution algorithms: A new tool for  evolutionary computation. Springer Netherlands, 2002. ISBN 0792374665.  N. Le Roux, P.-A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm.  In NIPS, 2007.  L. Malag\u00f2, M. Matteucci, and B. Dal Seno. An information geometry perspective on estimation of distribution algorithms: boundary analysis. In GECCO (Companion), pages 2081-2088, 2008.  L. Malag\u00f2, M. Matteucci, and G. Pistone. Towards the geometry of estimation of distribution algorithms based on the exponential family. In H.-G. Beyer and W.B. Langdon, editors, FOGA, Proceedings, pages 230-242. ACM, 2011. ISBN 978-1-4503-0633-1.  G. Montavon and K.-R. M\u00fcller. Deep boltzmann machines and the centering trick. In G. Montavon, G.B. Orr, and K.-R. M\u00fcller, editors, Neural Networks: Tricks of the Trade (2nd ed.), volume 7700 of Lecture Notes in Computer Science, pages 621-637. Springer, 2012. ISBN 978-3-642-35288-1.  J.J. Mor\u00e9, B.S. Garbow, and K.E. Hillstrom. Testing unconstrained optimization software.  ACM Transactions on Mathematical Software (TOMS), 7(1):17-41, 1981.  J.A. Nelder and R. Mead. A simplex method for function minimization. The Computer  Journal, pages 308-313, 1965.  Y. Ollivier, L. Arnold, A. Auger, and N. Hansen. Information-geometric optimization: A unifying picture via invariance principles. ArXiv preprints, arXiv:1106.3708v2, 2011. Ollivier, Arnold, Auger and Hansen  M. Pelikan, D.E. Goldberg, and F.G. Lobo. A survey of optimization by building and using probabilistic models. Computational optimization and applications, 21(1):5-20, 2002. ISSN 0926-6003.  P. Po\u0161\u00edk. Preventing premature convergence in a simple eda via global step size setting. In  Parallel Problem Solving from Nature-PPSN X, pages 549-558. Springer, 2008.  C.R. Rao. Information and the accuracy attainable in the estimation of statistical parameters.  Bull. Calcutta Math. Soc., 37:81-91, 1945. ISSN 0008-0659.  I. Rechenberg. Evolutionstrategie: Optimierung technischer Systeme nach Prinzipien der  biologischen Evolution. Frommann-Holzboog Verlag, Stuttgart, 1973.  I. Rechenberg. Evolutionsstrategie \u201994. Frommann-Holzboog Verlag, 1994.  R. Ros and N. Hansen. A simple modification in CMA-ES achieving linear time and space complexity. In G. Rudolph, T. Jansen, S. Lucas, C. Polini, and N. Beume, editors, Proceedings of Parallel Problem Solving from Nature (PPSN X), volume 5199 of Lecture Notes in Computer Science, pages 296-305. Springer, 2008.  R.Y. Rubinstein. The cross-entropy method for combinatorial and continuous optimization. Methodology and Computing in Applied Probability, 1:127-190, 1999. ISSN 1387-5841. URL http://dx.doi.org/10.1023/A:1010091220143. 10.1023/A:1010091220143.  R.Y. Rubinstein and D.P. Kroese. The cross-entropy method: a unified approach to combi- natorial optimization, Monte-Carlo simulation, and machine learning. Springer-Verlag New York Inc, 2004. ISBN 038721240X.  R. Salakhutdinov. Learning in Markov random fields using tempered transitions. In Y. Bengio, D. Schuurmans, J. Lafferty, C.K.I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1598-1606. MIT Press, 2009.  R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning, ICML \u201908, pages 872-879, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4. URL http://doi.acm.org/10.1145/1390156.1390266.  B. Sareni and L. Kr\u00e4henb\u00fchl. Fitness sharing and niching methods revisited. IEEE Trans.  Evolutionary Computation, 2(3):97-106, 1998.  T. Schaul, T. Glasmachers, and J. Schmidhuber. High dimensions and heavy tails for natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, GECCO \u201911, pages 845-852, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0557-0. doi: 10.1145/2001576.2001692. URL http://doi.acm.org/10. 1145/2001576.2001692.  R.L. Schilling. Measures, integrals and martingales. Cambridge University Press, New York, 2005. ISBN 978-0-521-61525-9; 0-521-61525-9. doi: 10.1017/CBO9780511810886. URL http://dx.doi.org/10.1017/CBO9780511810886. Information-Geometric Optimization  L. Schwartz. Analyse. II, volume 43 of Collection Enseignement des Sciences [Collection: The Teaching of Science]. Hermann, Paris, 1992. Calcul diff\u00e9rentiel et \u00e9quations diff\u00e9rentielles, With the collaboration of K. Zizi.  H.-P. Schwefel. Evolution and Optimum Seeking. Sixth-generation computer technology  series. John Wiley & Sons, Inc. New York, NY, USA, 1995. ISBN 0471571482.  F. Silva and L. Almeida. Acceleration techniques for the backpropagation algorithm. Neural  Networks, pages 110-119, 1990.  P. Smolensky. Information processing in dynamical systems: foundations of harmony theory. In D. Rumelhart and J. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 6, pages 194-281. MIT Press, Cambridge, MA, USA, 1986. ISBN 0-262-68053-X.  Y. Sun, D. Wierstra, T. Schaul, and J. Schmidhuber. Efficient natural evolution strategies. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation, GECCO \u201909, pages 539-546, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-325-9. doi: http://doi.acm.org/10.1145/1569901.1569976. URL http://doi.acm.org/10.1145/ 1569901.1569976.  T. Suttorp, N. Hansen, and C. Igel. Efficient covariance matrix update for variable metric  evolution strategies. Machine Learning, 75(2):167-197, 2009.  H. Thorisson. Coupling, Stationarity, and Regeneration. Springer, 2000.  V. Torczon. On the convergence of pattern search algorithms. SIAM Journal on optimization,  7(1):1-25, 1997.  arXiv:nlin/0408040, 2004.  M. Toussaint. Notes on information geometry and evolutionary processes. ArXiv preprints,  M. Wagner, A. Auger, and M. Schoenauer. EEDA : A new robust estimation of distribution algorithms. Research Report RR-5190, INRIA, 2004. URL http://hal.inria.fr/ inria-00070802/en/.  D. Whitley. The genitor algorithm and selection pressure: Why rank-based allocation of reproductive trials is best. In Proceedings of the third international conference on Genetic algorithms, pages 116-121, 1989.  D. Wierstra, T. Schaul, J. Peters, and J. Schmidhuber. Natural evolution strategies. In  IEEE Congress on Evolutionary Computation, pages 3381-3387, 2008.  D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber. Natural evolution strategies. Journal of Machine Learning Research, 15:949-980, 2014. URL http://jmlr.org/papers/v15/wierstra14a.html.  E. Zhou and J. Hu. Gradient-based adaptive stochastic search for non-differentiable opti- mization. IEEE Transactions on Automatic Control, 59(7):1818-1832, July 2014. ISSN 0018-9286. doi: 10.1109/TAC.2014.2310052. "}, "Breaking the Curse of Dimensionality with Convex Neural Networks": {"volumn": 18, "url": "http://jmlr.org/papers/v18/14-546.html", "header": "Breaking the Curse of Dimensionality with Convex Neural Networks", "author": "Francis Bach", "time": "18(19):1\u221253, 2017.", "abstract": "We consider neural networks with a single hidden layer and non- decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity- inducing norms on the input weights, we show that high- dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non- convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non- convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.", "pdf_url": "http://jmlr.org/papers/volume18/14-546/14-546.pdf", "keywords": ["Neural networks", "non-parametric estimation", "convex optimization", "convex relaxation."], "reference": "P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.  Princeton University Press, 2009.  R. A. Adams and J. F. Fournier. Sobolev Spaces, volume 140. Academic Press, 2003.  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere:  an Introduction, volume 2044. Springer, 2012.  48   Bach  = max 2(cid:54)1 u (cid:107) (cid:107)  max \u03bb(cid:62)0  (cid:107)  = max 2(cid:54)1 u (cid:107) (cid:107)  max \u03bb(cid:62)0 \u2212  = max 2(cid:54)1 u (cid:107) (cid:107)  max \u03bb(cid:62)0 \u2212  +  min 2(cid:54)1 v (cid:107) \u03bb 2 \u03bb 2  +  1 2 (cid:107) 1 2 (cid:107) \u03bb 2  (a  \u2212  \u2212  a + A1/2u  B1/2v  b  \u2212  \u2212  2 2 + (cid:107)  \u03bb 2  v  ( (cid:107)  (cid:107)  2 2 \u2212  1)  a  b + A1/2u (cid:107)\u2212  \u2212  1 2  (a  \u2212  b + A1/2u)(cid:62)B(B + \u03bbI)\u2212  1(a  b + A1/2u)  \u2212  b + A1/2u)(cid:62)(B + \u03bbI)\u2212  1(a  b + A1/2u)  \u2212  with v = (B + \u03bbI)\u2212  1B1/2(a  b + A1/2u). The interval in \u03bb which is su\ufb03cient to explore is  [0,  \u03bbmin(B) + (  \u03bb  \u2208  \u2212  a (cid:107)  b (cid:107)  \u22122 + \u03bbmax(A1/2))2],  which are bounds that are independent of u. Given \u03bb (cid:62) 0, we have the problem of  min \u00b5(cid:62)0  = min \u00b5(cid:62)0  \u03bb 2 \u03bb 2  (a  (a  \u2212  \u2212  max Rd u  \u2208 max Rd u  \u2208  b + A1/2u)(cid:62)(B + \u03bbI)\u2212  1(a  b + A1/2u)  \u00b5 2  \u2212  ( (cid:107)  2 u 2 \u2212 (cid:107)  1)  \u2212  \u03bb 2  b)(cid:62)(B + \u03bbI)\u2212  1(a  b) +  + \u03bbu(cid:62)A1/2(B + \u03bbI)\u2212  1(a  b)  \u2212  \u00b5  \u03bb  \u2212  \u2212 2  u(cid:62)(\u00b5I  \u03bbA1/2(B + \u03bbI)\u2212  1A1/2)u  (a  \u03bb = min 2 \u00b5(cid:62)0 +\u03bb2(a  \u2212  b)(cid:62)(B + \u03bbI)\u2212  \u2212 b)(cid:62)(B + \u03bbI)\u2212  \u2212 \u03bb  1(a  b) +  \u2212  \u00b5  \u2212 2  \u2212  1 2  \u2212  \u2212  1A1/2(\u00b5I  \u03bbA1/2(B + \u03bbI)\u2212  1A1/2)\u2212  1A1/2(B + \u03bbI)\u2212  1(a  b)  \u2212  We have u = ( 1A + I)(a  \u00b5\u2212  \u00b5 I \u03bb b). We need \u00b5 \u03bb  \u2212  A1/2(B + \u03bbI)\u2212  1A1/2)\u2212  1A1/2(B + \u03bbI)\u2212  1(a  b), leading to w  (\u03bb\u2212  1B  (cid:62) \u03bbmax(A1/2(B + \u03bbI)\u2212  \u2212 1A1/2). Moreover  \u221d  \u2212  \u2212 0 (cid:54) \u00b5  \u03bb \u2212  \u03bbmax(A1/2(B + \u03bbI)\u2212  1A1/2) (cid:54)  A1/2(B + \u03bbI)\u2212  1(a  (cid:107)  b)  . (cid:107)  \u2212  This means that the (cid:96)2-Haussdor\ufb00 distance may be computed by solving in \u03bb and \u00b5, by exhaustive search with respect to \u03bb and by binary search (or Newton\u2019s method) for \u00b5. The complexity of each iteration is that of a singular value decomposition, that is O(d3). For more details on optimization of quadratic functions on the unit-sphere, see Forsythe and Golub (1965).  References  P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.  Princeton University Press, 2009.  R. A. Adams and J. F. Fournier. Sobolev Spaces, volume 140. Academic Press, 2003.  K. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere:  an Introduction, volume 2044. Springer, 2012. Breaking the Curse of Dimensionality with Convex Neural Networks  F. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine  Learning Research, 9:1179-1225, 2008a.  F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning.  In  Advances in Neural Information Processing Systems (NIPS), 2008b.  F. Bach. Convex relaxations of structured matrix factorizations. Technical Report 00861118,  HAL, 2013.  F. Bach. Duality between subgradient and conditional gradient methods. SIAM Journal  on Optimization, 25(1):115-129, 2015.  F. Bach. On the equivalence between kernel quadrature rules and random feature expan-  sions. Journal of Machine Learning Research, 18:1-38, 2017.  A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.  IEEE Transactions on Information Theory, 39(3):930-945, 1993.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2003.  P. L. Bartlett, M. I. Jordan, and J. D. McAuli\ufb00e. Convexity, classification, and risk bounds.  Journal of the American Statistical Association, 101(473):138-156, 2006.  A. Barvinok. A Course in Convexity, volume 54. American Mathematical Society, 2002.  Y. Bengio, N. Le Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural networks.  In Advances in Neural Information Processing Systems (NIPS), 2006.  A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and  Statistics, volume 3. Springer, 2004.  E. D. Bolker. A class of convex bodies. Transactions of the American Mathematical Society,  145:323-345, 1969.  L. Bottou and O. Bousquet. The tradeo\ufb00s of large scale learning. In Advances in Neural  Information Processing Systems (NIPS), 2008.  J. Bourgain and J. Lindenstrauss. Projection bodies. In Geometric Aspects of Functional  Analysis, pages 250-270. Springer, 1988.  J. Bourgain, J. Lindenstrauss, and V. Milman. Approximation of zonoids by zonotopes.  Acta Mathematica, 162(1):73-141, 1989.  S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  L. Breiman. Hinging hyperplanes for regression, classification, and function approximation.  IEEE Transactions on Information Theory, 39(3):999-1013, 1993.  P. B\u00a8uhlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory  and applications. Springer, 2011. Bach  M. Burger and A. Neubauer. Error bounds for approximation with neural networks. Journal  of Approximation Theory, 112(2):235-250, 2001.  Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information  Processing Systems (NIPS), 2009.  A. S. Dalalyan, A. Juditsky, and V. Spokoiny. A new algorithm for estimating the e\ufb00ective dimension-reduction subspace. Journal of Machine Learning Research, 9:1647-1678, 2008.  V. F. Dem\u2019yanov and A. M. Rubinov. The minimization of a smooth convex functional on  a convex set. SIAM Journal on Control, 5(2):280-294, 1967.  R. A. DeVore, R. Howard, and C. Micchelli. Optimal nonlinear approximation. Manuscripta  Mathematica, 63(4):469-478, 1989.  M. Dudik, Z. Harchaoui, and J. Malick. Lifted coordinate descent for learning with trace- norm regularization. In Proceedings of the International Conference on Artificial Intelli- gence and Statistics (AISTATS), 2012.  J. C. Dunn and S. Harshbarger. Conditional gradient algorithms with open loop step size  rules. Journal of Mathematical Analysis and Applications, 62(2):432-444, 1978.  H. Edelsbrunner. Algorithms in Combinatorial Geometry, volume 10. Springer, 1987.  L. C. Evans and R. F. Gariepy. Measure Theory and Fine Properties of Functions, volume 5.  CRC Press, 1991.  G. E. Forsythe and G. H. Golub. On the stationary values of a second-degree polynomial on the unit sphere. Journal of the Society for Industrial & Applied Mathematics, 13(4): 1050-1068, 1965.  M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics  Quarterly, 3(1-2):95-110, 1956.  J. H. Friedman and W. Stuetzle. Projection pursuit regression. Journal of the American  Statistical Association, 76(376):817-823, 1981.  C. Frye and C. J. Efthimiou. Spherical Harmonics in p Dimensions. Technical Report  1205.3548, ArXiv, 2012.  K. Fukumizu, F. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5:73-99, 2004.  C. Gu. Smoothing Spline ANOVA Models, volume 297. Springer, 2013.  L. J. Guibas, A. Nguyen, and L. Zhang. Zonotopes as bounding volumes. In Proceedings of  the ACM-SIAM symposium on Discrete Algorithms, 2003.  V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. SIAM  Journal on Computing, 39(2):742-765, 2009. Breaking the Curse of Dimensionality with Convex Neural Networks  L. Gy\u00a8orfi and A. Krzyzak. A Distribution-free Theory of Nonparametric Regression.  Springer, 2002.  Z. Harchaoui, A. Juditsky, and A. Nemirovski. Conditional gradient algorithms for norm- regularized smooth convex optimization. Mathematical Programming, pages 1-38, 2013.  T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,  2009. 2nd edition.  T. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Chapman & Hall, 1990.  S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 1994.  G. E. Hinton and Z. Ghahramani. Generative models for discovering sparse distributed representations. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 352(1358):1177-1190, 1997.  M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceed-  ings of the International Conference on Machine Learning (ICML), 2013.  S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Process- ing Systems (NIPS), 2009.  A. R. Klivans and A. A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. In Annual Symposium on Foundations of Computer Science (FOCS), 2006.  V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions  on Information Theory, 47(5):1902-1914, 2001.  S. K\u00a8onig. Computational aspects of the Hausdor\ufb00 distance in unbounded dimension. Tech-  nical Report 1401.1434, ArXiv, 2014.  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolu- tional neural networks. In Advances in Neural Information Processing Systems, 2012.  V. Kurkova and M. Sanguineti. Bounds on rates of variable-basis and neural-network approximation. IEEE Transactions on Information Theory, 47(6):2659-2665, Sep 2001.  G. Lan. The complexity of large-scale convex programming under a linear optimization  oracle. Technical Report 1309.5550, arXiv, 2013.  N. Le Roux and Y. Bengio. Continuous neural networks. In Proceedings of the International  Conference on Artificial Intelligence and Statistics (AISTATS), 2007.  W. S. Lee, P. L. Bartlett, and R. C. Williamson. E\ufb03cient agnostic learning of neural networks with bounded fan-in. IEEE Transactions on Information Theory, 42(6):2118- 2132, 1996.  M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6 (6):861-867, 1993. Bach  K.-C. Li. Sliced inverse regression for dimension reduction. Journal of the American Sta-  tistical Association, 86(414):316-327, 1991.  Y. Lin and H. H. Zhang. Component selection and smoothing in multivariate nonparametric  regression. Annals of Statistics, 34(5):2272-2297, 2006.  Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational e\ufb03ciency of training neural networks. In Advances in Neural Information Processing Systems, 2014.  V. Maiorov. Approximation by neural networks and learning theory. Journal of Complexity,  22(1):102-117, 2006.  V. E. Maiorov and R. Meir. On the near optimality of the stochastic approximation of smooth functions by neural networks. Advances in Computational Mathematics, 13(1): 79-103, 2000.  Y. Makovoz. Uniform approximation by neural networks. Journal of Approximation Theory,  J. Matou\u02c7sek. Improved upper bounds for approximation by zonotopes. Acta Mathematica,  95(2):215-228, 1998.  177(1):55-73, 1996.  H. N. Mhaskar. On the tractability of multivariate integration and approximation by neural  networks. Journal of Complexity, 20(4):561-590, 2004.  H. N. Mhaskar. Weighted quadrature formulas and approximation by zonal function net-  works on the sphere. Journal of Complexity, 22(3):348-370, 2006.  G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of  deep neural networks. In Advances in neural information processing systems, 2014.  V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines.  In Proceedings of International Conference on Machine Learning (ICML), 2010.  R. M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto,  1995.  Y. Nesterov. Semidefinite relaxation and nonconvex quadratic optimization. Optimization  Methods and Software, 9(1-3):141-160, 1998.  Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic  Publishers, 2004.  103(1):127-152, 2005.  Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,  P. P. Petrushev. Approximation by ridge functions and neural networks. SIAM Journal on  Mathematical Analysis, 30(1):155-189, 1998.  A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica,  8:143-195, 1999. Breaking the Curse of Dimensionality with Convex Neural Networks  A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in  Neural Information Processing Systems (NIPS), 2007.  P. Ravikumar, H. Liu, J. La\ufb00erty, and L. Wasserman. SpAM: Sparse additive models. In  Advances in Neural Information Processing Systems (NIPS), 2008.  R. T. Rockafellar. Convex Analysis. Princeton University Press, 1997.  F. Rosenblatt. The perceptron: a probabilistic model for information storage and organi-  zation in the brain. Psychological Review, 65(6):386, 1958.  S. Rosset, J. Zhu, and T. Hastie. Boosting as a regularized path to a maximum margin  classifier. Journal of Machine Learning Research, 5:941-973, 2004.  S. Rosset, G. Swirszcz, N. Srebro, and J. Zhu. (cid:96)1-regularization in infinite dimensional feature spaces. In Proceedings of the Conference on Learning Theory (COLT), 2007.  W. Rudin. Real and Complex Analysis. Tata McGraw-Hill Education, 1987.  D. E. Rumelhart, G. E Hinton, and R. J. Williams. Learning representations by back-  propagating errors. Nature, 323:533-536, 1986.  R. Schneider. Zu einem problem von shephard \u00a8uber die projektionen konvexer k\u00a8orper.  Mathematische Zeitschrift, 101(1):71-82, 1967.  S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge  University Press, 2004.  A. J. Smola, Z. L. Ovari, and R. C. Williamson. Regularization with dot-product kernels.  Advances in Neural Information Processing Systems (NIPS), 2001.  K. Sridharan. Learning from an Optimization Viewpoint. PhD thesis, Toyota Technological  Institute at Chicago, 2012.  U. von Luxburg and O. Bousquet. Distance-based classification with Lipschitz functions.  Journal of Machine Learning Research, 5:669-695, 2004.  H. Whitney. Analytic extensions of di\ufb00erentiable functions defined in closed sets. Transac-  tions of the American Mathematical Society, 36(1):63-89, 1934.  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.  X. Zhang, D. Schuurmans, and Y. Yu. Accelerated training for matrix-norm regularization: A boosting approach. In Advances in Neural Information Processing Systems (NIPS), 2012. "}, "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-178.html", "header": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions", "author": "Francis Bach", "time": "18(21):1\u221238, 2017.", "abstract": "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in $L_2$- and $L_\\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.", "pdf_url": "http://jmlr.org/papers/volume18/15-178/15-178.pdf", "keywords": ["Quadrature", "positive-definite kernels", "integral operators."], "reference": "R. A. Adams and J. F. Fournier. Sobolev Spaces, volume 140. Academic Press, 2003.  F. Bach. Sharp analysis of low-rank kernel matrix approximations. In Proceedings of the  International Conference on Learning Theory (COLT), 2013.  F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of  Machine Learning Research, 18:1-53, 2017.  F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and con- ditional gradient algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2012.  C. R. Baker. Joint measures and cross-covariance operators. Transactions of the American  Mathematical Society, 186:273-289, 1973.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2003.  A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and  Statistics, volume 3. Springer, 2004.  R. Bhatia. Positive definite matrices. Princeton University Press, 2009.  M. Sh. Birman and M. Z. Solomyak. Estimates of singular numbers of integral operators.  Russian Mathematical Surveys, 32(1):15-89, 1977.  A. D. Bull. Convergence rates of e\ufb03cient global optimization algorithms. Journal of Machine  Learning Research, 12:2879-2904, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm.  Found. Comput. Math., 7(3):331-368, 2007.  K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science,  10(3):273-304, 1995.  Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In Proceedings of  the Conference on Uncertainty in Artificial Intelligence (UAI), 2010.  35   On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions  Combining the last two inequalities, we get (cid:107)\u03b2j \u2212 \u03b2j(cid:48)(cid:107)2 (cid:62) less than the \u2206-packing number of the ball of radius r = 2/ (r/\u2206)n(2 + \u2206/r)n (see, e.g., Massart, 2003, Lemma 4.14). Since \u2206/r = we have  (cid:113) \u03b4\u00b5s 72n tr \u03a3 = \u2206. Thus, es/8 is \u221a n, which is itself less than (cid:113) \u03b4\u00b5s  4\u00b772 tr \u03a3  (cid:54) 1 \u221a 12,  (cid:18) 1 2  s 8  (cid:54) n  log  4 \u00b7 72 tr \u03a3 \u03b4\u00b5s  1 \u221a 12 . Given that we have to choose \u00b5s (cid:62) 144\u03bb for the result to hold,  + log(2 +)  (cid:19) .  This implies n (cid:62) +29 this implies the desired result, since 4 log(1440) (cid:62) 29.  4 log tr \u03a3 \u03b4\u00b5s  s  References  R. A. Adams and J. F. Fournier. Sobolev Spaces, volume 140. Academic Press, 2003.  F. Bach. Sharp analysis of low-rank kernel matrix approximations. In Proceedings of the  International Conference on Learning Theory (COLT), 2013.  F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of  Machine Learning Research, 18:1-53, 2017.  F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and con- ditional gradient algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2012.  C. R. Baker. Joint measures and cross-covariance operators. Transactions of the American  Mathematical Society, 186:273-289, 1973.  P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and  structural results. Journal of Machine Learning Research, 3:463-482, 2003.  A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and  Statistics, volume 3. Springer, 2004.  R. Bhatia. Positive definite matrices. Princeton University Press, 2009.  M. Sh. Birman and M. Z. Solomyak. Estimates of singular numbers of integral operators.  Russian Mathematical Surveys, 32(1):15-89, 1977.  A. D. Bull. Convergence rates of e\ufb03cient global optimization algorithms. Journal of Machine  Learning Research, 12:2879-2904, 2011.  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm.  Found. Comput. Math., 7(3):331-368, 2007.  K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science,  10(3):273-304, 1995.  Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In Proceedings of  the Conference on Uncertainty in Artificial Intelligence (UAI), 2010. Bach  Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information  Processing Systems (NIPS), 2009.  W. G. Cochran and G. M. Cox. Experimental designs. John Wiley & Sons, 1957.  D. Cruz-Uribe and C. J. Neugebauer. Sharp error bounds for the trapezoidal rule and Simpson\u2019s rule. Journal of Inequalities in Pure and Applied Mathematics, 3(4), 2002.  B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. Balcan, and L. Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems (NIPS), 2014.  A. Dieuleveut and F. Bach. Non-parametric stochastic approximation with large step sizes.  The Annals of Statistics, 44(4):1363-1399, 2015.  A. El Alaoui and M. W. Mahoney. Fast randomized kernel methods with statistical guar-  antees. In Advances in Neural Information Processing Systems (NIPS), 2015.  S. Fine and K. Scheinberg. E\ufb03cient SVM training using low-rank kernel representations.  Journal of Machine Learning Research, 2:243-264, 2001.  E. M. Furrer and D. W. Nychka. A framework to understand the asymptotic properties of  kriging and splines. Journal of the Korean Statistical Society, 36(1):57-76, 2007.  A. Gelman. Bayesian Data Analysis. CRC Press, 2004.  Z. Harchaoui, F. Bach, and E. Moulines. Testing for homogeneity with kernel Fisher dis-  criminant analysis. Technical Report 00270806, HAL, April 2008.  T. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Chapman & Hall, 1990.  K. Hesse. A lower bound for the worst-case cubature error on spheres of arbitrary dimension.  Numerische Mathematik, 103(3):413-433, 2006.  F. B. Hildebrand. Introduction to Numerical Analysis. Courier Dover Publications, 1987.  R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 2012.  D. Hsu, S. M. Kakade, and T. Zhang. Tail inequalities for sums of random matrices that depend on the intrinsic dimension. Electronic Communications in Probability, 17(14): 1-13, 2012.  D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. Foun-  dations of Computational Mathematics, 14(3):569-600, 2014.  G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew. Extreme learning machine: theory and applica-  tions. Neurocomputing, 70(1):489-501, 2006.  F. Husz\u00b4ar and D. Duvenaud. Optimally-weighted herding is Bayesian quadrature. Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2012.  In  T. Kato. Perturbation theory for linear operators. Springer Science & Business Media, 1995. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions  H. K\u00a8onig. Eigenvalues of compact operators with applications to integral operators. Linear  Algebra and its Applications, 84:111-122, 1986.  M. Langberg and L. J. Schulman. Universal epsilon-approximators for integrals. In Pro-  ceedings of ACM-SIAM Symposium on Discrete Algorithms (SODA), 2010.  Q. Le, T. Sarl\u00b4os, and A. Smola. Fastfood: approximating kernel expansions in log-linear time. In Proceedings of the International Conference on Machine Learning (ICML), 2013.  M. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends  in Machine Learning, 3(2):123-224, 2011.  M. W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis.  Proceedings of the National Academy of Sciences, 106(3):697-702, 2009.  P. Massart. Concentration Inequalities and Model Selection: Ecole d\u2019\u00b4et\u00b4e de Probabilit\u00b4es de  Saint-Flour 23. Springer, 2003.  S. Mendelson and J. Neeman. Regularization in kernel learning. The Annals of Statistics,  38(1):526-565, 2010.  S. Minsker. On some extensions of Bernstein\u2019s inequality for self-adjoint operators. Technical  Report 1112.5448, arXiv, 2011.  W. J. Moroko\ufb00 and R. E. Ca\ufb02isch. Quasi-random sequences and their discrepancies. SIAM  Journal on Scientific Computing, 15(6):1251-1279, 1994.  R. M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto,  1995.  Verlag, 1988.  1501.03379, arXiv, 2015.  48(6):1527-1531, 1988.  (3):245-260, 1991.  E. Novak. Deterministic and Stochastic Error Bounds in Numerical Analysis. Springer-  C. J. Oates and M. Girolami. Variance reduction for quasi-Monte-Carlo. Technical Report  H. Ogawa. An operator pseudo-inversion lemma. SIAM Journal on Applied Mathematics,  A. O\u2019Hagan. Bayes-Hermite quadrature. Journal of statistical planning and inference, 29  A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in  Neural Information Processing Systems (NIPS), 2007.  A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems (NIPS), 2009.  C. E. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo.  In Advances in Neural  Information Processing Systems (NIPS), 2003. Bach  C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer New York, 2005.  S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to  Algorithms. Cambridge University Press, 2014.  J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge  University Press, 2004.  1979.  B. Simon. Trace ideals and their applications, volume 35. Cambridge University Press,  S. Smale and F. Cucker. On the mathematical foundations of learning. Bulletin of the  American Mathematical Society, 39(1):1-49, 2001.  A. Smola, A. Gretton, L. Song, and B. Sch\u00a8olkopf. A Hilbert space embedding for distribu-  tions. In Algorithmic Learning Theory, pages 13-31. Springer, 2007.  A. J. Smola and B. Sch\u00a8olkopf. Sparse greedy matrix approximation for machine learning. In Proceedings of the International Conference on Machine Learning (ICML), 2000.  A. J. Smola, Z. L. Ovari, and R. C. Williamson. Regularization with dot-product kernels.  Advances in Neural Information Processing Systems (NIPS), 2001.  N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Informa- tion Theory, 58(5):3250-3265, 2012.  B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00a8olkopf, and G. R. G. Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:1517-1561, 2010.  I. Steinwart, D. R. Hush, and C. Scovel. Optimal rates for regularized least squares re- gression. In Proceedings of the International Conference on Learning Theory (COLT), 2009.  G. Wahba. Spline Models for observational data. SIAM, 1990.  H. Widom. Asymptotic behavior of the eigenvalues of certain integral equations I. Trans-  actions of the American Mathematical Society, 109:278-295, 1963.  C. Williams and M. Seeger. Using the Nystr\u00a8om method to speed up kernel machines. In  Advances in Neural Information Processing Systems (NIPS), 2001.  J. Yang, V. Sindhwani, H. Avron, and M. Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. In Proceedings of the International Conference on Machine Learning (ICML), 2014.  L. Zwald, G. Blanchard, P. Massart, and R. Vert. Kernel projection machine: a new tool for pattern recognition. In Advances in Neural Information Processing Systems (NIPS), 2004. "}, "Analyzing Tensor Power Method Dynamics in Overcomplete Regime": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-486.html", "header": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime", "author": "Animashree An, kumar, Rong Ge, Majid Janzamin", "time": "18(22):1\u221240, 2017.", "abstract": "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components $k$ is much larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models.", "pdf_url": "http://jmlr.org/papers/volume18/15-486/15-486.pdf", "keywords": ["unsupervised learning", "latent variable models"], "reference": "A. Anandkumar, D. Hsu, and S. M. Kakade. A Method of Moments for Mixture Models  and Hidden Markov Models. In Proc. of Conf. on Learning Theory, June 2012.  A. Anandkumar, R. Ge, and M. Janzamin. Learning Overcomplete Latent Variable Models through Tensor Methods. In Proceedings of the Conference on Learning Theory (COLT), Paris, France, July 2015.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014a.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods. arXiv preprint arXiv:1408.0553, Aug. 2014b.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgar- sky. Tensor Decompositions for Learning Latent Variable Models. Journal of Machine Learning Research, 15:2773-2832, 2014c.  Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians.  The Annals of Applied Probability, 15(1A):69-92, 2005.  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. arXiv preprint arXiv:1001.3448, Jan. 2010.  A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of tensor  decompositions. arXiv preprint arXiv:1311.3651, 2013.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projec- Information Theory, IEEE Transactions on, 52  tions: Universal encoding strategies? (12):5406-5425, 2006.  Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear  Algebra and its Applications, 438(2):942-952, January 2013.  Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, pages 215-223, 2011.  Sanjoy Dasgupta. Learning mixutres of gaussians. In FOCS, 1999.  L. De Lathauwer, J. Castaing, and J.-F. Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. Signal Processing, IEEE Transactions on, 55(6):2965-2973, 2007.  Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares  algorithms. arXiv preprint arXiv:1504.05287, April 2015.  N. Goyal, S. Vempala, and Y. Xiao. Fourier pca. arXiv preprint arXiv:1306.5825, 2013.  39   Tensor Power Method Dynamics in Overcomplete Regime  References  A. Anandkumar, D. Hsu, and S. M. Kakade. A Method of Moments for Mixture Models  and Hidden Markov Models. In Proc. of Conf. on Learning Theory, June 2012.  A. Anandkumar, R. Ge, and M. Janzamin. Learning Overcomplete Latent Variable Models through Tensor Methods. In Proceedings of the Conference on Learning Theory (COLT), Paris, France, July 2015.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014a.  Anima Anandkumar, Rong Ge, and Majid Janzamin. Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods. arXiv preprint arXiv:1408.0553, Aug. 2014b.  Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgar- sky. Tensor Decompositions for Learning Latent Variable Models. Journal of Machine Learning Research, 15:2773-2832, 2014c.  Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians.  The Annals of Applied Probability, 15(1A):69-92, 2005.  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. arXiv preprint arXiv:1001.3448, Jan. 2010.  A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of tensor  decompositions. arXiv preprint arXiv:1311.3651, 2013.  Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projec- Information Theory, IEEE Transactions on, 52  tions: Universal encoding strategies? (12):5406-5425, 2006.  Dustin Cartwright and Bernd Sturmfels. The number of eigenvalues of a tensor. Linear  Algebra and its Applications, 438(2):942-952, January 2013.  Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, pages 215-223, 2011.  Sanjoy Dasgupta. Learning mixutres of gaussians. In FOCS, 1999.  L. De Lathauwer, J. Castaing, and J.-F. Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. Signal Processing, IEEE Transactions on, 55(6):2965-2973, 2007.  Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares  algorithms. arXiv preprint arXiv:1504.05287, April 2015.  N. Goyal, S. Vempala, and Y. Xiao. Fourier pca. arXiv preprint arXiv:1306.5825, 2013. Anandkumar, Ge, and Janzamin  Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are NP hard. Journal of  ACM, 60(6), November 2013.  Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university press, 2012.  D. Hsu and S. M. Kakade. Learning Mixtures of Spherical Gaussians: Moment Methods  and Spectral Decompositions. arXiv preprint arXiv:1206.5766, 2012.  Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment meth- ods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 11-20. ACM, 2013.  A. T. Kalai, A. Moitra, and G. Valiant. E\ufb03ciently learning mixtures of two gaussians. In  STOC, 2010.  tation, 12(2):337-365, 2000.  M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. Neural compu-  Jorma K. Merikoski and Ravinder Kumar.  Inequalities for spreads of matrix sums and  products. Applied Mathematics E-Notes, 4:150-159, 2004.  A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In  FOCS, 2010.  FOCS, 2002.  Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Trans-  actions of the Royal Society of London. A, 186:343-414, 1895.  S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In  T. Zhang and G. Golub. Rank-one approximation to high order tensors. SIAM Journal on  Matrix Analysis and Applications, 23:534-550, 2001. "}, "Identifying a Minimal Class of Models for High--dimensional Data": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-172.html", "header": "Identifying a Minimal Class of Models for High--dimensional Data", "author": "Daniel Nevo, Ya'acov Ritov", "time": "18(24):1\u221229, 2017.", "abstract": "Model selection consistency in the high--dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets.", "pdf_url": "http://jmlr.org/papers/volume18/16-172/16-172.pdf", "keywords": ["Model Selection; High-dimensional Data; Lasso; Elastic Net; Simulated An nealing"], "reference": "Hirotugu Akaike. A new look at the statistical model identification. IEEE transactions on  automatic control, 19(6):716-723, 1974.  Peter J Bickel and Mu Cai. Discussion of Sara van de Geer: Generic chaining and the (cid:96)1  penalty. Journal of Statistical Planning and Inference, 143(6):1013-1018, 2012.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso  and dantzig selector. Annals of Statistics, 37(4):1705-1732, 2009.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Hierarchical selection of vari- theory powering ables in sparse high-dimensional regression. applications-a Festschrift for Lawrence D. Brown, volume 6, pages 56-69. Institute of Mathematical Statistics, 2010.  In Borrowing strength:  Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions.  Annals of Statistics, 41(3):1111-1141, 2013.  Leo Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996.  S. P. Brooks and B. J. T. Morgan. Optimization using simulated annealing. The Statistician,  pages 241-257, 1995.  27   Minimal Class of Models  Appendix B. Supplementary table for Section 5.2  Predictor Description prec jant jult age65 pphs educ h facl dens nwht wtcl linc  Mean annual precipitation in inches Mean January temperature in degrees F Mean July temperature in degrees F Percentage of population aged 65 or older Population per household Median school years completed by those over 22 Percentage of housing units which are sound and with all facilities Population per square mile in urbanized areas Percentage of non-white population in urbanized areas Percentage of employed in white collar occupations Percentage of families with income < 3,000 dollars in urbanized areas Relative pollution potential of hydrocarbon Relative pollution potential of nitric oxides Relative pollution potential of sulfur dioxide Annual average percentage of relative humidity at 1pm  HC NOx SUL hum  Table 4: Potential predictors for mortality rate in Section 5.2  References  Hirotugu Akaike. A new look at the statistical model identification. IEEE transactions on  automatic control, 19(6):716-723, 1974.  Peter J Bickel and Mu Cai. Discussion of Sara van de Geer: Generic chaining and the (cid:96)1  penalty. Journal of Statistical Planning and Inference, 143(6):1013-1018, 2012.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso  and dantzig selector. Annals of Statistics, 37(4):1705-1732, 2009.  Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Hierarchical selection of vari- theory powering ables in sparse high-dimensional regression. applications-a Festschrift for Lawrence D. Brown, volume 6, pages 56-69. Institute of Mathematical Statistics, 2010.  In Borrowing strength:  Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions.  Annals of Statistics, 41(3):1111-1141, 2013.  Leo Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996.  S. P. Brooks and B. J. T. Morgan. Optimization using simulated annealing. The Statistician,  pages 241-257, 1995. Nevo and Ritov  S. P. Brooks, N Friel, and R King. Classical model selection via simulated annealing. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):503-520, 2003.  Peter B\u00a8uhlmann, Markus Kalisch, and Lukas Meier. High-dimensional statistics with a view toward applications in biology. Annual Review of Statistics and Its Application, 1: 255-278, 2014.  Florentina Bunea, AlexandreB. Tsybakov, and MartenH. Wegkamp. Aggregation and Sparsity Via (cid:96)1 Penalized Least Squares. In Learning Theory, volume 4005 of Lecture Notes in Computer Science, pages 379-391. Springer Berlin Heidelberg, 2006. URL http://dx.doi.org/10.1007/11776420_29.  Florentina Bunea, Alexandre Tsybakov, and Marten Wegkamp. Sparsity oracle inequalities  for the lasso. Electronic Journal of Statistics, 1:169-194, 2007.  Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1-22, 2010.  Eitan Greenshtein and Ya\u2019Acov Ritov. Persistence in high-dimensional linear predictor  selection and the virtue of overparametrization. Bernoulli, 10(6):971-988, 2004.  W Keith Hastings. Monte Carlo sampling methods using Markov chains and their applica-  tions. Biometrika, 57(1):97-109, 1970.  Nils Lid Hjort and Gerda Claeskens. Frequentist model average estimators. Journal of the  American Statistical Association, 98(464):879-899, 2003.  Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthog-  onal problems. Technometrics, 12(1):55-67, 1970.  Jennifer A Hoeting, David Madigan, Adrian E Raftery, and Chris T Volinsky. Bayesian  model averaging: A tutorial. Statistical Science, pages 382-401, 1999.  Jian Huang, Shuangge Ma, and Cun-Hui Zhang. Adaptive lasso for sparse high-dimensional  regression models. Statistica Sinica, 18(4):1603, 2008.  Jinzhu Jia and Bin Yu. On model selection consistency of the Elastic Net when p >> n.  Statistica Sinica, 20:595-611, 2010.  Scott Kirkpatrick, D. Jr. Gelatt, and Mario P Vecchi. Optimization by simulated annealing.  science, 220(4598):671-680, 1983.  Gary C McDonald and Richard C Schwing. Instabilities of regression estimates relating air  pollution to mortality. Technometrics, 15(3):463-481, 1973.  Nicolai Meinshausen and Peter B\u00a8uhlmann. High-dimensional graphs and variable selection  with the lasso. Annals of Statistics, 34(3):1436-1462, 2006.  Nicolai Meinshausen and Peter B\u00a8uhlmann. Stability selection. Journal of the Royal Statis-  tical Society: Series B (Statistical Methodology), 72(4):417-473, 2010. Minimal Class of Models  Nicolai Meinshausen and Bin Yu. Lasso-type recovery of sparse representations for high-  dimensional data. Annals of Statistics, 37:246-270, 2009.  Nicolai Meinshausen, Lukas Meier, and Peter B\u00a8uhlmann. P-values for high-dimensional  regression. Journal of the American Statistical Association, 104(488), 2009.  Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21:1087, 1953.  Robert B O\u2019Hara and Mikko J Sillanp\u00a8a\u00a8a. A review of Bayesian variable selection methods:  what, how and which. Bayesian analysis, 4(1):85-117, 2009.  Philippe Rigollet and Alexandre B Tsybakov. Sparse estimation by exponential weighting.  Statistical Science, 27(4):558-575, 2012.  Gideon Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461-464,  Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika, 71:879-898,  1978.  2012.  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal  Statistical Society. Series B (Methodological), pages 267-288, 1996.  Joel A Tropp. Greed is good: Algorithmic results for sparse approximation. Information  Theory, IEEE Transactions on, 50(10):2231-2242, 2004.  Hansheng Wang, Bo Li, and Chenlei Leng. Shrinkage tuning parameter selection with a diverging number of parameters. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):671-683, 2009.  Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.  Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in high-  dimensional linear regression. Annals of Statistics, 36(4):1567-1594, 2008.  Tong Zhang. On the consistency of feature selection using greedy least squares regression.  Journal of Machine Learning Research, 10(3), 2009.  Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine  Learning Research, 7:2541-2563, 2006.  Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical  association, 101(476):1418-1429, 2006.  Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301-320, 2005. "}, "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-397.html", "header": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models", "author": "Alexandre Bouchard-C\u00c3\u00b4t\u00c3\u00a9, Arnaud Doucet, Andrew Roth", "time": "18(28):1\u221239, 2017.", "abstract": "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at  github.com/aroth85/pgsm .", "pdf_url": "http://jmlr.org/papers/volume18/15-397/15-397.pdf"}, "Certifiably Optimal Low Rank Factor Analysis": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-613.html", "header": "Certifiably Optimal Low Rank Factor Analysis", "author": "Dimitris Bertsimas, Martin S. Copenhaver, Rahul Mazumder", "time": "18(29):1\u221253, 2017.", "abstract": "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix ($\\B\\Sigma$) by the sum of a Positive Semidefinite (PSD) low-rank component ($\\B\\Theta$) and a diagonal matrix ($\\B\\Phi$) (with nonnegative entries) subject to $\\B\\Sigma - \\B\\Phi$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are  certifiably optimal  in many cases, even for problems with thousands of variables. Our techniques are general and make  no  assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization.", "pdf_url": "http://jmlr.org/papers/volume18/15-613/15-613.pdf"}, "Group Sparse Optimization via lp,q Regularization": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-651.html", "header": "Group Sparse Optimization via lp,q Regularization", "author": "Yaohua Hu, Chong Li, Kaiwen Meng, Jing Qin, Xiaoqi Yang", "time": "18(30):1\u221252, 2017.", "abstract": "In this paper, we investigate a group sparse optimization problem via $\\ell_{p,q}$ regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order $\\mathcal{O}(\\lambda^\\frac{2}{2-q})$ for any point in a level set of the $\\ell_{p,q}$ regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order $\\mathcal{O}(\\lambda^2)$ for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the $\\ell_{p,q}$ regularization problems, either by analytically solving some specific $\\ell_{p,q}$ regularization subproblems, or by using the Newton method to solve general $\\ell_{p,q}$ regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the $\\ell_{1,q}$ regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual $\\ell_{q}$ regularization problem ($0<q<1$) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation.", "pdf_url": "http://jmlr.org/papers/volume18/15-651/15-651.pdf"}, "Online Bayesian Passive-Aggressive Learning": {"volumn": 18, "url": "http://jmlr.org/papers/v18/14-188.html", "header": "Online Bayesian Passive-Aggressive Learning", "author": "Tianlin Shi, Jun Zhu", "time": "18(33):1\u221239, 2017.", "abstract": "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive- Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.", "pdf_url": "http://jmlr.org/papers/volume18/14-188/14-188.pdf"}, "A Spectral Algorithm for Inference in Hidden semi-Markov Models": {"volumn": 18, "url": "http://jmlr.org/papers/v18/15-468.html", "header": "A Spectral Algorithm for Inference in Hidden semi-Markov Models", "author": "Igor Melnyk, Arindam Banerjee", "time": "18(35):1\u221239, 2017.", "abstract": "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets.", "pdf_url": "http://jmlr.org/papers/volume18/15-468/15-468.pdf"}, "Bridging Supervised Learning and Test-Based Co-optimization": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-223.html", "header": "Bridging Supervised Learning and Test-Based Co-optimization", "author": "Elena Popovici", "time": "18(38):1\u221239, 2017.", "abstract": "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of co-optimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.  [ ][ ]", "pdf_url": "http://jmlr.org/papers/volume18/16-223/16-223.pdf"}, "COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Evolution": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-132.html", "header": "COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Evolution", "author": "Mehrdad Farajtabar, Yichen Wang, Manuel Gomez-Rodriguez, Shuang Li, Hongyuan Zha, Le Song", "time": "18(41):1\u221249, 2017.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume18/16-132/16-132.pdf"}, "Learning Local Dependence In Ordered Data": {"volumn": 18, "url": "http://jmlr.org/papers/v18/16-198.html", "header": "Learning Local Dependence In Ordered Data", "author": "Guo Yu, Jacob Bien", "time": "18(42):1\u221260, 2017.", "abstract": "In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification.", "pdf_url": "http://jmlr.org/papers/volume18/16-198/16-198.pdf"}}