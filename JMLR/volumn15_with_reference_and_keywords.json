{"Bridging Viterbi and Posterior Decoding: A Generalized Risk Approach to Hidden Path Inference Based on Hidden Markov Models": {"volumn": 15, "url": "http://jmlr.org/papers/v15/lember14a.html", "header": "Bridging Viterbi and Posterior Decoding: A Generalized Risk Approach to Hidden Path Inference Based on Hidden Markov Models", "author": "J\u00c3\u00bcri Lember, Alexey A. Koloydenko", "time": "15(1):1\u221258, 2014.", "abstract": "Motivated by the unceasing interest in hidden Markov models (HMMs), this paper re-examines hidden path inference in these models, using primarily a risk-based framework. While the most common  maximum a posteriori  (MAP), or Viterbi, path estimator and the  minimum error , or  Posterior Decoder  (PD) have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed elsewhere. The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk- based decoders are more clearly interpretable, and, most importantly, work \"out-of-the box\" in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in the conclusion.", "pdf_url": "http://jmlr.org/papers/volume15/lember14a/lember14a.pdf", "keywords": ["admissible path", "decoder", "HMM", "hybrid", "interpolation", "MAP sequence", "min imum error", "optimal accuracy", "power transform", "risk", "segmental classification", "symbol-by symbol", "posterior decoding", "Viterbi algorithm"], "reference": "Stephen F. Altschul, Warren Gish, Webb Miller, Eugene W. Myers, and David J. Lipman. Basic local alignment search tool. Journal of Molecular Biology, 215(3):403 - 410, 1990.  Zafer Aydin, Yucel Altunbasak, and Mark Borodovsky. Protein secondary structure pre- diction for a single-sequence using hidden semi-Markov models. BMC Bioinformatics, 7 (1):178, 2006.  Lalit R. Bahl, John Cocke, Frederick Jelinek, and Josef Raviv. Optimal decoding of linear codes for minimizing symbol error rate (corresp.). IEEE Transactions on Information Theory, 20(2):284-287, 1974.  Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weis- sig, Ilya N. Shindyalov, and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235-242, 2000.  Julian Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical  Society. Series B. Methodological, 48(3):259-302, 1986.  Julian Besag and Peter J. Green. Spatial statistics and Bayesian computation. Journal of  the Royal Statistical Society. Series B. Methodological, 55(1):25-37, 1993.  Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science  and Statistics. Springer, New York, 2006.  53   Bridging Viterbi and Posterior Decoding  \uf8eb  \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed  (cid:99)P1  (cid:99)P2  (cid:99)P3  (cid:99)P4  (cid:99)P5  (cid:99)P6  0.1059 0.0636 0.0643 0.1036 0.1230 0.1230 0.0107 0.0171 0.0135 0.0081 0.0111 0.0128 0.0538 0.0319 0.0775 0.0634 0.0415 0.0345 0.0973 0.0477 0.0620 0.1120 0.0852 0.0848 0.0436 0.0576 0.0330 0.0371 0.0386 0.0399 0.0303 0.0484 0.1133 0.0447 0.0321 0.0229 0.0203 0.0227 0.0259 0.0188 0.0197 0.0221 0.0564 0.1010 0.0372 0.0557 0.0694 0.0593 0.0672 0.0443 0.0574 0.0560 0.0671 0.0810 0.1227 0.1068 0.0674 0.0994 0.1279 0.1477 0.0240 0.0219 0.0181 0.0214 0.0293 0.0304 0.0299 0.0252 0.0561 0.0259 0.0338 0.0336 0.0333 0.0208 0.0757 0.0472 0.0067 0.0031 0.0443 0.0270 0.0330 0.0469 0.0497 0.0472 0.0594 0.0464 0.0470 0.0522 0.0677 0.0697 0.0496 0.0496 0.0744 0.0485 0.0422 0.0491 0.0395 0.0641 0.0572 0.0465 0.0412 0.0375 0.0591 0.1386 0.0473 0.0685 0.0677 0.0545 0.0168 0.0172 0.0111 0.0135 0.0130 0.0124 0.0359 0.0483 0.0286 0.0306 0.0332 0.0344  \uf8f6  \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8  .  A C D E F G H I K L M N P Q R S T V W Y  References  Stephen F. Altschul, Warren Gish, Webb Miller, Eugene W. Myers, and David J. Lipman. Basic local alignment search tool. Journal of Molecular Biology, 215(3):403 - 410, 1990.  Zafer Aydin, Yucel Altunbasak, and Mark Borodovsky. Protein secondary structure pre- diction for a single-sequence using hidden semi-Markov models. BMC Bioinformatics, 7 (1):178, 2006.  Lalit R. Bahl, John Cocke, Frederick Jelinek, and Josef Raviv. Optimal decoding of linear codes for minimizing symbol error rate (corresp.). IEEE Transactions on Information Theory, 20(2):284-287, 1974.  Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weis- sig, Ilya N. Shindyalov, and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235-242, 2000.  Julian Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical  Society. Series B. Methodological, 48(3):259-302, 1986.  Julian Besag and Peter J. Green. Spatial statistics and Bayesian computation. Journal of  the Royal Statistical Society. Series B. Methodological, 55(1):25-37, 1993.  Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science  and Statistics. Springer, New York, 2006. Lember and Koloydenko  Matthew Brand, Nuria Oliver, and Alex Pentland. Coupled hidden Markov models for complex action recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 994-999, S.Juan, Puerto Rico, 1997.  Bro\u02c7na Brejov\u00b4a, Daniel G. Brown, and Tom\u00b4a\u02c7s Vina\u02c7r. The most probable annotation problem in hmms and its application to bioinformatics. Journal of Computer and System Sciences, 73(7):1060 - 1077, 2007a.  Bro\u02c7na Brejov\u00b4a, Daniel G. Brown, and Tom\u00b4a\u02c7s Vina\u02c7r. Advances in hidden Markov models for sequence annotation. In Ion I. M\u02c7andoiu and Alexander Zelikovski, editors, Bioinformatics Algorithms: Techniques and Applications, pages 55-92. John Wiley & Sons, Inc., 2007b.  Gary D. Brushe, Robert E. Mahony, and John B. Moore. A soft output hybrid algorithm for ML/MAP sequence estimation. IEEE Transactions on Information Theory, 44(7): 3129-3140, 1998.  Chris Burge and Samuel Karlin. Prediction of complete gene structures in human genomic  DNA. Journal of Molecular Biology, 268(1):78 - 94, 1997.  Olivier Capp\u00b4e, Eric Moulines, and Tobias Ryd\u00b4en.  Inference in Hidden Markov Models.  Springer Series in Statistics. Springer, New York, 2005.  Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46  (2):255-308, 2009.  Luis E. Carvalho and Charles E. Lawrence. Centroid estimation in discrete high-dimensional spaces with applications in biology. Proceedings of the National Academy of Sciences of the United States of America, 105(9):3209-3214, 2008.  Christiane Cocozza-Thivent and Abdelkrim Bekkhoucha. Estimation in Pickard random fields and application to image processing. Pattern Recognition, 26(5):747-761, 1993.  Richard Durbin, Sean Eddy, Anders Krogh, and Graeme Mitchison. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, 1998.  Sean Eddy. What is a hidden Markov model? Nature Biotechnology, 22(10):1315 - 1316,  2004.  Yariv Ephraim and Neri Merhav. Hidden Markov processes. IEEE Transactions on Infor-  mation Theory, 48(6):1518-1569, June 2002.  Piero Fariselli, Pier Martelli, and Rita Casadio. A new decoding algorithm for hidden Markov models improves the prediction of the topology of all-beta membrane proteins. BMC Bioinformatics, 6(Suppl 4):S12, 2005.  Kuzman Ganchev, Jo\u02dcao V. Gra\u00b8ca, and Ben Taskar. Better alignments = better transla- tions? In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 986-993, Columbus, Ohio, 2008. Bridging Viterbi and Posterior Decoding  Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721-741, 1984.  Peter J. Green and Sylvia Richardson. Hidden Markov models and disease mapping. Journal  of the American Statistical Association, 97(460):1055-1070, 2002.  Jeremiah F. Hayes, Thomas M. Cover, and Juan B. Riera. Optimal sequence detection IEEE Transactions on  and optimal symbol-by-symbol detection: similar algorithms. Communications, 30(1):152-157, January 1982.  Ian Holmes and Richard Durbin. Dynamic programming alignment accuracy. Journal of  Computational Biology, 5(3):493-504, 1998.  Xuedong Huang, Yasuo. Ariki, and Mervyn Jack. Hidden Markov Models for Speech Recog-  nition. Edinburgh University Press, Edinburgh, UK, 1990.  Frederick Jelinek. Continuous speech recognition by statistical methods. Proceedings of the  IEEE, 64:532-556, April 1976.  Massachusetts, 2001.  Frederick Jelinek. Statistical Methods for Speech Recognition. The MIT Press, Cambridge,  Dhiraj Joshi, Jia Li, and James Z. Wang. A computationally e\ufb03cient approach to the estimation of two- and three-dimensional hidden Markov models. IEEE Transactions on Image Processing, 15(7):1871-1886, 2006.  Lukas K\u00a8all, Anders Krogh, and Erik L. L. Sonnhammer. An HMM posterior decoder for sequence feature prediction that includes homology information. Bioinformatics, 21 (suppl 1):i251-257, 2005.  Alexey A. Koloydenko and J\u00a8uri Lember. Infinite Viterbi alignments in the two state hidden Markov models. Acta et Commentationes Universitatis Tartuensis de Mathematica, (12): 109-124, 2008.  Timo Koski. Hidden Markov Models for Bioinformatics, volume 2 of Computational Biology  Series. Kluwer Academic Publishers, Dordrecht, 2001.  Anders Krogh. Two methods for improving performance of an HMM and their application In Proceedings of the Fifth International Conference on Intelligent  for gene finding. Systems for Molecular Biology, pages 179-186, Halkidiki, Greece, 1997.  Anders Krogh. An Introduction to Hidden Markov Models for Biological Sequences. In David B.Searls Steven L. Salzberg and Simon Kasif, editors, Computational Methods in Molecular Biology. Elsevier Science, first edition, 1998.  Kristi Kuljus and J\u00a8uri Lember. Asymptotic risks of Viterbi segmentation. Stochastic Pro-  cesses and Their Applications, 122(9):3312-3341, 2012.  Hans K\u00a8unsch, Stuart Geman, and Athanasios Kehagias. Hidden Markov random fields. The  Annals of Applied Probability, 5(3):577-602, 1995. Lember and Koloydenko  Ste\ufb00en L. Lauritzen. Graphical models, volume 17 of Oxford Statistical Science Series.  Oxford University Press, New York, 1996.  J\u00a8uri Lember. On approximation of smoothing probabilities for hidden Markov models.  Statistics and Probability Letters, 81(2):310-316, 2011a.  J\u00a8uri Lember. A correction on approximation of smoothing probabilities for hidden Markov  models. Statistics and Probability Letters, 81(9):1463-1464, September 2011b.  J\u00a8uri Lember and Alexey A. Koloydenko. The Adjusted Viterbi training for hidden Markov  models. Bernoulli, 14(1):180-206, 2008.  J\u00a8uri Lember and Alexey A. Koloydenko. A constructive proof of the existence of Viterbi  processes. IEEE Transactions on Information Theory, 56(4):2017-2033, 2010.  J\u00a8uri Lember, Kristi Kuljus, and Alexey A. Koloydenko. Theory of segmentation. In Prze- myslaw Dymarski, editor, Hidden Markov Models, Theory and Applications, Bioinformat- ics, pages 51-84. InTech, 2011.  Jia Li, Robert M. Gray, and Richard A. Olshen. Multiresolution image classification by hierarchical modeling with two-dimensional hidden Markov models. IEEE Transactions on Information Theory, 46(5):1826-1841, 2000.  Shu Lin and Daniel J. Costello Jr. Error Control Coding: Fundamental and Applications. Computer Applications in Electrical Engineering. Prentice-Hall, Inc., Englewood Cli\ufb00s, New Jersey, 1983.  William H. Majoros and Uwe Ohler. Advancing the state of the art in computational gene prediction. In Sorin Istrail, Pavel Pevzner, and Michael Waterman, editors, Knowledge Discovery and Emergent Complexity in Bioinformatics, volume 4366 of Lecture Notes in Computer Science, pages 81-106. Springer Berlin / Heidelberg, 2007.  Christopher D. Manning and Hinrich Sch\u00a8utze. Foundations of Statistical Natural Language  Processing. MIT Press, Cambridge, Massachusetts, 1999.  Jose L. Marroquin, Edgar Arce Santana, and Salvador Botello. Hidden markov measure field models for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(11):1380-1387, 2003.  Joshua Mason, Kathryn Watkins, Jason Eisner, and Adam Stubblefield. A natural language approach to automated cryptanalysis of two-time pads. In Proceedings of the 13th ACM Conference on Computer and Communications Security, pages 235-244, Alexandria, Vir- ginia, 2006.  MATLAB. Version 7.13.0.564 (R2011b). The MathWorks, Inc., Natick, Massachusetts,  2011.  Erik McDermott and Timothy J. Hazen. Minimum classification error training of landmark models for real-time continuous speech recognition. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, Montreal, Quebec, 2004. Bridging Viterbi and Posterior Decoding  Clare A. McGrory, D. Michael Titterington, Robert W. Reeves, and Anthony N. Pettitt. Variational Bayes for estimating the parameters of a hidden Potts model. Statistics and Computing, 19(3):329-340, 2009.  Hermann Ney, Volker Steinbiss, Reinhold Haeb-Umbach, B.-H. Tran, and Ute Essen. An overview of the Philips research system for large vocabulary continuous speech recogni- tion. International Journal of Pattern Recognition and Artificial Intelligence, 8(1):33-70, 1994.  Mukund Padmanabhan and Michael A. Picheny. Large-vocabulary speech recognition al-  gorithms. Computer, 35(4):42 - 50, 2002.  Lawrence Rabiner. A tutorial on hidden Markov models and selected applications in speech  recognition. Proceedings of the IEEE, 77(2):257-286, 1989.  Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Prentice-  Hall, Inc., Upper Saddle River, New Jersey, 1993.  Lawrence R. Rabiner, Jay G. Wilpon, and Biing-Hwang Juang. A segmental k-means training procedure for connected word recognition. AT&T Technical Journal, 65(3):21- 31, 1986.  Patrick Robertson, Emmanuelle Villebrun, and Peter Hoeher. A comparison of optimal and sub-optimal MAP decoding algorithms operating in the log domain. In Proceedings of IEEE International Conference on Communications, volume 2, pages 1009-1013, Seattle, Washington, 1995.  Havard Rue. New loss functions in Bayesian imaging. Journal of the American Statistical  Association, 90(431):900-908, 1995.  Asaf A. Salamov and Victor V. Solovyev. Prediction of protein secondary structure by combining nearest-neighbor algorithms and multiple sequence alignments. Journal of Molecular Biology, 247(1):11 - 15, 1995.  Kengo Sato, Michiaki Hamada, Kiyoshi Asai, and Toutai Mituyama. Centroidfold: a web server for RNA secondary structure prediction. Nucleic Acids Research, 37(suppl 2): W277-W280, 2009.  Han Shu, I. Lee Hetherington, and James Glass. Baum-Welch training for segment-based speech recognition. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding, pages 43-48, St. Thomas, U. S. Virgin Islands, 2003.  Softberry,  Inc.  SSENVID: Protein secondary structure and environment assign- ment from atomic coordinates. http://linux1.softberry.com/berry.phtml?topic= ssenvid&group=help&subgroup=propt, 2001. Accessed: 15.10.2011.  Volker Steinbiss, Herman Ney, Xavier L. Aubert, Stefan Besling, Christian Dugast, Ute Es- sen, Daryl Geller, Reinhold Haeb-Umbach, Reinhard Kneser, Humberto G. Meier, Martin Oerder, and B.-H. Tran. The Philips research system for continuous-speech recognition. Philips Journal of Research, 49:317-352, 1995. Lember and Koloydenko  Nikko Str\u00a8om, I. Lee Hetherington, Timothy J. Hazen, Eric Sandness, and James Glass. Acoustic modeling improvements in a segment-based speech recognizer. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding, pages 139-142, Keystone, Colorado, 1999.  The MathWorks, Inc. Statistics ToolboxTM User\u2019s Guide. Natick, Massachusetts, R2012a  edition, 2012.  Andrew Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13(2):260-269, 1967.  Stephan Vogel, Hermann Ney, and Christoph Tillmann. HMM-based word alignment in sta- tistical translation. In Proceedings of the 16th Conference on Computational Linguistics, volume 2, pages 836-841, Copenhagen, Denmark, 1996.  Gerhard Winkler. Image Analysis, Random Fields and Markov chain Monte Carlo Methods, volume 27 of Applications of Mathematics (New York). Springer-Verlag, Berlin, second edition, 2003.  Christopher Yau and Chris C. Holmes. A decision theoretic approach for segmental classi- fication using Hidden Markov models. ArXiv e-prints, 2010. URL http://arxiv.org/ abs/1007.4532. "}, "Fast SVM Training Using Approximate Extreme Points": {"volumn": 15, "url": "http://jmlr.org/papers/v15/nandan14a.html", "header": "Fast SVM Training Using Approximate Extreme Points", "author": "Manu Nandan, Pramod P. Khargonekar, Sachin S. Talathi", "time": "15(2):59\u221298, 2014.", "abstract": "Applications of non-linear kernel support vector machines (SVMs) to large data sets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training data set. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine data sets compared AESVM to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005), BVM (Tsang et al., 2007), LASVM (Bordes et al., 2005), SVM perf  (Joachims and Yu, 2009), and the random features method (Rahimi and Recht, 2007). Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times.", "pdf_url": "http://jmlr.org/papers/volume15/nandan14a/nandan14a.pdf", "keywords": ["kernels", "extreme points"], "reference": "K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classifiers.  In Proceedings of the Seventeenth International Conference on Machine Learning, pages 57- 64, 2000.  A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings  of the 23rd International Conference on Machine Learning, pages 97-104, 2006.  M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for selection.  Journal of Computer and System Sciences, 7:448-461, August 1973.  A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classifiers with online and active learning. Journal of Machine Learning Research, 6:1579-1619, December 2005.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J. Cervantes, X. Li, W. Yu, and K. Li. Support vector machine classification for large data sets via minimum enclosing ball clustering. Neurocomputing, 71:611-619, January 2008.  C. C. Chang and C. J. Lin. IJCNN 2001 challenge: Generalization ability and text decoding. In Proceedings of International Joint Conference on Neural Networks, volume 2, pages 1031 -1036, 2001.  95  D1D2D3D4D5D6D7D8D901020304050607080\u2190CTS value of SVMperffor D4 is 277.7Classification Time SpeedupDatasetsAESVM, \u03b5= 10\u22122CVMBVMLASVMSVMperf Fast SVM Training Using Approximate Extreme Points  Figure 7: Plot of classification time speedup for optimal hyper-parameters (compared to  LIBSVM) of all SVM solvers  References  K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classifiers.  In Proceedings of the Seventeenth International Conference on Machine Learning, pages 57- 64, 2000.  A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings  of the 23rd International Conference on Machine Learning, pages 97-104, 2006.  M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for selection.  Journal of Computer and System Sciences, 7:448-461, August 1973.  A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classifiers with online and active learning. Journal of Machine Learning Research, 6:1579-1619, December 2005.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J. Cervantes, X. Li, W. Yu, and K. Li. Support vector machine classification for large data sets via minimum enclosing ball clustering. Neurocomputing, 71:611-619, January 2008.  C. C. Chang and C. J. Lin. IJCNN 2001 challenge: Generalization ability and text decoding. In Proceedings of International Joint Conference on Neural Networks, volume 2, pages 1031 -1036, 2001.D1D2D3D4D5D6D7D8D901020304050607080\u2190CTS value of SVMperffor D4 is 277.7Classification Time SpeedupDatasetsAESVM, \u03b5= 10\u22122CVMBVMLASVMSVMperf Nandan, Khargonekar and Talathi  C.C Chang and C.J Lin. LIBSVM: A library for support vector machines. ACM Trans- actions on Intelligent Systems and Technology, 2:1-27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.  K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm.  ACM Transaction on Algorithms, 6(4):63:1-63:30, September 2010.  R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture of SVMs for very large scale  problems. Neural Computing, 14(5):1105-1114, 2002.  P. Drineas and M. W. Mahoney. On the Nystr\u00a8om method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, December 2005.  R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training support vector machines. Journal of Machine Learning Research, 6:1889- 1918, 2005.  R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and C. J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871-1874, June 2008.  S. Fine and K. Scheinberg. E\ufb03cient SVM training using low-rank kernel representations.  Journal of Machine Learning Research, 2:243-264, 2002.  V. Franc and S. Sonnenburg. Optimized cutting plane algorithm for support vector ma- chines. In Proceedings of the 25th International Conference on Machine Learning, pages 320-327, 2008.  B. G\u00a8artner and M. Jaggi. Coresets for polytope distance. In Proceedings of the 25th Annual  Symposium on Computational Geometry, pages 33-42, 2009.  J. Guo, N. Takahashi, and T. Nishi. A learning algorithm for improving the classification speed of support vector machines. In Proceedings of the 2005 European Conference on Circuit Theory and Design, volume 3, pages 381 - 384, 2005.  C. J. Hsieh, K. W. Chang, C. J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate In Proceedings of the 25th International  descent method for large-scale linear SVM. Conference on Machine Learning, pages 408-415, 2008.  T. Joachims. Making large-scale support vector machine learning practical. In Advances in  Kernel Methods, pages 169-184. MIT Press, 1999.  T. Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 217-226. ACM, 2006.  T. Joachims and C. N. J. Yu. Sparse kernel SVMs via cutting-plane training. Machine  Learning, 76:179-193, September 2009. Fast SVM Training Using Approximate Extreme Points  B. Kalu\u02c7za, V. Mirchevska, E. Dovgan, M. Lu\u02c7strek, and M. Gams. An agent-based approach to care in independent living. In Ambient Intelligence, pages 177-186. Springer, 2010.  J. Kelley. The cutting-plane method for solving convex programs. Journal of the Society  for Industrial and Applied Mathematics, 8(4):703-712, 1960.  Y. Lecun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner. Gradient-based learning applied to docu-  ment recognition. Proceedings of the IEEE, 86(11):2278 -2324, 1998.  Y. J. Lee and O. L. Mangasarian. Rsvm: Reduced support vector machines.  In Pro- ceedings of the First SIAM International Conference on Data Mining, pages 5-7. SIAM Philadelphia, 2001.  M. Nandan, S. S. Talathi, S. Myers, W. L. Ditto, P. P. Khargonekar, and P. R. Carney. Support vector machines for seizure detection in an animal model of chronic epilepsy. Journal of Neural Engineering, 7(3), 2010.  E. Osuna and O. Castro. Convex hull in feature space for support vector machines. In Pro- ceedings of the 8th Ibero-American Conference on AI: Advances in Artificial Intelligence, pages 411-419, 2002.  E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An application to face detection. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 130 -136, 1997.  D. Pavlov, D. Chudova, and P. Smyth. Towards scalable support vector machines us- ing squashing. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 295-299. ACM, 2000.  J. C. Platt. Fast training of support vector machines using sequential minimal optimization.  In Advances in Kernel Methods, pages 185-208. MIT Press, 1999.  A. Rahimi and B. Recht. Random features for large-scale kernel machines. Advances in  Neural Information Processing Systems, pages 1177-1184, 2007.  R. T. Rockafellar. Convex Analysis. Princeton University Press, 1996.  B. Sch\u00a8olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regular-  ization, Optimization, and Beyond. MIT Press, 2001.  B. Sch\u00a8olkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector  algorithms. Neural Computation, 12(5):1207-1245, 2000.  S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set size. In Proceedings of the 25th International Conference on Machine Learning, pages 928-935, 2008.  S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-  gradient solver for SVM. Mathematical Programming, 127:3-30, March 2011. Nandan, Khargonekar and Talathi  S. S. Talathi, D. U. Hwang, M. L. Spano, J. Simonotto, M. D. Furman, S. M. Myers, J. T. Winters, W. L. Ditto, and P. R. Carney. Non-parametric early seizure detection in an animal model of temporal lobe epilepsy. Journal of Neural Engineering, 5:85-98, 2008.  M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani. A detailed analysis of the KDD CUP 99 data set. In Proceedings of the 2009 IEEE Symposium Computational Intelligence for Security and Defense Applications, pages 53-58, 2009.  D. Tax and R. Duin. Support vector data description. Machine Learning, 54(1):45-66,  2004.  C. H. Teo, S. V. N. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized  risk minimization. Journal of Machine Learning Research, 11:311-365, 2010.  I. W. Tsang, J. T. Kwok, P. Cheung, and N. Cristianini. Core vector machines: Fast SVM training on very large data sets. Journal of Machine Learning Research, 6:363-392, 2005.  I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing balls. In Proceedings of the 24th International Conference on Machine Learning, pages 911-918, 2007.  H. Yu, J. Yang, and J. Han. Classifying large data sets using SVMs with hierarchical clus- ters. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 306-315, 2003.  G. X. Yuan, C. H. Ho, and C. J. Lin. An improved GLMNET for l1-regularized logis- tic regression. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 33-41, 2011.  T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the 21st International Conference on Machine Learning, pages 919-926, 2004. "}, "A Junction Tree Framework for Undirected Graphical Model Selection": {"volumn": 15, "url": "http://jmlr.org/papers/v15/vats14a.html", "header": "A Junction Tree Framework for Undirected Graphical Model Selection", "author": "Divyanshu Vats, Robert D. Nowak", "time": "15(5):147\u2212191, 2014.", "abstract": "An undirected graphical model is a joint probability distribution defined on an undirected graph $G^*$, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph $G^*$ given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in $G^*$. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation.", "pdf_url": "http://jmlr.org/papers/volume15/vats14a/vats14a.pdf", "keywords": ["Graphical models", "Markov random fields", "junction trees", "model selection", "graphical model selection", "high-dimensional statistics", "graph decomposition"], "reference": "A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-dimensional Gaussian graphical model selection: Walk summability and local separation criterion. Journal of Machine Learning Research, 13:2293-2337, 2012a.  A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-dimensional structure learning of Ising models: Local separation criterion. Annals of Statistics, 40(3):1346-1375, 2012b.  S. Arnborg, D. G. Corneil, and A. Proskurowski. Complexity of finding embeddings in  ak-tree. SIAM Journal on Algebraic Discrete Methods, 8(2):277-284, 1987.  S. Arnborg and A. Proskurowski. Linear time algorithms for NP-hard problems restricted  to partial k-trees. Discrete Applied Mathematics, 23(1):11-24, April 1989.  186  (cid:54) (cid:54) (cid:54)  Appendix E. Proof of Theorem 9  To prove the theorem, it is su\ufb03cient to establish that  Vats and Nowak  \u03c10 = \u2126  \u03b7T log(p)/n  ,  \u03c11 = \u2126  \u03b7 log(p1)/n  \u03c12 = \u2126  \u03b7 log(p2)/n  \u03c1T = \u2126  (cid:16)(cid:112)  (cid:17) \u03b7 log(pT )/n  .  (cid:16)(cid:112)  (cid:16)(cid:112)  (cid:16)(cid:112)  (cid:17)  (cid:17)  ,  ,  (cid:17)  (15)  (16)  (17)  (18)  Let H be the graph estimated in Step 1. An error occurs if for an edge (i, j)  G\u2217 \u03bb0 n. Using the proof of \u03c1ij|S| \u2264 | 2)), it is easy to see that n = \u2126(\u03c1\u22122 0 \u03b7T log(p)) is su\ufb03cient n = O(\u03c10)  0. Further, the threshold is chosen such that \u03bb0  \u03b7T and  | \u2264  \u2208  (cid:98)  there exists a subset of vertices S such that Theorem 26 (see analysis of P ( for P (E(G\u2217) 0 as n and (\u03bb0  n)2 = \u2126 (\u03b7T log(p)/n). This proves (15). G1 and  E(H))  E \u2192  S |  \u2192  (cid:54)\u2282  \u222a  T and V2  T , respectively. For  In Step 2, we estimate the graphs  G2 by applying the PC-Algorithm to the G1, given that all edges that have a separator vertices V1 \u222a of size \u03b7T have been removed, we can again use the analysis in the proof of Theorem 26 to n = O(\u03c11) and (\u03bb1 show that for \u03bb1 (cid:98) 1 \u03b7 log(p1)) is su\ufb03cient for G\u2217 . This proves (16). Using similar analysis, H) T ] P ( | \\ we can prove (17) and (18).  (cid:98) n)2 = \u2126 (\u03b7 log(p1)/n), n = \u2126(\u03c1\u22122  = G\u2217[V1  0 as n  \u2192 \u221e  KT )  G1  \u2192  \u2282  \u222a  (cid:98)  (cid:98) The probability of error can be written ask=1 (cid:88) G\u2217  P (G\u2217  H) +  P (  Gk  = G\u2217[Vk  Pe  \u2264  (cid:54)\u2282  T ] \\  \u222a  KT  G\u2217 |  \u2282  H)  + P (  GT  = G\u2217[T ] |  \u2282  (cid:98) H,  G = G[V1  T ]\u2217  KT , G\u2217[V2  T ] = G[V2  \u222a  \\  \u222a  T ] \\  \u222a  KT ) .  Given (15)-(18), each term on the right goes to 0 as n  (cid:98)  (cid:98)  , so Pe  0 as n  \u2192 \u221e  \u2192  . \u2192 \u221e  References  A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-dimensional Gaussian graphical model selection: Walk summability and local separation criterion. Journal of Machine Learning Research, 13:2293-2337, 2012a.  A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-dimensional structure learning of Ising models: Local separation criterion. Annals of Statistics, 40(3):1346-1375, 2012b.  S. Arnborg, D. G. Corneil, and A. Proskurowski. Complexity of finding embeddings in  ak-tree. SIAM Journal on Algebraic Discrete Methods, 8(2):277-284, 1987.  S. Arnborg and A. Proskurowski. Linear time algorithms for NP-hard problems restricted  to partial k-trees. Discrete Applied Mathematics, 23(1):11-24, April 1989.(cid:54) (cid:54) (cid:54)  A Junction Tree Framework for Undirected Graphical Model Selection  F. R. Bach and M. I. Jordan. Thin junction trees.  In Advances in Neural Information  Processing Systems (NIPS), pages 569-576. MIT Press, 2001.  O. Banerjee, L. E. Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maxi- mum likelihood estimation for multivariate Gaussian or binary data. Journal of Machine Learning Research, 9:485-516, June 2008.  A. Berry, P. Heggernes, and G. Simonet. The minimum degree heuristic and the minimal triangulation process. In Graph-Theoretic Concepts in Computer Science, pages 58-70. Springer, 2003.  G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random fields from sam- ples: Some observations and algorithms. In Ashish Goel, Klaus Jansen, Jos Rolim, and Ronitt Rubinfeld, editors, Approximation, Randomization and Combinatorial Optimiza- tion. Algorithms and Techniques, volume 5171 of Lecture Notes in Computer Science, pages 343-356. Springer Berlin, 2008.  F. Bromberg, D. Margaritis, and V. Honavar. E\ufb03cient Markov network structure discovery using independence tests. Journal of Artificial Intelligence Research (JAIR), 35:449-484, 2009.  T. Cai, W. Liu, and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix estimation. Journal of the American Statistical Association, 106(494):594-607, 2011.  V. Chandrasekaran, P.A. Parrilo, and A.S. Willsky. Latent variable graphical model selec-  tion via convex optimization. Annals of Statistics, 40(4):1935-1967, 2012.  A. Chechetka and C. Guestrin. E\ufb03cient principled learning of thin junction trees.  In Advances in Neural Information Processing Systems (NIPS), pages 273-280, December 2007.  J. Chen and Z. Chen. Extended Bayesian information criteria for model selection with large  model spaces. Biometrika, 95(3):759-771, 2008.  M. J. Choi, V. Y. F. Tan, A. Anandkumar, and A. S. Willsky. Learning latent tree graphical  models. Journal of Machine Learning Research, 12:1771-1812, May 2011.  C. Chow and C. Liu. Approximating discrete probability distributions with dependence  trees. IEEE Transactions on Information Theory, 14(3):462-467, May 1968.  J. Fan and J. Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849- 911, 2008.  R. Foygel and M. Drton. Extended Bayesian information criteria for Gaussian graphical models. In Advances in Neural Information Processing Systems (NIPS), pages 604-612, 2010. Vats and Nowak  J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the  graphical Lasso. Biostatistics, 9(3):432-441, July 2008.  P. Giudici and P. J. Green. Decomposable graphical gaussian model determination.  Biometrika, 86(4):785-801, 1999.  W. Hoe\ufb00ding. A non-parametric test of independence. The Annals of Mathematical Statis-  tics, 19(4):546-557, 1948.  C. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. In Advances in Neural Information Processing Systems 24, pages 2330-2338, 2011.  A. Jalali, C. Johnson, and P. Ravikumar. On learning discrete graphical models using greedy methods. In Advances in Neural Information Processing Systems (NIPS), pages 1935-1943, 2011.  C. C. Johnson, A. Jalali, and P. Ravikumar. High-dimensional sparse inverse covariance estimation using greedy methods. Journal of Machine Learning Research - Proceedings Track, 22:574-582, 2012.  M. Kalisch and P. B\u00a8uhlmann. Estimating high-dimensional directed acyclic graphs with  the PC algorithm. Journal of Machine Learning Research, 8:613-636, 2007.  D. Karger and N. Srebro. Learning Markov networks: Maximum bounded tree-width In Proceedings of the twelfth annual ACM-SIAM symposium on Discrete al-  graphs. gorithms, pages 392-401, 2001.  U. B. Kjaerul\ufb00. Triangulation of graphs - algorithms giving small total state space. Technical Report Research Report R-90-09, Department of Mathematics and Computer Science, Aalborg University, Denmark, 1990.  D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. The  MIT Press, 2009.  posable graphs. (ICML), 2013.  K. S. S. Kumar and F. Bach. Convex relaxations for learning bounded-treewidth decom- In Proceedings of the International Conference on Machine Learning  J. La\ufb00erty, H. Liu, and L. Wasserman. Sparse nonparametric graphical models. Statistical  Science, 27(4):519-537, 2012.  S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society. Series B (Methodological), 50(2):157-224, 1988.  S. L. Lauritzen. Graphical Models. Oxford University Press, USA, 1996.  L. Li and K. C. Toh. An inexact interior point method for (cid:96)1-regularized sparse covariance  selection. Mathematical Programming Computation, 2(3):291-315, 2010. A Junction Tree Framework for Undirected Graphical Model Selection  H. Liu, K. Roeder, and L. Wasserman. Stability approach to regularization selection (stars) for high dimensional graphical models. In Advances in Neural Information Processing Systems (NIPS), 2010.  H. Liu, F. Han, M. Yuan, J. La\ufb00erty, and L. Wasserman. High dimensional semiparametric  Gaussian copula graphical models. Annals of Statistics, 40(4):2293-2326, 2012a.  H. Liu, J. La\ufb00erty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. The Journal of Machine Learning Research, 10: 2295-2328, 2009.  H. Liu, F. Han, and C. Zhang. Transelliptical graphical models. In Advances in Neural  Information Processing Systems (NIPS), pages 809-817, 2012b.  P. Loh and M. J. Wainwright. Structure estimation for discrete graphical models: General- ized covariance matrices and their inverses. In Advances in Neural Information Processing Systems (NIPS), pages 2096-2104, 2012.  Z. Ma, X. Xie, and Z. Geng. Structural learning of chain graphs via decomposition. Journal  of Machine Learning Research, 9:2847-2880, December 2008.  D. M. Malioutov, J. K. Johnson, and A. S. Willsky. Walk-sums and belief propagation in gaussian graphical models. The Journal of Machine Learning Research, 7:2031-2064, 2006.  F. M. Malvestuto. Approximating discrete probability distributions with decomposable models. IEEE Transactions on Systems, Man and Cybernetics, 21(5):1287-1294, 1991.  R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale graphical lasso. Journal of Machine Learning Research, 13:781-794, March 2012. ISSN 1532-4435.  N. Meinshausen and P. B\u00a8uhlmann. Stability selection. Journal of the Royal Statistical  Society: Series B (Statistical Methodology), 72(4):417-473, 2010.  N. Meinshausen and P. B\u00a8uhlmann. High-dimensional graphs and variable selection with the  lasso. Annals of Statistics, 34(3):1436-1462, 2006.  P. Netrapalli, S. Banerjee, S. Sanghavi, and S. Shakkottai. Greedy learning of Markov network structure. In 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1295-1302, 2010.  A. Dobra, C. Hans, B. Jones, J. R. Nevins, and G. Yao, and M. West. Sparse graphical models for exploring gene expression data. Journal of Multivariate Analysis, 90:196-212, 2004.  M. J Rasch, A. Gretton, Y. Murayama, W. Maass, N. K Logothetis, L. Wiskott, G. Kem- permann, L. Wiskott, G. Kempermann, B. Sch\u00a8olkopf, et al. A kernel two-sample test. Journal of Machine Learning Research, 2:299, 2012. Vats and Nowak  P. Ravikumar, M. J. Wainwright, and J. La\ufb00erty. High-dimensional Ising model selection  using (cid:96)1-regularized logistic regression. Annals of Statistics, 38(3):1287-1319, 2010.  P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing (cid:96)1-penalized log-determinant divergence. Electronic Journal of Statistics, 5:935-980, 2011.  N. Robertson and P. D. Seymour. Graph minors. II. Algorithmic aspects of tree-width.  Journal of Algorithms, 7(3):309 - 322, 1986.  K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse covariance selection via alternating linearization methods. In Advances in Neural Information Processing Systems (NIPS), pages 2101-2109, 2010.  T. P. Speed and H. T. Kiiveri. Gaussian Markov distributions over finite graphs. The  Annals of Statistics, 14(1):138-150, 1986.  P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social  Science Computer Review, 9:62-72, 1991.  P. Spirtes, C. Glymour, and R. Scheines. Causality from probability. In Advanced Computing  for the Social Sciences, 1990.  R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal  Statistical Society, Series B, 58(1):267-288, 1996.  S. van de Geer, P. B\u00a8uhlmann, and S. Zhou. The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the lasso). Electronic Journal of Statistics, 5:688-749, 2011.  D. Vats. High-dimensional screening using multiple grouping of variables. IEEE Transac-  tions On Signal Processing, to appear.  D. Vats and J. M. F. Moura. Finding non-overlapping clusters for generalized inference over graphical models. IEEE Transactions on Signal Processing, 60(12):6368 -6381, Dec. 2012.  M. J. Wainwright. Stochastic Processes on Graphs: Geometric and Variational Approaches.  PhD thesis, Department of EECS, Massachusetts Institute of Technology, 2002.  M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery us- ing (cid:96)1-constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5):2183-2202, 2009. ISSN 0018-9448.  W. Wang, M. J. Wainwright, and K. Ramchandran. Information-theoretic bounds on model selection for Gaussian Markov random fields. In IEEE International Symposium on In- formation Theory (ISIT), 2010.  A. Wille, P. Zimmermann, E. Vranov\u00b4a, A. F\u00a8urholz, O. Laule, S. Bleuler, L. Hennig, A. Prelic, P. Von Rohr, L. Thiele, et al. Sparse graphical Gaussian modeling of the isoprenoid gene network in arabidopsis thaliana. Genome Biol, 5(11):R92, 2004. A Junction Tree Framework for Undirected Graphical Model Selection  D. M. Witten, J. H. Friedman, and N. Simon. New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4):892-900, 2011.  X. Xie and Z. Geng. A recursive method for structural learning of directed acyclic graphs.  Journal of Machine Learning Research, 9:459-483, 2008.  L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal  graphical models. The Annals of Statistics, 40(5):2541-2571, 2012.  E. Yang, G. Allen, Z. Liu, and P. Ravikumar. Graphical models via generalized linear models. In Advances in Neural Information Processing Systems, pages 1367-1375, 2012.  J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):2282-2312, 2005.  M. Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model.  Biometrika, 94(1):19-35, 2007.  K. Zhang, J. Peters, D. Janzing, and B. Sch\u00a8olkopf. Kernel-based conditional independence  test and application in causal discovery. Arxiv preprint arXiv:1202.3775, 2012.  H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical  Association, 101(476):1418-1429, 2006. "}, "Axioms for Graph Clustering Quality Functions": {"volumn": 15, "url": "http://jmlr.org/papers/v15/vanlaarhoven14a.html", "header": "Axioms for Graph Clustering Quality Functions", "author": "Twan van Laarhoven, Elena Marchiori", "time": "15(6):193\u2212215, 2014.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume15/vanlaarhoven14a/vanlaarhoven14a.pdf", "keywords": ["graph clustering", "modularity", "axiomatic framework"], "reference": "Margareta Ackerman and Shai Ben-David. Measures of clustering quality: A working set of axioms for clustering. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and L\u00b4eon Bottou, editors, NIPS, pages 121-128. Curran Associates, Inc., 2008.  Margareta Ackerman and Shai Ben-David. A characterization of linkage-based hierarchical  clustering. Journal of Machine Learning Research, 2013.  Margareta Ackerman, Shai Ben-David, and David Loker. Towards property-based classi- fication of clustering paradigms. In John D. La\ufb00erty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors, NIPS, pages 10-18. Curran Associates, Inc., 2010a.  Margareta Ackerman, Shai Ben-David, and David Loker. Characterization of linkage-based clustering. In Adam Tauman Kalai and Mehryar Mohri, editors, COLT, pages 270-281. Omnipress, 2010b. ISBN 978-0-9822529-2-5.  Margareta Ackerman, Shai Ben-David, Simina Br\u02c6anzei, and David Loker. Weighted clus-  tering. In J\u00a8org Ho\ufb00mann and Bart Selman, editors, AAAI. AAAI Press, 2012.  Margareta Ackerman, Shai Ben-David, David Loker, and Sivan Sabato. Clustering oli- garchies. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), volume 31 of JMLR Workshop and Conference Proceedings, pages 66-74, 2013.  Helio Almeida, Dorgival Guedes, Wagner Meira Jr., and Mohammed J. Zaki. Is there a best quality metric for graph clusters? In Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis Vazirgiannis, editors, Machine Learning and Knowledge Discovery in Databases, volume 6911 of Lecture Notes in Computer Science, pages 44-59. Springer Berlin Heidelberg, 2011. ISBN 978-3-642-23779-9.  Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. J. Stat. Mech. Theory Exp., 2008(10):P10008, 2008. ISSN 1742-5468. doi: 10.1088/1742-5468/2008/10/P10008. URL http://dx.doi. org/10.1088/1742-5468/2008/10/P10008.  B\u00b4ela Bollob\u00b4as. The Evolution of Random Graphs - the Giant Component, pages 130-159.  Cambridge University Press, 2001. ISBN 9780521797221.  Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran IEEE Transactions doi:  Nikoloski, and Dorothea Wagner. On modularity clustering. on Knowledge and Data Engineering, 20(2):172-188, 2008. 10.1109/TKDE.2007.190689.  ISSN 1041-4347.  S\u00b4ebastien Bubeck and Ulrike von Luxburg. Nearest neighbor clustering: A baseline method for consistent clustering with arbitrary objective functions. J. Mach. Learn. Res., 10: ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 657-698, June 2009. 1577069.1577092.  213   Axioms for Graph Clustering Quality Functions  References  Margareta Ackerman and Shai Ben-David. Measures of clustering quality: A working set of axioms for clustering. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and L\u00b4eon Bottou, editors, NIPS, pages 121-128. Curran Associates, Inc., 2008.  Margareta Ackerman and Shai Ben-David. A characterization of linkage-based hierarchical  clustering. Journal of Machine Learning Research, 2013.  Margareta Ackerman, Shai Ben-David, and David Loker. Towards property-based classi- fication of clustering paradigms. In John D. La\ufb00erty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors, NIPS, pages 10-18. Curran Associates, Inc., 2010a.  Margareta Ackerman, Shai Ben-David, and David Loker. Characterization of linkage-based clustering. In Adam Tauman Kalai and Mehryar Mohri, editors, COLT, pages 270-281. Omnipress, 2010b. ISBN 978-0-9822529-2-5.  Margareta Ackerman, Shai Ben-David, Simina Br\u02c6anzei, and David Loker. Weighted clus-  tering. In J\u00a8org Ho\ufb00mann and Bart Selman, editors, AAAI. AAAI Press, 2012.  Margareta Ackerman, Shai Ben-David, David Loker, and Sivan Sabato. Clustering oli- garchies. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), volume 31 of JMLR Workshop and Conference Proceedings, pages 66-74, 2013.  Helio Almeida, Dorgival Guedes, Wagner Meira Jr., and Mohammed J. Zaki. Is there a best quality metric for graph clusters? In Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis Vazirgiannis, editors, Machine Learning and Knowledge Discovery in Databases, volume 6911 of Lecture Notes in Computer Science, pages 44-59. Springer Berlin Heidelberg, 2011. ISBN 978-3-642-23779-9.  Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. J. Stat. Mech. Theory Exp., 2008(10):P10008, 2008. ISSN 1742-5468. doi: 10.1088/1742-5468/2008/10/P10008. URL http://dx.doi. org/10.1088/1742-5468/2008/10/P10008.  B\u00b4ela Bollob\u00b4as. The Evolution of Random Graphs - the Giant Component, pages 130-159.  Cambridge University Press, 2001. ISBN 9780521797221.  Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran IEEE Transactions doi:  Nikoloski, and Dorothea Wagner. On modularity clustering. on Knowledge and Data Engineering, 20(2):172-188, 2008. 10.1109/TKDE.2007.190689.  ISSN 1041-4347.  S\u00b4ebastien Bubeck and Ulrike von Luxburg. Nearest neighbor clustering: A baseline method for consistent clustering with arbitrary objective functions. J. Mach. Learn. Res., 10: ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 657-698, June 2009. 1577069.1577092. van Laarhoven and Marchiori  Gunnar Carlsson, Facundo M\u00b4emoli, Alejandro Ribeiro, and Santiago Segarra. Axiomatic construction of hierarchical clustering in asymmetric networks. CoRR, abs/1301.7724, 2013.  Thang N. Dinh and My T. Thai. Community detection in scale-free networks: Approxima- tion algorithms for maximizing modularity. IEEE Journal on Selected Areas in Commu- nications, 31(6):997-1006, 2013.  Santo Fortunato and Marc Barth\u00b4elemy. Resolution limit in community detection. Proc.  Natl. Acad. Sci. USA, 104(1):36-41, 2007. doi: 10.1073/pnas.0605965104.  Sreenivas Gollapudi and Aneesh Sharma. An axiomatic approach for result diversification. In Proceedings of the 18th International Conference on World Wide Web, pages 381-390, 2009.  Benjamin H. Good, Yves A. de Montjoye, and Aaron Clauset. Performance of modularity maximization in practical contexts. Phys. Rev. E, 81(4):046106, April 2010. doi: 10. 1103/PhysRevE.81.046106. URL http://dx.doi.org/10.1103/PhysRevE.81.046106.  Jon M. Kleinberg. An impossibility theorem for clustering. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, NIPS, pages 446-453. MIT Press, 2002. ISBN 0-262-02550-7.  Andrea Lancichinetti and Santo Fortunato. Limits of modularity maximization in commu- nity detection. Phys. Rev. E, 84:066122, December 2011. doi: 10.1103/PhysRevE.84. 066122. URL http://dx.doi.org/10.1103/PhysRevE.84.066122.  Marina Meila. Comparing clusterings: an axiomatic view.  In Proceedings of the 22nd  International Conference on Machine Learning, pages 577-584. ACM, 2005.  Mark E. J. Newman. Finding community structure in networks using the eigenvectors of matrices. Phys. Rev. E, 74(3):036104, July 2006. doi: 10.1103/PhysRevE.74.036104. URL http://dx.doi.org/10.1103/PhysRevE.74.036104.  Mark E. J. Newman and Michelle Girvan. Finding and evaluating community structure in networks. Phys. Rev. E, 69:026113, Feb 2004. doi: 10.1103/PhysRevE.69.026113. URL http://pre.aps.org/abstract/PRE/v69/i2/e026113.  Jan Puzicha, Thomas Hofmann, and Joachim M. Buhmann. A theory of proximity based clustering: Structure detection by optimization. Pattern Recognition, 33:617-634, 1999.  J\u00a8org Reichardt and Stefan Bornholdt. Detecting fuzzy community structures in complex net- works with a Potts model. Phys. Rev. Lett., 93:218701, 2004. doi: 10.1103/PhysRevLett. 93.218701.  J\u00a8org Reichardt and Stefan Bornholdt. Statistical mechanics of community detection. Phys-  ical Review E, 74(1):016110, 2006.  J\u00a8org Reichardt and Stefan Bornholdt. Partitioning and modularity of graphs with arbitrary degree distribution. Phys. Rev. E, 76:015102, Jul 2007. doi: 10.1103/PhysRevE.76. 015102. URL http://link.aps.org/doi/10.1103/PhysRevE.76.015102. Axioms for Graph Clustering Quality Functions  Jianhua Ruan and Weixiong Zhang. Identifying network communities with a high resolution. Phys. Rev. E, 77:016104, Jan 2008. doi: 10.1103/PhysRevE.77.016104. URL http: //link.aps.org/doi/10.1103/PhysRevE.77.016104.  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. volume 22, pages 888-905, Washington, DC, USA, August 2000. IEEE Computer Society. doi: 10.1109/ 34.868688. URL http://dx.doi.org/10.1109/34.868688.  Vincent A. Traag, Paul Van Dooren, and Yurii E. Nesterov. Narrow scope for resolution- limit-free community detection. Phys. Rev. E, 84:016114, Jul 2011. doi: 10.1103/ PhysRevE.84.016114. URL http://link.aps.org/doi/10.1103/PhysRevE.84.016114.  Vincent A. Traag, Gautier Krings, and Paul Van Dooren. Significant scales in community  structure. Submitted, Jun 2013. URL http://arxiv.org/abs/1306.3398.  Twan van Laarhoven and Elena Marchiori. Graph clustering with local search optimization: The resolution bias of the objective function matters most. Phys. Rev. E, 87:012812, Jan 2013. doi: 10.1103/PhysRevE.87.012812. URL http://link.aps.org/doi/10.1103/ PhysRevE.87.012812.  Reza Bosagh Zadeh and Shai Ben-David. A uniqueness theorem for clustering. In Pro- ceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, pages 639-646, Arlington, Virginia, United States, 2009. AUAI Press. ISBN 978-0- 9749039-5-8. URL http://dl.acm.org/citation.cfm?id=1795114.1795189. "}, "Convex vs Non-Convex Estimators for Regression and Sparse Estimation: the Mean Squared Error Properties of ARD and GLasso": {"volumn": 15, "url": "http://jmlr.org/papers/v15/aravkin14a.html", "header": "Convex vs Non-Convex Estimators for Regression and Sparse Estimation: the Mean Squared Error Properties of ARD and GLasso", "author": "Aleksandr Aravkin, James V. Burke, Alessandro Chiuso, Gianluigi Pillonetto", "time": "15(7):217\u2212252, 2014.", "abstract": "We study a simple linear regression problem for grouped variables; we are interested in methods which jointly perform estimation and  variable selection , that is, that automatically set to zero groups of variables in the regression vector. The Group Lasso (GLasso), a well known approach used to tackle this problem which is also a special case of Multiple Kernel Learning (MKL), boils down to solving convex optimization problems. On the other hand, a Bayesian approach commonly known as Sparse Bayesian Learning (SBL), a version of which is the well known Automatic Relevance Determination (ARD), lead to non- convex problems. In this paper we discuss the relation between ARD (and a penalized version which we call PARD) and Glasso, and study their asymptotic properties in terms of the Mean Squared Error in estimating the unknown parameter. The theoretical arguments developed here are independent of the correctness of the prior models and clarify the advantages of PARD over GLasso.", "pdf_url": "http://jmlr.org/papers/volume15/aravkin14a/aravkin14a.pdf", "keywords": ["marginal likelihood"], "reference": "A. Aravkin, J. Burke, A. Chiuso, and G. Pillonetto. On the estimation of hyperparameters for empirical bayes estimators: Maximum marginal likelihood vs minimum MSE. In Proc. IFAC Symposium on System Identification (SysId 2012), 2012.  F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proceedings of the 21st International Conference on Machine Learning, page 4148, 2004.  F.R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine  Learning Research, 9:1179-1225, 2008.  D. Bauer. Asymptotic properties of subspace estimators. Automatica, 41:359-376, 2005.  J.O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics.  Springer, second edition, 1985.  E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger  than n. Annals of Statistics, 35:2313-2351, 2007.  F. Chatelin. Spectral Approximation of Linear Operators. Academic Press, NewYork, 1983.  T. Chen, H. Ohlsson, and L. Ljung. On the estimation of transfer functions, regularization  and gaussian processes - revisited. In IFAC World Congress 2011, Milano, 2011.  A. Chiuso and G. Pillonetto. Nonparametric sparse estimators for identification of large scale linear systems. In Proceedings of IEEE Conf. on Dec. and Control, Atlanta, 2010a.  A. Chiuso and G. Pillonetto. Learning sparse dynamic linear systems using stable spline In Proceedings of Neural Information Processing  kernels and exponential hyperpriors. Symposium, Vancouver, 2010b.  A. Chiuso and G. Pillonetto. A Bayesian approach to sparse dynamic network identification.  Automatica, 48:1553-1565, 2012.  D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289-1306,  2006.  122, 2008.  B. Efron. Microarrays, empirical Bayes and the two-groups model. Statistical Science, 23:  B. Efron and C. Morris. Stein\u2019s estimation rule and its competitors-an empirical bayes  approach. Journal of the American Statistical Association, 68(341):117-130, 1973.  B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of  Statistics, 32:407-499, 2004.  T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods.  Journal of Machine Learning Research, 6:615-637, 2005.  250   Aravkin, Burke, Chiuso and Pillonetto  References  A. Aravkin, J. Burke, A. Chiuso, and G. Pillonetto. On the estimation of hyperparameters for empirical bayes estimators: Maximum marginal likelihood vs minimum MSE. In Proc. IFAC Symposium on System Identification (SysId 2012), 2012.  F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proceedings of the 21st International Conference on Machine Learning, page 4148, 2004.  F.R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine  Learning Research, 9:1179-1225, 2008.  D. Bauer. Asymptotic properties of subspace estimators. Automatica, 41:359-376, 2005.  J.O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics.  Springer, second edition, 1985.  E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger  than n. Annals of Statistics, 35:2313-2351, 2007.  F. Chatelin. Spectral Approximation of Linear Operators. Academic Press, NewYork, 1983.  T. Chen, H. Ohlsson, and L. Ljung. On the estimation of transfer functions, regularization  and gaussian processes - revisited. In IFAC World Congress 2011, Milano, 2011.  A. Chiuso and G. Pillonetto. Nonparametric sparse estimators for identification of large scale linear systems. In Proceedings of IEEE Conf. on Dec. and Control, Atlanta, 2010a.  A. Chiuso and G. Pillonetto. Learning sparse dynamic linear systems using stable spline In Proceedings of Neural Information Processing  kernels and exponential hyperpriors. Symposium, Vancouver, 2010b.  A. Chiuso and G. Pillonetto. A Bayesian approach to sparse dynamic network identification.  Automatica, 48:1553-1565, 2012.  D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289-1306,  2006.  122, 2008.  B. Efron. Microarrays, empirical Bayes and the two-groups model. Statistical Science, 23:  B. Efron and C. Morris. Stein\u2019s estimation rule and its competitors-an empirical bayes  approach. Journal of the American Statistical Association, 68(341):117-130, 1973.  B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of  Statistics, 32:407-499, 2004.  T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods.  Journal of Machine Learning Research, 6:615-637, 2005. Hyperparameter Group Lasso  J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348-1360, december 2001.  T. J. Hastie and R. J. Tibshirani. Generalized additive models. In Monographs on Statistics  and Applied Probability, volume 43. Chapman and Hall, London, UK, 1990.  Roger A. Horn and Charles R. Johnson. Topics in Matrix Analysis. Cambridge University  Press, 1994.  W. James and C. Stein. Estimation with quadratic loss. In Proc. 4th Berkeley Sympos. Math. Statist. and Prob., Vol. I, pages 361-379. Univ. California Press, Berkeley, Calif., 1961.  H. Leeb and B. P\u00a8otscher. Model selection and inference: Facts and fiction. Econometric  Theory, 21:2159, 2005.  D.J.C. Mackay. Bayesian non-linear modelling for the prediction competition. ASHRAE  Trans., 100(2):3704-3716, 1994.  J. S. Maritz and T. Lwin. Empirical Bayes Method. Chapman and Hall, 1989.  T. Park and G. Casella. The Bayesian Lasso. Journal of the American Statistical Associa-  tion, 103(482):681-686, June 2008.  G. Pillonetto and G. De Nicolao. A new kernel-based approach for linear system identifi-  cation. Automatica, 46(1):81-93, 2010.  G. Pillonetto, F. Dinuzzo, and G. De Nicolao. Bayesian online multitask learning of Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(2).  G. Pillonetto, A. Chiuso, and G. De Nicolao. Prediction error identification of linear systems:  a nonparametric Gaussian regression approach. Automatica, 45(2):291-305, 2011.  M. Schmidt, E. Van Den Berg, M. P. Friedlander, and Kevin Murphy. Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. In Proc. of Conf. on Artificial Intelligence and Statistics, pages 456-463, 2009.  C.M. Stein. Estimation of the mean of a multivariate normal distribution. The Annals of  Statistics, 9(6):1135-1151, 1981.  R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal  Statistical Society, Series B., 58, 1996.  M. Tipping. Sparse bayesian learning and the relevance vector machine. Journal of Machine  Learning Research, 1:211-244, 2001.  M.K. Titsias and M. Lzaro-Gredilla. Spike and slab variational inference for multi-task and multiple kernel learning. Advances in Neural Information Processing Systems 25 (NIPS 2011), 2011. Aravkin, Burke, Chiuso and Pillonetto  R. Tomioka and T. Suzuki. Regularization strategies and empirical bayesian learning for  MKL. Journal of Machine Learning Research, 2011.  D.P. Wipf and S. Nagarajan. A new view of automatic relevance determination. In Proc.  of NIPS, 2007.  D.P. Wipf and B.D. Rao. An empirical bayesian strategy for solving the simultaneous sparse approximation problem. IEEE Transactions on Signal Processing, 55(7):3704-3716, 2007.  D.P. Wipf, B.D. Rao, and S. Nagarajan. Latent variable Bayesian models for promoting  sparsity. IEEE Transactions on Information Theory, 57(9):6236-6255, 2011.  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.  Journal of the Royal Statistical Society, Series B, 68:49-67, 2006.  P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning  Research, 7:2541-2563, Nov. 2006.  H. Zou. The adaptive Lasso and it oracle properties. Journal of the American Statistical  Association, 101(476):1418-1429, 2006. "}, "Information Theoretical Estimators Toolbox": {"volumn": 15, "url": "http://jmlr.org/papers/v15/szabo14a.html", "header": "Information Theoretical Estimators Toolbox", "author": "Zolt\u00c3\u00a1n Szab\u00c3\u00b3", "time": "15(9):283\u2212287, 2014.", "abstract": "We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.", "pdf_url": "http://jmlr.org/papers/volume15/szabo14a/szabo14a.pdf", "keywords": ["estimation", "independent subspace analysis and its extensions", "modularity", "Matlab/Octave", "multi-platform", "GNU GPLv3 (\u2265)"], "reference": "Mich\u00e9le Basseville. Divergence measures for statistical data processing - an annotated bib-  liography. Signal Processing, 93:621-633, 2013.  Jan Beirlant, Edward J. Dudewicz, L\u00e1szl\u00f3 Gy\u0151rfi, and Edward C. van der Meulen. Non- parametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6:17-39, 1997.  Jean-Fran\u00e7ois Cardoso. Multidimensional independent component analysis. In International  Conference on Acoustics, Speech, and Signal Processing, pages 1941-1944, 1998.  Seungjin Choi, Andrzej Cichocki, Hyung-Min Park, and Soo-Yound Lee. Blind source sepa- ration and independent component analysis. Neural Information Processing - Letters and Reviews, 6:1-57, 2005.  Andrzej Cichocki and Shun-ichi Amari. Adaptive Blind Signal and Image Processing. John  Wiley & Sons, 2002.  Sons, New York, USA, 1991.  Wiley & Sons, 2001.  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley and  Aapo Hyv\u00e4rinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. John  Barnab\u00e1s P\u00f3czos, Zoubin Ghahramani, and Je\ufb00 Schneider. Copula-based kernel dependency  measures. In International Conference on Machine Learning, pages 775-782, 2012.  Claude E. Shannon. A mathematical theory of communication. Bell System Technical  Journal, 27(3):379-423, 1948.  Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, and Gert R. G. Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550-1599, 2012.  Zolt\u00e1n Szab\u00f3, Barnab\u00e1s P\u00f3czos, and Andr\u00e1s L\u0151rincz. Undercomplete blind subspace decon-  volution. Journal of Machine Learning Research, 8:1063-1095, 2007.  Zolt\u00e1n Szab\u00f3, Barnab\u00e1s P\u00f3czos, and Andr\u00e1s L\u0151rincz. Separation theorem for independent  subspace analysis and its consequences. Pattern Recognition, 45:1782-1791, 2012.  Thomas Villmann and Sven Haase. Mathematical aspects of divergence based vector quan- tization using Fr\u00e9chet-derivatives. Technical report, University of Applied Sciences Mit- tweida, 2010.  Quing Wang, Sanjeev R. Kulkarni, and Sergio Verd\u00fa. Divergence estimation for multidi- mensional densities via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55:2392-2405, 2009.  Donghui Yan, Ling Huang, and Michael I. Jordan. Fast approximate spectral clustering. In International Conference on Knowledge Discovery and Data Mining, pages 907-916, 2009.  287   Information Theoretical Estimators (ITE) Toolbox  References  Mich\u00e9le Basseville. Divergence measures for statistical data processing - an annotated bib-  liography. Signal Processing, 93:621-633, 2013.  Jan Beirlant, Edward J. Dudewicz, L\u00e1szl\u00f3 Gy\u0151rfi, and Edward C. van der Meulen. Non- parametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6:17-39, 1997.  Jean-Fran\u00e7ois Cardoso. Multidimensional independent component analysis. In International  Conference on Acoustics, Speech, and Signal Processing, pages 1941-1944, 1998.  Seungjin Choi, Andrzej Cichocki, Hyung-Min Park, and Soo-Yound Lee. Blind source sepa- ration and independent component analysis. Neural Information Processing - Letters and Reviews, 6:1-57, 2005.  Andrzej Cichocki and Shun-ichi Amari. Adaptive Blind Signal and Image Processing. John  Wiley & Sons, 2002.  Sons, New York, USA, 1991.  Wiley & Sons, 2001.  Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley and  Aapo Hyv\u00e4rinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. John  Barnab\u00e1s P\u00f3czos, Zoubin Ghahramani, and Je\ufb00 Schneider. Copula-based kernel dependency  measures. In International Conference on Machine Learning, pages 775-782, 2012.  Claude E. Shannon. A mathematical theory of communication. Bell System Technical  Journal, 27(3):379-423, 1948.  Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, and Gert R. G. Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550-1599, 2012.  Zolt\u00e1n Szab\u00f3, Barnab\u00e1s P\u00f3czos, and Andr\u00e1s L\u0151rincz. Undercomplete blind subspace decon-  volution. Journal of Machine Learning Research, 8:1063-1095, 2007.  Zolt\u00e1n Szab\u00f3, Barnab\u00e1s P\u00f3czos, and Andr\u00e1s L\u0151rincz. Separation theorem for independent  subspace analysis and its consequences. Pattern Recognition, 45:1782-1791, 2012.  Thomas Villmann and Sven Haase. Mathematical aspects of divergence based vector quan- tization using Fr\u00e9chet-derivatives. Technical report, University of Applied Sciences Mit- tweida, 2010.  Quing Wang, Sanjeev R. Kulkarni, and Sergio Verd\u00fa. Divergence estimation for multidi- mensional densities via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55:2392-2405, 2009.  Donghui Yan, Ling Huang, and Michael I. Jordan. Fast approximate spectral clustering. In International Conference on Knowledge Discovery and Data Mining, pages 907-916, 2009. "}, "Early Stopping and Non-parametric Regression: An Optimal Data-dependent Stopping Rule": {"volumn": 15, "url": "http://jmlr.org/papers/v15/raskutti14a.html", "header": "Early Stopping and Non-parametric Regression: An Optimal Data-dependent Stopping Rule", "author": "Garvesh Raskutti, Martin J. Wainwright, Bin Yu", "time": "15(11):335\u2212366, 2014.", "abstract": "Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non- parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient- descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the $L^2(\\mathbb{P})$ and $L^2(\\mathbb{P}_n)$ norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator.", "pdf_url": "http://jmlr.org/papers/volume15/raskutti14a/raskutti14a.pdf", "keywords": ["rule", "reproducing kernel hilbert space", "rademacher complexity", "empirical processes"], "reference": "R. S. Anderssen and P. M. Prenter. A formal comparison of methods proposed for the numerical solution of first kind integral equations. Jour. Australian Math. Soc. (Ser. B), 22:488-500, 1981.  N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical  Society, 68:337-404, 1950.  A. R. Barron, A. Cohen, W. Dahmen, and R. A. DeVore. Approximation and learning by  greedy algorithms. Annals of Statistics, 36(1):64-94, 2008.  P. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research,  8:2347-2368, 2007.  P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of  Statistics, 33(4):1497-1537, 2005.  F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory.  J. Complexity, 23:52-72, 2007.  M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of  the classes W \u03b1  p . Math. USSR-Sbornik, 2(3):295-317, 1967.  G. Blanchard and M. Kramer. Optimal learning rates for kernel conjugate gradient regres-  sion. In Proceedings of the NIPS Conference, 2010.  P. Buhlmann and B. Yu. Boosting with L2 loss: Regression and classification. Journal of  American Statistical Association, 98:324-340, 2003.  A. Caponetto and Y. Yao. Adaptation for regularization operators in learning theory. Technical Report CBCL Paper #265/AI Technical Report #063, Massachusetts Institute of Technology, September 2006.  A. Caponetto and Y. Yao. Cross-validation based adaptation for regularization operators  in learning theory. Analysis and Applications, 8(2):161-183, 2010.  A. Caponneto. Optimal rates for regularization operators in learning theory. Technical Report CBCL Paper #264/AI Technical Report #062, Massachusetts Institute of Tech- nology, September 2006.  P. Drineas and M. W. Mahoney. On the Nystr\u00a8om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, 2005.  Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  J.H. Friedman and B. Popescu. Gradient directed regularization. Technical report, Stanford  University, 2004.  364   Raskutti, Wainwright and Yu  References  R. S. Anderssen and P. M. Prenter. A formal comparison of methods proposed for the numerical solution of first kind integral equations. Jour. Australian Math. Soc. (Ser. B), 22:488-500, 1981.  N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical  Society, 68:337-404, 1950.  A. R. Barron, A. Cohen, W. Dahmen, and R. A. DeVore. Approximation and learning by  greedy algorithms. Annals of Statistics, 36(1):64-94, 2008.  P. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research,  8:2347-2368, 2007.  P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of  Statistics, 33(4):1497-1537, 2005.  F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory.  J. Complexity, 23:52-72, 2007.  M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of  the classes W \u03b1  p . Math. USSR-Sbornik, 2(3):295-317, 1967.  G. Blanchard and M. Kramer. Optimal learning rates for kernel conjugate gradient regres-  sion. In Proceedings of the NIPS Conference, 2010.  P. Buhlmann and B. Yu. Boosting with L2 loss: Regression and classification. Journal of  American Statistical Association, 98:324-340, 2003.  A. Caponetto and Y. Yao. Adaptation for regularization operators in learning theory. Technical Report CBCL Paper #265/AI Technical Report #063, Massachusetts Institute of Technology, September 2006.  A. Caponetto and Y. Yao. Cross-validation based adaptation for regularization operators  in learning theory. Analysis and Applications, 8(2):161-183, 2010.  A. Caponneto. Optimal rates for regularization operators in learning theory. Technical Report CBCL Paper #264/AI Technical Report #062, Massachusetts Institute of Tech- nology, September 2006.  P. Drineas and M. W. Mahoney. On the Nystr\u00a8om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153-2175, 2005.  Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.  J.H. Friedman and B. Popescu. Gradient directed regularization. Technical report, Stanford  University, 2004. Early Stopping and Non-parametric Regression  C. Gu. Smoothing Spline ANOVA Models. Springer Series in Statistics. Springer, New York,  NY, 2002.  M. G. Gu and H. T. Zhu. Maximum likelihood estimation by markov chain monte carlo  approximation. J. R. Statist. Soc. B, 63:339-355, 2001.  P. Hall and J.S. Marron. On variance estimation in nonparametric regression. Biometrika,  77:415-419, 1990.  W. Jiang. Process consistency for adaboost. Annals of Statistics, 32:13-29, 2004.  G. Kimeldorf and G. Wahba. Some results on Tchebyche\ufb03an spline functions. Jour. Math.  Anal. Appl., 33:82-95, 1971.  V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization.  Annals of Statistics, 34(6):2593-2656, 2006.  M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Mono-  graphs. American Mathematical Society, Providence, RI, 2001.  L. Mason, J. Baxter, P., and M. Frean. Boosting algorithms as gradient descent. In Neural  Information Processing Systems (NIPS), December 1999.  S. Mendelson. Geometric parameters of kernel machines. In Proceedings of COLT, pages  29-43, 2002.  J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society A, 209:415-446, 1909.  N. Morgan and H. Bourlard. Generalization and parameter estimation in feedforward nets:  Some experiments. In Proceedings of Neural Information Processing Systems, 1990.  L. Orecchia and M. W. Mahoney. Implementing regularization implicitly via approximate  eigenvector computation. In ICML \u201911, 2011.  G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Journal of Machine Learning Research, 12: 389-427, March 2012.  S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman Scientific &  Technical, Harlow, UK, 1988.  B. Sch\u00a8olkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.  C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of  Statistics, 9(6):1135-1151, 1981.  C. J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13  (2):689-705, 1985.  O. N. Strand. Theory and methods related to the singular value expansion and Landweber\u2019s iteration for integral equations of the first kind. SIAM J. Numer. Anal., 11:798-825, 1974. Raskutti, Wainwright and Yu  S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.  E. De Vito, S. Pereverzyev, and L. Rosasco. Adaptive kernel methods using the balancing  principle. Foundations of Computational Mathematics, 10(4):455-479, 2010.  G. Wahba. Three topics in ill-posed problems. In M. Engl and G. Groetsch, editors, Inverse  and ill-posed problems, pages 37-50. Academic Press, 1987.  G. Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference Series  in Applied Mathematics. SIAM, Philadelphia, PN, 1990.  H. L. Weinert, editor. Reproducing Kernel Hilbert Spaces : Applications in Statistical Signal  Processing. Hutchinson Ross Publishing Co., Stroudsburg, PA, 1982.  F. T. Wright. A bound on tail probabilities for quadratic forms in independent random variables whose distributions are not necessarily symmetric. Annals of Probability, 1(6): 1068-1070, 1973.  Y. Yang and A. Barron. Information-theoretic determination of minimax rates of conver-  gence. Annals of Statistics, 27(5):1564-1599, 1999.  Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning.  Constructive Approximation, 26:289-315, 2007.  T. Zhang. Learning bounds for kernel regression using e\ufb00ective data dimensionality. Neural  Computation, 17(9):2077-2098, 2005.  T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annal  of Statistics, 33:1538-1579, 2005. "}, "Unbiased Generative Semi-Supervised Learning": {"volumn": 15, "url": "http://jmlr.org/papers/v15/foxroberts14a.html", "header": "Unbiased Generative Semi-Supervised Learning", "author": "Patrick Fox-Roberts, Edward Rosten", "time": "15(12):367\u2212443, 2014.", "abstract": "Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution, a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs---an unusual property for a semi- supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence.", "pdf_url": "http://jmlr.org/papers/volume15/foxroberts14a/foxroberts14a.pdf", "keywords": ["Kullback-Leibler", "semi-supervised", "asymptotic bounds", "bias", "generative model"], "reference": "M. Balcan and A. Blum. An augmented PAC model for semi-supervised learning. In Oliver Chapelle, B. Sch\u00a8olkopf, and A. Zien, editors, Semi-Supervised Learning, pages 383-404. MIT Press, 2005.  C. Beecks, A. Ivanescu, S. Kirchho\ufb00, and T. Seidl. Modeling image similarity by gaussian mixture models and the signature quadratic form distance. In Computer Vision (ICCV), 2011 IEEE International Conference On, pages 1754-1761, 2011.  C. Bishop. Pattern Recognition And Machine Learning. Springer, 2006.  A. Blum and N. Balcan. A discriminative model for semi-supervised learning. In Journal  Of The ACM (JACM), volume 57, pages 19:1-19:46, 2010.  A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training.  In Proceedings Of The Workshop On Computational Learning Theory, pages 92-100, 1998.  V. Castelli and T. Cover. On the exponential value of labeled samples. Pattern Recognition  Letters, 16(1):105 - 111, 1995.  V. Castelli and T. Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. Information Theory, IEEE Transactions on, 42(6):2102 - 2117, 1996.  F. Cozman and I. Cohen. Unlabeled data can degrade classification performance of gener- ative classifiers. In Fifteenth International Florida Artificial Intelligence Society Confer- ence, pages 327-331, 2002.  F. Cozman and I. Cohen. Risks of semi-supervised learning: How unlabelled data can degrade performance of generative classifiers. In O. Chapelle, B. Sch\u00a8olkopf, and A. Zien, editors, Semi-Supervised Learning, pages 57-72. MIT press, 2006.  F. Cozman, I. Cohen, M. Cirelo, and E. Polit\u00b4ecnica. Semi-supervised learning of mixture In Proceedings Of The 20th International Conference On Machine Learning  models. (ICML), pages 99-106, 2003.  J. Dillon, K. Balasubramanian, and G. Lebanon. Asymptotic analysis of generative semi- supervised learning. In Proceedings Of The 27th International Conference On Machine Learning (ICML), 2010.  G. Druck, C. Pal, A. McCallum, and X. Zhu. Semi-supervised classification with hybrid generative/discriminative methods. In KDD \u201907: Proceedings Of The 13th ACM SIGKDD International Conference On Knowledge Discovery And Data Mining, pages 280-289, 2007.  A. Frank and A. Asuncion. UCI machine learning repository, 2010. URL http://archive.  ics.uci.edu/ml.  151-168. MIT Press, 2006.  Y. Grandvalet and Y. Bengio. Entropy Regularization. In Semi-Supervised Learning, pages  440   Fox-Roberts and Rosten  References  M. Balcan and A. Blum. An augmented PAC model for semi-supervised learning. In Oliver Chapelle, B. Sch\u00a8olkopf, and A. Zien, editors, Semi-Supervised Learning, pages 383-404. MIT Press, 2005.  C. Beecks, A. Ivanescu, S. Kirchho\ufb00, and T. Seidl. Modeling image similarity by gaussian mixture models and the signature quadratic form distance. In Computer Vision (ICCV), 2011 IEEE International Conference On, pages 1754-1761, 2011.  C. Bishop. Pattern Recognition And Machine Learning. Springer, 2006.  A. Blum and N. Balcan. A discriminative model for semi-supervised learning. In Journal  Of The ACM (JACM), volume 57, pages 19:1-19:46, 2010.  A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training.  In Proceedings Of The Workshop On Computational Learning Theory, pages 92-100, 1998.  V. Castelli and T. Cover. On the exponential value of labeled samples. Pattern Recognition  Letters, 16(1):105 - 111, 1995.  V. Castelli and T. Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. Information Theory, IEEE Transactions on, 42(6):2102 - 2117, 1996.  F. Cozman and I. Cohen. Unlabeled data can degrade classification performance of gener- ative classifiers. In Fifteenth International Florida Artificial Intelligence Society Confer- ence, pages 327-331, 2002.  F. Cozman and I. Cohen. Risks of semi-supervised learning: How unlabelled data can degrade performance of generative classifiers. In O. Chapelle, B. Sch\u00a8olkopf, and A. Zien, editors, Semi-Supervised Learning, pages 57-72. MIT press, 2006.  F. Cozman, I. Cohen, M. Cirelo, and E. Polit\u00b4ecnica. Semi-supervised learning of mixture In Proceedings Of The 20th International Conference On Machine Learning  models. (ICML), pages 99-106, 2003.  J. Dillon, K. Balasubramanian, and G. Lebanon. Asymptotic analysis of generative semi- supervised learning. In Proceedings Of The 27th International Conference On Machine Learning (ICML), 2010.  G. Druck, C. Pal, A. McCallum, and X. Zhu. Semi-supervised classification with hybrid generative/discriminative methods. In KDD \u201907: Proceedings Of The 13th ACM SIGKDD International Conference On Knowledge Discovery And Data Mining, pages 280-289, 2007.  A. Frank and A. Asuncion. UCI machine learning repository, 2010. URL http://archive.  ics.uci.edu/ml.  151-168. MIT Press, 2006.  Y. Grandvalet and Y. Bengio. Entropy Regularization. In Semi-Supervised Learning, pages Unbiased Generative Semi-Supervised Learning  T. Ho and E. Kleinberg. Building projectable classifiers of arbitrary complexity. In Inter- national Conference On Pattern Recognition (ICPR), volume 2, pages 880-885, 1996.  C. Hsu, C. Chang, and C. Lin. A practical guide to support vector classification. Technical  report, Department of Computer Science, National Taiwan University, 2003.  G. Hughes. On the mean accuracy of statistical pattern recognizers. Information Theory,  IEEE Transactions On, 14(1):55 - 63, 1968.  T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Proceedings Of The 1998 Conference On Advances In Neural Information Processing Systems II, pages 487-493, 1999.  E. Jaynes. Probability Theory. Cambridge University Press, 2003.  H. Kang, S. Yoo, and D. Han. Senti-lexicon and improved na\u00a8\u0131ve bayes algorithms for sentiment analysis of restaurant reviews. Expert Systems With Applications, 39(5):6000 - 6010, 2012.  B. Krishnapuram, D. Williams, Y. Xue, A. Hartemink, L. Carin, and M. Figueiredo. On In Advances In Neural Information Processing Systems  semi-supervised classification. (NIPS), pages 721-728, 2005.  J. Lagarias, J. Reeds, M. Wright, and P. Wright. Convergence properties of the Nelder- Mead simplex method in low dimensions. SIAM Journal Of Optimization, 9(1):112-147, 1998.  J. Lasserre, C. Bishop, and T. Minka. Principled hybrids of generative and discriminative models. In Computer Vision And Pattern Recognition (CVPR), volume 1, pages 87 - 94, 2006.  J. L\u00a8ucke and J. Eggert. Expectation truncation and the benefits of preselection in training generative models. In Journal Of Machine Learning Research (JMLR), pages 2855-2900, October 2010.  D. MacKay. Information Theory, Inference, And Learning Algorithms. Cambridge Univer-  sity Press, 2003.  G. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning via Expec- tation Regularization. In Proceedings Of The 24th International Conference On Machine Learning (ICML), pages 593-600, 2007.  A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Genera- tive/discriminative training for clustering and classification. In National Conference On Artificial Intelligence, pages 433-439, 2006.  A. Ng and M. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes. Advances In Neural Information Processing Systems (NIPS), 2:841-848, 2002. Fox-Roberts and Rosten  K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and  unlabeled documents using EM. Machine Learning, 39:103-134, 2000.  J. Nocedal and S. Wright. Numerical Optimization, Springer Series In Operations Research.  Springer-Verlag, 1999.  J. Ratsaby and S. Venkatesh. Learning from a mixture of labeled and unlabeled examples In Conference On Learning Theory (COLT), pages  with parametric side information. 412-417, 1995.  I. Rauschert and R. Collins. A generative model for simultaneous estimation of human body shape and pixel-level segmentation. In Proceedings Of The 12th European Conference On Computer Vision - Volume Part V, European Conference On Computer Vision (ECCV), pages 704-717. Springer-Verlag, 2012.  S. Rosset, J. Zhu, H. Zou, and T. Hastie. A method for inferring label sampling mecha- nisms in semi-supervised learning. In Advances In Neural Information Processing Systems (NIPS), volume 17, pages 1161 - 1168, 2005.  B. Shahshahani and D. Landgrebe. The e\ufb00ect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. Geoscience And Remote Sensing, IEEE Transactions on, 32(5):1087 -1095, 1994.  A. Subramanya and J. Bilmes. Soft-supervised learning for text classification. In Proceedings Of The Conference On Empirical Methods In Natural Language Processing, pages 1090- 1099, 2008.  A. Subramanya and J. Bilmes. Entropic graph regularization in non-parametric semi- supervised classification. In Advances In Neural Information Processing Systems (NIPS), December 2009.  U. Syed and B. Taskar. Semi-supervised learning with adversarially missing label informa- tion. In Advances In Neural Information Processing Systems (NIPS), pages 2244-2252, 2010.  M. Szummer and T. Jaakkola. Information Regularization with partially labeled data. In  Advances In Neural Information Processing Systems (NIPS), 2002.  V. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc, 1998.  J. Wang, X. Shen, and W. Pan. On transductive support vector machines. In Prediction  And Discovery. American Mathematical Society, 2007.  T. Yang and C. Priebe. The e\ufb00ect of model misspecification on semi-supervised classifica-  tion. Pattern Analysis And Machine Intelligence (PAMI), 33:2093-2103, 2011.  I. Yeh, K. Yang, and T. Ting. Knowledge discovery on RFM model using Bernoulli sequence.  Expert Systems With Applications, 36(3, Part 2):5866 - 5871, 2009.  T. Zhang. The value of unlabeled data for classification problems. In International Con-  ference On Machine Learning (ICML), pages 1191-1198, 2000. Unbiased Generative Semi-Supervised Learning  X. Zhu. Semi-supervised learning literature survey. Technical report, Department of Com-  puter Sciences, University of Wisconsin, Madison, 2005.  F. Zhuang, P. Luo, Z. Shen, Q. He, Y. Xiong, Z. Shi, and H. Xiong. Mining distinction and commonality across multiple domains using generative model for text classification. Knowledge And Data Engineering, IEEE Transactions On, 24(11):2025-2039, 2012. "}, "Node-Based Learning of Multiple Gaussian Graphical Models": {"volumn": 15, "url": "http://jmlr.org/papers/v15/mohan14a.html", "header": "Node-Based Learning of Multiple Gaussian Graphical Models", "author": "Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee", "time": "15(13):445\u2212488, 2014.", "abstract": "We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a  node-based  approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the $K$ networks are due to individual nodes that are  perturbed  across conditions, or (2) similarities among the $K$ networks are due to the presence of  common hub nodes  that are shared across all $K$ networks. Using a  row-column overlap norm  penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set.", "pdf_url": "http://jmlr.org/papers/volume15/mohan14a/mohan14a.pdf", "keywords": ["graphical model", "structured sparsity", "alternating direction method of multi pliers", "gene regulatory network", "lasso", "multivariate normal"], "reference": "A. Argyriou, C.A. Micchelli, and M. Pontil. E\ufb03cient first order methods for linear composite  regularizers. arXiv:1104.1436 [cs.LG], 2011.  O. Banerjee, L. E. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maxi- mum likelihood estimation for multivariate Gaussian or binary data. JMLR, 9:485-516, 2008.  484   Mohan, London, Fazel, Witten and Lee  F.2 Updates for ADMM Algorithm for CNJGL Let L\u03c1({\u0398i}, {Zi}, { \u02dcV i}, {W i}, {F i}, {Gi}, {Qi}) denote the augmented Lagrangian (16). Below, we derive the update rules for the primal variables { \u02dcV i}. The update rules for the other primal variables are similar to the derivations discussed for PNJGL, and hence we omit their derivations.  The update rules for \u02dcV 1, \u02dcV 2, . . . , \u02dcV K are coupled, so we derive them simultaneously.  i=1, {F i}K  i=1, {Gi}K  i=1, {Qi}K i=1  (cid:1)  Note that { \u02dcV i}K  i=1 = argmin A1,...,AK  L\u03c1  = argmin A1,...,AK  \u03bb2  i=1, {Ai}K i=1, {W i}K \uf8f9  (cid:0){\u0398i}K i=1, {Zi}K (cid:13) \uf8ee (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)  A1 \u2212 diag(A1) ... AK \u2212 diag(AK)  p (cid:88)  \uf8ef \uf8f0  j=1  (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) q  \uf8fa \uf8fb  j  +  (cid:18)  \u03c1  K (cid:88)  i=1  (cid:13) (cid:13) Ai \u2212 (cid:13) (cid:13)  1 2  (W i)T + \u0398i \u2212 W i +  (F i \u2212 Gi)  1 \u03c1  (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) F  .  Let Ci = 1  2 ((W i)T + \u0398i \u2212 W i + 1 \u03c1 (F i \u2212 Gi)). Then the update \uf8ee \uf8ee \uf8f9  \uf8eb  \uf8f6  \uf8ee  \uf8f9  \u02dcV 1 ... \u02dcV K  \uf8ef \uf8f0  \uf8fa \uf8fb \u2190 Tq  \uf8ec \uf8ed  \uf8ef \uf8f0  C1 \u2212 diag(C1) ... CK \u2212 diag(CK)  \uf8fa \uf8fb ,  \u03bb2 2\u03c1  \uf8f7 \uf8f8 +  \uf8ef \uf8f0  diag(C1) ... diag(CK)  \uf8f9  \uf8fa \uf8fb  follows by inspection.  Appendix G. Additional Simulation Results  Here we present more detailed results for an instance of the simulation study described in Section 6, for the case n = 25. Figure 16 illustrates how the PPC, TPPC, PCC and TPCC metrics are computed. As described in Table 1, for PNJGL, PPC is given by the number of columns of \u02c6V whose (cid:96)2 norms exceed the threshold ts. Figure 16(a) indicates that the two perturbed nodes in the data are identified as perturbed by PNJGL. Furthermore, given the large gap between the perturbed and non-perturbed columns, PPC is relatively insensitive to the choice of ts. Similar results apply to the TPPC, PCC and TPCC metrics.  In order to generate Figure 16, PNJGL, FGL, CNJGL, GGL, and GL were performed using tuning parameter values that led to the best identification of perturbed and cohub nodes. However, the results displayed in Figure 16 were quite robust to the choice of tuning parameter.  References  A. Argyriou, C.A. Micchelli, and M. Pontil. E\ufb03cient first order methods for linear composite  regularizers. arXiv:1104.1436 [cs.LG], 2011.  O. Banerjee, L. E. El Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maxi- mum likelihood estimation for multivariate Gaussian or binary data. JMLR, 9:485-516, 2008. Node-based Learning of Multiple GGMs  (a) \u02c6V : PNJGL  (b) ( \u02c6\u03981 \u2212 \u02c6\u03982): FGL  (c) ( \u02c6\u03981 \u2212 \u02c6\u03982): GL  (d) \u02c6V 1: CNJGL  (e) \u02c6\u03981: GGL  (f) \u02c6\u03981: GL  (g) \u02c6V 2: CNJGL  (h) \u02c6\u03982: GGL  (i) \u02c6\u03982: GL  Figure 16: In all plots, the x-axis indexes the columns of the indicated matrix, and the y- axis displays the (cid:96)2 norms of the columns of the indicated matrix, with diagonal elements removed. The sample size is n = 25. Perturbed nodes are indicated in red (with square markers), and cohub nodes are indicated in blue (with circle markers). (a)-(c): Detection of perturbed nodes by PNJGL with q = 2, FGL, and GL. (d)-(i): Detection of cohub nodes by CNJGL with q = 2, GGL, and GL. (a): PNJGL with q = 2 was performed with \u03bb1 = 2.5 and \u03bb2 = 12.5. (b): FGL was performed with \u03bb1 = 2.5 and \u03bb2 = 0.75. (c): GL was performed with \u03bb = 1.5. (d), (g): CNJGL was performed with q = 2 and \u03bb1 = 0.5, \u03bb2 = 37.5. (e), (h): GGL was performed with \u03bb1 = 0.5 and \u03bb2 = 2.5. (f ), (i): GL was performed with \u03bb = 0.75.  S.P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in ML, 3(1):1-122, 2010.  E.J. Candes, M. Wakin, and S. Boyd. Enhancing sparsity by reweighted l1 minimization.  Journal of Fourier Analysis and Applications, 14:877-905, 2007.  A. Cardoso-Cachopo, 2009. URL http://web.ist.utl.pt/acardoso/datasets/.  N. Chalhoub and S.J. Baker. PTEN and the PI3-kinase pathway in cancer. Annual Review  of Pathology, 4:127-150, 2009.050100123050100123050100123050100123050100123050100123050100123050100123050100123 Mohan, London, Fazel, Witten and Lee  X. Chen, Q. Lin, S. Kim, J.G. Carbonell, and E.P. Xing. Smoothing proximal gradient method for general structured sparse learning. Proceedings of the Conference on Uncer- tainty in Artificial Intelligence, 2011.  P. Danaher, P. Wang, and D. Witten. The joint graphical lasso for inverse covariance estimation across multiple classes. Journal of the Royal Statistical Society, Series B, 2013.  A. D\u2019Aspremont, O. Banerjee, and L. El Ghaoui. First-order methods for sparse covariance  selection. SIAM Journal on Matrix Analysis and Applications, 30(1):56-66, 2008.  J. Duchi and Y. Singer. E\ufb03cient online and batch learning using forward backward splitting.  Journal of Machine Learning Research, pages 2899 - 2934, 2009.  J. Eckstein. Augmented lagrangian and alternating direction methods for convex optimiza- tion: A tutorial and some illustrative computational results. Technical Report RUTCOR Research Report RRR 32-2012, Rutgers University, 2012.  J. Eckstein and D.P. Bertsekas. On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Programing: Series A and B, 55(3):293 - 318, 1992.  J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the  graphical lasso. Biostatistics, 9:432-441, 2007.  D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathematics with Applications, 2(1): 17-40, 1976.  A.J. Gentles, A.A. Alizadeh, S.-I. Lee, J.H. Myklebust, C.M. Shachaf, R. Levy, D. Koller, and S.K. Plevritis. A pluripotency signature predicts histologic transformation and in- \ufb02uences survival in follicular lymphoma patients. Blood, 114(15):3158-66, 2009.  M. George. B-a scale-free network generation and visualization, 2007. Matlab code.  M. Grant and S. Boyd. cvx version 1.21. \u201chttp://cvxr.com/cvx\u201d, October 2010.  J. Guo, E. Levina, G. Michailidis, and J. Zhu. Joint estimation of multiple graphical models.  Biometrika, 98(1):1-15, 2011.  D. Han and Z. Yuan. A note on the alternating direction method of multipliers. Journal of  Optimization Theory and Applications, 155(1):227-238, 2012.  S. Hara and T. Washio. Learning a common substructure of multiple graphical gaussian  models. Neural Networks, 38:23-38, 2013.  B. He, M. Tao, and X. Yuan. Alternating direction method with gaussian back substitution for separable convex programming. SIAM Journal of Optimization, pages 313 - 340, 2012. Node-based Learning of Multiple GGMs  M. Hong and Z. Luo. On the linear convergence of the alternating direction method of  multipliers. arXiv:1208:3922 [math.OC], 2012.  C.J. Hsieh, M. Sustik, I. Dhillon, and P. Ravikumar. Sparse inverse covariance estimation using quadratic approximation. Advances in Neural Information Processing Systems, 2011.  L. Jacob, G. Obozinski, and J.P. Vert. Group lasso with overlap and graph lasso. Proceedings  of the 26th International Conference on Machine Learning, 2009.  W. Evan Johnson and C. Li. Adjusting batch e\ufb00ects in microarray expression data using  empirical bayes methods. Biostatistics, 8(1):118-27, 2006.  M. Kolar, L. Song, A. Ahmed, and E.P. Xing. Estimating time-varying networks. Annals  of Applied Statistics, 4 (1):94-123, 2010.  S.L. Lauritzen. Graphical Models. Oxford Science Publications, 1996.  W. K. Lim, J. Wang, C. Lefebvre, and A.. Clifano. Comparative analysis of microarray normalization procedures: e\ufb00ects on reverse engineering gene networks. Bioinformatics, 23(13):282-288, 2007.  M. Lobo, M. Fazel, and S. Boyd. Portfolio optimization with linear and fixed transaction  costs. Annals of Operations Research, 152(1):376 - 394, 2007.  S. Ma, L. Xue, and H. Zou. Alternating direction methods for latent variable Gaussian  graphical model selection. Neural Computation, 2013.  K.V. Mardia, J. Kent, and J.M. Bibby. Multivariate Analysis. Academic Press, 1979.  L. Matthews, G. Gopinath, M. Gillespie, M. Caudy, D. Croft, B. de Bono, P. Garap- ati, J. Hemish, H. Hermjakob, B. Jassal, A. Kanapin, S. Lewis, S. Mahajan, B. May, E. Schmidt, I. Vastrik, G. Wu, E. Birney, L. Stein, and P. D\u2019Eustachio. Reactome knowl- edgebase of biological pathways and processes. Nucleic Acids Research, 37:D619-22, 2008.  R. Mazumder and T. Hastie. Exact covariance-thresholding into connected components for large-scale graphical lasso. Journal of Machine learning Research, 13:723 - 736, 2012.  M. Meinshausen and P. Buhlmann. Stability selection (with discussion). Journal of the  Royal Statistical Society, Series B, 72:417-473, 2010.  K. Mohan, M. Chung, S. Han, D. Witten, S. Lee, and M. Fazel. Structured learning of gaussian graphical models. Advances in Neural Information Processing Systems, 2012.  S. Mosci, S. Villa, A. Verri, and L. Rosasco. A primal-dual algorithm for group sparse regu- larization with overlapping groups. Advances in Neural Information Processing Systems, pages 2604 - 2612, 2010.  J.F.C Mota, J.M.F Xavier, P.M.Q Aguiar, and M. Puschel. A proof of convergence for the alternating direction method of multipliers applied to polyhedral-constrained functions. arXiv:1112.2295 [math.OC], 2011. Mohan, London, Fazel, Witten and Lee  G. Obozinski, L. Jacob, and J.P. Vert. Group lasso with overlaps: the latent group lasso  approach. arXiv preprint arXiv:1110.0413, 2011.  P. Ravikumar, M.J. Wainwright, G. Raskutti, and B. Yu. Model selection in gaussian graphical models: high-dimensional consistency of l1-regularized MLE. Advances in Neu- ral Information Processing Systems, 2008.  P. Ravikumar, M.J. Wainwright, and J.D. La\ufb00erty. High-dimensional Ising model selection  using l1-regularized logistic regression. Annals of Statisitcs, 38(3):1287-1319, 2010.  A. Rothman, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation.  Electronic Journal of Statistics, 2:494-515, 2008.  K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse covariance selection via alternating  linearization methods. Advances in Neural Information Processing Systems, 2010.  M. Tao and X. Yuan. Recovering low-rank and sparse components of matrices from incom-  plete and noisy observations. SIAM J. Optimization, 21(1):57-81, 2011.  R.E. Tarjan. Depth-first search and linear graph algorithms. SIAM Journal on Computing,  1(2):146-160, 1972.  R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal  Statistical Society, Series B, 58:267-288, 1996.  R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via  the fused lasso. Journal of the Royal Statistical Society, Series B, 67:91-108, 2005.  R.G.W. Verhaak et al. Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1. Cancer Cell, 17(1):98-110, 2010.  D.M. Witten, J.H. Friedman, and N. Simon. New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4):892-900, 2011.  E. Yang, P. Ravikumar, G.I. Allen, and Z. Liu. Graphical models via generalized linear  models. Advances in Neural Information Processing Systems, 2012.  M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model.  Biometrika, 94(10):19-35, 2007a.  M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.  Journal of the Royal Statistical Society, Series B, 68:49-67, 2007b.  B. Zhang and Y. Wang. Learning structural changes of Gaussian graphical models in controlled experiments. Proc. 26th Conference on Uncertainty in Artifical Intelligence, 2010.  H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical  Association, 101:1418-1429, 2006. "}, "The FASTCLIME Package for Linear Programming and Large-Scale Precision Matrix Estimation in R": {"volumn": 15, "url": "http://jmlr.org/papers/v15/pang14a.html", "header": "The FASTCLIME Package for Linear Programming and Large-Scale Precision Matrix Estimation in R", "author": "Haotian Pang, Han Liu, Robert V, erbei", "time": "15(14):489\u2212493, 2014.", "abstract": "We develop an  R  package FASTCLIME for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalable and sophisticated tool for solving large- scale linear programs. As an illustrative example, one use of our LP solver is to implement an important sparse precision matrix estimation method called CLIME (Constrained $L_1$ Minimization Estimator). Compared with existing packages for this problem such as CLIME and FLARE, our package has three advantages: (1) it efficiently calculates the full piecewise- linear regularization path; (2) it provides an accurate dual certificate as stopping criterion; (3) it is completely coded in C and is highly portable. This package is designed to be useful to statisticians and machine learning researchers for solving a wide range of problems.", "pdf_url": "http://jmlr.org/papers/volume15/pang14a/pang14a.pdf", "keywords": ["high dimensional data", "sparse precision matrix", "linear programming", "para metric simplex method", "undirected graphical model"], "reference": "O. Banerjee, L. E. Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum  likelihood estimation. Journal of Machine Learning Research, 9:485-516, 2008.  T. Cai, W. Liu, and X. Luo. A constrained l1 minimization approach to sparse precision  matrix estimation. J. American Statistical Association, 106:594-607, 2011.  J. Friedman, T. Hastie, H. H\u00a8o\ufb02ing, and R. Tibshirani. Pathwise coordinate optimization.  Annals of Applied Statistics, 1(2):302-332, 2007a.  J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the  graphical lasso. Biostatistics, 9(3):432-441, 2007b.  J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear  models via coordinate descent. Journal of Statistical Software, 33(1), 2010.  C-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. Advances in Neural Information Processing Systems, 24, 2011.  X. Li, T. Zhao, X. Yuan, and H. Liu. An R package \ufb02are for high dimensional linear  regression and precision matrix estimator. R Package Vigette, 2012.  N. Meinshausen and P. B\u00a8uhlmann. High dimensional graphs and variable selection with the  lasso. Annals of Statistics, 34(3):1436-1462, 2006.  R. Vanderbei. Linear Programming, Fundations and Extensions. Springer, 2008.  M. Yuan. High dimensional inverse covariance matrix estimation via linear programming.  Journal of Machine Learning Research, 11:2261-2286, 2011.  T. Zhao and H. Liu. The huge package for high-dimensional undirected graph estimation  in R. Journal of Machine Learning Research, 13:1059-1062, 2012.  493   The fastclime Package for Large-Scale Precision Matrix Estimation in R  References  O. Banerjee, L. E. Ghaoui, and A. d\u2019Aspremont. Model selection through sparse maximum  likelihood estimation. Journal of Machine Learning Research, 9:485-516, 2008.  T. Cai, W. Liu, and X. Luo. A constrained l1 minimization approach to sparse precision  matrix estimation. J. American Statistical Association, 106:594-607, 2011.  J. Friedman, T. Hastie, H. H\u00a8o\ufb02ing, and R. Tibshirani. Pathwise coordinate optimization.  Annals of Applied Statistics, 1(2):302-332, 2007a.  J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the  graphical lasso. Biostatistics, 9(3):432-441, 2007b.  J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear  models via coordinate descent. Journal of Statistical Software, 33(1), 2010.  C-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. Advances in Neural Information Processing Systems, 24, 2011.  X. Li, T. Zhao, X. Yuan, and H. Liu. An R package \ufb02are for high dimensional linear  regression and precision matrix estimator. R Package Vigette, 2012.  N. Meinshausen and P. B\u00a8uhlmann. High dimensional graphs and variable selection with the  lasso. Annals of Statistics, 34(3):1436-1462, 2006.  R. Vanderbei. Linear Programming, Fundations and Extensions. Springer, 2008.  M. Yuan. High dimensional inverse covariance matrix estimation via linear programming.  Journal of Machine Learning Research, 11:2261-2286, 2011.  T. Zhao and H. Liu. The huge package for high-dimensional undirected graph estimation  in R. Journal of Machine Learning Research, 13:1059-1062, 2012. "}, "LIBOL: A Library for Online Learning Algorithms": {"volumn": 15, "url": "http://jmlr.org/papers/v15/hoi14a.html", "header": "LIBOL: A Library for Online Learning Algorithms", "author": "Steven C.H. Hoi, Jialei Wang, Peilin Zhao", "time": "15(15):495\u2212499, 2014.", "abstract": "LIBOL  is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large- scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users.  LIBOL  is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.", "pdf_url": "http://jmlr.org/papers/volume15/hoi14a/hoi14a.pdf", "keywords": ["online learning", "massive-scale classification", "big data analytics"], "reference": "2010.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order perceptron algo-  rithm. SIAM J. Comput., 34(3):640-668, 2005.  Koby Crammer and Daniel D. Lee. Learning via gaussian herding. In NIPS, pages 451-459,  Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551-585, 2006.  Koby Crammer, Alex Kulesza, and Mark Dredze. Adaptive regularization of weight vectors.  In NIPS, pages 345-352, 2009.  Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-weighted linear classifi-  cation. In ICML, pages 264-271, 2008.  Claudio Gentile. A new approximate maximal margin classification algorithm. Journal of  Machine Learning Research, 2:213-242, 2001.  Steven C. H. Hoi, Rong Jin, Peilin Zhao, and Tianbao Yang. Online multiple kernel classi-  fication. Machine Learning, 90(2):289-316, 2013a.  Steven C. H. Hoi, Jialei Wang, Peilin Zhao, Jinfeng Zhuang, and Zhi-Yong Liu. Large scale  online kernel classification. In IJCAI, 2013b.  Yi Li and Philip M. Long. The relaxed online maximum margin algorithm. Machine  Learning, 46(1-3):361-387, 2002.  In NIPS, pages 1840-1848, 2010.  Francesco Orabona and Koby Crammer. New adaptive algorithms for online classification.  Frank Rosenblatt. The perceptron: A probabilistic model for information storage and  organization in the brain. Psych. Rev., 7:551-585, 1958.  Jialei Wang, Peilin Zhao, and Steven C. H. Hoi. Exact soft confidence-weighted learning.  ICML, 2012a.  145, 2009.  Jialei Wang, Peilin Zhao, and Steven CH Hoi. Cost-sensitive online classification. In IEEE 12th International Conference on Data Mining (ICDM), pages 1140-1145. IEEE, 2012b.  Liu Yang, Rong Jin, and Jieping Ye. Online learning by ellipsoid method. In ICML, page  Peilin Zhao, Steven C. H. Hoi, and Rong Jin. Double updating online learning. Journal of  Machine Learning Research, 12:1587-1615, 2011a.  Peilin Zhao, Steven C. H. Hoi, Rong Jin, and Tianbao Yang. Online auc maximization. In  ICML, pages 233-240, 2011b.  In ICML, pages 928-936, 2003.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent.  499   LIBOL: A Library for Online Learning Algorithms  References  2010.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order perceptron algo-  rithm. SIAM J. Comput., 34(3):640-668, 2005.  Koby Crammer and Daniel D. Lee. Learning via gaussian herding. In NIPS, pages 451-459,  Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551-585, 2006.  Koby Crammer, Alex Kulesza, and Mark Dredze. Adaptive regularization of weight vectors.  In NIPS, pages 345-352, 2009.  Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-weighted linear classifi-  cation. In ICML, pages 264-271, 2008.  Claudio Gentile. A new approximate maximal margin classification algorithm. Journal of  Machine Learning Research, 2:213-242, 2001.  Steven C. H. Hoi, Rong Jin, Peilin Zhao, and Tianbao Yang. Online multiple kernel classi-  fication. Machine Learning, 90(2):289-316, 2013a.  Steven C. H. Hoi, Jialei Wang, Peilin Zhao, Jinfeng Zhuang, and Zhi-Yong Liu. Large scale  online kernel classification. In IJCAI, 2013b.  Yi Li and Philip M. Long. The relaxed online maximum margin algorithm. Machine  Learning, 46(1-3):361-387, 2002.  In NIPS, pages 1840-1848, 2010.  Francesco Orabona and Koby Crammer. New adaptive algorithms for online classification.  Frank Rosenblatt. The perceptron: A probabilistic model for information storage and  organization in the brain. Psych. Rev., 7:551-585, 1958.  Jialei Wang, Peilin Zhao, and Steven C. H. Hoi. Exact soft confidence-weighted learning.  ICML, 2012a.  145, 2009.  Jialei Wang, Peilin Zhao, and Steven CH Hoi. Cost-sensitive online classification. In IEEE 12th International Conference on Data Mining (ICDM), pages 1140-1145. IEEE, 2012b.  Liu Yang, Rong Jin, and Jieping Ye. Online learning by ellipsoid method. In ICML, page  Peilin Zhao, Steven C. H. Hoi, and Rong Jin. Double updating online learning. Journal of  Machine Learning Research, 12:1587-1615, 2011a.  Peilin Zhao, Steven C. H. Hoi, Rong Jin, and Tianbao Yang. Online auc maximization. In  ICML, pages 233-240, 2011b.  In ICML, pages 928-936, 2003.  Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. "}, "Improving Markov Network Structure Learning Using Decision Trees": {"volumn": 15, "url": "http://jmlr.org/papers/v15/lowd14a.html", "header": "Improving Markov Network Structure Learning Using Decision Trees", "author": "Daniel Lowd, Jesse Davis", "time": "15(16):501\u2212532, 2014.", "abstract": "Most existing algorithms for learning Markov network structure either are limited to learning interactions among few variables or are very slow, due to the large space of possible structures. In this paper, we propose three new methods for using decision trees to learn Markov network structures. The advantage of using decision trees is that they are very fast to learn and can represent complex interactions among many variables. The first method, DTSL, learns a decision tree to predict each variable and converts each tree into a set of conjunctive features that define the Markov network structure. The second, DT-BLM, builds on DTSL by using it to initialize a search-based Markov network learning algorithm recently proposed by Davis and Domingos (2010). The third, DT+L1, combines the features learned by DTSL with those learned by an L1-regularized logistic regression method (L1) proposed by Ravikumar et al. (2009). In an extensive empirical evaluation on 20 data sets, DTSL is comparable to L1 and significantly faster and more accurate than two other baselines. DT-BLM is slower than DTSL, but obtains slightly higher accuracy. DT+L1 combines the strengths of DTSL and L1 to perform significantly better than either of them with only a modest increase in training time.", "pdf_url": "http://jmlr.org/papers/volume15/lowd14a/lowd14a.pdf", "keywords": ["Markov networks", "structure learning", "decision trees", "probabilistic methods"], "reference": "G. Andrew and J. Gao. Scalable training of l1-regularized log-linear models. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 33-40. ACM Press, 2007.  J. Besag. Statistical analysis of non-lattice data. The Statistician, 24:179-195, 1975.  C. Blake and C. J. Merz. UCI repository of machine learning databases. Machine-readable data repository, Department of Information and Computer Science, University of Cali- fornia at Irvine, Irvine, CA, 2000. http://www.ics.uci.edu/\u223cmlearn/MLRepository.html.  F. Bromberg, D. Margaritis, and V. Honavar. E\ufb03cient Markov network structure discovery using independence tests. Journal of Artificial Intelligence Research, 35(2):449-484, 2009.  D. Chickering, D. Heckerman, and C. Meek. A Bayesian approach to learning Bayesian networks with local structure. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, pages 80-89, Providence, RI, 1997. Morgan Kaufmann.  J. Davis and P. Domingos. Bottom-up learning of Markov network structure. In Proceedings of the Twenty-Seventh International Conference on Machine Learning, pages 271-278, Haifa, Israel, 2010. ACM Press.  S. Della Pietra, V. Della Pietra, and J. La\ufb00erty. Inducing features of random fields. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 19:380-392, 1997.  P. Domingos and G. Hulten. Mining high-speed data streams. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 71-80, Boston, MA, 2000. ACM Press.  530   Lowd and Davis  Future work includes exploring other methods of learning local structure, such as rule sets, boosted decision trees, and neural networks; determining su\ufb03cient conditions for the asymptotic consistency of local learning; further improving speed, perhaps by using frequent itemsets; and incorporating faster methods for weight learning, since structure learning is no longer the bottleneck.  Acknowledgments  The authors would like to thank the anonymous reviewers for many helpful suggestions. We also thank Jan Van Haaren for his valuable feedback on the article. DL is partly supported by ARO grant W911NF-08-1-0242 and NSF grant IIS-1118050. The views and conclusions contained in this document are those of the authors and should not be inter- preted as necessarily representing the o\ufb03cial policies, either expressed or implied, of ARO or the United States Government. JD is partially supported by the research fund KU Leu- ven (CREA/11/015 and OT/11/051), and EU FP7 Marie Curie Career Integration Grant (#294068).  References  G. Andrew and J. Gao. Scalable training of l1-regularized log-linear models. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 33-40. ACM Press, 2007.  J. Besag. Statistical analysis of non-lattice data. The Statistician, 24:179-195, 1975.  C. Blake and C. J. Merz. UCI repository of machine learning databases. Machine-readable data repository, Department of Information and Computer Science, University of Cali- fornia at Irvine, Irvine, CA, 2000. http://www.ics.uci.edu/\u223cmlearn/MLRepository.html.  F. Bromberg, D. Margaritis, and V. Honavar. E\ufb03cient Markov network structure discovery using independence tests. Journal of Artificial Intelligence Research, 35(2):449-484, 2009.  D. Chickering, D. Heckerman, and C. Meek. A Bayesian approach to learning Bayesian networks with local structure. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, pages 80-89, Providence, RI, 1997. Morgan Kaufmann.  J. Davis and P. Domingos. Bottom-up learning of Markov network structure. In Proceedings of the Twenty-Seventh International Conference on Machine Learning, pages 271-278, Haifa, Israel, 2010. ACM Press.  S. Della Pietra, V. Della Pietra, and J. La\ufb00erty. Inducing features of random fields. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 19:380-392, 1997.  P. Domingos and G. Hulten. Mining high-speed data streams. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 71-80, Boston, MA, 2000. ACM Press. Improving Markov Network Structure Learning Using Decision Trees  R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. Journal of Machine Learning Research, (9):1871-1874, 2008.  W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo  in Practice. Chapman and Hall, London, UK, 1996.  K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collabo-  rative filtering algorithm. Information Retrieval, 4(2):133-151, 2001.  D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Research, 1:49-75, 2000.  G. Hulten and P. Domingos. Mining complex models from arbitrarily large databases in constant time. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 525-531, Edmonton, Canada, 2002. ACM Press.  R. Kohavi, C. Brodley, B. Frasca, L. Mason, and Z. Zheng. KDD-Cup 2000 organizers\u2019  report: Peeling the onion. SIGKDD Explorations, 2(2):86-98, 2000.  A. Kulesza and F. Pereira. Structured learning with approximate inference. In Advances  in Neural Information Processing Systems 20, pages 785-792, 2007.  S.-I. Lee, V. Ganapathi, and D. Koller. E\ufb03cient structure learning of Markov networks using L1-regularization. In Advances in Neural Information Processing Systems 19, pages 817-824. MIT Press, 2007.  D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.  Mathematical Programming, 45(3):503-528, 1989.  D. Lowd and J. Davis. Learning Markov network structure with decision trees. In Proceed- ings of the 10th IEEE International Conference on Data Mining (ICDM), pages 334-343, Sydney, Australia, 2010. IEEE Computer Society Press.  A. McCallum. E\ufb03ciently inducing features of conditional random fields. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, pages 403-410, Acapulco, Mexico, 2003. Morgan Kaufmann.  K. Murphy, Y. Weiss, and M. Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pages 467-475. Morgan Kaufmann, Stockholm, Sweden, 1999.  P. Ravikumar, M. J. Wainwright, and J. La\ufb00erty. High-dimensional ising model selection using L1-regularized logistic regression. Annals of Statistics, 38(3):1287-1319, 2010.  M. Schmidt and K. Murphy. Convex structure learning in log-linear models: Beyond pair- wise potentials. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), pages 709-716, 2010. Lowd and Davis  P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Springer, New  York, NY, 1993.  I. Tsamardinos, L. Brown, and C. Aliferis. The max-min hill-climbing Bayesian network  structure learning algorithm. Machine Learning, 65(1):31-78, 2006.  J. Van Haaren and J. Davis. Markov network structure learning: A randomized feature gen- eration approach. In Proceedings of the Twenty-Sixth National Conference on Artificial Intelligence, pages 1148-1154. AAAI Press, 2012.  C. Ziegler, S. McNee, J. Konstan, and G. Lausen. Improving recommendation lists through In Proceedings of the Fourteenth International World Wide Web  topic diversification. Conference, pages 22-32, 2005. "}, "Ground Metric Learning": {"volumn": 15, "url": "http://jmlr.org/papers/v15/cuturi14a.html", "header": "Ground Metric Learning", "author": "Marco Cuturi, David Avis", "time": "15(17):533\u2212564, 2014.", "abstract": "Optimal transport distances have been used for more than a decade in machine learning to compare histograms of features. They have one parameter: the  ground metric , which can be any metric between the features themselves. As is the case for all parameterized distances, optimal transport distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on  a priori  knowledge of the features, which limited considerably the scope of application of optimal transport distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two convex polyhedral functions over a convex set of metric matrices. We follow the presentation of our algorithms with promising experimental results which show that this approach is useful both for retrieval and binary/multiclass classification tasks.", "pdf_url": "http://jmlr.org/papers/volume15/cuturi14a/cuturi14a.pdf", "keywords": ["optimal transport distance", "earth mover\u2019s distance", "metric learning", "metric nearness"], "reference": "Prentice Hall, 1993.  R. Ahuja, T. Magnanti, and J. Orlin. Network Flows: Theory, Algorithms and Applications.  J. Aitchison. The Statistical Analysis of Compositional Data. Chapman & Hall, 1986.  J. Aitchison and J. Egozcue. Compositional data analysis: Where are we and where should  we be heading? Mathematical Geology, 37(7):829-850, 2005.  S.-I. Amari and H. Nagaoka. Methods of Information Geometry. AMS vol. 191, 2001.  K. Ba, H. Nguyen, H. Nguyen, and R. Rubinfeld. Sublinear time algorithms for earth  movers distance. Theory of Computing Systems, 48(2):428-442, 2011.  A. Barvinok. What does a random contingency table look like? Combinatorics, Probability  and Computing, 19(04):517-539, 2010.  A. Bellet, A. Habrard, and M. Sebban. A survey on metric learning for feature vectors and  structured data. arXiv:1306.6709, 2013.  D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997.  D. Blei and J. La\ufb00erty. Topic models. Text Mining: Classification, Clustering, and Appli-  cations, 10:71, 2009.  D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine  Learning Research, 3:993-1022, 2003.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J. Brickell, I. Dhillon, S. Sra, and J. Tropp. The metric nearness problem. SIAM Journal  of Matrix Analysis and Applications, 30(1):375-396, 2008.  R. A. Brualdi. Combinatorial Matrix Classes, volume 108. Cambridge University Press,  2006.  O. Chapelle, P. Ha\ufb00ner, and V. Vapnik. SVMs for histogram based image classification.  IEEE Transactions on Neural Networks, 10(5):1055, Sept. 1999.  M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances  in Neural Information Processing Systems 26, pages 2292-2300. 2013.  J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. In Proceedings of the 24th International Conference on Machine Learning, pages 209-216. ACM, 2007.  M. Deza and E. Deza. Encyclopedia of Distances. Springer Verlag, 2009.  P. Diaconis and B. Efron. Testing for independence in a two-way table: new interpretations  of the chi-square statistic. The Annals of Statistics, 13(3):845-913, 1985.  562   Cuturi and Avis  References  Prentice Hall, 1993.  R. Ahuja, T. Magnanti, and J. Orlin. Network Flows: Theory, Algorithms and Applications.  J. Aitchison. The Statistical Analysis of Compositional Data. Chapman & Hall, 1986.  J. Aitchison and J. Egozcue. Compositional data analysis: Where are we and where should  we be heading? Mathematical Geology, 37(7):829-850, 2005.  S.-I. Amari and H. Nagaoka. Methods of Information Geometry. AMS vol. 191, 2001.  K. Ba, H. Nguyen, H. Nguyen, and R. Rubinfeld. Sublinear time algorithms for earth  movers distance. Theory of Computing Systems, 48(2):428-442, 2011.  A. Barvinok. What does a random contingency table look like? Combinatorics, Probability  and Computing, 19(04):517-539, 2010.  A. Bellet, A. Habrard, and M. Sebban. A survey on metric learning for feature vectors and  structured data. arXiv:1306.6709, 2013.  D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997.  D. Blei and J. La\ufb00erty. Topic models. Text Mining: Classification, Clustering, and Appli-  cations, 10:71, 2009.  D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine  Learning Research, 3:993-1022, 2003.  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  J. Brickell, I. Dhillon, S. Sra, and J. Tropp. The metric nearness problem. SIAM Journal  of Matrix Analysis and Applications, 30(1):375-396, 2008.  R. A. Brualdi. Combinatorial Matrix Classes, volume 108. Cambridge University Press,  2006.  O. Chapelle, P. Ha\ufb00ner, and V. Vapnik. SVMs for histogram based image classification.  IEEE Transactions on Neural Networks, 10(5):1055, Sept. 1999.  M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances  in Neural Information Processing Systems 26, pages 2292-2300. 2013.  J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. In Proceedings of the 24th International Conference on Machine Learning, pages 209-216. ACM, 2007.  M. Deza and E. Deza. Encyclopedia of Distances. Springer Verlag, 2009.  P. Diaconis and B. Efron. Testing for independence in a two-way table: new interpretations  of the chi-square statistic. The Annals of Statistics, 13(3):845-913, 1985. Ground Metric Learning  M. Douze, H. J\u00b4egou, H. Sandhawalia, L. Amsaleg, and C. Schmid. Evaluation of GIST de- scriptors for web-scale image search. In Proceedings of the ACM International Conference on Image and Video Retrieval. Article 19, ACM, 2009.  L. Ford and Fulkerson. Flows in Networks. Princeton University Press, 1962.  A. Frangioni, A. Lodi, and G. Rinaldi. New approaches for optimizing over the semimetric  polytope. Mathematical Programming, 104(2):375-388, 2005.  I. Good. Maximum entropy for hypothesis formulation, especially for multidimensional  contingency tables. The Annals of Mathematical Statistics, pages 911-934, 1963.  J. Gudmundsson, O. Klein, C. Knauer, and M. Smid. Small manhattan networks and algo- rithmic applications for the earth movers distance. In Proceedings of the 23rd European Workshop on Computational Geometry, pages 174-177, 2007.  T. Joachims. Learning to Classify Text Using Support Vector Machines: Methods, Theory,  and Algorithms. Kluwer Academic Publishers, 2002.  L. Kantorovich and G. Rubinshtein. On a space of totally additive functions. Vestn Lening.  Univ., 13:52-59, 1958.  H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized kernels between labeled graphs. In Proceedings of the 20th International Conference on Machine Learning, pages 321-328, 2003.  D. Kedem, S. Tyree, K. Weinberger, F. Sha, and G. Lanckriet. Non-linear metric learning.  In Advances in Neural Information Processing Systems 25, pages 2582-2590, 2012.  B. Kulis. Metric learning: A survey. Foundations & Trends in Machine Learning, 5(4):  287-364, 2012.  S. Lauritzen. Lectures on Contingency Tables. Aalborg Univ. Press, 1982.  G. Lebanon. Metric learning for text documents. IEEE Transactions on Pattern Analysis  and Machine Intelligence, 28(4):497-508, 2006.  C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: a string kernel for svm protein classific ation. In Proceedings of the Pacific Symposium on Biology 2002, pages 564-575, 2002.  E. Levina and P. Bickel. The earth mover\u2019s distance is the Mallows distance: some insights from statistics. In Proceedings of the Eighth IEEE International Conference on Computer Vision, volume 2, pages 251-256. IEEE, 2001.  H. Ling and K. Okada. An e\ufb03cient earth mover\u2019s distance algorithm for robust histogram comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 840-853, 2007. Cuturi and Avis  D. Lowe. Object recognition from local scale-invariant features. In Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pages 1150 -1157 vol.2, 1999.  C. Mallows. A note on asymptotic joint normality. The Annals of Mathematical Statistics,  pages 508-515, 1972.  A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the  spatial envelope. International Journal of Computer Vision, 42(3):145-175, 2001.  O. Pele and M. Werman. Fast and robust earth mover\u2019s distances. In Proceedings of the  International Conference on Computer Vision\u201909, 2009.  S. Rachev. Probability Metrics and the Stability of Stochastic Models. Wiley series in probability and mathematical statistics: Applied probability and statistics. Wiley, 1991.  S. Rachev and L. R\u00a8uschendorf. Mass Transportation Problems: Theory, volume 1. Springer  Verlag, 1998.  T. Rockafellar. Convex Analysis. Princeton University Press, 1970.  Y. Rubner, L. Guibas, and C. Tomasi. The earth movers distance, multi-dimensional scal- ing, and color-based image retrieval. In Proceedings of the ARPA Image Understanding Workshop, pages 661-668, 1997.  Y. Rubner, C. Tomasi, and L. Guibas. The earth mover\u2019s distance as a metric for image  retrieval. International Journal of Computer Vision, 40, 2000.  M. Swain and D. Ballard. Color indexing. International Journal of Computer Vision, 7(1):  11-32, 1991.  ematics, 2003.  A. Vershik. Kantorovich metric:  initial history and little-known applications. Journal of  Mathematical Sciences, 133(4):1410-1417, 2006.  C. Villani. Topics in Optimal Transportation, volume 58. AMS Graduate Studies in Math-  F. Wang and L. J. Guibas. Supervised earth movers distance learning and its computer vision applications. In Computer Vision-ECCV 2012, pages 442-455. Springer, 2012.  K. Weinberger and L. Saul. Distance metric learning for large margin nearest neighbor  classification. The Journal of Machine Learning Research, 10:207-244, 2009.  K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classification. In Advances in Neural Information Processing Systems 18, pages 1473-1480, 2006.  E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with applica- tion to clustering with side-information. In Advances in Neural Information Processing Systems 15, pages 505-512. MIT Press, 2003. "}, "Link Prediction in Graphs with Autoregressive Features": {"volumn": 15, "url": "http://jmlr.org/papers/v15/richard14a.html", "header": "Link Prediction in Graphs with Autoregressive Features", "author": "Emile Richard, St\u00c3\u00a9phane Ga\u00c3\u00afffas, Nicolas Vayatis", "time": "15(18):565\u2212593, 2014.", "abstract": "In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices. On the adjacency matrix it takes into account both sparsity and low rank properties and on the VAR it encodes the sparsity. The analysis involves oracle inequalities that illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank. The estimate is computed efficiently using proximal methods, and evaluated through numerical experiments.", "pdf_url": "http://jmlr.org/papers/volume15/richard14a/richard14a.pdf", "keywords": ["graphs", "link prediction", "low-rank", "sparsity", "autoregression"], "reference": "J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative filtering: Operator estimation with spectral regularization. Journal of Machine Learning Research, 10:803-826, 2009.  A. Andreas, M. Pontil, Y. Ying, and C. A. Micchelli. A spectral regularization framework for multi-task structure learning. In Advances in Neural Information Processing Systems (NIPS), pages 25-32, 2007.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal of Imaging Sciences, 2(1):183-202, 2009.  D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for convex opti-  mization: a survey. Optimization for Machine Learning, page 85, 2011.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and Dantzig  selector. Ann. Statist., 37(4):1705-1732, 2009.  590   Richard, Ga\u00efffas and Vayatis  This leads to  (cid:104) ST \u2265 \u03c3(cid:112)2eVT (x + (cid:96)T ), VT > v(cid:3) =  P  P(cid:2)ST \u2265 \u03c3(cid:112)2eVT (x + (cid:96)T ), vk < VT \u2264 vk+1  (cid:105)  ST \u2265 \u03c3  2vk+1(x + 2 log log(ek \u2228 e)), vk < VT \u2264 vk+1  (cid:105)  (cid:88)  k\u22650  (cid:113)  (cid:88)  (cid:104) P  =  k\u22650 \u2264 e\u2212x(1 +  (cid:88)  k\u22651  k\u22122) \u2264 3.47e\u2212x.  On {VT \u2264 v} the proof is the same: we decompose onto the disjoint sets {vk+1 < VT \u2264 vk} where this time vk = ve\u2212k, and we arrive at  (cid:104) ST \u2265 \u03c3(cid:112)2eVT (x + (cid:96)T ), VT \u2264 v(cid:3) \u2264 3.47e\u2212x.  P  This leads to  (cid:20) T  (cid:88)  P  t=1  \u03c9j(At\u22121)Nt,k \u2265 \u03c3  2e  \u03c9j(At\u22121)2(x + (cid:96)T,j)  \u2264 7e\u2212x  (cid:17)1/2(cid:21)  (cid:16)  T (cid:88)  t=1  for any 1 \u2264 j, k \u2264 d, where we introduced  (cid:96)T,j = 2 log log  (cid:16) (cid:80)T  t=1 \u03c9j(At\u22121)2 T  \u2228  T t=1 \u03c9j(At\u22121)2  (cid:80)T  (cid:17)  \u2228 e  .  The conclusion follows from an union bound on 1 \u2264 j, k \u2264 d, and from the use of the same argument for the term \u03c9(AT )N (cid:62)  T +1. This concludes the proof of Proposition 6.  References  J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative filtering: Operator estimation with spectral regularization. Journal of Machine Learning Research, 10:803-826, 2009.  A. Andreas, M. Pontil, Y. Ying, and C. A. Micchelli. A spectral regularization framework for multi-task structure learning. In Advances in Neural Information Processing Systems (NIPS), pages 25-32, 2007.  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal of Imaging Sciences, 2(1):183-202, 2009.  D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for convex opti-  mization: a survey. Optimization for Machine Learning, page 85, 2011.  P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and Dantzig  selector. Ann. Statist., 37(4):1705-1732, 2009. Link Prediction in Graphs with Autoregressive Features  L. Breiman and J. H. Friedman. Predicting multivariate responses in multiple linear regres- sion. Journal of the Royal Statistical Society (JRSS): Series B (Statistical Methodology), 59:3-54, 1997.  E. J. Cand\u00e8s and T. Tao. Decoding by linear programming.  In Proceedings of the 46th  Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2005.  E. J. Cand\u00e8s and T. Tao. The power of convex relaxation: Near-optimal matrix completion.  Information Theory, IEEE Transactions on, 56(5), 2009.  E. J. Cand\u00e8s and M. Wakin. An introduction to compressive sampling. IEEE Signal Pro-  cessing Magazine, 12(51):21-30, 2008.  P. L. Combettes and J. C. Pesquet. Proximal splitting methods in signal processing. Fixed- Point Algorithms for Inverse Problems in Science and Engineering, pages 185-212, 2011.  R. A. Davis, P. Zang, and T. Zheng. Sparse vector autoregressive modeling. arXiv preprint  arXiv:1207.0520, 2012.  1289-1306, 2006.  D. L. Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):  T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods.  Journal of Machine Learning Research, 6:615-637, 2005.  S. Ga\u00ef\ufb00as and G. Lecu\u00e9. Sharp oracle inequalities for high-dimensional matrix prediction.  Information Theory, IEEE Transactions on, 57(10):6942 -6957, oct. 2011.  Z. Huang and D. K. J. Lin. The time-series link prediction problem with applications in  communication surveillance. INFORMS J. on Computing, 21(2):286-303, 2009.  M. Kolar and E. P. Xing. On time varying undirected graphs. In International Conference  on Artificial Intelligence and Statistics, pages 407-415, 2011.  V. Koltchinskii. Sparsity in penalized empirical risk minimization. Ann. Inst. Henri Poincar\u00e9  Probab. Stat., 45(1):7-57, 2009a.  V. Koltchinskii. The Dantzig selector and sparsity oracle inequalities. Bernoulli, 15(3):  799-828, 2009b.  V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and optimal  rates for noisy low-rank matrix completion. Ann. Statist., 39(5):2302-2329, 2011.  Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 426-434. ACM, 2008.  Y. Koren. Collaborative filtering with temporal dynamics. Communications of the ACM,  53(4):89-97, 2010. Richard, Ga\u00efffas and Vayatis  A. S. Lewis. The convex analysis of unitarily invariant matrix functions. J. Convex Anal.,  2(1-2):173-183, 1995.  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American Society for Information Science and Technology, 58(7):1019-1031, 2007.  L. L\u00fc and T. Zhou. Link prediction in complex networks: A survey. Physica A: Statistical  Mechanics and its Applications, 390(6):1150-1170, 2011.  S. A. Myers and J. Leskovec. On the convexity of latent social network inference. In Advances  in Neural Information Processing Systems (NIPS), 2010.  Y. Nardi and A. Rinaldo. Autoregressive process modeling via the lasso procedure. Journal  of Multivariate Analysis, 102(3):528-549, 2011.  Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,  103(1):127-152, 2005.  E. Richard, N. Baskiotis, T. Evgeniou, and N. Vayatis. Link discovery using graph feature  tracking. In Advances in Neural Information Processing Systems (NIPS), 2010.  E. Richard, S. Gai\ufb00as, and N. Vayatis. Link prediction in graphs with autoregressive features.  In Advances in Neural Information Processing Systems (NIPS), 2012a.  E. Richard, P.-A. Savalle, and N. Vayatis. Estimation of simultaneously sparse and low-rank matrices. In Proceedings of 29th Annual International Conference on Machine Learning, 2012b.  E. M. Rogers. Di\ufb00usion of Innovations. London: The Free Press, 1962.  P. Sarkar, D. Chakrabarti, and A. W. Moore. Theoretical justification of popular link In International Conference on Learning Theory (COLT), pages  prediction heuristics. 295-307, 2010.  P. Sarkar, D. Chakrabarti, and M. I. Jordan. Nonparametric link prediction in dynamic networks. In Proceedings of 29th Annual International Conference on Machine Learning, 2012.  A. Shojaie, S. Basu, and G. Michailidis. Adaptive thresholding for reconstructing regulatory  networks from time course gene expression data. Statistics In Biosciences, 2011.  N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In  Advances in Neural Information Processing Systems (NIPS). 2005.  B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. Link prediction in relational data. In  Advances in Neural Information Processing Systems (NIPS), 2003.  J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Com-  putational Mathematics, 12(4):389-434, 2012.  R. S. Tsay. Analysis of Financial Time Series. Wiley-Interscience; 3rd edition, 2005. Link Prediction in Graphs with Autoregressive Features  P. Tseng. On accelerated proximal gradient methods for convex-concave optimization.  Preprint, 2008.  S. A. van de Geer and P. B\u00fchlmann. On the conditions used to prove oracle results for the  Lasso. Electron. J. Stat., 3:1360-1392, 2009.  D. Q. Vu, A. Asuncion, D. Hunter, and P. Smyth. Continuous-time regression models for longitudinal networks. In Advances in Neural Information Processing Systems (NIPS), 2011.  K. Zhang, T. Evgeniou, V. Padmanabhan, and E. Richard. Content contributor management  and network e\ufb00ects in a ugc environment. Marketing Science, 2011. "}, "Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression": {"volumn": 15, "url": "http://jmlr.org/papers/v15/bach14a.html", "header": "Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression", "author": "Francis Bach", "time": "15(19):595\u2212627, 2014.", "abstract": "In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after $N$ iterations, with a constant step-size proportional to $1/R^2 \\sqrt{N}$ where $N$ is the number of observations and $R$ is the maximum norm of the observations, the convergence rate is always of order $O(1/\\sqrt{N})$, and improves to $O(R^2 / \\mu N)$ where $\\mu$ is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than $R^2/\\sqrt{N}$). Since $\\mu$ does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to  unknown local  strong convexity of the objective function. Our proof relies on the generalized self-concordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features.", "pdf_url": "http://jmlr.org/papers/volume15/bach14a/bach14a.pdf", "keywords": [""], "reference": "A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  384-414, 2010.  for machine learning. 2011.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms In Advances in Neural Information Processing Systems (NIPS),  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classifiers with online and  active learning. Journal of Machine Learning Research, 6:1579-1619, 2005.  L. Bottou and O. Bousquet. The tradeo\ufb00s of large scale learning. In Advances in Neural  Information Processing Systems (NIPS), 2008.  L. Bottou and Y. Le Cun. On-line learning for very large data sets. Applied Stochastic  Models in Business and Industry, 21(2):137-151, 2005.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic  Theory of Independence. Oxford University Press, 2013.  M. N. Broadie, D. M. Cicek, and A. Zeevi. General bounds and finite-time improvement for stochastic approximation algorithms. Technical report, Columbia University, 2009.  J. Duchi and Y. Singer. E\ufb03cient online and batch learning using forward backward splitting.  Journal of Machine Learning Research, 10:2899-2934, 2009.  625   Adaptivity of Averaged Stochastic Gradient Descent  For \u03b3 = 1 \u221a 2R2  N  , with \u03b1 = R(cid:107)\u03b80 \u2212 \u03b8\u2217(cid:107), (cid:3) = 1 and (cid:52) = 6\u03b12 + 6\u03b1, we get  (cid:20) 2(cid:52)2 + 8(cid:52)(32 + 40 + 115) + 8(100 + 800 + 532 + 1544)  (cid:21)  (cid:20) 2(cid:52)2 + 1496(cid:52) + 23808  (cid:21)  (cid:20) 72\u03b14 + 144\u03b13 + 72\u03b12 + 1496 \u00d7 6\u03b12 + 1496 \u00d7 6\u03b1 + 23808  (cid:21)  (cid:20) 1296\u03b14 + 18144\u03b13 + 95256\u03b12 + 222264\u03b1 + 194481  (cid:21)  =  R2 N \u00b52  (cid:0)6\u03b1 + 21(cid:1)4.  The previous bound is valid as long as \u00b5  (cid:62) 10000  20 = 500. If it is not satisfied, then  \u221a  R  N  Lemma 1 shows that it is still valid.  E(cid:107)\u00af\u03b8N \u2212 \u03b8\u2217(cid:107)2 (cid:54) 8R2 N \u00b52 (cid:54) 8R2 N \u00b52 (cid:54) 8R2 N \u00b52 (cid:54) R2 N \u00b52  References  A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Trans- actions on Information Theory, 58(5):3235-3249, 2012.  F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:  384-414, 2010.  for machine learning. 2011.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms In Advances in Neural Information Processing Systems (NIPS),  F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with con- vergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013.  A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classifiers with online and  active learning. Journal of Machine Learning Research, 6:1579-1619, 2005.  L. Bottou and O. Bousquet. The tradeo\ufb00s of large scale learning. In Advances in Neural  Information Processing Systems (NIPS), 2008.  L. Bottou and Y. Le Cun. On-line learning for very large data sets. Applied Stochastic  Models in Business and Industry, 21(2):137-151, 2005.  S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic  Theory of Independence. Oxford University Press, 2013.  M. N. Broadie, D. M. Cicek, and A. Zeevi. General bounds and finite-time improvement for stochastic approximation algorithms. Technical report, Columbia University, 2009.  J. Duchi and Y. Singer. E\ufb03cient online and batch learning using forward backward splitting.  Journal of Machine Learning Research, 10:2899-2934, 2009. Bach  V. Fabian. On asymptotic normality in stochastic approximation. The Annals of Mathe-  matical Statistics, 39(4):1327-1332, 1968.  D. A. Freedman. On tail probabilities for martingales. Annals of Probability, 3(1):100-118,  1975.  E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the Conference on Learning Theory (COLT), 2001.  A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly  convex functions. Technical Report 00508933, HAL, 2010.  S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex pro- gramming algorithms. In Advances in Neural Information Processing Systems (NIPS), 2009.  H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and  Applications. Springer-Verlag, second edition, 2003.  S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) con- vergence rate for projected stochastic subgradient descent. Technical Report 1212.2002, ArXiv, 2012.  J. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML), 2001.  G. Lan. An optimal method for stochastic composite optimization. Mathematical Program-  ming, 133(1-2):365-397, 2012.  N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. In Advances in Neural Information Processing Systems (NIPS), 2012.  H. B. McMahan and M. Streeter. Open problem: Better bounds for online logistic regression.  In COLT/ICML Joint Open Problem Session, 2012.  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609, 2009.  A. S. Nemirovski and D. B. Yudin. Problem Complexity and Method E\ufb03ciency in Opti-  mization. Wiley & Sons, 1983.  Academic Publishers, 2004.  Y. Nesterov.  Introductory Lectures on Convex Optimization: a Basic Course. Kluwer  Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Pro-  gramming, 120(1):221-259, 2009. Adaptivity of Averaged Stochastic Gradient Descent  Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Program-  ming. SIAM studies in Applied Mathematics, 1994.  Y. Nesterov and J. P. Vial. Confidence level solutions for stochastic programming. Auto-  matica, 44(6):1559-1568, 2008.  I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The  Annals of Probability, 22(4):1679-1706, 1994.  B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging.  SIAM Journal on Control and Optimization, 30(4):838-855, 1992.  D. Ruppert. E\ufb03cient estimations from a slowly convergent Robbins-Monro process. Tech- nical Report 781, Cornell University Operations Research and Industrial Engineering, 1988.  B. Sch\u00a8olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2001.  S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In Proceedings of the International Conference on Machine Learning (ICML), 2008.  S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of the International Conference on Machine Learning (ICML), 2007.  S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization.  In Proceedings of the Conference on Learning Theory (COLT), 2009.  J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge  University Press, 2004.  A. W. Van der Vaart. Asymptotic Statistics. Cambridge Univ. Press, 1998.  Z. Wang, K. Crammer, and S. Vucetic. Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale SVM training. Journal of Machine Learning Research, 13:3103-3131, 2012.  L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.  Journal of Machine Learning Research, 9:2543-2596, 2010. "}, "Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers": {"volumn": 15, "url": "http://jmlr.org/papers/v15/moore14a.html", "header": "Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers", "author": "Brett L Moore, Larry D Pyeatt, Vivekan, Kulkarni, Periklis Panousis, Kevin Padrez, Anthony G Doufas", "time": "15(21):655\u2212696, 2014.", "abstract": "Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.", "pdf_url": "http://jmlr.org/papers/volume15/moore14a/moore14a.pdf", "keywords": ["closed-loop control"], "reference": "A R Absalom and G N C Kenny. Closed-loop control of propofol anaesthesia using Bispectral IndexTM: Performance assessment in patients receiving computer-controlled propofol and manually controlled remifentanil infusions for minor surgery. Brit J Anaesth, 90(6):737- 41, 2003.  A R Absalom, N Sutcli\ufb00e, and G N C Kenny. Closed-loop control of anesthesia using Bispectral IndexTM: Performance assessment in patients undergoing major orthopedic surgery under combined general and regional anesthesia. Anesthesiology, 96(1):67-73, Jan 2002.  M E Ausems, C C Hug, Jr, D R Stanski, and A G Burm. Plasma concentrations of alfentanil required to supplement nitrous oxide anesthesia for general surgery. Anesthesiology, 65 (4):362-73, Oct 1986.  M S Avidan, L Zhang, B A Burnside, K J Finkel, A C Searleman, J A Selvidge, L Saager, M S Turner, S Rao, M Bottros, C Hantler, E Jacobsohn, and A S Evers. Anesthesia awareness and the bispectral index. New Eng J Med, 11(358):1097-1108, Mar 2008.  J M Bailey, C T Mora, and S L Shafer. Pharmacokinetics of propofol in adult patients  undergoing coronory revascularization. Anesthesiology, 84:1288-97, 1996.  L Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc. 12th International Conference on Machine Learning, pages 30-37. Morgan Kaufmann, 1995.  L Barvais, I Rausin, J B Glen, S C Hunter, D D\u2019Hulster, F Cantraine, and A d\u2019Hollander. Administration of propofol by target-controlled infusion in patients undergoing coronary artery surgery. J Cardiothorac Vasc Anesth, 10(7):877-83, Dec 1996.  H R Berenji and P Kehdkar. Learning and tuning fuzzy logic controllers through reinforce-  ments. IEEE Transactions on Neural Networks, 3(5):724-740, 1992.  M J Bloom, A Bekker, C V Seshagiri, and S D Greenwald. Changes in BIS variability re\ufb02ect changes in remifentanil infusion during spinal surgery. Presented at the American Society of Anesthesiologists Annual Meeting, Oct 2008.  A Bonarini, A Lazaric, F Montrone, and M Restelli. Reinforcement distribution in fuzzy  Q-learning. Fuzzy Sets and Systems, 160(10):1420-1443, 2009.  J Boyan and A W Moore. Generalization in reinforcement learning: Safely approximating the value function. In G Tesauro, D S Touretzky, and T K Leen, editors, Advances in Neural Information Processing Systems 7, pages 369-376, Cambridge, MA, 1995. The MIT Press.  690   Moore, Kulkarni, Panousis, Padrez, Pyeatt and Doufas  aspects were funded by the authors. The authors would like to thank the Stanford Univer- sity School of Medicine operating room sta\ufb00 for their support, as well as Aspect Medical (now Covidien) for providing an A-2000 BIS monitor.  References  A R Absalom and G N C Kenny. Closed-loop control of propofol anaesthesia using Bispectral IndexTM: Performance assessment in patients receiving computer-controlled propofol and manually controlled remifentanil infusions for minor surgery. Brit J Anaesth, 90(6):737- 41, 2003.  A R Absalom, N Sutcli\ufb00e, and G N C Kenny. Closed-loop control of anesthesia using Bispectral IndexTM: Performance assessment in patients undergoing major orthopedic surgery under combined general and regional anesthesia. Anesthesiology, 96(1):67-73, Jan 2002.  M E Ausems, C C Hug, Jr, D R Stanski, and A G Burm. Plasma concentrations of alfentanil required to supplement nitrous oxide anesthesia for general surgery. Anesthesiology, 65 (4):362-73, Oct 1986.  M S Avidan, L Zhang, B A Burnside, K J Finkel, A C Searleman, J A Selvidge, L Saager, M S Turner, S Rao, M Bottros, C Hantler, E Jacobsohn, and A S Evers. Anesthesia awareness and the bispectral index. New Eng J Med, 11(358):1097-1108, Mar 2008.  J M Bailey, C T Mora, and S L Shafer. Pharmacokinetics of propofol in adult patients  undergoing coronory revascularization. Anesthesiology, 84:1288-97, 1996.  L Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc. 12th International Conference on Machine Learning, pages 30-37. Morgan Kaufmann, 1995.  L Barvais, I Rausin, J B Glen, S C Hunter, D D\u2019Hulster, F Cantraine, and A d\u2019Hollander. Administration of propofol by target-controlled infusion in patients undergoing coronary artery surgery. J Cardiothorac Vasc Anesth, 10(7):877-83, Dec 1996.  H R Berenji and P Kehdkar. Learning and tuning fuzzy logic controllers through reinforce-  ments. IEEE Transactions on Neural Networks, 3(5):724-740, 1992.  M J Bloom, A Bekker, C V Seshagiri, and S D Greenwald. Changes in BIS variability re\ufb02ect changes in remifentanil infusion during spinal surgery. Presented at the American Society of Anesthesiologists Annual Meeting, Oct 2008.  A Bonarini, A Lazaric, F Montrone, and M Restelli. Reinforcement distribution in fuzzy  Q-learning. Fuzzy Sets and Systems, 160(10):1420-1443, 2009.  J Boyan and A W Moore. Generalization in reinforcement learning: Safely approximating the value function. In G Tesauro, D S Touretzky, and T K Leen, editors, Advances in Neural Information Processing Systems 7, pages 369-376, Cambridge, MA, 1995. The MIT Press. Reinforcement Learning for Closed-Loop Propofol Anesthesia  E Brown, R Lydic, and N Schi\ufb00. General anesthesia, sleep, and coma. N Engl J Med, 363  (27):2638-50, Dec 2010.  A Carregal, A Lorenzo, J A Taboada, and J L Barreiro. Intraoperative control of mean arterial pressure and heart rate with alfentanyl with fuzzy logic. Rev Esp Anestesiol Reanim, 47(3):108-113, Mar 2000.  D Chapman and L P Kaelbling. Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In Proceedings of the International Joint Conference on Artificial Intelligence, Sydney, Australia, 1991.  S Ching, B M Westover, M Liberman, J J Chemali, J Kenny, K Solt, P L Purdon, and E N Brown. Real-time closed-loop control in a rodent model of medically induced coma using burst suppression. Anesthesiology, 119(4):848-860, Oct 2013.  A Dahaba. Di\ufb00erent conditions that could result in the bispectral index indicating an  incorrect hypnotic state. Anesth Analg, 101(3):765-73, Sep 2005.  P Dayan. The convergence of TD(\u03bb) for general \u03bb. Machine Learning, 8:341-362, 1992.  T De Smet, M M R F Struys, M M Neckebroek, K Van den Hauwe, S Bonte, and E P Mortier. The accuracy and clinical feasibility of a new Bayesian-based closed-loop control system for propofol administration using the bispectral index as a controlled variable. Anesth Analg, 107:1200-1210, 2008.  A G Doufas, M Bakhshandeh, A R Bjorksten, S L Shafer, and D I Sessler. Induction speed is not a determinant of propofol pharmacodynamics. Anesthesiology, 101:1112-21, 2004.  D Ernst, G B Stan, J Goncalves, and L Wehenkel. Clinical data based optimal STI strategies for HIV; a reinforcement learning approach. In Machine Learning Conference of Belgium and The Netherlands (Benelearn), pages 65-72, 2006.  D Ernst, M Glavic, F Capitanescu, and L Wehenkel. Reinforcement learning versus model predictive control: A comparison on a power system problem. Trans Sys Man Cyber Part B, 39(2):517-529, 2009. ISSN 1083-4419.  V Esmaeili, A Assareh, M B Shamsollahi, M H Moradi, and N M Arefian. Estimating the depth of anesthesia using fuzzy soft computation applied to EEG features. Intell Data Anal, 12(4):393-407, 2008.  S P Fitzgibbon, D M Powers, K J Pope, and C R Clark. Removal of EEG noise and artifact  using blind source separation. J Clin Neurophysiol, 24(3):232-43, Jun 2007.  A E Gaweda, M K Muezzinoglu, A A Jacobs, G R Arono\ufb00, and M E Brier. Model predictive control with reinforcement learning for drug delivery in renal anemia management. Conf Proc IEEE Eng Med Biol Soc, 1:5177-80, 2006.  A Gentilini, C Frei, A H Glattfelder, M Morari, and T Schnider. Identification and targeting policies for computer controlled infusion pumps. Crit Rev Biomed Eng, 28(1&2):179-185, 2000. Moore, Kulkarni, Panousis, Padrez, Pyeatt and Doufas  E Gepts. Pharmacokinetic concepts for TCI anaesthesia. Anaesthesia, 53(Suppl 1):4-12,  Apr 1998.  P S Glass, M Bloom, L Kearse, C Rosow, P Sebel, and P Manberg. Bispectral analysis measures sedation and memory e\ufb00ects of propofol, midazolam, iso\ufb02urane, and alfentanil in healthy volunteers. Anesthesiology, 86(4):836-847, Apr 1997.  P Y Glorennec and L Jou\ufb00e. Fuzzy Q-learning.  In Proceedings of Fuzz-IEEE\u201997, Sixth  International Conference on Fuzzy Systems, volume 3, pages 659-662, 1997.  S D Greenwald and C E Rosow. BIS and EMG variability increase before somatic responses during surgery. Presented at the American Society of Anesthesiologists Annual Meeting, Oct 2006.  A Guez, R D Vincent, M Avoli, and J Pineau. Adaptive treatment of epilepsy via batch- mode reinforcement learning. In IAAI\u201908: Proceedings of the 20th national conference on Innovative Applications of Artificial Intelligence, pages 1671-1678. AAAI Press, 2008. ISBN 978-1-57735-368-3.  V Gullapalli. Learning control under extreme uncertainty. In Stephen Jos\u00b4e Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural Information Processing Systems, volume 5, pages 327-334. Morgan Kaufmann, San Mateo, CA, 1993. URL citeseer.nj. nec.com/312133.html.  J O Hahn, G A Dumont, and J M Ansermino. Closed-loop anesthetic drug concentration estimation using clinical-e\ufb00ect feedback. IEEE Trans Biomed Eng, 58(1):3-6, Jan 2011.  T M Hemmerling, S Charabati, C Zaouter, C Minardi, and P A Mathieu. A randomized controlled trial demonstrates that a novel closed-loop propofol system performs better hypnosis control than manual administration. Can J Anaesth, 57(8):725-735, Aug 2010.  C Hu, W S Lovejoy, and S L Shafer. Comparison of some control strategies for three- compartment PK/PD models. Journal of Pharmacokinetics and Biopharmaceutics, 22 (6):525-550, 1994.  L P Kaelbling, M L Littman, and A W Moore. Reinforcement learning: A survey. Journal  of Artificial Intelligence Research, 4:237-285, 1996.  L P Kaelbling, M L Littman, and A R Cassandra. Planning and acting in partially observ-  able stochastic domains. Artificial Intelligence, 101:99-134, 1998.  L A Kearse Jr., P Manberg, N Chamoun, F deBros, and A Zaslavsky. Bispectral analysis of the electroencephalogram correlates with patient movement to skin incision during propofol/nitrous oxide anesthesia. Anesthesiology, 81(6):1365-70, Dec 1994.  G N C Kenny and H Mantzaridis. Closed-loop control of propofol anaesthesia. Brit J  Anaesth, 83(2):223-8, Aug 1999.  K Leslie, A R Absalom, and G N C Kenny. Closed loop control of sedation for colonoscopy  using the Bispectral Index. Anaesthesia, 57(7):690-709, Jul 2002. Reinforcement Learning for Closed-Loop Propofol Anesthesia  M Lindholm, S Tr\u00a8a\ufb00, F Granath, S D Greenwald, A Ekbom, C Lennmarken, and R H Sandin. Mortality within 2 years after surgery in relation to low intraoperative bispectral index values and preexisting malignant disease. Anesth Analg, 108(2):508-512, Feb 2009.  M Littman. Markov games as a framework for multi-agent reinforcement learning.  In Proceedings of the 11th International Conference on Machine Learning (ML-94), pages 157-163, New Brunswick, NJ, 1994. Morgan Kaufmann.  N Liu, T Chazot, A Genty, A Landais, A Restoux, K McGee, P A Lalo\u00a8e, B Trillat, L Barvais, and M Fischler. Titration of propofol for anesthetic induction and maintenance guided by the bispectral index: Closed-loop versus manual control: A prospective, randomized, multicenter study. Anesthesiology, 104(4):686-695, April 2006.  N Liu, M Le Guen, F Benabbes-Lambert, T Chazot, B Trillat, D I Sessler, and M Fischler. Feasibility of closed-loop titration of propofol and remifentanil guided by the spectral M-Entropy monitor. Anesthesiology, 116(2):286-295, Feb 2012.  N Liu, O Pruszkowski, J E Leroy, T Chazot, B Trillat, A Colchen, F Gonin, and M Fischler. Automatic administration of propofol and remifentanil guided by the bispectral index during rigid bronchoscopic procedures: A randomized trial. Can J Anaesth, 60(9):881- 887, Sep 2013.  S Mahadevan. Average reward reinforcement learning: Foundations, algorithms, and em-  pirical results. Machine Learning, 22(1-3):159-195, 1996.  J D Mart\u00b4\u0131n-Guerrero, F Gomez, E Soria-Olivas, J Schmidhuber, M Climente-Mart\u00b4\u0131, and N V Jim\u00b4enez-Torres. A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients. Expert Syst Appl, 36(6):9737-9742, Aug 2009.  D M Mathews, L Clark, J Johansen, E Matute, and C V Seshagiri. Increases in electroen- cephalogram and electromyogram variability are associated with an increased incidence of intraoperative somatic response. Anesth Analg, 114(4):759-70, Apr 2012.  B L Moore. Intelligent control of closed-loop sedation in simulated ICU patients. Master\u2019s  thesis, Texas Tech University, 2003.  B L Moore, E D Sinzinger, T M Quasny, and L D Pyeatt. Intelligent control of closed-loop  sedation in simulated ICU patients. In FLAIRS 2004. AAAI Press, 2004.  B L Moore, L D Pyeatt, and A G Doufas. Fuzzy control for closed-loop, patient-specific hypnosis in intraoperative patients: A simulation study. In Conf Proc IEEE Eng Med Biol Soc, volume 1, 2009.  B L Moore, A G Doufas, and L D Pyeatt. Reinforcement learning: A novel method for optimal control of propofol-induced hypnosis. Anesth Analg, 112(2):360-367, Feb 2011a.  B L Moore, T M Quasny, and A G Doufas. Reinforcement learning versus proportional- integral-derivative control of hypnosis in a simulated intraoperative patient. Anesth Analg, 112(2):350-359, Feb 2011b. Moore, Kulkarni, Panousis, Padrez, Pyeatt and Doufas  E P Mortier, M M R F Struys, T De Smet, Y D I Versichelen, and G Rolly. Closed- loop controlled administration of propofol using bispectral analysis. Anaesthesia, 53(8): 749-754, Aug 1998.  P Myles, K Leslie, J McNeil, A Forbes, and M Chan. Bispectral index monitoring to prevent awareness during anaesthesia: the B-Aware randomised controlled trial. Lancet, 363(9423):1757-1763, 2000.  D Nieuwenhuijs, E L Coleman, N J Douglas, G B Drummond, and A Dahan. Bispectral index values and spectral edge frequency at di\ufb00erent stages of physiologic sleep. Anesth Analg, 94(1):125-129, Jan 2002.  D A O\u2019Hara, D K Bogen, and A Noordergraaf. The use of computers for controlling the  delivery of anesthesia. Anesthesiology, 77(3):563-81, Sep 1992.  K T Olkkola, H Schwilden, and C Ap\ufb00elstaedt. Model-based adaptive closed-loop feedback control of atracurium-induced neuromuscular blockade. Acta Anaesth Scand, 35(5):420-3, Jul 1991.  S Omatu, M Khalid, and R Yusof. Neuro-control and its Applications, chapter 4, pages  152-160. Advances in Industrial Control. Springer, 1996.  S Pilge, R Zanner, G Schneider, J Blum, M Kreuzer, and E F Kochs. Analysis of cerebral state, bispectral, and narcotrend indices. Anesthesiology, 104(3):488-494, Mar 2006.  I J Rampil. A primer for EEG signal processing in anesthesia. Anesthesiology, 89(4):  980-1002, Oct 1997.  M Renna, T Wigmore, A Mofeez, and C Gillbe. Biasing e\ufb00ect of the electromyogram on BIS: A controlled study during high-dose fentanyl induction. J Clin Monit Comput, 17 (6):377-81, Aug 2002.  A E Rigby-Jones and J R Sneyd. Pharmacokinetics and pharmacodynamics:  Is there  anything new? Anaesthesia, 67(1):5-11, Jan 2012.  H R\u00a8opcke, M Knen-Bergmann, M Cuhls, T Bouillon, and A Hoeft. Propofol and remifen- tanil pharmacodynamic interaction during orthopedic surgical procedures as measured by e\ufb00ects on bispectral index. J Clin Anesth, 13(3):198-207, May 2001a.  H R\u00a8opcke, B Rehberg, M Koenen-Bergmann, T Bouillon, J Bruhn, and A Hoeft. Surgical stimulation shifts EEG concentration-response relationship of des\ufb02urane. Anesthesiology, 94(3):255-113, Mar 2001b.  S Russel and P Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, 2nd  edition, 2002.  F Sahba, H R Tizhoosh, and M M A Salama. Application of reinforcement learning for  segmentation of transrectal ultrasound images. BMC Med Imaging, 8(8), 2008.  T Sakai, A Matsuki, P F White, and A H Giesecke. Use of an EEG-bispectral closed-loop delivery system for administering propofol. Acta Anesth Scand, 44:1007-1010, 2000. Reinforcement Learning for Closed-Loop Propofol Anesthesia  R H Sandin, G Enlund, P Samuelsson, and C Lennmarken. Awareness during anaesthesia:  A prospective case study. Lancet, 355(9205):707-711, 2000.  J Schaublin, M Derighetti, P Feigenwinter, S Petersen-Felix, and A M Zbinden. Fuzzy logic control of mechanical ventilation during anaesthesia. Brit J Anaesth, 77(5):636-41, Nov 1996.  T Schnider, C F Minto, P L Gambus, C Andresen, D B Goodale, S L Shafer, and E J Youngs. The in\ufb02uence of method of administration and covariates on the pharmacokinetics of propofol in adult volunteers. Anesthesiology, 88(5):1170-1182, May 1998.  T W Schnider, C F Minto, S L Shafer, P L Gambus, C Andresen, D B Goodale, and E J Youngs. The in\ufb02uence of age on propofol pharmacodynamics. Anesthesiology, 90(6): 1502-16, Jun 1999.  H Schwilden, J Sch\u00a8uttler, and H Stoeckel. Closed-loop feedback control of methohexital anesthesia by quantitative EEG analysis in humans. Anesthesiology, 67(3):341-7, Sep 1987.  P S Sebel, T A Bowdle, M M Ghoneim, I J Rampil, R E Padilla, T J Gan, and K B Domino. The incidence of awareness during anesthesia: A multicenter United States study. Anesth Analg, 99:833-839, 2004.  F S Servin. TCI compared with manually controlled infusion of propofol: A multicentre  study. Anaesthesia, 53(Suppl 1):82-86, Apr 1998.  M M Shanechi, J J Chemali, M Liberman M, K Solt, and E N Brown. A brain-machine interface for control of medically-induced coma. PLOS Compu Biol, 9(10):1-17, Oct 2013.  J C Sigl and N G Chamoun. An introduction to bispectral analyis for the electroencephalo-  gram. J Clin Monitor, 10(6):392-404, November 1994.  J W Sleigh, J Andrzejowski, A Steyn-Ross, and M Steyn-Ross. The bispectral index: A  measure of depth of sleep? Anesth Analg, 88(3):659-661, Mar 1999.  M M R F Struys, T De Smet, S D Greenwald, A R Abasalom, S Bing\u00b4e, and E P Mortier. Closed-loop controlled administration of propofol using bispectral analysis. Anesthesiol- ogy, 95(1):6-17, Jul 2001.  M M R F Struys, T De Smet, S D Greenwald, A R Absalom, S Bing\u00b4e, and E P Mortier. Performance evaluation of two published closed-loop control systems using bispectral index monitoring: A simulation study. Anesthesiology, 100(3):640-7, Mar 2004.  M M R F Struys, M J Coppens, N De Neve, E P Mortier, A G Doufas, J F P Van Bocxlaer, and S L Shafer. In\ufb02uence of administration rate on propofol plasma-e\ufb00ect site equilibration. Anesthesiology, 07(3):386-396, Sept 2007.  R Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse In Touretzky, Mozer, and Hasselmo, editors, Advances in Neural Information  coding. Processing Systems, volume 8, pages 1038-1044. The MIT Press, 1996. Moore, Kulkarni, Panousis, Padrez, Pyeatt and Doufas  R S Sutton and A G Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  G Tesauro. Temporal di\ufb00erence learning of backgammon strategy. In Proceedings of the International Conference on Machine Learning, pages 451-457. Morgan Kaufmann, 1992.  D R Theil, T E Stanley, 3rd, W D White, D Goodman, P S Glass, S A Bai, J R Jacobs, and J G Reves. Midazolam and fentanyl continuous infusion anesthesia for cardiac surgery: A comparison of computer-assisted versus manual infusion systems. J Cardiothorac Vasc Anesth, 7(3):300-6, Jun 1993.  J N Tsitsiklas and B Van Roy. An analysis of temporal di\ufb00erence learning with function approximation. Technical Report LIDS-P-2322, Massachusetts Institute of Technology, 1996.  C Vanlersberghe and F Camu. Modern Anesthetics (Handbook of Experimental Pharmacol-  ogy), volume 182, chapter Propofol, pages 227-252. Springer, 2008.  J R Varvel, D L Donoho, and S L Shafer. Measuring the predictive performance of computer-  controlled infusion pumps. J Pharmacokinet Biopharm, 20:63-94, Feb 1992.  C J C H Watkins. Learning from Delayed Rewards. PhD dissertation, Cambridge University,  Computer Science Department, 1989.  W Wood. Variability of human drug response. Anesthesiology, 71(4):631-634, Nov 1989.  L Zadeh. Fuzzy sets. Information and Control, 8:338-353, 1965.  Y Zhao, M R Kosorok, and D Zeng. Reinforcement learning design for cancer clinical trials.  Stat Med, 28(26):3294-315, Nov 2009. "}, "Clustering Hidden Markov Models with Variational HEM": {"volumn": 15, "url": "http://jmlr.org/papers/v15/coviello14a.html", "header": "Clustering Hidden Markov Models with Variational HEM", "author": "Emanuele Coviello, Antoni B. Chan, Gert R.G. Lanckriet", "time": "15(22):697\u2212747, 2014.", "abstract": "The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a \u00c3\u00a2\u00c2\u0080\u00c2\u009ccluster center\u00c3\u00a2\u00c2\u0080\u00c2\u009d, that is, a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand- writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization.", "pdf_url": "http://jmlr.org/papers/volume15/coviello14a/coviello14a.pdf", "keywords": ["Hierarchical EM algorithm", "clustering", "hidden Markov model", "hidden Markov mixture model", "variational approximation", "time-series classification"], "reference": "Dimitris Achlioptas, Frank McSherry, and Bernhard Sch\u00a8olkopf. Sampling techniques for kernel methods. In Advances in Neural Information Processing Systems 14, volume 1, pages 335-342. MIT Press, 2002.  Jonathan Alon, Stan Sclaro\ufb00, George Kollios, and Vladimir Pavlovic. Discovering clusters in motion time-series data. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages 368-375. IEEE, 2003.  Arthur Asuncion and David J Newman. UCI machine learning repository, 2010. URL  http://archive.ics.uci.edu/ml.  Claus Bahlmann and Hans Burkhardt. Measuring HMM similarity with the Bayes proba- bility of error and its application to online handwriting recognition. In Proceedings of the Sixth International Conference on Document Analysis and Recognition, pages 406-411. IEEE, 2001.  Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. The Journal of Machine Learning Research, 6:1705-1749, 2005.  742   Coviello, Chan and Lanckriet  experiment (d) number of clusters C(cid:48) varies, S(cid:48) = 3  experiment (e) number of HMM states S(cid:48) varies, C(cid:48) = 4 fixed  VHEM-H3M 0.665 0.673 PPK-SC3 0.782 0.729  4 (true) 0.811 0.768  5 0.816 0.781  2 0.801 0.766  3 (true) 0.811 0.768  4 0.828 0.781  5 0.832 0.798  Table 7: Results on clustering synthetic data with VHEM-H3M and PPK-SC, when the model order does not match the order of the true model used for generating the data. We fist vary the number of clusters C(cid:48), keeping S(cid:48) = 3 fixed to the true value. Then, we vary the number of HMM states S(cid:48), keeping C(cid:48) = 4 fixed to the true value. Performance is measured in terms of Rand-index, and is averaged over K \u2208 {2, 4, 8, 16, 32}.  Performance are more sensitive to selecting a su\ufb03cient number of clusters than using the right number of HMM states. In particular, when using fewer clusters than the true number of classes (e.g., C(cid:48) < C), the Rand-index degrades for both VHEM-H3M and PPK-SC, see experiment (d) in Table 7. On the opposite, performance are relatively stable when the number of HMM states does not match the true one, for example, S(cid:48) (cid:54)= S, see experiment (e) in Table 7. In particular, when using fewer HMM states (e.g., S(cid:48) < S) the model can still capture some of the dynamics, and the drop in performance is not significant. It is also interesting to note that using a larger number of HMM states (e.g., S(cid:48) > S) leads to slightly better results. The reason is that, when estimating the HMMs on the corrupted data sequences, there are additional states to better account for the e\ufb00ect of the noise.  References  Dimitris Achlioptas, Frank McSherry, and Bernhard Sch\u00a8olkopf. Sampling techniques for kernel methods. In Advances in Neural Information Processing Systems 14, volume 1, pages 335-342. MIT Press, 2002.  Jonathan Alon, Stan Sclaro\ufb00, George Kollios, and Vladimir Pavlovic. Discovering clusters in motion time-series data. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages 368-375. IEEE, 2003.  Arthur Asuncion and David J Newman. UCI machine learning repository, 2010. URL  http://archive.ics.uci.edu/ml.  Claus Bahlmann and Hans Burkhardt. Measuring HMM similarity with the Bayes proba- bility of error and its application to online handwriting recognition. In Proceedings of the Sixth International Conference on Document Analysis and Recognition, pages 406-411. IEEE, 2001.  Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. The Journal of Machine Learning Research, 6:1705-1749, 2005. Clustering HMMs with Variational HEM  Eloi Batlle, Jaume Masip, and Enric Guaus. Automatic song identification in noisy broad- cast audio. In Proceeding of the International Conference on Signal and Image Processing. IASTED, 2002.  Jerome R Bellegarda and David Nahamoo. Tied mixture continuous parameter modeling for speech recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 38(12):2033-2045, 1990.  Yoshua Bengio, Jean-Fran\u00b8cois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spec- tral clustering. In Advances in Neural Information Processing Systems, volume 16, pages 177-184. MIT Press, 2004.  David M Blei and Michael I Jordan. Variational inference for Dirichlet process mixtures.  Bayesian Analysis, 1(1):121-143, 2006.  William M Campbell. A SVM/HMM system for speaker recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 2, pages II-209. IEEE, 2003.  Gustavo Carneiro, Antoni B Chan, Pedro J Moreno, and Nuno Vasconcelos. Supervised learning of semantic classes for image annotation and retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(3):394-410, 2007.  Antoni B Chan, Emanuele Coviello, and Gert RG Lanckriet. Clustering Dynamic Tex- tures with the Hierarchical EM Algorithm. In Proceeding of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010a.  Antoni B Chan, Emanuele Coviello, and Gert RG Lanckriet. Derivation of the hierarchical EM Algorithm for Dynamic Textures. Technical report, City University of Hong Kong, 2010b.  Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning sequence kernels. In IEEE Workshop on Machine Learning for Signal Processing, pages 2-8. IEEE, 2008.  Emanuele Coviello, Antoni B Chan, and Gert RG Lanckriet. Time series models for semantic music annotation. IEEE Transactions on Audio, Speech, and Language Processing, 5(19): 1343-1359, 2011.  Emanuele Coviello, Antoni Chan, and Gert Lanckriet. The variational hierarchical EM In Advances in Neural Information  algorithm for clustering hidden Markov models. Processing Systems 25, pages 413-421, 2012a.  Emanuele Coviello, Adeel Mumtaz, Antoni B Chan, and Gert RG Lanckriet. Growing a Bag of Systems Tree for fast and accurate classification. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1979-1986. IEEE, 2012b.  Imre Csisz\u00b4ar and G\u00b4abor Tusn\u00b4ady.  Information geometry and alternating minimization  procedures. Statistics and decisions, 1984. Coviello, Chan and Lanckriet  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from in- complete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1-38, 1977.  Inderjit S. Dhillon. Di\ufb00erential entropic clustering of multivariate Gaussians. In Advances  in Neural Information Processing Systems, volume 19, page 337. MIT Press, 2007.  Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and Stefano Soatto. Dynamic  textures. International Journal on Computer Vision, 51(2):91-109, 2003.  Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. Automatic gen- In Advances in Neural Information  eration of social tags for music recommendation. Processing Systems, pages 385-392. MIT Press, 2008.  Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping using the nystr\u00a8om method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214-225, 2004.  Jacob Goldberger and Sam Roweis. Hierarchical clustering of a mixture model. In Advances  in Neural Information Processing Systems, pages 505-512. MIT Press, 2004.  Asela Gunawardana and William Byrne. Convergence theorems for generalized alternating  minimization procedures. Journal of Machine Learning Research, 6:2049-73, 2005.  Peter Hall, Joel L Horowitz, and Bing-Yi Jing. On blocking rules for the bootstrap with  dependent data. Biometrika, 82(3):561-574, 1995.  Trevor Hastie, Robert Tibshirani, Jerome Friedman, and James Franklin. The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelli- gencer, 27(2):83-85, 2005.  Christian Hennig. Cluster-wise assessment of cluster stability. Computational Statistics &  Data Analysis, 52(1):258-271, 2007.  John R Hershey and Peder A Olsen. Variational Bhattacharyya divergence for hidden In IEEE Computer Society International Conference on Acoustics,  Markov models. Speech and Signal Processing, pages 4557-4560. IEEE, 2008.  John R Hershey, Peder A Olsen, and Steven J Rennie. Variational Kullback-Leibler diver- gence for hidden Markov models. In IEEE Workshop on Automatic Speech Recognition & Understanding, pages 323-328. IEEE, 2007.  Matthew D Ho\ufb00man, David M Blei, and Perry R Cook. Easy as CBA: A simple probabilistic model for tagging music. In Proceedings of the 10th International Symposium on Music Information Retrieval, pages 369-374, 2009.  Xuedong D Huang and Mervyn A Jack. Semi-continuous hidden Markov models for speech  signals. Computer Speech & Language, 3(3):239-251, 1989.  Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2  (1):193-218, 1985. Clustering HMMs with Variational HEM  Tommi S. Jaakkola. Tutorial on Variational Approximation Methods. In Advanced Mean  Field Methods: Theory and Practice, pages 129-159. MIT Press, 2000.  Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. The Journal  of Machine Learning Research, 5:819-844, 2004.  Tony Jebara, Yingbo Song, and Kapil Thadani. Spectral clustering and embedding with  hidden Markov models. In Machine Learning: ECML 2007, pages 164-175. 2007.  Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183- 233, 1999.  Biing-Hwang Juang and Lawrence R Rabiner. A probabilistic distance measure for hidden  Markov models. AT&T Technical Journal, 64(2):391-408, February 1985.  Leonard Kaufman and Peter Rousseeuw. Clustering by means of medoids. Statistical Data  Analysis Based on the L1-Norm and Related Methods, pages 405-416, 1987.  Eamonn Keogh and Chotirat Ann Ratanamahatana. Exact indexing of dynamic time warp-  ing. Knowledge and Information Systems, 7(3):358-386, 2005.  Eamonn J Keogh and Michael J Pazzani. Scaling up dynamic time warping for datamining In Proceedings of the Sixth ACM SIGKDD International Conference on  applications. Knowledge Discovery and Data Mining, pages 285-289. ACM, 2000.  Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, and Michael I Jordan. Bootstrapping big data. Advances in Neural Information Processing Systems, Workshop: Big Learning: Algorithms, Systems, and Tools for Learning at Scale, 2011.  Anders Krogh, Michael Brown, I Saira Mian, Kimmen Sj\u00a8olander, and David Haussler. Hid- den Markov models in computational biology. Applications to protein modeling. Journal of Molecular Biology, 235(5):1501-1531, 1994.  Pavel Kuksa, Pai-Hsi Huang, and Vladimir Pavlovic. Scalable algorithms for string kernels with inexact matching. In Advances in Neural Information Processing Systems, pages 881-888, 2008.  Christina Leslie, Eleazar Eskin, and William Sta\ufb00ord Noble. The spectrum kernel: A string kernel for SVM protein classification. In Proceedings of the Pacific Symposium on Biocomputing, volume 7, pages 566-575, 2002.  Marcus Liwicki and Horst Bunke. Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard. In Document Analysis and Recognition, 2005. Proceedings. Eighth International Conference on, pages 956-961. IEEE, 2005.  Rune B Lyngso, Christian NS Pedersen, and Henrik Nielsen. Metrics and similarity measures for hidden Markov models. In Proceedings International Conference on Intelligent Systems for Molecular Biology, pages 178-186, 1999. Coviello, Chan and Lanckriet  David JC MacKay. Information Theory, Inference and Learning Algorithms. Cambridge  university press, 2003.  Michael I Mandel and Daniel PW Ellis. Multiple-instance learning for music information retrieval. In Proceedings of the 9th International Symposium on Music Information Re- trieval, pages 577-582, 2008.  Nag, Kin-Hong Wong, and Frank Fallside. Script recognition using hidden Markov models. In IEEE Computer Society International Conference on Acoustics, Speech, and Signal Processing, volume 11, pages 2071-2074. IEEE, 1986.  Radford M Neal and Geo\ufb00rey E Hinton. A view of the EM algorithm that justifies incre- mental, sparse, and other variants. Learning in Graphical Models, 89:355-370, 1998.  Evert J Nystr\u00a8om. \u00a8Uber die praktische au\ufb02\u00a8osung von integralgleichungen mit anwendungen  auf randwertaufgaben. Acta Mathematica, 54(1):185-204, 1930.  Tim Oates, Laura Firoiu, and Paul R Cohen. Clustering time series with hidden markov models and dynamic time warping. In Joint Conferences on Artificial Intelligence, Work- shop on Neural, Symbolic and Reinforcement Learning Methods for Sequence Learning, pages 17-21, 1999.  Antonello Panuccio, Manuele Bicego, and Vittorio Murino. A hidden Markov model-based approach to sequential data clustering. Structural, Syntactic, and Statistical Pattern Recognition, pages 734-743, 2002.  William D Penny and Stephen J Roberts. Notes on variational learning. Technical report,  Oxford University, 2000.  Yuting Qi, John William Paisley, and Lawrence Carin. Music analysis using hidden Markov  mixture models. IEEE Transactions on Signal Processing, 55(11):5209-5224, 2007.  Lawrence R Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Prentice  Hall, Upper Saddle River (NJ, USA), 1993.  Jeremy Reed and Chin-Hui Lee. A study on music genre classification based on universal acoustic models. In Proceedings of the 7th International Symposium on Music Information Retrieval, pages 89-94, 2006.  Jose Rodriguez-Serrano and Florent Perronnin. A model-based sequence similarity with application to handwritten word-spotting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34:2108 - 2120, 2012.  Nicolas Scaringella and Giorgio Zoia. On the modeling of time information for automatic genre recognition systems in audio signals. In Proc. 6th International Symposium Music Information Retrieval, pages 666-671, 2005.  Padhraic Smyth. Clustering sequences with hidden Markov models. In Advances in Neural  Information Processing Systems, pages 648-654. MIT Press, 1997. Clustering HMMs with Variational HEM  Theodoros Theodoridis and Huosheng Hu. Action classification of 3d human models using dynamic ANNs for mobile robot surveillance. In IEEE Computer Society International Conference on Robotics and Biomimetics, pages 371-376. IEEE, 2007.  Douglas Turnbull, Luke Barrington, David Torres, and Gert RG Lanckriet. Semantic an- notation and retrieval of music and sound e\ufb00ects. IEEE Transactions on Audio, Speech and Language Processing, 16(2):467-476, February 2008.  Nuno Vasconcelos. Image indexing with mixture hierarchies. In IEEE Computer Society  Conference on Computer Vision and Pattern Recognition, 2001.  Nuno Vasconcelos and Andrew Lippman. Learning mixture hierarchies.  In Advances in  Neural Information Processing Systems, 1998.  Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1-305, 2008.  Ben H Williams, Marc Toussaint, and Amos J Storkey. Extracting motion primitives from natural handwriting data. In Proceeding of the 16th International Conference on Artificial Neural Networks, pages 634-643. Springer, 2006.  Jie Yin and Qiang Yang.  Integrating hidden Markov models and spectral analysis for sensory time series clustering. In IEEE Computer Society International Conference on Data Mining. IEEE, 2005.  Shi Zhong and Joydeep Ghosh. A unified framework for model-based clustering. The  Journal of Machine Learning Research, 4:1001-1037, 2003. "}, "A Novel M-Estimator for Robust PCA": {"volumn": 15, "url": "http://jmlr.org/papers/v15/zhang14a.html", "header": "A Novel M-Estimator for Robust PCA", "author": "Teng Zhang, Gilad Lerman", "time": "15(23):749\u2212808, 2014.", "abstract": "We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate \u00c3\u00a2\u00c2\u0080\u00c2\u009crobust inverse sample covariance\u00c3\u00a2\u00c2\u0080\u00c2\u009d by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy.", "pdf_url": "http://jmlr.org/papers/volume15/zhang14a/zhang14a.pdf", "keywords": ["re-weighted least squares", "convex relaxation"], "reference": "A. Agarwal, S. Negahban, and M. J. Wainwright. Noisy matrix decomposition via convex relaxation: optimal rates in high dimensions. Ann. Statist., 40(2):1171-1197, 2012a. ISSN 0090-5364. doi: 10.1214/12-AOS1000.  802   Teng Zhang and Gilad Lerman  8. Conclusion  We proposed an M-estimator for the problems of exact and near subspace recovery. Sub- stantial theory has been developed to quantify the recovery obtained by this estimator as well as its numerical approximation. Numerical experiments demonstrated state-of-the-art speed and accuracy for our corresponding implementation on both synthetic and real data sets.  This work broadens the perspective of two recent ground-breaking theoretical works for subspace recovery by Cand`es et al. (2011) and Xu et al. (2012). We hope that it will motivate additional approaches to this problem.  There are many interesting open problems that stem from our work. We believe that by modifying or extending the framework described in here, one can even yield better results in various scenarios. For example, we have discussed in \u00a71.2 the modification by Lerman et al. (2012) suggesting tighter convex relaxation of orthogonal projectors when d is known. We also discussed in \u00a71.2 adaptation by Wang and Singer (2013) of the basic ideas in here to the di\ufb00erent synchronization problem. Another direction was recently followed up by Coudron and Lerman (2012), where they established exact asymptotic subspace recovery under specific sampling assumptions, which may allow relatively large magnitude of noise. It is interesting to follow this direction and establish exact recovery when using in theory a sequence of IRLS regularization parameters {\u03b4i}i\u2208N approaching zero (in analogy to the work of Daubechies et al. 2010).  An interesting generalization that was not pursued so far is robust data modeling by multiple subspaces or by locally-linear structures. It is also interesting to know whether one can adapt the current framework so that it can detect linear structure in the presence of both sparse elementwise corruption (as in Cand`es et al. 2011) and the type of outliers addressed in here.  Acknowledgments  This work was supported by NSF grants DMS-09-15064 and DMS-09-56072, GL was also partially supported by the IMA (during 2010-2012). Arthur Szlam has inspired our extended research on robust l1-type subspace recovery. We thank John Wright for referring us to Xu et al. (2010b) shortly after it appeared online and for some guidance with the real data sets. GL thanks Emmanuel Cand`es for inviting him to visit Stanford university in May 2010 and for his constructive criticism on the lack of a theoretically guaranteed algorithm for the l1 subspace recovery of Lerman and Zhang (2010).  Supp. webpage: http://www.math.umn.edu/~lerman/gms.  References  A. Agarwal, S. Negahban, and M. J. Wainwright. Noisy matrix decomposition via convex relaxation: optimal rates in high dimensions. Ann. Statist., 40(2):1171-1197, 2012a. ISSN 0090-5364. doi: 10.1214/12-AOS1000. A Novel M-Estimator for Robust PCA  A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452- 2482, 2012b.  L. P. Ammann. Robust singular value decompositions: A new approach to projection pursuit. Journal of the American Statistical Association, 88(422):pp. 505-514, 1993. ISSN 01621459.  E. Arias-Castro, D. L. Donoho, X. Huo, and C. A. Tovey. Connect the dots: how many random points can a regular curve pass through? Adv. in Appl. Probab., 37(3):571-603, 2005.  E. Arias-Castro, G. Chen, and G. Lerman. Spectral clustering based on local linear approx-  imations. Electron. J. Statist., 5:1537-1587, 2011.  O. Arslan. Convergence behavior of an iterative reweighting algorithm to compute multivari- ate M-estimates for location and scatter. Journal of Statistical Planning and Inference, 118(1-2):115 - 128, 2004. ISSN 0378-3758. doi: 10.1016/S0378-3758(02)00402-0.  A. Bargiela and J. K. Hartley. Orthogonal linear regression algorithm based on augmented matrix formulation. Comput. Oper. Res., 20:829-836, October 1993. ISSN 0305-0548. doi: 10.1016/0305-0548(93)90104-Q.  R. Basri and D. Jacobs. Lambertian re\ufb02ectance and linear subspaces. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 25(2):218-233, February 2003.  R. Basri, T. Hassner, and L. Zelnik-Manor. Approximate nearest subspace search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(2):266-278, 2011. ISSN 0162-8828. doi: http://doi.ieeecomputersociety.org/10.1109/TPAMI.2010.110.  R. Bhatia and D. Drissi. Generalized Lyapunov equations and positive definite functions. SIAM J. Matrix Anal. Appl., 27(1):103-114, May 2005. ISSN 0895-4798. doi: 10.1137/ 040608970.  P. Bradley and O. Mangasarian. k-plane clustering. J. Global optim., 16(1):23-32, 2000.  S. C. Brubaker. Robust PCA and clustering in noisy mixtures.  In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201909, pages 1078-1087, Philadelphia, PA, USA, 2009. Society for Industrial and Applied Mathematics.  E. J. Cand`es, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccu- rate measurements. Communications on Pure and Applied Mathematics, 59(8):1207-1223, 2006. doi: 10.1002/cpa.20124.  E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal  of the ACM (JACM), 58(3):11, 2011.  T. F. Chan and P. Mulet. On the convergence of the lagged di\ufb00usivity fixed point method ISSN  in total variation image restoration. SIAM J. Numer. Anal., 36:354-367, 1999. 0036-1429. doi: 10.1137/S0036142997327075. Teng Zhang and Gilad Lerman  V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM J. Optim., 21(2):572-596, 2011. ISSN 1052-6234. doi: 10.1137/090761793.  T.-J. Chin, J. Yu, and D. Suter. Accelerated hypothesis generation for multistructure data via preference analysis. IEEE Trans. Pattern Anal. Mach. Intell., 34(4):625-638, April 2012. ISSN 0162-8828. doi: 10.1109/TPAMI.2011.169.  A. K. Cline. Rate of convergence of Lawson\u2019s algorithm. Mathematics of Computation, 26  (117):pp. 167-176, 1972. ISSN 00255718.  M. Coudron and G. Lerman. On the sample complexity of robust PCA. In NIPS, pages  3230-3238, 2012.  C. Croux and G. Haesbroeck. Principal component analysis based on robust estimators of the covariance or correlation matrix: In\ufb02uence functions and e\ufb03ciencies. Biometrika, 87: 603-618, 2000.  C. Croux, P. Filzmoser, and M. Oliveira. Algorithms for projectionc pursuit robust principal component analysis. Chemometrics and Intelligent Laboratory Systems, 87(2):218-225, 2007.  A. d\u2019Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct formulation for sparse PCA using semidefinite programming. SIAM Review, 49(3):434-448, 2007. doi: 10.1137/050645506.  I. Daubechies, R. DeVore, M. Fornasier, and C. S. Gunturk. Iteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathe- matics, 63:1-38, 2010. doi: 10.1002/cpa.20303.  G. David and S. Semmes. Singular integrals and rectifiable sets in Rn: au-del`a des graphes  Lipschitziens. Ast\u00b4erisque, 193:1-145, 1991.  P. L. Davies. Asymptotic behaviour of s-estimates of multivariate location parameters and dispersion matrices. The Annals of Statistics, 15(3):pp. 1269-1292, 1987. ISSN 00905364.  C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM J.  on Numerical Analysis, 7:1-46, 1970.  S. J. Devlin, R. Gnandesikan, and J. R. Kettenring. Robust estimation of dispersion matrices and principal components. Journal of the American Statistical Association, 76(374):pp. 354-362, 1981. ISSN 01621459.  C. Ding, D. Zhou, X. He, and H. Zha. R1-PCA: rotational invariant L1-norm principal component analysis for robust subspace factorization. In ICML \u201906: Proceedings of the 23rd International Conference on Machine Learning, pages 281-288, New York, NY, USA, 2006. ACM. ISBN 1-59593-383-2. doi: 10.1145/1143844.1143880.  M. Fornasier, H. Rauhut, and R. Ward. Low-rank matrix recovery via iteratively reweighted least squares minimization. SIAM J. Optim., 21(4):1614-1640, 2011. ISSN 1052-6234. doi: 10.1137/100811404. A Novel M-Estimator for Robust PCA  M. Hardt and A. Moitra. Algorithms and hardness for robust subspace recovery. In COLT,  pages 354-375, 2013.  J. Ho, M. Yang, J. Lim, K. Lee, and D. Kriegman. Clustering appearances of objects under varying illumination conditions. In Proceedings of International Conference on Computer Vision and Pattern Recognition, volume 1, pages 11-18, 2003.  D. Hsu, S.M. Kakade, and Tong Zhang. Robust matrix decomposition with sparse corrup- tions. Information Theory, IEEE Transactions on, 57(11):7221 -7234, nov. 2011. ISSN 0018-9448. doi: 10.1109/TIT.2011.2158250.  P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley Series in Probability and ISBN 978-0-470-12990-6. doi: 10.  Statistics. Wiley, Hoboken, NJ, 2nd edition, 2009. 1002/9780470434697.  Q. Ke and T. Kanade. Robust subspace computation using L1 norm. Technical report,  Carnegie Mellon, 2003.  J. T. Kent and D. E. Tyler. Redescending M-estimates of multivariate location and scatter.  The Annals of Statistics, 19(4):pp. 2102-2119, 1991. ISSN 00905364.  H. W. Kuhn. A note on Fermat\u2019s problem. Mathematical Programming, 4:98-107, 1973.  ISSN 0025-5610. 10.1007/BF01584648.  N. Kwak. Principal component analysis based on L1-norm maximization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672-1680, 2008. doi: 10.1109/ TPAMI.2008.114.  C. L. Lawson. Contributions to the Theory of Linear Least Maximum Approximation. PhD  thesis, University of California, Los Angeles, 1961.  K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27 (5):684-698, 2005. ISSN 0162-8828. doi: 10.1109/TPAMI.2005.92.  G. Lerman and T. Zhang.  lp-Recovery of the most significant subspace among multiple subspaces with outliers. ArXiv e-prints, December 2010. To Appear in Constructive Approximation.  G. Lerman and T. Zhang. Robust recovery of multiple subspaces by geometric lp minimiza- tion. Ann. Statist., 39(5):2686-2715, 2011. ISSN 0090-5364. doi: 10.1214/11-AOS914.  G. Lerman, M. McCoy, J. A. Tropp, and T. Zhang. Robust computation of linear models,  or how to find a needle in a haystack. ArXiv e-prints, February 2012.  G. Li and Z. Chen. Projection-pursuit approach to robust dispersion matrices and princi- pal components: Primary theory and monte carlo. Journal of the American Statistical Association, 80(391):759-766, 1985. ISSN 01621459. doi: 10.2307/2288497. Teng Zhang and Gilad Lerman  L. Li, W. Huang, I. Gu, and Q. Tian. Statistical modeling of complex backgrounds for Image Processing, IEEE Transactions on, 13(11):1459 -  foreground object detection. 1472, nov. 2004. ISSN 1057-7149. doi: 10.1109/TIP.2004.836169.  Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix. In In Intl. Workshop on Comp. Adv. in Multi-Sensor Adapt. Processing, Aruba, Dutch Antilles, 2009.  G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation by low-rank representation. In  ICML, 2010.  G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank representation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):171 -184, 2013. ISSN 0162-8828. doi: 10.1109/TPAMI.2012.88.  H. P. Lopuha\u00a8a and P. J. Rousseeuw. Breakdown points of a\ufb03ne equivariant estimators of multivariate location and covariance matrices. Ann. Statist., 19(1):229-248, 1991. ISSN 0090-5364.  R. A. Maronna. Robust M-estimators of multivariate location and scatter. The Annals of  Statistics, 4(1):pp. 51-67, 1976. ISSN 00905364.  R. A. Maronna, R. D. Martin, and V. J. Yohai. Robust Statistics: Theory and methods. Wiley Series in Probability and Statistics. John Wiley & Sons Ltd., Chichester, 2006. ISBN 978-0-470-01092-1; 0-470-01092-4.  M. McCoy and J. Tropp. Two proposals for robust PCA using semidefinite programming.  Elec. J. Stat., 5:1123-1160, 2011.  S. Mendelson. A few notes on statistical learning theory. In Lecture Notes in Computer  Science, volume 2600, pages 1-40. Springer-Verlag, 2003.  H. Nyquist. Least orthogonal absolute deviations. Computational Statistics & Data Anal-  ysis, 6(4):361 - 367, 1988. ISSN 0167-9473. doi: 10.1016/0167-9473(88)90076-X.  M. R. Osborne and G. A. Watson. An analysis of the total approximation problem in separable norms, and an algorithm for the total l1 problem. SIAM Journal on Scientific and Statistical Computing, 6(2):410-424, 1985. doi: 10.1137/0906029.  A. M. Ostrowski. Solution of Equations and Systems of Equations. Academic Press, Second  edition, September 1966. ISBN 0471889873.  M. Soltanolkotabi and E. J. Cand`es. A geometric analysis of subspace clustering with  outliers. Ann. Stat., 40(4):2195-2238, 2012. doi: 10.1214/12-AOS1034.  H. Sp\u00a8ath and G. A. Watson. On orthogonal linear approximation. Numer. Math., 51:  531-543, October 1987. ISSN 0029-599X. doi: 10.1007/BF01400354.  C. V. Stewart. Robust parameter estimation in computer vision. SIAM Reviews, 41:513-  537, 1999. A Novel M-Estimator for Robust PCA  T. Tao. Topics in Random Matrix Theory, volume 132 of Graduate Studies in Mathematics.  American Mathematical Society, Providence, RI, 2012. ISBN 978-0-8218-7430-1.  M. Tipping and C. Bishop. Mixtures of probabilistic principal component analysers. Neural  Computation, 11(2):443-482, 1999.  F. De La Torre and M. J. Black. Robust principal component analysis for computer vision. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 1, pages 362 -369 vol.1, 2001. doi: 10.1109/ICCV.2001.937541.  F. De La Torre and M. J. Black. A framework for robust subspace learning. ISSN 0920-5691.  ternational Journal of Computer Vision, 54:117-142, 2003. 10.1023/A:1023709501986.  In- doi:  P. Tseng. Nearest q-\ufb02at to m points. Journal of Optimization Theory and Applications,  105:249-252, 2000. ISSN 0022-3239. 10.1023/A:1004678431677.  D. E. Tyler. A distribution-free M -estimator of multivariate scatter. Ann. Statist., 15(1):  234-251, 1987. ISSN 0090-5364. doi: 10.1214/aos/1176350263.  R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Com-  pressed sensing, pages 210-268. Cambridge Univ. Press, Cambridge, 2012.  H. Voss and U. Eckhardt. Linear convergence of generalized weiszfeld\u2019s method. Computing,  25:243-251, 1980. ISSN 0010-485X. doi: 10.1007/BF02242002.  L. Wang and A. Singer. Exact and stable recovery of rotations for robust synchronization.  Information and Inference, 2013. doi: 10.1093/imaiai/iat005.  G. A. Watson. Some Problems in Orthogonal Distance and Non-Orthogonal Distance Re- gression. Defense Technical Information Center, 2001. URL http://books.google.com/ books?id=WKKWGwAACAAJ.  G. A. Watson. On the gauss-newton method for l1 orthogonal distance regression. IMA Journal of Numerical Analysis, 22(3):345-357, 2002. doi: 10.1093/imanum/22.3.345.  E. Weiszfeld. Sur le point pour lequel la somme des distances de n points donne\u2019s est  minimum. Tohoku Math. J., 43:35-386, 1937.  H. Xu, C. Caramanis, and S. Mannor. Principal component analysis with contaminated  data: The high dimensional case. In COLT, pages 490-502, 2010a.  H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. In NIPS, pages  2496-2504, 2010b.  H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit.  Information ISSN 0018-9448. doi: 10.1109/TIT.  Theory, IEEE Transactions on, PP(99):1, 2012. 2011.2173156. Teng Zhang and Gilad Lerman  L. Xu and A.L. Yuille. Robust principal component analysis by self-organizing rules based on statistical physics approach. Neural Networks, IEEE Transactions on, 6(1):131-143, 1995. ISSN 1045-9227. doi: 10.1109/72.363442.  T. Zhang. Robust subspace recovery by geodesically convex optimization. ArXiv e-prints,  2012.  T. Zhang, A. Szlam, and G. Lerman. Median K-\ufb02ats for hybrid linear modeling with In Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th many outliers. International Conference on Computer Vision, pages 234-241, Kyoto, Japan, 2009. doi: 10.1109/ICCVW.2009.5457695.  T. Zhang, A. Szlam, Y. Wang, and G. Lerman. Randomized hybrid linear modeling by local best-fit \ufb02ats. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1927 -1934, jun. 2010. doi: 10.1109/CVPR.2010.5539866.  T. Zhang, A. Szlam, Y. Wang, and G. Lerman. Hybrid linear modeling via local best-fit \ufb02ats. International Journal of Computer Vision, 100:217-240, 2012. ISSN 0920-5691. doi: 10.1007/s11263-012-0535-6. "}, "Policy Evaluation with Temporal Differences: A Survey and Comparison": {"volumn": 15, "url": "http://jmlr.org/papers/v15/dann14a.html", "header": "Policy Evaluation with Temporal Differences: A Survey and Comparison", "author": "Christoph Dann, Gerhard Neumann, Jan Peters", "time": "15(24):809\u2212883, 2014.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume15/dann14a/dann14a.pdf", "keywords": ["ment learning"], "reference": "J. S. Albus. A new approach to manipulator control: The cerebellar model articulation con- troller (CMAC). Journal of Dynamic Systems Measurement and Control, 97(September): 220-227, 1975.  S.-i. Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):  251-276, Feb. 1998.  A. Antos, C. Szepesv\u00e1ri, and R. Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89-129, 2008.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems 24, 2011.  877   Policy Evaluation with Temporal Differences  Algorithm 12 parametric GPTD (Init: P 0 = I, po = 0, d0 = 0, s\u22121  0 = 0)  \u2206\u03c6t+1 =\u03c6t \u2212 \u03b3\u03c6t+1  pt+1 =pt  + P t\u2206\u03c6t+1  \u03b3\u03c32 t st \u03b3\u03c32 t st  dt+1 =dt  + rt \u2212 \u2206\u03c6T  t+1\u03b8t  st+1 =\u03c32  t + \u03b32\u03c32  t+1 \u2212  \u03b32\u03c34 t st (cid:21)T  pt  (cid:20)  +  pt+1 +  \u03b3\u03c32 t st  \u2206\u03c6t+1  \u03b8t+1 =\u03b8t +  pt+1dt+1  P t+1 =P t \u2212  pt+1pT  t+1  1 st+1 1 st+1  Algorithm 11 recursive BRM(\u03bb) (Init: M 0 = (cid:15)I, x0 = 0, y0 = 1, z0 = 0) Compute auxiliary values:  \u2206\u03c6t+1 =\u03c6t \u2212 \u03b3\u03c1t\u03c6t+1 \u03b3\u03bb\u03c1t\u22121\u221a yt yt\u2206\u03c6t+1 + pt+1xt,  pt+1 = U t+1 = (cid:2)\u221a V t+1 = (cid:2)\u221a yt\u2206\u03c6t+1 + pt+1xt, \u2212pt+1xt \u221a yt\u03c1trt + pt+1zt, W t+1 = [ Bt+1 =M tU t+1[I + V t+1M tU t+1]\u22121  pt+1xt  \u2212pt+1zt]T  (cid:3)  (cid:3)T  Update traces:  M t+1 =M t \u2212 Bt+1V t+1M t  yt+1 =(\u03b3\u03bb\u03c1t)2yt + 1 xt+1 =(\u03b3\u03bb\u03c1t\u22121)xt + yt\u2206\u03c6t+1 zt+1 =(\u03b3\u03bb\u03c1t\u22121)zt + rt\u03c1tyt  Update estimate:  \u03b8t+1 =\u03b8t + Bt+1(W t+1 \u2212 V t+1\u03b8t)  Algorithm 13 Residual-gradient algorithm without double-samples  \u03b8t+1 =\u03b8t + \u03b1t\u03c1t\u03b4t(\u03c6t \u2212 \u03b3\u03c6t+1)  References  J. S. Albus. A new approach to manipulator control: The cerebellar model articulation con- troller (CMAC). Journal of Dynamic Systems Measurement and Control, 97(September): 220-227, 1975.  S.-i. Amari. Natural gradient works e\ufb03ciently in learning. Neural Computation, 10(2):  251-276, Feb. 1998.  A. Antos, C. Szepesv\u00e1ri, and R. Munos. Learning near-optimal policies with Bellman- residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89-129, 2008.  F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems 24, 2011. Dann, Neumann and Peters  L. Baird. Residual algorithms : Reinforcement learning with function approximation. In  Proceedings of the Twelfth International Conference on Machine Learning, 1995.  P. Balakrishna, R. Ganesan, and L. Sherry. Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures. Transportation Research Part C: Emerging Technologies, 18(6):950-962, 2010.  D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Bel-  mont, Massachusetts, 1996. ISBN 1-886529-10-8.  D. P. Bertsekas and H. Yu. Projected equation methods for approximate solution of large linear systems. Journal of Computational and Applied Mathematics, 227(1):27-50, 2009.  J. A. Boyan. Technical update: Least-squares temporal di\ufb00erence learning. Machine Learn-  ing, 49(2):233-246, 2002.  S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal di\ufb00erence  learning. Machine Learning, 22(1-3):33-57, 1996.  E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger  than n. The Annals of Statistics, 35(6):2313-2351, 2005.  D. Choi and B. Roy. A generalized Kalman filter for fixed point approximation and e\ufb03cient temporal-di\ufb00erence learning. Discrete Event Dynamic Systems, 16(2):207-239, 2006.  R. W. Cottle, J.-S. Pang, and R. E. Stone. The Linear Complementarity Problem. Computer  Science and Scientific Computing. Academic Press, 1992. ISBN 0121923509.  R. H. Crites and A. G. Barto. Elevator group control using multiple reinforcement learning  agents. Machine Learning, 33(2-3):235-262, 1998.  W. Dabney and A. G. Barto. Adaptive step-size for online temporal di\ufb00erence learning. In  Proceedings of the 26th AAAI Conference on Artificial Intelligence, 2012.  P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. A tutorial on the cross-  entropy method. Annals of Operations Research, (1):19-67, 2010.  M. P. Deisenroth. E\ufb03cient Reinforcement Learning using Gaussian Processes. PhD thesis,  Karlsruhe Institute of Technology, 2010.  M. P. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-e\ufb03cient approach to policy search. In Proceedings of the 28th International Conference on Machine Learning, 2011.  B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of  Statistics, 32(2):407-499, 2004.  Y. Engel. Algorithms and Representations for Reinforcement Learning. PhD thesis, Hebrew  University, 2005. Policy Evaluation with Temporal Differences  Y. Engel, S. Mannor, and R. Meir. Bayes meets Bellman: The Gaussian process approach to temporal di\ufb00erence learning. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.  Y. Engel, S. Mannor, and R. Meir. Reinforcement learning with Gaussian processes. In  Proceedings of the 22nd International Conference on Machine Learning, 2005.  A.-m. Farahmand and C. Szepesv\u00e1ri. Model selection in reinforcement learning. Machine  Learning, 85(3):299-332, 2011.  A.-m. Farahmand, M. Ghavamzadeh, C. Szepesv\u00e1ri, and S. Mannor. Regularized policy  iteration. In Advances in Neural Information Processing Systems 21, 2008.  J. Frank, S. Mannor, and D. Precup. Reinforcement learning in the presence of rare events.  In Proceedings of the 25th International Conference on Machine Learning, 2008.  M. Geist and O. Pietquin. Kalman temporal di\ufb00erences. Journal of Artificial Intelligence  Research, 39(1):483-532, 2010.  M. Geist and B. Scherrer.  l1-penalized projected Bellman residual. In Proceedings of the  Nineth European Workshop on Reinforcement Learning, 2011.  M. Geist and B. Scherrer. O\ufb00-policy learning with eligibility traces : A survey. Technical  report, INRIA Lorraine - LORIA, 2013.  M. Geist, B. Scherrer, A. Lazaric, and M. Ghavamzadeh. A Dantzig selector approach to temporal di\ufb00erence learning. In Proceedings of the 29th International Conference on Machine Learning, 2012.  S. Gelly and D. Silver. Achieving master level play in 9 x 9 computer go. In Proceedings of  the 23th AAAI Conference on Artificial Intelligence, 2008.  A. Geramifard, M. Bowling, and R. S. Sutton. Incremental least-squares temporal di\ufb00erence  learning. Proceedings of the 21th AAAI Conference on Artificial Intelligence, 2006a.  A. Geramifard, M. Bowling, M. Zinkevich, and R. S. Sutton. iLSTD: Eligibility traces and convergence analysis. In Advances in Neural Information Processing Systems 19, 2006b.  A. Geramifard, F. Doshi, J. Redding, N. Roy, and J. P. How. Online discovery of feature dependencies. In Proceedings of the 28th International Conference on Machine Learning, 2011.  A. Geramifard, T. J. Walsh, and J. P. How. Batch-iFDD for representation expansion in  large MDPs. In Conference on Uncertainty in Artificial Intelligence, 2013.  M. Ghavamzadeh, A. Lazaric, O.-A. Maillard, and R. Munos. LSTD with random projec-  tions. In Advances in Neural Information Processing Systems 23, 2010.  M. Ghavamzadeh, A. Lazaric, R. Munos, and M. Ho\ufb00man. Finite-sample analysis of Lasso- TD. In Proceedings of the 28th International Conference on Machine Learning, 2011. Dann, Neumann and Peters  P. W. Glynn and D. L. Iglehart. Importance sampling for stochastic simulations. Manage-  ment Science, 35(11):1367-1392, 1989.  H. Hachiya and M. Sugiyama. Feature selection for reinforcement learning: Evaluating im- plicit state-reward dependency via conditional mutual information. In European Con- ference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2010.  M. Ho\ufb00man, A. Lazaric, M. Ghavamzadeh, and R. Munos. Regularized least squares tem- poral di\ufb00erence learning with nested l2 and l1 penalization. In Proceedings of the Nineth European Workshop on Reinforcement Learning, 2011.  M. Hutter and S. Legg. Temporal di\ufb00erence updating without a learning rate. In Advances  in Neural Information Processing Systems 20, 2007.  R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical In Proceedings of the 27th International Conference on Machine  dictionary learning. Learning, 2010.  J. Johns and S. Mahadevan. Sparse approximate policy evaluation using graph-based basis  functions. Technical report, University of Massachusetts Amherst, 2009.  J. Johns, C. Painter-Wakefield, and R. Parr. Linear complementarity for regularized policy evaluation and improvement. In Advances in Neural Information Processing Systems 23, 2010.  T. Jung and D. Polani. Least squares SVM for least squares TD learning.  In European  Conference on Artificial Intelligence, 2006.  P. W. Keller, S. Mannor, and D. Precup. Automatic basis function construction for ap- proximate dynamic programming and reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning, 2006.  J. Z. Kolter and A. Y. Ng. Regularization and feature selection in least-squares temporal dif- ference learning. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.  R. M. Kretchmar and C. W. Anderson. Comparison of CMACs and radial basis functions for kocal function approximators in reinforcement learning. In International Conference on Neural Networks, 1997.  M. G. Lagoudakis and R. Parr. Least-squares policy iteration. The Journal of Machine  Learning Research, 4(Dec):1107-1149, 2003.  A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of LSTD. In Proceedings  of the 27th International Conference on Machine Learning, 2010.  L. Li. A worst-case comparison between temporal di\ufb00erence and residual gradient with linear function approximation. In Proceedings of the 25th International Conference on Machine Learning, 2008. Policy Evaluation with Temporal Differences  B. Liu, S. Mahadevan, and J. Liu. Regularized o\ufb00-policy TD-learning.  In Advances in  Neural Information Processing Systems 25, 2012.  M. Loth, M. Davy, and P. Preux. Sparse temporal di\ufb00erence learning using LASSO. In IEEE  Symposium on Adaptive Dynamic Programming And Reinforcement Learning, 2007.  H. R. Maei. Gradient Temporal-Di\ufb00erence Learning Algorithms. PhD thesis, University of  Alberta, 2011.  S. Mahadevan and M. Maggioni. Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes. Journal of Machine Learning Research, 8(Oct):2169-2231, 2007.  A. R. Mahmood, R. S. Sutton, T. Degris, and P. M. Pilarski. Tuning-free step-size adap- In IEEE International Conference on Acoustics, Speech and Signal Processing,  tation. 2012.  I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal di\ufb00erence  reinforcement learning. Annals of Operations Research, 134(1):215-238, 2005.  D. Meyer, H. Shen, and K. Diepold. l1-regularized gradient temporal-di\ufb00erence learning. In  Proceedings of the Tenth European Workshop on Reinforcement Learning, 2012.  A. Nedic and D. P. Bertsekas. Least squares policy evaluation algorithms with linear function  approximation. Discrete Event Dynamic Systems, 13(1-2):79-110, 2003.  C. Painter-Wakefield and R. Parr. Greedy algorithms for sparse reinforcement learning. In  Proceedings of the 29th International Conference on Machine Learning, 2012a.  C. Painter-Wakefield and R. Parr. L1 regularized linear temporal di\ufb00erence learning. Tech-  nical report, Duke University, Durham, NC, 2012b.  R. Parr, C. Painter-Wakefield, L. Li, and M. Littman. Analyzing feature generation for In Proceedings of the 24th International Conference on  value-function approximation. Machine Learning, 2007.  R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An analysis of lin- ear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th International Conference on Machine Learning, 2008.  Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogonal matching pursuit: recursive func- tion approximation with applications to wavelet decomposition. In Proceedings of the 27th Asilomar Conference on Signals, Systems and Computers, 1993.  M. Petrik, G. Taylor, R. Parr, and S. Zilberstein. Feature selection using regularization in approximate linear programs for Markov decision processes. In Proceedings of the 27th International Conference on Machine Learning, 2010.  B. A. Pires. Statistical Analysis of L1-penalized Linear Estimation with Applications. Master  thesis, University of Alberta, 2011. Dann, Neumann and Peters  C. E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In Advances  in Neural Information Processing Systems 16, 2003.  M. Riedmiller and T. Gabel. On experiences in a complex and competitive gaming do- In IEEE Symposium on Computational  main: Reinforcement learning meets robocup. Intelligence and Games, 2007.  H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical  Statistics, 22(3):400-407, 1951.  M. Rosenblatt. Markov Processes. Structure and Asymptotic Behavior. Springer, 1971. ISBN  978-3642652400.  N. L. Roux and A. Fitzgibbon. A fast natural Newton method. In Proceedings of the 27th  International Conference on Machine Learning, 2010.  A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal  of Research and Development, 3(3):210-229, 1959.  B. Scherrer. Should one compute the temporal di\ufb00erence fix point or minimize the Bellman residual? the unified oblique projection view. In Proceedings of the 27th International Conference on Machine Learning, 2010.  B. Scherrer and M. Geist. Recursive least-squares learning with eligibility traces. In Pro-  ceedings of the Nineth European Workshop on Reinforcement Learning, 2011.  R. Schoknecht. Optimality of reinforcement learning algorithms with linear function ap-  proximation. In Advances in Neural Information Processing Systems 15, 2002.  P. J. Schweitzer and A. Seidmann. Generalized polynomial approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110(2):568-582, 1985.  D. Silver, R. Sutton, and M. M\u00fcller. Reinforcement learning of local shape in the game of  go. In International Joint Conference on Artificial Intelligence, 2007.  S. Sra, S. Nowozin, and S. J. Wright. Optimization for Machine Learning. MIT Press, 2012.  ISBN 9780262016469.  3(1):9-44, 1988.  R. S. Sutton. Learning to predict by the methods of temporal di\ufb00erences. Machine Learning,  R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Adaptive compu-  tation and machine learning. MIT Press, 1998. ISBN 9780262193986.  R. S. Sutton, D. Precup, and S. Singh.  Intra-option learning about temporally abstract actions. In Proceedings of the 15th International Conference on Machine Learning, 1998.  R. S. Sutton, C. Szepesv\u00e1ri, and H. R. Maei. A convergent O(n) algorithm for o\ufb00-policy temporal-di\ufb00erence learning with linear function approximation. In Advances in Neural Information Processing Systems 21, 2008. Policy Evaluation with Temporal Differences  R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesv\u00e1ri, and E. Wiewiora. Fast gradient-descent methods for temporal-di\ufb00erence learning with lin- ear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.  G. Taylor and R. Parr. Kernelized value function approximation for reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.  G. Tesauro. TD-gammon, a self-teaching backgammon program, achieves master-level play.  Neural Computation, 6(2):215-219, 1994.  J. N. Tsitsiklis and B. van Roy. An analysis of temporal-di\ufb00erence learning with function  approximation. IEEE Transactions On Automatic Control, 42(5):674-690, 1997.  R. J. Williams and L. C. Baird. Tight performance bounds on greedy policies based on imperfect value functions. In Yale Workshop on Adaptive and Learning Systems, 1993.  X. Xu, T. Xie, D. Hu, and X. Lu. Kernel least-squares temporal di\ufb00erence learning. Inter-  national Journal of Information Technology, 11(9):54-63, 2005.  H. Yu. Convergence of least squares temporal di\ufb00erence methods under general conditions.  In Proceedings of the 27th International Conference on Machine Learning, 2010.  P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and  hierarchical variable selection. The Annals of Statistics, 37(6A):3468-3497, 2009. "}, "Active Learning Using Smooth Relative Regret Approximations with Applications": {"volumn": 15, "url": "http://jmlr.org/papers/v15/ailon14a.html", "header": "Active Learning Using Smooth Relative Regret Approximations with Applications", "author": "Nir Ailon, Ron Begleiter, Esther Ezra", "time": "15(25):885\u2212920, 2014.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume15/ailon14a/ailon14a.pdf", "keywords": ["active learning", "learning to rank from pairwise preferences", "semi-supervised clustering", "clustering with side information", "disagreement coe\ufb03cient", "smooth relative regret approximation"], "reference": "Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. Journal of Machine Learning Research, 13:137-164, 2012.  Nir Ailon, Bernard Chazelle, Seshadhri Comandur, and Ding Liu. Estimating the distance  to a monotone function. Random Struct. Algorithms, 31(3):371-383, 2007.  Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information:  Ranking and clustering. Journal of the ACM, 55(5):23:1-23:27, October 2008.  Noga Alon. Ranking tournaments. SIAM Journal on Discrete Mathematics, 20, 2006.  Francis R. Bach. Active learning for misspecified generalized linear models. In B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in Neural Information Processing Systems 19, pages 65-72. MIT Press, Cambridge, MA, 2007.  Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In  ICML, pages 65-72, 2006.  In COLT, pages 35-50, 2007.  Maria-Florina Balcan, Andrei Z. Broder, and Tong Zhang. Margin based active learning.  Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman. The true sample complexity  of active learning. In COLT, pages 45-56, 2008.  Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Approximate clustering without the approximation. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201909, pages 1068-1077, 2009.  916   Ailon, Begleiter and Ezra  vectors. We showed how to take the underlying geometry into account in our framework. We did not do so for clustering with side information. The work of Eriksson et al. (2011) indicates that incorporating geometric information into our analysis is a fruitful direction to pursue.  Our work made no assumptions on the noise, except maybe for its magnitude. An- other promising future research direction would be to incorporate various standard noise assumptions known to improve active learning rates (especially the model of Mammen and Tsybakov, 1999; Tsybakov, 2004) within our setting.  Acknowledgments  We thank Alekh Agarwal, Nina Balcan, Miroslav Dudik, Ran El-Yaniv, Sariel Har-Peled, John Langford, Rob Schapire, Masashi Sugiyama, and Yair Weiner for helpful discussions. Nir Ailon acknowledges the support of a Marie Curie International Reintegration Grant PIRG07-GA-2010-268403. Esther Ezra acknowledges the support of a National Science Foundation Grant CCF-12-16689.  References  Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. Journal of Machine Learning Research, 13:137-164, 2012.  Nir Ailon, Bernard Chazelle, Seshadhri Comandur, and Ding Liu. Estimating the distance  to a monotone function. Random Struct. Algorithms, 31(3):371-383, 2007.  Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information:  Ranking and clustering. Journal of the ACM, 55(5):23:1-23:27, October 2008.  Noga Alon. Ranking tournaments. SIAM Journal on Discrete Mathematics, 20, 2006.  Francis R. Bach. Active learning for misspecified generalized linear models. In B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in Neural Information Processing Systems 19, pages 65-72. MIT Press, Cambridge, MA, 2007.  Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In  ICML, pages 65-72, 2006.  In COLT, pages 35-50, 2007.  Maria-Florina Balcan, Andrei Z. Broder, and Tong Zhang. Margin based active learning.  Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman. The true sample complexity  of active learning. In COLT, pages 45-56, 2008.  Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Approximate clustering without the approximation. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201909, pages 1068-1077, 2009. Active Learning Using SRRAs  Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning,  56:89-113, 2004.  Sugato Basu. Semi-supervised Clustering: Probabilistic Models, Algorithms and Experi- ments. PhD thesis, Department of Computer Sciences, University of Texas at Austin, 2005.  Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Jour-  nal of Computational Biology, 6(3/4):281-297, 1999.  Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learn- ing. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, 2009.  Alina Beygelzimer, Daniel Hsu, John Langford, and Tong Zhang. Agnostic active learning  without constraints. In NIPS, 2010.  Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In SODA, pages  268-276, 2008.  Rui Castro and Robert Nowak. Minimax bounds for active learning. IEEE Transactions  on Information Theory, 54(5):2339-2353, 2008.  Rui Castro, Rebecca Willett, and Robert Nowak. Faster rates in regression via active  learning. In NIPS, 2005.  Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. Linear classification and  selective sampling under low noise conditions. In NIPS, pages 249-256, 2008.  Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. Learning noisy linear clas-  sifiers via adaptive and selective sampling. Machine Learning, 83(1):71-102, 2011.  Nicol`o Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. Active learning  on trees and graphs. In COLT, pages 320-332, 2010.  Nicol`o Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. A correlation clustering approach to link classification in signed networks. In Proceedings of the 25th Annual Conference on Learning Theory. JMLR Workshop and Conference Proceedings, volume 23 of JMLR Workshop and Conference Proceedings, pages 34.1-34.20, 2012.  Moses Charikar and Anthony Wirth. Maximizing quadratic programs: Extending  grothendieck\u2019s inequality. In FOCS, pages 54-60. IEEE Computer Society, 2004.  David Cohn, Rich Caruana, and Andrew Mccallum. Semi-supervised clustering with user feedback. unpublished manuscript, 2000. URL http://www.cs.umass.edu/~mccallum/ papers/semisup-aaai2000s.ps.  Don Coppersmith, Lisa K. Fleischer, and Atri Rurda. Ordering by weighted number of wins gives a good ranking for weighted tournaments. ACM Trans. Algorithms, 6:55:1-55:13, July 2010. Ailon, Begleiter and Ezra  Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.  Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In ICML, pages  208-215, 2008.  algorithm. In NIPS, 2007.  Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni. A general agnostic active learning  Mark de Berg, Otfried Cheong, Marc van Kreveld, and Mark Overmars. Computational  Geometry: Algorithms and Applications. Springer-Verlag, Berlin, 3rd edition, 2008.  Ayhan Demiriz, Kristin Bennett, and Mark J. Embrechts. Semi-supervised clustering using genetic algorithms. In In Artificial Neural Networks in Engineering (ANNIE-99, pages 809-814. ASME Press, 1999.  Persi Diaconis and R. L. Graham. Spearman\u2019s Footrule as a measure of disarray. Journal  of the Royal Statistical Society, 39(2):262-268, 1977.  Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification.  Journal of Machine Learning Research, 11:1605-1641, 2010.  Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert D. Nowak. Active clustering: Robust and e\ufb03cient hierarchical clustering using adaptively selected similarities. Journal of Machine Learning Research - Proceedings Track, 15:260-268, 2011.  Esther Ezra and Shai Fine. On the cover of convex polyhedra in d-space. Unpublished  manuscript, 2007.  Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28:133-168, September 1997.  Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a fixed number of  clusters. Theory of Computing, 2(1):249-266, 2006.  Martin Gr\u00a8otschel, L\u00b4aszl\u00b4o Lov\u00b4asz, and Alexander Schrijver. Geometric Algorithms and Com- binatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer-Verlag, 1988.  Shirley Halevy and Eyal Kushilevitz. Distribution-free property-testing. SIAM J. Comput.,  37(4):1107-1138, 2007.  Steve Hanneke. A bound on the label complexity of agnostic active learning. In ICML,  2007.  2011.  Steve Hanneke. Adaptive rates of convergence in active learning. In COLT, 2009.  Steve Hanneke. Rates of convergence in active learning. Annals of Statistics, 39(1):333-361,  Steve Hanneke and Liu Yang. Negative results for active learning with convex losses. Journal  of Machine Learning Research - Proceedings Track, 9:321-325, 2010. Active Learning Using SRRAs  David Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Control, 100(1):78-150, September 1992.  Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin ranking boundaries for ordinal regression. In Advances in Large Margin Classifiers, chapter 7, pages 115-132. The MIT Press, 2000.  Kevin G. Jamieson and Rob Nowak. Active ranking using pairwise comparisons. In NIPS  24, pages 2240-2248, 2011.  Thorsten Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.  Matti K\u00a8a\u00a8ari\u00a8ainen. Active learning in the non-realizable case. In ALT, pages 63-77, 2006.  Narendra Karmarkar. A new polynomial-time algorithm for linear programming. Combi-  natorica, 4:373-395, 1984.  Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing, STOC \u201907, pages 95-103, 2007.  Leonid Kha\u02c7ciyan. Polynomial algorithm for linear programming. Soviet Doklady, 244:  1093-1096, 1979.  Dan Klein, Sepandar D. Kamvar, and Christopher D. Manning. From instance-level con- straints to space-level constraints: Making the most of prior knowledge in data clustering. In ICML, pages 307-314, 2002.  Vladimir Koltchinskii. Rademacher complexities and bounding the excess risk in active  learning. Journal of Machine Learning Research, 11:2457-2485, 2010.  Yi Li, Philip M. Long, and Aravind Srinivasan. Improved bounds on the sample complexity  of learning. Journal of Computer and System Sciences, 62:2001, 2000.  Bo Long, Olivier Chapelle, Ya Zhang, Yi Chang, Zhaohui Zheng, and Belle Tseng. Active learning for ranking through expected loss optimization. In Proceedings of the 33rd Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 267-274, 2010.  Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Annals of  Statistics, 27:1808-1829, 1999.  Brian Mcfee and Gert Lanckriet. Metric learning to rank. In In Proceedings of the 27th  Annual International Conference on Machine Learning (ICML), 2010.  Stanislav Minsker. Plug-in approach to active learning. Journal of Machine Learning  Research, 13:67-90, 2012.  ICML, pages 433-440, 2011.  Francesco Orabona and Nicol`o Cesa-Bianchi. Better algorithms for selective sampling. In Ailon, Begleiter and Ezra  Kira Radinsky and Nir Ailon. Ranking from pairs and triplets: information quality, evalu-  ation methods and query complexity. In WSDM, pages 105-114, 2011.  Burr Settles. Active learning literature survey. Technical Report 1648, University of  Wisconsin-Madison, 2009.  Ohad Shamir and Naftali Tishby. Spectral clustering on a budget. Journal of Machine  Learning Research - Proceedings Track, 15:661-669, 2011.  Ron Shamir, Roded Sharan, and Dekel Tsur. Cluster graph modification problems. Discrete  Applied Math, 144:173-182, nov 2004.  Micha Sharir and Pankaj K. Agarwal. Davenport-Schinzel Sequences and Their Geometric  Applications. Cambridge University Press, 1995.  Masashi Sugiyama. Active learning in approximately linear regression based on conditional expectation of generalization error. Journal of Machine Learning Research, 7:141-166, 2006.  Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals  of Statistics, 32:135-166, 2004.  Konstantin Voevodski, Maria-Florina Balcan, Heiko R\u00a8oglin, Shang-Hua Teng, and Yu Xia. Active clustering of biological sequences. Journal of Machine Learning Research, 13: 203-225, 2012.  Liwei Wang. Smoothness, disagreement coe\ufb03cient, and the label complexity of agnostic  active learning. Journal of Machine Learning Research, 12:2269-2292, 2011.  Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. Distance metric In Advances in Neural  learning, with application to clustering with side-information. Information Processing Systems 15, pages 505-512. MIT Press, 2002.  Liu Yang, Steve Hanneke, and Jaime G. Carbonell. Bayesian active learning using arbitrary  binary valued queries. In ALT, pages 50-58, 2010.  Liu Yang, Steve Hanneke, and Jaime G. Carbonell. The sample complexity of self-verifying bayesian active learning. Journal of Machine Learning Research - Proceedings Track, 15: 816-822, 2011. "}, "Natural Evolution Strategies": {"volumn": 15, "url": "http://jmlr.org/papers/v15/wierstra14a.html", "header": "Natural Evolution Strategies", "author": "Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, J\\\"{u}rgen Schmidhuber", "time": "15(27):949\u2212980, 2014.", "abstract": "This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.", "pdf_url": "http://jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf"}, "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability": {"volumn": 15, "url": "http://jmlr.org/papers/v15/mizutani14a.html", "header": "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability", "author": "Tomohiko Mizutani", "time": "15(29):1011\u22121039, 2014.", "abstract": "We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results.", "pdf_url": "http://jmlr.org/papers/volume15/mizutani14a/mizutani14a.pdf"}, "Improving Prediction from Dirichlet Process Mixtures via Enrichment": {"volumn": 15, "url": "http://jmlr.org/papers/v15/wade14a.html", "header": "Improving Prediction from Dirichlet Process Mixtures via Enrichment", "author": "Sara Wade, David B. Dunson, Sonia Petrone, Lorenzo Trippa", "time": "15(30):1041\u22121071, 2014.", "abstract": "Flexible covariate-dependent density estimation can be achieved by modelling the joint density of the response and covariates as a Dirichlet process mixture. An appealing aspect of this approach is that computations are relatively easy. In this paper, we examine the predictive performance of these models with an increasing number of covariates. Even for a moderate number of covariates, we find that the likelihood for $x$ tends to dominate the posterior of the latent random partition, degrading the predictive performance of the model. To overcome this, we suggest using a different nonparametric prior, namely an enriched Dirichlet process. Our proposal maintains a simple allocation rule, so that computations remain relatively simple. Advantages are shown through both predictive equations and examples, including an application to diagnosis Alzheimer's disease.", "pdf_url": "http://jmlr.org/papers/volume15/wade14a/wade14a.pdf"}, "A Reliable Effective Terascale Linear Learning System": {"volumn": 15, "url": "http://jmlr.org/papers/v15/agarwal14a.html", "header": "A Reliable Effective Terascale Linear Learning System", "author": "Alekh Agarwal, Oliveier Chapelle, Miroslav Dud\\'{i}k, John Langford", "time": "15(32):1111\u22121133, 2014.", "abstract": "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, (The number of features here refers to the number of non-zero entries in the data matrix.) billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. (All the empirical evaluation reported in this work was carried out between May-Oct 2011.) We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.", "pdf_url": "http://jmlr.org/papers/volume15/agarwal14a/agarwal14a.pdf"}, "New Learning Methods for Supervised and Unsupervised Preference Aggregation": {"volumn": 15, "url": "http://jmlr.org/papers/v15/volkovs14a.html", "header": "New Learning Methods for Supervised and Unsupervised Preference Aggregation", "author": "Maksims N. Volkovs, Richard S. Zemel", "time": "15(33):1135\u22121176, 2014.", "abstract": "In this paper we present a general treatment of the preference aggregation problem, in which multiple preferences over objects must be combined into a single consensus ranking. We consider two instances of this problem: unsupervised aggregation where no information about a target ranking is available, and supervised aggregation where ground truth preferences are provided. For each problem class we develop novel learning methods that are applicable to a wide range of preference types. (The code for all models introduced in this paper is available at  www.cs.toronto.edu/~mvolkovs .)  Specifically, for unsupervised aggregation we introduce the Multinomial Preference model (MPM) which uses a multinomial generative process to model the observed preferences. For the supervised problem we develop a supervised extension for MPM and then propose two fully supervised models. The first model employs SVD factorization to derive effective item features, transforming the aggregation problems into a learning-to-rank one. The second model aims to eliminate the costly SVD factorization and instantiates a probabilistic CRF framework, deriving unary and pairwise potentials directly from the observed preferences. Using a probabilistic framework allows us to directly optimize the expectation of any target metric, such as NDCG or ERR. All the proposed models operate on pairwise preferences and can thus be applied to a wide range of preference types. We empirically validate the models on rank aggregation and collaborative filtering data sets and demonstrate superior empirical accuracy.", "pdf_url": "http://jmlr.org/papers/volume15/volkovs14a/volkovs14a.pdf"}, "Prediction and Clustering in Signed Networks: A Local to Global Perspective": {"volumn": 15, "url": "http://jmlr.org/papers/v15/chiang14a.html", "header": "Prediction and Clustering in Signed Networks: A Local to Global Perspective", "author": "Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S. Dhillon, Ambuj Tewari", "time": "15(34):1177\u22121213, 2014.", "abstract": "The study of social networks is a burgeoning research area. However, most existing work is on networks that simply encode whether relationships exist or not. In contrast, relationships in  signed  networks can be positive (\u00c3\u00a2\u00c2\u0080\u00c2\u009clike\", \u00c3\u00a2\u00c2\u0080\u00c2\u009ctrust\") or negative (\u00c3\u00a2\u00c2\u0080\u00c2\u009cdislike\", \u00c3\u00a2\u00c2\u0080\u00c2\u009cdistrust\"). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Local patterns of social balance have been used in the past for sign prediction. We define more general measures of social imbalance (MOIs) based on $\\ell$-cycles in the network and give a simple sign prediction rule. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, also has a balance theoretic interpretation when applied to signed networks. Motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. We provide theoretical performance guarantees for our low-rank matrix completion approach via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of social balance, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks.", "pdf_url": "http://jmlr.org/papers/volume15/chiang14a/chiang14a.pdf"}, "Bayesian Nonparametric Comorbidity Analysis of Psychiatric Disorders": {"volumn": 15, "url": "http://jmlr.org/papers/v15/ruiz14a.html", "header": "Bayesian Nonparametric Comorbidity Analysis of Psychiatric Disorders", "author": "Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fern, o Perez-Cruz", "time": "15(35):1215\u22121247, 2014.", "abstract": "The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial- logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.", "pdf_url": "http://jmlr.org/papers/volume15/ruiz14a/ruiz14a.pdf"}, "Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization": {"volumn": 15, "url": "http://jmlr.org/papers/v15/gillis14a.html", "header": "Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization", "author": "Nicolas Gillis, Robert Luce", "time": "15(36):1249\u22121280, 2014.", "abstract": "Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the  separability assumption , under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, R\u00c3\u0083\u00c2\u00a9 and Tropp (`Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic data sets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods.", "pdf_url": "http://jmlr.org/papers/volume15/gillis14a/gillis14a.pdf"}, "Follow the Leader If You Can, Hedge If You Must": {"volumn": 15, "url": "http://jmlr.org/papers/v15/rooij14a.html", "header": "Follow the Leader If You Can, Hedge If You Must", "author": "Steven de Rooij, Tim van Erven, Peter D. Gr\u00c3\u00bcnwald, Wouter M. Koolen", "time": "15(37):1281\u22121316, 2014.", "abstract": "Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case  guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains.", "pdf_url": "http://jmlr.org/papers/volume15/rooij14a/rooij14a.pdf"}, "Structured Prediction via Output Space Search": {"volumn": 15, "url": "http://jmlr.org/papers/v15/doppa14a.html", "header": "Structured Prediction via Output Space Search", "author": "Janardhan Rao Doppa, Alan Fern, Prasad Tadepalli", "time": "15(38):1317\u22121350, 2014.", "abstract": "We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time- bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we describe a novel approach to automatically defining an effective search space over structured outputs, which is able to leverage the availability of powerful classification learning algorithms. In particular, we define the limited-discrepancy search space and relate the quality of that space to the quality of learned classifiers. We also define a sparse version of the search space to improve the efficiency of our overall approach. Second, we give a generic cost function learning approach that is applicable to a wide range of search procedures. The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains show that a small amount of search in limited discrepancy search space is often sufficient for significantly improving on state-of-the-art structured- prediction  performance. We also demonstrate significant speed improvements for our approach using sparse search spaces with little or no loss in accuracy.", "pdf_url": "http://jmlr.org/papers/volume15/doppa14a/doppa14a.pdf"}, "Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing": {"volumn": 15, "url": "http://jmlr.org/papers/v15/wand14a.html", "header": "Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing", "author": "Matt P. W, ", "time": "15(39):1351\u22121369, 2014.", "abstract": "Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.", "pdf_url": "http://jmlr.org/papers/volume15/wand14a/wand14a.pdf"}, "Towards Ultrahigh Dimensional Feature Selection for Big Data": {"volumn": 15, "url": "http://jmlr.org/papers/v15/tan14a.html", "header": "Towards Ultrahigh Dimensional Feature Selection for Big Data", "author": "Mingkui Tan, Ivor W. Tsang, Li Wang", "time": "15(40):1371\u22121429, 2014.", "abstract": "In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on  Big Data , and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient  feature generating paradigm . Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with $O(10^{14})$ features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training efficiency.", "pdf_url": "http://jmlr.org/papers/volume15/tan14a/tan14a.pdf"}, "Adaptive Sampling for Large Scale Boosting": {"volumn": 15, "url": "http://jmlr.org/papers/v15/dubout14a.html", "header": "Adaptive Sampling for Large Scale Boosting", "author": "Charles Dubout, Francois Fleuret", "time": "15(41):1431\u22121453, 2014.", "abstract": "Classical boosting algorithms, such as AdaBoost, build a strong classifier without concern for the computational cost. Some applications, in particular in computer vision, may involve millions of training examples and very large feature spaces. In such contexts, the training time of off-the-shelf boosting algorithms may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features or the examples used to train the weak learners. Even if some of these methods provide a guaranteed speed improvement, they offer no insurance of being more efficient than any other, given the same amount of time. The contributions of this paper are twofold: (1) a strategy to better deal with the increasingly common case where features come from multiple sources (for example, color, shape, texture, etc., in the case of images) and therefore can be partitioned into meaningful subsets; (2) new algorithms which balance at every boosting iteration the number of weak learners and the number of training examples to look at in order to maximize the expected loss reduction. Experiments in image classification and object recognition on four standard computer vision data sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.", "pdf_url": "http://jmlr.org/papers/volume15/dubout14a/dubout14a.pdf"}, "Manopt, a Matlab Toolbox for Optimization on Manifolds": {"volumn": 15, "url": "http://jmlr.org/papers/v15/boumal14a.html", "header": "Manopt, a Matlab Toolbox for Optimization on Manifolds", "author": "Nicolas Boumal, Bamdev Mishra, P.-A. Absil, Rodolphe Sepulchre", "time": "15(42):1455\u22121459, 2014.", "abstract": "", "pdf_url": "http://jmlr.org/papers/volume15/boumal14a/boumal14a.pdf"}, "Training Highly Multiclass Classifiers": {"volumn": 15, "url": "http://jmlr.org/papers/v15/gupta14a.html", "header": "Training Highly Multiclass Classifiers", "author": "Maya R. Gupta, Samy Bengio, Jason Weston", "time": "15(43):1461\u22121492, 2014.", "abstract": "Classification problems with thousands or more classes often have a large range of class-confusabilities, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent, can also be profitably applied to the nonconvex joint training of supervised dimensionality reduction and linear classifiers as done in Wsabie. Experiments on ImageNet benchmark data sets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs- all linear SVMs and Wsabie.", "pdf_url": "http://jmlr.org/papers/volume15/gupta14a/gupta14a.pdf"}, "Locally Adaptive Factor Processes for Multivariate Time Series": {"volumn": 15, "url": "http://jmlr.org/papers/v15/durante14a.html", "header": "Locally Adaptive Factor Processes for Multivariate Time Series", "author": "Daniele Durante, Bruno Scarpa, David B. Dunson", "time": "15(44):1493\u22121522, 2014.", "abstract": "In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time- varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.", "pdf_url": "http://jmlr.org/papers/volume15/durante14a/durante14a.pdf"}, "Iteration Complexity of Feasible Descent Methods for Convex Optimization": {"volumn": 15, "url": "http://jmlr.org/papers/v15/wang14a.html", "header": "Iteration Complexity of Feasible Descent Methods for Convex Optimization", "author": "Po-Wei Wang, Chih-Jen Lin", "time": "15(45):1523\u22121548, 2014.", "abstract": "In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove $O(\\log(1/\\epsilon))$ time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression.", "pdf_url": "http://jmlr.org/papers/volume15/wang14a/wang14a.pdf"}, "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models": {"volumn": 15, "url": "http://jmlr.org/papers/v15/janzamin14a.html", "header": "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models", "author": "Majid Janzamin, Animashree Anandkumar", "time": "15(46):1549\u22121591, 2014.", "abstract": "Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. %We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular $\\ell_1$-penalized maximum- likelihood  estimator ($\\ell_1$-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples $n$ scales as $n = \\Omega(d^2 \\log p)$,  where $p$ is the number of variables and $d$ is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.", "pdf_url": "http://jmlr.org/papers/volume15/janzamin14a/janzamin14a.pdf"}, "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo": {"volumn": 15, "url": "http://jmlr.org/papers/v15/hoffman14a.html", "header": "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo", "author": "Matthew D. Hoffman, Andrew Gelman", "time": "15(47):1593\u22121623, 2014.", "abstract": "Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size $\\epsilon$ and a desired number of steps $L$. In particular, if $L$ is too small then the algorithm exhibits undesirable random walk behavior, while if $L$ is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps $L$. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter $\\epsilon$ on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient \u00c3\u00a2\u00c2\u0080\u00c2\u009cturnkey\u00c3\u00a2\u00c2\u0080\u00c2\u009d samplers.", "pdf_url": "http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf"}, "Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife": {"volumn": 15, "url": "http://jmlr.org/papers/v15/wager14a.html", "header": "Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife", "author": "Stefan Wager, Trevor Hastie, Bradley Efron", "time": "15(48):1625\u22121651, 2014.", "abstract": "We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number $B$ of bootstrap replicates, and working with a large $B$ can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require $B = \\Theta (n^{1.5})$ bootstrap replicates to converge, where $n$ is the size of the training set. We propose improved versions that only require $B = \\Theta (n)$ replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.", "pdf_url": "http://jmlr.org/papers/volume15/wager14a/wager14a.pdf"}, "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses": {"volumn": 15, "url": "http://jmlr.org/papers/v15/agarwal14b.html", "header": "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses", "author": "Shivani Agarwal", "time": "15(49):1653\u22121674, 2014.", "abstract": "The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non- pairwise) logistic and exponential losses. In this paper, we show that such (non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as  strongly proper . Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).", "pdf_url": "http://jmlr.org/papers/volume15/agarwal14b/agarwal14b.pdf"}, "Graph Estimation From Multi-Attribute Data": {"volumn": 15, "url": "http://jmlr.org/papers/v15/kolar14a.html", "header": "Graph Estimation From Multi-Attribute Data", "author": "Mladen Kolar, Han Liu, Eric P. Xing", "time": "15(51):1713\u22121750, 2014.", "abstract": "Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data. For example, they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents, or multi-view feature vectors. In this paper, we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate (or multi-attribute) nodal data. The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes. Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection (Dempster, 1972).  We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective. Extensive simulation studies demonstrate the effectiveness of the method under various conditions. We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from positron emission tomography data. Finally, we provide sufficient conditions under which the true graphical structure can be recovered correctly.", "pdf_url": "http://jmlr.org/papers/volume15/kolar14a/kolar14a.pdf"}, "Hitting and Commute Times in Large Random Neighborhood Graphs": {"volumn": 15, "url": "http://jmlr.org/papers/v15/vonluxburg14a.html", "header": "Hitting and Commute Times in Large Random Neighborhood Graphs", "author": "Ulrike von Luxburg, Agnes Radl, Matthias Hein", "time": "15(52):1751\u22121798, 2014.", "abstract": "In machine learning, a popular tool to analyze the structure of graphs is the hitting time and the commute distance (resistance distance). For two vertices $u$ and $v$, the hitting time $H_{uv}$ is the expected time it takes a random walk to travel from $u$ to $v$. The commute distance is its symmetrized version $C_{uv} = H_{uv} + H_{vu}$. In our paper we study the behavior of hitting times and commute distances when the number $n$ of vertices in the graph tends to infinity. We focus on random geometric graphs ($\\epsilon$-graphs, kNN graphs and Gaussian similarity graphs), but our results also extend to graphs with a given expected degree distribution or Erdos-Renyi graphs with planted partitions. We prove that in these graph families, the suitably rescaled hitting time $H_{uv}$ converges to $1/d_v$ and the rescaled commute time to $1/d_u + 1/d_v$ where $d_u$ and $d_v$ denote the degrees of vertices $u$ and $v$. In these cases, hitting and commute times do not provide information about the structure of the graph, and their use is discouraged in many machine learning applications.", "pdf_url": "http://jmlr.org/papers/volume15/vonluxburg14a/vonluxburg14a.pdf"}, "Bayesian Inference with Posterior Regularization and Applications to Infinite Latent SVMs": {"volumn": 15, "url": "http://jmlr.org/papers/v15/zhu14b.html", "header": "Bayesian Inference with Posterior Regularization and Applications to Infinite Latent SVMs", "author": "Jun Zhu, Ning Chen, Eric P. Xing", "time": "15(53):1799\u22121847, 2014.", "abstract": "Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present  regularized Bayesian inference  (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes,  infinite latent support vector machines  (iLSVM) and  multi-task infinite latent support vector machines  (MT-iLSVM), which explore the large- margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.", "pdf_url": "http://jmlr.org/papers/volume15/zhu14b/zhu14b.pdf"}, "Expectation Propagation for Neural Networks with Sparsity-Promoting Priors": {"volumn": 15, "url": "http://jmlr.org/papers/v15/jylanki14a.html", "header": "Expectation Propagation for Neural Networks with Sparsity-Promoting Priors", "author": "Pasi Jyl\u00c3\u00a4nki, Aapo Nummenmaa, Aki Vehtari", "time": "15(54):1849\u22121901, 2014.", "abstract": "We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity- favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally efficient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters.", "pdf_url": "http://jmlr.org/papers/volume15/jylanki14a/jylanki14a.pdf"}, "Sparse Factor Analysis for Learning and Content Analytics": {"volumn": 15, "url": "http://jmlr.org/papers/v15/lan14a.html", "header": "Sparse Factor Analysis for Learning and Content Analytics", "author": "Andrew S. Lan, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk", "time": "15(57):1959\u22122008, 2014.", "abstract": "We develop a new model and algorithms for machine learning-based  learning analytics , which estimate a learner's knowledge of the concepts underlying a domain, and {\\em{content analytics}}, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood-based solution and a Bayesian solution to the resulting  SPARse Factor Analysis  (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.", "pdf_url": "http://jmlr.org/papers/volume15/lan14a/lan14a.pdf"}}